[{"id": "2105.00020", "submitter": "Ruowei Jiang", "authors": "Zeqi Li, Ruowei Jiang and Parham Aarabi", "title": "Continuous Face Aging via Self-estimated Residual Age Embedding", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face synthesis, including face aging, in particular, has been one of the\nmajor topics that witnessed a substantial improvement in image fidelity by\nusing generative adversarial networks (GANs). Most existing face aging\napproaches divide the dataset into several age groups and leverage group-based\ntraining strategies, which lacks the ability to provide fine-controlled\ncontinuous aging synthesis in nature. In this work, we propose a unified\nnetwork structure that embeds a linear age estimator into a GAN-based model,\nwhere the embedded age estimator is trained jointly with the encoder and\ndecoder to estimate the age of a face image and provide a personalized target\nage embedding for age progression/regression. The personalized target age\nembedding is synthesized by incorporating both personalized residual age\nembedding of the current age and exemplar-face aging basis of the target age,\nwhere all preceding aging bases are derived from the learned weights of the\nlinear age estimator. This formulation brings the unified perspective of\nestimating the age and generating personalized aged face, where self-estimated\nage embeddings can be learned for every single age. The qualitative and\nquantitative evaluations on different datasets further demonstrate the\nsignificant improvement in the continuous face aging aspect over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:06:17 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Zeqi", ""], ["Jiang", "Ruowei", ""], ["Aarabi", "Parham", ""]]}, {"id": "2105.00043", "submitter": "Suraj Kothawade", "authors": "Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan, Jeff Bilmes,\n  Rishabh Iyer", "title": "Submodular Mutual Information for Targeted Data Subset Selection", "comments": "Accepted to ICLR 2021 S2D-OLAD Workshop; https://s2d-olad.github.io/.\n  arXiv admin note: substantial text overlap with arXiv:2103.00128", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of data, it is becoming increasingly difficult to train\nor improve deep learning models with the right subset of data. We show that\nthis problem can be effectively solved at an additional labeling cost by\ntargeted data subset selection(TSS) where a subset of unlabeled data points\nsimilar to an auxiliary set are added to the training data. We do so by using a\nrich class of Submodular Mutual Information (SMI) functions and demonstrate its\neffectiveness for image classification on CIFAR-10 and MNIST datasets. Lastly,\nwe compare the performance of SMI functions for TSS with other state-of-the-art\nmethods for closely related problems like active learning. Using SMI functions,\nwe observe ~20-30% gain over the model's performance before re-training with\nadded targeted subset; ~12% more than other methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:53:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kothawade", "Suraj", ""], ["Kaushal", "Vishal", ""], ["Ramakrishnan", "Ganesh", ""], ["Bilmes", "Jeff", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2105.00067", "submitter": "Swetha Sirnam", "authors": "Sirnam Swetha, Hilde Kuehne, Yogesh S Rawat, Mubarak Shah", "title": "Unsupervised Discriminative Embedding for Sub-Action Learning in Complex\n  Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Action recognition and detection in the context of long untrimmed video\nsequences has seen an increased attention from the research community. However,\nannotation of complex activities is usually time consuming and challenging in\npractice. Therefore, recent works started to tackle the problem of unsupervised\nlearning of sub-actions in complex activities. This paper proposes a novel\napproach for unsupervised sub-action learning in complex activities. The\nproposed method maps both visual and temporal representations to a latent space\nwhere the sub-actions are learnt discriminatively in an end-to-end fashion. To\nthis end, we propose to learn sub-actions as latent concepts and a novel\ndiscriminative latent concept learning (DLCL) module aids in learning\nsub-actions. The proposed DLCL module lends on the idea of latent concepts to\nlearn compact representations in the latent embedding space in an unsupervised\nway. The result is a set of latent vectors that can be interpreted as cluster\ncenters in the embedding space. The latent space itself is formed by a joint\nvisual and temporal embedding capturing the visual similarity and temporal\nordering of the data. Our joint learning with discriminative latent concept\nmodule is novel which eliminates the need for explicit clustering. We validate\nour approach on three benchmark datasets and show that the proposed combination\nof visual-temporal embedding and discriminative latent concepts allow to learn\nrobust action representations in an unsupervised setting.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:07:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Swetha", "Sirnam", ""], ["Kuehne", "Hilde", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "2105.00097", "submitter": "Nikita Araslanov", "authors": "Nikita Araslanov and Stefan Roth", "title": "Self-supervised Augmentation Consistency for Adapting Semantic\n  Segmentation", "comments": "To appear at CVPR 2021. Code: https://github.com/visinf/da-sac", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to domain adaptation for semantic segmentation that is\nboth practical and highly accurate. In contrast to previous work, we abandon\nthe use of computationally involved adversarial objectives, network ensembles\nand style transfer. Instead, we employ standard data augmentation techniques\n$-$ photometric noise, flipping and scaling $-$ and ensure consistency of the\nsemantic predictions across these image transformations. We develop this\nprinciple in a lightweight self-supervised framework trained on co-evolving\npseudo labels without the need for cumbersome extra training rounds. Simple in\ntraining from a practitioner's standpoint, our approach is remarkably\neffective. We achieve significant improvements of the state-of-the-art\nsegmentation accuracy after adaptation, consistent both across different\nchoices of the backbone architecture and adaptation scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 21:32:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Araslanov", "Nikita", ""], ["Roth", "Stefan", ""]]}, {"id": "2105.00101", "submitter": "Xiaofeng Liu", "authors": "Yubin Ge, Site Li, Xuyang Li, Fangfang Fan, Wanqing Xie, Jane You,\n  Xiaofeng Liu", "title": "Embedding Semantic Hierarchy in Discrete Optimal Transport for Risk\n  Minimization", "comments": "Accepted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widely-used cross-entropy (CE) loss-based deep networks achieved\nsignificant progress w.r.t. the classification accuracy. However, the CE loss\ncan essentially ignore the risk of misclassification which is usually measured\nby the distance between the prediction and label in a semantic hierarchical\ntree. In this paper, we propose to incorporate the risk-aware inter-class\ncorrelation in a discrete optimal transport (DOT) training framework by\nconfiguring its ground distance matrix. The ground distance matrix can be\npre-defined following a priori of hierarchical semantic risk. Specifically, we\ndefine the tree induced error (TIE) on a hierarchical semantic tree and extend\nit to its increasing function from the optimization perspective. The semantic\nsimilarity in each level of a tree is integrated with the information gain. We\nachieve promising results on several large scale image classification tasks\nwith a semantic tree structure in a plug and play manner.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 21:47:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ge", "Yubin", ""], ["Li", "Site", ""], ["Li", "Xuyang", ""], ["Fan", "Fangfang", ""], ["Xie", "Wanqing", ""], ["You", "Jane", ""], ["Liu", "Xiaofeng", ""]]}, {"id": "2105.00113", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky", "title": "IPatch: A Remote Adversarial Patch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Applications such as autonomous vehicles and medical screening use deep\nlearning models to localize and identify hundreds of objects in a single frame.\nIn the past, it has been shown how an attacker can fool these models by placing\nan adversarial patch within a scene. However, these patches must be placed in\nthe target location and do not explicitly alter the semantics elsewhere in the\nimage.\n  In this paper, we introduce a new type of adversarial patch which alters a\nmodel's perception of an image's semantics. These patches can be placed\nanywhere within an image to change the classification or semantics of locations\nfar from the patch. We call this new class of adversarial examples `remote\nadversarial patches' (RAP).\n  We implement our own RAP called IPatch and perform an in-depth analysis on\nimage segmentation RAP attacks using five state-of-the-art architectures with\neight different encoders on the CamVid street view dataset. Moreover, we\ndemonstrate that the attack can be extended to object recognition models with\npreliminary results on the popular YOLOv3 model. We found that the patch can\nchange the classification of a remote target region with a success rate of up\nto 93% on average.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:34:32 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:21:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mirsky", "Yisroel", ""]]}, {"id": "2105.00114", "submitter": "Jinkyu Lee", "authors": "Jinkyu Lee, Muhyun Back, Sung Soo Hwang and Il Yong Chun", "title": "Improved Real-Time Monocular SLAM Using Semantic Segmentation on\n  Selective Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular simultaneous localization and mapping (SLAM) is emerging in\nadvanced driver assistance systems and autonomous driving, because a single\ncamera is cheap and easy to install. Conventional monocular SLAM has two major\nchallenges leading inaccurate localization and mapping. First, it is\nchallenging to estimate scales in localization and mapping. Second,\nconventional monocular SLAM uses inappropriate mapping factors such as dynamic\nobjects and low-parallax ares in mapping. This paper proposes an improved\nreal-time monocular SLAM that resolves the aforementioned challenges by\nefficiently using deep learning-based semantic segmentation. To achieve the\nreal-time execution of the proposed method, we apply semantic segmentation only\nto downsampled keyframes in parallel with mapping processes. In addition, the\nproposed method corrects scales of camera poses and three-dimensional (3D)\npoints, using estimated ground plane from road-labeled 3D points and the real\ncamera height. The proposed method also removes inappropriate corner features\nlabeled as moving objects and low parallax areas. Experiments with six video\nsequences demonstrate that the proposed monocular SLAM system achieves\nsignificantly more accurate trajectory tracking accuracy compared to\nstate-of-the-art monocular SLAM and comparable trajectory tracking accuracy\ncompared to state-of-the-art stereo SLAM.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:34:45 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Jinkyu", ""], ["Back", "Muhyun", ""], ["Hwang", "Sung Soo", ""], ["Chun", "Il Yong", ""]]}, {"id": "2105.00125", "submitter": "Bo Liu", "authors": "Bo Liu, Mandar Dixit, Roland Kwitt, Gang Hua, Nuno Vasconcelos", "title": "Sparse Pose Trajectory Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method to learn, even using a dataset where objects appear only\nin sparsely sampled views (e.g. Pix3D), the ability to synthesize a pose\ntrajectory for an arbitrary reference image. This is achieved with a\ncross-modal pose trajectory transfer mechanism. First, a domain transfer\nfunction is trained to predict, from an RGB image of the object, its 2D depth\nmap. Then, a set of image views is generated by learning to simulate object\nrotation in the depth space. Finally, the generated poses are mapped from this\nlatent space into a set of corresponding RGB images using a learned identity\npreserving transform. This results in a dense pose trajectory of the object in\nimage space. For each object type (e.g., a specific Ikea chair model), a 3D CAD\nmodel is used to render a full pose trajectory of 2D depth maps. In the absence\nof dense pose sampling in image space, these latent space trajectories provide\ncross-modal guidance for learning. The learned pose trajectories can be\ntransferred to unseen examples, effectively synthesizing all object views in\nimage space. Our method is evaluated on the Pix3D and ShapeNet datasets, in the\nsetting of novel view synthesis under sparse pose supervision, demonstrating\nsubstantial improvements over recent art.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:07:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Bo", ""], ["Dixit", "Mandar", ""], ["Kwitt", "Roland", ""], ["Hua", "Gang", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2105.00127", "submitter": "Bo Liu", "authors": "Bo Liu, Haoxiang Li, Hao Kang, Gang Hua, Nuno Vasconcelos", "title": "Breadcrumbs: Adversarial Class-Balanced Sampling for Long-tailed\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of long-tailed recognition, where the number of examples per\nclass is highly unbalanced, is considered. While training with class-balanced\nsampling has been shown effective for this problem, it is known to over-fit to\nfew-shot classes. It is hypothesized that this is due to the repeated sampling\nof examples and can be addressed by feature space augmentation. A new feature\naugmentation strategy, EMANATE, based on back-tracking of features across\nepochs during training, is proposed. It is shown that, unlike class-balanced\nsampling, this is an adversarial augmentation strategy. A new sampling\nprocedure, Breadcrumb, is then introduced to implement adversarial\nclass-balanced sampling without extra computation. Experiments on three popular\nlong-tailed recognition datasets show that Breadcrumb training produces\nclassifiers that outperform existing solutions to the problem.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:21:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Bo", ""], ["Li", "Haoxiang", ""], ["Kang", "Hao", ""], ["Hua", "Gang", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2105.00131", "submitter": "Bo Liu", "authors": "Bo Liu, Haoxiang Li, Hao Kang, Gang Hua, Nuno Vasconcelos", "title": "GistNet: a Geometric Structure Transfer Network for Long-Tailed\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of long-tailed recognition, where the number of examples per\nclass is highly unbalanced, is considered. It is hypothesized that the well\nknown tendency of standard classifier training to overfit to popular classes\ncan be exploited for effective transfer learning. Rather than eliminating this\noverfitting, e.g. by adopting popular class-balanced sampling methods, the\nlearning algorithm should instead leverage this overfitting to transfer\ngeometric information from popular to low-shot classes. A new classifier\narchitecture, GistNet, is proposed to support this goal, using constellations\nof classifier parameters to encode the class geometry. A new learning algorithm\nis then proposed for GeometrIc Structure Transfer (GIST), with resort to a\ncombination of loss functions that combine class-balanced and random sampling\nto guarantee that, while overfitting to the popular classes is restricted to\ngeometric parameters, it is leveraged to transfer class geometry from popular\nto few-shot classes. This enables better generalization for few-shot classes\nwithout the need for the manual specification of class weights, or even the\nexplicit grouping of classes into different types. Experiments on two popular\nlong-tailed recognition datasets show that GistNet outperforms existing\nsolutions to this problem.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:37:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Bo", ""], ["Li", "Haoxiang", ""], ["Kang", "Hao", ""], ["Hua", "Gang", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2105.00133", "submitter": "Bo Liu", "authors": "Bo Liu, Haoxiang Li, Hao Kang, Nuno Vasconcelos, Gang Hua", "title": "Semi-supervised Long-tailed Recognition using Alternate Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Main challenges in long-tailed recognition come from the imbalanced data\ndistribution and sample scarcity in its tail classes. While techniques have\nbeen proposed to achieve a more balanced training loss and to improve tail\nclasses data variations with synthesized samples, we resort to leverage readily\navailable unlabeled data to boost recognition accuracy. The idea leads to a new\nrecognition setting, namely semi-supervised long-tailed recognition. We argue\nthis setting better resembles the real-world data collection and annotation\nprocess and hence can help close the gap to real-world scenarios. To address\nthe semi-supervised long-tailed recognition problem, we present an alternate\nsampling framework combining the intuitions from successful methods in these\ntwo research areas. The classifier and feature embedding are learned separately\nand updated iteratively. The class-balanced sampling strategy has been\nimplemented to train the classifier in a way not affected by the pseudo labels'\nquality on the unlabeled data. A consistency loss has been introduced to limit\nthe impact from unlabeled data while leveraging them to update the feature\nembedding. We demonstrate significant accuracy improvements over other\ncompetitive methods on two datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:43:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Bo", ""], ["Li", "Haoxiang", ""], ["Kang", "Hao", ""], ["Vasconcelos", "Nuno", ""], ["Hua", "Gang", ""]]}, {"id": "2105.00149", "submitter": "Zhaoxin Fan", "authors": "Zhaoxin Fan, Zhenbo Song, Hongyan Liu, Zhiwu Lu, Jun He and Xiaoyong\n  Du", "title": "SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale\n  Place Recognition", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud-based large scale place recognition is fundamental for many\napplications like Simultaneous Localization and Mapping (SLAM). Although many\nmodels have been proposed and have achieved good performance by learning\nshort-range local features, long-range contextual properties have often been\nneglected. Moreover, the model size has also become a bottleneck for their wide\napplications. To overcome these challenges, we propose a super light-weight\nnetwork model termed SVT-Net for large scale place recognition. Specifically,\non top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based\nSparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer\n(CSVT) are proposed to learn both short-range local features and long-range\ncontextual features in this model. Consisting of ASVT and CSVT, SVT-Net can\nachieve state-of-the-art on benchmark datasets in terms of both accuracy and\nspeed with a super-light model size (0.9M). Meanwhile, two simplified versions\nof SVT-Net are introduced, which also achieve state-of-the-art and further\nreduce the model size to 0.8M and 0.4M respectively.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 02:23:49 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:19:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fan", "Zhaoxin", ""], ["Song", "Zhenbo", ""], ["Liu", "Hongyan", ""], ["Lu", "Zhiwu", ""], ["He", "Jun", ""], ["Du", "Xiaoyong", ""]]}, {"id": "2105.00150", "submitter": "Yuta Koreeda", "authors": "Yuta Koreeda, Christopher D. Manning", "title": "Capturing Logical Structure of Visually Structured Documents with\n  Multimodal Transition Parser", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many NLP papers, tasks and pipelines assume raw, clean texts, many\ntexts we encounter in the wild are not so clean, with many of them being\nvisually structured documents (VSDs) such as PDFs. Conventional preprocessing\ntools for VSDs mainly focused on word segmentation and coarse layout analysis,\nwhile fine-grained logical structure analysis (such as identifying paragraph\nboundaries and their hierarchies) of VSDs is underexplored. To that end, we\nproposed to formulate the task as prediction of transition labels between text\nfragments that maps the fragments to a tree, and developed a feature-based\nmachine learning system that fuses visual, textual and semantic cues. Our\nsystem significantly outperformed baselines in identifying different structures\nin VSDs. For example, our system obtained a paragraph boundary detection F1\nscore of 0.951 which is significantly better than a popular PDF-to-text tool\nwith a F1 score of 0.739.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 02:33:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Koreeda", "Yuta", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2105.00158", "submitter": "Shuiwang Li", "authors": "Shuiwang Li, Qijun Zhao, Ziliang Feng, Li Lu", "title": "Equivalence of Correlation Filter and Convolution Filter in Visual\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Discriminative) Correlation Filter has been successfully applied to visual\ntracking and has advanced the field significantly in recent years. Correlation\nfilter-based trackers consider visual tracking as a problem of matching the\nfeature template of the object and candidate regions in the detection sample,\nin which correlation filter provides the means to calculate the similarities.\nIn contrast, convolution filter is usually used for blurring, sharpening,\nembossing, edge detection, etc in image processing. On the surface, correlation\nfilter and convolution filter are usually used for different purposes. In this\npaper, however, we proves, for the first time, that correlation filter and\nconvolution filter are equivalent in the sense that their minimum mean-square\nerrors (MMSEs) in visual tracking are equal, under the condition that the\noptimal solutions exist and the ideal filter response is Gaussian and\ncentrosymmetric. This result gives researchers the freedom to choose\ncorrelation or convolution in formulating their trackers. It also suggests that\nthe explanation of the ideal response in terms of similarities is not\nessential.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 04:05:37 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 11:19:00 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Li", "Shuiwang", ""], ["Zhao", "Qijun", ""], ["Feng", "Ziliang", ""], ["Lu", "Li", ""]]}, {"id": "2105.00187", "submitter": "Shahroz Tariq", "authors": "Shahroz Tariq, Sangyup Lee and Simon S. Woo", "title": "One Detector to Rule Them All: Towards a General Deepfake Attack\n  Detection Framework", "comments": "14 pages, 8 Figures, 6 Tables, Accepted for publication in The Web\n  Conference WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449809", "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based video manipulation methods have become widely accessible\nto the masses. With little to no effort, people can quickly learn how to\ngenerate deepfake (DF) videos. While deep learning-based detection methods have\nbeen proposed to identify specific types of DFs, their performance suffers for\nother types of deepfake methods, including real-world deepfakes, on which they\nare not sufficiently trained. In other words, most of the proposed deep\nlearning-based detection methods lack transferability and generalizability.\nBeyond detecting a single type of DF from benchmark deepfake datasets, we focus\non developing a generalized approach to detect multiple types of DFs, including\ndeepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW)\nvideos. To better cope with unknown and unseen deepfakes, we introduce a\nConvolutional LSTM-based Residual Network (CLRNet), which adopts a unique model\ntraining strategy and explores spatial as well as the temporal information in\ndeepfakes. Through extensive experiments, we show that existing defense methods\nare not ready for real-world deployment. Whereas our defense method (CLRNet)\nachieves far better generalization when detecting various benchmark deepfake\nmethods (97.57% on average). Furthermore, we evaluate our approach with a\nhigh-quality DeepFake-in-the-Wild dataset, collected from the Internet\ncontaining numerous videos and having more than 150,000 frames. Our CLRNet\nmodel demonstrated that it generalizes well against high-quality DFW videos by\nachieving 93.86% detection accuracy, outperforming existing state-of-the-art\ndefense methods by a considerable margin.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:02:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tariq", "Shahroz", ""], ["Lee", "Sangyup", ""], ["Woo", "Simon S.", ""]]}, {"id": "2105.00194", "submitter": "Jong Chul Ye", "authors": "Hyungjin Chung and Jong Chul Ye", "title": "Feature Disentanglement in generating three-dimensional structure from\n  two-dimensional slice with sliceGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep generative models are known to be able to model arbitrary probability\ndistributions. Among these, a recent deep generative model, dubbed sliceGAN,\nproposed a new way of using the generative adversarial network (GAN) to capture\nthe micro-structural characteristics of a two-dimensional (2D) slice and\ngenerate three-dimensional (3D) volumes with similar properties. While 3D\nmicrographs are largely beneficial in simulating diverse material behavior,\nthey are often much harder to obtain than their 2D counterparts. Hence,\nsliceGAN opens up many interesting directions of research by learning the\nrepresentative distribution from 2D slices, and transferring the learned\nknowledge to generate arbitrary 3D volumes. However, one limitation of sliceGAN\nis that latent space steering is not possible. Hence, we combine sliceGAN with\nAdaIN to endow the model with the ability to disentangle the features and\ncontrol the synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:29:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chung", "Hyungjin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2105.00195", "submitter": "Jannik Z\\\"urn", "authors": "Jannik Z\\\"urn, Johan Vertens, Wolfram Burgard", "title": "Lane Graph Estimation for Scene Understanding in Urban Driving", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane-level scene annotations provide invaluable data in autonomous vehicles\nfor trajectory planning in complex environments such as urban areas and cities.\nHowever, obtaining such data is time-consuming and expensive since lane\nannotations have to be annotated manually by humans and are as such hard to\nscale to large areas. In this work, we propose a novel approach for lane\ngeometry estimation from bird's-eye-view images. We formulate the problem of\nlane shape and lane connections estimation as a graph estimation problem where\nlane anchor points are graph nodes and lane segments are graph edges. We train\na graph estimation model on multimodal bird's-eye-view data processed from the\npopular NuScenes dataset and its map expansion pack. We furthermore estimate\nthe direction of the lane connection for each lane segment with a separate\nmodel which results in a directed lane graph. We illustrate the performance of\nour LaneGraphNet model on the challenging NuScenes dataset and provide\nextensive qualitative and quantitative evaluation. Our model shows promising\nperformance for most evaluated urban scenes and can serve as a step towards\nautomated generation of HD lane annotations for autonomous driving.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 08:38:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Z\u00fcrn", "Jannik", ""], ["Vertens", "Johan", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2105.00203", "submitter": "Ahmed Aldahdooh", "authors": "Ahmed Aldahdooh, Wassim Hamidouche, Sid Ahmed Fezza, Olivier Deforges", "title": "Adversarial Example Detection for DNN Models: A Review", "comments": "Preprint, submitted to Artificial Intelligence Review journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) has shown great success in many human-related tasks, which\nhas led to its adoption in many computer vision based applications, such as\nsecurity surveillance system, autonomous vehicles and healthcare. Such\nsafety-critical applications have to draw its path to success deployment once\nthey have the capability to overcome safety-critical challenges. Among these\nchallenges are the defense against or/and the detection of the adversarial\nexample (AE). Adversary can carefully craft small, often imperceptible, noise\ncalled perturbations, to be added to the clean image to generate the AE. The\naim of AE is to fool the DL model which makes it a potential risk for DL\napplications. Many test-time evasion attacks and countermeasures, i.e., defense\nor detection methods, are proposed in the literature. Moreover, few reviews and\nsurveys were published and theoretically showed the taxonomy of the threats and\nthe countermeasure methods with little focus in AE detection methods. In this\npaper, we attempt to provide a theoretical and experimental review for AE\ndetection methods. A detailed discussion for such methods is provided and\nexperimental results for eight state-of-the-art detectors are presented under\ndifferent scenarios on four datasets. We also provide potential challenges and\nfuture perspectives for this research direction.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 09:55:17 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Aldahdooh", "Ahmed", ""], ["Hamidouche", "Wassim", ""], ["Fezza", "Sid Ahmed", ""], ["Deforges", "Olivier", ""]]}, {"id": "2105.00220", "submitter": "Kensuke Nakamura", "authors": "Kensuke Nakamura and Simon Korman and Byung-Woo Hong", "title": "Stabilization of generative adversarial networks via noisy scale-space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GAN) is a framework for generating fake data\nbased on given reals but is unstable in the optimization. In order to stabilize\nGANs, the noise enlarges the overlap of the real and fake distributions at the\ncost of significant variance. The data smoothing may reduce the dimensionality\nof data but suppresses the capability of GANs to learn high-frequency\ninformation. Based on these observations, we propose a data representation for\nGANs, called noisy scale-space, that recursively applies the smoothing with\nnoise to data in order to preserve the data variance while replacing\nhigh-frequency information by random data, leading to a coarse-to-fine training\nof GANs. We also present a synthetic data-set using the Hadamard bases that\nenables us to visualize the true distribution of data. We experiment with a\nDCGAN with the noise scale-space (NSS-GAN) using major data-sets in which\nNSS-GAN overtook state-of-the-arts in most cases independent of the image\ncontent.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:32:16 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 01:12:19 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Nakamura", "Kensuke", ""], ["Korman", "Simon", ""], ["Hong", "Byung-Woo", ""]]}, {"id": "2105.00234", "submitter": "Guang Yang A", "authors": "Jun Chen, Guang Yang, Habib Khan, Heye Zhang, Yanping Zhang, Shu Zhao,\n  Raad Mohiaddin, Tom Wong, David Firmin, Jennifer Keegan", "title": "JAS-GAN: Generative Adversarial Network Based Joint Atrium and Scar\n  Segmentations on Unbalanced Atrial Targets", "comments": "Accepted by IEEE Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated and accurate segmentations of left atrium (LA) and atrial scars\nfrom late gadolinium-enhanced cardiac magnetic resonance (LGE CMR) images are\nin high demand for quantifying atrial scars. The previous quantification of\natrial scars relies on a two-phase segmentation for LA and atrial scars due to\ntheir large volume difference (unbalanced atrial targets). In this paper, we\npropose an inter-cascade generative adversarial network, namely JAS-GAN, to\nsegment the unbalanced atrial targets from LGE CMR images automatically and\naccurately in an end-to-end way. Firstly, JAS-GAN investigates an adaptive\nattention cascade to automatically correlate the segmentation tasks of the\nunbalanced atrial targets. The adaptive attention cascade mainly models the\ninclusion relationship of the two unbalanced atrial targets, where the\nestimated LA acts as the attention map to adaptively focus on the small atrial\nscars roughly. Then, an adversarial regularization is applied to the\nsegmentation tasks of the unbalanced atrial targets for making a consistent\noptimization. It mainly forces the estimated joint distribution of LA and\natrial scars to match the real ones. We evaluated the performance of our\nJAS-GAN on a 3D LGE CMR dataset with 192 scans. Compared with the\nstate-of-the-art methods, our proposed approach yielded better segmentation\nperformance (Average Dice Similarity Coefficient (DSC) values of 0.946 and\n0.821 for LA and atrial scars, respectively), which indicated the effectiveness\nof our proposed approach for segmenting unbalanced atrial targets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 12:33:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Jun", ""], ["Yang", "Guang", ""], ["Khan", "Habib", ""], ["Zhang", "Heye", ""], ["Zhang", "Yanping", ""], ["Zhao", "Shu", ""], ["Mohiaddin", "Raad", ""], ["Wong", "Tom", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""]]}, {"id": "2105.00240", "submitter": "Jong Chul Ye", "authors": "Hyungjin Chung, Jaehyun Kim, Jeong Hee Yoon, Jeong Min Lee, and Jong\n  Chul Ye", "title": "Simultaneous super-resolution and motion artifact removal in\n  diffusion-weighted MRI using unsupervised deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Diffusion-weighted MRI is nowadays performed routinely due to its prognostic\nability, yet the quality of the scans are often unsatisfactory which can\nsubsequently hamper the clinical utility. To overcome the limitations, here we\npropose a fully unsupervised quality enhancement scheme, which boosts the\nresolution and removes the motion artifact simultaneously. This process is done\nby first training the network using optimal transport driven cycleGAN with\nstochastic degradation block which learns to remove aliasing artifacts and\nenhance the resolution, then using the trained network in the test stage by\nutilizing bootstrap subsampling and aggregation for motion artifact\nsuppression. We further show that we can control the trade-off between the\namount of artifact correction and resolution by controlling the bootstrap\nsubsampling ratio at the inference stage. To the best of our knowledge, the\nproposed method is the first to tackle super-resolution and motion artifact\ncorrection simultaneously in the context of MRI using unsupervised learning. We\ndemonstrate the efficiency of our method by applying it to both quantitative\nevaluation using simulation study, and to in vivo diffusion-weighted MR scans,\nwhich shows that our method is superior to the current state-of-the-art\nmethods. The proposed method is flexible in that it can be applied to various\nquality enhancement schemes in other types of MR scans, and also directly to\nthe quality enhancement of apparent diffusion coefficient maps.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:13:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chung", "Hyungjin", ""], ["Kim", "Jaehyun", ""], ["Yoon", "Jeong Hee", ""], ["Lee", "Jeong Min", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2105.00241", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh", "title": "Enhancing Fine-Grained Classification for Low Resolution Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low resolution fine-grained classification has widespread applicability for\napplications where data is captured at a distance such as surveillance and\nmobile photography. While fine-grained classification with high resolution\nimages has received significant attention, limited attention has been given to\nlow resolution images. These images suffer from the inherent challenge of\nlimited information content and the absence of fine details useful for\nsub-category classification. This results in low inter-class variations across\nsamples of visually similar classes. In order to address these challenges, this\nresearch proposes a novel attribute-assisted loss, which utilizes ancillary\ninformation to learn discriminative features for classification. The proposed\nloss function enables a model to learn class-specific discriminative features,\nwhile incorporating attribute-level separability. Evaluation is performed on\nmultiple datasets with different models, for four resolutions varying from\n32x32 to 224x224. Different experiments demonstrate the efficacy of the\nproposed attributeassisted loss for low resolution fine-grained classification.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:19:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2105.00249", "submitter": "Wei Guo", "authors": "Wei Guo, Benedetta Tondi and Mauro Barni", "title": "A Master Key Backdoor for Universal Impersonation Attack against\n  DNN-based Face Verification", "comments": null, "journal-ref": "pattern recognition letters 2021", "doi": "10.1016/j.patrec.2021.01.009", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new attack against face verification systems based on Deep\nNeural Networks (DNN). The attack relies on the introduction into the network\nof a hidden backdoor, whose activation at test time induces a verification\nerror allowing the attacker to impersonate any user. The new attack, named\nMaster Key backdoor attack, operates by interfering with the training phase, so\nto instruct the DNN to always output a positive verification answer when the\nface of the attacker is presented at its input. With respect to existing\nattacks, the new backdoor attack offers much more flexibility, since the\nattacker does not need to know the identity of the victim beforehand. In this\nway, he can deploy a Universal Impersonation attack in an open-set framework,\nallowing him to impersonate any enrolled users, even those that were not yet\nenrolled in the system when the attack was conceived. We present a practical\nimplementation of the attack targeting a Siamese-DNN face verification system,\nand show its effectiveness when the system is trained on VGGFace2 dataset and\ntested on LFW and YTF datasets. According to our experiments, the Master Key\nbackdoor attack provides a high attack success rate even when the ratio of\npoisoned training data is as small as 0.01, thus raising a new alarm regarding\nthe use of DNN-based face verification systems in security-critical\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:51:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Guo", "Wei", ""], ["Tondi", "Benedetta", ""], ["Barni", "Mauro", ""]]}, {"id": "2105.00256", "submitter": "Hossein Aboutalebi", "authors": "Hossein Aboutalebi, Maya Pavlova, Mohammad Javad Shafiee, Ali Sabri,\n  Amer Alaref, Alexander Wong", "title": "COVID-Net CXR-S: Deep Convolutional Neural Network for Severity\n  Assessment of COVID-19 Cases from Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is still struggling in controlling and containing the spread of the\nCOVID-19 pandemic caused by the SARS-CoV-2 virus. The medical conditions\nassociated with SARS-CoV-2 infections have resulted in a surge in the number of\npatients at clinics and hospitals, leading to a significantly increased strain\non healthcare resources. As such, an important part of managing and handling\npatients with SARS-CoV-2 infections within the clinical workflow is severity\nassessment, which is often conducted with the use of chest x-ray (CXR) images.\nIn this work, we introduce COVID-Net CXR-S, a convolutional neural network for\npredicting the airspace severity of a SARS-CoV-2 positive patient based on a\nCXR image of the patient's chest. More specifically, we leveraged transfer\nlearning to transfer representational knowledge gained from over 16,000 CXR\nimages from a multinational cohort of over 15,000 patient cases into a custom\nnetwork architecture for severity assessment. Experimental results with a\nmulti-national patient cohort curated by the Radiological Society of North\nAmerica (RSNA) RICORD initiative showed that the proposed COVID-Net CXR-S has\npotential to be a powerful tool for computer-aided severity assessment of CXR\nimages of COVID-19 positive patients. Furthermore, radiologist validation on\nselect cases by two board-certified radiologists with over 10 and 19 years of\nexperience, respectively, showed consistency between radiologist interpretation\nand critical factors leveraged by COVID-Net CXR-S for severity assessment.\nWhile not a production-ready solution, the ultimate goal for the open source\nrelease of COVID-Net CXR-S is to act as a catalyst for clinical scientists,\nmachine learning researchers, as well as citizen scientists to develop\ninnovative new clinical decision support solutions for helping clinicians\naround the world manage the continuing pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:15:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Aboutalebi", "Hossein", ""], ["Pavlova", "Maya", ""], ["Shafiee", "Mohammad Javad", ""], ["Sabri", "Ali", ""], ["Alaref", "Amer", ""], ["Wong", "Alexander", ""]]}, {"id": "2105.00261", "submitter": "Ruizhi Shao", "authors": "Yang Zheng, Ruizhi Shao, Yuxiang Zhang, Tao Yu, Zerong Zheng, Qionghai\n  Dai, Yebin Liu", "title": "DeepMultiCap: Performance Capture of Multiple Characters Using Sparse\n  Multiview Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepMultiCap, a novel method for multi-person performance capture\nusing sparse multi-view cameras. Our method can capture time varying surface\ndetails without the need of using pre-scanned template models. To tackle with\nthe serious occlusion challenge for close interacting scenes, we combine a\nrecently proposed pixel-aligned implicit function with parametric model for\nrobust reconstruction of the invisible surface areas. An effective\nattention-aware module is designed to obtain the fine-grained geometry details\nfrom multi-view images, where high-fidelity results can be generated. In\naddition to the spatial attention method, for video inputs, we further propose\na novel temporal fusion method to alleviate the noise and temporal\ninconsistencies for moving character reconstruction. For quantitative\nevaluation, we contribute a high quality multi-person dataset, MultiHuman,\nwhich consists of 150 static scenes with different levels of occlusions and\nground truth 3D human models. Experimental results demonstrate the\nstate-of-the-art performance of our method and the well generalization to real\nmultiview video data, which outperforms the prior works by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:32:13 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zheng", "Yang", ""], ["Shao", "Ruizhi", ""], ["Zhang", "Yuxiang", ""], ["Yu", "Tao", ""], ["Zheng", "Zerong", ""], ["Dai", "Qionghai", ""], ["Liu", "Yebin", ""]]}, {"id": "2105.00268", "submitter": "Lei Yang", "authors": "Lei Yang, Xinyu Zhang, Li Wang, Minghan Zhu, Jun Li", "title": "Lite-FPN for Keypoint-based Monocular 3D Object Detection", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection with a single image is an essential and challenging task\nfor autonomous driving. Recently, keypoint-based monocular 3D object detection\nhas made tremendous progress and achieved great speed-accuracy trade-off.\nHowever, there still exists a huge gap with LIDAR-based methods in terms of\naccuracy. To improve their performance without sacrificing efficiency, we\npropose a sort of lightweight feature pyramid network called Lite-FPN to\nachieve multi-scale feature fusion in an effective and efficient way, which can\nboost the multi-scale detection capability of keypoint-based detectors.\nBesides, the misalignment between classification score and localization\nprecision is further relieved by introducing a novel regression loss named\nattention loss. With the proposed loss, predictions with high confidence but\npoor localization are treated with more attention during the training phase.\nComparative experiments based on several state-of-the-art keypoint-based\ndetectors on the KITTI dataset show that our proposed methods manage to achieve\nsignificant improvements in both accuracy and frame rate. The code and\npretrained models will be released at\n\\url{https://github.com/yanglei18/Lite-FPN}.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:44:31 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 15:27:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yang", "Lei", ""], ["Zhang", "Xinyu", ""], ["Wang", "Li", ""], ["Zhu", "Minghan", ""], ["Li", "Jun", ""]]}, {"id": "2105.00273", "submitter": "Fabio Hern\\'an Gil Zuluaga", "authors": "Fabio Hern\\'an Gil Zuluaga, Francesco Bardozzo, Jorge Iv\\'an R\\'ios\n  Pati\\~no, Roberto Tagliaferri", "title": "Blind microscopy image denoising with a deep residual and multiscale\n  encoder/decoder network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In computer-aided diagnosis (CAD) focused on microscopy, denoising improves\nthe quality of image analysis. In general, the accuracy of this process may\ndepend both on the experience of the microscopist and on the equipment\nsensitivity and specificity. A medical image could be corrupted by both\nintrinsic noise, due to the device limitations, and, by extrinsic signal\nperturbations during image acquisition. Nowadays, CAD deep learning\napplications pre-process images with image denoising models to reinforce\nlearning and prediction. In this work, an innovative and lightweight deep\nmultiscale convolutional encoder-decoder neural network is proposed.\nSpecifically, the encoder uses deterministic mapping to map features into a\nhidden representation. Then, the latent representation is rebuilt to generate\nthe reconstructed denoised image. Residual learning strategies are used to\nimprove and accelerate the training process using skip connections in bridging\nacross convolutional and deconvolutional layers. The proposed model reaches on\naverage 38.38 of PSNR and 0.98 of SSIM on a test set of 57458 images overcoming\nstate-of-the-art models in the same application domain\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:54:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zuluaga", "Fabio Hern\u00e1n Gil", ""], ["Bardozzo", "Francesco", ""], ["Pati\u00f1o", "Jorge Iv\u00e1n R\u00edos", ""], ["Tagliaferri", "Roberto", ""]]}, {"id": "2105.00278", "submitter": "Rj Yang", "authors": "Ruijie Yang, Yunhong Wang and Yuanfang Guo", "title": "A Perceptual Distortion Reduction Framework for Adversarial Perturbation\n  Generation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the adversarial attack methods suffer from large perceptual\ndistortions such as visible artifacts, when the attack strength is relatively\nhigh. These perceptual distortions contain a certain portion which contributes\nless to the attack success rate. This portion of distortions, which is induced\nby unnecessary modifications and lack of proper perceptual distortion\nconstraint, is the target of the proposed framework. In this paper, we propose\na perceptual distortion reduction framework to tackle this problem from two\nperspectives. We guide the perturbation addition process to reduce unnecessary\nmodifications by proposing an activated region transfer attention mask, which\nintends to transfer the activated regions of the target model from the correct\nprediction to incorrect ones. Note that an ensemble model is adopted to predict\nthe activated regions of the unseen models in the black-box setting of our\nframework. Besides, we propose a perceptual distortion constraint and add it\ninto the objective function of adversarial attack to jointly optimize the\nperceptual distortions and attack success rate. Extensive experiments have\nverified the effectiveness of our framework on several baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:08:10 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yang", "Ruijie", ""], ["Wang", "Yunhong", ""], ["Guo", "Yuanfang", ""]]}, {"id": "2105.00290", "submitter": "Yunhao Ge", "authors": "Yunhao Ge, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna Karanam, Terrence\n  Chen, Laurent Itti, Ziyan Wu", "title": "A Peek Into the Reasoning of Neural Networks: Interpreting with\n  Structural Visual Concepts", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite substantial progress in applying neural networks (NN) to a wide\nvariety of areas, they still largely suffer from a lack of transparency and\ninterpretability. While recent developments in explainable artificial\nintelligence attempt to bridge this gap (e.g., by visualizing the correlation\nbetween input pixels and final outputs), these approaches are limited to\nexplaining low-level relationships, and crucially, do not provide insights on\nerror correction. In this work, we propose a framework (VRX) to interpret\nclassification NNs with intuitive structural visual concepts. Given a trained\nclassification model, the proposed VRX extracts relevant class-specific visual\nconcepts and organizes them using structural concept graphs (SCG) based on\npairwise concept relationships. By means of knowledge distillation, we show VRX\ncan take a step towards mimicking the reasoning process of NNs and provide\nlogical, concept-level explanations for final model decisions. With extensive\nexperiments, we empirically show VRX can meaningfully answer \"why\" and \"why\nnot\" questions about the prediction, providing easy-to-understand insights\nabout the reasoning process. We also show that these insights can potentially\nprovide guidance on improving NN's performance.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:47:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ge", "Yunhao", ""], ["Xiao", "Yao", ""], ["Xu", "Zhi", ""], ["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Itti", "Laurent", ""], ["Wu", "Ziyan", ""]]}, {"id": "2105.00310", "submitter": "Ali Hamdi", "authors": "Ali Hamdi, Amr Aboeleneen, Khaled Shaban", "title": "MARL: Multimodal Attentional Representation Learning for Disease\n  Prediction", "comments": "8 pages, submitted to IEEE-FUZZ'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing learning models often utilise CT-scan images to predict lung\ndiseases. These models are posed by high uncertainties that affect lung\nsegmentation and visual feature learning. We introduce MARL, a novel Multimodal\nAttentional Representation Learning model architecture that learns useful\nfeatures from multimodal data under uncertainty. We feed the proposed model\nwith both the lung CT-scan images and their perspective historical patients'\nbiological records collected over times. Such rich data offers to analyse both\nspatial and temporal aspects of the disease. MARL employs Fuzzy-based image\nspatial segmentation to overcome uncertainties in CT-scan images. We then\nutilise a pre-trained Convolutional Neural Network (CNN) to learn visual\nrepresentation vectors from images. We augment patients' data with statistical\nfeatures from the segmented images. We develop a Long Short-Term Memory (LSTM)\nnetwork to represent the augmented data and learn sequential patterns of\ndisease progressions. Finally, we inject both CNN and LSTM feature vectors to\nan attention layer to help focus on the best learning features. We evaluated\nMARL on regression of lung disease progression and status classification. MARL\noutperforms state-of-the-art CNN architectures, such as EfficientNet and\nDenseNet, and baseline prediction models. It achieves a 91% R^2 score, which is\nhigher than the other models by a range of 8% to 27%. Also, MARL achieves 97%\nand 92% accuracy for binary and multi-class classification, respectively. MARL\nimproves the accuracy of state-of-the-art CNN models with a range of 19% to\n57%. The results show that combining spatial and sequential temporal features\nproduces better discriminative feature.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 17:47:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hamdi", "Ali", ""], ["Aboeleneen", "Amr", ""], ["Shaban", "Khaled", ""]]}, {"id": "2105.00363", "submitter": "Ao Zhang", "authors": "Ao Zhang, Farzan Erlik Nowruzi, Robert Laganiere", "title": "RADDet: Range-Azimuth-Doppler based Radar Object Detection for Dynamic\n  Road Users", "comments": "Accepted by 18th Conference on Robots and Vision (CRV), CRV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection using automotive radars has not been explored with deep\nlearning models in comparison to the camera based approaches. This can be\nattributed to the lack of public radar datasets. In this paper, we collect a\nnovel radar dataset that contains radar data in the form of\nRange-Azimuth-Doppler tensors along with the bounding boxes on the tensor for\ndynamic road users, category labels, and 2D bounding boxes on the Cartesian\nBird-Eye-View range map. To build the dataset, we propose an instance-wise\nauto-annotation method. Furthermore, a novel Range-Azimuth-Doppler based\nmulti-class object detection deep learning model is proposed. The algorithm is\na one-stage anchor-based detector that generates both 3D bounding boxes and 2D\nbounding boxes on Range-Azimuth-Doppler and Cartesian domains, respectively.\nOur proposed algorithm achieves 56.3% AP with IOU of 0.3 on 3D bounding box\npredictions, and 51.6% with IOU of 0.5 on 2D bounding box prediction. Our\ndataset and the code can be found at\nhttps://github.com/ZhangAoCanada/RADDet.git.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 00:25:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Ao", ""], ["Nowruzi", "Farzan Erlik", ""], ["Laganiere", "Robert", ""]]}, {"id": "2105.00368", "submitter": "Jhacson Meza", "authors": "Jhacson Meza, Lenny A. Romero, Andres G. Marrugo", "title": "MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo\n  Pose Estimation", "comments": "Accepted at CVPR 2021 LXCV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the attention marker-less pose estimation has attracted in recent\nyears, marker-based approaches still provide unbeatable accuracy under\ncontrolled environmental conditions. Thus, they are used in many fields such as\nrobotics or biomedical applications but are primarily implemented through\nclassical approaches, which require lots of heuristics and parameter tuning for\nreliable performance under different environments. In this work, we propose\nMarkerPose, a robust, real-time pose estimation system based on a planar target\nof three circles and a stereo vision system. MarkerPose is meant for\nhigh-accuracy pose estimation applications. Our method consists of two deep\nneural networks for marker point detection. A SuperPoint-like network for\npixel-level accuracy keypoint localization and classification, and we introduce\nEllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level\naccuracy keypoint detection. The marker's pose is estimated through stereo\ntriangulation. The target point detection is robust to low lighting and motion\nblur conditions. We compared MarkerPose with a detection method based on\nclassical computer vision techniques using a robotic arm for validation. The\nresults show our method provides better accuracy than the classical technique.\nFinally, we demonstrate the suitability of MarkerPose in a 3D freehand\nultrasound system, which is an application where highly accurate pose\nestimation is required. Code is available in Python and C++ at\nhttps://github.com/jhacsonmeza/MarkerPose.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:09:13 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 22:41:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Meza", "Jhacson", ""], ["Romero", "Lenny A.", ""], ["Marrugo", "Andres G.", ""]]}, {"id": "2105.00369", "submitter": "Michael Burke Dr", "authors": "Michael Burke, Subramanian Ramamoorthy", "title": "Learning data association without data association: An EM approach to\n  neural assignment prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association is a fundamental component of effective multi-object\ntracking. Current approaches to data-association tend to frame this as an\nassignment problem relying on gating and distance-based cost matrices, or\noffset the challenge of data association to a problem of tracking by detection.\nThe latter is typically formulated as a supervised learning problem, and\nrequires labelling information about tracked object identities to train a model\nfor object recognition. This paper introduces an expectation maximisation\napproach to train neural models for data association, which does not require\nlabelling information. Here, a Sinkhorn network is trained to predict\nassignment matrices that maximise the marginal likelihood of trajectory\nobservations. Importantly, networks trained using the proposed approach can be\nre-used in downstream tracking applications.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:11:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Burke", "Michael", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "2105.00373", "submitter": "Sharad Chitlangia", "authors": "Sharad Chitlangia, Zuxin Liu, Akhil Agnihotri, Ding Zhao", "title": "Improving Perception via Sensor Placement: Designing Multi-LiDAR Systems\n  for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed an increasing interest in improving the\nperception performance of LiDARs on autonomous vehicles. While most of the\nexisting works focus on developing novel model architectures to process point\ncloud data, we study the problem from an optimal sensing perspective. To this\nend, together with a fast evaluation function based on ray tracing within the\nperception region of a LiDAR configuration, we propose an easy-to-compute\ninformation-theoretic surrogate cost metric based on Probabilistic Occupancy\nGrids (POG) to optimize LiDAR placement for maximal sensing. We show a\ncorrelation between our surrogate function and common object detection\nperformance metrics. We demonstrate the efficacy of our approach by verifying\nour results in a robust and reproducible data collection and extraction\nframework based on the CARLA simulator. Our results confirm that sensor\nplacement is an important factor in 3D point cloud-based object detection and\ncould lead to a variation of performance by 10% ~ 20% on the state-of-the-art\nperception algorithms. We believe that this is one of the first studies to use\nLiDAR placement to improve the performance of perception.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:52:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chitlangia", "Sharad", ""], ["Liu", "Zuxin", ""], ["Agnihotri", "Akhil", ""], ["Zhao", "Ding", ""]]}, {"id": "2105.00374", "submitter": "Kumar Abhishek", "authors": "Mengliu Zhao, Jeremy Kawahara, Sajjad Shamanian, Kumar Abhishek,\n  Priyanka Chandrashekar, Ghassan Hamarneh", "title": "Detection and Longitudinal Tracking of Pigmented Skin Lesions in 3D\n  Total-Body Skin Textured Meshes", "comments": "12 pages, 11 figures; Zhao and Kawahara: joint first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automated approach to detect and longitudinally track skin\nlesions on 3D total-body skin surfaces scans. The acquired 3D mesh of the\nsubject is unwrapped to a 2D texture image, where a trained region\nconvolutional neural network (R-CNN) localizes the lesions within the 2D\ndomain. These detected skin lesions are mapped back to the 3D surface of the\nsubject and, for subjects imaged multiple times, the anatomical correspondences\namong pairs of meshes and the geodesic distances among lesions are leveraged in\nour longitudinal lesion tracking algorithm.\n  We evaluated the proposed approach using three sources of data. Firstly, we\naugmented the 3D meshes of human subjects from the public FAUST dataset with a\nvariety of poses, textures, and images of lesions. Secondly, using a handheld\nstructured light 3D scanner, we imaged a mannequin with multiple synthetic skin\nlesions at selected location and with varying shapes, sizes, and colours.\nFinally, we used 3DBodyTex, a publicly available dataset composed of 3D scans\nimaging the colored (textured) skin of 200 human subjects. We manually\nannotated locations that appeared to the human eye to contain a pigmented skin\nlesion as well as tracked a subset of lesions occurring on the same subject\nimaged in different poses.\n  Our results, on test subjects annotated by three human annotators, suggest\nthat the trained R-CNN detects lesions at a similar performance level as the\nhuman annotators. Our lesion tracking algorithm achieves an average accuracy of\n80% when identifying corresponding pairs of lesions across subjects imaged in\ndifferent poses. As there currently is no other large-scale publicly available\ndataset of 3D total-body skin lesions, we publicly release the 10 mannequin\nmeshes and over 25,000 3DBodyTex manual annotations, which we hope will further\nresearch on total-body skin lesion analysis.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:52:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhao", "Mengliu", ""], ["Kawahara", "Jeremy", ""], ["Shamanian", "Sajjad", ""], ["Abhishek", "Kumar", ""], ["Chandrashekar", "Priyanka", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2105.00379", "submitter": "Ting-Yao Hu", "authors": "Ting-Yao Hu, Zhi-Qi Cheng, Alexander G. Hauptmann", "title": "Subspace Representation Learning for Few-shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a subspace representation learning (SRL) framework\nto tackle few-shot image classification tasks. It exploits a subspace in local\nCNN feature space to represent an image, and measures the similarity between\ntwo images according to a weighted subspace distance (WSD). When K images are\navailable for each class, we develop two types of template subspaces to\naggregate K-shot information: the prototypical subspace (PS) and the\ndiscriminative subspace (DS). Based on the SRL framework, we extend metric\nlearning based techniques from vector to subspace representation. While most\nprevious works adopted global vector representation, using subspace\nrepresentation can effectively preserve the spatial structure, and diversity\nwithin an image. We demonstrate the effectiveness of the SRL framework on three\npublic benchmark datasets: MiniImageNet, TieredImageNet and Caltech-UCSD\nBirds-200-2011 (CUB), and the experimental results illustrate\ncompetitive/superior performance of our method compared to the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 02:29:32 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 01:57:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hu", "Ting-Yao", ""], ["Cheng", "Zhi-Qi", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "2105.00381", "submitter": "Yunxiang Li", "authors": "Yunxiang Li, Guodong Zeng, Yifan Zhang, Jun Wang, Qianni Zhang, Qun\n  Jin, Lingling Sun, Qisi Lian, Neng Xia, Ruizi Peng, Kai Tang, Yaqi Wang,\n  Shuai Wang", "title": "Anatomy-Guided Parallel Bottleneck Transformer Network for Automated\n  Evaluation of Root Canal Therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: Accurate evaluation of the root canal filling result in X-ray\nimage is a significant step for the root canal therapy, which is based on the\nrelative position between the apical area boundary of tooth root and the top of\nfilled gutta-percha in root canal as well as the shape of the tooth root and so\non to classify the result as correct-filling, under-filling or over-filling.\nMethods: We propose a novel anatomy-guided Transformer diagnosis network. For\nobtaining accurate anatomy-guided features, a polynomial curve fitting\nsegmentation is proposed to segment the fuzzy boundary. And a Parallel\nBottleneck Transformer network (PBT-Net) is introduced as the classification\nnetwork for the final evaluation. Results, and conclusion: Our numerical\nexperiments show that our anatomy-guided PBT-Net improves the accuracy from\n40\\% to 85\\% relative to the baseline classification network. Comparing with\nthe SOTA segmentation network indicates that the ASD is significantly reduced\nby 30.3\\% through our fitting segmentation. Significance: Polynomial curve\nfitting segmentation has a great segmentation effect for extremely fuzzy\nboundaries. The prior knowledge guided classification network is suitable for\nthe evaluation of root canal therapy greatly. And the new proposed Parallel\nBottleneck Transformer for realizing self-attention is general in design,\nfacilitating a broad use in most backbone networks.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 02:38:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Yunxiang", ""], ["Zeng", "Guodong", ""], ["Zhang", "Yifan", ""], ["Wang", "Jun", ""], ["Zhang", "Qianni", ""], ["Jin", "Qun", ""], ["Sun", "Lingling", ""], ["Lian", "Qisi", ""], ["Xia", "Neng", ""], ["Peng", "Ruizi", ""], ["Tang", "Kai", ""], ["Wang", "Yaqi", ""], ["Wang", "Shuai", ""]]}, {"id": "2105.00401", "submitter": "Yingying Yan", "authors": "Haiping Hu, Yingying Yan, Qiuyu Zhu, Guohui Zheng", "title": "Generation and frame characteristics of predefined evenly-distributed\n  class centroids for pattern classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predefined evenly-distributed class centroids (PEDCC) can be widely used in\nmodels and algorithms of pattern classification, such as CNN classifiers,\nclassification autoencoders, clustering, and semi-supervised learning, etc. Its\nbasic idea is to predefine the class centers, which are evenly-distributed on\nthe unit hypersphere in feature space, to maximize the inter-class distance.\nThe previous method of generating PEDCC uses an iterative algorithm based on a\ncharge model, that is, the initial values of various centers (charge positions)\nare randomly set from the normal distribution, and the charge positions are\nupdated iteratively with the help of the repulsive force between charges of the\nsame polarity. The class centers generated by the algorithm will produce some\nerrors with the theoretically evenly-distributed points, and the generation\ntime will be longer. This paper takes advantage of regular polyhedron in\nhigh-dimensional space and the evenly distribution of points on the n\ndimensional hypersphere to generate PEDCC mathematically. Then, we discussed\nthe basic and extensive characteristics of the frames formed by PEDCC. Finally,\nexperiments show that new algorithm is not only faster than the iterative\nmethod, but also more accurate in position. The mathematical analysis and\nexperimental results of this paper can provide a theoretical tool for using\nPEDCC to solve the key problems in the field of pattern recognition, such as\ninterpretable supervised/unsupervised learning, incremental learning,\nuncertainty analysis and so on.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 06:35:51 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 02:52:43 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hu", "Haiping", ""], ["Yan", "Yingying", ""], ["Zhu", "Qiuyu", ""], ["Zheng", "Guohui", ""]]}, {"id": "2105.00402", "submitter": "Sang Dinh", "authors": "Dinh Viet Sang, Tran Quang Chung, Phan Ngoc Lan, Dao Viet Hang, Dao\n  Van Long, Nguyen Thi Thuy", "title": "AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Colorectal cancer is among the most common malignancies and can develop from\nhigh-risk colon polyps. Colonoscopy is an effective screening tool to detect\nand remove polyps, especially in the case of precancerous lesions. However, the\nmissing rate in clinical practice is relatively high due to many factors. The\nprocedure could benefit greatly from using AI models for automatic polyp\nsegmentation, which provide valuable insights for improving colon polyp\ndetection. However, precise segmentation is still challenging due to variations\nof polyps in size, shape, texture, and color. This paper proposes a novel\nneural network architecture called AG-CUResNeSt, which enhances Coupled UNets\nusing the robust ResNeSt backbone and attention gates. The network is capable\nof effectively combining multi-level features to yield accurate polyp\nsegmentation. Experimental results on five popular benchmark datasets show that\nour proposed method achieves state-of-the-art accuracy compared to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 06:36:36 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 18:57:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Sang", "Dinh Viet", ""], ["Chung", "Tran Quang", ""], ["Lan", "Phan Ngoc", ""], ["Hang", "Dao Viet", ""], ["Van Long", "Dao", ""], ["Thuy", "Nguyen Thi", ""]]}, {"id": "2105.00405", "submitter": "Wenhai Wang", "authors": "Wenhai Wang, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang,\n  Tong Lu, Chunhua Shen", "title": "PAN++: Towards Efficient and Accurate End-to-End Spotting of\n  Arbitrarily-Shaped Text", "comments": "Accepted to TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene text detection and recognition have been well explored in the past few\nyears. Despite the progress, efficient and accurate end-to-end spotting of\narbitrarily-shaped text remains challenging. In this work, we propose an\nend-to-end text spotting framework, termed PAN++, which can efficiently detect\nand recognize text of arbitrary shapes in natural scenes. PAN++ is based on the\nkernel representation that reformulates a text line as a text kernel (central\nregion) surrounded by peripheral pixels. By systematically comparing with\nexisting scene text representations, we show that our kernel representation can\nnot only describe arbitrarily-shaped text but also well distinguish adjacent\ntext. Moreover, as a pixel-based representation, the kernel representation can\nbe predicted by a single fully convolutional network, which is very friendly to\nreal-time applications. Taking the advantages of the kernel representation, we\ndesign a series of components as follows: 1) a computationally efficient\nfeature enhancement network composed of stacked Feature Pyramid Enhancement\nModules (FPEMs); 2) a lightweight detection head cooperating with Pixel\nAggregation (PA); and 3) an efficient attention-based recognition head with\nMasked RoI. Benefiting from the kernel representation and the tailored\ncomponents, our method achieves high inference speed while maintaining\ncompetitive accuracy. Extensive experiments show the superiority of our method.\nFor example, the proposed PAN++ achieves an end-to-end text spotting F-measure\nof 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms\nthe previous best method. Code will be available at: https://git.io/PAN.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 07:04:30 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 07:46:43 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 13:38:02 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Wenhai", ""], ["Xie", "Enze", ""], ["Li", "Xiang", ""], ["Liu", "Xuebo", ""], ["Liang", "Ding", ""], ["Yang", "Zhibo", ""], ["Lu", "Tong", ""], ["Shen", "Chunhua", ""]]}, {"id": "2105.00409", "submitter": "Tobi Delbruck", "authors": "Tobi Delbruck, Rui Graca, Marcin Paluch", "title": "Feedback control of event cameras", "comments": "Accepted at 2021 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW); Third International Workshop on Event-Based\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic vision sensor event cameras produce a variable data rate stream of\nbrightness change events. Event production at the pixel level is controlled by\nthreshold, bandwidth, and refractory period bias current parameter settings.\nBiases must be adjusted to match application requirements and the optimal\nsettings depend on many factors. As a first step towards automatic control of\nbiases, this paper proposes fixed-step feedback controllers that use\nmeasurements of event rate and noise. The controllers regulate the event rate\nwithin an acceptable range using threshold and refractory period control, and\nregulate noise using bandwidth control. Experiments demonstrate model validity\nand feedback control.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 07:41:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Delbruck", "Tobi", ""], ["Graca", "Rui", ""], ["Paluch", "Marcin", ""]]}, {"id": "2105.00421", "submitter": "Yeyun Zou", "authors": "Yeyun Zou, Qiyu Xie", "title": "A survey on VQA_Datasets and Approaches", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/ITCA52113.2020.00069", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual question answering (VQA) is a task that combines both the techniques\nof computer vision and natural language processing. It requires models to\nanswer a text-based question according to the information contained in a\nvisual. In recent years, the research field of VQA has been expanded. Research\nthat focuses on the VQA, examining the reasoning ability and VQA on scientific\ndiagrams, has also been explored more. Meanwhile, more multimodal feature\nfusion mechanisms have been proposed. This paper will review and analyze\nexisting datasets, metrics, and models proposed for the VQA task.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:50:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zou", "Yeyun", ""], ["Xie", "Qiyu", ""]]}, {"id": "2105.00425", "submitter": "Islem Rekik", "authors": "Megi Isallari and Islem Rekik", "title": "Brain Graph Super-Resolution Using Adversarial Graph Neural Network with\n  Application to Functional Brain Connectivity", "comments": "arXiv admin note: text overlap with arXiv:2009.11080", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain image analysis has advanced substantially in recent years with the\nproliferation of neuroimaging datasets acquired at different resolutions. While\nresearch on brain image super-resolution has undergone a rapid development in\nthe recent years, brain graph super-resolution is still poorly investigated\nbecause of the complex nature of non-Euclidean graph data. In this paper, we\npropose the first-ever deep graph super-resolution (GSR) framework that\nattempts to automatically generate high-resolution (HR) brain graphs with N'\nnodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR)\ngraphs with N nodes where N < N'. First, we formalize our GSR problem as a node\nfeature embedding learning task. Once the HR nodes' embeddings are learned, the\npairwise connectivity strength between brain ROIs can be derived through an\naggregation rule based on a novel Graph U-Net architecture. While typically the\nGraph U-Net is a node-focused architecture where graph embedding depends mainly\non node attributes, we propose a graph-focused architecture where the node\nfeature embedding is based on the graph topology. Second, inspired by graph\nspectral theory, we break the symmetry of the U-Net architecture by\nsuper-resolving the low-resolution brain graph structure and node content with\na GSR layer and two graph convolutional network layers to further learn the\nnode embeddings in the HR graph. Third, to handle the domain shift between the\nground-truth and the predicted HR brain graphs, we incorporate adversarial\nregularization to align their respective distributions. Our proposed AGSR-Net\nframework outperformed its variants for predicting high-resolution functional\nbrain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub\nat https://github.com/basiralab/AGSR-Net.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 09:09:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Isallari", "Megi", ""], ["Rekik", "Islem", ""]]}, {"id": "2105.00447", "submitter": "Masoud Jalayer", "authors": "Masoud Jalayer, Reza Jalayer, Amin Kaboli, Carlotta Orsenigo, Carlo\n  Vercellis", "title": "Automatic Visual Inspection of Rare Defects: A Framework based on\n  GP-WGAN and Enhanced Faster R-CNN", "comments": "13 pages, submitted for THE IEEE INTERNATIONAL CONFERENCE ON INDUSTRY\n  4.0, ARTIFICIAL INTELLIGENCE, AND COMMUNICATIONS TECHNOLOGY (IAICT2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current trend in industries such as semiconductors and foundry is to shift\ntheir visual inspection processes to Automatic Visual Inspection (AVI) systems,\nto reduce their costs, mistakes, and dependency on human experts. This paper\nproposes a two-staged fault diagnosis framework for AVI systems. In the first\nstage, a generation model is designed to synthesize new samples based on real\nsamples. The proposed augmentation algorithm extracts objects from the real\nsamples and blends them randomly, to generate new samples and enhance the\nperformance of the image processor. In the second stage, an improved deep\nlearning architecture based on Faster R-CNN, Feature Pyramid Network (FPN), and\na Residual Network is proposed to perform object detection on the enhanced\ndataset. The performance of the algorithm is validated and evaluated on two\nmulti-class datasets. The experimental results performed over a range of\nimbalance severities demonstrate the superiority of the proposed framework\ncompared to other solutions.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 11:34:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jalayer", "Masoud", ""], ["Jalayer", "Reza", ""], ["Kaboli", "Amin", ""], ["Orsenigo", "Carlotta", ""], ["Vercellis", "Carlo", ""]]}, {"id": "2105.00460", "submitter": "Dandan Zhang", "authors": "Dandan Zhang, Ruoxi Wang, Benny Lo", "title": "Surgical Gesture Recognition Based on Bidirectional Multi-Layer\n  Independently RNN with Explainable Spatial Feature Extraction", "comments": "11 pages,6 figures, Accepted by ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimally invasive surgery mainly consists of a series of sub-tasks, which\ncan be decomposed into basic gestures or contexts. As a prerequisite of\nautonomic operation, surgical gesture recognition can assist motion planning\nand decision-making, and build up context-aware knowledge to improve the\nsurgical robot control quality. In this work, we aim to develop an effective\nsurgical gesture recognition approach with an explainable feature extraction\nprocess. A Bidirectional Multi-Layer independently RNN (BML-indRNN) model is\nproposed in this paper, while spatial feature extraction is implemented via\nfine-tuning of a Deep Convolutional Neural Network(DCNN) model constructed\nbased on the VGG architecture. To eliminate the black-box effects of DCNN,\nGradient-weighted Class Activation Mapping (Grad-CAM) is employed. It can\nprovide explainable results by showing the regions of the surgical images that\nhave a strong relationship with the surgical gesture classification results.\nThe proposed method was evaluated based on the suturing task with data obtained\nfrom the public available JIGSAWS database. Comparative studies were conducted\nto verify the proposed framework. Results indicated that the testing accuracy\nfor the suturing task based on our proposed method is 87.13%, which outperforms\nmost of the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 12:47:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Dandan", ""], ["Wang", "Ruoxi", ""], ["Lo", "Benny", ""]]}, {"id": "2105.00463", "submitter": "Byungjai Kim", "authors": "Byungjai Kim, Kinam Kwon, Changheun Oh, and Hyunwook Park", "title": "Unsupervised Anomaly Detection in MR Images using Multi-Contrast\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection in medical imaging is to distinguish the relevant\nbiomarkers of diseases from those of normal tissues. Deep supervised learning\nmethods have shown potentials in various detection tasks, but its performances\nwould be limited in medical imaging fields where collecting annotated anomaly\ndata is limited and labor-intensive. Therefore, unsupervised anomaly detection\ncan be an effective tool for clinical practices, which uses only unlabeled\nnormal images as training data. In this paper, we developed an unsupervised\nlearning framework for pixel-wise anomaly detection in multi-contrast magnetic\nresonance imaging (MRI). The framework has two steps of feature generation and\ndensity estimation with Gaussian mixture model (GMM). A feature is derived\nthrough the learning of contrast-to-contrast translation that effectively\ncaptures the normal tissue characteristics in multi-contrast MRI. The feature\nis collaboratively used with another feature that is the low-dimensional\nrepresentation of multi-contrast images. In density estimation using GMM, a\nsimple but efficient way is introduced to handle the singularity problem which\ninterrupts the joint learning process. The proposed method outperforms previous\nanomaly detection approaches. Quantitative and qualitative analyses demonstrate\nthe effectiveness of the proposed method in anomaly detection for\nmulti-contrast MRI.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:05:36 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 00:21:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kim", "Byungjai", ""], ["Kwon", "Kinam", ""], ["Oh", "Changheun", ""], ["Park", "Hyunwook", ""]]}, {"id": "2105.00470", "submitter": "Wenxiao Wang", "authors": "Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, Hang Zhao", "title": "On Feature Decorrelation in Self-Supervised Learning", "comments": "The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In self-supervised representation learning, a common idea behind most of the\nstate-of-the-art approaches is to enforce the robustness of the representations\nto predefined augmentations. A potential issue of this idea is the existence of\ncompletely collapsed solutions (i.e., constant features), which are typically\navoided implicitly by carefully chosen implementation details. In this work, we\nstudy a relatively concise framework containing the most common components from\nrecent approaches. We verify the existence of complete collapse and discover\nanother reachable collapse pattern that is usually overlooked, namely\ndimensional collapse. We connect dimensional collapse with strong correlations\nbetween axes and consider such connection as a strong motivation for feature\ndecorrelation (i.e., standardizing the covariance matrix). The capability of\ncorrelation as an unsupervised metric and the gains from feature decorrelation\nare verified empirically to highlight the importance and the potential of this\ninsight.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:28:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hua", "Tianyu", ""], ["Wang", "Wenxiao", ""], ["Xue", "Zihui", ""], ["Wang", "Yue", ""], ["Ren", "Sucheng", ""], ["Zhao", "Hang", ""]]}, {"id": "2105.00480", "submitter": "Jinjian Li", "authors": "Jinjian Li, Chuandong Guo, Li Su, Xiangyu Wang, Quan Hu", "title": "SE-Harris and eSUSAN: Asynchronous Event-Based Corner Detection Using\n  Megapixel Resolution CeleX-V Camera", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel neuromorphic vision sensors with ultrahigh temporal\nresolution and low latency, both in the order of microseconds. Instead of image\nframes, event cameras generate an asynchronous event stream of per-pixel\nintensity changes with precise timestamps. The resulting sparse data structure\nimpedes applying many conventional computer vision techniques to event streams,\nand specific algorithms should be designed to leverage the information provided\nby event cameras. We propose a corner detection algorithm, eSUSAN, inspired by\nthe conventional SUSAN (smallest univalue segment assimilating nucleus)\nalgorithm for corner detection. The proposed eSUSAN extracts the univalue\nsegment assimilating nucleus from the circle kernel based on the similarity\nacross timestamps and distinguishes corner events by the number of pixels in\nthe nucleus area. Moreover, eSUSAN is fast enough to be applied to CeleX-V, the\nevent camera with the highest resolution available. Based on eSUSAN, we also\npropose the SE-Harris corner detector, which uses adaptive normalization based\non exponential decay to quickly construct a local surface of active events and\nthe event-based Harris detector to refine the corners identified by eSUSAN. We\nevaluated the proposed algorithms on a public dataset and CeleX-V data. Both\neSUSAN and SE-Harris exhibit higher real-time performance than existing\nalgorithms while maintaining high accuracy and tracking performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:06:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Jinjian", ""], ["Guo", "Chuandong", ""], ["Su", "Li", ""], ["Wang", "Xiangyu", ""], ["Hu", "Quan", ""]]}, {"id": "2105.00490", "submitter": "Jing Huang", "authors": "Jing Huang, Xiaolin Huang and Jie Yang", "title": "Residual Enhanced Multi-Hypergraph Neural Network", "comments": "ICIP 2021 submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hypergraphs are a generalized data structure of graphs to model higher-order\ncorrelations among entities, which have been successfully adopted into various\nresearch domains. Meanwhile, HyperGraph Neural Network (HGNN) is currently the\nde-facto method for hypergraph representation learning. However, HGNN aims at\nsingle hypergraph learning and uses a pre-concatenation approach when\nconfronting multi-modal datasets, which leads to sub-optimal exploitation of\nthe inter-correlations of multi-modal hypergraphs. HGNN also suffers the\nover-smoothing issue, that is, its performance drops significantly when layers\nare stacked up. To resolve these issues, we propose the Residual enhanced\nMulti-Hypergraph Neural Network, which can not only fuse multi-modal\ninformation from each hypergraph effectively, but also circumvent the\nover-smoothing issue associated with HGNN. We conduct experiments on two 3D\nbenchmarks, the NTU and the ModelNet40 datasets, and compare against multiple\nstate-of-the-art methods. Experimental results demonstrate that both the\nresidual hypergraph convolutions and the multi-fusion architecture can improve\nthe performance of the base model and the combined model achieves a new\nstate-of-the-art. Code is available at\n\\url{https://github.com/OneForward/ResMHGNN}.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:53:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Huang", "Jing", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""]]}, {"id": "2105.00526", "submitter": "Joonas L\\~omps", "authors": "Joonas L\\~omps, Artjom Lind, Amnir Hadachi", "title": "CDR Based Trajectories: Tentative for Filtering Ping-pong Handover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Call Detail Records (CDRs) coupled with the coverage area locations provide\nthe operator with an incredible amount of information on its customers'\nwhereabouts and movement. Due to the non-static and overlapping nature of the\nantenna coverage area there commonly exist situations where cellphones\ngeographically close to each other can be connected to different antennas due\nto handover rule - the operator hands over a certain cellphone to another\nantenna to spread the load between antennas. Hence, this aspect introduces a\nping-pong handover phenomena in the trajectories extracted from the CDR data\nwhich can be misleading in understanding the mobility pattern. To reconstruct\naccurate trajectories it is a must to reduce the number of those handovers\nappearing in the dataset. This letter presents a novel approach for filtering\nping-pong handovers from CDR based trajectories. Primarily, the approach is\nbased on anchors model utilizing different features and parameters extracted\nfrom the coverage areas and reconstructed trajectories mined from the CDR data.\nUsing this methodology we can significantly reduce the ping-pong handover noise\nin the trajectories, which gives a more accurate reconstruction of the\ncustomers' movement pattern.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 18:25:53 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 19:53:47 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["L\u00f5mps", "Joonas", ""], ["Lind", "Artjom", ""], ["Hadachi", "Amnir", ""]]}, {"id": "2105.00529", "submitter": "Jingjing Deng", "authors": "Hanchi Ren, Jingjing Deng and Xianghua Xie", "title": "GRNN: Generative Regression Neural Network -- A Data Leakage Attack for\n  Federated Learning", "comments": "Submitted to ACM Transactions on Intelligent Systems and Technology.\n  For associated source file, see https://github.com/Rand2AI/GRNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data privacy has become an increasingly important issue in machine learning.\nMany approaches have been developed to tackle this issue, e.g., cryptography\n(Homomorphic Encryption, Differential Privacy, etc.) and collaborative training\n(Secure Multi-Party Computation, Distributed Learning and Federated Learning).\nThese techniques have a particular focus on data encryption or secure local\ncomputation. They transfer the intermediate information to the third-party to\ncompute the final result. Gradient exchanging is commonly considered to be a\nsecure way of training a robust model collaboratively in deep learning.\nHowever, recent researches have demonstrated that sensitive information can be\nrecovered from the shared gradient. Generative Adversarial Networks (GAN), in\nparticular, have shown to be effective in recovering those information.\nHowever, GAN based techniques require additional information, such as class\nlabels which are generally unavailable for privacy persevered learning. In this\npaper, we show that, in Federated Learning (FL) system, image-based privacy\ndata can be easily recovered in full from the shared gradient only via our\nproposed Generative Regression Neural Network (GRNN). We formulate the attack\nto be a regression problem and optimise two branches of the generative model by\nminimising the distance between gradients. We evaluate our method on several\nimage classification tasks. The results illustrate that our proposed GRNN\noutperforms state-of-the-art methods with better stability, stronger\nrobustness, and higher accuracy. It also has no convergence requirement to the\nglobal FL model. Moreover, we demonstrate information leakage using face\nre-identification. Some defense strategies are also discussed in this work.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 18:39:37 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ren", "Hanchi", ""], ["Deng", "Jingjing", ""], ["Xie", "Xianghua", ""]]}, {"id": "2105.00558", "submitter": "Loc Trinh", "authors": "Loc Trinh, Yan Liu", "title": "An Examination of Fairness of AI Models for Deepfake Detection", "comments": "To appear in the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated that deep learning models can discriminate\nbased on protected classes like race and gender. In this work, we evaluate bias\npresent in deepfake datasets and detection models across protected subgroups.\nUsing facial datasets balanced by race and gender, we examine three popular\ndeepfake detectors and find large disparities in predictive performances across\nraces, with up to 10.7% difference in error rate between subgroups. A closer\nlook reveals that the widely used FaceForensics++ dataset is overwhelmingly\ncomposed of Caucasian subjects, with the majority being female Caucasians. Our\ninvestigation of the racial distribution of deepfakes reveals that the methods\nused to create deepfakes as positive training signals tend to produce\n\"irregular\" faces - when a person's face is swapped onto another person of a\ndifferent race or gender. This causes detectors to learn spurious correlations\nbetween the foreground faces and fakeness. Moreover, when detectors are trained\nwith the Blended Image (BI) dataset from Face X-Rays, we find that those\ndetectors develop systematic discrimination towards certain racial subgroups,\nprimarily female Asians.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 21:55:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Trinh", "Loc", ""], ["Liu", "Yan", ""]]}, {"id": "2105.00580", "submitter": "Siddharth Karamcheti", "authors": "Siddharth Karamcheti, Albert J. Zhai, Dylan P. Losey, Dorsa Sadigh", "title": "Learning Visually Guided Latent Actions for Assistive Teleoperation", "comments": "Accepted at Learning for Dynamics and Control (L4DC) 2021. 12 pages,\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is challenging for humans -- particularly those living with physical\ndisabilities -- to control high-dimensional, dexterous robots. Prior work\nexplores learning embedding functions that map a human's low-dimensional inputs\n(e.g., via a joystick) to complex, high-dimensional robot actions for assistive\nteleoperation; however, a central problem is that there are many more\nhigh-dimensional actions than available low-dimensional inputs. To extract the\ncorrect action and maximally assist their human controller, robots must reason\nover their context: for example, pressing a joystick down when interacting with\na coffee cup indicates a different action than when interacting with knife. In\nthis work, we develop assistive robots that condition their latent embeddings\non visual inputs. We explore a spectrum of visual encoders and show that\nincorporating object detectors pretrained on small amounts of cheap,\neasy-to-collect structured data enables i) accurately and robustly recognizing\nthe current context and ii) generalizing control embeddings to new objects and\ntasks. In user studies with a high-dimensional physical robot arm, participants\nleverage this approach to perform new tasks with unseen objects. Our results\nindicate that structured visual representations improve few-shot performance\nand are subjectively preferred by users.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:58:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Karamcheti", "Siddharth", ""], ["Zhai", "Albert J.", ""], ["Losey", "Dylan P.", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2105.00582", "submitter": "Emily Lin", "authors": "Emily Lin, Weicheng Kuo, Esther Yuh", "title": "Noisy Student learning for cross-institution brain hemorrhage detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed tomography (CT) is the imaging modality used in the diagnosis of\nneurological emergencies, including acute stroke and traumatic brain injury.\nAdvances in deep learning have led to models that can detect and segment\nhemorrhage on head CT. PatchFCN, one such supervised fully convolutional\nnetwork (FCN), recently demonstrated expert-level detection of intracranial\nhemorrhage on in-sample data. However, its potential for similar accuracy\noutside the training domain is hindered by its need for pixel-labeled data from\noutside institutions. Also recently, a semi-supervised technique, Noisy Student\n(NS) learning, demonstrated state-of-the-art performance on ImageNet by moving\nfrom a fully-supervised to a semi-supervised learning paradigm. We combine the\nPatchFCN and Noisy Student approaches, extending semi-supervised learning to an\nintracranial hemorrhage segmentation task. Surprisingly, the NS model\nperformance surpasses that of a fully-supervised oracle model trained with\nimage-level labels on the same data. It also performs comparably to another\nrecently reported supervised model trained on a labeled dataset 600x larger\nthan that used to train the NS model. To our knowledge, we are the first to\ndemonstrate the effectiveness of semi-supervised learning on a head CT\ndetection and segmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 00:14:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lin", "Emily", ""], ["Kuo", "Weicheng", ""], ["Yuh", "Esther", ""]]}, {"id": "2105.00591", "submitter": "Juliano S. Assine", "authors": "Juliano S. Assine, J. C. S. Santos Filho, Eduardo Valle", "title": "Single-Training Collaborative Object Detectors Adaptive to Bandwidth and\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, mobile deep-learning deployment progressed by leaps\nand bounds, but solutions still struggle to accommodate its severe and\nfluctuating operational restrictions, which include bandwidth, latency,\ncomputation, and energy. In this work, we help to bridge that gap, introducing\nthe first configurable solution for object detection that manages the triple\ncommunication-computation-accuracy trade-off with a single set of weights. Our\nsolution shows state-of-the-art results on COCO-2017, adding only a minor\npenalty on the base EfficientDet-D2 architecture. Our design is robust to the\nchoice of base architecture and compressor and should adapt well for future\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 01:08:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Assine", "Juliano S.", ""], ["Filho", "J. C. S. Santos", ""], ["Valle", "Eduardo", ""]]}, {"id": "2105.00622", "submitter": "Camilo Pestana", "authors": "Camilo Pestana, Wei Liu, David Glance, Robyn Owens, Ajmal Mian", "title": "Physical world assistive signals for deep neural network classifiers --\n  neither defense nor attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks lead the state of the art of computer vision tasks.\nDespite this, Neural Networks are brittle in that small changes in the input\ncan drastically affect their prediction outcome and confidence. Consequently\nand naturally, research in this area mainly focus on adversarial attacks and\ndefenses. In this paper, we take an alternative stance and introduce the\nconcept of Assistive Signals, which are optimized to improve a model's\nconfidence score regardless if it's under attack or not. We analyse some\ninteresting properties of these assistive perturbations and extend the idea to\noptimize assistive signals in the 3D space for real-life scenarios simulating\ndifferent lighting conditions and viewing angles. Experimental evaluations show\nthat the assistive signals generated by our optimization method increase the\naccuracy and confidence of deep models more than those generated by\nconventional methods that work in the 2D space. In addition, our Assistive\nSignals illustrate the intrinsic bias of ML models towards certain patterns in\nreal-life objects. We discuss how we can exploit these insights to re-think, or\navoid, some patterns that might contribute to, or degrade, the detectability of\nobjects in the real-world.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 04:02:48 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Pestana", "Camilo", ""], ["Liu", "Wei", ""], ["Glance", "David", ""], ["Owens", "Robyn", ""], ["Mian", "Ajmal", ""]]}, {"id": "2105.00623", "submitter": "Yixu Wang", "authors": "Yixu Wang, Jie Li, Hong Liu, Yan Wang, Yongjian Wu, Feiyue Huang,\n  Rongrong Ji", "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have verified that the functionality of black-box models can\nbe stolen with full probability outputs. However, under the more practical\nhard-label setting, we observe that existing methods suffer from catastrophic\nperformance degradation. We argue this is due to the lack of rich information\nin the probability prediction and the overfitting caused by hard labels. To\nthis end, we propose a novel hard-label model stealing method termed\n\\emph{black-box dissector}, which consists of two erasing-based modules. One is\na CAM-driven erasing strategy that is designed to increase the information\ncapacity hidden in hard labels from the victim model. The other is a\nrandom-erasing-based self-knowledge distillation module that utilizes soft\nlabels from the substitute model to mitigate overfitting. Extensive experiments\non four widely-used datasets consistently demonstrate that our method\noutperforms state-of-the-art methods, with an improvement of at most $8.27\\%$.\nWe also validate the effectiveness and practical potential of our method on\nreal-world APIs and defense methods. Furthermore, our method promotes other\ndownstream tasks, \\emph{i.e.}, transfer adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 04:12:31 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 04:05:53 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Yixu", ""], ["Li", "Jie", ""], ["Liu", "Hong", ""], ["Wang", "Yan", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "2105.00634", "submitter": "Weijun Tan", "authors": "Rushuai Liu and Weijun Tan", "title": "EQFace: A Simple Explicit Quality Network for Face Recognition", "comments": "Accepted to CVPR 2021 AMFG Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the deep learning makes big progresses in still-image face recognition,\nunconstrained video face recognition is still a challenging task due to low\nquality face images caused by pose, blur, occlusion, illumination etc. In this\npaper we propose a network for face recognition which gives an explicit and\nquantitative quality score at the same time when a feature vector is extracted.\nTo our knowledge this is the first network that implements these two functions\nin one network online. This network is very simple by adding a quality network\nbranch to the baseline network of face recognition. It does not require\ntraining datasets with annotated face quality labels. We evaluate this network\non both still-image face datasets and video face datasets and achieve the\nstate-of-the-art performance in many cases. This network enables a lot of\napplications where an explicit face quality scpre is used. We demonstrate three\napplications of the explicit face quality, one of which is a progressive\nfeature aggregation scheme in online video face recognition. We design an\nexperiment to prove the benefits of using the face quality in this application.\nCode will be available at \\url{https://github.com/deepcam-cn/facequality}.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 05:45:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Rushuai", ""], ["Tan", "Weijun", ""]]}, {"id": "2105.00636", "submitter": "Dian Chen", "authors": "Dian Chen, Vladlen Koltun, Philipp Kr\\\"ahenb\\\"uhl", "title": "Learning to drive from a world on rails", "comments": "Code and data available at: https://dotchen.github.io/world_on_rails/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn an interactive vision-based driving policy from pre-recorded driving\nlogs via a model-based approach. A forward model of the world supervises a\ndriving policy that predicts the outcome of any potential driving trajectory.\nTo support learning from pre-recorded logs, we assume that the world is on\nrails, meaning neither the agent nor its actions influence the environment.\nThis assumption greatly simplifies the learning problem, factorizing the\ndynamics into a nonreactive world model and a low-dimensional and compact\nforward model of the ego-vehicle. Our approach computes action-values for each\ntraining trajectory using a tabular dynamic-programming evaluation of the\nBellman equations; these action-values in turn supervise the final vision-based\ndriving policy. Despite the world-on-rails assumption, the final driving policy\nacts well in a dynamic and reactive world. Our method ranks first on the CARLA\nleaderboard, attaining a 25% higher driving score while using 40 times less\ndata. Our method is also an order of magnitude more sample-efficient than\nstate-of-the-art model-free reinforcement learning techniques on navigational\ntasks in the ProcGen benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 05:55:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Dian", ""], ["Koltun", "Vladlen", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2105.00637", "submitter": "Jie Hu", "authors": "Jie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue\n  Huang, Ling Shao, Rongrong Ji", "title": "ISTR: End-to-End Instance Segmentation with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end paradigms significantly improve the accuracy of various\ndeep-learning-based computer vision models. To this end, tasks like object\ndetection have been upgraded by replacing non-end-to-end components, such as\nremoving non-maximum suppression by training with a set loss based on bipartite\nmatching. However, such an upgrade is not applicable to instance segmentation,\ndue to its significantly higher output dimensions compared to object detection.\nIn this paper, we propose an instance segmentation Transformer, termed ISTR,\nwhich is the first end-to-end framework of its kind. ISTR predicts\nlow-dimensional mask embeddings, and matches them with ground truth mask\nembeddings for the set loss. Besides, ISTR concurrently conducts detection and\nsegmentation with a recurrent refinement strategy, which provides a new way to\nachieve instance segmentation compared to the existing top-down and bottom-up\nframeworks. Benefiting from the proposed end-to-end mechanism, ISTR\ndemonstrates state-of-the-art performance even with approximation-based\nsuboptimal embeddings. Specifically, ISTR obtains a 46.8/38.6 box/mask AP using\nResNet50-FPN, and a 48.1/39.9 box/mask AP using ResNet101-FPN, on the MS COCO\ndataset. Quantitative and qualitative results reveal the promising potential of\nISTR as a solid baseline for instance-level recognition. Code has been made\navailable at: https://github.com/hujiecpp/ISTR.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 06:00:09 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 03:10:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hu", "Jie", ""], ["Cao", "Liujuan", ""], ["Lu", "Yao", ""], ["Zhang", "ShengChuan", ""], ["Wang", "Yan", ""], ["Li", "Ke", ""], ["Huang", "Feiyue", ""], ["Shao", "Ling", ""], ["Ji", "Rongrong", ""]]}, {"id": "2105.00681", "submitter": "Wei-Ting Chen", "authors": "Hao-Hsiang Yang and Wei-Ting Chen and and Sy-Yen Kuo", "title": "S3Net: A Single Stream Structure for Depth Guided Image Relighting", "comments": "Accepted by CVPRW 2021. This solution obtains the 3 rd position in\n  the NTIRE 2021 Depth Guided Any-to-any Relighting Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth guided any-to-any image relighting aims to generate a relit image from\nthe original image and corresponding depth maps to match the illumination\nsetting of the given guided image and its depth map. To the best of our\nknowledge, this task is a new challenge that has not been addressed in the\nprevious literature. To address this issue, we propose a deep learning-based\nneural Single Stream Structure network called S3Net for depth guided image\nrelighting. This network is an encoder-decoder model. We concatenate all images\nand corresponding depth maps as the input and feed them into the model. The\ndecoder part contains the attention module and the enhanced module to focus on\nthe relighting-related regions in the guided images. Experiments performed on\nchallenging benchmark show that the proposed model achieves the 3 rd highest\nSSIM in the NTIRE 2021 Depth Guided Any-to-any Relighting Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:33:53 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 02:09:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Hao-Hsiang", ""], ["Chen", "Wei-Ting", ""], ["Kuo", "and Sy-Yen", ""]]}, {"id": "2105.00690", "submitter": "Wei-Ting Chen", "authors": "Hao-Hsiang Yang and Wei-Ting Chen and Hao-Lun Luo and Sy-Yen Kuo", "title": "Multi-modal Bifurcated Network for Depth Guided Image Relighting", "comments": "Accepted by CVPRW 2021. This solution is the winner in the NTIRE 2021\n  Depth Guide One-to-one Relighting Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image relighting aims to recalibrate the illumination setting in an image. In\nthis paper, we propose a deep learning-based method called multi-modal\nbifurcated network (MBNet) for depth guided image relighting. That is, given an\nimage and the corresponding depth maps, a new image with the given illuminant\nangle and color temperature is generated by our network. This model extracts\nthe image and the depth features by the bifurcated network in the encoder. To\nuse the two features effectively, we adopt the dynamic dilated pyramid modules\nin the decoder. Moreover, to increase the variety of training data, we propose\na novel data process pipeline to increase the number of the training data.\nExperiments conducted on the VIDIT dataset show that the proposed solution\nobtains the \\textbf{1}$^{st}$ place in terms of SSIM and PMS in the NTIRE 2021\nDepth Guide One-to-one Relighting Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:52:25 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 02:13:15 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Hao-Hsiang", ""], ["Chen", "Wei-Ting", ""], ["Luo", "Hao-Lun", ""], ["Kuo", "Sy-Yen", ""]]}, {"id": "2105.00708", "submitter": "Yan-Bo Lin", "authors": "Yan-Bo Lin and Yu-Chiang Frank Wang", "title": "Exploiting Audio-Visual Consistency with Partial Supervision for Spatial\n  Audio Generation", "comments": "AAAI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human perceives rich auditory experience with distinct sound heard by ears.\nVideos recorded with binaural audio particular simulate how human receives\nambient sound. However, a large number of videos are with monaural audio only,\nwhich would degrade the user experience due to the lack of ambient information.\nTo address this issue, we propose an audio spatialization framework to convert\na monaural video into a binaural one exploiting the relationship across audio\nand visual components. By preserving the left-right consistency in both audio\nand visual modalities, our learning strategy can be viewed as a self-supervised\nlearning technique, and alleviates the dependency on a large amount of video\ndata with ground truth binaural audio data during training. Experiments on\nbenchmark datasets confirm the effectiveness of our proposed framework in both\nsemi-supervised and fully supervised scenarios, with ablation studies and\nvisualization further support the use of our model for audio spatialization.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:34:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lin", "Yan-Bo", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2105.00717", "submitter": "Alon Shoshan", "authors": "Matan Fintz, Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard\n  Medioni", "title": "Synthetic Data for Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent improvements in synthetic data generation make it possible to produce\nimages that are highly photorealistic and indistinguishable from real ones.\nFurthermore, synthetic generation pipelines have the potential to generate an\nunlimited number of images. The combination of high photorealism and scale turn\nthe synthetic data into a promising candidate for potentially improving various\nmachine learning (ML) pipelines. Thus far, a large body of research in this\nfield has focused on using synthetic images for training, by augmenting and\nenlarging training data. In contrast to using synthetic data for training, in\nthis work we explore whether synthetic data can be beneficial for model\nselection. Considering the task of image classification, we demonstrate that\nwhen data is scarce, synthetic data can be used to replace the held out\nvalidation set, thus allowing to train on a larger dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:52:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fintz", "Matan", ""], ["Shoshan", "Alon", ""], ["Bhonker", "Nadav", ""], ["Kviatkovsky", "Igor", ""], ["Medioni", "Gerard", ""]]}, {"id": "2105.00728", "submitter": "Yiming Liu", "authors": "Yiming Liu, Ying Chen, Guangming Pan, Weichung Wang, Wei-Chih Liao,\n  Yee Liang Thian, Cheng E. Chee and Constantinos P. Anastassiades", "title": "Spectral Machine Learning for Pancreatic Mass Imaging Classification", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel spectral machine learning (SML) method in screening for\npancreatic mass using CT imaging. Our algorithm is trained with approximately\n30,000 images from 250 patients (50 patients with normal pancreas and 200\npatients with abnormal pancreas findings) based on public data sources. A test\naccuracy of 94.6 percents was achieved in the out-of-sample diagnosis\nclassification based on a total of approximately 15,000 images from 113\npatients, whereby 26 out of 32 patients with normal pancreas and all 81\npatients with abnormal pancreas findings were correctly diagnosed. SML is able\nto automatically choose fundamental images (on average 5 or 9 images for each\npatient) in the diagnosis classification and achieve the above mentioned\naccuracy. The computational time is 75 seconds for diagnosing 113 patients in a\nlaptop with standard CPU running environment. Factors that influenced high\nperformance of a well-designed integration of spectral learning and machine\nlearning included: 1) use of eigenvectors corresponding to several of the\nlargest eigenvalues of sample covariance matrix (spike eigenvectors) to choose\ninput attributes in classification training, taking into account only the\nfundamental information of the raw images with less noise; 2) removal of\nirrelevant pixels based on mean-level spectral test to lower the challenges of\nmemory capacity and enhance computational efficiency while maintaining superior\nclassification accuracy; 3) adoption of state-of-the-art machine learning\nclassification, gradient boosting and random forest. Our methodology showcases\npractical utility and improved accuracy of image diagnosis in pancreatic mass\nscreening in the era of AI.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 10:17:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Yiming", ""], ["Chen", "Ying", ""], ["Pan", "Guangming", ""], ["Wang", "Weichung", ""], ["Liao", "Wei-Chih", ""], ["Thian", "Yee Liang", ""], ["Chee", "Cheng E.", ""], ["Anastassiades", "Constantinos P.", ""]]}, {"id": "2105.00777", "submitter": "Hengyi Li", "authors": "Yoshiyuki Fujikawa, Hengyi Li, Xuebin Yue, Aravinda C V, Amar Prabhu\n  G, Lin Meng", "title": "Recognition of Oracle Bone Inscriptions by using Two Deep Learning\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oracle bone inscriptions (OBIs) contain some of the oldest characters in the\nworld and were used in China about 3000 years ago. As an ancient form of\nliterature, OBIs store a lot of information that can help us understand the\nworld history, character evaluations, and more. However, as OBIs were found\nonly discovered about 120 years ago, few studies have described them, and the\naging process has made the inscriptions less legible. Hence, automatic\ncharacter detection and recognition has become an important issue. This paper\naims to design a online OBI recognition system for helping preservation and\norganization the cultural heritage. We evaluated two deep learning models for\nOBI recognition, and have designed an API that can be accessed online for OBI\nrecognition. In the first stage, you only look once (YOLO) is applied for\ndetecting and recognizing OBIs. However, not all of the OBIs can be detected\ncorrectly by YOLO, so we next utilize MobileNet to recognize the undetected\nOBIs by manually cropping the undetected OBI in the image. MobileNet is used\nfor this second stage of recognition as our evaluation of ten state-of-the-art\nmodels showed that it is the best network for OBI recognition due to its\nsuperior performance in terms of accuracy, loss and time consumption. We\ninstalled our system on an application programming interface (API) and opened\nit for OBI detection and recognition.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:31:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 05:23:14 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Fujikawa", "Yoshiyuki", ""], ["Li", "Hengyi", ""], ["Yue", "Xuebin", ""], ["C", "Aravinda", "V"], ["G", "Amar Prabhu", ""], ["Meng", "Lin", ""]]}, {"id": "2105.00781", "submitter": "Jakub Nemcek", "authors": "Jakub Nemcek, Tomas Vicar, Roman Jakubicek", "title": "Weakly supervised deep learning-based intracranial hemorrhage\n  localization", "comments": "4 pages, 2 figures, Submitted to EMBC 2021 - paper has not been\n  reviewed yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial hemorrhage is a life-threatening disease, which requires fast\nmedical intervention. Owing to the duration of data annotation, head CT images\nare usually available only with slice-level labeling. This paper presents a\nweakly supervised method of precise hemorrhage localization in axial slices\nusing only position-free labels, which is based on multiple instance learning.\nAn algorithm is introduced that generates hemorrhage likelihood maps and finds\nthe coordinates of bleeding. The Dice coefficient of 58.08 % is achieved on\ndata from a publicly available dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:37:23 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nemcek", "Jakub", ""], ["Vicar", "Tomas", ""], ["Jakubicek", "Roman", ""]]}, {"id": "2105.00782", "submitter": "Lorenzo Nava", "authors": "Lorenzo Nava, Oriol Monserrat and Filippo Catani", "title": "Improving Landslide Detection on SAR Data through Deep Learning", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this letter, we use deep-learning convolution neural networks (CNNs) to\nassess the landslide mapping and classification performances on optical images\n(from Sentinel-2) and SAR images (from Sentinel-1). The training and test zones\nused to independently evaluate the performance of the CNNs on different\ndatasets are located in the eastern Iburi subprefecture in Hokkaido, where, at\n03.08 local time (JST) on September 6, 2018, an Mw 6.6 earthquake triggered\nabout 8000 coseismic landslides. We analyzed the conditions before and after\nthe earthquake exploiting multi-polarization SAR as well as optical data by\nmeans of a CNN implemented in TensorFlow that points out the locations where\nthe Landslide class is predicted as more likely. As expected, the CNN run on\noptical images proved itself excellent for the landslide detection task,\nachieving an overall accuracy of 99.20% while CNNs based on the combination of\nground range detected (GRD) SAR data reached overall accuracies beyond 94%. Our\nfindings show that the integrated use of SAR data may also allow for rapid\nmapping even during storms and under dense cloud cover and seems to provide\ncomparable accuracy to classical optical change detection in landslide\nrecognition and mapping.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:37:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nava", "Lorenzo", ""], ["Monserrat", "Oriol", ""], ["Catani", "Filippo", ""]]}, {"id": "2105.00794", "submitter": "Dennis Eschweiler", "authors": "Dennis Eschweiler and Johannes Stegmaier", "title": "Robust 3D Cell Segmentation: Extending the View of Cellpose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing data set sizes of digital microscopy imaging experiments demand\nfor an automation of segmentation processes to be able to extract meaningful\nbiomedical information. Due to the shortage of annotated 3D image data that can\nbe used for machine learning-based approaches, 3D segmentation approaches are\nrequired to be robust and to generalize well to unseen data. Reformulating the\nproblem of instance segmentation as a collection of diffusion gradient maps,\nproved to be such a generalist approach for cell segmentation tasks. In this\npaper, we extend the Cellpose approach to improve segmentation accuracy on 3D\nimage data and we further show how the formulation of the gradient maps can be\nsimplified while still being robust and reaching similar segmentation accuracy.\nWe quantitatively compared different experimental setups and validated on two\ndifferent data sets of 3D confocal microscopy images of A. thaliana.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:47:41 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Eschweiler", "Dennis", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "2105.00859", "submitter": "Hoel Kervadec", "authors": "Hoel Kervadec and Houda Bahig and Laurent Letourneau-Guillon and Jose\n  Dolz and Ismail Ben Ayed", "title": "Beyond pixel-wise supervision for segmentation: A few global shape\n  descriptors might be surprisingly good!", "comments": "Accepted at Medical Imaging with Deep Learning (MIDL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard losses for training deep segmentation networks could be seen as\nindividual classifications of pixels, instead of supervising the global shape\nof the predicted segmentations. While effective, they require exact knowledge\nof the label of each pixel in an image.\n  This study investigates how effective global geometric shape descriptors\ncould be, when used on their own as segmentation losses for training deep\nnetworks. Not only interesting theoretically, there exist deeper motivations to\nposing segmentation problems as a reconstruction of shape descriptors:\nAnnotations to obtain approximations of low-order shape moments could be much\nless cumbersome than their full-mask counterparts, and anatomical priors could\nbe readily encoded into invariant shape descriptions, which might alleviate the\nannotation burden. Also, and most importantly, we hypothesize that, given a\ntask, certain shape descriptions might be invariant across image acquisition\nprotocols/modalities and subject populations, which might open interesting\nresearch avenues for generalization in medical image segmentation.\n  We introduce and formulate a few shape descriptors in the context of deep\nsegmentation, and evaluate their potential as standalone losses on two\ndifferent challenging tasks. Inspired by recent works in constrained\noptimization for deep networks, we propose a way to use those descriptors to\nsupervise segmentation, without any pixel-level label. Very surprisingly, as\nlittle as 4 descriptors values per class can approach the performance of a\nsegmentation mask with 65k individual discrete labels. We also found that shape\ndescriptors can be a valid way to encode anatomical priors about the task,\nenabling to leverage expert knowledge without additional annotations. Our\nimplementation is publicly available and can be easily extended to other tasks\nand descriptors: https://github.com/hkervadec/shape_descriptors\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:44:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kervadec", "Hoel", ""], ["Bahig", "Houda", ""], ["Letourneau-Guillon", "Laurent", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2105.00916", "submitter": "Yujiang Wang", "authors": "Yuhu Chang, Yingying Zhao, Mingzhi Dong, Yujiang Wang, Yutian Lu, Qin\n  Lv, Robert P. Dick, Tun Lu, Ning Gu, Li Shang", "title": "MemX: An Attention-Aware Smart Eyewear System for Personalized Moment\n  Auto-capture", "comments": "Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT)", "journal-ref": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., Volume 5\n  Issue 2, Article 56. June 2021", "doi": "10.1145/3463509", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents MemX: a biologically-inspired attention-aware eyewear\nsystem developed with the goal of pursuing the long-awaited vision of a\npersonalized visual Memex. MemX captures human visual attention on the fly,\nanalyzes the salient visual content, and records moments of personal interest\nin the form of compact video snippets. Accurate attentive scene detection and\nanalysis on resource-constrained platforms is challenging because these tasks\nare computation and energy intensive. We propose a new temporal visual\nattention network that unifies human visual attention tracking and salient\nvisual content analysis. Attention tracking focuses computation-intensive video\nanalysis on salient regions, while video analysis makes human attention\ndetection and tracking more accurate. Using the YouTube-VIS dataset and 30\nparticipants, we experimentally show that MemX significantly improves the\nattention tracking accuracy over the eye-tracking-alone method, while\nmaintaining high system energy efficiency. We have also conducted 11 in-field\npilot studies across a range of daily usage scenarios, which demonstrate the\nfeasibility and potential benefits of MemX.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:54:16 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:07:30 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chang", "Yuhu", ""], ["Zhao", "Yingying", ""], ["Dong", "Mingzhi", ""], ["Wang", "Yujiang", ""], ["Lu", "Yutian", ""], ["Lv", "Qin", ""], ["Dick", "Robert P.", ""], ["Lu", "Tun", ""], ["Gu", "Ning", ""], ["Shang", "Li", ""]]}, {"id": "2105.00924", "submitter": "Devang Mahesh", "authors": "Narayani Bhatia, Devang Mahesh, Jashandeep Singh, and Manan Suri", "title": "Bird-Area Water-Bodies Dataset (BAWD) and Predictive AI Model for Avian\n  Botulism Outbreak (AVI-BoT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avian botulism caused by a bacterium, Clostridium botulinum, causes a\nparalytic disease in birds often leading to high fatality, and is usually\ndiagnosed using molecular techniques. Diagnostic techniques for Avian botulism\ninclude: Mouse Bioassay, ELISA, PCR, all of which are time-consuming, laborious\nand require invasive sample collection from affected sites. In this study, we\nbuild a first-ever multi-spectral, remote-sensing imagery based global\nBird-Area Water-bodies Dataset (BAWD) (i.e. fused satellite images of\nwater-body sites important for avian fauna) backed by on-ground reporting\nevidence of outbreaks. In the current version, BAWD covers a total ground area\nof 904 sq.km from two open source satellite projects (Sentinel and Landsat).\nBAWD consists of 17 topographically diverse global sites spanning across 4\ncontinents, with locations monitored over a time-span of 3 years (2016-2020).\nUsing BAWD and state-of-the-art deep-learning techniques we propose a\nfirst-ever Artificial Intelligence based (AI) model to predict potential\noutbreak of Avian botulism called AVI-BoT (Aerosol, Visible, Infra-red\n(NIR/SWIR) and Bands of Thermal). AVI-BoT uses fused multi-spectral satellite\nimages of water-bodies (10-bands) as input to generate a spatial prediction map\ndepicting probability of potential Avian botulism outbreaks. We also train and\ninvestigate a simpler (5-band) Causative-Factor model (based on prominent\nphysiological factors reported in literature as conducive for outbreak) to\npredict Avian botulism. Using AVI-BoT, we achieve a training accuracy of 0.94\nand validation accuracy of 0.96 on BAWD, far superior in comparison to our\nCausative factors model. The proposed technique presents a scale-able,\nlow-cost, non-invasive methodology for continuous monitoring of bird-habitats\nagainst botulism outbreaks with the potential of saving valuable fauna lives.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:00:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bhatia", "Narayani", ""], ["Mahesh", "Devang", ""], ["Singh", "Jashandeep", ""], ["Suri", "Manan", ""]]}, {"id": "2105.00929", "submitter": "Johanna Rock", "authors": "Alexander Fuchs, Johanna Rock, Mate Toth, Paul Meissner, Franz\n  Pernkopf", "title": "Complex-valued Convolutional Neural Networks for Enhanced Radar Signal\n  Denoising and Interference Mitigation", "comments": null, "journal-ref": "IEEE International Radar Conference 2021", "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Autonomous driving highly depends on capable sensors to perceive the\nenvironment and to deliver reliable information to the vehicles' control\nsystems. To increase its robustness, a diversified set of sensors is used,\nincluding radar sensors. Radar is a vital contribution of sensory information,\nproviding high resolution range as well as velocity measurements. The increased\nuse of radar sensors in road traffic introduces new challenges. As the so far\nunregulated frequency band becomes increasingly crowded, radar sensors suffer\nfrom mutual interference between multiple radar sensors. This interference must\nbe mitigated in order to ensure a high and consistent detection sensitivity. In\nthis paper, we propose the use of Complex-Valued Convolutional Neural Networks\n(CVCNNs) to address the issue of mutual interference between radar sensors. We\nextend previously developed methods to the complex domain in order to process\nradar data according to its physical characteristics. This not only increases\ndata efficiency, but also improves the conservation of phase information during\nfiltering, which is crucial for further processing, such as angle estimation.\nOur experiments show, that the use of CVCNNs increases data efficiency, speeds\nup network training and substantially improves the conservation of phase\ninformation during interference removal.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 10:06:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fuchs", "Alexander", ""], ["Rock", "Johanna", ""], ["Toth", "Mate", ""], ["Meissner", "Paul", ""], ["Pernkopf", "Franz", ""]]}, {"id": "2105.00930", "submitter": "Arnab Karmakar", "authors": "Arnab Karmakar and Deepak Mishra", "title": "Pose Invariant Person Re-Identification using Robust Pose-transformation\n  GAN", "comments": "Undergraduate thesis at Indian Institute of Space Science and\n  Technology, Under review in IEEE Systems, Man and Cybernetics (SMCA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The objective of person re-identification (re-ID) is to retrieve a person's\nimages from an image gallery, given a single instance of the person of\ninterest. Despite several advancements, learning discriminative\nidentity-sensitive and viewpoint invariant features for robust Person\nRe-identification is a major challenge owing to the large pose variation of\nhumans. This paper proposes a re-ID pipeline that utilizes the image generation\ncapability of Generative Adversarial Networks combined with pose clustering and\nfeature fusion to achieve pose invariant feature learning. The objective is to\nmodel a given person under different viewpoints and large pose changes and\nextract the most discriminative features from all the appearances. The pose\ntransformational GAN (pt-GAN) module is trained to generate a person's image in\nany given pose. In order to identify the most significant poses for\ndiscriminative feature extraction, a Pose Clustering module is proposed. The\ngiven instance of the person is modelled in varying poses and these features\nare effectively combined through the Feature Fusion Network. The final re-ID\nmodel consisting of these 3 sub-blocks, alleviates the pose dependence in\nperson re-ID. Also, The proposed model is robust to occlusion, scale, rotation\nand illumination, providing a framework for viewpoint invariant feature\nlearning. The proposed method outperforms the state-of-the-art GAN based models\nin 4 benchmark datasets. It also surpasses the state-of-the-art models that\nreport higher re-ID accuracy in terms of improvement over baseline.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:47:03 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:01:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Karmakar", "Arnab", ""], ["Mishra", "Deepak", ""]]}, {"id": "2105.00931", "submitter": "Unnat Jain", "authors": "Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik, Aniruddha Kembhavi, Luca\n  Weihs, Alexander Schwing", "title": "GridToPix: Training Embodied Agents with Minimal Supervision", "comments": "Project page: https://unnat.github.io/gridtopix/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep reinforcement learning (RL) promises freedom from hand-labeled\ndata, great successes, especially for Embodied AI, require significant work to\ncreate supervision via carefully shaped rewards. Indeed, without shaped\nrewards, i.e., with only terminal rewards, present-day Embodied AI results\ndegrade significantly across Embodied AI problems from single-agent\nHabitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent\nAI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent\nGoogle Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).\nAs training from shaped rewards doesn't scale to more realistic tasks, the\ncommunity needs to improve the success of training with terminal rewards. For\nthis we propose GridToPix: 1) train agents with terminal rewards in gridworlds\nthat generically mirror Embodied AI environments, i.e., they are independent of\nthe task; 2) distill the learned policy into agents that reside in complex\nvisual worlds. Despite learning from only terminal rewards with identical\nmodels and RL algorithms, GridToPix significantly improves results across\ntasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture\nMoving (success improves from 1% to 25%) to football gameplay (game score\nimproves from 0.1 to 0.6). GridToPix even helps to improve the results of\nshaped reward training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:59:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jain", "Unnat", ""], ["Liu", "Iou-Jen", ""], ["Lazebnik", "Svetlana", ""], ["Kembhavi", "Aniruddha", ""], ["Weihs", "Luca", ""], ["Schwing", "Alexander", ""]]}, {"id": "2105.00949", "submitter": "Yi Zhang", "authors": "Yi Zhang, Lu Zhang, Wassim Hamidouche and Olivier Deforges", "title": "CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient\n  Object Detection", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, numerous deep learning methods have been proposed to\naddress the task of segmenting salient objects from RGB images. However, these\napproaches depending on single modality fail to achieve the state-of-the-art\nperformance on widely used light field salient object detection (SOD) datasets,\nwhich collect large-scale natural images and provide multiple modalities such\nas multi-view, micro-lens images and depth maps. Most recently proposed light\nfield SOD methods have acquired improving detecting accuracy, yet still predict\nrough objects' structures and perform slow inference speed. To this end, we\npropose CMA-Net, which consists of two novel cascaded mutual attention modules\naiming at fusing the high level features from the modalities of all-in-focus\nand depth. Our proposed CMA-Net outperforms 30 SOD methods (by a large margin)\non two widely applied light field benchmark datasets. Besides, the proposed\nCMA-Net can run at a speed of 53 fps, thus being four times faster than the\nstate-of-the-art multi-modal SOD methods. Extensive quantitative and\nqualitative experiments illustrate both the effectiveness and efficiency of our\nCMA-Net, inspiring future development of multi-modal learning for both the\nRGB-D and light field SOD.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:32:12 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 08:46:20 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 11:58:37 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Yi", ""], ["Zhang", "Lu", ""], ["Hamidouche", "Wassim", ""], ["Deforges", "Olivier", ""]]}, {"id": "2105.00957", "submitter": "Tsung-Wei Ke", "authors": "Tsung-Wei Ke, Jyh-Jing Hwang, Stella X. Yu", "title": "Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive\n  Learning", "comments": "In ICLR 2021. Webpage & Code:\n  https://twke18.github.io/projects/spml.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised segmentation requires assigning a label to every pixel\nbased on training instances with partial annotations such as image-level tags,\nobject bounding boxes, labeled points and scribbles. This task is challenging,\nas coarse annotations (tags, boxes) lack precise pixel localization whereas\nsparse annotations (points, scribbles) lack broad region coverage. Existing\nmethods tackle these two types of weak supervision differently: Class\nactivation maps are used to localize coarse labels and iteratively refine the\nsegmentation model, whereas conditional random fields are used to propagate\nsparse labels to the entire image.\n  We formulate weakly supervised segmentation as a semi-supervised metric\nlearning problem, where pixels of the same (different) semantics need to be\nmapped to the same (distinctive) features. We propose 4 types of contrastive\nrelationships between pixels and segments in the feature space, capturing\nlow-level image similarity, semantic annotation, co-occurrence, and feature\naffinity They act as priors; the pixel-wise feature can be learned from\ntraining images with any partial annotations in a data-driven fashion. In\nparticular, unlabeled pixels in training images participate not only in\ndata-driven grouping within each image, but also in discriminative feature\nlearning within and across images. We deliver a universal weakly supervised\nsegmenter with significant gains on Pascal VOC and DensePose. Our code is\npublicly available at https://github.com/twke18/SPML.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:49:01 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 02:17:33 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ke", "Tsung-Wei", ""], ["Hwang", "Jyh-Jing", ""], ["Yu", "Stella X.", ""]]}, {"id": "2105.00967", "submitter": "Jun Li", "authors": "Jun Li, Zhaocong Wu, Zhongwen Hu, Canliang Jian, Shaojie Luo, Lichao\n  Mou, Xiao Xiang Zhu and Matthieu Molinier", "title": "A lightweight deep learning based cloud detection method for Sentinel-2A\n  imagery fusing multi-scale spectral and spatial features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clouds are a very important factor in the availability of optical remote\nsensing images. Recently, deep learning-based cloud detection methods have\nsurpassed classical methods based on rules and physical models of clouds.\nHowever, most of these deep models are very large which limits their\napplicability and explainability, while other models do not make use of the\nfull spectral information in multi-spectral images such as Sentinel-2. In this\npaper, we propose a lightweight network for cloud detection, fusing multi-scale\nspectral and spatial features (CDFM3SF) and tailored for processing all\nspectral bands in Sentinel- 2A images. The proposed method consists of an\nencoder and a decoder. In the encoder, three input branches are designed to\nhandle spectral bands at their native resolution and extract multiscale\nspectral features. Three novel components are designed: a mixed depth-wise\nseparable convolution (MDSC) and a shared and dilated residual block (SDRB) to\nextract multi-scale spatial features, and a concatenation and sum (CS)\noperation to fuse multi-scale spectral and spatial features with little\ncalculation and no additional parameters. The decoder of CD-FM3SF outputs three\ncloud masks at the same resolution as input bands to enhance the supervision\ninformation of small, middle and large clouds. To validate the performance of\nthe proposed method, we manually labeled 36 Sentinel-2A scenes evenly\ndistributed over mainland China. The experiment results demonstrate that\nCD-FM3SF outperforms traditional cloud detection methods and state-of-theart\ndeep learning-based methods in both accuracy and speed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:36:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Jun", ""], ["Wu", "Zhaocong", ""], ["Hu", "Zhongwen", ""], ["Jian", "Canliang", ""], ["Luo", "Shaojie", ""], ["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""], ["Molinier", "Matthieu", ""]]}, {"id": "2105.00999", "submitter": "Krushi Patel", "authors": "Krushi Patel, Andres M. Bur, Guanghui Wang", "title": "Enhanced U-Net: A Feature Enhancement Network for Polyp Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colonoscopy is a procedure to detect colorectal polyps which are the primary\ncause for developing colorectal cancer. However, polyp segmentation is a\nchallenging task due to the diverse shape, size, color, and texture of polyps,\nshuttle difference between polyp and its background, as well as low contrast of\nthe colonoscopic images. To address these challenges, we propose a feature\nenhancement network for accurate polyp segmentation in colonoscopy images.\nSpecifically, the proposed network enhances the semantic information using the\nnovel Semantic Feature Enhance Module (SFEM). Furthermore, instead of directly\nadding encoder features to the respective decoder layer, we introduce an\nAdaptive Global Context Module (AGCM), which focuses only on the encoder's\nsignificant and hard fine-grained features. The integration of these two\nmodules improves the quality of features layer by layer, which in turn enhances\nthe final feature representation. The proposed approach is evaluated on five\ncolonoscopy datasets and demonstrates superior performance compared to other\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:46:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Patel", "Krushi", ""], ["Bur", "Andres M.", ""], ["Wang", "Guanghui", ""]]}, {"id": "2105.01017", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep\n  Akata", "title": "Learning Graph Embeddings for Open World Compositional Zero-Shot\n  Learning", "comments": "arXiv admin note: text overlap with arXiv:2101.12609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions\nof state and object visual primitives seen during training. A problem with\nstandard CZSL is the assumption of knowing which unseen compositions will be\navailable at test time. In this work, we overcome this assumption operating on\nthe open world setting, where no limit is imposed on the compositional space at\ntest time, and the search space contains a large number of unseen compositions.\nTo address this problem, we propose a new approach, Compositional Cosine Graph\nEmbeddings (Co-CGE), based on two principles. First, Co-CGE models the\ndependency between states, objects and their compositions through a graph\nconvolutional neural network. The graph propagates information from seen to\nunseen concepts, improving their representations. Second, since not all unseen\ncompositions are equally feasible, and less feasible ones may damage the\nlearned representations, Co-CGE estimates a feasibility score for each unseen\ncomposition, using the scores as margins in a cosine similarity-based loss and\nas weights in the adjacency matrix of the graphs. Experiments show that our\napproach achieves state-of-the-art performances in standard CZSL while\noutperforming previous methods in the open world scenario.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:08:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Naeem", "Muhammad Ferjad", ""], ["Xian", "Yongqin", ""], ["Akata", "Zeynep", ""]]}, {"id": "2105.01029", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak and Neil Tenenholtz and Lester Mackey and Nicol\\`o Fusi", "title": "Initialization and Regularization of Factorized Neural Layers", "comments": "ICLR 2021 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorized layers--operations parameterized by products of two or more\nmatrices--occur in a variety of deep learning contexts, including compressed\nmodel training, certain types of knowledge distillation, and multi-head\nself-attention architectures. We study how to initialize and regularize deep\nnets containing such layers, examining two simple, understudied schemes,\nspectral initialization and Frobenius decay, for improving their performance.\nThe guiding insight is to design optimization routines for these networks that\nare as close as possible to that of their well-tuned, non-decomposed\ncounterparts; we back this intuition with an analysis of how the initialization\nand regularization schemes impact training with gradient descent, drawing on\nmodern attempts to understand the interplay of weight-decay and\nbatch-normalization. Empirically, we highlight the benefits of spectral\ninitialization and Frobenius decay across a variety of settings. In model\ncompression, we show that they enable low-rank methods to significantly\noutperform both unstructured sparsity and tensor methods on the task of\ntraining low-memory residual networks; analogs of the schemes also improve the\nperformance of tensor decomposition techniques. For knowledge distillation,\nFrobenius decay enables a simple, overcomplete baseline that yields a compact\nmodel from over-parameterized training without requiring retraining with or\npruning a teacher network. Finally, we show how both schemes applied to\nmulti-head attention lead to improved performance on both translation and\nunsupervised pre-training.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:28:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khodak", "Mikhail", ""], ["Tenenholtz", "Neil", ""], ["Mackey", "Lester", ""], ["Fusi", "Nicol\u00f2", ""]]}, {"id": "2105.01047", "submitter": "Samir Yitzhak Gadre", "authors": "Samir Yitzhak Gadre, Kiana Ehsani, Shuran Song", "title": "Act the Part: Learning Interaction Strategies for Articulated Object\n  Part Discovery", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People often use physical intuition when manipulating articulated objects,\nirrespective of object semantics. Motivated by this observation, we identify an\nimportant embodied task where an agent must play with objects to recover their\nparts. To this end, we introduce Act the Part (AtP) to learn how to interact\nwith articulated objects to discover and segment their pieces. By coupling\naction selection and motion segmentation, AtP is able to isolate structures to\nmake perceptual part recovery possible without semantic labels. Our experiments\nshow AtP learns efficient strategies for part discovery, can generalize to\nunseen categories, and is capable of conditional reasoning for the task.\nAlthough trained in simulation, we show convincing transfer to real world data\nwith no fine-tuning.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:48:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gadre", "Samir Yitzhak", ""], ["Ehsani", "Kiana", ""], ["Song", "Shuran", ""]]}, {"id": "2105.01057", "submitter": "Soshi Shimada", "authors": "Soshi Shimada and Vladislav Golyanik and Weipeng Xu and Patrick\n  P\\'erez and Christian Theobalt", "title": "Neural Monocular 3D Human Motion Capture with Physical Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new trainable system for physically plausible markerless 3D\nhuman motion capture, which achieves state-of-the-art results in a broad range\nof challenging scenarios. Unlike most neural methods for human motion capture,\nour approach, which we dub physionical, is aware of physical and environmental\nconstraints. It combines in a fully differentiable way several key innovations,\ni.e., 1. a proportional-derivative controller, with gains predicted by a neural\nnetwork, that reduces delays even in the presence of fast motions, 2. an\nexplicit rigid body dynamics model and 3. a novel optimisation layer that\nprevents physically implausible foot-floor penetration as a hard constraint.\nThe inputs to our system are 2D joint keypoints, which are canonicalised in a\nnovel way so as to reduce the dependency on intrinsic camera parameters -- both\nat train and test time. This enables more accurate global translation\nestimation without generalisability loss. Our model can be finetuned only with\n2D annotations when the 3D annotations are not available. It produces smooth\nand physically principled 3D motions in an interactive frame rate in a wide\nvariety of challenging scenes, including newly recorded ones. Its advantages\nare especially noticeable on in-the-wild sequences that significantly differ\nfrom common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP.\nQualitative results are available at\nhttp://gvv.mpi-inf.mpg.de/projects/PhysAware/\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:57:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Xu", "Weipeng", ""], ["P\u00e9rez", "Patrick", ""], ["Theobalt", "Christian", ""]]}, {"id": "2105.01058", "submitter": "Weijun Tan", "authors": "Delong Qi, Weijun Tan, Zhifu Liu, Qi Yao, Jingfeng Liu", "title": "A Gun Detection Dataset and Searching for Embedded Device Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gun violence is a severe problem in the world, particularly in the United\nStates. Computer vision methods have been studied to detect guns in\nsurveillance video cameras or smart IP cameras and to send a real-time alert to\nsafety personals. However, due to no public datasets, it is hard to benchmark\nhow well such methods work in real applications. In this paper we publish a\ndataset with 51K annotated gun images for gun detection and other 51K cropped\ngun chip images for gun classification we collect from a few different sources.\nTo our knowledge, this is the largest dataset for the study of gun detection.\nThis dataset can be downloaded at www.linksprite.com/gun-detection-datasets. We\nalso study to search for solutions for gun detection in embedded edge device\n(camera) and a gun/non-gun classification on a cloud server. This edge/cloud\nframework makes possible the deployment of gun detection in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:58:45 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Qi", "Delong", ""], ["Tan", "Weijun", ""], ["Liu", "Zhifu", ""], ["Yao", "Qi", ""], ["Liu", "Jingfeng", ""]]}, {"id": "2105.01060", "submitter": "Chuang Gan", "authors": "Yilun Du, Chuang Gan, Phillip Isola", "title": "Curious Representation Learning for Embodied Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Self-supervised representation learning has achieved remarkable success in\nrecent years. By subverting the need for supervised labels, such approaches are\nable to utilize the numerous unlabeled images that exist on the Internet and in\nphotographic datasets. Yet to build truly intelligent agents, we must construct\nrepresentation learning algorithms that can learn not only from datasets but\nalso learn from environments. An agent in a natural environment will not\ntypically be fed curated data. Instead, it must explore its environment to\nacquire the data it will learn from. We propose a framework, curious\nrepresentation learning (CRL), which jointly learns a reinforcement learning\npolicy and a visual representation model. The policy is trained to maximize the\nerror of the representation learner, and in doing so is incentivized to explore\nits environment. At the same time, the learned representation becomes stronger\nand stronger as the policy feeds it ever harder data to learn from. Our learned\nrepresentations enable promising transfer to downstream navigation tasks,\nperforming better than or comparably to ImageNet pretraining without using any\nsupervision at all. In addition, despite being trained in simulation, our\nlearned representations can obtain interpretable results on real images.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:59:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Du", "Yilun", ""], ["Gan", "Chuang", ""], ["Isola", "Phillip", ""]]}, {"id": "2105.01061", "submitter": "Alexander Raistrick", "authors": "Alexander Raistrick, Nilesh Kulkarni, David F. Fouhey", "title": "Collision Replay: What Does Bumping Into Things Tell You About Scene\n  Geometry?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does bumping into things in a scene tell you about scene geometry? In\nthis paper, we investigate the idea of learning from collisions. At the heart\nof our approach is the idea of collision replay, where we use examples of a\ncollision to provide supervision for observations at a past frame. We use\ncollision replay to train convolutional neural networks to predict a\ndistribution over collision time from new images. This distribution conveys\ninformation about the navigational affordances (e.g., corridors vs open spaces)\nand, as we show, can be converted into the distance function for the scene\ngeometry. We analyze this approach with an agent that has noisy actuation in a\nphotorealistic simulator.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:59:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Raistrick", "Alexander", ""], ["Kulkarni", "Nilesh", ""], ["Fouhey", "David F.", ""]]}, {"id": "2105.01129", "submitter": "Gaurav Sahu", "authors": "Gaurav Sahu, Robin Cohen, Olga Vechtomova", "title": "Towards A Multi-agent System for Online Hate Speech Detection", "comments": "Accepted to the 2nd International Workshop on Autonomous Agents for\n  Social Good (AASG), AAMAS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper envisions a multi-agent system for detecting the presence of hate\nspeech in online social media platforms such as Twitter and Facebook. We\nintroduce a novel framework employing deep learning techniques to coordinate\nthe channels of textual and im-age processing. Our experimental results aim to\ndemonstrate the effectiveness of our methods for classifying online content,\ntraining the proposed neural network model to effectively detect hateful\ninstances in the input. We conclude with a discussion of how our system may be\nof use to provide recommendations to users who are managing online social\nnetworks, showcasing the immense potential of intelligent multi-agent systems\ntowards delivering social good.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 19:06:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sahu", "Gaurav", ""], ["Cohen", "Robin", ""], ["Vechtomova", "Olga", ""]]}, {"id": "2105.01133", "submitter": "Vijay Yadav", "authors": "Li Zhang, Vijay Yadav, Vidya Koesmahargyo, Anzar Abbas, Isaac\n  Galatzer-Levy", "title": "Prediction of clinical tremor severity using Rank Consistent Ordinal\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tremor is a key diagnostic feature of Parkinson's Disease (PD), Essential\nTremor (ET), and other central nervous system (CNS) disorders. Clinicians or\ntrained raters assess tremor severity with TETRAS scores by observing patients.\nLacking quantitative measures, inter- or intra- observer variabilities are\nalmost inevitable as the distinction between adjacent tremor scores is subtle.\nMoreover, clinician assessments also require patient visits, which limits the\nfrequency of disease progress evaluation. Therefore it is beneficial to develop\nan automated assessment that can be performed remotely and repeatably at\npatients' convenience for continuous monitoring. In this work, we proposed to\ntrain a deep neural network (DNN) with rank-consistent ordinal regression using\n276 clinical videos from 36 essential tremor patients. The videos are coupled\nwith clinician assessed TETRAS scores, which are used as ground truth labels to\ntrain the DNN. To tackle the challenge of limited training data, optical flows\nare used to eliminate irrelevant background and statistic objects from RGB\nframes. In addition to optical flows, transfer learning is also applied to\nleverage pre-trained network weights from a related task of tremor frequency\nestimate. The approach was evaluated by splitting the clinical videos into\ntraining (67%) and testing sets (0.33%). The mean absolute error on TETRAS\nscore of the testing results is 0.45, indicating that most of the errors were\nfrom the mismatch of adjacent labels, which is expected and acceptable. The\nmodel predications also agree well with clinical ratings. This model is further\napplied to smart phone videos collected from a PD patient who has an implanted\ndevice to turn \"On\" or \"Off\" tremor. The model outputs were consistent with the\npatient tremor states. The results demonstrate that our trained model can be\nused as a means to assess and track tremor severity.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 19:22:05 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Li", ""], ["Yadav", "Vijay", ""], ["Koesmahargyo", "Vidya", ""], ["Abbas", "Anzar", ""], ["Galatzer-Levy", "Isaac", ""]]}, {"id": "2105.01147", "submitter": "Alessio Schiavo Alessio", "authors": "Alessio Schiavo, Filippo Minutella, Mattia Daole, Marsha Gomez Gomez", "title": "Sketches image analysis: Web image search engine usingLSH index and DNN\n  InceptionV3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The adoption of an appropriate approximate similarity search method is an\nessential prereq-uisite for developing a fast and efficient CBIR system,\nespecially when dealing with large amount ofdata. In this study we implement a\nweb image search engine on top of a Locality Sensitive Hashing(LSH) Index to\nallow fast similarity search on deep features. Specifically, we exploit\ntransfer learningfor deep features extraction from images. Firstly, we adopt\nInceptionV3 pretrained on ImageNet asfeatures extractor, secondly, we try out\nseveral CNNs built on top of InceptionV3 as convolutionalbase fine-tuned on our\ndataset. In both of the previous cases we index the features extracted within\nourLSH index implementation so as to compare the retrieval performances with\nand without fine-tuning.In our approach we try out two different LSH\nimplementations: the first one working with real numberfeature vectors and the\nsecond one with the binary transposed version of those vectors.\nInterestingly,we obtain the best performances when using the binary LSH,\nreaching almost the same result, in termsof mean average precision, obtained by\nperforming sequential scan of the features, thus avoiding thebias introduced by\nthe LSH index. Lastly, we carry out a performance analysis class by class in\nterms ofrecall againstmAPhighlighting, as expected, a strong positive\ncorrelation between the two.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:01:54 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Schiavo", "Alessio", ""], ["Minutella", "Filippo", ""], ["Daole", "Mattia", ""], ["Gomez", "Marsha Gomez", ""]]}, {"id": "2105.01151", "submitter": "\\`Oscar Lorente", "authors": "\\`Oscar Lorente, Josep R. Casas, Santiago Royo, Ivan Caminal", "title": "Pedestrian Detection in 3D Point Clouds using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting pedestrians is a crucial task in autonomous driving systems to\nensure the safety of drivers and pedestrians. The technologies involved in\nthese algorithms must be precise and reliable, regardless of environment\nconditions. Relying solely on RGB cameras may not be enough to recognize road\nenvironments in situations where cameras cannot capture scenes properly. Some\napproaches aim to compensate for these limitations by combining RGB cameras\nwith TOF sensors, such as LIDARs. However, there are few works that address\nthis problem using exclusively the 3D geometric information provided by LIDARs.\nIn this paper, we propose a PointNet++ based architecture to detect pedestrians\nin dense 3D point clouds. The aim is to explore the potential contribution of\ngeometric information alone in pedestrian detection systems. We also present a\nsemi-automatic labeling system that transfers pedestrian and non-pedestrian\nlabels from RGB images onto the 3D domain. The fact that our datasets have RGB\nregistered with point clouds enables label transferring by back projection from\n2D bounding boxes to point clouds, with only a light manual supervision to\nvalidate results. We train PointNet++ with the geometry of the resulting 3D\nlabelled clusters. The evaluation confirms the effectiveness of the proposed\nmethod, yielding precision and recall values around 98%.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:12:11 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lorente", "\u00d2scar", ""], ["Casas", "Josep R.", ""], ["Royo", "Santiago", ""], ["Caminal", "Ivan", ""]]}, {"id": "2105.01181", "submitter": "Ecem Sogancioglu", "authors": "Ecem Sogancioglu, Keelin Murphy, Ernst Th. Scholten, Luuk H. Boulogne,\n  Mathias Prokop, and Bram van Ginneken", "title": "Automated Estimation of Total Lung Volume using Chest Radiographs and\n  Deep Learning", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Total lung volume is an important quantitative biomarker and is used for the\nassessment of restrictive lung diseases. In this study, we investigate the\nperformance of several deep-learning approaches for automated measurement of\ntotal lung volume from chest radiographs. 7621 posteroanterior and lateral view\nchest radiographs (CXR) were collected from patients with chest CT available.\nSimilarly, 928 CXR studies were chosen from patients with pulmonary function\ntest (PFT) results. The reference total lung volume was calculated from lung\nsegmentation on CT or PFT data, respectively. This dataset was used to train\ndeep-learning architectures to predict total lung volume from chest\nradiographs. The experiments were constructed in a step-wise fashion with\nincreasing complexity to demonstrate the effect of training with CT-derived\nlabels only and the sources of error. The optimal models were tested on 291 CXR\nstudies with reference lung volume obtained from PFT. The optimal deep-learning\nregression model showed an MAE of 408 ml and a MAPE of 8.1\\% and Pearson's r =\n0.92 using both frontal and lateral chest radiographs as input. CT-derived\nlabels were useful for pre-training but the optimal performance was obtained by\nfine-tuning the network with PFT-derived labels. We demonstrate, for the first\ntime, that state-of-the-art deep learning solutions can accurately measure\ntotal lung volume from plain chest radiographs. The proposed model can be used\nto obtain total lung volume from routinely acquired chest radiographs at no\nadditional cost and could be a useful tool to identify trends over time in\npatients referred regularly for chest x-rays.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:35:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sogancioglu", "Ecem", ""], ["Murphy", "Keelin", ""], ["Scholten", "Ernst Th.", ""], ["Boulogne", "Luuk H.", ""], ["Prokop", "Mathias", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2105.01198", "submitter": "Maysam Behmanesh", "authors": "Maysam Behmanesh, Peyman Adibi, Hossein Karshenas", "title": "Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set\n  Theory for Imbalanced Data Classification", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Support vector machines (SVMs) are powerful supervised learning tools\ndeveloped to solve classification problems. However, SVMs are likely to perform\npoorly in the classification of imbalanced data. The rough set theory presents\na mathematical tool for inference in nondeterministic cases that provides\nmethods for removing irrelevant information from data. In this work, we propose\nan approach that efficiently used fuzzy rough set theory in weighted least\nsquares twin support vector machine called FRLSTSVM for classification of\nimbalanced data. The first innovation is introducing a new fuzzy rough\nset-based under-sampling strategy to make the classifier robust in terms of the\nimbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM,\ndata points from the minority class remain unchanged while a subset of data\npoints in the majority class are selected using a new method. In this model, we\nembed the weight biases in the LSTSVM formulations to overcome the bias\nphenomenon in the original twin SVM for the classification of imbalanced data.\nIn order to determine these weights in this formulation, we introduce a new\nstrategy that uses fuzzy rough set theory as the second innovation.\nExperimental results on the famous imbalanced datasets, compared to the related\ntraditional SVM-based methods, demonstrate the superiority of the proposed\nFRLSTSVM model in the imbalanced data classification.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:33:39 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 20:29:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Behmanesh", "Maysam", ""], ["Adibi", "Peyman", ""], ["Karshenas", "Hossein", ""]]}, {"id": "2105.01203", "submitter": "Md Jubaer Hossain Pantho", "authors": "Md Jubaer Hossain Pantho, Joel Mandebi Mbongue, Pankaj Bhowmik,\n  Christophe Bobda", "title": "Event Camera Simulator Design for Modeling Attention-based Inference\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a growing interest in realizing methodologies\nto integrate more and more computation at the level of the image sensor. The\nrising trend has seen an increased research interest in developing novel event\ncameras that can facilitate CNN computation directly in the sensor. However,\nevent-based cameras are not generally available in the market, limiting\nperformance exploration on high-level models and algorithms. This paper\npresents an event camera simulator that can be a potent tool for hardware\ndesign prototyping, parameter optimization, attention-based innovative\nalgorithm development, and benchmarking. The proposed simulator implements a\ndistributed computation model to identify relevant regions in an image frame.\nOur simulator's relevance computation model is realized as a collection of\nmodules and performs computations in parallel. The distributed computation\nmodel is configurable, making it highly useful for design space exploration.\nThe Rendering engine of the simulator samples frame-regions only when there is\na new event. The simulator closely emulates an image processing pipeline\nsimilar to that of physical cameras. Our experimental results show that the\nsimulator can effectively emulate event vision with low overheads.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:41:45 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Pantho", "Md Jubaer Hossain", ""], ["Mbongue", "Joel Mandebi", ""], ["Bhowmik", "Pankaj", ""], ["Bobda", "Christophe", ""]]}, {"id": "2105.01213", "submitter": "Hung-Min Hsu", "authors": "Hung-Min Hsu, Jiarui Cai, Yizhou Wang, Jenq-Neng Hwang, Kwang-Ju Kim", "title": "Multi-Target Multi-Camera Tracking of Vehicles using Metadata-Aided\n  Re-ID and Trajectory-Based Camera Link Model", "comments": "IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3078124", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework for multi-target multi-camera\ntracking (MTMCT) of vehicles based on metadata-aided re-identification\n(MA-ReID) and the trajectory-based camera link model (TCLM). Given a video\nsequence and the corresponding frame-by-frame vehicle detections, we first\naddress the isolated tracklets issue from single camera tracking (SCT) by the\nproposed traffic-aware single-camera tracking (TSCT). Then, after automatically\nconstructing the TCLM, we solve MTMCT by the MA-ReID. The TCLM is generated\nfrom camera topological configuration to obtain the spatial and temporal\ninformation to improve the performance of MTMCT by reducing the candidate\nsearch of ReID. We also use the temporal attention model to create more\ndiscriminative embeddings of trajectories from each camera to achieve robust\ndistance measures for vehicle ReID. Moreover, we train a metadata classifier\nfor MTMCT to obtain the metadata feature, which is concatenated with the\ntemporal attention based embeddings. Finally, the TCLM and hierarchical\nclustering are jointly applied for global ID assignment. The proposed method is\nevaluated on the CityFlow dataset, achieving IDF1 76.77%, which outperforms the\nstate-of-the-art MTMCT methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 23:20:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hsu", "Hung-Min", ""], ["Cai", "Jiarui", ""], ["Wang", "Yizhou", ""], ["Hwang", "Jenq-Neng", ""], ["Kim", "Kwang-Ju", ""]]}, {"id": "2105.01218", "submitter": "Youbao Tang", "authors": "Youbao Tang, Jinzheng Cai, Ke Yan, Lingyun Huang, Guotong Xie, Jing\n  Xiao, Jingjing Lu, Gigin Lin, and Le Lu", "title": "Weakly-Supervised Universal Lesion Segmentation with Regional Level Set\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately segmenting a variety of clinically significant lesions from whole\nbody computed tomography (CT) scans is a critical task on precision oncology\nimaging, denoted as universal lesion segmentation (ULS). Manual annotation is\nthe current clinical practice, being highly time-consuming and inconsistent on\ntumor's longitudinal assessment. Effectively training an automatic segmentation\nmodel is desirable but relies heavily on a large number of pixel-wise labelled\ndata. Existing weakly-supervised segmentation approaches often struggle with\nregions nearby the lesion boundaries. In this paper, we present a novel\nweakly-supervised universal lesion segmentation method by building an attention\nenhanced model based on the High-Resolution Network (HRNet), named AHRNet, and\npropose a regional level set (RLS) loss for optimizing lesion boundary\ndelineation. AHRNet provides advanced high-resolution deep image features by\ninvolving a decoder, dual-attention and scale attention mechanisms, which are\ncrucial to performing accurate lesion segmentation. RLS can optimize the model\nreliably and effectively in a weakly-supervised fashion, forcing the\nsegmentation close to lesion boundary. Extensive experimental results\ndemonstrate that our method achieves the best performance on the publicly\nlarge-scale DeepLesion dataset and a hold-out test set.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 23:33:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tang", "Youbao", ""], ["Cai", "Jinzheng", ""], ["Yan", "Ke", ""], ["Huang", "Lingyun", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Lu", "Jingjing", ""], ["Lin", "Gigin", ""], ["Lu", "Le", ""]]}, {"id": "2105.01237", "submitter": "Yinxiao Li", "authors": "Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, Ming-Hsuan Yang, Peyman\n  Milanfar", "title": "COMISR: Compression-Informed Video Super-Resolution", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video super-resolution methods focus on restoring high-resolution video\nframes from low-resolution videos without taking into account compression.\nHowever, most videos on the web or mobile devices are compressed, and the\ncompression can be severe when the bandwidth is limited. In this paper, we\npropose a new compression-informed video super-resolution model to restore\nhigh-resolution content without introducing artifacts caused by compression.\nThe proposed model consists of three modules for video super-resolution:\nbi-directional recurrent warping, detail-preserving flow estimation, and\nLaplacian enhancement. All these three modules are used to deal with\ncompression properties such as the location of the intra-frames in the input\nand smoothness in the output frames. For thorough performance evaluation, we\nconducted extensive experiments on standard datasets with a wide range of\ncompression rates, covering many real video use cases. We showed that our\nmethod not only recovers high-resolution content on uncompressed frames from\nthe widely-used benchmark datasets, but also achieves state-of-the-art\nperformance in super-resolving compressed videos based on numerous quantitative\nmetrics. We also evaluated the proposed method by simulating streaming from\nYouTube to demonstrate its effectiveness and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 01:24:44 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Li", "Yinxiao", ""], ["Jin", "Pengchong", ""], ["Yang", "Feng", ""], ["Liu", "Ce", ""], ["Yang", "Ming-Hsuan", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2105.01241", "submitter": "Bohan Zhuang", "authors": "Haoyu He, Jing Zhang, Bohan Zhuang, Jianfei Cai, Dacheng Tao", "title": "End-to-end One-shot Human Parsing", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous human parsing models are limited to parsing humans into pre-defined\nclasses, which is inflexible for applications that need to handle new classes.\nIn this paper, we define a new one-shot human parsing (OSHP) task that requires\nparsing humans into an open set of classes defined by any test example. During\ntraining, only base classes are exposed, which only overlap with part of\ntest-time classes. To address three main challenges in OSHP, i.e., small sizes,\ntesting bias, and similar parts, we devise a novel End-to-end One-shot human\nParsing Network (EOP-Net). Firstly, an end-to-end human parsing framework is\nproposed to mutually share semantic information with different granularities\nand help recognize the small-size human classes. Then, we devise two\ncollaborative metric learning modules to learn representative prototypes for\nbase classes, which can quickly adapt to unseen classes and mitigate the\ntesting bias. Moreover, we empirically find that robust prototypes empower\nfeature representations with higher transferability to the novel concepts,\nhence, we propose to adopt momentum-updated dynamic prototypes generated by\ngradually smoothing the training time prototypes and employ contrastive loss at\nthe prototype level. Experiments on three popular benchmarks tailored for OSHP\ndemonstrate that EOP-Net outperforms representative one-shot segmentation\nmodels by large margins, which serves as a strong benchmark for further\nresearch on this new task. The source code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 01:35:50 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["He", "Haoyu", ""], ["Zhang", "Jing", ""], ["Zhuang", "Bohan", ""], ["Cai", "Jianfei", ""], ["Tao", "Dacheng", ""]]}, {"id": "2105.01256", "submitter": "Muhannad Alkaddour", "authors": "Muhannad Alkaddour, Usman Tariq, Abhinav Dhall", "title": "Self-Supervised Approach for Facial Movement Based Optical Flow", "comments": "14 pages, 4 figures, 5 tables The supplemental material (error\n  histograms) can be found on\n  https://www.dropbox.com/s/o7158gi46tppvb1/SupplementalMaterial_OpticalFlow.docx?dl=0\n  Manuscript submitted to: IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing optical flow is a fundamental problem in computer vision. However,\ndeep learning-based optical flow techniques do not perform well for non-rigid\nmovements such as those found in faces, primarily due to lack of the training\ndata representing the fine facial motion. We hypothesize that learning optical\nflow on face motion data will improve the quality of predicted flow on faces.\nThe aim of this work is threefold: (1) exploring self-supervised techniques to\ngenerate optical flow ground truth for face images; (2) computing baseline\nresults on the effects of using face data to train Convolutional Neural\nNetworks (CNN) for predicting optical flow; and (3) using the learned optical\nflow in micro-expression recognition to demonstrate its effectiveness. We\ngenerate optical flow ground truth using facial key-points in the\nBP4D-Spontaneous dataset. The generated optical flow is used to train the\nFlowNetS architecture to test its performance on the generated dataset. The\nperformance of FlowNetS trained on face images surpassed that of other optical\nflow CNN architectures, demonstrating its usefulness. Our optical flow features\nare further compared with other methods using the STSTNet micro-expression\nclassifier, and the results indicate that the optical flow obtained using this\nwork has promising applications in facial expression analysis.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 02:38:11 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Alkaddour", "Muhannad", ""], ["Tariq", "Usman", ""], ["Dhall", "Abhinav", ""]]}, {"id": "2105.01284", "submitter": "Hossein Aboutalebi", "authors": "Hossein Aboutalebi, Saad Abbasi, Mohammad Javad Shafiee, Alexander\n  Wong", "title": "COVID-Net CT-S: 3D Convolutional Neural Network Architectures for\n  COVID-19 Severity Assessment using Chest CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The health and socioeconomic difficulties caused by the COVID-19 pandemic\ncontinues to cause enormous tensions around the world. In particular, this\nextraordinary surge in the number of cases has put considerable strain on\nhealth care systems around the world. A critical step in the treatment and\nmanagement of COVID-19 positive patients is severity assessment, which is\nchallenging even for expert radiologists given the subtleties at different\nstages of lung disease severity. Motivated by this challenge, we introduce\nCOVID-Net CT-S, a suite of deep convolutional neural networks for predicting\nlung disease severity due to COVID-19 infection. More specifically, a 3D\nresidual architecture design is leveraged to learn volumetric visual indicators\ncharacterizing the degree of COVID-19 lung disease severity. Experimental\nresults using the patient cohort collected by the China National Center for\nBioinformation (CNCB) showed that the proposed COVID-Net CT-S networks, by\nleveraging volumetric features, can achieve significantly improved severity\nassessment performance when compared to traditional severity assessment\nnetworks that learn and leverage 2D visual features to characterize COVID-19\nseverity.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:44:41 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Aboutalebi", "Hossein", ""], ["Abbasi", "Saad", ""], ["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "2105.01288", "submitter": "Tiange Xiang", "authors": "Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, Weidong Cai", "title": "Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete point cloud objects lack sufficient shape descriptors of 3D\ngeometries. In this paper, we present a novel method for aggregating\nhypothetical curves in point clouds. Sequences of connected points (curves) are\ninitially grouped by taking guided walks in the point clouds, and then\nsubsequently aggregated back to augment their point-wise features. We provide\nan effective implementation of the proposed aggregation strategy including a\nnovel curve grouping operator followed by a curve aggregation operator. Our\nmethod was benchmarked on several point cloud analysis tasks where we achieved\nthe state-of-the-art classification accuracy of 94.2% on the ModelNet40\nclassification task, instance IoU of 86.8 on the ShapeNetPart segmentation\ntask, and cosine error of 0.11 on the ModelNet40 normal estimation task.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:03:47 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:53:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Xiang", "Tiange", ""], ["Zhang", "Chaoyi", ""], ["Song", "Yang", ""], ["Yu", "Jianhui", ""], ["Cai", "Weidong", ""]]}, {"id": "2105.01289", "submitter": "Aniket Anand Deshmukh", "authors": "Aniket Anand Deshmukh, Jayanth Reddy Regatti, Eren Manavoglu, and Urun\n  Dogan", "title": "Representation Learning for Clustering via Building Consensus", "comments": "arXiv admin note: text overlap with arXiv:2010.01245", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on deep clustering and unsupervised representation\nlearning for images. Recent advances in deep clustering and unsupervised\nrepresentation learning are based on the idea that different views of an input\nimage (generated through data augmentation techniques) must be closer in the\nrepresentation space (exemplar consistency), and/or similar images have a\nsimilar cluster assignment (population consistency). We define an additional\nnotion of consistency, consensus consistency, which ensures that\nrepresentations are learnt to induce similar partitions for variations in the\nrepresentation space, different clustering algorithms or different\ninitializations of a clustering algorithm. We define a clustering loss by\nperforming variations in the representation space and seamlessly integrate all\nthree consistencies (consensus, exemplar and population) into an end-to-end\nlearning framework. The proposed algorithm, Consensus Clustering using\nUnsupervised Representation Learning (ConCURL) improves the clustering\nperformance over state-of-the art methods on four out of five image datasets.\nFurther, we extend the evaluation procedure for clustering to reflect the\nchallenges in real world clustering tasks, such as clustering performance in\nthe case of distribution shift. We also perform a detailed ablation study for a\ndeeper understanding of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:04:03 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Deshmukh", "Aniket Anand", ""], ["Regatti", "Jayanth Reddy", ""], ["Manavoglu", "Eren", ""], ["Dogan", "Urun", ""]]}, {"id": "2105.01290", "submitter": "Zitong Yu", "authors": "Zitong Yu, Yunxiao Qin, Hengshuang Zhao, Xiaobai Li, Guoying Zhao", "title": "Dual-Cross Central Difference Network for Face Anti-Spoofing", "comments": "Accepted by IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Face anti-spoofing (FAS) plays a vital role in securing face recognition\nsystems. Recently, central difference convolution (CDC) has shown its excellent\nrepresentation capacity for the FAS task via leveraging local gradient\nfeatures. However, aggregating central difference clues from all\nneighbors/directions simultaneously makes the CDC redundant and sub-optimized\nin the training phase. In this paper, we propose two Cross Central Difference\nConvolutions (C-CDC), which exploit the difference of the center and surround\nsparse local features from the horizontal/vertical and diagonal directions,\nrespectively. It is interesting to find that, with only five ninth parameters\nand less computational cost, C-CDC even outperforms the full directional CDC.\nBased on these two decoupled C-CDC, a powerful Dual-Cross Central Difference\nNetwork (DC-CDN) is established with Cross Feature Interaction Modules (CFIM)\nfor mutual relation mining and local detailed representation enhancement.\nFurthermore, a novel Patch Exchange (PE) augmentation strategy for FAS is\nproposed via simply exchanging the face patches as well as their dense labels\nfrom random samples. Thus, the augmented samples contain richer live/spoof\npatterns and diverse domain distributions, which benefits the intrinsic and\nrobust feature learning. Comprehensive experiments are performed on four\nbenchmark datasets with three testing protocols to demonstrate our\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:11:47 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Yu", "Zitong", ""], ["Qin", "Yunxiao", ""], ["Zhao", "Hengshuang", ""], ["Li", "Xiaobai", ""], ["Zhao", "Guoying", ""]]}, {"id": "2105.01294", "submitter": "Weilin Zhang", "authors": "Weilin Zhang, Yu-Xiong Wang", "title": "Hallucination Improves Few-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning to detect novel objects from few annotated examples is of great\npractical importance. A particularly challenging yet common regime occurs when\nthere are extremely limited examples (less than three). One critical factor in\nimproving few-shot detection is to address the lack of variation in training\ndata. We propose to build a better model of variation for novel classes by\ntransferring the shared within-class variation from base classes. To this end,\nwe introduce a hallucinator network that learns to generate additional, useful\ntraining examples in the region of interest (RoI) feature space, and\nincorporate it into a modern object detection model. Our approach yields\nsignificant performance improvements on two state-of-the-art few-shot detectors\nwith different proposal generation procedures. In particular, we achieve new\nstate of the art in the extremely-few-shot regime on the challenging COCO\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:19:53 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Weilin", ""], ["Wang", "Yu-Xiong", ""]]}, {"id": "2105.01299", "submitter": "Wei-Ting Chen", "authors": "Hao-Hsiang Yang and Kuan-Chih Huang and Wei-Ting Chen", "title": "LAFFNet: A Lightweight Adaptive Feature Fusion Network for Underwater\n  Image Enhancement", "comments": "Accepted by ICRA 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image enhancement is an important low-level computer vision task\nfor autonomous underwater vehicles and remotely operated vehicles to explore\nand understand the underwater environments. Recently, deep convolutional neural\nnetworks (CNNs) have been successfully used in many computer vision problems,\nand so does underwater image enhancement. There are many deep-learning-based\nmethods with impressive performance for underwater image enhancement, but their\nmemory and model parameter costs are hindrances in practical application. To\naddress this issue, we propose a lightweight adaptive feature fusion network\n(LAFFNet). The model is the encoder-decoder model with multiple adaptive\nfeature fusion (AAF) modules. AAF subsumes multiple branches with different\nkernel sizes to generate multi-scale feature maps. Furthermore, channel\nattention is used to merge these feature maps adaptively. Our method reduces\nthe number of parameters from 2.5M to 0.15M (around 94% reduction) but\noutperforms state-of-the-art algorithms by extensive experiments. Furthermore,\nwe demonstrate our LAFFNet effectively improves high-level vision tasks like\nsalience object detection and single image depth estimation.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:31:10 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 02:16:23 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Hao-Hsiang", ""], ["Huang", "Kuan-Chih", ""], ["Chen", "Wei-Ting", ""]]}, {"id": "2105.01342", "submitter": "Murari Mandal", "authors": "Murari Mandal, Santosh Kumar Vipparthi", "title": "An Empirical Review of Deep Learning Frameworks for Change Detection:\n  Model Design, Experimental Frameworks, Challenges and Research Needs", "comments": "IEEE Transactions on Intelligent Transportation Systems, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual change detection, aiming at segmentation of video frames into\nforeground and background regions, is one of the elementary tasks in computer\nvision and video analytics. The applications of change detection include\nanomaly detection, object tracking, traffic monitoring, human machine\ninteraction, behavior analysis, action recognition, and visual surveillance.\nSome of the challenges in change detection include background fluctuations,\nillumination variation, weather changes, intermittent object motion, shadow,\nfast/slow object motion, camera motion, heterogeneous object shapes and\nreal-time processing. Traditionally, this problem has been solved using\nhand-crafted features and background modelling techniques. In recent years,\ndeep learning frameworks have been successfully adopted for robust change\ndetection. This article aims to provide an empirical review of the\nstate-of-the-art deep learning methods for change detection. More specifically,\nwe present a detailed analysis of the technical characteristics of different\nmodel designs and experimental frameworks. We provide model design based\ncategorization of the existing approaches, including the 2D-CNN, 3D-CNN,\nConvLSTM, multi-scale features, residual connections, autoencoders and GAN\nbased methods. Moreover, an empirical analysis of the evaluation settings\nadopted by the existing deep learning methods is presented. To the best of our\nknowledge, this is a first attempt to comparatively analyze the different\nevaluation frameworks used in the existing deep change detection methods.\nFinally, we point out the research needs, future directions and draw our own\nconclusions.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:42:40 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Mandal", "Murari", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "2105.01353", "submitter": "Qigong Sun", "authors": "Qigong Sun, Xiufang Li, Yan Ren, Zhongjian Huang, Xu Liu, Licheng\n  Jiao, Fang Liu", "title": "One Model for All Quantization: A Quantized Network Supporting Hot-Swap\n  Bit-Width Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an effective technique to achieve the implementation of deep neural\nnetworks in edge devices, model quantization has been successfully applied in\nmany practical applications. No matter the methods of quantization aware\ntraining (QAT) or post-training quantization (PTQ), they all depend on the\ntarget bit-widths. When the precision of quantization is adjusted, it is\nnecessary to fine-tune the quantized model or minimize the quantization noise,\nwhich brings inconvenience in practical applications. In this work, we propose\na method to train a model for all quantization that supports diverse bit-widths\n(e.g., form 8-bit to 1-bit) to satisfy the online quantization bit-width\nadjustment. It is hot-swappable that can provide specific quantization\nstrategies for different candidates through multiscale quantization. We use\nwavelet decomposition and reconstruction to increase the diversity of weights,\nthus significantly improving the performance of each quantization candidate,\nespecially at ultra-low bit-widths (e.g., 3-bit, 2-bit, and 1-bit).\nExperimental results on ImageNet and COCO show that our method can achieve\naccuracy comparable performance to dedicated models trained at the same\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 08:10:50 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sun", "Qigong", ""], ["Li", "Xiufang", ""], ["Ren", "Yan", ""], ["Huang", "Zhongjian", ""], ["Liu", "Xu", ""], ["Jiao", "Licheng", ""], ["Liu", "Fang", ""]]}, {"id": "2105.01386", "submitter": "Thrupthi Ann John", "authors": "Thrupthi Ann John, Vineeth N Balasubramanian, C V Jawahar", "title": "Canonical Saliency Maps: Decoding Deep Face Models", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Deep Neural Network models for face processing tasks approach human-like\nperformance, their deployment in critical applications such as law enforcement\nand access control has seen an upswing, where any failure may have far-reaching\nconsequences. We need methods to build trust in deployed systems by making\ntheir working as transparent as possible. Existing visualization algorithms are\ndesigned for object recognition and do not give insightful results when applied\nto the face domain. In this work, we present 'Canonical Saliency Maps', a new\nmethod that highlights relevant facial areas by projecting saliency maps onto a\ncanonical face model. We present two kinds of Canonical Saliency Maps:\nimage-level maps and model-level maps. Image-level maps highlight facial\nfeatures responsible for the decision made by a deep face model on a given\nimage, thus helping to understand how a DNN made a prediction on the image.\nModel-level maps provide an understanding of what the entire DNN model focuses\non in each task and thus can be used to detect biases in the model. Our\nqualitative and quantitative results show the usefulness of the proposed\ncanonical saliency maps, which can be used on any deep face model regardless of\nthe architecture.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:42:56 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["John", "Thrupthi Ann", ""], ["Balasubramanian", "Vineeth N", ""], ["Jawahar", "C V", ""]]}, {"id": "2105.01388", "submitter": "Nishant Rai", "authors": "Nishant Rai, Aidas Liaudanskas, Srinivas Rao, Rodrigo Ortiz Cayon,\n  Matteo Munaro, Stefan Holzer", "title": "Weak Multi-View Supervision for Surface Mapping Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a weakly-supervised multi-view learning approach to learn\ncategory-specific surface mapping without dense annotations. We learn the\nunderlying surface geometry of common categories, such as human faces, cars,\nand airplanes, given instances from those categories. While traditional\napproaches solve this problem using extensive supervision in the form of\npixel-level annotations, we take advantage of the fact that pixel-level UV and\nmesh predictions can be combined with 3D reprojections to form consistency\ncycles. As a result of exploiting these cycles, we can establish a dense\ncorrespondence mapping between image pixels and the mesh acting as a\nself-supervisory signal, which in turn helps improve our overall estimates. Our\napproach leverages information from multiple views of the object to establish\nadditional consistency cycles, thus improving surface mapping understanding\nwithout the need for explicit annotations. We also propose the use of\ndeformation fields for predictions of an instance specific mesh. Given the lack\nof datasets providing multiple images of similar object instances from\ndifferent viewpoints, we generate and release a multi-view ShapeNet Cars and\nAirplanes dataset created by rendering ShapeNet meshes using a 360 degree\ncamera trajectory around the mesh. For the human faces category, we process and\nadapt an existing dataset to a multi-view setup. Through experimental\nevaluations, we show that, at test time, our method can generate accurate\nvariations away from the mean shape, is multi-view consistent, and performs\ncomparably to fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:46:26 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Rai", "Nishant", ""], ["Liaudanskas", "Aidas", ""], ["Rao", "Srinivas", ""], ["Cayon", "Rodrigo Ortiz", ""], ["Munaro", "Matteo", ""], ["Holzer", "Stefan", ""]]}, {"id": "2105.01447", "submitter": "Yunhao Zhou", "authors": "Yunhao Zhou, Yi Wang and Lap-Pui Chau", "title": "Moving Towards Centers: Re-ranking with Attention and Memory for\n  Re-identification", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-ranking utilizes contextual information to optimize the initial ranking\nlist of person or vehicle re-identification (re-ID), which boosts the retrieval\nperformance at post-processing steps. This paper proposes a re-ranking network\nto predict the correlations between the probe and top-ranked neighbor samples.\nSpecifically, all the feature embeddings of query and gallery images are\nexpanded and enhanced by a linear combination of their neighbors, with the\ncorrelation prediction serves as discriminative combination weights. The\ncombination process is equivalent to moving independent embeddings toward the\nidentity centers, improving cluster compactness. For correlation prediction, we\nfirst aggregate the contextual information for probe's k-nearest neighbors via\nthe Transformer encoder. Then, we distill and refine the probe-related features\ninto the Contextual Memory cell via attention mechanism. Like humans that\nretrieve images by not only considering probe images but also memorizing the\nretrieved ones, the Contextual Memory produces multi-view descriptions for each\ninstance. Finally, the neighbors are reconstructed with features fetched from\nthe Contextual Memory, and a binary classifier predicts their correlations with\nthe probe. Experiments on six widely-used person and vehicle re-ID benchmarks\ndemonstrate the effectiveness of the proposed method. Especially, our method\nsurpasses the state-of-the-art re-ranking approaches on large-scale datasets by\na significant margin, i.e., with an average 3.08% CMC@1 and 7.46% mAP\nimprovements on VERI-Wild, MSMT17, and VehicleID datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:14:30 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhou", "Yunhao", ""], ["Wang", "Yi", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2105.01456", "submitter": "Sagi Eppel", "authors": "Sagi Eppel, Haoping Xu, Alan Aspuru-Guzik", "title": "Computer vision for liquid samples in hospitals and medical labs using\n  hierarchical image segmentation and relations prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores the use of computer vision for image segmentation and\nclassification of medical fluid samples in transparent containers (for example,\ntubes, syringes, infusion bags). Handling fluids such as infusion fluids,\nblood, and urine samples is a significant part of the work carried out in\nmedical labs and hospitals. The ability to accurately identify and segment the\nliquids and the vessels that contain them from images can help in automating\nsuch processes. Modern computer vision typically involves training deep neural\nnets on large datasets of annotated images. This work presents a new dataset\ncontaining 1,300 annotated images of medical samples involving vessels\ncontaining liquids and solid material. The images are annotated with the type\nof liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid,\nfoam, suspension), the type of vessel (e.g., syringe, tube, cup, infusion\nbottle/bag), and the properties of the vessel (transparent, opaque). In\naddition, vessel parts such as corks, labels, spikes, and valves are annotated.\nRelations and hierarchies between vessels and materials are also annotated,\nsuch as which vessel contains which material or which vessels are linked or\ncontain each other. Three neural networks are trained on the dataset: One\nnetwork learns to detect vessels, a second net detects the materials and parts\ninside each vessel, and a third net identifies relationships and connectivity\nbetween vessels.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:39:39 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Eppel", "Sagi", ""], ["Xu", "Haoping", ""], ["Aspuru-Guzik", "Alan", ""]]}, {"id": "2105.01502", "submitter": "I-Hsuan Li", "authors": "I-Hsuan Li", "title": "Technical Report for Valence-Arousal Estimation on Affwild2 Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe our method for tackling the valence-arousal\nestimation challenge from ABAW FG-2020 Competition. The competition organizers\nprovide an in-the-wild Aff-Wild2 dataset for participants to analyze affective\nbehavior in real-life settings. We use MIMAMO Net \\cite{deng2020mimamo} model\nto achieve information about micro-motion and macro-motion for improving video\nemotion recognition and achieve Concordance Correlation Coefficient (CCC) of\n0.415 and 0.511 for valence and arousal on the reselected validation set.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:00:07 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 06:24:54 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Li", "I-Hsuan", ""]]}, {"id": "2105.01510", "submitter": "Saumik Bhattacharya", "authors": "Rangan Das, Bikram Boote, Saumik Bhattacharya, Ujjwal Maulik", "title": "Multipath Graph Convolutional Neural Networks", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph convolution networks have recently garnered a lot of attention for\nrepresentation learning on non-Euclidean feature spaces. Recent research has\nfocused on stacking multiple layers like in convolutional neural networks for\nthe increased expressive power of graph convolution networks. However, simply\nstacking multiple graph convolution layers lead to issues like vanishing\ngradient, over-fitting and over-smoothing. Such problems are much less when\nusing shallower networks, even though the shallow networks have lower\nexpressive power. In this work, we propose a novel Multipath Graph\nconvolutional neural network that aggregates the output of multiple different\nshallow networks. We train and test our model on various benchmarks datasets\nfor the task of node property prediction. Results show that the proposed method\nnot only attains increased test accuracy but also requires fewer training\nepochs to converge. The full implementation is available at\nhttps://github.com/rangan2510/MultiPathGCN\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:11:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Das", "Rangan", ""], ["Boote", "Bikram", ""], ["Bhattacharya", "Saumik", ""], ["Maulik", "Ujjwal", ""]]}, {"id": "2105.01517", "submitter": "Yanbei Chen", "authors": "Yanbei Chen, Thomas Hummel, A. Sophia Koepke, Zeynep Akata", "title": "Where and When: Space-Time Attention for Audio-Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explaining the decision of a multi-modal decision-maker requires to determine\nthe evidence from both modalities. Recent advances in XAI provide explanations\nfor models trained on still images. However, when it comes to modeling multiple\nsensory modalities in a dynamic world, it remains underexplored how to\ndemystify the mysterious dynamics of a complex multi-modal model. In this work,\nwe take a crucial step forward and explore learnable explanations for\naudio-visual recognition. Specifically, we propose a novel space-time attention\nnetwork that uncovers the synergistic dynamics of audio and visual data over\nboth space and time. Our model is capable of predicting the audio-visual video\nevents, while justifying its decision by localizing where the relevant visual\ncues appear, and when the predicted sounds occur in videos. We benchmark our\nmodel on three audio-visual video event datasets, comparing extensively to\nmultiple recent multi-modal representation learners and intrinsic explanation\nmodels. Experimental results demonstrate the clear superior performance of our\nmodel over the existing methods on audio-visual video event recognition.\nMoreover, we conduct an in-depth study to analyze the explainability of our\nmodel based on robustness analysis via perturbation tests and pointing games\nusing human annotations.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:16:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Chen", "Yanbei", ""], ["Hummel", "Thomas", ""], ["Koepke", "A. Sophia", ""], ["Akata", "Zeynep", ""]]}, {"id": "2105.01553", "submitter": "Huang Tongbin", "authors": "Heqing Huang, Tongbin Huang, Zhen Li, Zhiwei Wei, Shilei Lv", "title": "Combining Supervised and Un-supervised Learning for Automatic Citrus\n  Segmentation", "comments": "7 pages,4 figures,Prepare for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Citrus segmentation is a key step of automatic citrus picking. While most\ncurrent image segmentation approaches achieve good segmentation results by\npixel-wise segmentation, these supervised learning-based methods require a\nlarge amount of annotated data, and do not consider the continuous temporal\nchanges of citrus position in real-world applications. In this paper, we first\ntrain a simple CNN with a small number of labelled citrus images in a\nsupervised manner, which can roughly predict the citrus location from each\nframe. Then, we extend a state-of-the-art unsupervised learning approach to\npre-learn the citrus's potential movements between frames from unlabelled\ncitrus's videos. To take advantages of both networks, we employ the multimodal\ntransformer to combine supervised learned static information and unsupervised\nlearned movement information. The experimental results show that combing both\nnetwork allows the prediction accuracy reached at 88.3$\\%$ IOU and 93.6$\\%$\nprecision, outperforming the original supervised baseline 1.2$\\%$ and 2.4$\\%$.\nCompared with most of the existing citrus segmentation methods, our method uses\na small amount of supervised data and a large number of unsupervised data,\nwhile learning the pixel level location information and the temporal\ninformation of citrus changes to enhance the segmentation effect.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:09:01 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Huang", "Heqing", ""], ["Huang", "Tongbin", ""], ["Li", "Zhen", ""], ["Wei", "Zhiwei", ""], ["Lv", "Shilei", ""]]}, {"id": "2105.01563", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Yang Liu and Pan Ji and Dongwoo Kim and Lei Wang and\n  Bob McKay and Saeed Anwar and Tom Gedeon", "title": "Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton sequences are lightweight and compact, thus are ideal candidates for\naction recognition on edge devices. Recent skeleton-based action recognition\nmethods extract features from 3D joint coordinates as spatial-temporal cues,\nusing these representations in a graph neural network for feature fusion to\nboost recognition performance. The use of first- and second-order features,\n\\ie{} joint and bone representations, has led to high accuracy. Nonetheless,\nmany models are still confused by actions that have similar motion\ntrajectories. To address these issues, we propose fusing third-order features\nin the form of angular encoding into modern architectures to robustly capture\nthe relationships between joints and body parts. This simple fusion with\npopular spatial-temporal graph neural networks achieves new state-of-the-art\naccuracy in two large benchmarks, including NTU60 and NTU120, while employing\nfewer parameters and reduced run time. Our source code is publicly available\nat: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:23:29 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 00:43:14 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 10:54:01 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Qin", "Zhenyue", ""], ["Liu", "Yang", ""], ["Ji", "Pan", ""], ["Kim", "Dongwoo", ""], ["Wang", "Lei", ""], ["McKay", "Bob", ""], ["Anwar", "Saeed", ""], ["Gedeon", "Tom", ""]]}, {"id": "2105.01580", "submitter": "Le-Anh Tran", "authors": "Le-Anh Tran, Truong-Dong Do, Dong-Chul Park, My-Ha Le", "title": "Robustness Enhancement of Object Detection in Advanced Driver Assistance\n  Systems (ADAS)", "comments": "6 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified system integrating a compact object detector and a surrounding\nenvironmental condition classifier for enhancing the robustness of object\ndetection scheme in advanced driver assistance systems (ADAS) is proposed in\nthis paper. ADAS are invented to improve traffic safety and effectiveness in\nautonomous driving systems where object detection plays an extremely important\nrole. However, modern object detectors integrated in ADAS are still unstable\ndue to high latency and the variation of the environmental contexts in the\ndeployment phase. Our system is proposed to address the aforementioned\nproblems. The proposed system includes two main components: (1) a compact\none-stage object detector which is expected to be able to perform at a\ncomparable accuracy compared to state-of-the-art object detectors, and (2) an\nenvironmental condition detector that helps to send a warning signal to the\ncloud in case the self-driving car needs human actions due to the significance\nof the situation. The empirical results prove the reliability and the\nscalability of the proposed system to realistic scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:42:43 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tran", "Le-Anh", ""], ["Do", "Truong-Dong", ""], ["Park", "Dong-Chul", ""], ["Le", "My-Ha", ""]]}, {"id": "2105.01595", "submitter": "Hermann Blum", "authors": "Hermann Blum, Francesco Milano, Ren\\'e Zurbr\\\"ugg, Roland Siegward,\n  Cesar Cadena, Abel Gawel", "title": "Self-Improving Semantic Perception on a Construction Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel robotic system that can improve its semantic perception\nduring deployment. Contrary to the established approach of learning semantics\nfrom large datasets and deploying fixed models, we propose a framework in which\nsemantic models are continuously updated on the robot to adapt to the\ndeployment environments. Our system therefore tightly couples multi-sensor\nperception and localisation to continuously learn from self-supervised pseudo\nlabels. We study this system in the context of a construction robot registering\nLiDAR scans of cluttered environments against building models. Our experiments\nshow how the robot's semantic perception improves during deployment and how\nthis translates into improved 3D localisation by filtering the clutter out of\nthe LiDAR scan, even across drastically different environments. We further\nstudy the risk of catastrophic forgetting that such a continuous learning\nsetting poses. We find memory replay an effective measure to reduce forgetting\nand show how the robotic system can improve even when switching between\ndifferent environments. On average, our system improves by 60% in segmentation\nand 10% in localisation compared to deployment of a fixed model, and it keeps\nthis improvement up while adapting to further environments.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:06:12 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Blum", "Hermann", ""], ["Milano", "Francesco", ""], ["Zurbr\u00fcgg", "Ren\u00e9", ""], ["Siegward", "Roland", ""], ["Cadena", "Cesar", ""], ["Gawel", "Abel", ""]]}, {"id": "2105.01601", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas\n  Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas\n  Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey\n  Dosovitskiy", "title": "MLP-Mixer: An all-MLP Architecture for Vision", "comments": "v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in\n  Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the\n  x label in Figure 2(right)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are the go-to model for computer vision.\nRecently, attention-based networks, such as the Vision Transformer, have also\nbecome popular. In this paper we show that while convolutions and attention are\nboth sufficient for good performance, neither of them are necessary. We present\nMLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).\nMLP-Mixer contains two types of layers: one with MLPs applied independently to\nimage patches (i.e. \"mixing\" the per-location features), and one with MLPs\napplied across patches (i.e. \"mixing\" spatial information). When trained on\nlarge datasets, or with modern regularization schemes, MLP-Mixer attains\ncompetitive scores on image classification benchmarks, with pre-training and\ninference cost comparable to state-of-the-art models. We hope that these\nresults spark further research beyond the realms of well established CNNs and\nTransformers.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:17:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 12:48:26 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 09:50:52 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 09:36:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Houlsby", "Neil", ""], ["Kolesnikov", "Alexander", ""], ["Beyer", "Lucas", ""], ["Zhai", "Xiaohua", ""], ["Unterthiner", "Thomas", ""], ["Yung", "Jessica", ""], ["Steiner", "Andreas", ""], ["Keysers", "Daniel", ""], ["Uszkoreit", "Jakob", ""], ["Lucic", "Mario", ""], ["Dosovitskiy", "Alexey", ""]]}, {"id": "2105.01604", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo,\n  Daniel Cohen-Or", "title": "Orienting Point Clouds with Dipole Propagation", "comments": "SIGGRAPH 2021", "journal-ref": null, "doi": "10.1145/3450626.3459835", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a consistent normal orientation for point clouds is a\nnotoriously difficult problem in geometry processing, requiring attention to\nboth local and global shape characteristics. The normal direction of a point is\na function of the local surface neighborhood; yet, point clouds do not disclose\nthe full underlying surface structure. Even assuming known geodesic proximity,\ncalculating a consistent normal orientation requires the global context. In\nthis work, we introduce a novel approach for establishing a globally consistent\nnormal orientation for point clouds. Our solution separates the local and\nglobal components into two different sub-problems. In the local phase, we train\na neural network to learn a coherent normal direction per patch (i.e.,\nconsistently oriented normals within a single patch). In the global phase, we\npropagate the orientation across all coherent patches using a dipole\npropagation. Our dipole propagation decides to orient each patch using the\nelectric field defined by all previously orientated patches. This gives rise to\na global propagation that is stable, as well as being robust to nearby\nsurfaces, holes, sharp features and noise.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:25:36 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Zorin", "Denis", ""], ["Giryes", "Raja", ""], ["Panozzo", "Daniele", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.01605", "submitter": "Xiaojun Chang", "authors": "Xiangtan Lin and Pengzhen Ren and Yun Xiao and Xiaojun Chang and Alex\n  Hauptmann", "title": "Person Search Challenges and Solutions: A Survey", "comments": "8 pages; Accepted by IJCAI 2021 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person search has drawn increasing attention due to its real-world\napplications and research significance. Person search aims to find a probe\nperson in a gallery of scene images with a wide range of applications, such as\ncriminals search, multicamera tracking, missing person search, etc. Early\nperson search works focused on image-based person search, which uses person\nimage as the search query. Text-based person search is another major person\nsearch category that uses free-form natural language as the search query.\nPerson search is challenging, and corresponding solutions are diverse and\ncomplex. Therefore, systematic surveys on this topic are essential. This paper\nsurveyed the recent works on image-based and text-based person search from the\nperspective of challenges and solutions. Specifically, we provide a brief\nanalysis of highly influential person search methods considering the three\nsignificant challenges: the discriminative person features, the query-person\ngap, and the detection-identification inconsistency. We summarise and compare\nevaluation results. Finally, we discuss open issues and some promising future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:10:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lin", "Xiangtan", ""], ["Ren", "Pengzhen", ""], ["Xiao", "Yun", ""], ["Chang", "Xiaojun", ""], ["Hauptmann", "Alex", ""]]}, {"id": "2105.01622", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini", "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n  We study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n  We find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:55:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Carlini", "Nicholas", ""]]}, {"id": "2105.01634", "submitter": "Paulo Correia", "authors": "Pedro Albuquerque, Joao Machado, Tanmay Tulsidas Verlekar, Luis Ducla\n  Soares, Paulo Lobato Correia", "title": "Remote Pathological Gait Classification System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several pathologies can alter the way people walk, i.e. their gait. Gait\nanalysis can therefore be used to detect impairments and help diagnose\nillnesses and assess patient recovery. Using vision-based systems, diagnoses\ncould be done at home or in a clinic, with the needed computation being done\nremotely. State-of-the-art vision-based gait analysis systems use deep\nlearning, requiring large datasets for training. However, to our best\nknowledge, the biggest publicly available pathological gait dataset contains\nonly 10 subjects, simulating 4 gait pathologies. This paper presents a new\ndataset called GAIT-IT, captured from 21 subjects simulating 4 gait\npathologies, with 2 severity levels, besides normal gait, being considerably\nlarger than publicly available gait pathology datasets, allowing to train a\ndeep learning model for gait pathology classification. Moreover, it was\nrecorded in a professional studio, making it possible to obtain nearly perfect\nsilhouettes, free of segmentation errors. Recognizing the importance of remote\nhealthcare, this paper proposes a prototype of a web application allowing to\nupload a walking person's video, possibly acquired using a smartphone camera,\nand execute a web service that classifies the person's gait as normal or across\ndifferent pathologies. The web application has a user friendly interface and\ncould be used by healthcare professionals or other end users. An automatic gait\nanalysis system is also developed and integrated with the web application for\npathology classification. Compared to state-of-the-art solutions, it achieves a\ndrastic reduction in the number of model parameters, which means significantly\nlower memory requirements, as well as lower training and execution times.\nClassification accuracy is on par with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:21:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Albuquerque", "Pedro", ""], ["Machado", "Joao", ""], ["Verlekar", "Tanmay Tulsidas", ""], ["Soares", "Luis Ducla", ""], ["Correia", "Paulo Lobato", ""]]}, {"id": "2105.01646", "submitter": "Kirill Gavrilyuk", "authors": "Kirill Gavrilyuk, Mihir Jain, Ilia Karmanov, Cees G. M. Snoek", "title": "Motion-Augmented Self-Training for Video Recognition at Smaller Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to self-train a 3D convolutional neural network on\nan unlabeled video collection for deployment on small-scale video collections.\nAs smaller video datasets benefit more from motion than appearance, we strive\nto train our network using optical flow, but avoid its computation during\ninference. We propose the first motion-augmented self-training regime, we call\nMotionFit. We start with supervised training of a motion model on a small, and\nlabeled, video collection. With the motion model we generate pseudo-labels for\na large unlabeled video collection, which enables us to transfer knowledge by\nlearning to predict these pseudo-labels with an appearance model. Moreover, we\nintroduce a multi-clip loss as a simple yet efficient way to improve the\nquality of the pseudo-labeling, even without additional auxiliary tasks. We\nalso take into consideration the temporal granularity of videos during\nself-training of the appearance model, which was missed in previous works. As a\nresult we obtain a strong motion-augmented representation model suited for\nvideo downstream tasks like action recognition and clip retrieval. On\nsmall-scale video datasets, MotionFit outperforms alternatives for knowledge\ntransfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised\nlearning by 9%-18% using the same amount of class labels.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:43:19 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Gavrilyuk", "Kirill", ""], ["Jain", "Mihir", ""], ["Karmanov", "Ilia", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2105.01652", "submitter": "Sai Saketh Rambhatla", "authors": "Sai Saketh Rambhatla and Rama Chellappa and Abhinav Shrivastava", "title": "The Pursuit of Knowledge: Discovering and Localizing Novel Categories\n  using Dual Memory", "comments": "Minor changes to table comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle object category discovery, which is the problem of discovering and\nlocalizing novel objects in a large unlabeled dataset. While existing methods\nshow results on datasets with less cluttered scenes and fewer object instances\nper image, we present our results on the challenging COCO dataset. Moreover, we\nargue that, rather than discovering new categories from scratch, discovery\nalgorithms can benefit from identifying what is already known and focusing\ntheir attention on the unknown. We propose a method to use prior knowledge\nabout certain object categories to discover new categories by leveraging two\nmemory modules, namely Working and Semantic memory. We show the performance of\nour detector on the COCO minival dataset to demonstrate its in-the-wild\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:55:59 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 01:23:56 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Rambhatla", "Sai Saketh", ""], ["Chellappa", "Rama", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2105.01688", "submitter": "Anusua Trivedi", "authors": "Anusua Trivedi, Mohit Jain, Nikhil Kumar Gupta, Markus Hinsche,\n  Prashant Singh, Markus Matiaschek, Tristan Behrens, Mirco Militeri, Cameron\n  Birge, Shivangi Kaushik, Archisman Mohapatra, Rita Chatterjee, Rahul Dodhia,\n  Juan Lavista Ferres", "title": "Height Estimation of Children under Five Years using Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malnutrition is a global health crisis and is the leading cause of death\namong children under five. Detecting malnutrition requires anthropometric\nmeasurements of weight, height, and middle-upper arm circumference. However,\nmeasuring them accurately is a challenge, especially in the global south, due\nto limited resources. In this work, we propose a CNN-based approach to estimate\nthe height of standing children under five years from depth images collected\nusing a smart-phone. According to the SMART Methodology Manual [5], the\nacceptable accuracy for height is less than 1.4 cm. On training our deep\nlearning model on 87131 depth images, our model achieved an average mean\nabsolute error of 1.64% on 57064 test images. For 70.3% test images, we\nestimated height accurately within the acceptable 1.4 cm range. Thus, our\nproposed solution can accurately detect stunting (low height-for-age) in\nstanding children below five years of age.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:15:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Trivedi", "Anusua", ""], ["Jain", "Mohit", ""], ["Gupta", "Nikhil Kumar", ""], ["Hinsche", "Markus", ""], ["Singh", "Prashant", ""], ["Matiaschek", "Markus", ""], ["Behrens", "Tristan", ""], ["Militeri", "Mirco", ""], ["Birge", "Cameron", ""], ["Kaushik", "Shivangi", ""], ["Mohapatra", "Archisman", ""], ["Chatterjee", "Rita", ""], ["Dodhia", "Rahul", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2105.01695", "submitter": "Samarth Mishra", "authors": "Samarth Mishra, Zhongping Zhang, Yuan Shen, Ranjitha Kumar, Venkatesh\n  Saligrama, Bryan Plummer", "title": "Effectively Leveraging Attributes for Visual Similarity", "comments": "Accepted to CVPR2021 CVFAD Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring similarity between two images often requires performing complex\nreasoning along different axes (e.g., color, texture, or shape). Insights into\nwhat might be important for measuring similarity can can be provided by\nannotated attributes, but prior work tends to view these annotations as\ncomplete, resulting in them using a simplistic approach of predicting\nattributes on single images, which are, in turn, used to measure similarity.\nHowever, it is impractical for a dataset to fully annotate every attribute that\nmay be important. Thus, only representing images based on these incomplete\nannotations may miss out on key information. To address this issue, we propose\nthe Pairwise Attribute-informed similarity Network (PAN), which breaks\nsimilarity learning into capturing similarity conditions and relevance scores\nfrom a joint representation of two images. This enables our model to identify\nthat two images contain the same attribute, but can have it deemed irrelevant\n(e.g., due to fine-grained differences between them) and ignored for measuring\nsimilarity between the two images. Notably, while prior methods of using\nattribute annotations are often unable to outperform prior art, PAN obtains a\n4-9% improvement on compatibility prediction between clothing items on Polyvore\nOutfits, a 5\\% gain on few shot classification of images using Caltech-UCSD\nBirds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:28:35 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mishra", "Samarth", ""], ["Zhang", "Zhongping", ""], ["Shen", "Yuan", ""], ["Kumar", "Ranjitha", ""], ["Saligrama", "Venkatesh", ""], ["Plummer", "Bryan", ""]]}, {"id": "2105.01705", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc Gorriz Blanch, Issa Khalifeh, Alan Smeaton, Noel O'Connor, Marta\n  Mrak", "title": "Attention-based Stylisation for Exemplar Image Colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exemplar-based colourisation aims to add plausible colours to a grayscale\nimage using the guidance of a colour reference image. Most of the existing\nmethods tackle the task as a style transfer problem, using a convolutional\nneural network (CNN) to obtain deep representations of the content of both\ninputs. Stylised outputs are then obtained by computing similarities between\nboth feature representations in order to transfer the style of the reference to\nthe content of the target input. However, in order to gain robustness towards\ndissimilar references, the stylised outputs need to be refined with a second\ncolourisation network, which significantly increases the overall system\ncomplexity. This work reformulates the existing methodology introducing a novel\nend-to-end colourisation network that unifies the feature matching with the\ncolourisation process. The proposed architecture integrates attention modules\nat different resolutions that learn how to perform the style transfer task in\nan unsupervised way towards decoding realistic colour predictions. Moreover,\naxial attention is proposed to simplify the attention operations and to obtain\na fast but robust cost-effective architecture. Experimental validations\ndemonstrate efficiency of the proposed methodology which generates high quality\nand visual appealing colourisation. Furthermore, the complexity of the proposed\nmethodology is reduced compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:56:26 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Blanch", "Marc Gorriz", ""], ["Khalifeh", "Issa", ""], ["Smeaton", "Alan", ""], ["O'Connor", "Noel", ""], ["Mrak", "Marta", ""]]}, {"id": "2105.01710", "submitter": "Pengcheng Xi", "authors": "Jianxing Zhang, Pengcheng Xi, Ashkan Ebadi, Hilda Azimi, Stephane\n  Tremblay, Alexander Wong", "title": "COVID-19 Detection from Chest X-ray Images using Imprinted Weights\n  Approach", "comments": "Accepted to ICLR 2021 Workshop: Machine Learning for Preventing and\n  Combating Pandemics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has had devastating effects on the well-being of the\nglobal population. The pandemic has been so prominent partly due to the high\ninfection rate of the virus and its variants. In response, one of the most\neffective ways to stop infection is rapid diagnosis. The main-stream screening\nmethod, reverse transcription-polymerase chain reaction (RT-PCR), is\ntime-consuming, laborious and in short supply. Chest radiography is an\nalternative screening method for the COVID-19 and computer-aided diagnosis\n(CAD) has proven to be a viable solution at low cost and with fast speed;\nhowever, one of the challenges in training the CAD models is the limited number\nof training data, especially at the onset of the pandemic. This becomes\noutstanding precisely when the quick and cheap type of diagnosis is critically\nneeded for flattening the infection curve. To address this challenge, we\npropose the use of a low-shot learning approach named imprinted weights, taking\nadvantage of the abundance of samples from known illnesses such as pneumonia to\nimprove the detection performance on COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:01:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhang", "Jianxing", ""], ["Xi", "Pengcheng", ""], ["Ebadi", "Ashkan", ""], ["Azimi", "Hilda", ""], ["Tremblay", "Stephane", ""], ["Wong", "Alexander", ""]]}, {"id": "2105.01713", "submitter": "Weijun Tan", "authors": "Weijun Tan, Hongwei Guo, Rushuai Liu", "title": "A Fast Partial Video Copy Detection Using KNN and Global Feature\n  Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fast partial video copy detection framework in this paper. In\nthis framework all frame features of the reference videos are organized in a\nKNN searchable database. Instead of scanning all reference videos, the query\nvideo segment does a fast KNN search in the global feature database. The\nreturned results are used to generate a short list of candidate videos. A\nmodified temporal network is then used to localize the copy segment in the\ncandidate videos. We evaluate different choice of CNN features on the VCDB\ndataset. Our benchmark F1 score exceeds the state of the art by a big margin.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:03:21 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tan", "Weijun", ""], ["Guo", "Hongwei", ""], ["Liu", "Rushuai", ""]]}, {"id": "2105.01727", "submitter": "Stanislava Fedorova", "authors": "Stanislava Fedorova", "title": "GANs for Urban Design", "comments": "Presented in SimAUD 2021 9 pages, 8 figures Related github\n  repositories: https://github.com/STASYA00/urban_datasets\n  https://github.com/STASYA00/UrbanGen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Development and diffusion of machine learning and big data tools provide a\nnew tool for architects and urban planners that could be used as analytical or\ndesign instruments. The topic investigated in this paper is the application of\nGenerative Adversarial Networks to the design of an urban block. The research\npresents a flexible model able to adapt to the morphological characteristics of\na city. This method does not define explicitly any of the parameters of an\nurban block typical for a city, the algorithm learns them from the existing\nurban context. This approach has been applied to the cities with different\nmorphology: Milan, Amsterdam, Tallinn, Turin, and Bengaluru in order to see the\nperformance of the model and the possibility of style translation between\ndifferent cities. The data are gathered from Openstreetmap and Open Data\nportals of the cities. This research presents the results of the experiments\nand their quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:50:24 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Fedorova", "Stanislava", ""]]}, {"id": "2105.01764", "submitter": "Hao Sheng", "authors": "Hao Sheng, Keniel Yao, Sharad Goel", "title": "Surveilling Surveillance: Estimating the Prevalence of Surveillance\n  Cameras with Street View Data", "comments": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of video surveillance in public spaces -- both by government agencies\nand by private citizens -- has attracted considerable attention in recent\nyears, particularly in light of rapid advances in face-recognition technology.\nBut it has been difficult to systematically measure the prevalence and\nplacement of cameras, hampering efforts to assess the implications of\nsurveillance on privacy and public safety. Here we present a novel approach for\nestimating the spatial distribution of surveillance cameras: applying computer\nvision algorithms to large-scale street view image data. Specifically, we build\na camera detection model and apply it to 1.6 million street view images sampled\nfrom 10 large U.S. cities and 6 other major cities around the world, with\npositive model detections verified by human experts. After adjusting for the\nestimated recall of our model, and accounting for the spatial coverage of our\nsampled images, we are able to estimate the density of surveillance cameras\nvisible from the road. Across the 16 cities we consider, the estimated number\nof surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to\n0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that\ncameras are concentrated in commercial, industrial, and mixed zones, and in\nneighborhoods with higher shares of non-white residents -- a pattern that\npersists even after adjusting for land use. These results help inform ongoing\ndiscussions on the use of surveillance technology, including its potential\ndisparate impacts on communities of color.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:06:01 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Sheng", "Hao", ""], ["Yao", "Keniel", ""], ["Goel", "Sharad", ""]]}, {"id": "2105.01765", "submitter": "Lin Bai", "authors": "Lin Bai, Yiming Zhao and Xinming Huang", "title": "3D Vehicle Detection Using Camera and Low-Resolution LiDAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Light Detection And Ranging (LiDAR) has been widely used in\nautonomous vehicles for perception and localization. However, the cost of a\nhigh-resolution LiDAR is still prohibitively expensive, while its\nlow-resolution counterpart is much more affordable. Therefore, using\nlow-resolution LiDAR for autonomous driving perception tasks instead of\nhigh-resolution LiDAR is an economically feasible solution. In this paper, we\npropose a novel framework for 3D object detection in Bird-Eye View (BEV) using\na low-resolution LiDAR and a monocular camera. Taking the low-resolution LiDAR\npoint cloud and the monocular image as input, our depth completion network is\nable to produce dense point cloud that is subsequently processed by a\nvoxel-based network for 3D object detection. Evaluated with KITTI dataset, the\nexperimental results shows that the proposed approach performs significantly\nbetter than directly applying the 16-line LiDAR point cloud for object\ndetection. For both easy and moderate cases, our detection results are\ncomparable to those from 64-line high-resolution LiDAR. The network\narchitecture and performance evaluations are analyzed in detail.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:08:20 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bai", "Lin", ""], ["Zhao", "Yiming", ""], ["Huang", "Xinming", ""]]}, {"id": "2105.01768", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Texture for Colors: Natural Representations of Colors Using Variable\n  Bit-Depth Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous methods have been proposed to transform color and grayscale images\nto their single bit-per-pixel binary counterparts. Commonly, the goal is to\nenhance specific attributes of the original image to make it more amenable for\nanalysis. However, when the resulting binarized image is intended for human\nviewing, aesthetics must also be considered. Binarization techniques, such as\nhalf-toning, stippling, and hatching, have been widely used for modeling the\noriginal image's intensity profile. We present an automated method to transform\nan image to a set of binary textures that represent not only the intensities,\nbut also the colors of the original. The foundation of our method is\ninformation preservation: creating a set of textures that allows for the\nreconstruction of the original image's colors solely from the binarized\nrepresentation. We present techniques to ensure that the textures created are\nnot visually distracting, preserve the intensity profile of the images, and are\nnatural in that they map sets of colors that are perceptually similar to\npatterns that are similar. The approach uses deep-neural networks and is\nentirely self-supervised; no examples of good vs. bad binarizations are\nrequired. The system yields aesthetically pleasing binary images when tested on\na variety of image sources.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:22:02 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "2105.01793", "submitter": "David Jones", "authors": "David Jones, Nathan Jacobs", "title": "Intensity Harmonization for Airborne LiDAR", "comments": "IGARSS 2021. Project website with video and code at\n  https://davidthomasjones.me/publications/airborne-lidar-intensity-harmonization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constructing a point cloud for a large geographic region, such as a state or\ncountry, can require multiple years of effort. Often several vendors will be\nused to acquire LiDAR data, and a single region may be captured by multiple\nLiDAR scans. A key challenge is maintaining consistency between these scans,\nwhich includes point density, number of returns, and intensity. Intensity in\nparticular can be very different between scans, even in areas that are\noverlapping. Harmonizing the intensity between scans to remove these\ndiscrepancies is expensive and time consuming. In this paper, we propose a\nnovel method for point cloud harmonization based on deep neural networks. We\nevaluate our method quantitatively and qualitatively using a high quality real\nworld LiDAR dataset. We compare our method to several baselines, including\nstandard interpolation methods as well as histogram matching. We show that our\nmethod performs as well as the best baseline in areas with similar intensity\ndistributions, and outperforms all baselines in areas with different intensity\ndistributions. Source code is available at\nhttps://github.com/mvrl/lidar-harmonization .\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:26:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Jones", "David", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2105.01794", "submitter": "Marc Habermann", "authors": "Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard\n  Pons-Moll, Christian Theobalt", "title": "Real-time Deep Dynamic Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a deep videorealistic 3D human character model displaying highly\nrealistic shape, motion, and dynamic appearance learned in a new weakly\nsupervised way from multi-view imagery. In contrast to previous work, our\ncontrollable 3D character displays dynamics, e.g., the swing of the skirt,\ndependent on skeletal body motion in an efficient data-driven way, without\nrequiring complex physics simulation. Our character model also features a\nlearned dynamic texture model that accounts for photo-realistic\nmotion-dependent appearance details, as well as view-dependent lighting\neffects. During training, we do not need to resort to difficult dynamic 3D\ncapture of the human; instead we can train our model entirely from multi-view\nvideo in a weakly supervised manner. To this end, we propose a parametric and\ndifferentiable character representation which allows us to model coarse and\nfine dynamic deformations, e.g., garment wrinkles, as explicit space-time\ncoherent mesh geometry that is augmented with high-quality dynamic textures\ndependent on motion and view point. As input to the model, only an arbitrary 3D\nskeleton motion is required, making it directly compatible with the established\n3D animation pipeline. We use a novel graph convolutional network architecture\nto enable motion-dependent deformation learning of body and clothing, including\ndynamics, and a neural generative dynamic texture model creates corresponding\ndynamic texture maps. We show that by merely providing new skeletal motions,\nour model creates motion-dependent surface deformations, physically plausible\ndynamic clothing deformations, as well as video-realistic surface textures at a\nmuch higher level of detail than previous state of the art approaches, and even\nin real-time.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:28:55 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Habermann", "Marc", ""], ["Liu", "Lingjie", ""], ["Xu", "Weipeng", ""], ["Zollhoefer", "Michael", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "2105.01800", "submitter": "Guang Yang A", "authors": "Guang Yang, Jun Lv, Yutong Chen, Jiahao Huang, Jin Zhu", "title": "Generative Adversarial Networks (GAN) Powered Fast Magnetic Resonance\n  Imaging -- Mini Review, Comparison and Perspectives", "comments": "18 figures, Generative Adversarial Learning: Architectures and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a vital component of medical imaging.\nWhen compared to other image modalities, it has advantages such as the absence\nof radiation, superior soft tissue contrast, and complementary multiple\nsequence information. However, one drawback of MRI is its comparatively slow\nscanning and reconstruction compared to other image modalities, limiting its\nusage in some clinical applications when imaging time is critical. Traditional\ncompressive sensing based MRI (CS-MRI) reconstruction can speed up MRI\nacquisition, but suffers from a long iterative process and noise-induced\nartefacts. Recently, Deep Neural Networks (DNNs) have been used in sparse MRI\nreconstruction models to recreate relatively high-quality images from heavily\nundersampled k-space data, allowing for much faster MRI scanning. However,\nthere are still some hurdles to tackle. For example, directly training DNNs\nbased on L1/L2 distance to the target fully sampled images could result in\nblurry reconstruction because L1/L2 loss can only enforce overall image or\npatch similarity and does not take into account local information such as\nanatomical sharpness. It is also hard to preserve fine image details while\nmaintaining a natural appearance. More recently, Generative Adversarial\nNetworks (GAN) based methods are proposed to solve fast MRI with enhanced image\nperceptual quality. The encoder obtains a latent space for the undersampling\nimage, and the image is reconstructed by the decoder using the GAN loss. In\nthis chapter, we review the GAN powered fast MRI methods with a comparative\nstudy on various anatomical datasets to demonstrate the generalisability and\nrobustness of this kind of fast MRI while providing future perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:59:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Guang", ""], ["Lv", "Jun", ""], ["Chen", "Yutong", ""], ["Huang", "Jiahao", ""], ["Zhu", "Jin", ""]]}, {"id": "2105.01803", "submitter": "Zhe Yang", "authors": "Zhe Yang, Klara Nahrstedt, Hongpeng Guo, Qian Zhou", "title": "DeepRT: A Soft Real Time Scheduler for Computer Vision Applications on\n  the Edge", "comments": "Accepted by the Sixth ACM/IEEE Symposium on Edge Computing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of smartphone cameras and IoT cameras, together with the recent\nboom of deep learning and deep neural networks, proliferate various computer\nvision driven mobile and IoT applications deployed on the edge. This paper\nfocuses on applications which make soft real time requests to perform inference\non their data - they desire prompt responses within designated deadlines, but\noccasional deadline misses are acceptable. Supporting soft real time\napplications on a multi-tenant edge server is not easy, since the requests\nsharing the limited GPU computing resources of an edge server interfere with\neach other. In order to tackle this problem, we comprehensively evaluate how\nlatency and throughput respond to different GPU execution plans. Based on this\nanalysis, we propose a GPU scheduler, DeepRT, which provides latency guarantee\nto the requests while maintaining high overall system throughput. The key\ncomponent of DeepRT, DisBatcher, batches data from different requests as much\nas possible while it is proven to provide latency guarantee for requests\nadmitted by an Admission Control Module. DeepRT also includes an Adaptation\nModule which tackles overruns. Our evaluation results show that DeepRT\noutperforms state-of-the-art works in terms of the number of deadline misses\nand throughput.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:08:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Zhe", ""], ["Nahrstedt", "Klara", ""], ["Guo", "Hongpeng", ""], ["Zhou", "Qian", ""]]}, {"id": "2105.01816", "submitter": "Zichen Li", "authors": "Yuchen Ding, Zichen Li, David Yastremsky", "title": "Real-time Face Mask Detection in Video Data", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the ongoing COVID-19 pandemic, we present a robust deep\nlearning pipeline that is capable of identifying correct and incorrect\nmask-wearing from real-time video streams. To accomplish this goal, we devised\ntwo separate approaches and evaluated their performance and run-time\nefficiency. The first approach leverages a pre-trained face detector in\ncombination with a mask-wearing image classifier trained on a large-scale\nsynthetic dataset. The second approach utilizes a state-of-the-art object\ndetection network to perform localization and classification of faces in one\nshot, fine-tuned on a small set of labeled real-world images. The first\npipeline achieved a test accuracy of 99.97% on the synthetic dataset and\nmaintained 6 FPS running on video data. The second pipeline achieved a mAP(0.5)\nof 89% on real-world images while sustaining 52 FPS on video data. We have\nconcluded that if a larger dataset with bounding-box labels can be curated,\nthis task is best suited using object detection architectures such as YOLO and\nSSD due to their superior inference speed and satisfactory performance on key\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:03:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ding", "Yuchen", ""], ["Li", "Zichen", ""], ["Yastremsky", "David", ""]]}, {"id": "2105.01823", "submitter": "Yongbiao Chen", "authors": "Yongbiao Chen (1), Sheng Zhang (2), Fangxin Liu (1), Zhigang Chang\n  (1), Mang Ye (3), Zhengwei Qi (1) ((1) Shanghai Jiao Tong University, (2)\n  University of Southern California, (3) Wuhan University)", "title": "TransHash: Transformer-based Hamming Hashing for Efficient Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep hamming hashing has gained growing popularity in approximate nearest\nneighbour search for large-scale image retrieval. Until now, the deep hashing\nfor the image retrieval community has been dominated by convolutional neural\nnetwork architectures, e.g. \\texttt{Resnet}\\cite{he2016deep}. In this paper,\ninspired by the recent advancements of vision transformers, we present\n\\textbf{Transhash}, a pure transformer-based framework for deep hashing\nlearning. Concretely, our framework is composed of two major modules: (1) Based\non \\textit{Vision Transformer} (ViT), we design a siamese vision transformer\nbackbone for image feature extraction. To learn fine-grained features, we\ninnovate a dual-stream feature learning on top of the transformer to learn\ndiscriminative global and local features. (2) Besides, we adopt a Bayesian\nlearning scheme with a dynamically constructed similarity matrix to learn\ncompact binary hash codes. The entire framework is jointly trained in an\nend-to-end manner.~To the best of our knowledge, this is the first work to\ntackle deep hashing learning problems without convolutional neural networks\n(\\textit{CNNs}). We perform comprehensive experiments on three widely-studied\ndatasets: \\textbf{CIFAR-10}, \\textbf{NUSWIDE} and \\textbf{IMAGENET}. The\nexperiments have evidenced our superiority against the existing\nstate-of-the-art deep hashing methods. Specifically, we achieve 8.2\\%, 2.6\\%,\n12.7\\% performance gains in terms of average \\textit{mAP} for different hash\nbit lengths on three public datasets, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:35:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Yongbiao", ""], ["Zhang", "Sheng", ""], ["Liu", "Fangxin", ""], ["Chang", "Zhigang", ""], ["Ye", "Mang", ""], ["Qi", "Zhengwei", ""]]}, {"id": "2105.01828", "submitter": "Youbao Tang", "authors": "Youbao Tang, Ke Yan, Jinzheng Cai, Lingyun Huang, Guotong Xie, Jing\n  Xiao, Jingjing Lu, Gigin Lin, and Le Lu", "title": "Lesion Segmentation and RECIST Diameter Prediction via Click-driven\n  Attention and Dual-path Connection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring lesion size is an important step to assess tumor growth and monitor\ndisease progression and therapy response in oncology image analysis. Although\nit is tedious and highly time-consuming, radiologists have to work on this task\nby using RECIST criteria (Response Evaluation Criteria In Solid Tumors)\nroutinely and manually. Even though lesion segmentation may be the more\naccurate and clinically more valuable means, physicians can not manually\nsegment lesions as now since much more heavy laboring will be required. In this\npaper, we present a prior-guided dual-path network (PDNet) to segment common\ntypes of lesions throughout the whole body and predict their RECIST diameters\naccurately and automatically. Similar to [1], a click guidance from\nradiologists is the only requirement. There are two key characteristics in\nPDNet: 1) Learning lesion-specific attention matrices in parallel from the\nclick prior information by the proposed prior encoder, named click-driven\nattention; 2) Aggregating the extracted multi-scale features comprehensively by\nintroducing top-down and bottom-up connections in the proposed decoder, named\ndual-path connection. Experiments show the superiority of our proposed PDNet in\nlesion segmentation and RECIST diameter prediction using the DeepLesion dataset\nand an external test set. PDNet learns comprehensive and representative deep\nimage features for our tasks and produces more accurate results on both lesion\nsegmentation and RECIST diameter prediction.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:00:14 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tang", "Youbao", ""], ["Yan", "Ke", ""], ["Cai", "Jinzheng", ""], ["Huang", "Lingyun", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Lu", "Jingjing", ""], ["Lin", "Gigin", ""], ["Lu", "Le", ""]]}, {"id": "2105.01834", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "Curvatures of Stiefel manifolds with deformation metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CV cs.LG cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute curvatures of a family of tractable metrics on Stiefel manifolds,\nintroduced recently by H{\\\"u}per, Markina and Silva Leite, which includes the\nwell-known embedded and canonical metrics on Stiefel manifolds as special\ncases. The metrics could be identified with the Cheeger deformation metrics. We\nidentify parameter values in the family to make a Stiefel manifold an Einstein\nmanifold and show Stiefel manifolds always carry an Einstein metric. We analyze\nthe sectional curvature range and identify the parameter range where the\nmanifold has non-negative sectional curvature. We provide the exact sectional\ncurvature range when the number of columns in a Stiefel matrix is $2$, and a\nconjectural range for other cases. We derive the formulas from two approaches,\none from a global curvature formula derived in our recent work, another using\ncurvature formulas for left-invariant metrics. The second approach leads to\ncurvature formulas for Cheeger deformation metrics on normal homogeneous\nspaces.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:13:38 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "2105.01839", "submitter": "Guang Feng", "authors": "Guang Feng, Zhiwei Hu, Lihe Zhang, Huchuan Lu", "title": "Encoder Fusion Network with Co-Attention Embedding for Referring Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, referring image segmentation has aroused widespread interest.\nPrevious methods perform the multi-modal fusion between language and vision at\nthe decoding side of the network. And, linguistic feature interacts with visual\nfeature of each scale separately, which ignores the continuous guidance of\nlanguage to multi-scale visual features. In this work, we propose an encoder\nfusion network (EFN), which transforms the visual encoder into a multi-modal\nfeature learning network, and uses language to refine the multi-modal features\nprogressively. Moreover, a co-attention mechanism is embedded in the EFN to\nrealize the parallel update of multi-modal features, which can promote the\nconsistent of the cross-modal information representation in the semantic space.\nFinally, we propose a boundary enhancement module (BEM) to make the network pay\nmore attention to the fine structure. The experiment results on four benchmark\ndatasets demonstrate that the proposed approach achieves the state-of-the-art\nperformance under different evaluation metrics without any post-processing.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:27:25 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Feng", "Guang", ""], ["Hu", "Zhiwei", ""], ["Zhang", "Lihe", ""], ["Lu", "Huchuan", ""]]}, {"id": "2105.01840", "submitter": "Chi-Shiang Wang", "authors": "Chi-Shiang Wang, Fang-Yi Su, Tsung-Lu Michael Lee, Yi-Shan Tsai,\n  Jung-Hsien Chiang", "title": "CUAB: Convolutional Uncertainty Attention Block Enhanced the Chest X-ray\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have been successfully\nimplemented to various image recognition applications, such as medical image\nanalysis, object detection, and image segmentation. Many studies and\napplications have been working on improving the performance of CNN algorithms\nand models. The strategies that aim to improve the performance of CNNs can be\ngrouped into three major approaches: (1) deeper and wider network architecture,\n(2) automatic architecture search, and (3) convolutional attention block.\nUnlike approaches (1) and (2), the convolutional attention block approach is\nmore flexible with lower cost. It enhances the CNN performance by extracting\nmore efficient features. However, the existing attention blocks focus on\nenhancing the significant features, which lose some potential features in the\nuncertainty information. Inspired by the test time augmentation and test-time\ndropout approaches, we developed a novel convolutional uncertainty attention\nblock (CUAB) that can leverage the uncertainty information to improve CNN-based\nmodels. The proposed module discovers potential information from the uncertain\nregions on feature maps in computer vision tasks. It is a flexible functional\nattention block that can be applied to any position in the convolutional block\nin CNN models. We evaluated the CUAB with notable backbone models, ResNet and\nResNeXt, on a medical image segmentation task. The CUAB achieved a dice score\nof 73% and 84% in pneumonia and pneumothorax segmentation, respectively,\nthereby outperforming the original model and other notable attention\napproaches. The results demonstrated that the CUAB can efficiently utilize the\nuncertainty information to improve the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:28:04 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wang", "Chi-Shiang", ""], ["Su", "Fang-Yi", ""], ["Lee", "Tsung-Lu Michael", ""], ["Tsai", "Yi-Shan", ""], ["Chiang", "Jung-Hsien", ""]]}, {"id": "2105.01844", "submitter": "Mohamed Elmahdy", "authors": "Mohamed S. Elmahdy, Laurens Beljaards, Sahar Yousefi, Hessam Sokooti,\n  Fons Verbeek, U. A. van der Heide, and Marius Staring", "title": "Joint Registration and Segmentation via Multi-Task Learning for Adaptive\n  Radiotherapy of Prostate Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image registration and segmentation are two of the most frequent\ntasks in medical image analysis. As these tasks are complementary and\ncorrelated, it would be beneficial to apply them simultaneously in a joint\nmanner. In this paper, we formulate registration and segmentation as a joint\nproblem via a Multi-Task Learning (MTL) setting, allowing these tasks to\nleverage their strengths and mitigate their weaknesses through the sharing of\nbeneficial information. We propose to merge these tasks not only on the loss\nlevel, but on the architectural level as well. We studied this approach in the\ncontext of adaptive image-guided radiotherapy for prostate cancer, where\nplanning and follow-up CT images as well as their corresponding contours are\navailable for training. The study involves two datasets from different\nmanufacturers and institutes. The first dataset was divided into training (12\npatients) and validation (6 patients), and was used to optimize and validate\nthe methodology, while the second dataset (14 patients) was used as an\nindependent test set. We carried out an extensive quantitative comparison\nbetween the quality of the automatically generated contours from different\nnetwork architectures as well as loss weighting methods. Moreover, we evaluated\nthe quality of the generated deformation vector field (DVF). We show that MTL\nalgorithms outperform their Single-Task Learning (STL) counterparts and achieve\nbetter generalization on the independent test set. The best algorithm achieved\na mean surface distance of $1.06 \\pm 0.3$ mm, $1.27 \\pm 0.4$ mm, $0.91 \\pm 0.4$\nmm, and $1.76 \\pm 0.8$ mm on the validation set for the prostate, seminal\nvesicles, bladder, and rectum, respectively. The high accuracy of the proposed\nmethod combined with the fast inference speed, makes it a promising method for\nautomatic re-contouring of follow-up scans for adaptive radiotherapy.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:45:49 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Elmahdy", "Mohamed S.", ""], ["Beljaards", "Laurens", ""], ["Yousefi", "Sahar", ""], ["Sokooti", "Hessam", ""], ["Verbeek", "Fons", ""], ["van der Heide", "U. A.", ""], ["Staring", "Marius", ""]]}, {"id": "2105.01846", "submitter": "Xianbiao Qi", "authors": "Yelin He and Xianbiao Qi and Jiaquan Ye and Peng Gao and Yihao Chen\n  and Bingcong Li and Xin Tang and Rong Xiao", "title": "PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Table\n  Image Recognition to Latex", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our solution for the ICDAR 2021 Competition on Scientific\nTable Image Recognition to LaTeX. This competition has two sub-tasks: Table\nStructure Reconstruction (TSR) and Table Content Reconstruction (TCR). We treat\nboth sub-tasks as two individual image-to-sequence recognition problems. We\nleverage our previously proposed algorithm MASTER \\cite{lu2019master}, which is\noriginally proposed for scene text recognition. We optimize the MASTER model\nfrom several perspectives: network structure, optimizer, normalization method,\npre-trained model, resolution of input image, data augmentation, and model\nensemble. Our method achieves 0.7444 Exact Match and 0.8765 Exact Match @95\\%\non the TSR task, and obtains 0.5586 Exact Match and 0.7386 Exact Match 95\\% on\nthe TCR task.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 03:15:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["He", "Yelin", ""], ["Qi", "Xianbiao", ""], ["Ye", "Jiaquan", ""], ["Gao", "Peng", ""], ["Chen", "Yihao", ""], ["Li", "Bingcong", ""], ["Tang", "Xin", ""], ["Xiao", "Rong", ""]]}, {"id": "2105.01848", "submitter": "Xianbiao Qi", "authors": "Jiaquan Ye and Xianbiao Qi and Yelin He and Yihao Chen and Dengyi Gu\n  and Peng Gao and Rong Xiao", "title": "PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific\n  Literature Parsing Task B: Table Recognition to HTML", "comments": "8 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our solution for ICDAR 2021 competition on scientific\nliterature parsing taskB: table recognition to HTML. In our method, we divide\nthe table content recognition task into foursub-tasks: table structure\nrecognition, text line detection, text line recognition, and box assignment.Our\ntable structure recognition algorithm is customized based on MASTER [1], a\nrobust image textrecognition algorithm. PSENet [2] is used to detect each text\nline in the table image. For text linerecognition, our model is also built on\nMASTER. Finally, in the box assignment phase, we associatedthe text boxes\ndetected by PSENet with the structure item reconstructed by table structure\nprediction,and fill the recognized content of the text line into the\ncorresponding item. Our proposed methodachieves a 96.84% TEDS score on 9,115\nvalidation samples in the development phase, and a 96.32%TEDS score on 9,064\nsamples in the final evaluation phase.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 03:20:26 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ye", "Jiaquan", ""], ["Qi", "Xianbiao", ""], ["He", "Yelin", ""], ["Chen", "Yihao", ""], ["Gu", "Dengyi", ""], ["Gao", "Peng", ""], ["Xiao", "Rong", ""]]}, {"id": "2105.01859", "submitter": "Tao Yu", "authors": "Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, Yebin\n  Liu", "title": "Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer\n  RGBD Sensors", "comments": "CVPR 2021 Oral Paper, Project Page:\n  http://www.liuyebin.com/Function4D/Function4D.html, THuman2.0 dataset\n  available. Youtube: https://www.youtube.com/watch?v=-rWUn4fEQNU&t=126s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human volumetric capture is a long-standing topic in computer vision and\ncomputer graphics. Although high-quality results can be achieved using\nsophisticated off-line systems, real-time human volumetric capture of complex\nscenarios, especially using light-weight setups, remains challenging. In this\npaper, we propose a human volumetric capture method that combines temporal\nvolumetric fusion and deep implicit functions. To achieve high-quality and\ntemporal-continuous reconstruction, we propose dynamic sliding fusion to fuse\nneighboring depth observations together with topology consistency. Moreover,\nfor detailed and complete surface generation, we propose detail-preserving deep\nimplicit functions for RGBD input which can not only preserve the geometric\ndetails on the depth inputs but also generate more plausible texturing results.\nResults and experiments show that our method outperforms existing methods in\nterms of view sparsity, generalization capacity, reconstruction quality, and\nrun-time efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 04:12:38 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 07:38:27 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yu", "Tao", ""], ["Zheng", "Zerong", ""], ["Guo", "Kaiwen", ""], ["Liu", "Pengpeng", ""], ["Dai", "Qionghai", ""], ["Liu", "Yebin", ""]]}, {"id": "2105.01879", "submitter": "Rui Huang", "authors": "Rui Huang and Yixuan Li", "title": "MOS: Towards Scaling Out-of-distribution Detection for Large Semantic\n  Space", "comments": "Paper accepted as an oral presentation in CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting out-of-distribution (OOD) inputs is a central challenge for safely\ndeploying machine learning models in the real world. Existing solutions are\nmainly driven by small datasets, with low resolution and very few class labels\n(e.g., CIFAR). As a result, OOD detection for large-scale image classification\ntasks remains largely unexplored. In this paper, we bridge this critical gap by\nproposing a group-based OOD detection framework, along with a novel OOD scoring\nfunction termed MOS. Our key idea is to decompose the large semantic space into\nsmaller groups with similar concepts, which allows simplifying the decision\nboundaries between in- vs. out-of-distribution data for effective OOD\ndetection. Our method scales substantially better for high-dimensional class\nspace than previous approaches. We evaluate models trained on ImageNet against\nfour carefully curated OOD datasets, spanning diverse semantics. MOS\nestablishes state-of-the-art performance, reducing the average FPR95 by 14.33%\nwhile achieving 6x speedup in inference compared to the previous best method.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:58:29 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Huang", "Rui", ""], ["Li", "Yixuan", ""]]}, {"id": "2105.01882", "submitter": "Gautam Tata", "authors": "Gautam Tata, Sarah-Jeanne Royer, Olivier Poirion and Jay Lowe", "title": "DeepPlastic: A Novel Approach to Detecting Epipelagic Bound Plastic\n  Using Deep Visual Models", "comments": "8 Pages, 6 Figures, 2 Tables - Added Paragraph for Code Availability\n  - Submitted preprint to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quantification of positively buoyant marine plastic debris is critical to\nunderstanding how concentrations of trash from across the world's ocean and\nidentifying high concentration garbage hotspots in dire need of trash removal.\nCurrently, the most common monitoring method to quantify floating plastic\nrequires the use of a manta trawl. Techniques requiring manta trawls (or\nsimilar surface collection devices) utilize physical removal of marine plastic\ndebris as the first step and then analyze collected samples as a second step.\nThe need for physical removal before analysis incurs high costs and requires\nintensive labor preventing scalable deployment of a real-time marine plastic\nmonitoring service across the entirety of Earth's ocean bodies. Without better\nmonitoring and sampling methods, the total impact of plastic pollution on the\nenvironment as a whole, and details of impact within specific oceanic regions,\nwill remain unknown. This study presents a highly scalable workflow that\nutilizes images captured within the epipelagic layer of the ocean as an input.\nIt produces real-time quantification of marine plastic debris for accurate\nquantification and physical removal. The workflow includes creating and\npreprocessing a domain-specific dataset, building an object detection model\nutilizing a deep neural network, and evaluating the model's performance.\nYOLOv5-S was the best performing model, which operates at a Mean Average\nPrecision (mAP) of 0.851 and an F1-Score of 0.89 while maintaining\nnear-real-time speed.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:04:26 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 21:54:01 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 06:50:10 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tata", "Gautam", ""], ["Royer", "Sarah-Jeanne", ""], ["Poirion", "Olivier", ""], ["Lowe", "Jay", ""]]}, {"id": "2105.01883", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding", "title": "RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for\n  Image Recognition", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RepMLP, a multi-layer-perceptron-style neural network building\nblock for image recognition, which is composed of a series of fully-connected\n(FC) layers. Compared to convolutional layers, FC layers are more efficient,\nbetter at modeling the long-range dependencies and positional patterns, but\nworse at capturing the local structures, hence usually less favored for image\nrecognition. We propose a structural re-parameterization technique that adds\nlocal prior into an FC to make it powerful for image recognition. Specifically,\nwe construct convolutional layers inside a RepMLP during training and merge\nthem into the FC for inference. On CIFAR, a simple pure-MLP model shows\nperformance very close to CNN. By inserting RepMLP in traditional CNN, we\nimprove ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and\n2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight\nthat combining the global representational capacity and positional perception\nof FC with the local prior of convolution can improve the performance of neural\nnetwork with faster speed on both the tasks with translation invariance (e.g.,\nsemantic segmentation) and those with aligned images and positional patterns\n(e.g., face recognition). The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:17:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ding", "Xiaohan", ""], ["Zhang", "Xiangyu", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""]]}, {"id": "2105.01899", "submitter": "Tsung Wei Tsai", "authors": "Tsung Wei Tsai, Chongxuan Li, Jun Zhu", "title": "MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering", "comments": "International Conference on Learning Representations (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Mixture of Contrastive Experts (MiCE), a unified probabilistic\nclustering framework that simultaneously exploits the discriminative\nrepresentations learned by contrastive learning and the semantic structures\ncaptured by a latent mixture model. Motivated by the mixture of experts, MiCE\nemploys a gating function to partition an unlabeled dataset into subsets\naccording to the latent semantics and multiple experts to discriminate distinct\nsubsets of instances assigned to them in a contrastive learning manner. To\nsolve the nontrivial inference and learning problems caused by the latent\nvariables, we further develop a scalable variant of the\nExpectation-Maximization (EM) algorithm for MiCE and provide proof of the\nconvergence. Empirically, we evaluate the clustering performance of MiCE on\nfour widely adopted natural image datasets. MiCE achieves significantly better\nresults than various previous methods and a strong contrastive learning\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:17:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tsai", "Tsung Wei", ""], ["Li", "Chongxuan", ""], ["Zhu", "Jun", ""]]}, {"id": "2105.01905", "submitter": "Yang Li", "authors": "Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, Matthias\n  Nie{\\ss}ner", "title": "4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface", "comments": "Data: https://github.com/rabbityl/DeformingThings4D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking non-rigidly deforming scenes using range sensors has numerous\napplications including computer vision, AR/VR, and robotics. However, due to\nocclusions and physical limitations of range sensors, existing methods only\nhandle the visible surface, thus causing discontinuities and incompleteness in\nthe motion field. To this end, we introduce 4DComplete, a novel data-driven\napproach that estimates the non-rigid motion for the unobserved geometry.\n4DComplete takes as input a partial shape and motion observation, extracts 4D\ntime-space embedding, and jointly infers the missing geometry and motion field\nusing a sparse fully-convolutional network. For network training, we\nconstructed a large-scale synthetic dataset called DeformingThings4D, which\nconsists of 1972 animation sequences spanning 31 different animals or humanoid\ncategories with dense 4D annotation. Experiments show that 4DComplete 1)\nreconstructs high-resolution volumetric shape and motion field from a partial\nobservation, 2) learns an entangled 4D feature representation that benefits\nboth shape and motion estimation, 3) yields more accurate and natural\ndeformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP)\ndeformation, and 4) generalizes well to unseen objects in real-world sequences.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:39:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Li", "Yang", ""], ["Takehara", "Hikari", ""], ["Taketomi", "Takafumi", ""], ["Zheng", "Bo", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2105.01919", "submitter": "Puzuo Wang", "authors": "Puzuo Wang, Wei Yao", "title": "Weakly Supervised Pseudo-Label assisted Learning for ALS Point Cloud\n  Semantic Segmentation", "comments": "accepted for publication in the ISPRS Annals of the Photogrammetry,\n  Remote Sensing and Spatial Information Sciences (online from July 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Competitive point cloud semantic segmentation results usually rely on a large\namount of labeled data. However, data annotation is a time-consuming and\nlabor-intensive task, particularly for three-dimensional point cloud data.\nThus, obtaining accurate results with limited ground truth as training data is\nconsiderably important. As a simple and effective method, pseudo labels can use\ninformation from unlabeled data for training neural networks. In this study, we\npropose a pseudo-label-assisted point cloud segmentation method with very few\nsparsely sampled labels that are normally randomly selected for each class. An\nadaptive thresholding strategy was proposed to generate a pseudo-label based on\nthe prediction probability. Pseudo-label learning is an iterative process, and\npseudo labels were updated solely on ground-truth weak labels as the model\nconverged to improve the training efficiency. Experiments using the ISPRS 3D\nsematic labeling benchmark dataset indicated that our proposed method achieved\nan equally competitive result compared to that using a full supervision scheme\nwith only up to 2$\\unicode{x2030}$ of labeled points from the original training\nset, with an overall accuracy of 83.7% and an average F1 score of 70.2%.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:07:21 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wang", "Puzuo", ""], ["Yao", "Wei", ""]]}, {"id": "2105.01922", "submitter": "Leon Amadeus Varga", "authors": "Leon Amadeus Varga, Benjamin Kiefer, Martin Messmer and Andreas Zell", "title": "SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water", "comments": "Leon Amadeus Varga, Benjamin Kiefer, Martin Messmer contributed\n  equally to this work. The order of names is determined by coin flipping.\n  Submitted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) are of crucial importance in search and\nrescue missions in maritime environments due to their flexible and fast\noperation capabilities. Modern computer vision algorithms are of great interest\nin aiding such missions. However, they are dependent on large amounts of\nreal-case training data from UAVs, which is only available for traffic\nscenarios on land. Moreover, current object detection and tracking data sets\nonly provide limited environmental information or none at all, neglecting a\nvaluable source of information. Therefore, this paper introduces a large-scaled\nvisual object detection and tracking benchmark (SeaDronesSee) aiming to bridge\nthe gap from land-based vision systems to sea-based ones. We collect and\nannotate over 54,000 frames with 400,000 instances captured from various\naltitudes and viewing angles ranging from 5 to 260 meters and 0 to 90 degrees\nwhile providing the respective meta information for altitude, viewing angle and\nother meta data. We evaluate multiple state-of-the-art computer vision\nalgorithms on this newly established benchmark serving as baselines. We provide\nan evaluation server where researchers can upload their prediction and compare\ntheir results on a central leaderboard\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:18:36 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Varga", "Leon Amadeus", ""], ["Kiefer", "Benjamin", ""], ["Messmer", "Martin", ""], ["Zell", "Andreas", ""]]}, {"id": "2105.01924", "submitter": "Jonas Wurst", "authors": "Jonas Wurst, Lakshman Balasubramanian, Michael Botsch and Wolfgang\n  Utschick", "title": "Novelty Detection and Analysis of Traffic Scenario Infrastructures in\n  the Latent Space of a Vision Transformer-Based Triplet Autoencoder", "comments": "Accepted for IEEE Intelligent Vehicles 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting unknown and untested scenarios is crucial for scenario-based\ntesting. Scenario-based testing is considered to be a possible approach to\nvalidate autonomous vehicles. A traffic scenario consists of multiple\ncomponents, with infrastructure being one of it. In this work, a method to\ndetect novel traffic scenarios based on their infrastructure images is\npresented. An autoencoder triplet network provides latent representations for\ninfrastructure images which are used for outlier detection. The triplet\ntraining of the network is based on the connectivity graphs of the\ninfrastructure. By using the proposed architecture, expert-knowledge is used to\nshape the latent space such that it incorporates a pre-defined similarity in\nthe neighborhood relationships of an autoencoder. An ablation study on the\narchitecture is highlighting the importance of the triplet autoencoder\ncombination. The best performing architecture is based on vision transformers,\na convolution-free attention-based network. The presented method outperforms\nother state-of-the-art outlier detection approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:24:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wurst", "Jonas", ""], ["Balasubramanian", "Lakshman", ""], ["Botsch", "Michael", ""], ["Utschick", "Wolfgang", ""]]}, {"id": "2105.01928", "submitter": "Yuxin Fang", "authors": "Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan,\n  Bin Feng, Wenyu Liu", "title": "Instances as Queries", "comments": "14 pages, 8 figures, including the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, query based object detection frameworks achieve comparable\nperformance with previous state-of-the-art object detectors. However, how to\nfully leverage such frameworks to perform instance segmentation remains an open\nproblem. In this paper, we present QueryInst (Instances as Queries), a query\nbased instance segmentation method driven by parallel supervision on dynamic\nmask heads. The key insight of QueryInst is to leverage the intrinsic\none-to-one correspondence in object queries across different stages, as well as\none-to-one correspondence between mask RoI features and object queries in the\nsame stage. This approach eliminates the explicit multi-stage mask head\nconnection and the proposal distribution inconsistency issues inherent in\nnon-query based multi-stage instance segmentation methods. We conduct extensive\nexperiments on three challenging benchmarks, i.e., COCO, CityScapes, and\nYouTube-VIS to evaluate the effectiveness of QueryInst in instance segmentation\nand video instance segmentation (VIS) task. Specifically, using ResNet-101-FPN\nbackbone, QueryInst obtains 48.1 box AP and 42.8 mask AP on COCO test-dev,\nwhich is 2 points higher than HTC in terms of both box AP and mask AP, while\nruns 2.4 times faster. For video instance segmentation, QueryInst achieves the\nbest performance among all online VIS approaches and strikes a decent\nspeed-accuracy trade-off. Code is available at\n\\url{https://github.com/hustvl/QueryInst}.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:38:25 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 16:46:21 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 16:36:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Fang", "Yuxin", ""], ["Yang", "Shusheng", ""], ["Wang", "Xinggang", ""], ["Li", "Yu", ""], ["Fang", "Chen", ""], ["Shan", "Ying", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2105.01937", "submitter": "Brian Gordon", "authors": "Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, Daniel Cohen-Or", "title": "FLEX: Parameter-free Multi-view 3D Human Motion Reconstruction", "comments": "Project page: https://briang13.github.io/FLEX/ Video:\n  https://youtu.be/nMMmfWxA3xI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing availability of video recordings made by multiple cameras has\noffered new means for mitigating occlusion and depth ambiguities in pose and\nmotion reconstruction methods. Yet, multi-view algorithms strongly depend on\ncamera parameters, in particular, the relative positions among the cameras.\nSuch dependency becomes a hurdle once shifting to dynamic capture in\nuncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an\nend-to-end parameter-free multi-view model. FLEX is parameter-free in the sense\nthat it does not require any camera parameters, neither intrinsic nor\nextrinsic. Our key idea is that the 3D angles between skeletal parts, as well\nas bone lengths, are invariant to the camera position. Hence, learning 3D\nrotations and bone lengths rather than locations allows predicting common\nvalues for all camera views. Our network takes multiple video streams, learns\nfused deep features through a novel multi-view fusion layer, and reconstructs a\nsingle consistent skeleton with temporally coherent joint rotations. We\ndemonstrate quantitative and qualitative results on the Human3.6M and KTH\nMulti-view Football II datasets. We compare our model to state-of-the-art\nmethods that are not parameter-free and show that in the absence of camera\nparameters, we outperform them by a large margin while obtaining comparable\nresults when camera parameters are available. Code, trained models, video\ndemonstration, and additional materials will be available on our project page.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:08:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gordon", "Brian", ""], ["Raab", "Sigal", ""], ["Azov", "Guy", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.01938", "submitter": "Jing Gao", "authors": "Jing Gao, Tilo Burghardt, William Andrew, Andrew W. Dowsey, Neill W.\n  Campbell", "title": "Towards Self-Supervision for Video Identification of Individual\n  Holstein-Friesian Cattle: The Cows2021 Dataset", "comments": "6 pages, 8 figures, 1 table, dataset will be available, code will be\n  available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we publish the largest identity-annotated Holstein-Friesian\ncattle dataset Cows2021 and a first self-supervision framework for video\nidentification of individual animals. The dataset contains 10,402 RGB images\nwith labels for localisation and identity as well as 301 videos from the same\nherd. The data shows top-down in-barn imagery, which captures the breed's\nindividually distinctive black and white coat pattern. Motivated by the\nlabelling burden involved in constructing visual cattle identification systems,\nwe propose exploiting the temporal coat pattern appearance across videos as a\nself-supervision signal for animal identity learning. Using an\nindividual-agnostic cattle detector that yields oriented bounding-boxes,\nrotation-normalised tracklets of individuals are formed via\ntracking-by-detection and enriched via augmentations. This produces a\n`positive' sample set per tracklet, which is paired against a `negative' set\nsampled from random cattle of other videos. Frame-triplet contrastive learning\nis then employed to construct a metric latent space. The fitting of a Gaussian\nMixture Model to this space yields a cattle identity classifier. Results show\nan accuracy of Top-1 57.0% and Top-4: 76.9% and an Adjusted Rand Index: 0.53\ncompared to the ground truth. Whilst supervised training surpasses this\nbenchmark by a large margin, we conclude that self-supervision can nevertheless\nplay a highly effective role in speeding up labelling efforts when initially\nconstructing supervision information. We provide all data and full source code\nalongside an analysis and evaluation of the system.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:08:19 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gao", "Jing", ""], ["Burghardt", "Tilo", ""], ["Andrew", "William", ""], ["Dowsey", "Andrew W.", ""], ["Campbell", "Neill W.", ""]]}, {"id": "2105.01946", "submitter": "Vassilis Vassiliades", "authors": "Giorgos Demosthenous and Vassilis Vassiliades", "title": "Continual Learning on the Edge with TensorFlow Lite", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deploying sophisticated deep learning models on embedded devices with the\npurpose of solving real-world problems is a struggle using today's technology.\nPrivacy and data limitations, network connection issues, and the need for fast\nmodel adaptation are some of the challenges that constitute today's approaches\nunfit for many applications on the edge and make real-time on-device training a\nnecessity. Google is currently working on tackling these challenges by\nembedding an experimental transfer learning API to their TensorFlow Lite,\nmachine learning library. In this paper, we show that although transfer\nlearning is a good first step for on-device model training, it suffers from\ncatastrophic forgetting when faced with more realistic scenarios. We present\nthis issue by testing a simple transfer learning model on the CORe50 benchmark\nas well as by demonstrating its limitations directly on an Android application\nwe developed. In addition, we expand the TensorFlow Lite library to include\ncontinual learning capabilities, by integrating a simple replay approach into\nthe head of the current transfer learning model. We test our continual learning\nmodel on the CORe50 benchmark to show that it tackles catastrophic forgetting,\nand we demonstrate its ability to continually learn, even under non-ideal\nconditions, using the application we developed. Finally, we open-source the\ncode of our Android application to enable developers to integrate continual\nlearning to their own smartphone applications, as well as to facilitate further\ndevelopment of continual learning functionality into the TensorFlow Lite\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:32:06 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Demosthenous", "Giorgos", ""], ["Vassiliades", "Vassilis", ""]]}, {"id": "2105.01951", "submitter": "Kin-Ming Wong", "authors": "Kin-Ming Wong", "title": "Multi-scale Image Decomposition using a Local Statistical Edge Model", "comments": "9 pages and 13 figures. To be published in the Proceedings of IEEE\n  7th International Conference on Virtual Reality, 2021", "journal-ref": null, "doi": "10.1109/ICVR51878.2021.9483837", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a progressive image decomposition method based on a novel\nnon-linear filter named Sub-window Variance filter. Our method is specifically\ndesigned for image detail enhancement purpose; this application requires\nextraction of image details which are small in terms of both spatial and\nvariation scales. We propose a local statistical edge model which develops its\nedge awareness using spatially defined image statistics. Our decomposition\nmethod is controlled by two intuitive parameters which allow the users to\ndefine what image details to suppress or enhance. By using the summed-area\ntable acceleration method, our decomposition pipeline is highly parallel. The\nproposed filter is gradient preserving and this allows our enhancement results\nfree from the gradient-reversal artefact. In our evaluations, we compare our\nmethod in various multi-scale image detail manipulation applications with other\nmainstream solutions.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:38:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wong", "Kin-Ming", ""]]}, {"id": "2105.01957", "submitter": "Dmitry Nikulin", "authors": "Dmitry Nikulin, Roman Suvorov, Aleksei Ivakhnenko, Victor Lempitsky", "title": "Perceptual Gradient Networks", "comments": "28 pages, 15 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of deep learning for image generation use perceptual losses\nfor either training or fine-tuning of the generator networks. The use of\nperceptual loss however incurs repeated forward-backward passes in a large\nimage classification network as well as a considerable memory overhead required\nto store the activations of this network. It is therefore desirable or\nsometimes even critical to get rid of these overheads.\n  In this work, we propose a way to train generator networks using\napproximations of perceptual loss that are computed without forward-backward\npasses. Instead, we use a simpler perceptual gradient network that directly\nsynthesizes the gradient field of a perceptual loss. We introduce the concept\nof proxy targets, which stabilize the predicted gradient, meaning that learning\nwith it does not lead to divergence or oscillations. In addition, our method\nallows interpretation of the predicted gradient, providing insight into the\ninternals of perceptual loss and suggesting potential ways to improve it in\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:58:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Nikulin", "Dmitry", ""], ["Suvorov", "Roman", ""], ["Ivakhnenko", "Aleksei", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2105.01993", "submitter": "Yangyang Guo", "authors": "Yangyang Guo and Liqiang Nie and Zhiyong Cheng and Feng Ji and Ji\n  Zhang and Alberto Del Bimbo", "title": "AdaVQA: Overcoming Language Priors with Adapted Margin Cosine Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of studies point out that current Visual Question Answering (VQA)\nmodels are severely affected by the language prior problem, which refers to\nblindly making predictions based on the language shortcut. Some efforts have\nbeen devoted to overcoming this issue with delicate models. However, there is\nno research to address it from the angle of the answer feature space learning,\ndespite of the fact that existing VQA methods all cast VQA as a classification\ntask. Inspired by this, in this work, we attempt to tackle the language prior\nproblem from the viewpoint of the feature space learning. To this end, an\nadapted margin cosine loss is designed to discriminate the frequent and the\nsparse answer feature space under each question type properly. As a result, the\nlimited patterns within the language modality are largely reduced, thereby less\nlanguage priors would be introduced by our method. We apply this loss function\nto several baseline models and evaluate its effectiveness on two VQA-CP\nbenchmarks. Experimental results demonstrate that our adapted margin cosine\nloss can greatly enhance the baseline models with an absolute performance gain\nof 15\\% on average, strongly verifying the potential of tackling the language\nprior problem in VQA from the angle of the answer feature space learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:41:38 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Guo", "Yangyang", ""], ["Nie", "Liqiang", ""], ["Cheng", "Zhiyong", ""], ["Ji", "Feng", ""], ["Zhang", "Ji", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2105.01998", "submitter": "Wei Yao", "authors": "Przemyslaw Polewski, Jacquelyn Shelton, Wei Yao and Marco Heurich", "title": "Instance segmentation of fallen trees in aerial color infrared imagery\n  using active multi-contour evolution with fully convolutional network-based\n  intensity priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a framework for segmenting instances of a common\nobject class by multiple active contour evolution over semantic segmentation\nmaps of images obtained through fully convolutional networks. The contour\nevolution is cast as an energy minimization problem, where the aggregate energy\nfunctional incorporates a data fit term, an explicit shape model, and accounts\nfor object overlap. Efficient solution neighborhood operators are proposed,\nenabling optimization through metaheuristics such as simulated annealing. We\ninstantiate the proposed framework in the context of segmenting individual\nfallen stems from high-resolution aerial multispectral imagery. We validated\nour approach on 3 real-world scenes of varying complexity. The test plots were\nsituated in regions of the Bavarian Forest National Park, Germany, which\nsustained a heavy bark beetle infestation. Evaluations were performed on both\nthe polygon and line segment level, showing that the multi-contour segmentation\ncan achieve up to 0.93 precision and 0.82 recall. An improvement of up to 7\npercentage points (pp) in recall and 6 in precision compared to an iterative\nsample consensus line segment detection was achieved. Despite the simplicity of\nthe applied shape parametrization, an explicit shape model incorporated into\nthe energy function improved the results by up to 4 pp of recall. Finally, we\nshow the importance of using a deep learning based semantic segmentation method\nas the basis for individual stem detection. Our method is a step towards\nincreased accessibility of automatic fallen tree mapping, due to higher cost\nefficiency of aerial imagery acquisition compared to laser scanning. The\nprecise fallen tree maps could be further used as a basis for plant and animal\nhabitat modeling, studies on carbon sequestration as well as soil quality in\nforest ecosystems.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:54:05 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Polewski", "Przemyslaw", ""], ["Shelton", "Jacquelyn", ""], ["Yao", "Wei", ""], ["Heurich", "Marco", ""]]}, {"id": "2105.02001", "submitter": "Robert Alexander Marsden", "authors": "Robert A. Marsden, Alexander Bartler, Mario D\\\"obler, Bin Yang", "title": "Contrastive Learning and Self-Training for Unsupervised Domain\n  Adaptation in Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have considerably improved\nstate-of-the-art results for semantic segmentation. Nevertheless, even modern\narchitectures lack the ability to generalize well to a test dataset that\noriginates from a different domain. To avoid the costly annotation of training\ndata for unseen domains, unsupervised domain adaptation (UDA) attempts to\nprovide efficient knowledge transfer from a labeled source domain to an\nunlabeled target domain. Previous work has mainly focused on minimizing the\ndiscrepancy between the two domains by using adversarial training or\nself-training. While adversarial training may fail to align the correct\nsemantic categories as it minimizes the discrepancy between the global\ndistributions, self-training raises the question of how to provide reliable\npseudo-labels. To align the correct semantic categories across domains, we\npropose a contrastive learning approach that adapts category-wise centroids\nacross domains. Furthermore, we extend our method with self-training, where we\nuse a memory-efficient temporal ensemble to generate consistent and reliable\npseudo-labels. Although both contrastive learning and self-training (CLST)\nthrough temporal ensembling enable knowledge transfer between two domains, it\nis their combination that leads to a symbiotic structure. We validate our\napproach on two domain adaptation benchmarks: GTA5 $\\rightarrow$ Cityscapes and\nSYNTHIA $\\rightarrow$ Cityscapes. Our method achieves better or comparable\nresults than the state-of-the-art. We will make the code publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:55:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Marsden", "Robert A.", ""], ["Bartler", "Alexander", ""], ["D\u00f6bler", "Mario", ""], ["Yang", "Bin", ""]]}, {"id": "2105.02010", "submitter": "Jan Quenzel", "authors": "Jan Quenzel and Sven Behnke", "title": "Real-time Multi-Adaptive-Resolution-Surfel 6D LiDAR Odometry using\n  Continuous-time Trajectory Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous Localization and Mapping (SLAM) is an essential capability for\nautonomous robots, but due to high data rates of 3D LiDARs real-time SLAM is\nchallenging. We propose a real-time method for 6D LiDAR odometry. Our approach\ncombines a continuous-time B-Spline trajectory representation with a Gaussian\nMixture Model (GMM) formulation to jointly align local multi-resolution surfel\nmaps. Sparse voxel grids and permutohedral lattices ensure fast access to map\nsurfels, and an adaptive resolution selection scheme effectively speeds up\nregistration. A thorough experimental evaluation shows the performance of our\napproach on two datasets and during real-robot experiments.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:14:39 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Quenzel", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "2105.02039", "submitter": "Weihong Ma", "authors": "Weihong Ma, Hesuo Zhang, Shuang Yan, Guangshun Yao, Yichao Huang, Hui\n  Li, Yaqiang Wu, Lianwen Jin", "title": "Towards an efficient framework for Data Extraction from Chart Images", "comments": "accepted by ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we fill the research gap by adopting state-of-the-art computer\nvision techniques for the data extraction stage in a data mining system. As\nshown in Fig.1, this stage contains two subtasks, namely, plot element\ndetection and data conversion. For building a robust box detector, we\ncomprehensively compare different deep learning-based methods and find a\nsuitable method to detect box with high precision. For building a robust point\ndetector, a fully convolutional network with feature fusion module is adopted,\nwhich can distinguish close points compared to traditional methods. The\nproposed system can effectively handle various chart data without making\nheuristic assumptions. For data conversion, we translate the detected element\ninto data with semantic value. A network is proposed to measure feature\nsimilarities between legends and detected elements in the legend matching\nphase. Furthermore, we provide a baseline on the competition of Harvesting raw\ntables from Infographics. Some key factors have been found to improve the\nperformance of each stage. Experimental results demonstrate the effectiveness\nof the proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:18:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ma", "Weihong", ""], ["Zhang", "Hesuo", ""], ["Yan", "Shuang", ""], ["Yao", "Guangshun", ""], ["Huang", "Yichao", ""], ["Li", "Hui", ""], ["Wu", "Yaqiang", ""], ["Jin", "Lianwen", ""]]}, {"id": "2105.02045", "submitter": "Zihao Wang", "authors": "Wang Zihao, Demarcy Thomas, Vandersteen Clair, Gnansia Dan, Raffaelli\n  Charles, Guevara Nicolas, Delingette Herv\\'e", "title": "Bayesian Logistic Shape Model Inference: application to cochlea image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating shape information is essential for the delineation of many\norgans and anatomical structures in medical images. While previous work has\nmainly focused on parametric spatial transformations applied on reference\ntemplate shapes, in this paper, we address the Bayesian inference of parametric\nshape models for segmenting medical images with the objective to provide\ninterpretable results. The proposed framework defines a likelihood appearance\nprobability and a prior label probability based on a generic shape function\nthrough a logistic function. A reference length parameter defined in the\nsigmoid controls the trade-off between shape and appearance information. The\ninference of shape parameters is performed within an Expectation-Maximisation\napproach where a Gauss-Newton optimization stage allows to provide an\napproximation of the posterior probability of shape parameters. This framework\nis applied to the segmentation of cochlea structures from clinical CT images\nconstrained by a 10 parameter shape model. It is evaluated on three different\ndatasets, one of which includes more than 200 patient images. The results show\nperformances comparable to supervised methods and better than previously\nproposed unsupervised ones. It also enables an analysis of parameter\ndistributions and the quantification of segmentation uncertainty including the\neffect of the shape model.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:21:42 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zihao", "Wang", ""], ["Thomas", "Demarcy", ""], ["Clair", "Vandersteen", ""], ["Dan", "Gnansia", ""], ["Charles", "Raffaelli", ""], ["Nicolas", "Guevara", ""], ["Herv\u00e9", "Delingette", ""]]}, {"id": "2105.02046", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Meng Wang", "title": "MCGNet: Partial Multi-view Few-shot Learning via Meta-alignment and\n  Context Gated-aggregation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new challenging task named as \\textbf{partial\nmulti-view few-shot learning}, which unifies two tasks, i.e. few-shot learning\nand partial multi-view learning, together. Different from the traditional\nfew-shot learning, this task aims to solve the few-shot learning problem given\nthe incomplete multi-view prior knowledge, which conforms more with the\nreal-world applications. However, this brings about two difficulties within\nthis task. First, the gaps among different views can be large and hard to\nreduce, especially with sample scarcity. Second, due to the incomplete view\ninformation, few-shot learning becomes more challenging than the traditional\none. To deal with the above issues, we propose a new \\textbf{Meta-alignment and\nContext Gated-aggregation Network} by equipping meta-alignment and context\ngated-aggregation with partial multi-view GNNs. Specifically, the\nmeta-alignment effectively maps the features from different views into a more\ncompact latent space, thereby reducing the view gaps. Moreover, the context\ngated-aggregation alleviates the view-missing influence by leveraging the\ncross-view context. Extensive experiments are conducted on the PIE and ORL\ndataset for evaluating our proposed method. By comparing with other few-shot\nlearning methods, our method obtains the state-of-the-art performance\nespecially with heavily-missing views.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:34:43 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhou", "Yuan", ""], ["Guo", "Yanrong", ""], ["Hao", "Shijie", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""]]}, {"id": "2105.02047", "submitter": "Florian Kluger", "authors": "Florian Kluger, Hanno Ackermann, Eric Brachmann, Michael Ying Yang,\n  Bodo Rosenhahn", "title": "Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perceive and construct the surrounding world as an arrangement of\nsimple parametric models. In particular, man-made environments commonly consist\nof volumetric primitives such as cuboids or cylinders. Inferring these\nprimitives is an important step to attain high-level, abstract scene\ndescriptions. Previous approaches directly estimate shape parameters from a 2D\nor 3D input, and are only able to reproduce simple objects, yet unable to\naccurately parse more complex 3D scenes. In contrast, we propose a robust\nestimator for primitive fitting, which can meaningfully abstract real-world\nenvironments using cuboids. A RANSAC estimator guided by a neural network fits\nthese primitives to 3D features, such as a depth map. We condition the network\non previously detected parts of the scene, thus parsing it one-by-one. To\nobtain 3D features from a single RGB image, we additionally optimise a feature\nextraction CNN in an end-to-end manner. However, naively minimising\npoint-to-primitive distances leads to large or spurious cuboids occluding parts\nof the scene behind. We thus propose an occlusion-aware distance metric\ncorrectly handling opaque scenes. The proposed algorithm does not require\nlabour-intensive labels, such as cuboid annotations, for training. Results on\nthe challenging NYU Depth v2 dataset demonstrate that the proposed algorithm\nsuccessfully abstracts cluttered real-world 3D scene layouts.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:36:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kluger", "Florian", ""], ["Ackermann", "Hanno", ""], ["Brachmann", "Eric", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2105.02061", "submitter": "Mengyang Sun", "authors": "Wei Suo, Mengyang Sun, Peng Wang, Qi Wu", "title": "Proposal-free One-stage Referring Expression via Grid-Word\n  Cross-Attention", "comments": "To be published in the 30th International Joint Conference on\n  Artificial Intelligence (IJCAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring Expression Comprehension (REC) has become one of the most important\ntasks in visual reasoning, since it is an essential step for many\nvision-and-language tasks such as visual question answering. However, it has\nnot been widely used in many downstream tasks because it suffers 1) two-stage\nmethods exist heavy computation cost and inevitable error accumulation, and 2)\none-stage methods have to depend on lots of hyper-parameters (such as anchors)\nto generate bounding box. In this paper, we present a proposal-free one-stage\n(PFOS) model that is able to regress the region-of-interest from the image,\nbased on a textual query, in an end-to-end manner. Instead of using the\ndominant anchor proposal fashion, we directly take the dense-grid of an image\nas input for a cross-attention transformer that learns grid-word\ncorrespondences. The final bounding box is predicted directly from the image\nwithout the time-consuming anchor selection process that previous methods\nsuffer. Our model achieves the state-of-the-art performance on four referring\nexpression datasets with higher efficiency, comparing to previous best\none-stage and two-stage methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:53:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Suo", "Wei", ""], ["Sun", "Mengyang", ""], ["Wang", "Peng", ""], ["Wu", "Qi", ""]]}, {"id": "2105.02089", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Deep Spherical Manifold Gaussian Kernel for Unsupervised Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Unsupervised Domain adaptation is an effective method in addressing the\ndomain shift issue when transferring knowledge from an existing richly labeled\ndomain to a new domain. Existing manifold-based methods either are based on\ntraditional models or largely rely on Grassmannian manifold via minimizing\ndifferences of single covariance matrices of two domains. In addition, existing\npseudo-labeling algorithms inadequately consider the quality of pseudo labels\nin aligning the conditional distribution between two domains. In this work, a\ndeep spherical manifold Gaussian kernel (DSGK) framework is proposed to map the\nsource and target subspaces into a spherical manifold and reduce the\ndiscrepancy between them by embedding both extracted features and a Gaussian\nkernel. To align the conditional distributions, we further develop an\neasy-to-hard pseudo label refinement process to improve the quality of the\npseudo labels and then reduce categorical spherical manifold Gaussian kernel\ngeodesic loss. Extensive experimental results show that DSGK outperforms\nstate-of-the-art methods, especially on challenging cross-domain learning\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:39:20 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2105.02103", "submitter": "Evgeny Smirnov", "authors": "Evgeny Smirnov, Nikita Garaev, Vasiliy Galyuk", "title": "Prototype Memory for Large-scale Face Representation Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face representation learning using datasets with massive number of identities\nrequires appropriate training methods. Softmax-based approach, currently the\nstate-of-the-art in face recognition, in its usual \"full softmax\" form is not\nsuitable for datasets with millions of persons. Several methods, based on the\n\"sampled softmax\" approach, were proposed to remove this limitation. These\nmethods, however, have a set of disadvantages. One of them is a problem of\n\"prototype obsolescence\": classifier weights (prototypes) of the rarely sampled\nclasses, receive too scarce gradients and become outdated and detached from the\ncurrent encoder state, resulting in an incorrect training signals. This problem\nis especially serious in ultra-large-scale datasets. In this paper, we propose\na novel face representation learning model called Prototype Memory, which\nalleviates this problem and allows training on a dataset of any size. Prototype\nMemory consists of the limited-size memory module for storing recent class\nprototypes and employs a set of algorithms to update it in appropriate way. New\nclass prototypes are generated on the fly using exemplar embeddings in the\ncurrent mini-batch. These prototypes are enqueued to the memory and used in a\nrole of classifier weights for usual softmax classification-based training. To\nprevent obsolescence and keep the memory in close connection with encoder,\nprototypes are regularly refreshed, and oldest ones are dequeued and disposed.\nPrototype Memory is computationally efficient and independent of dataset size.\nIt can be used with various loss functions, hard example mining algorithms and\nencoder architectures. We prove the effectiveness of the proposed model by\nextensive experiments on popular face recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:08:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Smirnov", "Evgeny", ""], ["Garaev", "Nikita", ""], ["Galyuk", "Vasiliy", ""]]}, {"id": "2105.02104", "submitter": "Lynton Ardizzone", "authors": "Lynton Ardizzone, Jakob Kruse, Carsten L\\\"uth, Niels Bracher, Carsten\n  Rother, Ullrich K\\\"othe", "title": "Conditional Invertible Neural Networks for Diverse Image-to-Image\n  Translation", "comments": "arXiv admin note: text overlap with arXiv:1907.02392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new architecture called a conditional invertible neural\nnetwork (cINN), and use it to address the task of diverse image-to-image\ntranslation for natural images. This is not easily possible with existing INN\nmodels due to some fundamental limitations. The cINN combines the purely\ngenerative INN model with an unconstrained feed-forward network, which\nefficiently preprocesses the conditioning image into maximally informative\nfeatures. All parameters of a cINN are jointly optimized with a stable, maximum\nlikelihood-based training procedure. Even though INN-based models have received\nfar less attention in the literature than GANs, they have been shown to have\nsome remarkable properties absent in GANs, e.g. apparent immunity to mode\ncollapse. We find that our cINNs leverage these properties for image-to-image\ntranslation, demonstrated on day to night translation and image colorization.\nFurthermore, we take advantage of our bidirectional cINN architecture to\nexplore and manipulate emergent properties of the latent space, such as\nchanging the image style in an intuitive way.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:10:37 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ardizzone", "Lynton", ""], ["Kruse", "Jakob", ""], ["L\u00fcth", "Carsten", ""], ["Bracher", "Niels", ""], ["Rother", "Carsten", ""], ["K\u00f6the", "Ullrich", ""]]}, {"id": "2105.02151", "submitter": "Wei Yao", "authors": "Rong Huang, Wei Yao, Yusheng Xu, Zhen Ye and Uwe Stilla", "title": "Pairwise Point Cloud Registration using Graph Matching and\n  Rotation-invariant Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Registration is a fundamental but critical task in point cloud processing,\nwhich usually depends on finding element correspondence from two point clouds.\nHowever, the finding of reliable correspondence relies on establishing a robust\nand discriminative description of elements and the correct matching of\ncorresponding elements. In this letter, we develop a coarse-to-fine\nregistration strategy, which utilizes rotation-invariant features and a new\nweighted graph matching method for iteratively finding correspondence. In the\ngraph matching method, the similarity of nodes and edges in Euclidean and\nfeature space are formulated to construct the optimization function. The\nproposed strategy is evaluated using two benchmark datasets and compared with\nseveral state-of-the-art methods. Regarding the experimental results, our\nproposed method can achieve a fine registration with rotation errors of less\nthan 0.2 degrees and translation errors of less than 0.1m.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:03:05 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Huang", "Rong", ""], ["Yao", "Wei", ""], ["Xu", "Yusheng", ""], ["Ye", "Zhen", ""], ["Stilla", "Uwe", ""]]}, {"id": "2105.02158", "submitter": "Zizheng Que", "authors": "Zizheng Que, Guo Lu, Dong Xu", "title": "VoxelContext-Net: An Octree based Framework for Point Cloud Compression", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-stage deep learning framework called\nVoxelContext-Net for both static and dynamic point cloud compression. Taking\nadvantages of both octree based methods and voxel based schemes, our approach\nemploys the voxel context to compress the octree structured data. Specifically,\nwe first extract the local voxel representation that encodes the spatial\nneighbouring context information for each node in the constructed octree. Then,\nin the entropy coding stage, we propose a voxel context based deep entropy\nmodel to compress the symbols of non-leaf nodes in a lossless way. Furthermore,\nfor dynamic point cloud compression, we additionally introduce the local voxel\nrepresentations from the temporal neighbouring point clouds to exploit temporal\ndependency. More importantly, to alleviate the distortion from the octree\nconstruction procedure, we propose a voxel context based 3D coordinate\nrefinement method to produce more accurate reconstructed point cloud at the\ndecoder side, which is applicable to both static and dynamic point cloud\ncompression. The comprehensive experiments on both static and dynamic point\ncloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate\nthe effectiveness of our newly proposed method VoxelContext-Net for 3D point\ncloud geometry compression.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:12:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Que", "Zizheng", ""], ["Lu", "Guo", ""], ["Xu", "Dong", ""]]}, {"id": "2105.02170", "submitter": "Qi Dong", "authors": "Qi Dong, Zhuowen Tu, Haofu Liao, Yuting Zhang, Vijay Mahadevan,\n  Stefano Soatto", "title": "Visual Composite Set Detection Using Part-and-Sum Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision applications such as visual relationship detection and\nhuman-object interaction can be formulated as a composite (structured) set\ndetection problem in which both the parts (subject, object, and predicate) and\nthe sum (triplet as a whole) are to be detected in a hierarchical fashion. In\nthis paper, we present a new approach, denoted Part-and-Sum detection\nTransformer (PST), to perform end-to-end composite set detection. Different\nfrom existing Transformers in which queries are at a single level, we\nsimultaneously model the joint part and sum hypotheses/interactions with\ncomposite queries and attention modules. We explicitly incorporate sum queries\nto enable better modeling of the part-and-sum relations that are absent in the\nstandard Transformers. Our approach also uses novel tensor-based part queries\nand vector-based sum queries, and models their joint interaction. We report\nexperiments on two vision tasks, visual relationship detection, and\nhuman-object interaction, and demonstrate that PST achieves state-of-the-art\nresults among single-stage models, while nearly matching the results of\ncustom-designed two-stage models.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:31:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Dong", "Qi", ""], ["Tu", "Zhuowen", ""], ["Liao", "Haofu", ""], ["Zhang", "Yuting", ""], ["Mahadevan", "Vijay", ""], ["Soatto", "Stefano", ""]]}, {"id": "2105.02173", "submitter": "Zhixiang Chen", "authors": "Zhixiang Chen and Tae-Kyun Kim", "title": "Learning Feature Aggregation for Deep 3D Morphable Models", "comments": "Published in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D morphable models are widely used for the shape representation of an object\nclass in computer vision and graphics applications. In this work, we focus on\ndeep 3D morphable models that directly apply deep learning on 3D mesh data with\na hierarchical structure to capture information at multiple scales. While great\nefforts have been made to design the convolution operator, how to best\naggregate vertex features across hierarchical levels deserves further\nattention. In contrast to resorting to mesh decimation, we propose an attention\nbased module to learn mapping matrices for better feature aggregation across\nhierarchical levels. Specifically, the mapping matrices are generated by a\ncompatibility function of the keys and queries. The keys and queries are\ntrainable variables, learned by optimizing the target objective, and shared by\nall data samples of the same object class. Our proposed module can be used as a\ntrain-only drop-in replacement for the feature aggregation in existing\narchitectures for both downsampling and upsampling. Our experiments show that\nthrough the end-to-end training of the mapping matrices, we achieve\nstate-of-the-art results on a variety of 3D shape datasets in comparison to\nexisting morphable models.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:41:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Zhixiang", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2105.02184", "submitter": "Enze Xie", "authors": "Enze Xie, Wenhai Wang, Mingyu Ding, Ruimao Zhang, Ping Luo", "title": "PolarMask++: Enhanced Polar Representation for Single-Shot Instance\n  Segmentation and Beyond", "comments": "TPAMI 2021 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the complexity of the pipeline of instance segmentation is crucial\nfor real-world applications. This work addresses this issue by introducing an\nanchor-box free and single-shot instance segmentation framework, termed\nPolarMask, which reformulates the instance segmentation problem as predicting\nthe contours of objects in the polar coordinate, with several appealing\nbenefits. (1) The polar representation unifies instance segmentation (masks)\nand object detection (bounding boxes) into a single framework, reducing the\ndesign and computational complexity. (2) Two modules are carefully designed\n(i.e. soft polar centerness and polar IoU loss) to sample high-quality center\nexamples and optimize polar contour regression, making the performance of\nPolarMask does not depend on the bounding box prediction results and thus\nbecomes more efficient in training. (3) PolarMask is fully convolutional and\ncan be easily embedded into most off-the-shelf detection methods. To further\nimprove the accuracy of the framework, a Refined Feature Pyramid is introduced\nto further improve the feature representation at different scales, termed\nPolarMask++. Extensive experiments demonstrate the effectiveness of both\nPolarMask and PolarMask++, which achieve competitive results on instance\nsegmentation in the challenging COCO dataset with single-model and single-scale\ntraining and testing, as well as new state-of-the-art results on rotate text\ndetection and cell segmentation. We hope the proposed polar representation can\nprovide a new perspective for designing algorithms to solve single-shot\ninstance segmentation. The codes and models are available at:\ngithub.com/xieenze/PolarMask.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:55:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Xie", "Enze", ""], ["Wang", "Wenhai", ""], ["Ding", "Mingyu", ""], ["Zhang", "Ruimao", ""], ["Luo", "Ping", ""]]}, {"id": "2105.02186", "submitter": "Dylan Stewart", "authors": "Dylan Stewart, Alina Zare, Sergio Marconi, Ben Weinstein, Ethan White,\n  Sarah Graves, Stephanie Bohlman, Aditya Singh", "title": "Addressing Annotation Imprecision for Tree Crown Delineation Using the\n  RandCrowns Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised methods for object delineation in remote sensing require labeled\nground-truth data. Gathering sufficient high quality ground-truth data is\ndifficult, especially when the targets are of irregular shape or difficult to\ndistinguish from the background or neighboring objects. Tree crown delineation\nprovides key information from remote sensing images for forestry, ecology, and\nmanagement. However, tree crowns in remote sensing imagery are often difficult\nto label and annotate due to irregular shape, overlapping canopies, shadowing,\nand indistinct edges. There are also multiple approaches to annotation in this\nfield (e.g., rectangular boxes vs. convex polygons) that further contribute to\nannotation imprecision. However, current evaluation methods do not account for\nthis uncertainty in annotations, and quantitative metrics for evaluation can\nvary across multiple annotators. We address these limitations using an\nadaptation of the Rand index for weakly-labeled crown delineation that we call\nRandCrowns. The RandCrowns metric reformulates the Rand index by adjusting the\nareas over which each term of the index is computed to account for uncertain\nand imprecise object delineation labels. Quantitative comparisons to the\ncommonly used intersection over union (Jaccard similarity) method shows a\ndecrease in the variance generated by differences among multiple annotators.\nCombined with qualitative examples, our results suggest that this RandCrowns\nmetric is more robust for scoring target delineations in the presence of\nuncertainty and imprecision in annotations that are inherent to tree crown\ndelineation. Although the focus of this paper is on evaluation of tree crown\ndelineations, annotation imprecision is a challenge that is common across\nremote sensing of the environment (and many computer vision problems in\ngeneral).\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:57:23 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Stewart", "Dylan", ""], ["Zare", "Alina", ""], ["Marconi", "Sergio", ""], ["Weinstein", "Ben", ""], ["White", "Ethan", ""], ["Graves", "Sarah", ""], ["Bohlman", "Stephanie", ""], ["Singh", "Aditya", ""]]}, {"id": "2105.02188", "submitter": "Maria Tirindelli", "authors": "Maria Tirindelli, Christine Eilers, Walter Simson, Magdalini Paschali,\n  Mohammad Farid Azampour, Nassir Navab", "title": "Rethinking Ultrasound Augmentation: A Physics-Inspired Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Ultrasound (US), despite its wide use, is characterized by artifacts\nand operator dependency. Those attributes hinder the gathering and utilization\nof US datasets for the training of Deep Neural Networks used for\nComputer-Assisted Intervention Systems. Data augmentation is commonly used to\nenhance model generalization and performance. However, common data augmentation\ntechniques, such as affine transformations do not align with the physics of US\nand, when used carelessly can lead to unrealistic US images. To this end, we\npropose a set of physics-inspired transformations, including deformation,\nreverb and Signal-to-Noise Ratio, that we apply on US B-mode images for data\naugmentation. We evaluate our method on a new spine US dataset for the tasks of\nbone segmentation and classification.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:59:38 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tirindelli", "Maria", ""], ["Eilers", "Christine", ""], ["Simson", "Walter", ""], ["Paschali", "Magdalini", ""], ["Azampour", "Mohammad Farid", ""], ["Navab", "Nassir", ""]]}, {"id": "2105.02195", "submitter": "Dan Xu", "authors": "Dan Xu, Andrea Vedaldi, Joao F. Henriques", "title": "Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to train deep networks to decompose videos into 3D\ngeometry (camera and depth), moving objects, and their motions, with no\nsupervision. We build on the idea of view synthesis, which uses classical\ncamera geometry to re-render a source image from a different point-of-view,\nspecified by a predicted relative pose and depth map. By minimizing the error\nbetween the synthetic image and the corresponding real image in a video, the\ndeep network that predicts pose and depth can be trained completely\nunsupervised. However, the view synthesis equations rely on a strong\nassumption: that objects do not move. This rigid-world assumption limits the\npredictive power, and rules out learning about objects automatically. We\npropose a simple solution: minimize the error on small regions of the image\ninstead. While the scene as a whole may be non-rigid, it is always possible to\nfind small regions that are approximately rigid, such as inside a moving\nobject. Our network can then predict different poses for each region, in a\nsliding window from a learned dense pose map. This represents a significantly\nricher model, including 6D object motions, with little additional complexity.\nWe achieve very competitive performance on unsupervised odometry and depth\nprediction on KITTI. We also demonstrate new capabilities on EPIC-Kitchens, a\nchallenging dataset of indoor videos, where there is no ground truth\ninformation for depth, odometry, object segmentation or motion. Yet all are\nrecovered automatically by our method.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:08:10 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:49:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Xu", "Dan", ""], ["Vedaldi", "Andrea", ""], ["Henriques", "Joao F.", ""]]}, {"id": "2105.02197", "submitter": "Olivier Vincent", "authors": "Olivier Vincent, Charley Gros, Julien Cohen-Adad", "title": "Impact of individual rater style on deep learning uncertainty in medical\n  imaging segmentation", "comments": "17 pages, 8 figures, in submission at MELBA journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While multiple studies have explored the relation between inter-rater\nvariability and deep learning model uncertainty in medical segmentation tasks,\nlittle is known about the impact of individual rater style. This study\nquantifies rater style in the form of bias and consistency and explores their\nimpacts when used to train deep learning models. Two multi-rater public\ndatasets were used, consisting of brain multiple sclerosis lesion and spinal\ncord grey matter segmentation. On both datasets, results show a correlation\n($R^2 = 0.60$ and $0.93$) between rater bias and deep learning uncertainty. The\nimpact of label fusion between raters' annotations on this relationship is also\nexplored, and we show that multi-center consensuses are more effective than\nsingle-center consensuses to reduce uncertainty, since rater style is mostly\ncenter-specific.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:11:18 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Vincent", "Olivier", ""], ["Gros", "Charley", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "2105.02201", "submitter": "HongYu Liu", "authors": "Hongyu Liu and Ziyu Wan and Wei Huang and Yibing Song and Xintong Han\n  and Jing Liao", "title": "PD-GAN: Probabilistic Diverse GAN for Image Inpainting", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PD-GAN, a probabilistic diverse GAN for image inpainting. Given an\ninput image with arbitrary hole regions, PD-GAN produces multiple inpainting\nresults with diverse and visually realistic content. Our PD-GAN is built upon a\nvanilla GAN which generates images based on random noise. During image\ngeneration, we modulate deep features of input random noise from coarse-to-fine\nby injecting an initially restored image and the hole regions in multiple\nscales. We argue that during hole filling, the pixels near the hole boundary\nshould be more deterministic (i.e., with higher probability trusting the\ncontext and initially restored image to create natural inpainting boundary),\nwhile those pixels lie in the center of the hole should enjoy more degrees of\nfreedom (i.e., more likely to depend on the random noise for enhancing\ndiversity). To this end, we propose spatially probabilistic diversity\nnormalization (SPDNorm) inside the modulation to model the probability of\ngenerating a pixel conditioned on the context information. SPDNorm dynamically\nbalances the realism and diversity inside the hole region, making the generated\ncontent more diverse towards the hole center and resemble neighboring image\ncontent more towards the hole boundary. Meanwhile, we propose a perceptual\ndiversity loss to further empower PD-GAN for diverse content generation.\nExperiments on benchmark datasets including CelebA-HQ, Places2 and Paris Street\nView indicate that PD-GAN is effective for diverse and visually realistic image\nrestoration.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:20:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Liu", "Hongyu", ""], ["Wan", "Ziyu", ""], ["Huang", "Wei", ""], ["Song", "Yibing", ""], ["Han", "Xintong", ""], ["Liao", "Jing", ""]]}, {"id": "2105.02209", "submitter": "Amirsaeed Yazdani", "authors": "Amirsaeed Yazdani, Tiantong Guo, Vishal Monga", "title": "Physically Inspired Dense Fusion Networks for Relighting", "comments": "Rank second in NTIRE 2021 One-to-one depth guided image relighting\n  challenge, accepted by CVPRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image relighting has emerged as a problem of significant research interest\ninspired by augmented reality applications. Physics-based traditional methods,\nas well as black box deep learning models, have been developed. The existing\ndeep networks have exploited training to achieve a new state of the art;\nhowever, they may perform poorly when training is limited or does not represent\nproblem phenomenology, such as the addition or removal of dense shadows. We\npropose a model which enriches neural networks with physical insight. More\nprecisely, our method generates the relighted image with new illumination\nsettings via two different strategies and subsequently fuses them using a\nweight map (w). In the first strategy, our model predicts the material\nreflectance parameters (albedo) and illumination/geometry parameters of the\nscene (shading) for the relit image (we refer to this strategy as intrinsic\nimage decomposition (IID)). The second strategy is solely based on the black\nbox approach, where the model optimizes its weights based on the ground-truth\nimages and the loss terms in the training stage and generates the relit output\ndirectly (we refer to this strategy as direct). While our proposed method\napplies to both one-to-one and any-to-any relighting problems, for each case we\nintroduce problem-specific components that enrich the model performance: 1) For\none-to-one relighting we incorporate normal vectors of the surfaces in the\nscene to adjust gloss and shadows accordingly in the image. 2) For any-to-any\nrelighting, we propose an additional multiscale block to the architecture to\nenhance feature extraction. Experimental results on the VIDIT 2020 and the\nVIDIT 2021 dataset (used in the NTIRE 2021 relighting challenge) reveals that\nour proposal can outperform many state-of-the-art methods in terms of\nwell-known fidelity metrics and perceptual loss.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:33:45 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yazdani", "Amirsaeed", ""], ["Guo", "Tiantong", ""], ["Monga", "Vishal", ""]]}, {"id": "2105.02216", "submitter": "Junhwa Hur", "authors": "Junhwa Hur, Stefan Roth", "title": "Self-Supervised Multi-Frame Monocular Scene Flow", "comments": "To appear at CVPR 2021. Code available:\n  https://github.com/visinf/multi-mono-sf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D scene flow from a sequence of monocular images has been gaining\nincreased attention due to the simple, economical capture setup. Owing to the\nsevere ill-posedness of the problem, the accuracy of current methods has been\nlimited, especially that of efficient, real-time approaches. In this paper, we\nintroduce a multi-frame monocular scene flow network based on self-supervised\nlearning, improving the accuracy over previous networks while retaining\nreal-time efficiency. Based on an advanced two-frame baseline with a\nsplit-decoder design, we propose (i) a multi-frame model using a triple frame\ninput and convolutional LSTM connections, (ii) an occlusion-aware census loss\nfor better accuracy, and (iii) a gradient detaching strategy to improve\ntraining stability. On the KITTI dataset, we observe state-of-the-art accuracy\namong monocular scene flow methods based on self-supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:49:55 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "2105.02282", "submitter": "Zihao Wang", "authors": "Zihao Wang, Herv\\'e Delingette", "title": "Attention for Image Registration (AiR): an unsupervised Transformer\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration as an important basis in signal processing task often\nencounter the problem of stability and efficiency. Non-learning registration\napproaches rely on the optimization of the similarity metrics between the fix\nand moving images. Yet, those approaches are usually costly in both time and\nspace complexity. The problem can be worse when the size of the image is large\nor the deformations between the images are severe. Recently, deep learning, or\nprecisely saying, the convolutional neural network (CNN) based image\nregistration methods have been widely investigated in the research community\nand show promising effectiveness to overcome the weakness of non-learning based\nmethods. To explore the advanced learning approaches in image registration\nproblem for solving practical issues, we present in this paper a method of\nintroducing attention mechanism in deformable image registration problem. The\nproposed approach is based on learning the deformation field with a Transformer\nframework (AiR) that does not rely on the CNN but can be efficiently trained on\nGPGPU devices also. In a more vivid interpretation: we treat the image\nregistration problem as the same as a language translation task and introducing\na Transformer to tackle the problem. Our method learns an unsupervised\ngenerated deformation map and is tested on two benchmark datasets. The source\ncode of the AiR will be released at Gitlab.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:49:32 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Zihao", ""], ["Delingette", "Herv\u00e9", ""]]}, {"id": "2105.02290", "submitter": "Dhaval Kadia", "authors": "Dhaval D. Kadia, Md Zahangir Alom, Ranga Burada, Tam V. Nguyen,\n  Vijayan K. Asari", "title": "R2U3D: Recurrent Residual 3D U-Net for Lung Segmentation", "comments": "The paper is under review in a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  3D lung segmentation is essential since it processes the volumetric\ninformation of the lungs, removes the unnecessary areas of the scan, and\nsegments the actual area of the lungs in a 3D volume. Recently, the deep\nlearning model, such as U-Net outperforms other network architectures for\nbiomedical image segmentation. In this paper, we propose a novel model, namely,\nRecurrent Residual 3D U-Net (R2U3D), for the 3D lung segmentation task. In\nparticular, the proposed model integrates 3D convolution into the Recurrent\nResidual Neural Network based on U-Net. It helps learn spatial dependencies in\n3D and increases the propagation of 3D volumetric information. The proposed\nR2U3D network is trained on the publicly available dataset LUNA16 and it\nachieves state-of-the-art performance on both LUNA16 (testing set) and VESSEL12\ndataset. In addition, we show that training the R2U3D model with a smaller\nnumber of CT scans, i.e., 100 scans, without applying data augmentation\nachieves an outstanding result in terms of Soft Dice Similarity Coefficient\n(Soft-DSC) of 0.9920.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 19:17:14 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kadia", "Dhaval D.", ""], ["Alom", "Md Zahangir", ""], ["Burada", "Ranga", ""], ["Nguyen", "Tam V.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "2105.02317", "submitter": "Candice Schumann", "authors": "Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari,\n  Caroline Pantofaru", "title": "A Step Toward More Inclusive People Annotations for Fairness", "comments": null, "journal-ref": "AIES (2021)", "doi": "10.1145/3461702.3462594", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Open Images Dataset contains approximately 9 million images and is a\nwidely accepted dataset for computer vision research. As is common practice for\nlarge datasets, the annotations are not exhaustive, with bounding boxes and\nattribute labels for only a subset of the classes in each image. In this paper,\nwe present a new set of annotations on a subset of the Open Images dataset\ncalled the MIAP (More Inclusive Annotations for People) subset, containing\nbounding boxes and attributes for all of the people visible in those images.\nThe attributes and labeling methodology for the MIAP subset were designed to\nenable research into model fairness. In addition, we analyze the original\nannotation methodology for the person class and its subclasses, discussing the\nresulting patterns in order to inform future annotation efforts. By considering\nboth the original and exhaustive annotation sets, researchers can also now\nstudy how systematic patterns in training annotations affect modeling.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:44:56 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Schumann", "Candice", ""], ["Ricco", "Susanna", ""], ["Prabhu", "Utsav", ""], ["Ferrari", "Vittorio", ""], ["Pantofaru", "Caroline", ""]]}, {"id": "2105.02319", "submitter": "Mohammed Daoudi", "authors": "Qingkai Zhen, Di Huang, Yunhong Wang, Hassen Drira, Boulbaba Ben Amor,\n  Mohamed Daoudi", "title": "Magnifying Subtle Facial Motions for Effective 4D Expression Recognition", "comments": "International Conference On Pattern Recognition 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, an effective pipeline to automatic 4D Facial Expression\nRecognition (4D FER) is proposed. It combines two growing but disparate ideas\nin Computer Vision -- computing the spatial facial deformations using tools\nfrom Riemannian geometry and magnifying them using temporal filtering. The flow\nof 3D faces is first analyzed to capture the spatial deformations based on the\nrecently-developed Riemannian approach, where registration and comparison of\nneighboring 3D faces are led jointly. Then, the obtained temporal evolution of\nthese deformations are fed into a magnification method in order to amplify the\nfacial activities over the time. The latter, main contribution of this paper,\nallows revealing subtle (hidden) deformations which enhance the emotion\nclassification performance. We evaluated our approach on BU-4DFE dataset, the\nstate-of-art 94.18% average performance and an improvement that exceeds 10% in\nclassification accuracy, after magnifying extracted geometric features\n(deformations), are achieved.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:47:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhen", "Qingkai", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""], ["Drira", "Hassen", ""], ["Amor", "Boulbaba Ben", ""], ["Daoudi", "Mohamed", ""]]}, {"id": "2105.02320", "submitter": "Zhongqi Miao", "authors": "Zhongqi Miao, Ziwei Liu, Kaitlyn M. Gaynor, Meredith S. Palmer, Stella\n  X. Yu, Wayne M. Getz", "title": "Iterative Human and Automated Identification of Wildlife Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camera trapping is increasingly used to monitor wildlife, but this technology\ntypically requires extensive data annotation. Recently, deep learning has\nsignificantly advanced automatic wildlife recognition. However, current methods\nare hampered by a dependence on large static data sets when wildlife data is\nintrinsically dynamic and involves long-tailed distributions. These two\ndrawbacks can be overcome through a hybrid combination of machine learning and\nhumans in the loop. Our proposed iterative human and automated identification\napproach is capable of learning from wildlife imagery data with a long-tailed\ndistribution. Additionally, it includes self-updating learning that facilitates\ncapturing the community dynamics of rapidly changing natural systems. Extensive\nexperiments show that our approach can achieve a ~90% accuracy employing only\n~20% of the human annotations of existing approaches. Our synergistic\ncollaboration of humans and machines transforms deep learning from a relatively\ninefficient post-annotation tool to a collaborative on-going annotation tool\nthat vastly relieves the burden of human annotation and enables efficient and\nconstant model updates.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:51:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Miao", "Zhongqi", ""], ["Liu", "Ziwei", ""], ["Gaynor", "Kaitlyn M.", ""], ["Palmer", "Meredith S.", ""], ["Yu", "Stella X.", ""], ["Getz", "Wayne M.", ""]]}, {"id": "2105.02340", "submitter": "Bartosz Krawczyk", "authors": "Damien Dablain, Bartosz Krawczyk, Nitesh V. Chawla", "title": "DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite over two decades of progress, imbalanced data is still considered a\nsignificant challenge for contemporary machine learning models. Modern advances\nin deep learning have magnified the importance of the imbalanced data problem.\nThe two main approaches to address this issue are based on loss function\nmodifications and instance resampling. Instance sampling is typically based on\nGenerative Adversarial Networks (GANs), which may suffer from mode collapse.\nTherefore, there is a need for an oversampling method that is specifically\ntailored to deep learning models, can work on raw images while preserving their\nproperties, and is capable of generating high quality, artificial images that\ncan enhance minority classes and balance the training set. We propose DeepSMOTE\n- a novel oversampling algorithm for deep learning models. It is simple, yet\neffective in its design. It consists of three major components: (i) an\nencoder/decoder framework; (ii) SMOTE-based oversampling; and (iii) a dedicated\nloss function that is enhanced with a penalty term. An important advantage of\nDeepSMOTE over GAN-based oversampling is that DeepSMOTE does not require a\ndiscriminator, and it generates high-quality artificial images that are both\ninformation-rich and suitable for visual inspection. DeepSMOTE code is publicly\navailable at: https://github.com/dd1github/DeepSMOTE\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 21:49:37 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dablain", "Damien", ""], ["Krawczyk", "Bartosz", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "2105.02351", "submitter": "Necati Cihan Camgoz Dr.", "authors": "Necati Cihan Camgoz, Ben Saunders, Guillaume Rochette, Marco\n  Giovanelli, Giacomo Inches, Robin Nachtrab-Ribback, Richard Bowden", "title": "Content4All Open Research Sign Language Translation Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computational sign language research lacks the large-scale datasets that\nenables the creation of useful reallife applications. To date, most research\nhas been limited to prototype systems on small domains of discourse, e.g.\nweather forecasts. To address this issue and to push the field forward, we\nrelease six datasets comprised of 190 hours of footage on the larger domain of\nnews. From this, 20 hours of footage have been annotated by Deaf experts and\ninterpreters and is made publicly available for research purposes. In this\npaper, we share the dataset collection process and tools developed to enable\nthe alignment of sign language video and subtitles, as well as baseline\ntranslation results to underpin future research.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:14:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Camgoz", "Necati Cihan", ""], ["Saunders", "Ben", ""], ["Rochette", "Guillaume", ""], ["Giovanelli", "Marco", ""], ["Inches", "Giacomo", ""], ["Nachtrab-Ribback", "Robin", ""], ["Bowden", "Richard", ""]]}, {"id": "2105.02357", "submitter": "Samanta Knapi\\v{c}", "authors": "Samanta Knapi\\v{c}, Avleen Malhi, Rohit Saluja, Kary Fr\\\"amling", "title": "Explainable Artificial Intelligence for Human Decision-Support System in\n  Medical Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the present paper we present the potential of Explainable Artificial\nIntelligence methods for decision-support in medical image analysis scenarios.\nWith three types of explainable methods applied to the same medical image data\nset our aim was to improve the comprehensibility of the decisions provided by\nthe Convolutional Neural Network (CNN). The visual explanations were provided\non in-vivo gastral images obtained from a Video capsule endoscopy (VCE), with\nthe goal of increasing the health professionals' trust in the black box\npredictions. We implemented two post-hoc interpretable machine learning methods\nLIME and SHAP and the alternative explanation approach CIU, centered on the\nContextual Value and Utility (CIU). The produced explanations were evaluated\nusing human evaluation. We conducted three user studies based on the\nexplanations provided by LIME, SHAP and CIU. Users from different non-medical\nbackgrounds carried out a series of tests in the web-based survey setting and\nstated their experience and understanding of the given explanations. Three user\ngroups (n=20, 20, 20) with three distinct forms of explanations were\nquantitatively analyzed. We have found that, as hypothesized, the CIU\nexplainable method performed better than both LIME and SHAP methods in terms of\nincreasing support for human decision-making as well as being more transparent\nand thus understandable to users. Additionally, CIU outperformed LIME and SHAP\nby generating explanations more rapidly. Our findings suggest that there are\nnotable differences in human decision-making between various explanation\nsupport settings. In line with that, we present three potential explainable\nmethods that can with future improvements in implementation be generalized on\ndifferent medical data sets and can provide great decision-support for medical\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:29:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Knapi\u010d", "Samanta", ""], ["Malhi", "Avleen", ""], ["Saluja", "Rohit", ""], ["Fr\u00e4mling", "Kary", ""]]}, {"id": "2105.02358", "submitter": "Meng-Hao Guo", "authors": "Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, Shi-Min Hu", "title": "Beyond Self-attention: External Attention using Two Linear Layers for\n  Visual Tasks", "comments": "11 pages, 6 figures. external attention and EAMLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms, especially self-attention, have played an increasingly\nimportant role in deep feature representation for visual tasks. Self-attention\nupdates the feature at each position by computing a weighted sum of features\nusing pair-wise affinities across all positions to capture the long-range\ndependency within a single sample. However, self-attention has quadratic\ncomplexity and ignores potential correlation between different samples. This\npaper proposes a novel attention mechanism which we call external attention,\nbased on two external, small, learnable, shared memories, which can be\nimplemented easily by simply using two cascaded linear layers and two\nnormalization layers; it conveniently replaces self-attention in existing\npopular architectures. External attention has linear complexity and implicitly\nconsiders the correlations between all data samples. We further incorporate the\nmulti-head mechanism into external attention to provide an all-MLP\narchitecture, external attention MLP (EAMLP), for image classification.\nExtensive experiments on image classification, object detection, semantic\nsegmentation, instance segmentation, image generation, and point cloud analysis\nreveal that our method provides results comparable or superior to the\nself-attention mechanism and some of its variants, with much lower\ncomputational and memory costs.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:29:52 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:49:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Guo", "Meng-Hao", ""], ["Liu", "Zheng-Ning", ""], ["Mu", "Tai-Jiang", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2105.02359", "submitter": "Borja Bovcon", "authors": "Borja Bovcon, Jon Muhovi\\v{c}, Du\\v{s}ko Vranac, Dean Mozeti\\v{c},\n  Janez Per\\v{s}, Matej Kristan", "title": "MODS -- A USV-oriented object detection and obstacle segmentation\n  benchmark", "comments": "15 pages, 15 figures. The dataset, as well as the proposed evaluation\n  metrics, will be published on our website: www.vicos.si", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Small-sized unmanned surface vehicles (USV) are coastal water devices with a\nbroad range of applications such as environmental control and surveillance. A\ncrucial capability for autonomous operation is obstacle detection for timely\nreaction and collision avoidance, which has been recently explored in the\ncontext of camera-based visual scene interpretation. Owing to curated datasets,\nsubstantial advances in scene interpretation have been made in a related field\nof unmanned ground vehicles. However, the current maritime datasets do not\nadequately capture the complexity of real-world USV scenes and the evaluation\nprotocols are not standardised, which makes cross-paper comparison of different\nmethods difficult and hiders the progress. To address these issues, we\nintroduce a new obstacle detection benchmark MODS, which considers two major\nperception tasks: maritime object detection and the more general maritime\nobstacle segmentation. We present a new diverse maritime evaluation dataset\ncontaining approximately 81k stereo images synchronized with an on-board IMU,\nwith over 60k objects annotated. We propose a new obstacle segmentation\nperformance evaluation protocol that reflects the detection accuracy in a way\nmeaningful for practical USV navigation. Seventeen recent state-of-the-art\nobject detection and obstacle segmentation methods are evaluated using the\nproposed protocol, creating a benchmark to facilitate development of the field.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:40:27 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bovcon", "Borja", ""], ["Muhovi\u010d", "Jon", ""], ["Vranac", "Du\u0161ko", ""], ["Mozeti\u010d", "Dean", ""], ["Per\u0161", "Janez", ""], ["Kristan", "Matej", ""]]}, {"id": "2105.02391", "submitter": "Zeliang Song", "authors": "Zeliang Song, Xiaofei Zhou", "title": "Exploring Explicit and Implicit Visual Relationships for Image\n  Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is one of the most challenging tasks in AI, which aims to\nautomatically generate textual sentences for an image. Recent methods for image\ncaptioning follow encoder-decoder framework that transforms the sequence of\nsalient regions in an image into natural language descriptions. However, these\nmodels usually lack the comprehensive understanding of the contextual\ninteractions reflected on various visual relationships between objects. In this\npaper, we explore explicit and implicit visual relationships to enrich\nregion-level representations for image captioning. Explicitly, we build\nsemantic graph over object pairs and exploit gated graph convolutional networks\n(Gated GCN) to selectively aggregate local neighbors' information. Implicitly,\nwe draw global interactions among the detected objects through region-based\nbidirectional encoder representations from transformers (Region BERT) without\nextra relational annotations. To evaluate the effectiveness and superiority of\nour proposed method, we conduct extensive experiments on Microsoft COCO\nbenchmark and achieve remarkable improvements compared with strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 01:47:51 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Song", "Zeliang", ""], ["Zhou", "Xiaofei", ""]]}, {"id": "2105.02400", "submitter": "Jaehyup Lee", "authors": "Jaehyup Lee, Soomin Seo and Munchurl Kim", "title": "SIPSA-Net: Shift-Invariant Pan Sharpening with Moving Object Alignment\n  for Satellite Imagery", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pan-sharpening is a process of merging a high-resolution (HR) panchromatic\n(PAN) image and its corresponding low-resolution (LR) multi-spectral (MS) image\nto create an HR-MS and pan-sharpened image. However, due to the different\nsensors' locations, characteristics and acquisition time, PAN and MS image\npairs often tend to have various amounts of misalignment. Conventional\ndeep-learning-based methods that were trained with such misaligned PAN-MS image\npairs suffer from diverse artifacts such as double-edge and blur artifacts in\nthe resultant PAN-sharpened images. In this paper, we propose a novel framework\ncalled shift-invariant pan-sharpening with moving object alignment (SIPSA-Net)\nwhich is the first method to take into account such large misalignment of\nmoving object regions for PAN sharpening. The SISPA-Net has a feature alignment\nmodule (FAM) that can adjust one feature to be aligned to another feature, even\nbetween the two different PAN and MS domains. For better alignment in\npan-sharpened images, a shift-invariant spectral loss is newly designed, which\nignores the inherent misalignment in the original MS input, thereby having the\nsame effect as optimizing the spectral loss with a well-aligned MS image.\nExtensive experimental results show that our SIPSA-Net can generate\npan-sharpened images with remarkable improvements in terms of visual quality\nand alignment, compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 02:27:50 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lee", "Jaehyup", ""], ["Seo", "Soomin", ""], ["Kim", "Munchurl", ""]]}, {"id": "2105.02406", "submitter": "Wei Yao", "authors": "Jacquelyn Shelton, Przemyslaw Polewski and Wei Yao", "title": "In the Danger Zone: U-Net Driven Quantile Regression can Predict\n  High-risk SARS-CoV-2 Regions via Pollutant Particulate Matter and Satellite\n  Imagery", "comments": "accepted for ICML 2020 Workshop on Healthcare Systems, Population\n  Health, and the Role of Health-Tech", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the outbreak of COVID-19 policy makers have been relying upon\nnon-pharmacological interventions to control the outbreak. With air pollution\nas a potential transmission vector there is need to include it in intervention\nstrategies. We propose a U-net driven quantile regression model to predict\n$PM_{2.5}$ air pollution based on easily obtainable satellite imagery. We\ndemonstrate that our approach can reconstruct $PM_{2.5}$ concentrations on\nground-truth data and predict reasonable $PM_{2.5}$ values with their spatial\ndistribution, even for locations where pollution data is unavailable. Such\npredictions of $PM_{2.5}$ characteristics could crucially advise public policy\nstrategies geared to reduce the transmission of and lethality of COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 02:50:54 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Shelton", "Jacquelyn", ""], ["Polewski", "Przemyslaw", ""], ["Yao", "Wei", ""]]}, {"id": "2105.02408", "submitter": "Jinpu Zhang", "authors": "Jinpu Zhang and Yuehuan Wang", "title": "Spatio-Temporal Matching for Siamese Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity matching is a core operation in Siamese trackers. Most Siamese\ntrackers carry out similarity learning via cross correlation that originates\nfrom the image matching field. However, unlike 2-D image matching, the matching\nnetwork in object tracking requires 4-D information (height, width, channel and\ntime). Cross correlation neglects the information from channel and time\ndimensions, and thus produces ambiguous matching. This paper proposes a\nspatio-temporal matching process to thoroughly explore the capability of 4-D\nmatching in space (height, width and channel) and time. In spatial matching, we\nintroduce a space-variant channel-guided correlation (SVC-Corr) to recalibrate\nchannel-wise feature responses for each spatial location, which can guide the\ngeneration of the target-aware matching features. In temporal matching, we\ninvestigate the time-domain context relations of the target and the background\nand develop an aberrance repressed module (ARM). By restricting the abrupt\nalteration in the interframe response maps, our ARM can clearly suppress\naberrances and thus enables more robust and accurate object tracking.\nFurthermore, a novel anchor-free tracking framework is presented to accommodate\nthese innovations. Experiments on challenging benchmarks including OTB100,\nVOT2018, VOT2020, GOT-10k, and LaSOT demonstrate the state-of-the-art\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 02:55:58 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhang", "Jinpu", ""], ["Wang", "Yuehuan", ""]]}, {"id": "2105.02412", "submitter": "Wenqi Zhao", "authors": "Wenqi Zhao, Liangcai Gao, Zuoyu Yan, Shuai Peng, Lin Du, Ziyin Zhang", "title": "Handwritten Mathematical Expression Recognition with Bidirectionally\n  Trained Transformer", "comments": "Accept by ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder models have made great progress on handwritten mathematical\nexpression recognition recently. However, it is still a challenge for existing\nmethods to assign attention to image features accurately. Moreover, those\nencoder-decoder models usually adopt RNN-based models in their decoder part,\nwhich makes them inefficient in processing long $\\LaTeX{}$ sequences. In this\npaper, a transformer-based decoder is employed to replace RNN-based ones, which\nmakes the whole model architecture very concise. Furthermore, a novel training\nstrategy is introduced to fully exploit the potential of the transformer in\nbidirectional language modeling. Compared to several methods that do not use\ndata augmentation, experiments demonstrate that our model improves the ExpRate\nof current state-of-the-art methods on CROHME 2014 by 2.23%. Similarly, on\nCROHME 2016 and CROHME 2019, we improve the ExpRate by 1.92% and 2.28%\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:11:54 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 17:00:55 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 08:47:18 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhao", "Wenqi", ""], ["Gao", "Liangcai", ""], ["Yan", "Zuoyu", ""], ["Peng", "Shuai", ""], ["Du", "Lin", ""], ["Zhang", "Ziyin", ""]]}, {"id": "2105.02414", "submitter": "Mehul S. Raval", "authors": "Hiren Galiyawala, Mehul S Raval", "title": "Person Retrieval in Surveillance Using Textual Query: A Review", "comments": "45 pages, 17 figures, 6 Tables", "journal-ref": "Springer Multimedia Tools and Application, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancement of research in biometrics, computer vision, and natural\nlanguage processing has discovered opportunities for person retrieval from\nsurveillance videos using textual query. The prime objective of a surveillance\nsystem is to locate a person using a description, e.g., a short woman with a\npink t-shirt and white skirt carrying a black purse. She has brown hair. Such a\ndescription contains attributes like gender, height, type of clothing, colour\nof clothing, hair colour, and accessories. Such attributes are formally known\nas soft biometrics. They help bridge the semantic gap between a human\ndescription and a machine as a textual query contains the person's soft\nbiometric attributes. It is also not feasible to manually search through huge\nvolumes of surveillance footage to retrieve a specific person. Hence, automatic\nperson retrieval using vision and language-based algorithms is becoming\npopular. In comparison to other state-of-the-art reviews, the contribution of\nthe paper is as follows: 1. Recommends most discriminative soft biometrics for\nspecifiic challenging conditions. 2. Integrates benchmark datasets and\nretrieval methods for objective performance evaluation. 3. A complete snapshot\nof techniques based on features, classifiers, number of soft biometric\nattributes, type of the deep neural networks, and performance measures. 4. The\ncomprehensive coverage of person retrieval from handcrafted features based\nmethods to end-to-end approaches based on natural language description.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:17:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Galiyawala", "Hiren", ""], ["Raval", "Mehul S", ""]]}, {"id": "2105.02426", "submitter": "Gaoang Wang", "authors": "Gaoang Wang, Yizhou Wang, Renshu Gu, Weijie Hu, Jenq-Neng Hwang", "title": "Split and Connect: A Universal Tracklet Booster for Multi-Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-object tracking (MOT) is an essential task in the computer vision\nfield. With the fast development of deep learning technology in recent years,\nMOT has achieved great improvement. However, some challenges still remain, such\nas sensitiveness to occlusion, instability under different lighting conditions,\nnon-robustness to deformable objects, etc. To address such common challenges in\nmost of the existing trackers, in this paper, a tracklet booster algorithm is\nproposed, which can be built upon any other tracker. The motivation is simple\nand straightforward: split tracklets on potential ID-switch positions and then\nconnect multiple tracklets into one if they are from the same object. In other\nwords, the tracklet booster consists of two parts, i.e., Splitter and\nConnector. First, an architecture with stacked temporal dilated convolution\nblocks is employed for the splitting position prediction via label smoothing\nstrategy with adaptive Gaussian kernels. Then, a multi-head self-attention\nbased encoder is exploited for the tracklet embedding, which is further used to\nconnect tracklets into larger groups. We conduct sufficient experiments on\nMOT17 and MOT20 benchmark datasets, which demonstrates promising results.\nCombined with the proposed tracklet booster, existing trackers usually can\nachieve large improvements on the IDF1 score, which shows the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:49:19 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Gaoang", ""], ["Wang", "Yizhou", ""], ["Gu", "Renshu", ""], ["Hu", "Weijie", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "2105.02431", "submitter": "Jingtan Piao", "authors": "Jingtan Piao, Keqiang Sun, KwanYee Lin, Quan Wang, Hongsheng Li", "title": "Inverting Generative Adversarial Renderer for Face Reconstruction", "comments": "cvpr2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a monocular face image as input, 3D face geometry reconstruction aims\nto recover a corresponding 3D face mesh. Recently, both optimization-based and\nlearning-based face reconstruction methods have taken advantage of the emerging\ndifferentiable renderer and shown promising results. However, the\ndifferentiable renderer, mainly based on graphics rules, simplifies the\nrealistic mechanism of the illumination, reflection, \\etc, of the real world,\nthus cannot produce realistic images. This brings a lot of domain-shift noise\nto the optimization or training process. In this work, we introduce a novel\nGenerative Adversarial Renderer (GAR) and propose to tailor its inverted\nversion to the general fitting pipeline, to tackle the above problem.\nSpecifically, the carefully designed neural renderer takes a face normal map\nand a latent code representing other factors as inputs and renders a realistic\nface image. Since the GAR learns to model the complicated real-world image,\ninstead of relying on the simplified graphics rules, it is capable of producing\nrealistic images, which essentially inhibits the domain-shift noise in training\nand optimization. Equipped with the elaborated GAR, we further proposed a novel\napproach to predict 3D face parameters, in which we first obtain fine initial\nparameters via Renderer Inverting and then refine it with gradient-based\noptimizers. Extensive experiments have been conducted to demonstrate the\neffectiveness of the proposed generative adversarial renderer and the novel\noptimization-based face reconstruction framework. Our method achieves\nstate-of-the-art performances on multiple face reconstruction datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 04:16:06 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 04:44:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Piao", "Jingtan", ""], ["Sun", "Keqiang", ""], ["Lin", "KwanYee", ""], ["Wang", "Quan", ""], ["Li", "Hongsheng", ""]]}, {"id": "2105.02432", "submitter": "Taotao Jing", "authors": "Taotao Jing, Hongfu Liu, Zhengming Ding", "title": "Towards Novel Target Discovery Through Open-Set Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-set domain adaptation (OSDA) considers that the target domain contains\nsamples from novel categories unobserved in external source domain.\nUnfortunately, existing OSDA methods always ignore the demand for the\ninformation of unseen categories and simply recognize them as \"unknown\" set\nwithout further explanation. This motivates us to understand the unknown\ncategories more specifically by exploring the underlying structures and\nrecovering their interpretable semantic attributes. In this paper, we propose a\nnovel framework to accurately identify the seen categories in target domain,\nand effectively recover the semantic attributes for unseen categories.\nSpecifically, structure preserving partial alignment is developed to recognize\nthe seen categories through domain-invariant feature learning. Attribute\npropagation over visual graph is designed to smoothly transit attributes from\nseen to unseen categories via visual-semantic mapping. Moreover, two new\ncross-main benchmarks are constructed to evaluate the proposed framework in the\nnovel and practical challenge. Experimental results on open-set recognition and\nsemantic recovery demonstrate the superiority of the proposed method over other\ncompared baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 04:22:29 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 22:32:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jing", "Taotao", ""], ["Liu", "Hongfu", ""], ["Ding", "Zhengming", ""]]}, {"id": "2105.02439", "submitter": "Satya Krishna Gorti", "authors": "Junwei Ma, Satya Krishna Gorti, Maksims Volkovs, Guangwei Yu", "title": "Weakly Supervised Action Selection Learning in Video", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing actions in video is a core task in computer vision. The weakly\nsupervised temporal localization problem investigates whether this task can be\nadequately solved with only video-level labels, significantly reducing the\namount of expensive and error-prone annotation that is required. A common\napproach is to train a frame-level classifier where frames with the highest\nclass probability are selected to make a video-level prediction. Frame level\nactivations are then used for localization. However, the absence of frame-level\nannotations cause the classifier to impart class bias on every frame. To\naddress this, we propose the Action Selection Learning (ASL) approach to\ncapture the general concept of action, a property we refer to as \"actionness\".\nUnder ASL, the model is trained with a novel class-agnostic task to predict\nwhich frames will be selected by the classifier. Empirically, we show that ASL\noutperforms leading baselines on two popular benchmarks THUMOS-14 and\nActivityNet-1.2, with 10.3% and 5.7% relative improvement respectively. We\nfurther analyze the properties of ASL and demonstrate the importance of\nactionness. Full code for this work is available here:\nhttps://github.com/layer6ai-labs/ASL.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 04:39:29 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ma", "Junwei", ""], ["Gorti", "Satya Krishna", ""], ["Volkovs", "Maksims", ""], ["Yu", "Guangwei", ""]]}, {"id": "2105.02440", "submitter": "Dawei Du", "authors": "Longyin Wen, Dawei Du, Pengfei Zhu, Qinghua Hu, Qilong Wang, Liefeng\n  Bo, Siwei Lyu", "title": "Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark", "comments": "Accpted to CVPR 2021. Dataset and codes can be found in\n  https://github.com/VisDrone/DroneCrowd. arXiv admin note: text overlap with\n  arXiv:1912.01811", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To promote the developments of object detection, tracking and counting\nalgorithms in drone-captured videos, we construct a benchmark with a new\ndrone-captured largescale dataset, named as DroneCrowd, formed by 112 video\nclips with 33,600 HD frames in various scenarios. Notably, we annotate 20,800\npeople trajectories with 4.8 million heads and several video-level attributes.\nMeanwhile, we design the Space-Time Neighbor-Aware Network (STNNet) as a strong\nbaseline to solve object detection, tracking and counting jointly in dense\ncrowds. STNNet is formed by the feature extraction module, followed by the\ndensity map estimation heads, and localization and association subnets. To\nexploit the context information of neighboring objects, we design the\nneighboring context loss to guide the association subnet training, which\nenforces consistent relative position of nearby objects in temporal domain.\nExtensive experiments on our DroneCrowd dataset demonstrate that STNNet\nperforms favorably against the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 04:46:14 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wen", "Longyin", ""], ["Du", "Dawei", ""], ["Zhu", "Pengfei", ""], ["Hu", "Qinghua", ""], ["Wang", "Qilong", ""], ["Bo", "Liefeng", ""], ["Lyu", "Siwei", ""]]}, {"id": "2105.02451", "submitter": "Peizhuo Li", "authors": "Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga\n  Sorkine-Hornung, Baoquan Chen", "title": "Learning Skeletal Articulations with Neural Blend Shapes", "comments": "SIGGRAPH 2021. Project page:\n  https://peizhuoli.github.io/neural-blend-shapes/ , Video:\n  https://youtu.be/antc20EFh6k", "journal-ref": null, "doi": "10.1145/3450626.3459852", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animating a newly designed character using motion capture (mocap) data is a\nlong standing problem in computer animation. A key consideration is the\nskeletal structure that should correspond to the available mocap data, and the\nshape deformation in the joint regions, which often requires a tailored,\npose-specific refinement. In this work, we develop a neural technique for\narticulating 3D characters using enveloping with a pre-defined skeletal\nstructure which produces high quality pose dependent deformations. Our\nframework learns to rig and skin characters with the same articulation\nstructure (e.g., bipeds or quadrupeds), and builds the desired skeleton\nhierarchy into the network architecture. Furthermore, we propose neural blend\nshapes--a set of corrective pose-dependent shapes which improve the deformation\nquality in the joint regions in order to address the notorious artifacts\nresulting from standard rigging and skinning. Our system estimates neural blend\nshapes for input meshes with arbitrary connectivity, as well as weighting\ncoefficients which are conditioned on the input joint rotations. Unlike recent\ndeep learning techniques which supervise the network with ground-truth rigging\nand skinning parameters, our approach does not assume that the training data\nhas a specific underlying deformation model. Instead, during training, the\nnetwork observes deformed shapes and learns to infer the corresponding rig,\nskin and blend shapes using indirect supervision. During inference, we\ndemonstrate that our network generalizes to unseen characters with arbitrary\nmesh connectivity, including unrigged characters built by 3D artists.\nConforming to standard skeletal animation models enables direct plug-and-play\nin standard animation software, as well as game engines.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 05:58:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Peizhuo", ""], ["Aberman", "Kfir", ""], ["Hanocka", "Rana", ""], ["Liu", "Libin", ""], ["Sorkine-Hornung", "Olga", ""], ["Chen", "Baoquan", ""]]}, {"id": "2105.02453", "submitter": "Taiping Yao", "authors": "Zhihong Chen, Taiping Yao, Kekai Sheng, Shouhong Ding, Ying Tai, Jilin\n  Li, Feiyue Huang, Xinyu Jin", "title": "Generalizable Representation Learning for Mixture Domain Face\n  Anti-Spoofing", "comments": "Accepted for publication in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing approach based on domain generalization(DG) has drawn\ngrowing attention due to its robustness forunseen scenarios. Existing DG\nmethods assume that the do-main label is known.However, in real-world\napplications, thecollected dataset always contains mixture domains, where\nthedomain label is unknown. In this case, most of existing meth-ods may not\nwork. Further, even if we can obtain the domainlabel as existing methods, we\nthink this is just a sub-optimalpartition. To overcome the limitation, we\npropose domain dy-namic adjustment meta-learning (D2AM) without using do-main\nlabels, which iteratively divides mixture domains viadiscriminative domain\nrepresentation and trains a generaliz-able face anti-spoofing with\nmeta-learning. Specifically, wedesign a domain feature based on Instance\nNormalization(IN) and propose a domain representation learning module(DRLM) to\nextract discriminative domain features for cluster-ing. Moreover, to reduce the\nside effect of outliers on cluster-ing performance, we additionally utilize\nmaximum mean dis-crepancy (MMD) to align the distribution of sample featuresto\na prior distribution, which improves the reliability of clus tering. Extensive\nexperiments show that the proposed methodoutperforms conventional DG-based face\nanti-spoofing meth-ods, including those utilizing domain labels. Furthermore,\nweenhance the interpretability through visualizatio\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 06:04:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chen", "Zhihong", ""], ["Yao", "Taiping", ""], ["Sheng", "Kekai", ""], ["Ding", "Shouhong", ""], ["Tai", "Ying", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Jin", "Xinyu", ""]]}, {"id": "2105.02460", "submitter": "Manh Duong Phung", "authors": "Manh Duong Phung, Cong Hoang Quach and Quang Vinh Tran", "title": "Development of a Fast and Robust Gaze Tracking System for Game\n  Applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1611.09427", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, a novel eye tracking system using a visual camera is developed\nto extract human's gaze, and it can be used in modern game machines to bring\nnew and innovative interactive experience to players. Central to the components\nof the system, is a robust iris-center and eye-corner detection algorithm\nbasing on it the gaze is continuously and adaptively extracted. Evaluation\ntests were applied to nine people to evaluate the accuracy of the system and\nthe results were 2.50 degrees (view angle) in horizontal direction and 3.07\ndegrees in vertical direction.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 06:41:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Phung", "Manh Duong", ""], ["Quach", "Cong Hoang", ""], ["Tran", "Quang Vinh", ""]]}, {"id": "2105.02465", "submitter": "Jianfeng Zhang", "authors": "Kehong Gong, Jianfeng Zhang, Jiashi Feng", "title": "PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose\n  Estimation", "comments": "CVPR 2021 Oral Paper, code available:\n  https://github.com/jfzhang95/PoseAug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing 3D human pose estimators suffer poor generalization performance to\nnew datasets, largely due to the limited diversity of 2D-3D pose pairs in the\ntraining data. To address this problem, we present PoseAug, a new\nauto-augmentation framework that learns to augment the available training poses\ntowards a greater diversity and thus improve generalization of the trained\n2D-to-3D pose estimator. Specifically, PoseAug introduces a novel pose\naugmentor that learns to adjust various geometry factors (e.g., posture, body\nsize, view point and position) of a pose through differentiable operations.\nWith such differentiable capacity, the augmentor can be jointly optimized with\nthe 3D pose estimator and take the estimation error as feedback to generate\nmore diverse and harder poses in an online manner. Moreover, PoseAug introduces\na novel part-aware Kinematic Chain Space for evaluating local joint-angle\nplausibility and develops a discriminative module accordingly to ensure the\nplausibility of the augmented poses. These elaborate designs enable PoseAug to\ngenerate more diverse yet plausible poses than existing offline augmentation\nmethods, and thus yield better generalization of the pose estimator. PoseAug is\ngeneric and easy to be applied to various 3D pose estimators. Extensive\nexperiments demonstrate that PoseAug brings clear improvements on both\nintra-scenario and cross-scenario datasets. Notably, it achieves 88.6% 3D PCK\non MPI-INF-3DHP under cross-dataset evaluation setup, improving upon the\nprevious best data augmentation based method by 9.1%. Code can be found at:\nhttps://github.com/jfzhang95/PoseAug.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 06:57:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Gong", "Kehong", ""], ["Zhang", "Jianfeng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2105.02467", "submitter": "Jianfeng Zhang", "authors": "Jianfeng Zhang, Dongdong Yu, Jun Hao Liew, Xuecheng Nie, Jiashi Feng", "title": "Body Meshes as Points", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the challenging multi-person 3D body mesh estimation task in this\nwork. Existing methods are mostly two-stage based--one stage for person\nlocalization and the other stage for individual body mesh estimation, leading\nto redundant pipelines with high computation cost and degraded performance for\ncomplex scenes (e.g., occluded person instances). In this work, we present a\nsingle-stage model, Body Meshes as Points (BMP), to simplify the pipeline and\nlift both efficiency and performance. In particular, BMP adopts a new method\nthat represents multiple person instances as points in the spatial-depth space\nwhere each point is associated with one body mesh. Hinging on such\nrepresentations, BMP can directly predict body meshes for multiple persons in a\nsingle stage by concurrently localizing person instance points and estimating\nthe corresponding body meshes. To better reason about depth ordering of all the\npersons within the same scene, BMP designs a simple yet effective\ninter-instance ordinal depth loss to obtain depth-coherent body mesh\nestimation. BMP also introduces a novel keypoint-aware augmentation to enhance\nmodel robustness to occluded person instances. Comprehensive experiments on\nbenchmarks Panoptic, MuPoTS-3D and 3DPW clearly demonstrate the\nstate-of-the-art efficiency of BMP for multi-person body mesh estimation,\ntogether with outstanding accuracy. Code can be found at:\nhttps://github.com/jfzhang95/BMP.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 06:58:38 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 06:04:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Jianfeng", ""], ["Yu", "Dongdong", ""], ["Liew", "Jun Hao", ""], ["Nie", "Xuecheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2105.02468", "submitter": "Leonardo Petrini", "authors": "Leonardo Petrini, Alessandro Favero, Mario Geiger, Matthieu Wyart", "title": "Relative stability toward diffeomorphisms indicates performance in deep\n  nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why deep nets can classify data in large dimensions remains a\nchallenge. It has been proposed that they do so by becoming stable to\ndiffeomorphisms, yet existing empirical measurements support that it is often\nnot the case. We revisit this question by defining a maximum-entropy\ndistribution on diffeomorphisms, that allows to study typical diffeomorphisms\nof a given norm. We confirm that stability toward diffeomorphisms does not\nstrongly correlate to performance on benchmark data sets of images. By\ncontrast, we find that the stability toward diffeomorphisms relative to that of\ngeneric transformations $R_f$ correlates remarkably with the test error\n$\\epsilon_t$. It is of order unity at initialization but decreases by several\ndecades during training for state-of-the-art architectures. For CIFAR10 and 15\nknown architectures, we find $\\epsilon_t\\approx 0.2\\sqrt{R_f}$, suggesting that\nobtaining a small $R_f$ is important to achieve good performance. We study how\n$R_f$ depends on the size of the training set and compare it to a simple model\nof invariant learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:03:30 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 13:18:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Petrini", "Leonardo", ""], ["Favero", "Alessandro", ""], ["Geiger", "Mario", ""], ["Wyart", "Matthieu", ""]]}, {"id": "2105.02480", "submitter": "Zhenbang Li", "authors": "Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang,\n  Weiming Hu", "title": "A Simple and Strong Baseline for Universal Targeted Attacks on Siamese\n  Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Siamese trackers are shown to be vulnerable to adversarial attacks recently.\nHowever, the existing attack methods craft the perturbations for each video\nindependently, which comes at a non-negligible computational cost. In this\npaper, we show the existence of universal perturbations that can enable the\ntargeted attack, e.g., forcing a tracker to follow the ground-truth trajectory\nwith specified offsets, to be video-agnostic and free from inference in a\nnetwork. Specifically, we attack a tracker by adding a universal imperceptible\nperturbation to the template image and adding a fake target, i.e., a small\nuniversal adversarial patch, into the search images adhering to the predefined\ntrajectory, so that the tracker outputs the location and size of the fake\ntarget instead of the real target. Our approach allows perturbing a novel video\nto come at no additional cost except the mere addition operations -- and not\nrequire gradient optimization or network inference. Experimental results on\nseveral datasets demonstrate that our approach can effectively fool the Siamese\ntrackers in a targeted attack manner. We show that the proposed perturbations\nare not only universal across videos, but also generalize well across different\ntrackers. Such perturbations are therefore doubly universal, both with respect\nto the data and the network architectures. We will make our code publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:26:36 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Zhenbang", ""], ["Shi", "Yaya", ""], ["Gao", "Jin", ""], ["Wang", "Shaoru", ""], ["Li", "Bing", ""], ["Liang", "Pengpeng", ""], ["Hu", "Weiming", ""]]}, {"id": "2105.02481", "submitter": "Fabio Valerio Massoli", "authors": "Fabio Valerio Massoli, Donato Cafarelli, Claudio Gennaro, Giuseppe\n  Amato, Fabrizio Falchi", "title": "MAFER: a Multi-resolution Approach to Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Emotions play a central role in the social life of every human being, and\ntheir study, which represents a multidisciplinary subject, embraces a great\nvariety of research fields. Especially concerning the latter, the analysis of\nfacial expressions represents a very active research area due to its relevance\nto human-computer interaction applications. In such a context, Facial\nExpression Recognition (FER) is the task of recognizing expressions on human\nfaces. Typically, face images are acquired by cameras that have, by nature,\ndifferent characteristics, such as the output resolution. It has been already\nshown in the literature that Deep Learning models applied to face recognition\nexperience a degradation in their performance when tested against\nmulti-resolution scenarios. Since the FER task involves analyzing face images\nthat can be acquired with heterogeneous sources, thus involving images with\ndifferent quality, it is plausible to expect that resolution plays an important\nrole in such a case too. Stemming from such a hypothesis, we prove the benefits\nof multi-resolution training for models tasked with recognizing facial\nexpressions. Hence, we propose a two-step learning procedure, named MAFER, to\ntrain DCNNs to empower them to generate robust predictions across a wide range\nof resolutions. A relevant feature of MAFER is that it is task-agnostic, i.e.,\nit can be used complementarily to other objective-related techniques. To assess\nthe effectiveness of the proposed approach, we performed an extensive\nexperimental campaign on publicly available datasets: \\fer{}, \\raf{}, and\n\\oulu{}. For a multi-resolution context, we observe that with our approach,\nlearning models improve upon the current SotA while reporting comparable\nresults in fix-resolution contexts. Finally, we analyze the performance of our\nmodels and observe the higher discrimination power of deep features generated\nfrom them.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:26:58 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Massoli", "Fabio Valerio", ""], ["Cafarelli", "Donato", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "2105.02489", "submitter": "Tianyuan Huang", "authors": "Tianyuan Huang, Zhecheng Wang, Hao Sheng, Andrew Y. Ng, Ram Rajagopal", "title": "Learning Neighborhood Representation from Multi-Modal Multi-Graph:\n  Image, Text, Mobility Graph and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent urbanization has coincided with the enrichment of geotagged data, such\nas street view and point-of-interest (POI). Region embedding enhanced by the\nricher data modalities has enabled researchers and city administrators to\nunderstand the built environment, socioeconomics, and the dynamics of cities\nbetter. While some efforts have been made to simultaneously use multi-modal\ninputs, existing methods can be improved by incorporating different measures of\n'proximity' in the same embedding space - leveraging not only the data that\ncharacterizes the regions (e.g., street view, local businesses pattern) but\nalso those that depict the relationship between regions (e.g., trips, road\nnetwork). To this end, we propose a novel approach to integrate multi-modal\ngeotagged inputs as either node or edge features of a multi-graph based on\ntheir relations with the neighborhood region (e.g., tiles, census block, ZIP\ncode region, etc.). We then learn the neighborhood representation based on a\ncontrastive-sampling scheme from the multi-graph. Specifically, we use street\nview images and POI features to characterize neighborhoods (nodes) and use\nhuman mobility to characterize the relationship between neighborhoods (directed\nedges). We show the effectiveness of the proposed methods with quantitative\ndownstream tasks as well as qualitative analysis of the embedding space: The\nembedding we trained outperforms the ones using only unimodal data as regional\ninputs.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:44:05 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Huang", "Tianyuan", ""], ["Wang", "Zhecheng", ""], ["Sheng", "Hao", ""], ["Ng", "Andrew Y.", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2105.02498", "submitter": "Yue Song", "authors": "Yue Song, Nicu Sebe, Wei Wang", "title": "Why Approximate Matrix Square Root Outperforms Accurate SVD in Global\n  Covariance Pooling?", "comments": "Accepted by ICCV21 as poster presetation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global covariance pooling (GCP) aims at exploiting the second-order\nstatistics of the convolutional feature. Its effectiveness has been\ndemonstrated in boosting the classification performance of Convolutional Neural\nNetworks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute\nthe matrix square root. However, the approximate matrix square root calculated\nusing Newton-Schulz iteration \\cite{li2018towards} outperforms the accurate one\ncomputed via SVD \\cite{li2017second}. We empirically analyze the reason behind\nthe performance gap from the perspectives of data precision and gradient\nsmoothness. Various remedies for computing smooth SVD gradients are\ninvestigated. Based on our observation and analyses, a hybrid training protocol\nis proposed for SVD-based GCP meta-layers such that competitive performances\ncan be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP\nmeta-layer that uses SVD in the forward pass, and Pad\\'e Approximants in the\nbackward propagation to compute the gradients. The proposed meta-layer has been\nintegrated into different CNN models and achieves state-of-the-art performances\non both large-scale and fine-grained datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:03:45 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 09:52:15 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Song", "Yue", ""], ["Sebe", "Nicu", ""], ["Wang", "Wei", ""]]}, {"id": "2105.02501", "submitter": "Fan Bai", "authors": "Fan Bai, Jiaxiang Wu, Pengcheng Shen, Shaoxin Li and Shuigeng Zhou", "title": "Federated Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been extensively studied in computer vision and\nartificial intelligence communities in recent years. An important issue of face\nrecognition is data privacy, which receives more and more public concerns. As a\ncommon privacy-preserving technique, Federated Learning is proposed to train a\nmodel cooperatively without sharing data between parties. However, as far as we\nknow, it has not been successfully applied in face recognition. This paper\nproposes a framework named FedFace to innovate federated learning for face\nrecognition. Specifically, FedFace relies on two major innovative algorithms,\nPartially Federated Momentum (PFM) and Federated Validation (FV). PFM locally\napplies an estimated equivalent global momentum to approximating the\ncentralized momentum-SGD efficiently. FV repeatedly searches for better\nfederated aggregating weightings via testing the aggregated models on some\nprivate validation datasets, which can improve the model's generalization\nability. The ablation study and extensive experiments validate the\neffectiveness of the FedFace method and show that it is comparable to or even\nbetter than the centralized baseline in performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:07:25 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bai", "Fan", ""], ["Wu", "Jiaxiang", ""], ["Shen", "Pengcheng", ""], ["Li", "Shaoxin", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "2105.02531", "submitter": "Seyed Mehdi Ayyoubzadeh", "authors": "Seyed Mehdi Ayyoubzadeh, Ali Royat", "title": "(ASNA) An Attention-based Siamese-Difference Neural Network with\n  Surrogate Ranking Loss function for Perceptual Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks (DCNN) that leverage the\nadversarial training framework for image restoration and enhancement have\nsignificantly improved the processed images' sharpness. Surprisingly, although\nthese DCNNs produced crispier images than other methods visually, they may get\na lower quality score when popular measures are employed for evaluating them.\nTherefore it is necessary to develop a quantitative metric to reflect their\nperformances, which is well-aligned with the perceived quality of an image.\nFamous quantitative metrics such as Peak signal-to-noise ratio (PSNR), The\nstructural similarity index measure (SSIM), and Perceptual Index (PI) are not\nwell-correlated with the mean opinion score (MOS) for an image, especially for\nthe neural networks trained with adversarial loss functions.\n  This paper has proposed a convolutional neural network using an extension\narchitecture of the traditional Siamese network so-called Siamese-Difference\nneural network. We have equipped this architecture with the spatial and\nchannel-wise attention mechanism to increase our method's performance.\n  Finally, we employed an auxiliary loss function to train our model. The\nsuggested additional cost function surrogates ranking loss to increase\nSpearman's rank correlation coefficient while it is differentiable concerning\nthe neural network parameters. Our method achieved superior performance in\n\\textbf{\\textit{NTIRE 2021 Perceptual Image Quality Assessment}} Challenge. The\nimplementations of our proposed method are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:04:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ayyoubzadeh", "Seyed Mehdi", ""], ["Royat", "Ali", ""]]}, {"id": "2105.02545", "submitter": "Chong Luo", "authors": "Guangting Wang, Yizhou Zhou, Chong Luo, Wenxuan Xie, Wenjun Zeng, and\n  Zhiwei Xiong", "title": "Unsupervised Visual Representation Learning by Tracking Patches in Video", "comments": "To appear in CVPR'21. Code available at github.com/microsoft/CtP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inspired by the fact that human eyes continue to develop tracking ability in\nearly and middle childhood, we propose to use tracking as a proxy task for a\ncomputer vision system to learn the visual representations. Modelled on the\nCatch game played by the children, we design a Catch-the-Patch (CtP) game for a\n3D-CNN model to learn visual representations that would help with video-related\ntasks. In the proposed pretraining framework, we cut an image patch from a\ngiven video and let it scale and move according to a pre-set trajectory. The\nproxy task is to estimate the position and size of the image patch in a\nsequence of video frames, given only the target bounding box in the first\nframe. We discover that using multiple image patches simultaneously brings\nclear benefits. We further increase the difficulty of the game by randomly\nmaking patches invisible. Extensive experiments on mainstream benchmarks\ndemonstrate the superior performance of CtP against other video pretraining\nmethods. In addition, CtP-pretrained features are less sensitive to domain gaps\nthan those trained by a supervised action recognition task. When both trained\non Kinetics-400, we are pleasantly surprised to find that CtP-pretrained\nrepresentation achieves much higher action classification accuracy than its\nfully supervised counterpart on Something-Something dataset. Code is available\nonline: github.com/microsoft/CtP.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:46:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Guangting", ""], ["Zhou", "Yizhou", ""], ["Luo", "Chong", ""], ["Xie", "Wenxuan", ""], ["Zeng", "Wenjun", ""], ["Xiong", "Zhiwei", ""]]}, {"id": "2105.02566", "submitter": "Francesca Lizzi", "authors": "Francesca Lizzi, Abramo Agosti, Francesca Brero, Raffaella Fiamma\n  Cabini, Maria Evelina Fantacci, Silvia Figini, Alessandro Lascialfari,\n  Francesco Laruina, Piernicola Oliva, Stefano Piffer, Ian Postuma, Lisa\n  Rinaldi, Cinzia Talamonti, Alessandra Retico", "title": "Quantification of pulmonary involvement in COVID-19 pneumonia by means\n  of a cascade oftwo U-nets: training and assessment on multipledatasets using\n  different annotation criteria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The automatic assignment of a severity score to the CT scans of patients\naffected by COVID-19 pneumonia could reduce the workload in radiology\ndepartments. This study aims at exploiting Artificial intelligence (AI) for the\nidentification, segmentation and quantification of COVID-19 pulmonary lesions.\nWe investigated the effects of using multiple datasets, heterogeneously\npopulated and annotated according to different criteria. We developed an\nautomated analysis pipeline, the LungQuant system, based on a cascade of two\nU-nets. The first one (U-net_1) is devoted to the identification of the lung\nparenchyma, the second one (U-net_2) acts on a bounding box enclosing the\nsegmented lungs to identify the areas affected by COVID-19 lesions. Different\npublic datasets were used to train the U-nets and to evaluate their\nsegmentation performances, which have been quantified in terms of the Dice\nindex. The accuracy in predicting the CT-Severity Score (CT-SS) of the\nLungQuant system has been also evaluated. Both Dice and accuracy showed a\ndependency on the quality of annotations of the available data samples. On an\nindependent and publicly available benchmark dataset, the Dice values measured\nbetween the masks predicted by LungQuant system and the reference ones were\n0.95$\\pm$0.01 and 0.66$\\pm$0.13 for the segmentation of lungs and COVID-19\nlesions, respectively. The accuracy of 90% in the identification of the CT-SS\non this benchmark dataset was achieved. We analysed the impact of using data\nsamples with different annotation criteria in training an AI-based\nquantification system for pulmonary involvement in COVID-19 pneumonia. In terms\nof the Dice index, the U-net segmentation quality strongly depends on the\nquality of the lesion annotations. Nevertheless, the CT-SS can be accurately\npredicted on independent validation sets, demonstrating the satisfactory\ngeneralization ability of the LungQuant.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:21:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lizzi", "Francesca", ""], ["Agosti", "Abramo", ""], ["Brero", "Francesca", ""], ["Cabini", "Raffaella Fiamma", ""], ["Fantacci", "Maria Evelina", ""], ["Figini", "Silvia", ""], ["Lascialfari", "Alessandro", ""], ["Laruina", "Francesco", ""], ["Oliva", "Piernicola", ""], ["Piffer", "Stefano", ""], ["Postuma", "Ian", ""], ["Rinaldi", "Lisa", ""], ["Talamonti", "Cinzia", ""], ["Retico", "Alessandra", ""]]}, {"id": "2105.02572", "submitter": "Byeongjoon Noh", "authors": "Byeongjoon Noh and Hwasoo Yeo", "title": "A novel method of predictive collision risk area estimation for\n  proactive pedestrian accident prevention system in urban surveillance\n  infrastructure", "comments": "26 pages, 17 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road traffic accidents, especially vehicle pedestrian collisions in\ncrosswalk, globally pose a severe threat to human lives and have become a\nleading cause of premature deaths. In order to protect such vulnerable road\nusers from collisions, it is necessary to recognize possible conflict in\nadvance and warn to road users, not post facto. A breakthrough for proactively\npreventing pedestrian collisions is to recognize pedestrian's potential risks\nbased on vision sensors such as CCTVs. In this study, we propose a predictive\ncollision risk area estimation system at unsignalized crosswalks. The proposed\nsystem applied trajectories of vehicles and pedestrians from video footage\nafter preprocessing, and then predicted their trajectories by using deep LSTM\nnetworks. With use of predicted trajectories, this system can infer collision\nrisk areas statistically, further severity of levels is divided as danger,\nwarning, and relative safe. In order to validate the feasibility and\napplicability of the proposed system, we applied it and assess the severity of\npotential risks in two unsignalized spots in Osan city, Korea.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:29:44 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Noh", "Byeongjoon", ""], ["Yeo", "Hwasoo", ""]]}, {"id": "2105.02577", "submitter": "Shen Chen", "authors": "Shen Chen, Taiping Yao, Yang Chen, Shouhong Ding, Jilin Li, Rongrong\n  Ji", "title": "Local Relation Learning for Face Forgery Detection", "comments": "8 pages, 6 figures, Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of facial manipulation techniques, face forgery\ndetection has received considerable attention in digital media forensics due to\nsecurity concerns. Most existing methods formulate face forgery detection as a\nclassification problem and utilize binary labels or manipulated region masks as\nsupervision. However, without considering the correlation between local\nregions, these global supervisions are insufficient to learn a generalized\nfeature and prone to overfitting. To address this issue, we propose a novel\nperspective of face forgery detection via local relation learning.\nSpecifically, we propose a Multi-scale Patch Similarity Module (MPSM), which\nmeasures the similarity between features of local regions and forms a robust\nand generalized similarity pattern. Moreover, we propose an RGB-Frequency\nAttention Module (RFAM) to fuse information in both RGB and frequency domains\nfor more comprehensive local feature representation, which further improves the\nreliability of the similarity pattern. Extensive experiments show that the\nproposed method consistently outperforms the state-of-the-arts on widely-used\nbenchmarks. Furthermore, detailed visualization shows the robustness and\ninterpretability of our method.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:44:32 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chen", "Shen", ""], ["Yao", "Taiping", ""], ["Chen", "Yang", ""], ["Ding", "Shouhong", ""], ["Li", "Jilin", ""], ["Ji", "Rongrong", ""]]}, {"id": "2105.02582", "submitter": "Byeongjoon Noh", "authors": "Byeongjoon Noh, Dongho Ka, David Lee, and Hwasoo Yeo", "title": "Vision based Pedestrian Potential Risk Analysis based on Automated\n  Behavior Feature Extraction for Smart and Safe City", "comments": "26 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in vehicle safety technologies, road traffic\naccidents still pose a severe threat to human lives and have become a leading\ncause of premature deaths. In particular, crosswalks present a major threat to\npedestrians, but we lack dense behavioral data to investigate the risks they\nface. Therefore, we propose a comprehensive analytical model for pedestrian\npotential risk using video footage gathered by road security cameras deployed\nat such crossings. The proposed system automatically detects vehicles and\npedestrians, calculates trajectories by frames, and extracts behavioral\nfeatures affecting the likelihood of potentially dangerous scenes between these\nobjects. Finally, we design a data cube model by using the large amount of the\nextracted features accumulated in a data warehouse to perform multidimensional\nanalysis for potential risk scenes with levels of abstraction, but this is\nbeyond the scope of this paper, and will be detailed in a future study. In our\nexperiment, we focused on extracting the various behavioral features from\nmultiple crosswalks, and visualizing and interpreting their behaviors and\nrelationships among them by camera location to show how they may or may not\ncontribute to potential risk. We validated feasibility and applicability by\napplying it in multiple crosswalks in Osan city, Korea.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:03:10 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 11:12:12 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Noh", "Byeongjoon", ""], ["Ka", "Dongho", ""], ["Lee", "David", ""], ["Yeo", "Hwasoo", ""]]}, {"id": "2105.02615", "submitter": "Mohammad Samar Ansari", "authors": "Asra Aslam, Ekram Khan, Mohammad Samar Ansari, M.M. Sufyan Beg", "title": "A Novel Falling-Ball Algorithm for Image Segmentation", "comments": "Preprint submitted to Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation refers to the separation of objects from the background,\nand has been one of the most challenging aspects of digital image processing.\nPractically it is impossible to design a segmentation algorithm which has 100%\naccuracy, and therefore numerous segmentation techniques have been proposed in\nthe literature, each with certain limitations. In this paper, a novel\nFalling-Ball algorithm is presented, which is a region-based segmentation\nalgorithm, and an alternative to watershed transform (based on waterfall\nmodel). The proposed algorithm detects the catchment basins by assuming that a\nball falling from hilly terrains will stop in a catchment basin. Once catchment\nbasins are identified, the association of each pixel with one of the catchment\nbasin is obtained using multi-criterion fuzzy logic. Edges are constructed by\ndividing image into different catchment basins with the help of a membership\nfunction. Finally closed contour algorithm is applied to find closed regions\nand objects within closed regions are segmented using intensity information.\nThe performance of the proposed algorithm is evaluated both objectively as well\nas subjectively. Simulation results show that the proposed algorithms gives\nsuperior performance over conventional Sobel edge detection methods and the\nwatershed segmentation algorithm. For comparative analysis, various comparison\nmethods are used for demonstrating the superiority of proposed methods over\nexisting segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 12:41:10 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 16:36:45 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Aslam", "Asra", ""], ["Khan", "Ekram", ""], ["Ansari", "Mohammad Samar", ""], ["Beg", "M. M. Sufyan", ""]]}, {"id": "2105.02626", "submitter": "Xingjian Zhen", "authors": "Varun Nagaraj Rao, Xingjian Zhen, Karen Hovsepian, Mingwei Shen", "title": "A First Look: Towards Explainable TextVQA Models via Visual and Textual\n  Explanations", "comments": "This paper is done when Xingjian was an intern in Amazon PARS group,\n  summer 2020. This paper is accepted by NAACL-MAI-Workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable deep learning models are advantageous in many situations. Prior\nwork mostly provide unimodal explanations through post-hoc approaches not part\nof the original system design. Explanation mechanisms also ignore useful\ntextual information present in images. In this paper, we propose MTXNet, an\nend-to-end trainable multimodal architecture to generate multimodal\nexplanations, which focuses on the text in the image. We curate a novel dataset\nTextVQA-X, containing ground truth visual and multi-reference textual\nexplanations that can be leveraged during both training and evaluation. We then\nquantitatively show that training with multimodal explanations complements\nmodel performance and surpasses unimodal baselines by up to 7% in CIDEr scores\nand 2% in IoU. More importantly, we demonstrate that the multimodal\nexplanations are consistent with human interpretations, help justify the\nmodels' decision, and provide useful insights to help diagnose an incorrect\nprediction. Finally, we describe a real-world e-commerce application for using\nthe generated multimodal explanations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:36:17 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rao", "Varun Nagaraj", ""], ["Zhen", "Xingjian", ""], ["Hovsepian", "Karen", ""], ["Shen", "Mingwei", ""]]}, {"id": "2105.02636", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer and Cigdem Beyan and Fabian Ruth and Olaf Kramer and\n  Ulrich Trautwein and Enkelejda Kasneci", "title": "Estimating Presentation Competence using Multimodal Nonverbal Behavioral\n  Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public speaking and presentation competence plays an essential role in many\nareas of social interaction in our educational, professional, and everyday\nlife. Since our intention during a speech can differ from what is actually\nunderstood by the audience, the ability to appropriately convey our message\nrequires a complex set of skills. Presentation competence is cultivated in the\nearly school years and continuously developed over time. One approach that can\npromote efficient development of presentation competence is the automated\nanalysis of human behavior during a speech based on visual and audio features\nand machine learning. Furthermore, this analysis can be used to suggest\nimprovements and the development of skills related to presentation competence.\nIn this work, we investigate the contribution of different nonverbal behavioral\ncues, namely, facial, body pose-based, and audio-related features, to estimate\npresentation competence. The analyses were performed on videos of 251 students\nwhile the automated assessment is based on manual ratings according to the\nT\\\"ubingen Instrument for Presentation Competence (TIP). Our classification\nresults reached the best performance with early fusion in the same dataset\nevaluation (accuracy of 71.25%) and late fusion of speech, face, and body pose\nfeatures in the cross dataset evaluation (accuracy of 78.11%). Similarly,\nregression results performed the best with fusion strategies.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:09:41 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Beyan", "Cigdem", ""], ["Ruth", "Fabian", ""], ["Kramer", "Olaf", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2105.02646", "submitter": "Xuhui Li", "authors": "Zijian Yu, Xuhui Li, Huijuan Huang, Wen Zheng and Li Chen", "title": "Cascade Image Matting with Deformable Graph Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting refers to the estimation of the opacity of foreground objects.\nIt requires correct contours and fine details of foreground objects for the\nmatting results. To better accomplish human image matting tasks, we propose the\nCascade Image Matting Network with Deformable Graph Refinement, which can\nautomatically predict precise alpha mattes from single human images without any\nadditional inputs. We adopt a network cascade architecture to perform matting\nfrom low-to-high resolution, which corresponds to coarse-to-fine optimization.\nWe also introduce the Deformable Graph Refinement (DGR) module based on graph\nneural networks (GNNs) to overcome the limitations of convolutional neural\nnetworks (CNNs). The DGR module can effectively capture long-range relations\nand obtain more global and local information to help produce finer alpha\nmattes. We also reduce the computation complexity of the DGR module by\ndynamically predicting the neighbors and apply DGR module to higher--resolution\nfeatures. Experimental results demonstrate the ability of our CasDGR to achieve\nstate-of-the-art performance on synthetic datasets and produce good results on\nreal human images.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:26:51 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 03:54:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "Zijian", ""], ["Li", "Xuhui", ""], ["Huang", "Huijuan", ""], ["Zheng", "Wen", ""], ["Chen", "Li", ""]]}, {"id": "2105.02668", "submitter": "Xing Zhang", "authors": "Xing Zhang, Zuxuan Wu, Zejia Weng, Huazhu Fu, Jingjing Chen, Yu-Gang\n  Jiang, Larry Davis", "title": "VideoLT: Large-scale Long-tailed Video Recognition", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label distributions in real-world are oftentimes long-tailed and imbalanced,\nresulting in biased models towards dominant labels. While long-tailed\nrecognition has been extensively studied for image classification tasks,\nlimited effort has been made for video domain. In this paper, we introduce\nVideoLT, a large-scale long-tailed video recognition dataset, as a step toward\nreal-world video recognition. Our VideoLT contains 256,218 untrimmed videos,\nannotated into 1,004 classes with a long-tailed distribution. Through extensive\nstudies, we demonstrate that state-of-the-art methods used for long-tailed\nimage recognition do not perform well in the video domain due to the additional\ntemporal dimension in video data. This motivates us to propose FrameStack, a\nsimple yet effective method for long-tailed video recognition task. In\nparticular, FrameStack performs sampling at the frame-level in order to balance\nclass distributions, and the sampling ratio is dynamically determined using\nknowledge derived from the network during training. Experimental results\ndemonstrate that FrameStack can improve classification performance without\nsacrificing overall accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:47:44 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhang", "Xing", ""], ["Wu", "Zuxuan", ""], ["Weng", "Zejia", ""], ["Fu", "Huazhu", ""], ["Chen", "Jingjing", ""], ["Jiang", "Yu-Gang", ""], ["Davis", "Larry", ""]]}, {"id": "2105.02674", "submitter": "Ran Gu", "authors": "Jingyang Zhang, Ran Gu, Guotai Wang, Hongzhi Xie, Lixu Gu", "title": "SS-CADA: A Semi-Supervised Cross-Anatomy Domain Adaptation for Coronary\n  Artery Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The segmentation of coronary arteries by convolutional neural network is\npromising yet requires a large amount of labor-intensive manual annotations.\nTransferring knowledge from retinal vessels in widely-available public labeled\nfundus images (FIs) has a potential to reduce the annotation requirement for\ncoronary artery segmentation in X-ray angiograms (XAs) due to their common\ntubular structures. However, it is challenged by the cross-anatomy domain shift\ndue to the intrinsically different vesselness characteristics in different\nanatomical regions under even different imaging protocols. To solve this\nproblem, we propose a Semi-Supervised Cross-Anatomy Domain Adaptation (SS-CADA)\nwhich requires only limited annotations for coronary arteries in XAs. With the\nsupervision from a small number of labeled XAs and publicly available labeled\nFIs, we propose a vesselness-specific batch normalization (VSBN) to\nindividually normalize feature maps for them considering their different\ncross-anatomic vesselness characteristics. In addition, to further facilitate\nthe annotation efficiency, we employ a self-ensembling mean-teacher (SEMT) to\nexploit abundant unlabeled XAs by imposing a prediction consistency constraint.\nExtensive experiments show that our SS-CADA is able to solve the challenging\ncross-anatomy domain shift, achieving accurate segmentation for coronary\narteries given only a small number of labeled XAs.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:00:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhang", "Jingyang", ""], ["Gu", "Ran", ""], ["Wang", "Guotai", ""], ["Xie", "Hongzhi", ""], ["Gu", "Lixu", ""]]}, {"id": "2105.02679", "submitter": "Ciaran Eising", "authors": "Paul Moran, Leroy-Francisco Periera, Anbuchezhiyan Selvaraju, Tejash\n  Prakash, Pantelis Ermilios, John McDonald, Jonathan Horgan, Ciar\\'an Eising", "title": "A 2.5D Vehicle Odometry Estimation for Vision Applications", "comments": null, "journal-ref": "Proceedings of the 2020 Irish Machine Vision and Image Processing\n  Conference", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a method to estimate the pose of a sensor mounted on a\nvehicle as the vehicle moves through the world, an important topic for\nautonomous driving systems. Based on a set of commonly deployed vehicular\nodometric sensors, with outputs available on automotive communication buses\n(e.g. CAN or FlexRay), we describe a set of steps to combine a planar odometry\nbased on wheel sensors with a suspension model based on linear suspension\nsensors. The aim is to determine a more accurate estimate of the camera pose.\nWe outline its usage for applications in both visualisation and computer\nvision.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:01:46 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Moran", "Paul", ""], ["Periera", "Leroy-Francisco", ""], ["Selvaraju", "Anbuchezhiyan", ""], ["Prakash", "Tejash", ""], ["Ermilios", "Pantelis", ""], ["McDonald", "John", ""], ["Horgan", "Jonathan", ""], ["Eising", "Ciar\u00e1n", ""]]}, {"id": "2105.02693", "submitter": "Hongbo Zhang Dr.", "authors": "Jia-Xing Zhong, Hongbo Zhang", "title": "Uncertainty-aware INVASE: Enhanced Breast Cancer Diagnosis Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an uncertainty-aware INVASE to quantify predictive\nconfidence of healthcare problem. By introducing learnable Gaussian\ndistributions, we lever-age their variances to measure the degree of\nuncertainty. Based on the vanilla INVASE, two additional modules are proposed,\ni.e., an uncertainty quantification module in the predictor, and a reward\nshaping module in the selector. We conduct extensive experiments on UCI-WDBC\ndataset. Notably, our method eliminates almost all predictive bias with only\nabout 20% queries, while the uncertainty-agnostic counterpart requires nearly\n100% queries. The open-source implementation with a detailed tutorial is\navailable at\nhttps://github.com/jx-zhong-for-academic-purpose/Uncertainty-aware-INVASE/blob/main/tutorialinvase%2B.ipynb.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:30:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhong", "Jia-Xing", ""], ["Zhang", "Hongbo", ""]]}, {"id": "2105.02714", "submitter": "Dvir Ginzburg", "authors": "Dvir Ginzburg and Dan Raviv", "title": "Deep Weighted Consensus: Dense correspondence confidence maps for 3D\n  shape registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new paradigm for rigid alignment between point clouds based on\nlearnable weighted consensus which is robust to noise as well as the full\nspectrum of the rotation group.\n  Current models, learnable or axiomatic, work well for constrained\norientations and limited noise levels, usually by an end-to-end learner or an\niterative scheme. However, real-world tasks require us to deal with large\nrotations as well as outliers and all known models fail to deliver.\n  Here we present a different direction. We claim that we can align point\nclouds out of sampled matched points according to confidence level derived from\na dense, soft alignment map. The pipeline is differentiable, and converges\nunder large rotations in the full spectrum of SO(3), even with high noise\nlevels. We compared the network to recently presented methods such as DCP,\nPointNetLK, RPM-Net, PRnet, and axiomatic methods such as ICP and Go-ICP. We\nreport here a fundamental boost in performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:27:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ginzburg", "Dvir", ""], ["Raviv", "Dan", ""]]}, {"id": "2105.02723", "submitter": "Luke Melas-Kyriazi", "authors": "Luke Melas-Kyriazi", "title": "Do You Even Need Attention? A Stack of Feed-Forward Layers Does\n  Surprisingly Well on ImageNet", "comments": "Short Technical Report. GitHub:\n  https://github.com/lukemelas/do-you-even-need-attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The strong performance of vision transformers on image classification and\nother vision tasks is often attributed to the design of their multi-head\nattention layers. However, the extent to which attention is responsible for\nthis strong performance remains unclear. In this short report, we ask: is the\nattention layer even necessary? Specifically, we replace the attention layer in\na vision transformer with a feed-forward layer applied over the patch\ndimension. The resulting architecture is simply a series of feed-forward layers\napplied over the patch and feature dimensions in an alternating fashion. In\nexperiments on ImageNet, this architecture performs surprisingly well: a\nViT/DeiT-base-sized model obtains 74.9\\% top-1 accuracy, compared to 77.9\\% and\n79.9\\% for ViT and DeiT respectively. These results indicate that aspects of\nvision transformers other than attention, such as the patch embedding, may be\nmore responsible for their strong performance than previously thought. We hope\nthese results prompt the community to spend more time trying to understand why\nour current models are as effective as they are.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:42:39 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Melas-Kyriazi", "Luke", ""]]}, {"id": "2105.02726", "submitter": "Marvin Lerousseau", "authors": "Marvin Lerousseau and Maria Vakalopoulou and Nikos Paragios and Eric\n  Deutsch", "title": "Sparse convolutional context-aware multiple instance learning for whole\n  slide image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Whole slide microscopic slides display many cues about the underlying tissue\nguiding diagnostic and the choice of therapy for many diseases. However, their\nenormous size often in gigapixels hampers the use of traditional neural network\narchitectures. To tackle this issue, multiple instance learning (MIL)\nclassifies bags of patches instead of whole slide images. Most MIL strategies\nconsider that patches are independent and identically distributed. Our approach\npresents a paradigm shift through the integration of spatial information of\npatches with a sparse-input convolutional-based MIL strategy. The formulated\nframework is generic, flexible, scalable and is the first to introduce\ncontextual dependencies between decisions taken at the patch level. It achieved\nstate-of-the-art performance in pan-cancer subtype classification. The code of\nthis work will be made available.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:46:09 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lerousseau", "Marvin", ""], ["Vakalopoulou", "Maria", ""], ["Paragios", "Nikos", ""], ["Deutsch", "Eric", ""]]}, {"id": "2105.02742", "submitter": "Christopher K\\\"ummel", "authors": "Christopher Kissel, Christopher K\\\"ummel, Dennis Ritter, Kristian\n  Hildebrand", "title": "Pose-Guided Sign Language Video GAN with Dynamic Lambda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a novel approach for the synthesis of sign language videos using\nGANs. We extend the previous work of Stoll et al. by using the human semantic\nparser of the Soft-Gated Warping-GAN from to produce photorealistic videos\nguided by region-level spatial layouts. Synthesizing target poses improves\nperformance on independent and contrasting signers. Therefore, we have\nevaluated our system with the highly heterogeneous MS-ASL dataset with over 200\nsigners resulting in a SSIM of 0.893. Furthermore, we introduce a periodic\nweighting approach to the generator that reactivates the training and leads to\nquantitatively better results.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:12:09 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kissel", "Christopher", ""], ["K\u00fcmmel", "Christopher", ""], ["Ritter", "Dennis", ""], ["Hildebrand", "Kristian", ""]]}, {"id": "2105.02769", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Sergey Bartunov, Yujia Li, Ethan Keller, Stefano\n  Saliceti", "title": "Computer-Aided Design as Language", "comments": "24 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided Design (CAD) applications are used in manufacturing to model\neverything from coffee mugs to sports cars. These programs are complex and\nrequire years of training and experience to master. A component of all CAD\nmodels particularly difficult to make are the highly structured 2D sketches\nthat lie at the heart of every 3D construction. In this work, we propose a\nmachine learning model capable of automatically generating such sketches.\nThrough this, we pave the way for developing intelligent tools that would help\nengineers create better designs with less effort. Our method is a combination\nof a general-purpose language modeling technique alongside an off-the-shelf\ndata serialization protocol. We show that our approach has enough flexibility\nto accommodate the complexity of the domain and performs well for both\nunconditional synthesis and image-to-sketch translation.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:43:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Bartunov", "Sergey", ""], ["Li", "Yujia", ""], ["Keller", "Ethan", ""], ["Saliceti", "Stefano", ""]]}, {"id": "2105.02771", "submitter": "Mahdieh Kazemimoghadam", "authors": "Mahdieh Kazemimoghadam, Weicheng Chi, Asal Rahimi, Nathan Kim,\n  Prasanna Alluri, Chika Nwachukwu, Weiguo Lu and Xuejun Gu", "title": "Saliency-Guided Deep Learning Network for Automatic Tumor Bed Volume\n  Delineation in Post-operative Breast Irradiation", "comments": "https://iopscience.iop.org/article/10.1088/1361-6560/ac176d", "journal-ref": "Physics in Medicine & Biology 2021", "doi": "10.1088/1361-6560/ac176d", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient, reliable and reproducible target volume delineation is a key step\nin the effective planning of breast radiotherapy. However, post-operative\nbreast target delineation is challenging as the contrast between the tumor bed\nvolume (TBV) and normal breast tissue is relatively low in CT images. In this\nstudy, we propose to mimic the marker-guidance procedure in manual target\ndelineation. We developed a saliency-based deep learning segmentation (SDL-Seg)\nalgorithm for accurate TBV segmentation in post-operative breast irradiation.\nThe SDL-Seg algorithm incorporates saliency information in the form of markers'\nlocation cues into a U-Net model. The design forces the model to encode the\nlocation-related features, which underscores regions with high saliency levels\nand suppresses low saliency regions. The saliency maps were generated by\nidentifying markers on CT images. Markers' locations were then converted to\nprobability maps using a distance-transformation coupled with a Gaussian\nfilter. Subsequently, the CT images and the corresponding saliency maps formed\na multi-channel input for the SDL-Seg network. Our in-house dataset was\ncomprised of 145 prone CT images from 29 post-operative breast cancer patients,\nwho received 5-fraction partial breast irradiation (PBI) regimen on GammaPod.\nThe performance of the proposed method was compared against basic U-Net. Our\nmodel achieved mean (standard deviation) of 76.4 %, 6.76 mm, and 1.9 mm for\nDSC, HD95, and ASD respectively on the test set with computation time of below\n11 seconds per one CT volume. SDL-Seg showed superior performance relative to\nbasic U-Net for all the evaluation metrics while preserving low computation\ncost. The findings demonstrate that SDL-Seg is a promising approach for\nimproving the efficiency and accuracy of the on-line treatment planning\nprocedure of PBI, such as GammaPod based PBI.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:57:23 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 15:46:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Kazemimoghadam", "Mahdieh", ""], ["Chi", "Weicheng", ""], ["Rahimi", "Asal", ""], ["Kim", "Nathan", ""], ["Alluri", "Prasanna", ""], ["Nwachukwu", "Chika", ""], ["Lu", "Weiguo", ""], ["Gu", "Xuejun", ""]]}, {"id": "2105.02788", "submitter": "Julien Martel", "authors": "Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan,\n  Marco Monteiro and Gordon Wetzstein", "title": "ACORN: Adaptive Coordinate Networks for Neural Scene Representation", "comments": "J. N. P. Martel and D. B. Lindell equally contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural representations have emerged as a new paradigm for applications in\nrendering, imaging, geometric modeling, and simulation. Compared to traditional\nrepresentations such as meshes, point clouds, or volumes they can be flexibly\nincorporated into differentiable learning-based pipelines. While recent\nimprovements to neural representations now make it possible to represent\nsignals with fine details at moderate resolutions (e.g., for images and 3D\nshapes), adequately representing large-scale or complex scenes has proven a\nchallenge. Current neural representations fail to accurately represent images\nat resolutions greater than a megapixel or 3D scenes with more than a few\nhundred thousand polygons. Here, we introduce a new hybrid implicit-explicit\nnetwork architecture and training strategy that adaptively allocates resources\nduring training and inference based on the local complexity of a signal of\ninterest. Our approach uses a multiscale block-coordinate decomposition,\nsimilar to a quadtree or octree, that is optimized during training. The network\narchitecture operates in two stages: using the bulk of the network parameters,\na coordinate encoder generates a feature grid in a single forward pass. Then,\nhundreds or thousands of samples within each block can be efficiently evaluated\nusing a lightweight feature decoder. With this hybrid implicit-explicit network\narchitecture, we demonstrate the first experiments that fit gigapixel images to\nnearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in\nscale of over 1000x compared to the resolution of previously demonstrated\nimage-fitting experiments. Moreover, our approach is able to represent 3D\nshapes significantly faster and better than previous techniques; it reduces\ntraining times from days to hours or minutes and memory requirements by over an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:21:38 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Martel", "Julien N. P.", ""], ["Lindell", "David B.", ""], ["Lin", "Connor Z.", ""], ["Chan", "Eric R.", ""], ["Monteiro", "Marco", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2105.02794", "submitter": "Noam Elron", "authors": "Noam Elron, Alex Itskovich, Shahar S. Yuval, Noam Levy", "title": "Real-Time Video Super-Resolution by Joint Local Inference and Global\n  Parameter Estimation", "comments": "Technical report; accompanying a poster appearing in ICCP 2021", "journal-ref": "ICCP 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The state of the art in video super-resolution (SR) are techniques based on\ndeep learning, but they perform poorly on real-world videos (see Figure 1). The\nreason is that training image-pairs are commonly created by downscaling a\nhigh-resolution image to produce a low-resolution counterpart. Deep models are\ntherefore trained to undo downscaling and do not generalize to super-resolving\nreal-world images. Several recent publications present techniques for improving\nthe generalization of learning-based SR, but are all ill-suited for real-time\napplication.\n  We present a novel approach to synthesizing training data by simulating two\ndigital-camera image-capture processes at different scales. Our method produces\nimage-pairs in which both images have properties of natural images. Training an\nSR model using this data leads to far better generalization to real-world\nimages and videos.\n  In addition, deep video-SR models are characterized by a high\noperations-per-pixel count, which prohibits their application in real-time. We\npresent an efficient CNN architecture, which enables real-time application of\nvideo SR on low-power edge-devices. We split the SR task into two sub-tasks: a\ncontrol-flow which estimates global properties of the input video and adapts\nthe weights and biases of a processing-CNN that performs the actual processing.\nSince the process-CNN is tailored to the statistics of the input, its capacity\nkept low, while retaining effectivity. Also, since video-statistics evolve\nslowly, the control-flow operates at a much lower rate than the video\nframe-rate. This reduces the overall computational load by as much as two\norders of magnitude. This framework of decoupling the adaptivity of the\nalgorithm from the pixel processing, can be applied in a large family of\nreal-time video enhancement applications, e.g., video denoising, local\ntone-mapping, stabilization, etc.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:35:09 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Elron", "Noam", ""], ["Itskovich", "Alex", ""], ["Yuval", "Shahar S.", ""], ["Levy", "Noam", ""]]}, {"id": "2105.02799", "submitter": "Georgios Georgakis", "authors": "Karl Schmeckpeper, Georgios Georgakis, Kostas Daniilidis", "title": "Object-centric Video Prediction without Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to interact with the world, agents must be able to predict the\nresults of the world's dynamics. A natural approach to learn about these\ndynamics is through video prediction, as cameras are ubiquitous and powerful\nsensors. Direct pixel-to-pixel video prediction is difficult, does not take\nadvantage of known priors, and does not provide an easy interface to utilize\nthe learned dynamics. Object-centric video prediction offers a solution to\nthese problems by taking advantage of the simple prior that the world is made\nof objects and by providing a more natural interface for control. However,\nexisting object-centric video prediction pipelines require dense object\nannotations in training video sequences. In this work, we present\nObject-centric Prediction without Annotation (OPA), an object-centric video\nprediction method that takes advantage of priors from powerful computer vision\nmodels. We validate our method on a dataset comprised of video sequences of\nstacked objects falling, and demonstrate how to adapt a perception model in an\nenvironment through end-to-end video prediction training.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:42:38 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Schmeckpeper", "Karl", ""], ["Georgakis", "Georgios", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2105.02802", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Fernando Pereira, Paulo Lobato Correia, Ali\n  Etemad", "title": "Multi-Perspective LSTM for Joint Visual Representation Learning", "comments": "Accepted to CVPR2021. Project link: https://github.com/arsm/MPLSTM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel LSTM cell architecture capable of learning both intra- and\ninter-perspective relationships available in visual sequences captured from\nmultiple perspectives. Our architecture adopts a novel recurrent joint learning\nstrategy that uses additional gates and memories at the cell level. We\ndemonstrate that by using the proposed cell to create a network, more effective\nand richer visual representations are learned for recognition tasks. We\nvalidate the performance of our proposed architecture in the context of two\nmulti-perspective visual recognition tasks namely lip reading and face\nrecognition. Three relevant datasets are considered and the results are\ncompared against fusion strategies, other existing multi-input LSTM\narchitectures, and alternative recognition solutions. The experiments show the\nsuperior performance of our solution over the considered benchmarks, both in\nterms of recognition accuracy and complexity. We make our code publicly\navailable at https://github.com/arsm/MPLSTM.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:44:40 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Pereira", "Fernando", ""], ["Correia", "Paulo Lobato", ""], ["Etemad", "Ali", ""]]}, {"id": "2105.02803", "submitter": "Ruoxi Qin", "authors": "Ruoxi Qin, Linyuan Wang, Xingyuan Chen, Xuehui Du, Bin Yan", "title": "Dynamic Defense Approach for Adversarial Robustness in Deep Neural\n  Networks via Stochastic Ensemble Smoothed Model", "comments": "17 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to suffer from critical vulnerabilities\nunder adversarial attacks. This phenomenon stimulated the creation of different\nattack and defense strategies similar to those adopted in cyberspace security.\nThe dependence of such strategies on attack and defense mechanisms makes the\nassociated algorithms on both sides appear as closely reciprocating processes.\nThe defense strategies are particularly passive in these processes, and\nenhancing initiative of such strategies can be an effective way to get out of\nthis arms race. Inspired by the dynamic defense approach in cyberspace, this\npaper builds upon stochastic ensemble smoothing based on defense method of\nrandom smoothing and model ensemble. Proposed method employs network\narchitecture and smoothing parameters as ensemble attributes, and dynamically\nchange attribute-based ensemble model before every inference prediction\nrequest. The proposed method handles the extreme transferability and\nvulnerability of ensemble models under white-box attacks. Experimental\ncomparison of ASR-vs-distortion curves with different attack scenarios shows\nthat even the attacker with the highest attack capability cannot easily exceed\nthe attack success rate associated with the ensemble smoothed model, especially\nunder untargeted attacks.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:48:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Qin", "Ruoxi", ""], ["Wang", "Linyuan", ""], ["Chen", "Xingyuan", ""], ["Du", "Xuehui", ""], ["Yan", "Bin", ""]]}, {"id": "2105.02825", "submitter": "Leon Sixt", "authors": "Martin Schuessler, Philipp Wei{\\ss}, Leon Sixt", "title": "Two4Two: Evaluating Interpretable Machine Learning - A Synthetic Dataset\n  For Controlled Experiments", "comments": "6 pages, 3 figures, presented at the ICLR 2021 RAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of approaches exist to generate explanations for image\nclassification. However, few of these approaches are subjected to human-subject\nevaluations, partly because it is challenging to design controlled experiments\nwith natural image datasets, as they leave essential factors out of the\nresearcher's control. With our approach, researchers can describe their desired\ndataset with only a few parameters. Based on these, our library generates\nsynthetic image data of two 3D abstract animals. The resulting data is suitable\nfor algorithmic as well as human-subject evaluations. Our user study results\ndemonstrate that our method can create biases predictive enough for a\nclassifier and subtle enough to be noticeable only to every second participant\ninspecting the data visually. Our approach significantly lowers the barrier for\nconducting human subject evaluations, thereby facilitating more rigorous\ninvestigations into interpretable machine learning. For our library and\ndatasets see, https://github.com/mschuessler/two4two/\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:14:39 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Schuessler", "Martin", ""], ["Wei\u00df", "Philipp", ""], ["Sixt", "Leon", ""]]}, {"id": "2105.02835", "submitter": "Yuchen Fei", "authors": "Yuchen Fei, Bo Zhan, Mei Hong, Xi Wu, Jiliu Zhou, Yan Wang", "title": "Deep Learning based Multi-modal Computing with Feature Disentanglement\n  for MRI Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Different Magnetic resonance imaging (MRI) modalities of the same\nanatomical structure are required to present different pathological information\nfrom the physical level for diagnostic needs. However, it is often difficult to\nobtain full-sequence MRI images of patients owing to limitations such as time\nconsumption and high cost. The purpose of this work is to develop an algorithm\nfor target MRI sequences prediction with high accuracy, and provide more\ninformation for clinical diagnosis. Methods: We propose a deep learning based\nmulti-modal computing model for MRI synthesis with feature disentanglement\nstrategy. To take full advantage of the complementary information provided by\ndifferent modalities, multi-modal MRI sequences are utilized as input. Notably,\nthe proposed approach decomposes each input modality into modality-invariant\nspace with shared information and modality-specific space with specific\ninformation, so that features are extracted separately to effectively process\nthe input data. Subsequently, both of them are fused through the adaptive\ninstance normalization (AdaIN) layer in the decoder. In addition, to address\nthe lack of specific information of the target modality in the test phase, a\nlocal adaptive fusion (LAF) module is adopted to generate a modality-like\npseudo-target with specific information similar to the ground truth. Results:\nTo evaluate the synthesis performance, we verify our method on the BRATS2015\ndataset of 164 subjects. The experimental results demonstrate our approach\nsignificantly outperforms the benchmark method and other state-of-the-art\nmedical image synthesis methods in both quantitative and qualitative measures.\nCompared with the pix2pixGANs method, the PSNR improves from 23.68 to 24.8.\nConclusion: The proposed method could be effective in prediction of target MRI\nsequences, and useful for clinical diagnosis and treatment.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:22:22 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Fei", "Yuchen", ""], ["Zhan", "Bo", ""], ["Hong", "Mei", ""], ["Wu", "Xi", ""], ["Zhou", "Jiliu", ""], ["Wang", "Yan", ""]]}, {"id": "2105.02858", "submitter": "Alexander Siemenn", "authors": "Alexander E. Siemenn, Matthew Beveridge, Tonio Buonassisi, Iddo Drori", "title": "Online Preconditioning of Experimental Inkjet Hardware by Bayesian\n  Optimization in Loop", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-performance semiconductor optoelectronics such as perovskites have\nhigh-dimensional and vast composition spaces that govern the performance\nproperties of the material. To cost-effectively search these composition\nspaces, we utilize a high-throughput experimentation method of rapidly printing\ndiscrete droplets via inkjet deposition, in which each droplet is comprised of\na unique permutation of semiconductor materials. However, inkjet printer\nsystems are not optimized to run high-throughput experimentation on\nsemiconductor materials. Thus, in this work, we develop a computer\nvision-driven Bayesian optimization framework for optimizing the deposited\ndroplet structures from an inkjet printer such that it is tuned to perform\nhigh-throughput experimentation on semiconductor materials. The goal of this\nframework is to tune to the hardware conditions of the inkjet printer in the\nshortest amount of time using the fewest number of droplet samples such that we\nminimize the time and resources spent on setting the system up for material\ndiscovery applications. We demonstrate convergence on optimum inkjet hardware\nconditions in 10 minutes using Bayesian optimization of computer vision-scored\ndroplet structures. We compare our Bayesian optimization results with\nstochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:46:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Siemenn", "Alexander E.", ""], ["Beveridge", "Matthew", ""], ["Buonassisi", "Tonio", ""], ["Drori", "Iddo", ""]]}, {"id": "2105.02872", "submitter": "Sida Peng", "authors": "Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai,\n  Hujun Bao, Xiaowei Zhou", "title": "Animatable Neural Radiance Fields for Human Body Modeling", "comments": "The first two authors contributed equally to this paper. Project\n  page: https://zju3dv.github.io/animatable_nerf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of reconstructing an animatable human\nmodel from a multi-view video. Some recent works have proposed to decompose a\ndynamic scene into a canonical neural radiance field and a set of deformation\nfields that map observation-space points to the canonical space, thereby\nenabling them to learn the dynamic scene from images. However, they represent\nthe deformation field as translational vector field or SE(3) field, which makes\nthe optimization highly under-constrained. Moreover, these representations\ncannot be explicitly controlled by input motions. Instead, we introduce neural\nblend weight fields to produce the deformation fields. Based on the\nskeleton-driven deformation, blend weight fields are used with 3D human\nskeletons to generate observation-to-canonical and canonical-to-observation\ncorrespondences. Since 3D human skeletons are more observable, they can\nregularize the learning of deformation fields. Moreover, the learned blend\nweight fields can be combined with input skeletal motions to generate new\ndeformation fields to animate the human model. Experiments show that our\napproach significantly outperforms recent human synthesis methods. The code\nwill be available at https://zju3dv.github.io/animatable_nerf/.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Peng", "Sida", ""], ["Dong", "Junting", ""], ["Wang", "Qianqian", ""], ["Zhang", "Shangzhan", ""], ["Shuai", "Qing", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "2105.02875", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, Yiming Lin and Abhijeet Ghosh", "title": "Deep Polarization Imaging for 3D shape and SVBRDF Acquisition", "comments": "CVPR 2021 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for efficient acquisition of shape and spatially\nvarying reflectance of 3D objects using polarization cues. Unlike previous\nworks that have exploited polarization to estimate material or object\nappearance under certain constraints (known shape or multiview acquisition), we\nlift such restrictions by coupling polarization imaging with deep learning to\nachieve high quality estimate of 3D object shape (surface normals and depth)\nand SVBRDF using single-view polarization imaging under frontal flash\nillumination. In addition to acquired polarization images, we provide our deep\nnetwork with strong novel cues related to shape and reflectance, in the form of\na normalized Stokes map and an estimate of diffuse color. We additionally\ndescribe modifications to network architecture and training loss which provide\nfurther qualitative improvements. We demonstrate our approach to achieve\nsuperior results compared to recent works employing deep learning in\nconjunction with flash illumination.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Lin", "Yiming", ""], ["Ghosh", "Abhijeet", ""]]}, {"id": "2105.02877", "submitter": "Triantafyllos Afouras", "authors": "Hannah Bull, Triantafyllos Afouras, G\\\"ul Varol, Samuel Albanie,\n  Liliane Momeni, Andrew Zisserman", "title": "Aligning Subtitles in Sign Language Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to temporally align asynchronous subtitles in sign\nlanguage videos. In particular, we focus on sign-language interpreted TV\nbroadcast data comprising (i) a video of continuous signing, and (ii) subtitles\ncorresponding to the audio content. Previous work exploiting such\nweakly-aligned data only considered finding keyword-sign correspondences,\nwhereas we aim to localise a complete subtitle text in continuous signing. We\npropose a Transformer architecture tailored for this task, which we train on\nmanually annotated alignments covering over 15K subtitles that span 17.7 hours\nof video. We use BERT subtitle embeddings and CNN video representations learned\nfor sign recognition to encode the two signals, which interact through a series\nof attention layers. Our model outputs frame-level predictions, i.e., for each\nvideo frame, whether it belongs to the queried subtitle or not. Through\nextensive evaluations, we show substantial improvements over existing alignment\nbaselines that do not make use of subtitle text embeddings for learning. Our\nautomatic alignment model opens up possibilities for advancing machine\ntranslation of sign languages via providing continuously synchronized\nvideo-text data.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:59:36 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bull", "Hannah", ""], ["Afouras", "Triantafyllos", ""], ["Varol", "G\u00fcl", ""], ["Albanie", "Samuel", ""], ["Momeni", "Liliane", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2105.02878", "submitter": "Vladislav Golyanik", "authors": "Marcel Seelbach Benkner and Zorah L\\\"ahner and Vladislav Golyanik and\n  Christof Wunderlich and Christian Theobalt and Michael Moeller", "title": "Q-Match: Iterative Shape Matching via Quantum Annealing", "comments": "16 pages, 12 figures and two tables; project page:\n  http://gvv.mpi-inf.mpg.de/projects/QMATCH/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding shape correspondences can be formulated as an NP-hard quadratic\nassignment problem (QAP) that becomes infeasible for shapes with high sampling\ndensity. A promising research direction is to tackle such quadratic\noptimization problems over binary variables with quantum annealing, which, in\ntheory, allows to find globally optimal solutions relying on a new\ncomputational paradigm. Unfortunately, enforcing the linear equality\nconstraints in QAPs via a penalty significantly limits the success probability\nof such methods on currently available quantum hardware. To address this\nlimitation, this paper proposes Q-Match, i.e., a new iterative quantum method\nfor QAPs inspired by the alpha-expansion algorithm, which allows solving\nproblems of an order of magnitude larger than current quantum methods. It works\nby implicitly enforcing the QAP constraints by updating the current estimates\nin a cyclic fashion. Further, Q-Match can be applied for shape matching\nproblems iteratively, on a subset of well-chosen correspondences, allowing us\nto scale to real-world problems. Using the latest quantum annealer, the D-Wave\nAdvantage, we evaluate the proposed method on a subset of QAPLIB as well as on\nisometric shape matching problems from the FAUST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:59:38 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Benkner", "Marcel Seelbach", ""], ["L\u00e4hner", "Zorah", ""], ["Golyanik", "Vladislav", ""], ["Wunderlich", "Christof", ""], ["Theobalt", "Christian", ""], ["Moeller", "Michael", ""]]}, {"id": "2105.02922", "submitter": "Evangelos Ntavelis", "authors": "Evangelos Ntavelis and Jan Remund and Philipp Schmid", "title": "SkyCam: A Dataset of Sky Images and their Irradiance values", "comments": "https://github.com/vglsd/SkyCam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in Computer Vision and Deep Learning have enabled astonishing\nresults in a variety of fields and applications. Motivated by this success, the\nSkyCam Dataset aims to enable image-based Deep Learning solutions for\nshort-term, precise prediction of solar radiation on a local level. For the\nspan of a year, three different cameras in three topographically different\nlocations in Switzerland are acquiring images of the sky every 10 seconds.\nThirteen high resolution images with different exposure times are captured and\nused to create an additional HDR image. The images are paired with highly\nprecise irradiance values gathered from a high-accuracy pyranometer.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 19:35:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ntavelis", "Evangelos", ""], ["Remund", "Jan", ""], ["Schmid", "Philipp", ""]]}, {"id": "2105.02942", "submitter": "Peilin Kang", "authors": "Peilin Kang, Seyed-Mohsen Moosavi-Dezfooli", "title": "Understanding Catastrophic Overfitting in Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, FGSM adversarial training is found to be able to train a robust\nmodel which is comparable to the one trained by PGD but an order of magnitude\nfaster. However, there is a failure mode called catastrophic overfitting (CO)\nthat the classifier loses its robustness suddenly during the training and\nhardly recovers by itself. In this paper, we find CO is not only limited to\nFGSM, but also happens in $\\mbox{DF}^{\\infty}$-1 adversarial training. Then, we\nanalyze the geometric properties for both FGSM and $\\mbox{DF}^{\\infty}$-1 and\nfind they have totally different decision boundaries after CO. For FGSM, a new\ndecision boundary is generated along the direction of perturbation and makes\nthe small perturbation more effective than the large one. While for\n$\\mbox{DF}^{\\infty}$-1, there is no new decision boundary generated along the\ndirection of perturbation, instead the perturbation generated by\n$\\mbox{DF}^{\\infty}$-1 becomes smaller after CO and thus loses its\neffectiveness. We also experimentally analyze three hypotheses on potential\nfactors causing CO. And then based on the empirical analysis, we modify the\nRS-FGSM by not projecting perturbation back to the $l_\\infty$ ball. By this\nsmall modification, we could achieve $47.56 \\pm 0.37\\% $ PGD-50-10 accuracy on\nCIFAR10 with $\\epsilon=8/255$ in contrast to $43.57 \\pm 0.30\\% $ by RS-FGSM and\nalso further extend the working range of $\\epsilon$ from 8/255 to 11/255 on\nCIFAR10 without CO occurring.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:39:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kang", "Peilin", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""]]}, {"id": "2105.02953", "submitter": "Andrei Velichko", "authors": "Y. A. Izotov, A. A. Velichko, A. A. Ivshin and R. E. Novitskiy", "title": "Recognition of handwritten MNIST digits on low-memory 2 Kb RAM Arduino\n  board using LogNNet reservoir neural network", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1088/1757-899X/1155/1/012056", "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented compact algorithm for recognizing handwritten digits of the\nMNIST database, created on the LogNNet reservoir neural network, reaches the\nrecognition accuracy of 82%. The algorithm was tested on a low-memory Arduino\nboard with 2 Kb static RAM low-power microcontroller. The dependences of the\naccuracy and time of image recognition on the number of neurons in the\nreservoir have been investigated. The memory allocation demonstrates that the\nalgorithm stores all the necessary information in RAM without using additional\ndata storage, and operates with original images without preliminary processing.\nThe simple structure of the algorithm, with appropriate training, can be\nadapted for wide practical application, for example, for creating mobile\nbiosensors for early diagnosis of adverse events in medicine. The study results\nare important for the implementation of artificial intelligence on peripheral\nconstrained IoT devices and for edge computing.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:16:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Izotov", "Y. A.", ""], ["Velichko", "A. A.", ""], ["Ivshin", "A. A.", ""], ["Novitskiy", "R. E.", ""]]}, {"id": "2105.02954", "submitter": "Mohammed Tolba", "authors": "Mohammed F. Tolba, Huruy Tekle Tesfai, Hani Saleh, Baker Mohammad, and\n  Mahmoud Al-Qutayri", "title": "Deep Neural Networks Based Weight Approximation and Computation Reuse\n  for 2-D Image Classification", "comments": "10 pages 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are computationally and memory intensive, which\nmakes their hardware implementation a challenging task especially for resource\nconstrained devices such as IoT nodes. To address this challenge, this paper\nintroduces a new method to improve DNNs performance by fusing approximate\ncomputing with data reuse techniques to be used for image recognition\napplications. DNNs weights are approximated based on the linear and quadratic\napproximation methods during the training phase, then, all of the weights are\nreplaced with the linear/quadratic coefficients to execute the inference in a\nway where different weights could be computed using the same coefficients. This\nleads to a repetition of the weights across the processing element (PE) array,\nwhich in turn enables the reuse of the DNN sub-computations (computational\nreuse) and leverage the same data (data reuse) to reduce DNNs computations,\nmemory accesses, and improve energy efficiency albeit at the cost of increased\ntraining time. Complete analysis for both MNIST and CIFAR 10 datasets is\npresented for image recognition , where LeNet 5 revealed a reduction in the\nnumber of parameters by a factor of 1211.3x with a drop of less than 0.9% in\naccuracy. When compared to the state of the art Row Stationary (RS) method, the\nproposed architecture saved 54% of the total number of adders and multipliers\nneeded. Overall, the proposed approach is suitable for IoT edge devices as it\nreduces the memory size requirement as well as the number of needed memory\naccesses.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:16:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tolba", "Mohammed F.", ""], ["Tesfai", "Huruy Tekle", ""], ["Saleh", "Hani", ""], ["Mohammad", "Baker", ""], ["Al-Qutayri", "Mahmoud", ""]]}, {"id": "2105.02955", "submitter": "Ildar Rakhmatulin", "authors": "Rakhmatulin Ildar", "title": "Detect caterpillar, grasshopper, aphid and simulation program for\n  neutralizing them by laser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The protection of crops from pests is relevant for any cultivated crop. But\nmodern methods of pest control by pesticides carry many dangers for humans.\nTherefore, research into the development of safe and effective pest control\nmethods is promising. This manuscript presents a new method of pest control. We\nused neural networks for pest detection and developed a powerful laser device\n(5 W) for their neutralization. In the manuscript methods of processing images\nwith pests to extract the most useful feature are described in detail. Using\nthe following pets as an example: aphids, grasshopper, cabbage caterpillar, we\nanalyzed various neural network models and selected the optimal models and\ncharacteristics for each insect. In the paper the principle of operation of the\ndeveloped laser device is described in detail. We created the program to search\na pest in the video stream calculation of their coordinates and transmission\ndata with coordinates to the device with the laser.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:02:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ildar", "Rakhmatulin", ""]]}, {"id": "2105.02956", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Pierre-Alain Fayolle", "title": "Reconstruction of Convex Polytope Compositions from 3D Point-clouds", "comments": "In Proceedings of the 16th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications - Volume 1:\n  GRAPP, ISBN 978-989-758-488-6 ISSN 2184-4321, pages 75-84", "journal-ref": null, "doi": "10.5220/0010297100750084", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing a composition (union) of convex polytopes that perfectly fits\nthe corresponding input point-cloud is a hard optimization problem with\ninteresting applications in reverse engineering and rigid body dynamics\nsimulations. We propose a pipeline that first extracts a set of planes, then\npartitions the input point-cloud into weakly convex clusters and finally\ngenerates a set of convex polytopes as the intersection of fitted planes for\neach partition. Finding the best-fitting convex polytopes is formulated as a\ncombinatorial optimization problem over the set of fitted planes and is solved\nusing an Evolutionary Algorithm. For convex clustering, we employ two different\nmethods and detail their strengths and weaknesses in a thorough evaluation\nbased on multiple input data-sets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:14:55 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Friedrich", "Markus", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2105.02957", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dhaval Salwala, Edward Curry", "title": "VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the\n  Edge for the Internet of Multimedia Things", "comments": "22 pages, 24 figures, 9 tables, Journal accepted in IEEE Internet of\n  Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2021.3075336", "report-no": null, "categories": "cs.CV cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient video processing is a critical component in many IoMT applications\nto detect events of interest. Presently, many window optimization techniques\nhave been proposed in event processing with an underlying assumption that the\nincoming stream has a structured data model. Videos are highly complex due to\nthe lack of any underlying structured data model. Video stream sources such as\nCCTV cameras and smartphones are resource-constrained edge nodes. At the same\ntime, video content extraction is expensive and requires computationally\nintensive Deep Neural Network (DNN) models that are primarily deployed at\nhigh-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage\nallied windowing approach to accelerate video event analytics in an edge-cloud\nparadigm. VID-WIN runs parallelly across edge and cloud nodes and performs the\nquery and resource-aware optimization for state-based complex event matching.\nVID-WIN exploits the video content and DNN input knobs to accelerate the video\ninference process across nodes. The paper proposes a novel content-driven\nmicro-batch resizing, queryaware caching and micro-batch based utility\nfiltering strategy of video frames under resource-constrained edge nodes to\nimprove the overall system throughput, latency, and network usage. Extensive\nevaluations are performed over five real-world datasets. The experimental\nresults show that VID-WIN video event matching achieves ~2.3X higher throughput\nwith minimal latency and ~99% bandwidth reduction compared to other baselines\nwhile maintaining query-level accuracy and resource bounds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:08:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yadav", "Piyush", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2105.02958", "submitter": "Andrew Soroka", "authors": "Andrey Soroka (1), Alex Meshcheryakov (2), Sergey Gerasimov (1) ((1)\n  Faculty of Computational Mathematics and Cybernetics Lomonosov Moscow State\n  University, (2) Space Research Institute of RAS)", "title": "Morphological classification of astronomical images with limited\n  labelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.GA cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of morphological classification is complex for simple\nparameterization, but important for research in the galaxy evolution field.\nFuture galaxy surveys (e.g. EUCLID) will collect data about more than a $10^9$\ngalaxies. To obtain morphological information one needs to involve people to\nmark up galaxy images, which requires either a considerable amount of money or\na huge number of volunteers. We propose an effective semi-supervised approach\nfor galaxy morphology classification task, based on active learning of\nadversarial autoencoder (AAE) model. For a binary classification problem (top\nlevel question of Galaxy Zoo 2 decision tree) we achieved accuracy 93.1% on the\ntest part with only 0.86 millions markup actions, this model can easily scale\nup on any number of images. Our best model with additional markup achieves\naccuracy of 95.5%. To the best of our knowledge it is a first time AAE\nsemi-supervised learning model used in astronomy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:26:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Soroka", "Andrey", ""], ["Meshcheryakov", "Alex", ""], ["Gerasimov", "Sergey", ""]]}, {"id": "2105.02959", "submitter": "Ekanki Sharma", "authors": "Ekanki Sharma and Wilfried Elmenreich", "title": "A review on physical and data-driven based nowcasting methods using sky\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amongst all the renewable energy resources (RES), solar is the most popular\nform of energy source and is of particular interest for its widely integration\ninto the power grid. However, due to the intermittent nature of solar source,\nit is of the greatest significance to forecast solar irradiance to ensure\nuninterrupted and reliable power supply to serve the energy demand. There are\nseveral approaches to perform solar irradiance forecasting, for instance\nsatellite-based methods, sky image-based methods, machine learning-based\nmethods, and numerical weather prediction-based methods. In this paper, we\npresent a review on short-term intra-hour solar prediction techniques known as\nnowcasting methods using sky images. Along with this, we also report and\ndiscuss which sky image features are significant for the nowcasting methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:20:52 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Sharma", "Ekanki", ""], ["Elmenreich", "Wilfried", ""]]}, {"id": "2105.02960", "submitter": "Abu Sufian", "authors": "Abu Sufian, Changsheng You and Mianxiong Dong", "title": "A Deep Transfer Learning-based Edge Computing Method for Home Health\n  Monitoring", "comments": "6 pages, 4 figures. Pre-print copy", "journal-ref": "55th Annual Conference on Information Sciences and Systems (CISS),\n  2021", "doi": "10.1109/CISS50987.2021.9400321", "report-no": null, "categories": "cs.CV cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The health-care gets huge stress in a pandemic or epidemic situation. Some\ndiseases such as COVID-19 that causes a pandemic is highly spreadable from an\ninfected person to others. Therefore, providing health services at home for\nnon-critical infected patients with isolation shall assist to mitigate this\nkind of stress. In addition, this practice is also very useful for monitoring\nthe health-related activities of elders who live at home. The home health\nmonitoring, a continuous monitoring of a patient or elder at home using visual\nsensors is one such non-intrusive sub-area of health services at home. In this\narticle, we propose a transfer learning-based edge computing method for home\nhealth monitoring. Specifically, a pre-trained convolutional neural\nnetwork-based model can leverage edge devices with a small amount of\nground-labeled data and fine-tuning method to train the model. Therefore,\non-site computing of visual data captured by RGB, depth, or thermal sensor\ncould be possible in an affordable way. As a result, raw data captured by these\ntypes of sensors is not required to be sent outside from home. Therefore,\nprivacy, security, and bandwidth scarcity shall not be issues. Moreover,\nreal-time computing for the above-mentioned purposes shall be possible in an\neconomical way.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:01:41 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Sufian", "Abu", ""], ["You", "Changsheng", ""], ["Dong", "Mianxiong", ""]]}, {"id": "2105.02961", "submitter": "Peter Meltzer", "authors": "Peter Meltzer, Hooman Shayani, Amir Khasahmadi, Pradeep Kumar\n  Jayaraman, Aditya Sanghi and Joseph Lambourne", "title": "UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity\n  Measure for B-Reps", "comments": "13 pages, 20 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:14:01 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 08:38:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Meltzer", "Peter", ""], ["Shayani", "Hooman", ""], ["Khasahmadi", "Amir", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Sanghi", "Aditya", ""], ["Lambourne", "Joseph", ""]]}, {"id": "2105.02963", "submitter": "Praveen Ravirathinam", "authors": "Rahul Ghosh, Praveen Ravirathinam, Xiaowei Jia, Chenxi Lin, Zhenong\n  Jin, Vipin Kumar", "title": "Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of massive earth observing satellite data provide huge\nopportunities for land use and land cover mapping. However, such mapping effort\nis challenging due to the existence of various land cover classes, noisy data,\nand the lack of proper labels. Also, each land cover class typically has its\nown unique temporal pattern and can be identified only during certain periods.\nIn this article, we introduce a novel architecture that incorporates the UNet\nstructure with Bidirectional LSTM and Attention mechanism to jointly exploit\nthe spatial and temporal nature of satellite data and to better identify the\nunique temporal patterns of each land cover. We evaluate this method for\nmapping crops in multiple regions over the world. We compare our method with\nother state-of-the-art methods both quantitatively and qualitatively on two\nreal-world datasets which involve multiple land cover classes. We also\nvisualise the attention weights to study its effectiveness in mitigating noise\nand identifying discriminative time period.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 05:39:42 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ghosh", "Rahul", ""], ["Ravirathinam", "Praveen", ""], ["Jia", "Xiaowei", ""], ["Lin", "Chenxi", ""], ["Jin", "Zhenong", ""], ["Kumar", "Vipin", ""]]}, {"id": "2105.02964", "submitter": "Vlad Velici", "authors": "Vlad Velici, Adam Pr\\\"ugel-Bennett", "title": "Object detection for crabs in top-view seabed imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This report presents the application of object detection on a database of\nunderwater images of different species of crabs, as well as aerial images of\nsea lions and finally the Pascal VOC dataset. The model is an end-to-end object\ndetection neural network based on a convolutional network base and a Long\nShort-Term Memory detector.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 00:05:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Velici", "Vlad", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "2105.02966", "submitter": "Daniele Loiacono", "authors": "Edoardo Giacomello, Pier Luca Lanzi, Daniele Loiacono, Luca Nassano", "title": "Image Embedding and Model Ensembling for Automated Chest X-Ray\n  Interpretation", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray (CXR) is perhaps the most frequently-performed radiological\ninvestigation globally. In this work, we present and study several machine\nlearning approaches to develop automated CXR diagnostic models. In particular,\nwe trained several Convolutional Neural Networks (CNN) on the CheXpert dataset,\na large collection of more than 200k CXR labeled images. Then, we used the\ntrained CNNs to compute embeddings of the CXR images, in order to train two\nsets of tree-based classifiers from them. Finally, we described and compared\nthree ensembling strategies to combine together the classifiers trained. Rather\nthan expecting some performance-wise benefits, our goal in this work is showing\nthat the above two methodologies, i.e., the extraction of image embeddings and\nmodels ensembling, can be effective and viable to solve tasks that require\nmedical imaging understanding. Our results in that perspective are encouraging\nand worthy of further investigation.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:48:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Giacomello", "Edoardo", ""], ["Lanzi", "Pier Luca", ""], ["Loiacono", "Daniele", ""], ["Nassano", "Luca", ""]]}, {"id": "2105.02968", "submitter": "Jonas Kohler", "authors": "Adrian Hoffmann, Claudio Fanconi, Rahul Rade, Jonas Kohler", "title": "This Looks Like That... Does it? Shortcomings of Latent Space Prototype\n  Interpretability in Deep Networks", "comments": null, "journal-ref": "ICML 2021 Workshop on Theoretic Foundation, Criticism, and\n  Application Trend of Explainable AI", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks that yield human interpretable decisions by\narchitectural design have lately become an increasingly popular alternative to\npost hoc interpretation of traditional black-box models. Among these networks,\nthe arguably most widespread approach is so-called prototype learning, where\nsimilarities to learned latent prototypes serve as the basis of classifying an\nunseen data point. In this work, we point to an important shortcoming of such\napproaches. Namely, there is a semantic gap between similarity in latent space\nand similarity in input space, which can corrupt interpretability. We design\ntwo experiments that exemplify this issue on the so-called ProtoPNet.\nSpecifically, we find that this network's interpretability mechanism can be led\nastray by intentionally crafted or even JPEG compression artefacts, which can\nproduce incomprehensible decisions. We argue that practitioners ought to have\nthis shortcoming in mind when deploying prototype-based models in practice.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:28:34 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:48:08 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 10:16:34 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 12:28:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Hoffmann", "Adrian", ""], ["Fanconi", "Claudio", ""], ["Rade", "Rahul", ""], ["Kohler", "Jonas", ""]]}, {"id": "2105.02976", "submitter": "Gengshan Yang", "authors": "Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester\n  Cole, Huiwen Chang, Deva Ramanan, William T. Freeman, Ce Liu", "title": "LASR: Learning Articulated Shape Reconstruction from a Monocular Video", "comments": "CVPR 2021. Project page: https://lasr-google.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remarkable progress has been made in 3D reconstruction of rigid structures\nfrom a video or a collection of images. However, it is still challenging to\nreconstruct nonrigid structures from RGB inputs, due to its under-constrained\nnature. While template-based approaches, such as parametric shape models, have\nachieved great success in modeling the \"closed world\" of known object\ncategories, they cannot well handle the \"open-world\" of novel object categories\nor outlier shapes. In this work, we introduce a template-free approach to learn\n3D shapes from a single video. It adopts an analysis-by-synthesis strategy that\nforward-renders object silhouette, optical flow, and pixel values to compare\nwith video observations, which generates gradients to adjust the camera, shape\nand motion parameters. Without using a category-specific shape template, our\nmethod faithfully reconstructs nonrigid 3D structures from videos of human,\nanimals, and objects of unknown classes. Code will be available at\nlasr-google.github.io .\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:41:11 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yang", "Gengshan", ""], ["Sun", "Deqing", ""], ["Jampani", "Varun", ""], ["Vlasic", "Daniel", ""], ["Cole", "Forrester", ""], ["Chang", "Huiwen", ""], ["Ramanan", "Deva", ""], ["Freeman", "William T.", ""], ["Liu", "Ce", ""]]}, {"id": "2105.03014", "submitter": "Mingda Zhang", "authors": "Mingda Zhang, Chun-Te Chu, Andrey Zhmoginov, Andrew Howard, Brendan\n  Jou, Yukun Zhu, Li Zhang, Rebecca Hwa, Adriana Kovashka", "title": "BasisNet: Two-stage Model Synthesis for Efficient Inference", "comments": "To appear, 4th Workshop on Efficient Deep Learning for Computer\n  Vision (ECV2021), CVPR2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present BasisNet which combines recent advancements in\nefficient neural network architectures, conditional computation, and early\ntermination in a simple new form. Our approach incorporates a lightweight model\nto preview the input and generate input-dependent combination coefficients,\nwhich later controls the synthesis of a more accurate specialist model to make\nfinal prediction. The two-stage model synthesis strategy can be applied to any\nnetwork architectures and both stages are jointly trained. We also show that\nproper training recipes are critical for increasing generalizability for such\nhigh capacity neural networks. On ImageNet classification benchmark, our\nBasisNet with MobileNets as backbone demonstrated clear advantage on\naccuracy-efficiency trade-off over several strong baselines. Specifically,\nBasisNet-MobileNetV3 obtained 80.3% top-1 accuracy with only 290M Multiply-Add\noperations, halving the computational cost of previous state-of-the-art without\nsacrificing accuracy. With early termination, the average cost can be further\nreduced to 198M MAdds while maintaining accuracy of 80.0% on ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:21:56 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhang", "Mingda", ""], ["Chu", "Chun-Te", ""], ["Zhmoginov", "Andrey", ""], ["Howard", "Andrew", ""], ["Jou", "Brendan", ""], ["Zhu", "Yukun", ""], ["Zhang", "Li", ""], ["Hwa", "Rebecca", ""], ["Kovashka", "Adriana", ""]]}, {"id": "2105.03020", "submitter": "Christian Garbin", "authors": "Christian Garbin, Pranav Rajpurkar, Jeremy Irvin, Matthew P. Lungren,\n  Oge Marques", "title": "Structured dataset documentation: a datasheet for CheXpert", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Billions of X-ray images are taken worldwide each year. Machine learning, and\ndeep learning in particular, has shown potential to help radiologists triage\nand diagnose images. However, deep learning requires large datasets with\nreliable labels. The CheXpert dataset was created with the participation of\nboard-certified radiologists, resulting in the strong ground truth needed to\ntrain deep learning networks. Following the structured format of Datasheets for\nDatasets, this paper expands on the original CheXpert paper and other sources\nto show the critical role played by radiologists in the creation of reliable\nlabels and to describe the different aspects of the dataset composition in\ndetail. Such structured documentation intends to increase the awareness in the\nmachine learning and medical communities of the strengths, applications, and\nevolution of CheXpert, thereby advancing the field of medical image analysis.\nAnother objective of this paper is to put forward this dataset datasheet as an\nexample to the community of how to create detailed and structured descriptions\nof datasets. We believe that clearly documenting the creation process, the\ncontents, and applications of datasets accelerates the creation of useful and\nreliable models.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:45:03 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Garbin", "Christian", ""], ["Rajpurkar", "Pranav", ""], ["Irvin", "Jeremy", ""], ["Lungren", "Matthew P.", ""], ["Marques", "Oge", ""]]}, {"id": "2105.03026", "submitter": "Walid Hariri", "authors": "Walid Hariri", "title": "Efficient Masked Face Recognition Method during the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The coronavirus disease (COVID-19) is an unparalleled crisis leading to a\nhuge number of casualties and security problems. In order to reduce the spread\nof coronavirus, people often wear masks to protect themselves. This makes face\nrecognition a very difficult task since certain parts of the face are hidden. A\nprimary focus of researchers during the ongoing coronavirus pandemic is to come\nup with suggestions to handle this problem through rapid and efficient\nsolutions. In this paper, we propose a reliable method based on occlusion\nremoval and deep learning-based features in order to address the problem of the\nmasked face recognition process. The first step is to remove the masked face\nregion. Next, we apply three pre-trained deep Convolutional Neural Networks\n(CNN) namely, VGG-16, AlexNet, and ResNet-50, and use them to extract deep\nfeatures from the obtained regions (mostly eyes and forehead regions). The\nBag-of-features paradigm is then applied to the feature maps of the last\nconvolutional layer in order to quantize them and to get a slight\nrepresentation comparing to the fully connected layer of classical CNN.\nFinally, Multilayer Perceptron (MLP) is applied for the classification process.\nExperimental results on Real-World-Masked-Face-Dataset show high recognition\nperformance compared to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 01:32:37 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hariri", "Walid", ""]]}, {"id": "2105.03042", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Zhipeng Huang, Kecheng Zheng, Dong Liu, Xiaoyan Sun,\n  Zheng-Jun Zha", "title": "Adaptive Domain-Specific Normalization for Generalizable Person\n  Re-Identification", "comments": "Withdraw this paper for internal review. Since we were not familiar\n  with the use of arXiv, our initial manuscript was uploaded by mistake and we\n  found many inappropriate and unmodified parts of it (such as the experimental\n  results in Table 2,3, the Equation 13). I am sorry to say that this work\n  still needs to be further completed and we do not intend to use it for\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although existing person re-identification (Re-ID) methods have shown\nimpressive accuracy, most of them usually suffer from poor generalization on\nunseen target domain. Thus, generalizable person Re-ID has recently drawn\nincreasing attention, which trains a model on source domains that generalizes\nwell on unseen target domain without model updating. In this work, we propose a\nnovel adaptive domain-specific normalization approach (AdsNorm) for\ngeneralizable person Re-ID. It describes unseen target domain as a combination\nof the known source ones, and explicitly learns domain-specific representation\nwith target distribution to improve the model's generalization by a\nmeta-learning pipeline. Specifically, AdsNorm utilizes batch normalization\nlayers to collect individual source domains' characteristics, and maps source\ndomains into a shared latent space by using these characteristics, where the\ndomain relevance is measured by a distance function of different\ndomain-specific normalization statistics and features. At the testing stage,\nAdsNorm projects images from unseen target domain into the same latent space,\nand adaptively integrates the domain-specific features carrying the source\ndistributions by domain relevance for learning more generalizable aggregated\nrepresentation on unseen target domain. Considering that target domain is\nunavailable during training, a meta-learning algorithm combined with a\ncustomized relation loss is proposed to optimize an effective and efficient\nensemble model. Extensive experiments demonstrate that AdsNorm outperforms the\nstate-of-the-art methods. The code is available at:\nhttps://github.com/hzphzp/AdsNorm.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 02:54:55 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 02:12:15 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Liu", "Jiawei", ""], ["Huang", "Zhipeng", ""], ["Zheng", "Kecheng", ""], ["Liu", "Dong", ""], ["Sun", "Xiaoyan", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2105.03049", "submitter": "Furao Shen", "authors": "Shaokui Jiang, Baile Xu, Jian Zhao, Furao Shen", "title": "Faster and Simpler Siamese Network for Single Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Single object tracking (SOT) is currently one of the most important tasks in\ncomputer vision. With the development of the deep network and the release for a\nseries of large scale datasets for single object tracking, siamese networks\nhave been proposed and perform better than most of the traditional methods.\nHowever, recent siamese networks get deeper and slower to obtain better\nperformance. Most of these methods could only meet the needs of real-time\nobject tracking in ideal environments. In order to achieve a better balance\nbetween efficiency and accuracy, we propose a simpler siamese network for\nsingle object tracking, which runs fast in poor hardware configurations while\nremaining an excellent accuracy. We use a more efficient regression method to\ncompute the location of the tracked object in a shorter time without losing\nmuch precision. For improving the accuracy and speeding up the training\nprogress, we introduce the Squeeze-and-excitation (SE) network into the feature\nextractor. In this paper, we compare the proposed method with some\nstate-of-the-art trackers and analysis their performances. Using our method, a\nsiamese network could be trained with shorter time and less data. The fast\nprocessing speed enables combining object tracking with object detection or\nother tasks in real time.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 03:37:19 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Jiang", "Shaokui", ""], ["Xu", "Baile", ""], ["Zhao", "Jian", ""], ["Shen", "Furao", ""]]}, {"id": "2105.03053", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, Jing Zhang, Gang Xu, Ming-Ming Cheng, Ling Shao", "title": "Salient Objects in Clutter", "comments": "348 references, 22 pages, survey 201 models, benchmark 100 models.\n  Online benchmark: http://dpfan.net/SOCBenchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper identifies and addresses a serious design bias of existing salient\nobject detection (SOD) datasets, which unrealistically assume that each image\nshould contain at least one clear and uncluttered salient object. This design\nbias has led to a saturation in performance for state-of-the-art SOD models\nwhen evaluated on existing datasets. However, these models are still far from\nsatisfactory when applied to real-world scenes. Based on our analyses, we\npropose a new high-quality dataset and update the previous saliency benchmark.\nSpecifically, our dataset, called Salient Objects in Clutter (SOC), includes\nimages with both salient and non-salient objects from several common object\ncategories. In addition to object category annotations, each salient image is\naccompanied by attributes that reflect common challenges in real-world scenes,\nwhich can help provide deeper insight into the SOD problem. Further, with a\ngiven saliency encoder, e.g., the backbone network, existing saliency models\nare designed to achieve mapping from the training image set to the training\nground-truth set. We, therefore, argue that improving the dataset can yield\nhigher performance gains than focusing only on the decoder design. With this in\nmind, we investigate several dataset-enhancement strategies, including label\nsmoothing to implicitly emphasize salient boundaries, random image augmentation\nto adapt saliency models to various scenarios, and self-supervised learning as\na regularization strategy to learn from small datasets. Our extensive results\ndemonstrate the effectiveness of these tricks. We also provide a comprehensive\nbenchmark for SOD, which can be found in our repository:\nhttp://dpfan.net/SOCBenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 03:49:26 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Zhang", "Jing", ""], ["Xu", "Gang", ""], ["Cheng", "Ming-Ming", ""], ["Shao", "Ling", ""]]}, {"id": "2105.03056", "submitter": "Joshua Ball", "authors": "Joshua Ball", "title": "Few-Shot Learning for Image Classification of Common Flora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of meta-learning and transfer learning in the task of few-shot image\nclassification is a well researched area with many papers showcasing the\nadvantages of transfer learning over meta-learning in cases where data is\nplentiful and there is no major limitations to computational resources. In this\npaper we will showcase our experimental results from testing various\nstate-of-the-art transfer learning weights and architectures versus similar\nstate-of-the-art works in the meta-learning field for image classification\nutilizing Model-Agnostic Meta Learning (MAML). Our results show that both\npractices provide adequate performance when the dataset is sufficiently large,\nbut that they both also struggle when data sparsity is introduced to maintain\nsufficient performance. This problem is moderately reduced with the use of\nimage augmentation and the fine-tuning of hyperparameters. In this paper we\nwill discuss: (1) our process of developing a robust multi-class convolutional\nneural network (CNN) for the task of few-shot image classification, (2)\ndemonstrate that transfer learning is the superior method of helping create an\nimage classification model when the dataset is large and (3) that MAML\noutperforms transfer learning in the case where data is very limited. The code\nis available here: github.com/JBall1/Few-Shot-Limited-Data\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 03:54:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ball", "Joshua", ""]]}, {"id": "2105.03059", "submitter": "Xiaoshuang Shi", "authors": "Xiaoshuang Shi, Zhenhua Guo, Fuyong Xing, Yun Liang, Xiaofeng Zhu", "title": "Self-paced Resistance Learning against Overfitting on Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy labels composed of correct and corrupted ones are pervasive in\npractice. They might significantly deteriorate the performance of convolutional\nneural networks (CNNs), because CNNs are easily overfitted on corrupted labels.\nTo address this issue, inspired by an observation, deep neural networks might\nfirst memorize the probably correct-label data and then corrupt-label samples,\nwe propose a novel yet simple self-paced resistance framework to resist\ncorrupted labels, without using any clean validation data. The proposed\nframework first utilizes the memorization effect of CNNs to learn a curriculum,\nwhich contains confident samples and provides meaningful supervision for other\ntraining samples. Then it adopts selected confident samples and a proposed\nresistance loss to update model parameters; the resistance loss tends to smooth\nmodel parameters' update or attain equivalent prediction over each class,\nthereby resisting model overfitting on corrupted labels. Finally, we unify\nthese two modules into a single loss function and optimize it in an alternative\nlearning. Extensive experiments demonstrate the significantly superior\nperformance of the proposed framework over recent state-of-the-art methods on\nnoisy-label data. Source codes of the proposed method are available on\nhttps://github.com/xsshi2015/Self-paced-Resistance-Learning.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 04:17:20 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shi", "Xiaoshuang", ""], ["Guo", "Zhenhua", ""], ["Xing", "Fuyong", ""], ["Liang", "Yun", ""], ["Zhu", "Xiaofeng", ""]]}, {"id": "2105.03068", "submitter": "Bao Yiming", "authors": "Yiming Bao, Jun Wang, Tong Li, Linyan Wang, Jianwei Xu, Juan Ye and\n  Dahong Qian", "title": "Self-Adaptive Transfer Learning for Multicenter Glaucoma Classification\n  in Fundus Retina Images", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early diagnosis and screening of glaucoma are important for patients to\nreceive treatment in time and maintain eyesight. Nowadays, deep learning (DL)\nbased models have been successfully used for computer-aided diagnosis (CAD) of\nglaucoma from retina fundus images. However, a DL model pre-trained using a\ndataset from one hospital center may have poor performance on a dataset from\nanother new hospital center and therefore its applications in the real scene\nare limited. In this paper, we propose a self-adaptive transfer learning (SATL)\nstrategy to fill the domain gap between multicenter datasets. Specifically, the\nencoder of a DL model that is pre-trained on the source domain is used to\ninitialize the encoder of a reconstruction model. Then, the reconstruction\nmodel is trained using only unlabeled image data from the target domain, which\nmakes the encoder in the model adapt itself to extract useful high-level\nfeatures both for target domain images encoding and glaucoma classification,\nsimultaneously. Experimental results demonstrate that the proposed SATL\nstrategy is effective in the domain adaptation task between a private and two\npublic glaucoma diagnosis datasets, i.e. pri-RFG, REFUGE, and LAG. Moreover,\nthe proposed strategy is completely independent of the source domain data,\nwhich meets the real scene application and the privacy protection policy.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 05:20:37 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bao", "Yiming", ""], ["Wang", "Jun", ""], ["Li", "Tong", ""], ["Wang", "Linyan", ""], ["Xu", "Jianwei", ""], ["Ye", "Juan", ""], ["Qian", "Dahong", ""]]}, {"id": "2105.03072", "submitter": "Haoming Cai", "authors": "Jinjin Gu and Haoming Cai and Chao Dong and Jimmy S. Ren and Yu Qiao\n  and Shuhang Gu and Radu Timofte and Manri Cheon and Sungjun Yoon and\n  Byungyeon Kang and Junwoo Lee and Qing Zhang and Haiyang Guo and Yi Bin and\n  Yuqing Hou and Hengliang Luo and Jingyu Guo and Zirui Wang and Hai Wang and\n  Wenming Yang and Qingyan Bai and Shuwei Shi and Weihao Xia and Mingdeng Cao\n  and Jiahao Wang and Yifan Chen and Yujiu Yang and Yang Li and Tao Zhang and\n  Longtao Feng and Yiting Liao and Junlin Li and William Thong and Jose Costa\n  Pereira and Ales Leonardis and Steven McDonagh and Kele Xu and Lehan Yang and\n  Hengxing Cai and Pengfei Sun and Seyed Mehdi Ayyoubzadeh and Ali Royat and\n  Sid Ahmed Fezza and Dounia Hammou and Wassim Hamidouche and Sewoong Ahn and\n  Gwangjin Yoon and Koki Tsubota and Hiroaki Akutsu and Kiyoharu Aizawa", "title": "NTIRE 2021 Challenge on Perceptual Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports on the NTIRE 2021 challenge on perceptual image quality\nassessment (IQA), held in conjunction with the New Trends in Image Restoration\nand Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image\nprocessing technology, perceptual image processing algorithms based on\nGenerative Adversarial Networks (GAN) have produced images with more realistic\ntextures. These output images have completely different characteristics from\ntraditional distortions, thus pose a new challenge for IQA methods to evaluate\ntheir visual quality. In comparison with previous IQA challenges, the training\nand testing datasets in this challenge include the outputs of perceptual image\nprocessing algorithms and the corresponding subjective scores. Thus they can be\nused to develop and evaluate IQA methods on GAN-based distortions. The\nchallenge has 270 registered participants in total. In the final testing stage,\n13 participating teams submitted their models and fact sheets. Almost all of\nthem have achieved much better results than existing IQA methods, while the\nwinning method can demonstrate state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 05:36:54 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:39:54 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 14:17:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gu", "Jinjin", ""], ["Cai", "Haoming", ""], ["Dong", "Chao", ""], ["Ren", "Jimmy S.", ""], ["Qiao", "Yu", ""], ["Gu", "Shuhang", ""], ["Timofte", "Radu", ""], ["Cheon", "Manri", ""], ["Yoon", "Sungjun", ""], ["Kang", "Byungyeon", ""], ["Lee", "Junwoo", ""], ["Zhang", "Qing", ""], ["Guo", "Haiyang", ""], ["Bin", "Yi", ""], ["Hou", "Yuqing", ""], ["Luo", "Hengliang", ""], ["Guo", "Jingyu", ""], ["Wang", "Zirui", ""], ["Wang", "Hai", ""], ["Yang", "Wenming", ""], ["Bai", "Qingyan", ""], ["Shi", "Shuwei", ""], ["Xia", "Weihao", ""], ["Cao", "Mingdeng", ""], ["Wang", "Jiahao", ""], ["Chen", "Yifan", ""], ["Yang", "Yujiu", ""], ["Li", "Yang", ""], ["Zhang", "Tao", ""], ["Feng", "Longtao", ""], ["Liao", "Yiting", ""], ["Li", "Junlin", ""], ["Thong", "William", ""], ["Pereira", "Jose Costa", ""], ["Leonardis", "Ales", ""], ["McDonagh", "Steven", ""], ["Xu", "Kele", ""], ["Yang", "Lehan", ""], ["Cai", "Hengxing", ""], ["Sun", "Pengfei", ""], ["Ayyoubzadeh", "Seyed Mehdi", ""], ["Royat", "Ali", ""], ["Fezza", "Sid Ahmed", ""], ["Hammou", "Dounia", ""], ["Hamidouche", "Wassim", ""], ["Ahn", "Sewoong", ""], ["Yoon", "Gwangjin", ""], ["Tsubota", "Koki", ""], ["Akutsu", "Hiroaki", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2105.03085", "submitter": "Haoming Cai", "authors": "Haoming Cai and Jingwen He and Qiao Yu and Chao Dong", "title": "Toward Interactive Modulation for Photo-Realistic Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modulating image restoration level aims to generate a restored image by\naltering a factor that represents the restoration strength. Previous works\nmainly focused on optimizing the mean squared reconstruction error, which\nbrings high reconstruction accuracy but lacks finer texture details. This paper\npresents a Controllable Unet Generative Adversarial Network (CUGAN) to generate\nhigh-frequency textures in the modulation tasks. CUGAN consists of two modules\n-- base networks and condition networks. The base networks comprise a generator\nand a discriminator. In the generator, we realize the interactive control of\nrestoration levels by tuning the weights of different features from different\nscales in the Unet architecture. Moreover, we adaptively modulate the\nintermediate features in the discriminator according to the severity of\ndegradations. The condition networks accept the condition vector (encoded\ndegradation information) as input, then generate modulation parameters for both\nthe generator and the discriminator. During testing, users can control the\noutput effects by tweaking the condition vector. We also provide a smooth\ntransition between GAN and MSE effects by a simple transition method. Extensive\nexperiments demonstrate that the proposed CUGAN achieves excellent performance\non image restoration modulation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:05:56 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Cai", "Haoming", ""], ["He", "Jingwen", ""], ["Yu", "Qiao", ""], ["Dong", "Chao", ""]]}, {"id": "2105.03089", "submitter": "Lu Liu", "authors": "Lu Liu, Robby T. Tan", "title": "Human Object Interaction Detection using Two-Direction Spatial\n  Enhancement and Exclusive Object Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection aims to detect visual relations\nbetween human and objects in images. One significant problem of HOI detection\nis that non-interactive human-object pair can be easily mis-grouped and\nmisclassified as an action, especially when humans are close and performing\nsimilar actions in the scene. To address the mis-grouping problem, we propose a\nspatial enhancement approach to enforce fine-level spatial constraints in two\ndirections from human body parts to the object center, and from object parts to\nthe human center. At inference, we propose a human-object regrouping approach\nby considering the object-exclusive property of an action, where the target\nobject should not be shared by more than one human. By suppressing\nnon-interactive pairs, our approach can decrease the false positives.\nExperiments on V-COCO and HICO-DET datasets demonstrate our approach is more\nrobust compared to the existing methods under the presence of multiple humans\nand objects in the scene.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:18:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Liu", "Lu", ""], ["Tan", "Robby T.", ""]]}, {"id": "2105.03091", "submitter": "Ming Xu", "authors": "Ming Xu, Niko S\\\"underhauf, Michael Milford", "title": "Probabilistic Visual Place Recognition for Hierarchical Localization", "comments": "8 pages, 4 figures, RA-L standalone", "journal-ref": null, "doi": "10.1109/LRA.2020.3040134", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual localization techniques often comprise a hierarchical localization\npipeline, with a visual place recognition module used as a coarse localizer to\ninitialize a pose refinement stage. While improving the pose refinement step\nhas been the focus of much recent research, most work on the coarse\nlocalization stage has focused on improvements like increased invariance to\nappearance change, without improving what can be loose error tolerances. In\nthis letter, we propose two methods which adapt image retrieval techniques used\nfor visual place recognition to the Bayesian state estimation formulation for\nlocalization. We demonstrate significant improvements to the localization\naccuracy of the coarse localization stage using our methods, whilst retaining\nstate-of-the-art performance under severe appearance change. Using extensive\nexperimentation on the Oxford RobotCar dataset, results show that our approach\noutperforms comparable state-of-the-art methods in terms of precision-recall\nperformance for localizing image sequences. In addition, our proposed methods\nprovides the flexibility to contextually scale localization latency in order to\nachieve these improvements. The improved initial localization estimate opens up\nthe possibility of both improved overall localization performance and modified\npose refinement techniques that leverage this improved spatial prior.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:39:14 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Xu", "Ming", ""], ["S\u00fcnderhauf", "Niko", ""], ["Milford", "Michael", ""]]}, {"id": "2105.03117", "submitter": "Jamie Seol", "authors": "Hanbit Lee, Jinseok Seol, Sang-goo Lee", "title": "Contrastive Learning for Unsupervised Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation aims to learn a mapping between different groups\nof visually distinguishable images. While recent methods have shown impressive\nability to change even intricate appearance of images, they still rely on\ndomain labels in training a model to distinguish between distinct visual\nfeatures. Such dependency on labels often significantly limits the scope of\napplications since consistent and high-quality labels are expensive. Instead,\nwe wish to capture visual features from images themselves and apply them to\nenable realistic translation without human-generated labels. To this end, we\npropose an unsupervised image-to-image translation method based on contrastive\nlearning. The key idea is to learn a discriminator that differentiates between\ndistinctive styles and let the discriminator supervise a generator to transfer\nthose styles across images. During training, we randomly sample a pair of\nimages and train the generator to change the appearance of one towards another\nwhile keeping the original structure. Experimental results show that our method\noutperforms the leading unsupervised baselines in terms of visual quality and\ntranslation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:43:38 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Lee", "Hanbit", ""], ["Seol", "Jinseok", ""], ["Lee", "Sang-goo", ""]]}, {"id": "2105.03120", "submitter": "Berivan Isik", "authors": "Berivan Isik", "title": "Neural 3D Scene Compression via Model Compression", "comments": "Stanford CS 231A Final Project, 2021. WiCV at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering 3D scenes requires access to arbitrary viewpoints from the scene.\nStorage of such a 3D scene can be done in two ways; (1) storing 2D images taken\nfrom the 3D scene that can reconstruct the scene back through interpolations,\nor (2) storing a representation of the 3D scene itself that already encodes\nviews from all directions. So far, traditional 3D compression methods have\nfocused on the first type of storage and compressed the original 2D images with\nimage compression techniques. With this approach, the user first decodes the\nstored 2D images and then renders the 3D scene. However, this separated\nprocedure is inefficient since a large amount of 2D images have to be stored.\nIn this work, we take a different approach and compress a functional\nrepresentation of 3D scenes. In particular, we introduce a method to compress\n3D scenes by compressing the neural networks that represent the scenes as\nneural radiance fields. Our method provides more efficient storage of 3D scenes\nsince it does not store 2D images -- which are redundant when we render the\nscene from the neural functional representation.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:50:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Isik", "Berivan", ""]]}, {"id": "2105.03136", "submitter": "Parth Kothari", "authors": "Parth Kothari, Brian Sifringer and Alexandre Alahi", "title": "Interpretable Social Anchors for Human Trajectory Forecasting in Crowds", "comments": "To appear in Computer Vision and Pattern Recognition (CVPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trajectory forecasting in crowds, at its core, is a sequence prediction\nproblem with specific challenges of capturing inter-sequence dependencies\n(social interactions) and consequently predicting socially-compliant multimodal\ndistributions. In recent years, neural network-based methods have been shown to\noutperform hand-crafted methods on distance-based metrics. However, these\ndata-driven methods still suffer from one crucial limitation: lack of\ninterpretability. To overcome this limitation, we leverage the power of\ndiscrete choice models to learn interpretable rule-based intents, and\nsubsequently utilise the expressibility of neural networks to model\nscene-specific residual. Extensive experimentation on the interaction-centric\nbenchmark TrajNet++ demonstrates the effectiveness of our proposed architecture\nto explain its predictions without compromising the accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 09:22:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kothari", "Parth", ""], ["Sifringer", "Brian", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2105.03139", "submitter": "Mingyuan Mao", "authors": "Mingyuan Mao, Baochang Zhang, David Doermann, Jie Guo, Shumin Han,\n  Yuan Feng, Xiaodi Wang, Errui Ding", "title": "Probabilistic Ranking-Aware Ensembles for Enhanced Object Detections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model ensembles are becoming one of the most effective approaches for\nimproving object detection performance already optimized for a single detector.\nConventional methods directly fuse bounding boxes but typically fail to\nconsider proposal qualities when combining detectors. This leads to a new\nproblem of confidence discrepancy for the detector ensembles. The confidence\nhas little effect on single detectors but significantly affects detector\nensembles. To address this issue, we propose a novel ensemble called the\nProbabilistic Ranking Aware Ensemble (PRAE) that refines the confidence of\nbounding boxes from detectors. By simultaneously considering the category and\nthe location on the same validation set, we obtain a more reliable confidence\nbased on statistical probability. We can then rank the detected bounding boxes\nfor assembly. We also introduce a bandit approach to address the confidence\nimbalance problem caused by the need to deal with different numbers of boxes at\ndifferent confidence levels. We use our PRAE-based non-maximum suppression\n(P-NMS) to replace the conventional NMS method in ensemble learning.\nExperiments on the PASCAL VOC and COCO2017 datasets demonstrate that our PRAE\nmethod consistently outperforms state-of-the-art methods by significant\nmargins.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 09:37:06 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Mao", "Mingyuan", ""], ["Zhang", "Baochang", ""], ["Doermann", "David", ""], ["Guo", "Jie", ""], ["Han", "Shumin", ""], ["Feng", "Yuan", ""], ["Wang", "Xiaodi", ""], ["Ding", "Errui", ""]]}, {"id": "2105.03142", "submitter": "Frank Po Wen Lo", "authors": "Frank Po Wen Lo, Modou L Jobarteh, Yingnan Sun, Jianing Qiu, Shuo\n  Jiang, Gary Frost, Benny Lo", "title": "An Intelligent Passive Food Intake Assessment System with Egocentric\n  Cameras", "comments": "11 pages, 14 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malnutrition is a major public health concern in low-and-middle-income\ncountries (LMICs). Understanding food and nutrient intake across communities,\nhouseholds and individuals is critical to the development of health policies\nand interventions. To ease the procedure in conducting large-scale dietary\nassessments, we propose to implement an intelligent passive food intake\nassessment system via egocentric cameras particular for households in Ghana and\nUganda. Algorithms are first designed to remove redundant images for minimising\nthe storage memory. At run time, deep learning-based semantic segmentation is\napplied to recognise multi-food types and newly-designed handcrafted features\nare extracted for further consumed food weight monitoring. Comprehensive\nexperiments are conducted to validate our methods on an in-the-wild dataset\ncaptured under the settings which simulate the unique LMIC conditions with\nparticipants of Ghanaian and Kenyan origin eating common Ghanaian/Kenyan\ndishes. To demonstrate the efficacy, experienced dietitians are involved in\nthis research to perform the visual portion size estimation, and their\npredictions are compared to our proposed method. The promising results have\nshown that our method is able to reliably monitor food intake and give feedback\non users' eating behaviour which provides guidance for dietitians in regular\ndietary assessment.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 09:47:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Lo", "Frank Po Wen", ""], ["Jobarteh", "Modou L", ""], ["Sun", "Yingnan", ""], ["Qiu", "Jianing", ""], ["Jiang", "Shuo", ""], ["Frost", "Gary", ""], ["Lo", "Benny", ""]]}, {"id": "2105.03148", "submitter": "Pingli Ma", "authors": "Chen Li, Pingli Ma, Md Mamunur Rahaman, Yudong Yao, Jiawei Zhang,\n  Shuojia Zou, Xin Zhao, Marcin Grzegorzek", "title": "A State-of-the-art Survey of Object Detection Techniques in\n  Microorganism Image Analysis: from Traditional Image Processing and Classical\n  Machine Learning to Current Deep Convolutional Neural Networks and Potential\n  Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microorganisms play a vital role in human life. Therefore, microorganism\ndetection is of great significance to human beings. However, the traditional\nmanual microscopic detection methods have the disadvantages of long detection\ncycle, low detection accuracy in large orders, and great difficulty in\ndetecting uncommon microorganisms. Therefore, it is meaningful to apply\ncomputer image analysis technology to the field of microorganism detection.\nComputer image analysis can realize high-precision and high-efficiency\ndetection of microorganisms. In this review, first,we analyse the existing\nmicroorganism detection methods in chronological order, from traditional image\nprocessing and traditional machine learning to deep learning methods. Then, we\nanalyze and summarize these existing methods and introduce some potential\nmethods, including visual transformers. In the end, the future development\ndirection and challenges of microorganism detection are discussed. In general,\nwe have summarized 137 related technical papers from 1985 to the present. This\nreview will help researchers have a more comprehensive understanding of the\ndevelopment process, research status, and future trends in the field of\nmicroorganism detection and provide a reference for researchers in other\nfields.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:18:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Li", "Chen", ""], ["Ma", "Pingli", ""], ["Rahaman", "Md Mamunur", ""], ["Yao", "Yudong", ""], ["Zhang", "Jiawei", ""], ["Zou", "Shuojia", ""], ["Zhao", "Xin", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2105.03151", "submitter": "Dong Zhao", "authors": "Shuang Wang, Dong Zhao, Yi Li, Chi Zhang, Yuwei Guo, Qi Zang, Biao\n  Hou, Licheng Jiao", "title": "More Separable and Easier to Segment: A Cluster Alignment Method for\n  Cross-Domain Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature alignment between domains is one of the mainstream methods for\nUnsupervised Domain Adaptation (UDA) semantic segmentation. Existing feature\nalignment methods for semantic segmentation learn domain-invariant features by\nadversarial training to reduce domain discrepancy, but they have two limits: 1)\nassociations among pixels are not maintained, 2) the classifier trained on the\nsource domain couldn't adapted well to the target. In this paper, we propose a\nnew UDA semantic segmentation approach based on domain closeness assumption to\nalleviate the above problems. Specifically, a prototype clustering strategy is\napplied to cluster pixels with the same semantic, which will better maintain\nassociations among target domain pixels during the feature alignment. After\nclustering, to make the classifier more adaptive, a normalized cut loss based\non the affinity graph of the target domain is utilized, which will make the\ndecision boundary target-specific. Sufficient experiments conducted on GTA5\n$\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Cityscapes proved the\neffectiveness of our method, which illustrated that our results achieved the\nnew state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:24:18 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Shuang", ""], ["Zhao", "Dong", ""], ["Li", "Yi", ""], ["Zhang", "Chi", ""], ["Guo", "Yuwei", ""], ["Zang", "Qi", ""], ["Hou", "Biao", ""], ["Jiao", "Licheng", ""]]}, {"id": "2105.03162", "submitter": "Bangjie Yin", "authors": "Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo, Zelun Kong,\n  Shouhong Ding, Jilin Li and Cong Liu", "title": "Adv-Makeup: A New Imperceptible and Transferable Attack on Face\n  Recognition", "comments": "8 pages, 6 figures, 1 tables, 1 algorithm, To appear in IJCAI 2021 as\n  a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks, particularly face recognition models, have been shown\nto be vulnerable to both digital and physical adversarial examples. However,\nexisting adversarial examples against face recognition systems either lack\ntransferability to black-box models, or fail to be implemented in practice. In\nthis paper, we propose a unified adversarial face generation method -\nAdv-Makeup, which can realize imperceptible and transferable attack under\nblack-box setting. Adv-Makeup develops a task-driven makeup generation method\nwith the blending module to synthesize imperceptible eye shadow over the\norbital region on faces. And to achieve transferability, Adv-Makeup implements\na fine-grained meta-learning adversarial attack strategy to learn more general\nattack features from various models. Compared to existing techniques,\nsufficient visualization results demonstrate that Adv-Makeup is capable to\ngenerate much more imperceptible attacks under both digital and physical\nscenarios. Meanwhile, extensive quantitative experiments show that Adv-Makeup\ncan significantly improve the attack success rate under black-box setting, even\nattacking commercial systems.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:00:35 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yin", "Bangjie", ""], ["Wang", "Wenxuan", ""], ["Yao", "Taiping", ""], ["Guo", "Junfeng", ""], ["Kong", "Zelun", ""], ["Ding", "Shouhong", ""], ["Li", "Jilin", ""], ["Liu", "Cong", ""]]}, {"id": "2105.03164", "submitter": "Steve Dias Da Cruz", "authors": "Steve Dias Da Cruz and Bertram Taetz and Oliver Wasenm\\\"uller and\n  Thomas Stifter and Didier Stricker", "title": "Autoencoder Based Inter-Vehicle Generalization for In-Cabin Occupant\n  Classification", "comments": "This paper has been accepted at IEEE Intelligent Vehicles Symposium\n  (IV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common domain shift problem formulations consider the integration of multiple\nsource domains, or the target domain during training. Regarding the\ngeneralization of machine learning models between different car interiors, we\nformulate the criterion of training in a single vehicle: without access to the\ntarget distribution of the vehicle the model would be deployed to, neither with\naccess to multiple vehicles during training. We performed an investigation on\nthe SVIRO dataset for occupant classification on the rear bench and propose an\nautoencoder based approach to improve the transferability. The autoencoder is\non par with commonly used classification models when trained from scratch and\nsometimes out-performs models pre-trained on a large amount of data. Moreover,\nthe autoencoder can transform images from unknown vehicles into the vehicle it\nwas trained on. These results are corroborated by an evaluation on real\ninfrared images from two vehicle interiors.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:15:18 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Da Cruz", "Steve Dias", ""], ["Taetz", "Bertram", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stifter", "Thomas", ""], ["Stricker", "Didier", ""]]}, {"id": "2105.03186", "submitter": "Miao Hu", "authors": "Miao Hu and Yali Li and Lu Fang and Shengjin Wang", "title": "A^2-FPN: Attention Aggregation based Feature Pyramid Network for\n  Instance Segmentation", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning pyramidal feature representations is crucial for recognizing object\ninstances at different scales. Feature Pyramid Network (FPN) is the classic\narchitecture to build a feature pyramid with high-level semantics throughout.\nHowever, intrinsic defects in feature extraction and fusion inhibit FPN from\nfurther aggregating more discriminative features. In this work, we propose\nAttention Aggregation based Feature Pyramid Network (A^2-FPN), to improve\nmulti-scale feature learning through attention-guided feature aggregation. In\nfeature extraction, it extracts discriminative features by\ncollecting-distributing multi-level global context features, and mitigates the\nsemantic information loss due to drastically reduced channels. In feature\nfusion, it aggregates complementary information from adjacent features to\ngenerate location-wise reassembly kernels for content-aware sampling, and\nemploys channel-wise reweighting to enhance the semantic consistency before\nelement-wise addition. A^2-FPN shows consistent gains on different instance\nsegmentation frameworks. By replacing FPN with A^2-FPN in Mask R-CNN, our model\nboosts the performance by 2.1% and 1.6% mask AP when using ResNet-50 and\nResNet-101 as backbone, respectively. Moreover, A^2-FPN achieves an improvement\nof 2.0% and 1.4% mask AP when integrated into the strong baselines such as\nCascade Mask R-CNN and Hybrid Task Cascade.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:51:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hu", "Miao", ""], ["Li", "Yali", ""], ["Fang", "Lu", ""], ["Wang", "Shengjin", ""]]}, {"id": "2105.03211", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Shichen Liu, Yi Ma", "title": "NeRD: Neural 3D Reflection Symmetry Detector", "comments": "CVPR 2021. overlaps with arXiv:2006.10042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have shown that symmetry, a structural prior that most\nobjects exhibit, can support a variety of single-view 3D understanding tasks.\nHowever, detecting 3D symmetry from an image remains a challenging task.\nPrevious works either assume that the symmetry is given or detect the symmetry\nwith a heuristic-based method. In this paper, we present NeRD, a Neural 3D\nReflection Symmetry Detector, which combines the strength of learning-based\nrecognition and geometry-based reconstruction to accurately recover the normal\ndirection of objects' mirror planes. Specifically, we first enumerate the\nsymmetry planes with a coarse-to-fine strategy and then find the best ones by\nbuilding 3D cost volumes to examine the intra-image pixel correspondence from\nthe symmetry. Our experiments show that the symmetry planes detected with our\nmethod are significantly more accurate than the planes from direct CNN\nregression on both synthetic and real-world datasets. We also demonstrate that\nthe detected symmetry can be used to improve the performance of downstream\ntasks such as pose estimation and depth map regression. The code of this paper\nhas been made public at https://github.com/zhou13/nerd.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:25:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhou", "Yichao", ""], ["Liu", "Shichen", ""], ["Ma", "Yi", ""]]}, {"id": "2105.03232", "submitter": "Umair Iqbal Mr", "authors": "Umair Iqbal, Johan Barthelemy, Wanqing Li and Pascal Perez", "title": "Automating Visual Blockage Classification of Culverts with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockage of culverts by transported debris materials is reported as main\ncontributor in originating urban flash floods. Conventional modelling\napproaches had no success in addressing the problem largely because of\nunavailability of peak floods hydraulic data and highly non-linear behaviour of\ndebris at culvert. This article explores a new dimension to investigate the\nissue by proposing the use of Intelligent Video Analytic (IVA) algorithms for\nextracting blockage related information. Potential of using existing\nConvolutional Neural Network (CNN) algorithms (i.e., DarkNet53, DenseNet121,\nInceptionResNetV2, InceptionV3, MobileNet, ResNet50, VGG16, EfficientNetB3,\nNASNet) is investigated over a custom collected blockage dataset (i.e., Images\nof Culvert Openings and Blockage (ICOB)) to predict the blockage in a given\nimage. Models were evaluated based on their performance on test dataset (i.e.,\naccuracy, loss, precision, recall, F1-score, Jaccard-Index), Floating Point\nOperations Per Second (FLOPs) and response times to process a single test\ninstance. From the results, NASNet was reported most efficient in classifying\nthe blockage with the accuracy of 85\\%; however, EfficientNetB3 was recommended\nfor the hardware implementation because of its improved response time with\naccuracy comparable to NASNet (i.e., 83\\%). False Negative (FN) instances,\nFalse Positive (FP) instances and CNN layers activation suggested that\nbackground noise and oversimplified labelling criteria were two contributing\nfactors in degraded performance of existing CNN algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:40:09 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Iqbal", "Umair", ""], ["Barthelemy", "Johan", ""], ["Li", "Wanqing", ""], ["Perez", "Pascal", ""]]}, {"id": "2105.03233", "submitter": "Umair Iqbal Mr", "authors": "Umair Iqbal, Johan Barthelemy, Wanqing Li and Pascal Perez", "title": "Regression on Deep Visual Features using Artificial Neural Networks\n  (ANNs) to Predict Hydraulic Blockage at Culverts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross drainage hydraulic structures (i.e., culverts, bridges) in urban\nlandscapes are prone to getting blocked by transported debris which often\nresults in causing the flash floods. In context of Australia, Wollongong City\nCouncil (WCC) blockage conduit policy is the only formal guideline to consider\nblockage in design process. However, many argue that this policy is based on\nthe post floods visual inspections and hence can not be considered accurate\nrepresentation of hydraulic blockage. As a result of this on-going debate,\nvisual blockage and hydraulic blockage are considered two distinct terms with\nno established quantifiable relation among both. This paper attempts to relate\nboth terms by proposing the use of deep visual features for prediction of\nhydraulic blockage at a given culvert. An end-to-end machine learning pipeline\nis propounded which takes an image of culvert as input, extract visual features\nusing deep learning models, pre-process the visual features and feed into\nregression model to predict the corresponding hydraulic blockage. Dataset\n(i.e., Hydrology-Lab Dataset (HD), Visual Hydrology-Lab Dataset (VHD)) used in\nthis research was collected from in-lab experiments carried out using scaled\nphysical models of culverts where multiple blockage scenarios were replicated\nat scale. Performance of regression models was assessed using standard\nevaluation metrics. Furthermore, performance of overall machine learning\npipeline was assessed in terms of processing times for relative comparison of\nmodels and hardware requirement analysis. From the results ANN used with\nMobileNet extracted visual features achieved the best regression performance\nwith $R^{2}$ score of 0.7855. Positive value of $R^{2}$ score indicated the\npresence of correlation between visual features and hydraulic blockage and\nsuggested that both can be interrelated with each other.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 14:58:46 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Iqbal", "Umair", ""], ["Barthelemy", "Johan", ""], ["Li", "Wanqing", ""], ["Perez", "Pascal", ""]]}, {"id": "2105.03235", "submitter": "Arianna Salazar Miranda", "authors": "Arianna Salazar Miranda, Guangyu Du, Claire Gorman, Fabio Duarte,\n  Washington Fajardo, Carlo Ratti", "title": "Favelas 4D: Scalable methods for morphology analysis of informal\n  settlements using terrestrial laser scanning data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One billion people live in informal settlements worldwide. The complex and\nmultilayered spaces that characterize this unplanned form of urbanization pose\na challenge to traditional approaches to mapping and morphological analysis.\nThis study proposes a methodology to study the morphological properties of\ninformal settlements based on terrestrial LiDAR (Light Detection and Ranging)\ndata collected in Rocinha, the largest favela in Rio de Janeiro, Brazil. Our\nanalysis operates at two resolutions, including a \\emph{global} analysis\nfocused on comparing different streets of the favela to one another, and a\n\\emph{local} analysis unpacking the variation of morphological metrics within\nstreets. We show that our methodology reveals meaningful differences and\ncommonalities both in terms of the global morphological characteristics across\nstreets and their local distributions. Finally, we create morphological maps at\nhigh spatial resolution from LiDAR data, which can inform urban planning\nassessments of concerns related to crowding, structural safety, air quality,\nand accessibility in the favela. The methods for this study are automated and\ncan be easily scaled to analyze entire informal settlements, leveraging the\nincreasing availability of inexpensive LiDAR scanners on portable devices such\nas cellphones.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 15:32:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Miranda", "Arianna Salazar", ""], ["Du", "Guangyu", ""], ["Gorman", "Claire", ""], ["Duarte", "Fabio", ""], ["Fajardo", "Washington", ""], ["Ratti", "Carlo", ""]]}, {"id": "2105.03236", "submitter": "Mingkui Tan", "authors": "Guanghui Xu, Shuaicheng Niu, Mingkui Tan, Yucheng Luo, Qing Du, Qi Wu", "title": "Towards Accurate Text-based Image Captioning with Content Diversity\n  Exploration", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based image captioning (TextCap) which aims to read and reason images\nwith texts is crucial for a machine to understand a detailed and complex scene\nenvironment, considering that texts are omnipresent in daily life. This task,\nhowever, is very challenging because an image often contains complex texts and\nvisual information that is hard to be described comprehensively. Existing\nmethods attempt to extend the traditional image captioning methods to solve\nthis task, which focus on describing the overall scene of images by one global\ncaption. This is infeasible because the complex text and visual information\ncannot be described well within one caption. To resolve this difficulty, we\nseek to generate multiple captions that accurately describe different parts of\nan image in detail. To achieve this purpose, there are three key challenges: 1)\nit is hard to decide which parts of the texts of images to copy or paraphrase;\n2) it is non-trivial to capture the complex relationship between diverse texts\nin an image; 3) how to generate multiple captions with diverse content is still\nan open problem. To conquer these, we propose a novel Anchor-Captioner method.\nSpecifically, we first find the important tokens which are supposed to be paid\nmore attention to and consider them as anchors. Then, for each chosen anchor,\nwe group its relevant texts to construct the corresponding anchor-centred graph\n(ACG). Last, based on different ACGs, we conduct multi-view caption generation\nto improve the content diversity of generated captions. Experimental results\nshow that our method not only achieves SOTA performance but also generates\ndiverse captions to describe images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:57:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Xu", "Guanghui", ""], ["Niu", "Shuaicheng", ""], ["Tan", "Mingkui", ""], ["Luo", "Yucheng", ""], ["Du", "Qing", ""], ["Wu", "Qi", ""]]}, {"id": "2105.03237", "submitter": "Arnab Kumar Mondal", "authors": "Arnab Kumar Mondal, Vineet Jain and Kaleem Siddiqi", "title": "Mini-batch graphs for robust image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning models for classification tasks in computer vision are\ntrained using mini-batches. In the present article, we take advantage of the\nrelationships between samples in a mini-batch, using graph neural networks to\naggregate information from similar images. This helps mitigate the adverse\neffects of alterations to the input images on classification performance.\nDiverse experiments on image-based object and scene classification show that\nthis approach not only improves a classifier's performance but also increases\nits robustness to image perturbations and adversarial attacks. Further, we also\nshow that mini-batch graph neural networks can help to alleviate the problem of\nmode collapse in Generative Adversarial Networks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 03:43:32 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Mondal", "Arnab Kumar", ""], ["Jain", "Vineet", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "2105.03245", "submitter": "Yulin Wang", "authors": "Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, Gao\n  Huang", "title": "Adaptive Focus for Efficient Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the spatial redundancy in video recognition with\nthe aim to improve the computational efficiency. It is observed that the most\ninformative region in each frame of a video is usually a small image patch,\nwhich shifts smoothly across frames. Therefore, we model the patch localization\nproblem as a sequential decision task, and propose a reinforcement learning\nbased approach for efficient spatially adaptive video recognition (AdaFocus).\nIn specific, a light-weighted ConvNet is first adopted to quickly process the\nfull video sequence, whose features are used by a recurrent policy network to\nlocalize the most task-relevant regions. Then the selected patches are inferred\nby a high-capacity network for the final prediction. During offline inference,\nonce the informative patch sequence has been generated, the bulk of computation\ncan be done in parallel, and is efficient on modern GPU devices. In addition,\nwe demonstrate that the proposed method can be easily extended by further\nconsidering the temporal redundancy, e.g., dynamically skipping less valuable\nframes. Extensive experiments on five benchmark datasets, i.e., ActivityNet,\nFCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is\nsignificantly more efficient than the competitive baselines. Code will be\navailable at https://github.com/blackfeather-wang/AdaFocus.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:24:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Yulin", ""], ["Chen", "Zhaoxi", ""], ["Jiang", "Haojun", ""], ["Song", "Shiji", ""], ["Han", "Yizeng", ""], ["Huang", "Gao", ""]]}, {"id": "2105.03247", "submitter": "Tiancai Wang", "authors": "Fangao Zeng, Bin Dong, Tiancai Wang, Cheng Chen, Xiangyu Zhang, Yichen\n  Wei", "title": "MOTR: End-to-End Multiple-Object Tracking with TRansformer", "comments": "Tech Report. Code is available at\n  https://github.com/megvii-model/MOTR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in multiple-object tracking (MOT) task is temporal modeling\nof the object under track. Existing tracking-by-detection methods adopt simple\nheuristics, such as spatial or appearance similarity. Such methods, in spite of\ntheir commonality, are overly simple and insufficient to model complex\nvariations, such as tracking through occlusion. Inherently, existing methods\nlack the ability to learn temporal variations from data. In this paper, we\npresent MOTR, the first fully end-to-end multiple-object tracking framework. It\nlearns to model the long-range temporal variation of the objects. It performs\ntemporal association implicitly and avoids previous explicit heuristics. Built\non Transformer and DETR, MOTR introduces the concept of \"track query\". Each\ntrack query models the entire track of an object. It is transferred and updated\nframe-by-frame to perform object detection and tracking, in a seamless manner.\nTemporal aggregation network combined with multi-frame training is proposed to\nmodel the long-range temporal relation. Experimental results show that MOTR\nachieves state-of-the-art performance. Code is available at\nhttps://github.com/megvii-model/MOTR.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:27:01 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zeng", "Fangao", ""], ["Dong", "Bin", ""], ["Wang", "Tiancai", ""], ["Chen", "Cheng", ""], ["Zhang", "Xiangyu", ""], ["Wei", "Yichen", ""]]}, {"id": "2105.03260", "submitter": "Liu Liu", "authors": "Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, Cewu Lu", "title": "Towards Real-World Category-level Articulation Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human life is populated with articulated objects. Current Category-level\nArticulation Pose Estimation (CAPE) methods are studied under the\nsingle-instance setting with a fixed kinematic structure for each category.\nConsidering these limitations, we reform this problem setting for real-world\nenvironments and suggest a CAPE-Real (CAPER) task setting. This setting allows\nvaried kinematic structures within a semantic category, and multiple instances\nto co-exist in an observation of real world. To support this task, we build an\narticulated model repository ReArt-48 and present an efficient dataset\ngeneration pipeline, which contains Fast Articulated Object Modeling (FAOM) and\nSemi-Authentic MixEd Reality Technique (SAMERT). Accompanying the pipeline, we\nbuild a large-scale mixed reality dataset ReArtMix and a real world dataset\nReArtVal. We also propose an effective framework ReArtNOCS that exploits RGB-D\ninput to estimate part-level pose for multiple instances in a single forward\npass. Extensive experiments demonstrate that the proposed ReArtNOCS can achieve\ngood performance on both CAPER and CAPE settings. We believe it could serve as\na strong baseline for future research on the CAPER task.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:41:16 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Liu", "Liu", ""], ["Xue", "Han", ""], ["Xu", "Wenqiang", ""], ["Fu", "Haoyuan", ""], ["Lu", "Cewu", ""]]}, {"id": "2105.03270", "submitter": "Ergin Genc", "authors": "Ergin Utku Genc, Nilesh Ahuja, Ibrahima J Ndiour, Omesh Tickoo", "title": "Energy-Based Anomaly Detection and Localization", "comments": "9 pages, 3 figures, as submitted to EBM ICLR 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief sketches initial progress towards a unified energy-based solution\nfor the semi-supervised visual anomaly detection and localization problem. In\nthis setup, we have access to only anomaly-free training data and want to\ndetect and identify anomalies of an arbitrary nature on test data. We employ\nthe density estimates from the energy-based model (EBM) as normalcy scores that\ncan be used to discriminate normal images from anomalous ones. Further, we\nback-propagate the gradients of the energy score with respect to the image in\norder to generate a gradient map that provides pixel-level spatial localization\nof the anomalies in the image. In addition to the spatial localization, we show\nthat simple processing of the gradient map can also provide alternative\nnormalcy scores that either match or surpass the detection performance obtained\nwith the energy value. To quantitatively validate the performance of the\nproposed method, we conduct experiments on the MVTec industrial dataset. Though\nstill preliminary, our results are very promising and reveal the potential of\nEBMs for simultaneously detecting and localizing unforeseen anomalies in\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:49:17 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Genc", "Ergin Utku", ""], ["Ahuja", "Nilesh", ""], ["Ndiour", "Ibrahima J", ""], ["Tickoo", "Omesh", ""]]}, {"id": "2105.03303", "submitter": "Jun-Jie Huang", "authors": "Jun-Jie Huang, Pier Luigi Dragotti", "title": "LINN: Lifting Inspired Invertible Neural Network for Image Denoising", "comments": "Accepted by the 29th European Signal Processing Conference, EUSIPCO\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose an invertible neural network for image denoising\n(DnINN) inspired by the transform-based denoising framework. The proposed DnINN\nconsists of an invertible neural network called LINN whose architecture is\ninspired by the lifting scheme in wavelet theory and a sparsity-driven\ndenoising network which is used to remove noise from the transform\ncoefficients. The denoising operation is performed with a single\nsoft-thresholding operation or with a learned iterative shrinkage thresholding\nnetwork. The forward pass of LINN produces an over-complete representation\nwhich is more suitable for denoising. The denoised image is reconstructed using\nthe backward pass of LINN using the output of the denoising network. The\nsimulation results show that the proposed DnINN method achieves results\ncomparable to the DnCNN method while only requiring 1/4 of learnable\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:52:48 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Huang", "Jun-Jie", ""], ["Dragotti", "Pier Luigi", ""]]}, {"id": "2105.03341", "submitter": "Yifei Zhang", "authors": "Yifei Zhang, Yu Zhou, Weiping Wang", "title": "Exploring Instance Relations for Unsupervised Feature Embedding", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great progress achieved in unsupervised feature embedding,\nexisting contrastive learning methods typically pursue view-invariant\nrepresentations through attracting positive sample pairs and repelling negative\nsample pairs in the embedding space, while neglecting to systematically explore\ninstance relations. In this paper, we explore instance relations including\nintra-instance multi-view relation and inter-instance interpolation relation\nfor unsupervised feature embedding. Specifically, we embed intra-instance\nmulti-view relation by aligning the distribution of the distance between an\ninstance's different augmented samples and negative samples. We explore\ninter-instance interpolation relation by transferring the ratio of information\nfor image sample interpolation from pixel space to feature embedding space. The\nproposed approach, referred to as EIR, is simple-yet-effective and can be\neasily inserted into existing view-invariant contrastive learning based\nmethods. Experiments conducted on public benchmarks for image classification\nand retrieval report state-of-the-art or comparable performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:47:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhang", "Yifei", ""], ["Zhou", "Yu", ""], ["Wang", "Weiping", ""]]}, {"id": "2105.03342", "submitter": "Jireh Jam", "authors": "Jireh Jam, Connah Kendrick, Vincent Drouard, Kevin Walker, Moi Hoon\n  Yap", "title": "Foreground-guided Facial Inpainting with Fidelity Preservation", "comments": "7 pages, 5 figures, This paper is submitted to Conference on Computer\n  Analysis of Images and Patterns (CAIP 2021) and is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image inpainting, with high-fidelity preservation for image realism,\nis a very challenging task. This is due to the subtle texture in key facial\nfeatures (component) that are not easily transferable. Many image inpainting\ntechniques have been proposed with outstanding capabilities and high\nquantitative performances recorded. However, with facial inpainting, the\nfeatures are more conspicuous and the visual quality of the blended inpainted\nregions are more important qualitatively. Based on these facts, we design a\nforeground-guided facial inpainting framework that can extract and generate\nfacial features using convolutional neural network layers. It introduces the\nuse of foreground segmentation masks to preserve the fidelity. Specifically, we\npropose a new loss function with semantic capability reasoning of facial\nexpressions, natural and unnatural features (make-up). We conduct our\nexperiments using the CelebA-HQ dataset, segmentation masks from CelebAMask-HQ\n(for foreground guidance) and Quick Draw Mask (for missing regions). Our\nproposed method achieved comparable quantitative results when compare to the\nstate of the art but qualitatively, it demonstrated high-fidelity preservation\nof facial components.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:50:58 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Jam", "Jireh", ""], ["Kendrick", "Connah", ""], ["Drouard", "Vincent", ""], ["Walker", "Kevin", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2105.03358", "submitter": "Mohammad Abuzar Shaikh", "authors": "Soumyya Kanti Datta, Mohammad Abuzar Shaikh, Sargur N. Srihari,\n  Mingchen Gao", "title": "Soft-Attention Improves Skin Cancer Classification Performance", "comments": "8 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In clinical applications, neural networks must focus on and highlight the\nmost important parts of an input image. Soft-Attention mechanism enables a\nneural network toachieve this goal. This paper investigates the effectiveness\nof Soft-Attention in deep neural architectures. The central aim of\nSoft-Attention is to boost the value of important features and suppress the\nnoise-inducing features. We compare the performance of VGG, ResNet,\nInceptionResNetv2 and DenseNet architectures with and without the\nSoft-Attention mechanism, while classifying skin lesions. The original network\nwhen coupled with Soft-Attention outperforms the baseline[16] by 4.7% while\nachieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,\nSoft-Attention coupling improves the sensitivity score by 3.8% compared to\nbaseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly\navailable at github.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:13:23 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:25:53 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 19:24:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Datta", "Soumyya Kanti", ""], ["Shaikh", "Mohammad Abuzar", ""], ["Srihari", "Sargur N.", ""], ["Gao", "Mingchen", ""]]}, {"id": "2105.03404", "submitter": "Hugo Touvron", "authors": "Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord,\n  Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel\n  Synnaeve, Jakob Verbeek, Herv\\'e J\\'egou", "title": "ResMLP: Feedforward networks for image classification with\n  data-efficient training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ResMLP, an architecture built entirely upon multi-layer\nperceptrons for image classification. It is a simple residual network that\nalternates (i) a linear layer in which image patches interact, independently\nand identically across channels, and (ii) a two-layer feed-forward network in\nwhich channels interact independently per patch. When trained with a modern\ntraining strategy using heavy data-augmentation and optionally distillation, it\nattains surprisingly good accuracy/complexity trade-offs on ImageNet. We also\ntrain ResMLP models in a self-supervised setup, to further remove priors from\nemploying a labelled dataset. Finally, by adapting our model to machine\ntranslation we achieve surprisingly good results.\n  We share pre-trained models and our code based on the Timm library.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:31:44 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 16:06:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Touvron", "Hugo", ""], ["Bojanowski", "Piotr", ""], ["Caron", "Mathilde", ""], ["Cord", "Matthieu", ""], ["El-Nouby", "Alaaeldin", ""], ["Grave", "Edouard", ""], ["Izacard", "Gautier", ""], ["Joulin", "Armand", ""], ["Synnaeve", "Gabriel", ""], ["Verbeek", "Jakob", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2105.03464", "submitter": "Andrea Sabo", "authors": "Andrea Sabo, Sina Mehdizadeh, Andrea Iaboni, Babak Taati", "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults\n  with Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:36:49 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 14:03:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Sabo", "Andrea", ""], ["Mehdizadeh", "Sina", ""], ["Iaboni", "Andrea", ""], ["Taati", "Babak", ""]]}, {"id": "2105.03492", "submitter": "Aidan Boyd", "authors": "Aidan Boyd, Kevin Bowyer, Adam Czajka", "title": "Human-Aided Saliency Maps Improve Generalization of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has driven remarkable accuracy increases in many computer\nvision problems. One ongoing challenge is how to achieve the greatest accuracy\nin cases where training data is limited. A second ongoing challenge is that\ntrained models are sometimes fragile in the sense that the accuracy achieved\ndoes not generalize well, even to new data that is subjectively similar to the\ntraining set. We address these challenges in a novel way, with the first-ever\n(to our knowledge) exploration of encoding human judgement about salient\nregions of images into the training data. We compare the accuracy and\ngeneralization of a state-of-the-art deep learning algorithm for a difficult\nproblem in biometric presentation attack detection when trained on (a) original\nimages with typical data augmentations, and (b) the same original images\ntransformed to encode human judgement about salient image regions. The latter\napproach results in models that achieve higher accuracy and better\ngeneralization, decreasing the error of the LivDet-Iris 2020 winner from 29.78%\nto 16.37%, and achieving impressive generalization in a\nleave-one-attack-type-out evaluation scenario. This work opens a new area of\nstudy for how to embed human intelligence into training strategies for deep\nlearning to achieve high accuracy and generalization in cases of limited\ntraining data.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:24:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Boyd", "Aidan", ""], ["Bowyer", "Kevin", ""], ["Czajka", "Adam", ""]]}, {"id": "2105.03494", "submitter": "Sara Beery", "authors": "Sara Beery, Arushi Agarwal, Elijah Cole, Vighnesh Birodkar", "title": "The iWildCam 2021 Competition Dataset", "comments": "FGVC8 Workshop at CVPR 2021. arXiv admin note: substantial text\n  overlap with arXiv:2004.10340", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera traps enable the automatic collection of large quantities of image\ndata. Ecologists use camera traps to monitor animal populations all over the\nworld. In order to estimate the abundance of a species from camera trap data,\necologists need to know not just which species were seen, but also how many\nindividuals of each species were seen. Object detection techniques can be used\nto find the number of individuals in each image. However, since camera traps\ncollect images in motion-triggered bursts, simply adding up the number of\ndetections over all frames is likely to lead to an incorrect estimate.\nOvercoming these obstacles may require incorporating spatio-temporal reasoning\nor individual re-identification in addition to traditional species detection\nand classification.\n  We have prepared a challenge where the training data and test data are from\ndifferent cameras spread across the globe. The set of species seen in each\ncamera overlap, but are not identical. The challenge is to classify species and\ncount individual animals across sequences in the test cameras.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:27:22 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Beery", "Sara", ""], ["Agarwal", "Arushi", ""], ["Cole", "Elijah", ""], ["Birodkar", "Vighnesh", ""]]}, {"id": "2105.03533", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Alex Kendall, Martin Jagersand", "title": "Video Class Agnostic Segmentation with Contrastive Learning for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation in autonomous driving predominantly focuses on learning\nfrom large-scale data with a closed set of known classes without considering\nunknown objects. Motivated by safety reasons, we address the video class\nagnostic segmentation task, which considers unknown objects outside the closed\nset of known classes in our training data. We propose a novel auxiliary\ncontrastive loss to learn the segmentation of known classes and unknown\nobjects. Unlike previous work in contrastive learning that samples the anchor,\npositive and negative examples on an image level, our contrastive learning\nmethod leverages pixel-wise semantic and temporal guidance. We conduct\nexperiments on Cityscapes-VPS by withholding four classes from training and\nshow an improvement gain for both known and unknown objects segmentation with\nthe auxiliary contrastive loss. We further release a large-scale synthetic\ndataset for different autonomous driving scenarios that includes distinct and\nrare unknown objects. We conduct experiments on the full synthetic dataset and\na reduced small-scale version, and show how contrastive learning is more\neffective in small scale datasets. Our proposed models, dataset, and code will\nbe released at https://github.com/MSiam/video_class_agnostic_segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 23:07:06 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 02:40:17 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Siam", "Mennatullah", ""], ["Kendall", "Alex", ""], ["Jagersand", "Martin", ""]]}, {"id": "2105.03536", "submitter": "Lisa Wang", "authors": "AmirAli Abdolrashidi, Lisa Wang, Shivani Agrawal, Jonathan Malmaud,\n  Oleg Rybakov, Chas Leichner, Lukasz Lew", "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit", "comments": "8 pages. Accepted at the Efficient Deep Learning for Computer Vision\n  Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has become a popular technique to compress neural networks and\nreduce compute cost, but most prior work focuses on studying quantization\nwithout changing the network size. Many real-world applications of neural\nnetworks have compute cost and memory budgets, which can be traded off with\nmodel quality by changing the number of parameters. In this work, we use ResNet\nas a case study to systematically investigate the effects of quantization on\ninference compute cost-quality tradeoff curves. Our results suggest that for\neach bfloat16 ResNet model, there are quantized models with lower cost and\nhigher accuracy; in other words, the bfloat16 compute cost-quality tradeoff\ncurve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily\nquantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve\nstate-of-the-art results on ImageNet for 4-bit ResNet-50 with\nquantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We\ndemonstrate the regularizing effect of quantization by measuring the\ngeneralization gap. The quantization method we used is optimized for\npracticality: It requires little tuning and is designed with hardware\ncapabilities in mind. Our work motivates further research into optimal numeric\nformats for quantization, as well as the development of machine learning\naccelerators supporting these formats. As part of this work, we contribute a\nquantization library written in JAX, which is open-sourced at\nhttps://github.com/google-research/google-research/tree/master/aqt.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 23:28:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Abdolrashidi", "AmirAli", ""], ["Wang", "Lisa", ""], ["Agrawal", "Shivani", ""], ["Malmaud", "Jonathan", ""], ["Rybakov", "Oleg", ""], ["Leichner", "Chas", ""], ["Lew", "Lukasz", ""]]}, {"id": "2105.03569", "submitter": "Yumeng Zhang", "authors": "Yumeng Zhang, Li Chen, Yufeng Liu, Xiaoyan Guo, Wen Zheng, Junhai Yong", "title": "Improving Robustness for Pose Estimation via Stable Heatmap Regression", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have achieved excellent performance in pose estimation,\nbut the lack of robustness causes the keypoints to change drastically between\nsimilar images. In view of this problem, a stable heatmap regression method is\nproposed to alleviate network vulnerability to small perturbations. We utilize\nthe correlation between different rows and columns in a heatmap to alleviate\nthe multi-peaks problem, and design a highly differentiated heatmap regression\nto make a keypoint discriminative from surrounding points. A maximum stability\ntraining loss is used to simplify the optimization difficulty when minimizing\nthe prediction gap of two similar images. The proposed method achieves a\nsignificant advance in robustness over state-of-the-art approaches on two\nbenchmark datasets and maintains high performance.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:07:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Yumeng", ""], ["Chen", "Li", ""], ["Liu", "Yufeng", ""], ["Guo", "Xiaoyan", ""], ["Zheng", "Wen", ""], ["Yong", "Junhai", ""]]}, {"id": "2105.03570", "submitter": "Yu Wang", "authors": "Yu Wang, Rui Zhang, Shuo Zhang, Miao Li, YangYang Xia, XiShan Zhang,\n  ShaoLi Liu", "title": "Domain-Specific Suppression for Adaptive Object Detection", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain adaptation methods face performance degradation in object detection,\nas the complexity of tasks require more about the transferability of the model.\nWe propose a new perspective on how CNN models gain the transferability,\nviewing the weights of a model as a series of motion patterns. The directions\nof weights, and the gradients, can be divided into domain-specific and\ndomain-invariant parts, and the goal of domain adaptation is to concentrate on\nthe domain-invariant direction while eliminating the disturbance from\ndomain-specific one. Current UDA object detection methods view the two\ndirections as a whole while optimizing, which will cause domain-invariant\ndirection mismatch even if the output features are perfectly aligned. In this\npaper, we propose the domain-specific suppression, an exemplary and\ngeneralizable constraint to the original convolution gradients in\nbackpropagation to detach the two parts of directions and suppress the\ndomain-specific one. We further validate our theoretical analysis and methods\non several domain adaptive object detection tasks, including weather, camera\nconfiguration, and synthetic to real-world adaptation. Our experiment results\nshow significant advance over the state-of-the-art methods in the UDA object\ndetection field, performing a promotion of $10.2\\sim12.2\\%$ mAP on all these\ndomain adaptation scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:11:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Yu", ""], ["Zhang", "Rui", ""], ["Zhang", "Shuo", ""], ["Li", "Miao", ""], ["Xia", "YangYang", ""], ["Zhang", "XiShan", ""], ["Liu", "ShaoLi", ""]]}, {"id": "2105.03578", "submitter": "Dzung Doan Anh", "authors": "Anh-Dzung Doan and Daniyar Turmukhambetov and Yasir Latif and Tat-Jun\n  Chin and Soohyun Bae", "title": "Learning to Predict Repeatability of Interest Points", "comments": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many robotics applications require interest points that are highly repeatable\nunder varying viewpoints and lighting conditions. However, this requirement is\nvery challenging as the environment changes continuously and indefinitely,\nleading to appearance changes of interest points with respect to time. This\npaper proposes to predict the repeatability of an interest point as a function\nof time, which can tell us the lifespan of the interest point considering daily\nor seasonal variation. The repeatability predictor (RP) is formulated as a\nregressor trained on repeated interest points from multiple viewpoints over a\nlong period of time. Through comprehensive experiments, we demonstrate that our\nRP can estimate when a new interest point is repeated, and also highlight an\ninsightful analysis about this problem. For further comparison, we apply our RP\nto the map summarization under visual localization framework, which builds a\ncompact representation of the full context map given the query time. The\nexperimental result shows a careful selection of potentially repeatable\ninterest points predicted by our RP can significantly mitigate the degeneration\nof localization accuracy from map summarization.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:26:43 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Doan", "Anh-Dzung", ""], ["Turmukhambetov", "Daniyar", ""], ["Latif", "Yasir", ""], ["Chin", "Tat-Jun", ""], ["Bae", "Soohyun", ""]]}, {"id": "2105.03579", "submitter": "Xiao Huang", "authors": "Jiaming Wang, Zhenfeng Shao, Tao Lu, Xiao Huang, Ruiqian Zhang, Yu\n  Wang", "title": "Unsupervised Remote Sensing Super-Resolution via Migration Image Prior", "comments": "6 pages, 4 figures. IEEE International Conference on Multimedia and\n  Expo (ICME) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, satellites with high temporal resolution have fostered wide\nattention in various practical applications. Due to limitations of bandwidth\nand hardware cost, however, the spatial resolution of such satellites is\nconsiderably low, largely limiting their potentials in scenarios that require\nspatially explicit information. To improve image resolution, numerous\napproaches based on training low-high resolution pairs have been proposed to\naddress the super-resolution (SR) task. Despite their success, however,\nlow/high spatial resolution pairs are usually difficult to obtain in satellites\nwith a high temporal resolution, making such approaches in SR impractical to\nuse. In this paper, we proposed a new unsupervised learning framework, called\n\"MIP\", which achieves SR tasks without low/high resolution image pairs. First,\nrandom noise maps are fed into a designed generative adversarial network (GAN)\nfor reconstruction. Then, the proposed method converts the reference image to\nlatent space as the migration image prior. Finally, we update the input noise\nvia an implicit method, and further transfer the texture and structured\ninformation from the reference image. Extensive experimental results on the\nDraper dataset show that MIP achieves significant improvements over\nstate-of-the-art methods both quantitatively and qualitatively. The proposed\nMIP is open-sourced at http://github.com/jiaming-wang/MIP.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:29:35 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 14:20:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Jiaming", ""], ["Shao", "Zhenfeng", ""], ["Lu", "Tao", ""], ["Huang", "Xiao", ""], ["Zhang", "Ruiqian", ""], ["Wang", "Yu", ""]]}, {"id": "2105.03582", "submitter": "Jiapeng Tang", "authors": "Jiapeng Tang, Jiabao Lei, Dan Xu, Feiying Ma, Kui Jia, Lei Zhang", "title": "Sign-Agnostic CONet: Learning Implicit Surface Reconstructions by\n  Sign-Agnostic Optimization of Convolutional Occupancy Networks", "comments": "18 pages; 14 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface reconstruction from point clouds is a fundamental problem in the\ncomputer vision and graphics community. Recent state-of-the-arts solve this\nproblem by individually optimizing each local implicit field during inference.\nWithout considering the geometric relationships between local fields, they\ntypically require accurate normals to avoid the sign conflict problem in\noverlapping regions of local fields, which severely limits their applicability\nto raw scans where surface normals could be unavailable. Although SAL breaks\nthis limitation via sign-agnostic learning, it is still unexplored that how to\nextend this pipeline to local shape modeling. To this end, we propose to learn\nimplicit surface reconstruction by sign-agnostic optimization of convolutional\noccupancy networks, to simultaneously achieve advanced scalability, generality,\nand applicability in a unified framework. In the paper, we also show this goal\ncan be effectively achieved by a simple yet effective design, which optimizes\nthe occupancy fields that are conditioned on convolutional features from an\nhourglass network architecture with an unsigned binary cross-entropy loss.\nExtensive experimental comparison with previous state-of-the-arts on both\nobject-level and scene-level datasets demonstrate the superior accuracy of our\napproach for surface reconstruction from un-orientated point clouds.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:35:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tang", "Jiapeng", ""], ["Lei", "Jiabao", ""], ["Xu", "Dan", ""], ["Ma", "Feiying", ""], ["Jia", "Kui", ""], ["Zhang", "Lei", ""]]}, {"id": "2105.03588", "submitter": "Zhuofa Chen", "authors": "Yousif Khaireddin, Zhuofa Chen", "title": "Facial Emotion Recognition: State of the Art Performance on FER2013", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial emotion recognition (FER) is significant for human-computer\ninteraction such as clinical practice and behavioral description. Accurate and\nrobust FER by computer models remains challenging due to the heterogeneity of\nhuman faces and variations in images such as different facial pose and\nlighting. Among all techniques for FER, deep learning models, especially\nConvolutional Neural Networks (CNNs) have shown great potential due to their\npowerful automatic feature extraction and computational efficiency. In this\nwork, we achieve the highest single-network classification accuracy on the\nFER2013 dataset. We adopt the VGGNet architecture, rigorously fine-tune its\nhyperparameters, and experiment with various optimization methods. To our best\nknowledge, our model achieves state-of-the-art single-network accuracy of 73.28\n% on FER2013 without using extra training data.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 04:20:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Khaireddin", "Yousif", ""], ["Chen", "Zhuofa", ""]]}, {"id": "2105.03596", "submitter": "Lei Xun", "authors": "Wei Lou, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, Geoff V. Merrett", "title": "Dynamic-OFA: Runtime DNN Architecture Switching for Performance Scaling\n  on Heterogeneous Embedded Platforms", "comments": "Accepted at CVPR ECV Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile and embedded platforms are increasingly required to efficiently\nexecute computationally demanding DNNs across heterogeneous processing\nelements. At runtime, the available hardware resources to DNNs can vary\nconsiderably due to other concurrently running applications. The performance\nrequirements of the applications could also change under different scenarios.\nTo achieve the desired performance, dynamic DNNs have been proposed in which\nthe number of channels/layers can be scaled in real time to meet different\nrequirements under varying resource constraints. However, the training process\nof such dynamic DNNs can be costly, since platform-aware models of different\ndeployment scenarios must be retrained to become dynamic. This paper proposes\nDynamic-OFA, a novel dynamic DNN approach for state-of-the-art platform-aware\nNAS models (i.e. Once-for-all network (OFA)). Dynamic-OFA pre-samples a family\nof sub-networks from a static OFA backbone model, and contains a runtime\nmanager to choose different sub-networks under different runtime environments.\nAs such, Dynamic-OFA does not need the traditional dynamic DNN training\npipeline. Compared to the state-of-the-art, our experimental results using\nImageNet on a Jetson Xavier NX show that the approach is up to 3.5x (CPU), 2.4x\n(GPU) faster for similar ImageNet Top-1 accuracy, or 3.8% (CPU), 5.1% (GPU)\nhigher accuracy at similar latency.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:10:53 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 08:01:36 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lou", "Wei", ""], ["Xun", "Lei", ""], ["Sabet", "Amin", ""], ["Bi", "Jia", ""], ["Hare", "Jonathon", ""], ["Merrett", "Geoff V.", ""]]}, {"id": "2105.03600", "submitter": "Lei Xun", "authors": "Lei Xun, Long Tran-Thanh, Bashir M Al-Hashimi, Geoff V. Merrett", "title": "Incremental Training and Group Convolution Pruning for Runtime DNN\n  Performance Scaling on Heterogeneous Embedded Platforms", "comments": "Accepted at ACM/IEEE Workshop on Machine Learning for CAD (MLCAD)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference for Deep Neural Networks is increasingly being executed locally on\nmobile and embedded platforms due to its advantages in latency, privacy and\nconnectivity. Since modern System on Chips typically execute a combination of\ndifferent and dynamic workloads concurrently, it is challenging to consistently\nmeet inference time/energy budget at runtime because of the local computing\nresources available to the DNNs vary considerably. To address this challenge, a\nvariety of dynamic DNNs were proposed. However, these works have significant\nmemory overhead, limited runtime recoverable compression rate and narrow\ndynamic ranges of performance scaling. In this paper, we present a dynamic DNN\nusing incremental training and group convolution pruning. The channels of the\nDNN convolution layer are divided into groups, which are then trained\nincrementally. At runtime, following groups can be pruned for inference\ntime/energy reduction or added back for accuracy recovery without model\nretraining. In addition, we combine task mapping and Dynamic Voltage Frequency\nScaling (DVFS) with our dynamic DNN to deliver finer trade-off between accuracy\nand time/power/energy over a wider dynamic range. We illustrate the approach by\nmodifying AlexNet for the CIFAR10 image dataset and evaluate our work on two\nheterogeneous hardware platforms: Odroid XU3 (ARM big.LITTLE CPUs) and Nvidia\nJetson Nano (CPU and GPU). Compared to the existing works, our approach can\nprovide up to 2.36x (energy) and 2.73x (time) wider dynamic range with a 2.4x\nsmaller memory footprint at the same compression rate. It achieved 10.6x\n(energy) and 41.6x (time) wider dynamic range by combining with task mapping\nand DVFS.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:38:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xun", "Lei", ""], ["Tran-Thanh", "Long", ""], ["Al-Hashimi", "Bashir M", ""], ["Merrett", "Geoff V.", ""]]}, {"id": "2105.03608", "submitter": "Lei Xun", "authors": "Lei Xun, Long Tran-Thanh, Bashir M Al-Hashimi, Geoff V. Merrett", "title": "Optimising Resource Management for Embedded Machine Learning", "comments": "Accepted at DATE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning inference is increasingly being executed locally on mobile\nand embedded platforms, due to the clear advantages in latency, privacy and\nconnectivity. In this paper, we present approaches for online resource\nmanagement in heterogeneous multi-core systems and show how they can be applied\nto optimise the performance of machine learning workloads. Performance can be\ndefined using platform-dependent (e.g. speed, energy) and platform-independent\n(accuracy, confidence) metrics. In particular, we show how a Deep Neural\nNetwork (DNN) can be dynamically scalable to trade-off these various\nperformance metrics. Achieving consistent performance when executing on\ndifferent platforms is necessary yet challenging, due to the different\nresources provided and their capability, and their time-varying availability\nwhen executing alongside other workloads. Managing the interface between\navailable hardware resources (often numerous and heterogeneous in nature),\nsoftware requirements, and user experience is increasingly complex.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 06:10:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xun", "Lei", ""], ["Tran-Thanh", "Long", ""], ["Al-Hashimi", "Bashir M", ""], ["Merrett", "Geoff V.", ""]]}, {"id": "2105.03620", "submitter": "Chunhua Shen", "authors": "Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu\n  Liu, Hao Chen", "title": "ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text\n  Spotting", "comments": "Code is at: https://git.io/AdelaiDet. Journal extension of\n  arXiv:2002.10200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  End-to-end text-spotting, which aims to integrate detection and recognition\nin a unified framework, has attracted increasing attention due to its\nsimplicity of the two complimentary tasks. It remains an open problem\nespecially when processing arbitrarily-shaped text instances. Previous methods\ncan be roughly categorized into two groups: character-based and\nsegmentation-based, which often require character-level annotations and/or\ncomplex post-processing due to the unstructured output. Here, we tackle\nend-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet\nv2). Our main contributions are four-fold: 1) For the first time, we adaptively\nfit arbitrarily-shaped text by a parameterized Bezier curve, which, compared\nwith segmentation-based methods, can not only provide structured output but\nalso controllable representation. 2) We design a novel BezierAlign layer for\nextracting accurate convolution features of a text instance of arbitrary\nshapes, significantly improving the precision of recognition over previous\nmethods. 3) Different from previous methods, which often suffer from complex\npost-processing and sensitive hyper-parameters, our ABCNet v2 maintains a\nsimple pipeline with the only post-processing non-maximum suppression (NMS). 4)\nAs the performance of text recognition closely depends on feature alignment,\nABCNet v2 further adopts a simple yet effective coordinate convolution to\nencode the position of the convolutional filters, which leads to a considerable\nimprovement with negligible computation overhead. Comprehensive experiments\nconducted on various bilingual (English and Chinese) benchmark datasets\ndemonstrate that ABCNet v2 can achieve state-of-the-art performance while\nmaintaining very high efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 07:46:55 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 03:25:50 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 05:07:58 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Liu", "Yuliang", ""], ["Shen", "Chunhua", ""], ["Jin", "Lianwen", ""], ["He", "Tong", ""], ["Chen", "Peng", ""], ["Liu", "Chongyu", ""], ["Chen", "Hao", ""]]}, {"id": "2105.03632", "submitter": "Jawad Muhammad", "authors": "Jawad Muhammad, Yunlong Wang, Caiyong Wang, Kunbo Zhang, and Zhenan\n  Sun", "title": "CASIA-Face-Africa: A Large-scale African Face Image Database", "comments": "This paper has been accepted for publication in the journal IEEE TIFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a popular and well-studied area with wide applications in\nour society. However, racial bias had been proven to be inherent in most State\nOf The Art (SOTA) face recognition systems. Many investigative studies on face\nrecognition algorithms have reported higher false positive rates of African\nsubjects cohorts than the other cohorts. Lack of large-scale African face image\ndatabases in public domain is one of the main restrictions in studying the\nracial bias problem of face recognition. To this end, we collect a face image\ndatabase namely CASIA-Face-Africa which contains 38,546 images of 1,183 African\nsubjects. Multi-spectral cameras are utilized to capture the face images under\nvarious illumination settings. Demographic attributes and facial expressions of\nthe subjects are also carefully recorded. For landmark detection, each face\nimage in the database is manually labeled with 68 facial keypoints. A group of\nevaluation protocols are constructed according to different applications,\ntasks, partitions and scenarios. The performances of SOTA face recognition\nalgorithms without re-training are reported as baselines. The proposed database\nalong with its face landmark annotations, evaluation protocols and preliminary\nresults form a good benchmark to study the essential aspects of face biometrics\nfor African subjects, especially face image preprocessing, face feature\nanalysis and matching, facial expression recognition, sex/age estimation,\nethnic classification, face image generation, etc. The database can be\ndownloaded from our http://www.cripacsir.cn/dataset/\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 08:15:11 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:23:12 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Muhammad", "Jawad", ""], ["Wang", "Yunlong", ""], ["Wang", "Caiyong", ""], ["Zhang", "Kunbo", ""], ["Sun", "Zhenan", ""]]}, {"id": "2105.03647", "submitter": "Mahdyar Ravanbakhsh", "authors": "Tristan Kreuziger, Mahdyar Ravanbakhsh, Beg\\\"um Demir", "title": "A Novel Triplet Sampling Method for Multi-Label Remote Sensing Image\n  Search and Retrieval", "comments": "The paper is under review. Our code is available online at\n  https://git.tu-berlin.de/rsim/image-retrieval-from-triplets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning the similarity between remote sensing (RS) images forms the\nfoundation for content based RS image retrieval (CBIR). Recently, deep metric\nlearning approaches that map the semantic similarity of images into an\nembedding space have been found very popular in RS. A common approach for\nlearning the metric space relies on the selection of triplets of similar\n(positive) and dissimilar (negative) images to a reference image called as an\nanchor. Choosing triplets is a difficult task particularly for multi-label RS\nCBIR, where each training image is annotated by multiple class labels. To\naddress this problem, in this paper we propose a novel triplet sampling method\nin the framework of deep neural networks (DNNs) defined for multi-label RS CBIR\nproblems. The proposed method selects a small set of the most representative\nand informative triplets based on two main steps. In the first step, a set of\nanchors that are diverse to each other in the embedding space is selected from\nthe current mini-batch using an iterative algorithm. In the second step,\ndifferent sets of positive and negative images are chosen for each anchor by\nevaluating relevancy, hardness, and diversity of the images among each other\nbased on a novel ranking strategy. Experimental results obtained on two\nmulti-label benchmark achieves show that the selection of the most informative\nand representative triplets in the context of DNNs results in: i) reducing the\ncomputational complexity of the training phase of the DNNs without any\nsignificant loss on the performance; and ii) an increase in learning speed\nsince informative triplets allow fast convergence. The code of the proposed\nmethod is publicly available at\nhttps://git.tu-berlin.de/rsim/image-retrieval-from-triplets.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:16:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kreuziger", "Tristan", ""], ["Ravanbakhsh", "Mahdyar", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "2105.03669", "submitter": "Thomas Wollmann", "authors": "Johannes Otterbach, Thomas Wollmann", "title": "Chameleon: A Semi-AutoML framework targeting quick and scalable\n  development and deployment of production-ready ML systems for SMEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing, scaling, and deploying modern Machine Learning solutions remains\nchallenging for small- and middle-sized enterprises (SMEs). This is due to a\nhigh entry barrier of building and maintaining a dedicated IT team as well as\nthe difficulties of real-world data (RWD) compared to standard benchmark data.\nTo address this challenge, we discuss the implementation and concepts of\nChameleon, a semi-AutoML framework. The goal of Chameleon is fast and scalable\ndevelopment and deployment of production-ready machine learning systems into\nthe workflow of SMEs. We first discuss the RWD challenges faced by SMEs. After,\nwe outline the central part of the framework which is a model and loss-function\nzoo with RWD-relevant defaults. Subsequently, we present how one can use a\ntemplatable framework in order to automate the experiment iteration cycle, as\nwell as close the gap between development and deployment. Finally, we touch on\nour testing framework component allowing us to investigate common model failure\nmodes and support best practices of model deployment governance.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:43:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Otterbach", "Johannes", ""], ["Wollmann", "Thomas", ""]]}, {"id": "2105.03677", "submitter": "Ling Li", "authors": "Dong Liang, Fei Xue and Ling Li", "title": "Active Terahertz Imaging Dataset for Concealed Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concealed object detection in Terahertz imaging is an urgent need for public\nsecurity and counter-terrorism. In this paper, we provide a public dataset for\nevaluating multi-object detection algorithms in active Terahertz imaging\nresolution 5 mm by 5 mm. To the best of our knowledge, this is the first public\nTerahertz imaging dataset prepared to evaluate object detection algorithms.\nObject detection on this dataset is much more difficult than on those standard\npublic object detection datasets due to its inferior imaging quality. Facing\nthe problem of imbalanced samples in object detection and hard training\nsamples, we evaluate four popular detectors: YOLOv3, YOLOv4, FRCN-OHEM, and\nRetinaNet on this dataset. Experimental results indicate that the RetinaNet\nachieves the highest mAP. In addition, we demonstrate that hiding objects in\ndifferent parts of the human body affect detection accuracy. The dataset is\navailable at https://github.com/LingLIx/THz_Dataset.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 11:21:38 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liang", "Dong", ""], ["Xue", "Fei", ""], ["Li", "Ling", ""]]}, {"id": "2105.03688", "submitter": "Ziyao Li", "authors": "Ziyao Li, Shuwen Yang, Guojie Song and Lingsheng Cai", "title": "HamNet: Conformation-Guided Molecular Representation with Hamiltonian\n  Neural Networks", "comments": "in ICLR-2021 (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.chem-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Well-designed molecular representations (fingerprints) are vital to combine\nmedical chemistry and deep learning. Whereas incorporating 3D geometry of\nmolecules (i.e. conformations) in their representations seems beneficial,\ncurrent 3D algorithms are still in infancy. In this paper, we propose a novel\nmolecular representation algorithm which preserves 3D conformations of\nmolecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit\npositions and momentums of atoms in a molecule interact in the Hamiltonian\nEngine following the discretized Hamiltonian equations. These implicit\ncoordinations are supervised with real conformations with translation- &\nrotation-invariant losses, and further used as inputs to the Fingerprint\nGenerator, a message-passing neural network. Experiments show that the\nHamiltonian Engine can well preserve molecular conformations, and that the\nfingerprints generated by HamNet achieve state-of-the-art performances on\nMoleculeNet, a standard molecular machine learning benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 12:48:08 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Ziyao", ""], ["Yang", "Shuwen", ""], ["Song", "Guojie", ""], ["Cai", "Lingsheng", ""]]}, {"id": "2105.03689", "submitter": "Leo Yu Zhang Dr.", "authors": "Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Shengshan Hu, Jinyu Tian,\n  Jiantao Zhou", "title": "Self-Supervised Adversarial Example Detection by Disentangled\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models are known to be vulnerable to adversarial examples that\nare elaborately designed for malicious purposes and are imperceptible to the\nhuman perceptual system. Autoencoder, when trained solely over benign examples,\nhas been widely used for (self-supervised) adversarial detection based on the\nassumption that adversarial examples yield larger reconstruction error.\nHowever, because lacking adversarial examples in its training and the too\nstrong generalization ability of autoencoder, this assumption does not always\nhold true in practice. To alleviate this problem, we explore to detect\nadversarial examples by disentangled representations of images under the\nautoencoder structure. By disentangling input images as class features and\nsemantic features, we train an autoencoder, assisted by a discriminator\nnetwork, over both correctly paired class/semantic features and incorrectly\npaired class/semantic features to reconstruct benign and counterexamples. This\nmimics the behavior of adversarial examples and can reduce the unnecessary\ngeneralization ability of autoencoder. Compared with the state-of-the-art\nself-supervised detection methods, our method exhibits better performance in\nvarious measurements (i.e., AUC, FPR, TPR) over different datasets (MNIST,\nFashion-MNIST and CIFAR-10), different adversarial attack methods (FGSM, BIM,\nPGD, DeepFool, and CW) and different victim models (8-layer CNN and 16-layer\nVGG). We compare our method with the state-of-the-art self-supervised detection\nmethods under different adversarial attacks and different victim models (30\nattack settings), and it exhibits better performance in various measurements\n(AUC, FPR, TPR) for most attacks settings. Ideally, AUC is $1$ and our method\nachieves $0.99+$ on CIFAR-10 for all attacks. Notably, different from other\nAutoencoder-based detectors, our method can provide resistance to the adaptive\nadversary.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 12:48:18 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 12:37:42 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 12:07:49 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Zhaoxi", ""], ["Zhang", "Leo Yu", ""], ["Zheng", "Xufei", ""], ["Hu", "Shengshan", ""], ["Tian", "Jinyu", ""], ["Zhou", "Jiantao", ""]]}, {"id": "2105.03746", "submitter": "Huangjie Zheng", "authors": "Huangjie Zheng, Xu Chen, Jiangchao Yao, Hongxia Yang, Chunyuan Li, Ya\n  Zhang, Hao Zhang, Ivor Tsang, Jingren Zhou, Mingyuan Zhou", "title": "Contrastive Attraction and Contrastive Repulsion for Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning (CL) is effective in learning data representations\nwithout label supervision, where the encoder needs to contrast each positive\nsample over multiple negative samples via a one-vs-many softmax cross-entropy\nloss. However, conventional CL is sensitive to how many negative samples are\nincluded and how they are selected. Proposed in this paper is a doubly CL\nstrategy that contrasts positive samples and negative ones within themselves\nseparately. We realize this strategy with contrastive attraction and\ncontrastive repulsion (CACR) makes the query not only exert a greater force to\nattract more distant positive samples but also do so to repel closer negative\nsamples. Theoretical analysis reveals the connection between CACR and CL from\nthe perspectives of both positive attraction and negative repulsion and shows\nthe benefits in both efficiency and robustness brought by separately\ncontrasting within the sampled positive and negative pairs. Extensive\nlarge-scale experiments on standard vision tasks show that CACR not only\nconsistently outperforms existing CL methods on benchmark datasets in\nrepresentation learning, but also provides interpretable contrastive weights,\ndemonstrating the efficacy of the proposed doubly contrastive strategy.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 17:25:08 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 04:12:54 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zheng", "Huangjie", ""], ["Chen", "Xu", ""], ["Yao", "Jiangchao", ""], ["Yang", "Hongxia", ""], ["Li", "Chunyuan", ""], ["Zhang", "Ya", ""], ["Zhang", "Hao", ""], ["Tsang", "Ivor", ""], ["Zhou", "Jingren", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2105.03760", "submitter": "Mahmoud Z. Khairallah", "authors": "Mahmoud Z. Khairallah, Fabien Bonardi, David Roussel and Samia\n  Bouchafa", "title": "PCA Event-Based Optical Flow for Visual Odometry", "comments": "9 pages, 8 figures, not published yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of neuromorphic vision sensors such as event-based cameras, a\nparadigm shift is required for most computer vision algorithms. Among these\nalgorithms, optical flow estimation is a prime candidate for this process\nconsidering that it is linked to a neuromorphic vision approach. Usage of\noptical flow is widespread in robotics applications due to its richness and\naccuracy. We present a Principal Component Analysis (PCA) approach to the\nproblem of event-based optical flow estimation. In this approach, we examine\ndifferent regularization methods which efficiently enhance the estimation of\nthe optical flow. We show that the best variant of our proposed method,\ndedicated to the real-time context of visual odometry, is about two times\nfaster compared to state-of-the-art implementations while significantly\nimproves optical flow accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 18:30:44 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 01:27:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Khairallah", "Mahmoud Z.", ""], ["Bonardi", "Fabien", ""], ["Roussel", "David", ""], ["Bouchafa", "Samia", ""]]}, {"id": "2105.03761", "submitter": "Maxime Kayser", "authors": "Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde,\n  Virginie Do, Zeynep Akata, Thomas Lukasiewicz", "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an increasing number of works have introduced models capable of\ngenerating natural language explanations (NLEs) for their predictions on\nvision-language (VL) tasks. Such models are appealing because they can provide\nhuman-friendly and comprehensive explanations. However, there is still a lack\nof unified evaluation approaches for the explanations generated by these\nmodels. Moreover, there are currently only few datasets of NLEs for VL tasks.\nIn this work, we introduce e-ViL, a benchmark for explainable vision-language\ntasks that establishes a unified evaluation framework and provides the first\ncomprehensive comparison of existing approaches that generate NLEs for VL\ntasks. e-ViL spans four models and three datasets. Both automatic metrics and\nhuman evaluation are used to assess model-generated explanations. We also\nintroduce e-SNLI-VE, the largest existing VL dataset with NLEs (over 430k\ninstances). Finally, we propose a new model that combines UNITER, which learns\njoint embeddings of images and text, and GPT-2, a pre-trained language model\nthat is well-suited for text generation. It surpasses the previous\nstate-of-the-art by a large margin across all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 18:46:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kayser", "Maxime", ""], ["Camburu", "Oana-Maria", ""], ["Salewski", "Leonard", ""], ["Emde", "Cornelius", ""], ["Do", "Virginie", ""], ["Akata", "Zeynep", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2105.03781", "submitter": "Yingjun Du", "authors": "Yingjun Du, Haoliang Sun, Xiantong Zhen, Jun Xu, Yilong Yin, Ling\n  Shao, Cees G. M. Snoek", "title": "MetaKernel: Learning Variational Random Features with Limited Labels", "comments": "19 pages,7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2006.06707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning deals with the fundamental and challenging problem of\nlearning from a few annotated samples, while being able to generalize well on\nnew tasks. The crux of few-shot learning is to extract prior knowledge from\nrelated tasks to enable fast adaptation to a new task with a limited amount of\ndata. In this paper, we propose meta-learning kernels with random Fourier\nfeatures for few-shot learning, we call MetaKernel. Specifically, we propose\nlearning variational random features in a data-driven manner to obtain\ntask-specific kernels by leveraging the shared knowledge provided by related\ntasks in a meta-learning setting. We treat the random feature basis as the\nlatent variable, which is estimated by variational inference. The shared\nknowledge from related tasks is incorporated into a context inference of the\nposterior, which we achieve via a long-short term memory module. To establish\nmore expressive kernels, we deploy conditional normalizing flows based on\ncoupling layers to achieve a richer posterior distribution over random Fourier\nbases. The resultant kernels are more informative and discriminative, which\nfurther improves the few-shot learning. To evaluate our method, we conduct\nextensive experiments on both few-shot image classification and regression\ntasks. A thorough ablation study demonstrates that the effectiveness of each\nintroduced component in our method. The benchmark results on fourteen datasets\ndemonstrate MetaKernel consistently delivers at least comparable and often\nbetter performance than state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 21:24:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Du", "Yingjun", ""], ["Sun", "Haoliang", ""], ["Zhen", "Xiantong", ""], ["Xu", "Jun", ""], ["Yin", "Yilong", ""], ["Shao", "Ling", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2105.03790", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Viktoriia Sharmanska and Stefanos Zafeiriou", "title": "Distribution Matching for Heterogeneous Multi-Task Learning: a\n  Large-scale Face Study", "comments": "arXiv admin note: text overlap with arXiv:2103.15792,\n  arXiv:1910.11111", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-Task Learning has emerged as a methodology in which multiple tasks are\njointly learned by a shared learning algorithm, such as a DNN. MTL is based on\nthe assumption that the tasks under consideration are related; therefore it\nexploits shared knowledge for improving performance on each individual task.\nTasks are generally considered to be homogeneous, i.e., to refer to the same\ntype of problem. Moreover, MTL is usually based on ground truth annotations\nwith full, or partial overlap across tasks. In this work, we deal with\nheterogeneous MTL, simultaneously addressing detection, classification &\nregression problems. We explore task-relatedness as a means for co-training, in\na weakly-supervised way, tasks that contain little, or even non-overlapping\nannotations. Task-relatedness is introduced in MTL, either explicitly through\nprior expert knowledge, or through data-driven studies. We propose a novel\ndistribution matching approach, in which knowledge exchange is enabled between\ntasks, via matching of their predictions' distributions. Based on this\napproach, we build FaceBehaviorNet, the first framework for large-scale face\nanalysis, by jointly learning all facial behavior tasks. We develop case\nstudies for: i) continuous affect estimation, action unit detection, basic\nemotion recognition; ii) attribute detection, face identification.\n  We illustrate that co-training via task relatedness alleviates negative\ntransfer. Since FaceBehaviorNet learns features that encapsulate all aspects of\nfacial behavior, we conduct zero-/few-shot learning to perform tasks beyond the\nones that it has been trained for, such as compound emotion recognition. By\nconducting a very large experimental study, utilizing 10 databases, we\nillustrate that our approach outperforms, by large margins, the\nstate-of-the-art in all tasks and in all databases, even in these which have\nnot been used in its training.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 22:26:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Sharmanska", "Viktoriia", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2105.03797", "submitter": "Bin Wang", "authors": "Kaitai Zhang, Bin Wang, Wei Wang, Fahad Sohrab, Moncef Gabbouj and\n  C.-C. Jay Kuo", "title": "AnomalyHop: An SSL-based Image Anomaly Localization Method", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image anomaly localization method based on the successive subspace\nlearning (SSL) framework, called AnomalyHop, is proposed in this work.\nAnomalyHop consists of three modules: 1) feature extraction via successive\nsubspace learning (SSL), 2) normality feature distributions modeling via\nGaussian models, and 3) anomaly map generation and fusion. Comparing with\nstate-of-the-art image anomaly localization methods based on deep neural\nnetworks (DNNs), AnomalyHop is mathematically transparent, easy to train, and\nfast in its inference speed. Besides, its area under the ROC curve (ROC-AUC)\nperformance on the MVTec AD dataset is 95.9%, which is among the best of\nseveral benchmarking methods. Our codes are publicly available at Github.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 23:17:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Kaitai", ""], ["Wang", "Bin", ""], ["Wang", "Wei", ""], ["Sohrab", "Fahad", ""], ["Gabbouj", "Moncef", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2105.03804", "submitter": "Farzaneh Rajabi", "authors": "Austin Park, Farzaneh Rajabi, Ross Weber", "title": "Slash or burn: Power line and vegetation classification for wildfire\n  prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electric utilities are struggling to manage increasing wildfire risk in a\nhotter and drier climate. Utility transmission and distribution lines regularly\nignite destructive fires when they make contact with surrounding vegetation.\nTrimming vegetation to maintain the separation from utility assets is as\ncritical to safety as it is difficult. Each utility has tens of thousands of\nlinear miles to manage, poor knowledge of where those assets are located, and\nno way to prioritize trimming. Feature-enhanced convolutional neural networks\n(CNNs) have proven effective in this problem space. Histograms of oriented\ngradients (HOG) and Hough transforms are used to increase the salience of the\nlinear structures like power lines and poles. Data is frequently taken from\ndrone or satellite footage, but Google Street View offers an even more scalable\nand lower cost solution. This paper uses $1,320$ images scraped from Street\nView, transfer learning on popular CNNs, and feature engineering to place\nimages in one of three classes: (1) no utility systems, (2) utility systems\nwith no overgrown vegetation, or (3) utility systems with overgrown vegetation.\nThe CNN output thus yields a prioritized vegetation management system and\ncreates a geotagged map of utility assets as a byproduct. Test set accuracy\nwith reached $80.15\\%$ using VGG11 with a trained first layer and classifier,\nand a model ensemble correctly classified $88.88\\%$ of images with risky\nvegetation overgrowth.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 00:34:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Park", "Austin", ""], ["Rajabi", "Farzaneh", ""], ["Weber", "Ross", ""]]}, {"id": "2105.03807", "submitter": "Shu Chen", "authors": "Shu Chen, Lei Zhang and Beiji Zou", "title": "Estimation of 3D Human Pose Using Prior Knowledge", "comments": "letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating three-dimensional human poses from the positions of\ntwo-dimensional joints has shown promising results.However, using\ntwo-dimensional joint coordinates as input loses more information than\nimage-based approaches and results in ambiguity.In order to overcome this\nproblem, we combine bone length and camera parameters with two-dimensional\njoint coordinates for input.This combination is more discriminative than the\ntwo-dimensional joint coordinates in that it can improve the accuracy of the\nmodel's prediction depth and alleviate the ambiguity that comes from projecting\nthree-dimensional coordinates into two-dimensional space. Furthermore, we\nintroduce direction constraints which can better measure the difference between\nthe ground truth and the output of the proposed model. The experimental results\non the H36M show that the method performed better than other state-of-the-art\nthree-dimensional human pose estimation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:15:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Shu", ""], ["Zhang", "Lei", ""], ["Zou", "Beiji", ""]]}, {"id": "2105.03812", "submitter": "Deeksha Dangwal", "authors": "Deeksha Dangwal, Vincent T. Lee, Hyo Jin Kim, Tianwei Shen, Meghan\n  Cowan, Rajvi Shah, Caroline Trippel, Brandon Reagen, Timothy Sherwood,\n  Vasileios Balntas, Armin Alaghi, Eddy Ilg", "title": "Analysis and Mitigations of Reverse Engineering Attacks on Local Feature\n  Descriptors", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous driving and augmented reality evolve, a practical concern is\ndata privacy. In particular, these applications rely on localization based on\nuser images. The widely adopted technology uses local feature descriptors,\nwhich are derived from the images and it was long thought that they could not\nbe reverted back. However, recent work has demonstrated that under certain\nconditions reverse engineering attacks are possible and allow an adversary to\nreconstruct RGB images. This poses a potential risk to user privacy. We take\nthis a step further and model potential adversaries using a privacy threat\nmodel. Subsequently, we show under controlled conditions a reverse engineering\nattack on sparse feature maps and analyze the vulnerability of popular\ndescriptors including FREAK, SIFT and SOSNet. Finally, we evaluate potential\nmitigation techniques that select a subset of descriptors to carefully balance\nprivacy reconstruction risk while preserving image matching accuracy; our\nresults show that similar accuracy can be obtained when revealing less\ninformation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:41:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dangwal", "Deeksha", ""], ["Lee", "Vincent T.", ""], ["Kim", "Hyo Jin", ""], ["Shen", "Tianwei", ""], ["Cowan", "Meghan", ""], ["Shah", "Rajvi", ""], ["Trippel", "Caroline", ""], ["Reagen", "Brandon", ""], ["Sherwood", "Timothy", ""], ["Balntas", "Vasileios", ""], ["Alaghi", "Armin", ""], ["Ilg", "Eddy", ""]]}, {"id": "2105.03817", "submitter": "Moju Zhao", "authors": "Moju Zhao and Kei Okada and Masayuki Inaba", "title": "TrTr: Visual Tracking with Transformer", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-based discriminative trackers are currently the dominant tracking\nmethods due to their robustness and accuracy, and the Siamese-network-based\nmethods that depend on cross-correlation operation between features extracted\nfrom template and search images show the state-of-the-art tracking performance.\nHowever, general cross-correlation operation can only obtain relationship\nbetween local patches in two feature maps. In this paper, we propose a novel\ntracker network based on a powerful attention mechanism called Transformer\nencoder-decoder architecture to gain global and rich contextual\ninterdependencies. In this new architecture, features of the template image is\nprocessed by a self-attention module in the encoder part to learn strong\ncontext information, which is then sent to the decoder part to compute\ncross-attention with the search image features processed by another\nself-attention module. In addition, we design the classification and regression\nheads using the output of Transformer to localize target based on\nshape-agnostic anchor. We extensively evaluate our tracker TrTr, on VOT2018,\nVOT2019, OTB-100, UAV, NfS, TrackingNet, and LaSOT benchmarks and our method\nperforms favorably against state-of-the-art algorithms. Training code and\npretrained models are available at https://github.com/tongtybj/TrTr.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 02:32:28 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhao", "Moju", ""], ["Okada", "Kei", ""], ["Inaba", "Masayuki", ""]]}, {"id": "2105.03826", "submitter": "Seba Susan", "authors": "Kartik Arora, Ajul Raj, Arun Goel, Seba Susan", "title": "A Hybrid Model for Combining Neural Image Caption and k-Nearest Neighbor\n  Approach for Image Captioning", "comments": "Included in Proceedings of 3rd ICSCSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A hybrid model is proposed that integrates two popular image captioning\nmethods to generate a text-based summary describing the contents of the image.\nThe two image captioning models are the Neural Image Caption (NIC) and the\nk-nearest neighbor approach. These are trained individually on the training\nset. We extract a set of five features, from the validation set, for evaluating\nthe results of the two models that in turn is used to train a logistic\nregression classifier. The BLEU-4 scores of the two models are compared for\ngenerating the binary-value ground truth for the logistic regression\nclassifier. For the test set, the input images are first passed separately\nthrough the two models to generate the individual captions. The\nfive-dimensional feature set extracted from the two models is passed to the\nlogistic regression classifier to take a decision regarding the final caption\ngenerated which is the best of two captions generated by the models. Our\nimplementation of the k-nearest neighbor model achieves a BLEU-4 score of 15.95\nand the NIC model achieves a BLEU-4 score of 16.01, on the benchmark Flickr8k\ndataset. The proposed hybrid model is able to achieve a BLEU-4 score of 18.20\nproving the validity of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:49:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Arora", "Kartik", ""], ["Raj", "Ajul", ""], ["Goel", "Arun", ""], ["Susan", "Seba", ""]]}, {"id": "2105.03827", "submitter": "Wenhao Wu", "authors": "Yuxiang Zhao, Wenhao Wu, Yue He, Yingying Li, Xiao Tan, Shifeng Chen", "title": "Good Practices and A Strong Baseline for Traffic Anomaly Detection", "comments": "We rank $1^{st}$ in the CVPR 2021 NVIDIA AI CITY Challenge for\n  Traffic Anomaly detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The detection of traffic anomalies is a critical component of the intelligent\ncity transportation management system. Previous works have proposed a variety\nof notable insights and taken a step forward in this field, however, dealing\nwith the complex traffic environment remains a challenge. Moreover, the lack of\nhigh-quality data and the complexity of the traffic scene, motivate us to study\nthis problem from a hand-crafted perspective. In this paper, we propose a\nstraightforward and efficient framework that includes pre-processing, a dynamic\ntrack module, and post-processing. With video stabilization, background\nmodeling, and vehicle detection, the pro-processing phase aims to generate\ncandidate anomalies. The dynamic tracking module seeks and locates the start\ntime of anomalies by utilizing vehicle motion patterns and spatiotemporal\nstatus. Finally, we use post-processing to fine-tune the temporal boundary of\nanomalies. Not surprisingly, our proposed framework was ranked $1^{st}$ in the\nNVIDIA AI CITY 2021 leaderboard for traffic anomaly detection. The code is\navailable at: https://github.com/Endeavour10020/AICity2021-Anomaly-Detection .\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:51:37 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 12:47:27 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhao", "Yuxiang", ""], ["Wu", "Wenhao", ""], ["He", "Yue", ""], ["Li", "Yingying", ""], ["Tan", "Xiao", ""], ["Chen", "Shifeng", ""]]}, {"id": "2105.03830", "submitter": "Kaihao Zhang", "authors": "Kaihao Zhang, Wenhan Luo, Yanjiang Yu, Wenqi Ren, Fang Zhao,\n  Changsheng Li, Lin Ma, Wei Liu, Hongdong Li", "title": "Beyond Monocular Deraining: Parallel Stereo Deraining Network Via\n  Semantic Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain is a common natural phenomenon. Taking images in the rain however often\nresults in degraded quality of images, thus compromises the performance of many\ncomputer vision systems. Most existing de-rain algorithms use only one single\ninput image and aim to recover a clean image. Few work has exploited stereo\nimages. Moreover, even for single image based monocular deraining, many current\nmethods fail to complete the task satisfactorily because they mostly rely on\nper pixel loss functions and ignore semantic information. In this paper, we\npresent a Paired Rain Removal Network (PRRNet), which exploits both stereo\nimages and semantic information. Specifically, we develop a Semantic-Aware\nDeraining Module (SADM) which solves both tasks of semantic segmentation and\nderaining of scenes, and a Semantic-Fusion Network (SFNet) and a View-Fusion\nNetwork (VFNet) which fuse semantic information and multi-view information\nrespectively. In addition, we also introduce an Enhanced Paired Rain Removal\nNetwork (EPRRNet) which exploits semantic prior to remove rain streaks from\nstereo images. We first use a coarse deraining network to reduce the rain\nstreaks on the input images, and then adopt a pre-trained semantic segmentation\nnetwork to extract semantic features from the coarse derained image. Finally, a\nparallel stereo deraining network fuses semantic and multi-view information to\nrestore finer results. We also propose new stereo based rainy datasets for\nbenchmarking. Experiments on both monocular and the newly proposed stereo rainy\ndatasets demonstrate that the proposed method achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 04:15:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Kaihao", ""], ["Luo", "Wenhan", ""], ["Yu", "Yanjiang", ""], ["Ren", "Wenqi", ""], ["Zhao", "Fang", ""], ["Li", "Changsheng", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Li", "Hongdong", ""]]}, {"id": "2105.03832", "submitter": "Jasper Brown", "authors": "Jasper Brown, Salah Sukkarieh", "title": "Dataset and Performance Comparison of Deep Learning Architectures for\n  Plum Detection and Robotic Harvesting", "comments": "20 pages, 8 figures, 2 tables. Associated dataset at\n  http://data.acfr.usyd.edu.au/Agriculture/PlumDetection/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many automated operations in agriculture, such as weeding and plant counting,\nrequire robust and accurate object detectors. Robotic fruit harvesting is one\nof these, and is an important technology to address the increasing labour\nshortages and uncertainty suffered by tree crop growers. An eye-in-hand sensing\nsetup is commonly used in harvesting systems and provides benefits to sensing\naccuracy and flexibility. However, as the hand and camera move from viewing the\nentire trellis to picking a specific fruit, large changes in lighting, colour,\nobscuration and exposure occur. Object detection algorithms used in harvesting\nshould be robust to these challenges, but few datasets for assessing this\ncurrently exist. In this work, two new datasets are gathered during day and\nnight operation of an actual robotic plum harvesting system. A range of current\ngeneration deep learning object detectors are benchmarked against these.\nAdditionally, two methods for fusing depth and image information are tested for\ntheir impact on detector performance. Significant differences between day and\nnight accuracy of different detectors is found, transfer learning is identified\nas essential in all cases, and depth information fusion is assessed as only\nmarginally effective. The dataset and benchmark models are made available\nonline.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 04:18:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Brown", "Jasper", ""], ["Sukkarieh", "Salah", ""]]}, {"id": "2105.03834", "submitter": "Hyung-Jin Yoon", "authors": "Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris", "title": "Learning Image Attacks toward Vision Guided Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While adversarial neural networks have been shown successful for static image\nattacks, very few approaches have been developed for attacking online image\nstreams while taking into account the underlying physical dynamics of\nautonomous vehicles, their mission, and environment. This paper presents an\nonline adversarial machine learning framework that can effectively misguide\nautonomous vehicles' missions. In the existing image attack methods devised\ntoward autonomous vehicles, optimization steps are repeated for every image\nframe. This framework removes the need for fully converged optimization at\nevery frame to realize image attacks in real-time. Using reinforcement\nlearning, a generative neural network is trained over a set of image frames to\nobtain an attack policy that is more robust to dynamic and uncertain\nenvironments. A state estimator is introduced for processing image streams to\nreduce the attack policy's sensitivity to physical variables such as unknown\nposition and velocity. A simulation study is provided to validate the results.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 04:34:10 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 19:01:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yoon", "Hyung-Jin", ""], ["Jafarnejadsani", "Hamidreza", ""], ["Voulgaris", "Petros", ""]]}, {"id": "2105.03847", "submitter": "Hong-Ye Zeng", "authors": "Hong-Ye Zeng, Song-Han Ge, Yu-Chong Gao, De-Sen Zhou, Kang Zhou,\n  Xu-Ming He, Edmond Lou, Rui Zheng", "title": "Automatic segmentation of vertebral features on ultrasound spine images\n  using Stacked Hourglass Network", "comments": "9 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The spinous process angle (SPA) is one of the essential parameters\nto denote three-dimensional (3-D) deformity of spine. We propose an automatic\nsegmentation method based on Stacked Hourglass Network (SHN) to detect the\nspinous processes (SP) on ultrasound (US) spine images and to measure the SPAs\nof clinical scoliotic subjects. Methods: The network was trained to detect\nvertebral SP and laminae as five landmarks on 1200 ultrasound transverse images\nand validated on 100 images. All the processed transverse images with\nhighlighted SP and laminae were reconstructed into a 3D image volume, and the\nSPAs were measured on the projected coronal images. The trained network was\ntested on 400 images by calculating the percentage of correct keypoints (PCK);\nand the SPA measurements were evaluated on 50 scoliotic subjects by comparing\nthe results from US images and radiographs. Results: The trained network\nachieved a high average PCK (86.8%) on the test datasets, particularly the PCK\nof SP detection was 90.3%. The SPAs measured from US and radiographic methods\nshowed good correlation (r>0.85), and the mean absolute differences (MAD)\nbetween two modalities were 3.3{\\deg}, which was less than the clinical\nacceptance error (5{\\deg}). Conclusion: The vertebral features can be\naccurately segmented on US spine images using SHN, and the measurement results\nof SPA from US data was comparable to the gold standard from radiography.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 06:04:15 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 03:59:19 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zeng", "Hong-Ye", ""], ["Ge", "Song-Han", ""], ["Gao", "Yu-Chong", ""], ["Zhou", "De-Sen", ""], ["Zhou", "Kang", ""], ["He", "Xu-Ming", ""], ["Lou", "Edmond", ""], ["Zheng", "Rui", ""]]}, {"id": "2105.03857", "submitter": "Yimin Dou", "authors": "YiMin Dou, Kewen Li, Jianbing Zhu, Xiao Li, Yingjie Xi", "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice\n  Labels", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\nfully extract information from limited data and suppress seismic noise, we\npropose an attention module that can be used for active supervision training\nand embedded in the network. The attention heatmap label is generated by the\noriginal label, and letting it supervise the attention module using the\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\nfunction, the method can extract 3D seismic features from a few 2D slice\nlabels. And it also shows the advanced performance of the attention module,\nwhich can significantly suppress the noise in the seismic data while increasing\nthe model's sensitivity to the foreground. Finally, on the public test set, we\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\nlabel, and achieve similar performance to the 3D volume label training.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 07:13:40 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 08:36:17 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 14:11:53 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 07:01:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Dou", "YiMin", ""], ["Li", "Kewen", ""], ["Zhu", "Jianbing", ""], ["Li", "Xiao", ""], ["Xi", "Yingjie", ""]]}, {"id": "2105.03867", "submitter": "Weixuan Tang", "authors": "Weixuan Tang, Bin Li, Mauro Barni, Jin Li, Jiwu Huang", "title": "Improving Cost Learning for JPEG Steganography by Exploiting JPEG Domain\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although significant progress in automatic learning of steganographic cost\nhas been achieved recently, existing methods designed for spatial images are\nnot well applicable to JPEG images which are more common media in daily life.\nThe difficulties of migration mostly lie in the unique and complicated JPEG\ncharacteristics caused by 8x8 DCT mode structure. To address the issue, in this\npaper we extend an existing automatic cost learning scheme to JPEG, where the\nproposed scheme called JEC-RL (JPEG Embedding Cost with Reinforcement Learning)\nis explicitly designed to tailor the JPEG DCT structure. It works with the\nembedding action sampling mechanism under reinforcement learning, where a\npolicy network learns the optimal embedding policies via maximizing the rewards\nprovided by an environment network. The policy network is constructed following\na domain-transition design paradigm, where three modules including pixel-level\ntexture complexity evaluation, DCT feature extraction, and mode-wise\nrearrangement, are proposed. These modules operate in serial, gradually\nextracting useful features from a decompressed JPEG image and converting them\ninto embedding policies for DCT elements, while considering JPEG\ncharacteristics including inter-block and intra-block correlations\nsimultaneously. The environment network is designed in a gradient-oriented way\nto provide stable reward values by using a wide architecture equipped with a\nfixed preprocessing layer with 8x8 DCT basis filters. Extensive experiments and\nablation studies demonstrate that the proposed method can achieve good security\nperformance for JPEG images against both advanced feature based and modern CNN\nbased steganalyzers.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:10:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tang", "Weixuan", ""], ["Li", "Bin", ""], ["Barni", "Mauro", ""], ["Li", "Jin", ""], ["Huang", "Jiwu", ""]]}, {"id": "2105.03869", "submitter": "Jiaolong Xu", "authors": "Jiaolong Xu, Liang Xiao, Dawei Zhao, Yiming Nie, Bin Dai", "title": "Trajectory Prediction for Autonomous Driving with Topometric Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art autonomous driving systems rely on high definition (HD) maps\nfor localization and navigation. However, building and maintaining HD maps is\ntime-consuming and expensive. Furthermore, the HD maps assume structured\nenvironment such as the existence of major road and lanes, which are not\npresent in rural areas. In this work, we propose an end-to-end transformer\nnetworks based approach for map-less autonomous driving. The proposed model\ntakes raw LiDAR data and noisy topometric map as input and produces precise\nlocal trajectory for navigation. We demonstrate the effectiveness of our method\nin real-world driving data, including both urban and rural areas. The\nexperimental results show that the proposed method outperforms state-of-the-art\nmultimodal methods and is robust to the perturbations of the topometric map.\nThe code of the proposed method is publicly available at\n\\url{https://github.com/Jiaolong/trajectory-prediction}.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:16:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xu", "Jiaolong", ""], ["Xiao", "Liang", ""], ["Zhao", "Dawei", ""], ["Nie", "Yiming", ""], ["Dai", "Bin", ""]]}, {"id": "2105.03876", "submitter": "Saeed Bakhshi Germi", "authors": "Saeed Bakhshi Germi and Esa Rahtu and Heikki Huttunen", "title": "Selective Probabilistic Classifier Based on Hypothesis Testing", "comments": "Accepted in EUVIP 2021 conference. Comments: Copyright statement\n  added to first page in new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a simple yet effective method to deal with the\nviolation of the Closed-World Assumption for a classifier. Previous works tend\nto apply a threshold either on the classification scores or the loss function\nto reject the inputs that violate the assumption. However, these methods cannot\nachieve the low False Positive Ratio (FPR) required in safety applications. The\nproposed method is a rejection option based on hypothesis testing with\nprobabilistic networks. With probabilistic networks, it is possible to estimate\nthe distribution of outcomes instead of a single output. By utilizing Z-test\nover the mean and standard deviation for each class, the proposed method can\nestimate the statistical significance of the network certainty and reject\nuncertain outputs. The proposed method was experimented on with different\nconfigurations of the COCO and CIFAR datasets. The performance of the proposed\nmethod is compared with the Softmax Response, which is a known top-performing\nmethod. It is shown that the proposed method can achieve a broader range of\noperation and cover a lower FPR than the alternative.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:55:56 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 20:41:58 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Germi", "Saeed Bakhshi", ""], ["Rahtu", "Esa", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2105.03889", "submitter": "Zhiliang Peng", "authors": "Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin\n  Jiao, Qixiang Ye", "title": "Conformer: Local Features Coupling Global Representations for Visual\n  Recognition", "comments": "submitted to iccv2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within Convolutional Neural Network (CNN), the convolution operations are\ngood at extracting local features but experience difficulty to capture global\nrepresentations. Within visual transformer, the cascaded self-attention modules\ncan capture long-distance feature dependencies but unfortunately deteriorate\nlocal feature details. In this paper, we propose a hybrid network structure,\ntermed Conformer, to take advantage of convolutional operations and\nself-attention mechanisms for enhanced representation learning. Conformer roots\nin the Feature Coupling Unit (FCU), which fuses local features and global\nrepresentations under different resolutions in an interactive fashion.\nConformer adopts a concurrent structure so that local features and global\nrepresentations are retained to the maximum extent. Experiments show that\nConformer, under the comparable parameter complexity, outperforms the visual\ntransformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101\nby 3.7% and 3.6% mAPs for object detection and instance segmentation,\nrespectively, demonstrating the great potential to be a general backbone\nnetwork. Code is available at https://github.com/pengzhiliang/Conformer.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:00:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Peng", "Zhiliang", ""], ["Huang", "Wei", ""], ["Gu", "Shanzhi", ""], ["Xie", "Lingxi", ""], ["Wang", "Yaowei", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "2105.03891", "submitter": "Hao Cheng", "authors": "Hao Cheng, Li Feng, Hailong Liu, Takatsugu Hirayama, Hiroshi Murase\n  and Monika Sester", "title": "Interaction Detection Between Vehicles and Vulnerable Road Users: A Deep\n  Generative Approach with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intersections where vehicles are permitted to turn and interact with\nvulnerable road users (VRUs) like pedestrians and cyclists are among some of\nthe most challenging locations for automated and accurate recognition of road\nusers' behavior. In this paper, we propose a deep conditional generative model\nfor interaction detection at such locations. It aims to automatically analyze\nmassive video data about the continuity of road users' behavior. This task is\nessential for many intelligent transportation systems such as traffic safety\ncontrol and self-driving cars that depend on the understanding of road users'\nlocomotion. A Conditional Variational Auto-Encoder based model with Gaussian\nlatent variables is trained to encode road users' behavior and perform\nprobabilistic and diverse predictions of interactions. The model takes as input\nthe information of road users' type, position and motion automatically\nextracted by a deep learning object detector and optical flow from videos, and\ngenerates frame-wise probabilities that represent the dynamics of interactions\nbetween a turning vehicle and any VRUs involved. The model's efficacy was\nvalidated by testing on real--world datasets acquired from two different\nintersections. It achieved an F1-score above 0.96 at a right--turn intersection\nin Germany and 0.89 at a left--turn intersection in Japan, both with very busy\ntraffic flows.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:03:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cheng", "Hao", ""], ["Feng", "Li", ""], ["Liu", "Hailong", ""], ["Hirayama", "Takatsugu", ""], ["Murase", "Hiroshi", ""], ["Sester", "Monika", ""]]}, {"id": "2105.03897", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Gozde Bozdagi Akar", "title": "Binarized Weight Error Networks With a Transition Regularization Term", "comments": "Submitted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel binarized weight network (BT) for a\nresource-efficient neural structure. The proposed model estimates a binary\nrepresentation of weights by taking into account the approximation error with\nan additional term. This model increases representation capacity and stability,\nparticularly for shallow networks, while the computation load is theoretically\nreduced. In addition, a novel regularization term is introduced that is\nsuitable for all threshold-based binary precision networks. This term penalizes\nthe trainable parameters that are far from the thresholds at which binary\ntransitions occur. This step promotes a swift modification for binary-precision\nresponses at train time. The experimental results are carried out for two sets\nof tasks: visual classification and visual inverse problems. Benchmarks for\nCifar10, SVHN, Fashion, ImageNet2012, Set5, Set14, Urban and BSD100 datasets\nshow that our method outperforms all counterparts with binary precision.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:11:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "2105.03906", "submitter": "Oren Nuriel", "authors": "Oren Nuriel, Sharon Fogel, Ron Litman", "title": "TextAdaIN: Fine-Grained AdaIN for Robust Text Recognition", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging the characteristics of convolutional layers, image classifiers are\nextremely effective. However, recent works have exposed that in many cases they\nimmoderately rely on global image statistics that are easy to manipulate while\npreserving image semantics. In text recognition, we reveal that it is rather\nthe local image statistics which the networks overly depend on. Motivated by\nthis, we suggest an approach to regulate the reliance on local statistics that\nimproves overall text recognition performance.\n  Our method, termed TextAdaIN, creates local distortions in the feature map\nwhich prevent the network from overfitting to the local statistics. It does so\nby deliberately mismatching fine-grained feature statistics between samples in\na mini-batch. Despite TextAdaIN's simplicity, extensive experiments show its\neffectiveness compared to other, more complicated methods. TextAdaIN achieves\nstate-of-the-art results on standard handwritten text recognition benchmarks.\nAdditionally, it generalizes to multiple architectures and to the domain of\nscene text recognition. Furthermore, we demonstrate that integrating TextAdaIN\nimproves robustness towards image corruptions.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:47:48 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Nuriel", "Oren", ""], ["Fogel", "Sharon", ""], ["Litman", "Ron", ""]]}, {"id": "2105.03934", "submitter": "Md Shoaib Ahmed", "authors": "Md Shoaib Ahmed, Tanjim Taharat Aurpa, Md. Abul Kalam Azad", "title": "Fish Disease Detection Using Image Based Machine Learning Technique in\n  Aquaculture", "comments": "15 pages, 10 figures, 7 tables. Accepted Manuscript. Journal of King\n  Saud University - Computer and Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fish diseases in aquaculture constitute a significant hazard to nutriment\nsecurity. Identification of infected fishes in aquaculture remains challenging\nto find out at the early stage due to the dearth of necessary infrastructure.\nThe identification of infected fish timely is an obligatory step to thwart from\nspreading disease. In this work, we want to find out the salmon fish disease in\naquaculture, as salmon aquaculture is the fastest-growing food production\nsystem globally, accounting for 70 percent (2.5 million tons) of the market. In\nthe alliance of flawless image processing and machine learning mechanism, we\nidentify the infected fishes caused by the various pathogen. This work divides\ninto two portions. In the rudimentary portion, image pre-processing and\nsegmentation have been applied to reduce noise and exaggerate the image,\nrespectively. In the second portion, we extract the involved features to\nclassify the diseases with the help of the Support Vector Machine (SVM)\nalgorithm of machine learning with a kernel function. The processed images of\nthe first portion have passed through this (SVM) model. Then we harmonize a\ncomprehensive experiment with the proposed combination of techniques on the\nsalmon fish image dataset used to examine the fish disease. We have conveyed\nthis work on a novel dataset compromising with and without image augmentation.\nThe results have bought a judgment of our applied SVM performs notably with\n91.42 and 94.12 percent of accuracy, respectively, with and without\naugmentation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:22:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ahmed", "Md Shoaib", ""], ["Aurpa", "Tanjim Taharat", ""], ["Azad", "Md. Abul Kalam", ""]]}, {"id": "2105.03939", "submitter": "Li Shen", "authors": "Han Huang, Li Shen, Chaoyang He, Weisheng Dong, Haozhi Huang,\n  Guangming Shi", "title": "Lightweight Image Super-Resolution with Hierarchical and Differentiable\n  Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single Image Super-Resolution (SISR) tasks have achieved significant\nperformance with deep neural networks. However, the large number of parameters\nin CNN-based methods for SISR tasks require heavy computations. Although\nseveral efficient SISR models have been recently proposed, most are handcrafted\nand thus lack flexibility. In this work, we propose a novel differentiable\nNeural Architecture Search (NAS) approach on both the cell-level and\nnetwork-level to search for lightweight SISR models. Specifically, the\ncell-level search space is designed based on an information distillation\nmechanism, focusing on the combinations of lightweight operations and aiming to\nbuild a more lightweight and accurate SR structure. The network-level search\nspace is designed to consider the feature connections among the cells and aims\nto find which information flow benefits the cell most to boost the performance.\nUnlike the existing Reinforcement Learning (RL) or Evolutionary Algorithm (EA)\nbased NAS methods for SISR tasks, our search pipeline is fully differentiable,\nand the lightweight SISR models can be efficiently searched on both the\ncell-level and network-level jointly on a single GPU. Experiments show that our\nmethods can achieve state-of-the-art performance on the benchmark datasets in\nterms of PSNR, SSIM, and model complexity with merely 68G Multi-Adds for\n$\\times 2$ and 18G Multi-Adds for $\\times 4$ SR tasks. Code will be available\nat \\url{https://github.com/DawnHH/DLSR-PyTorch}.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:30:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Han", ""], ["Shen", "Li", ""], ["He", "Chaoyang", ""], ["Dong", "Weisheng", ""], ["Huang", "Haozhi", ""], ["Shi", "Guangming", ""]]}, {"id": "2105.03958", "submitter": "Fani Deligianni Dr", "authors": "Matthew Malek-Podjaski, Fani Deligianni", "title": "Preserving Privacy in Human-Motion Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human motion is a biomarker used extensively in clinical analysis to monitor\nthe progression of neurological diseases and mood disorders. Since perceptions\nof emotions are also interleaved with body posture and movements, emotion\nrecognition from human gait can be used to quantitatively monitor mood changes\nthat are often related to neurological disorders. Many existing solutions often\nuse shallow machine learning models with raw positional data or manually\nextracted features to achieve this. However, gait is composed of many highly\nexpressive characteristics that can be used to identify human subjects, and\nmost solutions fail to address this, disregarding the subject's privacy. This\nwork evaluates the effectiveness of existing methods at recognising emotions\nusing both 3D temporal joint signals and manually extracted features. We also\nshow that this data can easily be exploited to expose a subject's identity.\nTherefore to this end, we propose a cross-subject transfer learning technique\nfor training a multi-encoder autoencoder deep neural network to learn\ndisentangled latent representations of human motion features. By disentangling\nsubject biometrics from the gait data, we show that the subjects privacy is\npreserved while the affect recognition performance outperforms traditional\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 15:26:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Malek-Podjaski", "Matthew", ""], ["Deligianni", "Fani", ""]]}, {"id": "2105.03995", "submitter": "Md. Kamrul Hasan", "authors": "Chayan Mondal, Md. Kamrul Hasan, Md. Tasnim Jawad, Aishwariya Dutta,\n  Md.Rabiul Islam, Md. Abdul Awal, Mohiuddin Ahmad", "title": "Acute Lymphoblastic Leukemia Detection from Microscopic Images Using\n  Weighted Ensemble of Convolutional Neural Networks", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acute Lymphoblastic Leukemia (ALL) is a blood cell cancer characterized by\nnumerous immature lymphocytes. Even though automation in ALL prognosis is an\nessential aspect of cancer diagnosis, it is challenging due to the\nmorphological correlation between malignant and normal cells. The traditional\nALL classification strategy demands experienced pathologists to carefully read\nthe cell images, which is arduous, time-consuming, and often suffers\ninter-observer variations. This article has automated the ALL detection task\nfrom microscopic cell images, employing deep Convolutional Neural Networks\n(CNNs). We explore the weighted ensemble of different deep CNNs to recommend a\nbetter ALL cell classifier. The weights for the ensemble candidate models are\nestimated from their corresponding metrics, such as accuracy, F1-score, AUC,\nand kappa values. Various data augmentations and pre-processing are\nincorporated for achieving a better generalization of the network. We utilize\nthe publicly available C-NMC-2019 ALL dataset to conduct all the comprehensive\nexperiments. Our proposed weighted ensemble model, using the kappa values of\nthe ensemble candidates as their weights, has outputted a weighted F1-score of\n88.6 %, a balanced accuracy of 86.2 %, and an AUC of 0.941 in the preliminary\ntest set. The qualitative results displaying the gradient class activation maps\nconfirm that the introduced model has a concentrated learned region. In\ncontrast, the ensemble candidate models, such as Xception, VGG-16,\nDenseNet-121, MobileNet, and InceptionResNet-V2, separately produce coarse and\nscatter learned areas for most example cases. Since the proposed kappa\nvalue-based weighted ensemble yields a better result for the aimed task in this\narticle, it can experiment in other domains of medical diagnostic applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:58:48 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Mondal", "Chayan", ""], ["Hasan", "Md. Kamrul", ""], ["Jawad", "Md. Tasnim", ""], ["Dutta", "Aishwariya", ""], ["Islam", "Md. Rabiul", ""], ["Awal", "Md. Abdul", ""], ["Ahmad", "Mohiuddin", ""]]}, {"id": "2105.04014", "submitter": "Micha{\\l} Koziarski", "authors": "Micha{\\l} Koziarski, Bogus{\\l}aw Cyganek, Bogus{\\l}aw Olborski,\n  Zbigniew Antosz, Marcin \\.Zydak, Bogdan Kwolek, Pawe{\\l} W\\k{a}sowicz,\n  Andrzej Buka{\\l}a, Jakub Swad\\'zba, Piotr Sitkowski", "title": "DiagSet: a dataset for prostate cancer histopathological image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer diseases constitute one of the most significant societal challenges.\nIn this paper we introduce a novel histopathological dataset for prostate\ncancer detection. The proposed dataset, consisting of over 2.6 million tissue\npatches extracted from 430 fully annotated scans, 4675 scans with assigned\nbinary diagnosis, and 46 scans with diagnosis given independently by a group of\nhistopathologists, can be found at https://ai-econsilio.diag.pl. Furthermore,\nwe propose a machine learning framework for detection of cancerous tissue\nregions and prediction of scan-level diagnosis, utilizing thresholding and\nstatistical analysis to abstain from the decision in uncertain cases. During\nthe experimental evaluation we identify several factors negatively affecting\nthe performance of considered models, such as presence of label noise, data\nimbalance, and quantity of data, that can serve as a basis for further\nresearch. The proposed approach, composed of ensembles of deep neural networks\noperating on the histopathological scans at different scales, achieves 94.6%\naccuracy in patch-level recognition, and is compared in a scan-level diagnosis\nwith 9 human histopathologists.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 20:06:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Koziarski", "Micha\u0142", ""], ["Cyganek", "Bogus\u0142aw", ""], ["Olborski", "Bogus\u0142aw", ""], ["Antosz", "Zbigniew", ""], ["\u017bydak", "Marcin", ""], ["Kwolek", "Bogdan", ""], ["W\u0105sowicz", "Pawe\u0142", ""], ["Buka\u0142a", "Andrzej", ""], ["Swad\u017aba", "Jakub", ""], ["Sitkowski", "Piotr", ""]]}, {"id": "2105.04020", "submitter": "M. F. Mridha", "authors": "Farisa Benta Safir, Abu Quwsar Ohi, M.F. Mridha, Muhammad Mostafa\n  Monowar, Md. Abdul Hamid", "title": "End-to-End Optical Character Recognition for Bengali Handwritten Words", "comments": "Accepted in \"The 4th National Computing Colleges Conference\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical character recognition (OCR) is a process of converting analogue\ndocuments into digital using document images. Currently, many commercial and\nnon-commercial OCR systems exist for both handwritten and printed copies for\ndifferent languages. Despite this, very few works are available in case of\nrecognising Bengali words. Among them, most of the works focused on OCR of\nprinted Bengali characters. This paper introduces an end-to-end OCR system for\nBengali language. The proposed architecture implements an end to end strategy\nthat recognises handwritten Bengali words from handwritten word images. We\nexperiment with popular convolutional neural network (CNN) architectures,\nincluding DenseNet, Xception, NASNet, and MobileNet to build the OCR\narchitecture. Further, we experiment with two different recurrent neural\nnetworks (RNN) methods, LSTM and GRU. We evaluate the proposed architecture\nusing BanglaWritting dataset, which is a peer-reviewed Bengali handwritten\nimage dataset. The proposed method achieves 0.091 character error rate and\n0.273 word error rate performed using DenseNet121 model with GRU recurrent\nlayer.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 20:48:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Safir", "Farisa Benta", ""], ["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""], ["Monowar", "Muhammad Mostafa", ""], ["Hamid", "Md. Abdul", ""]]}, {"id": "2105.04040", "submitter": "Anadi Chaman", "authors": "Anadi Chaman and Ivan Dokmani\\'c", "title": "Truly shift-equivariant convolutional neural networks with adaptive\n  polyphase upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks lack shift equivariance due to the presence of\ndownsampling layers. In image classification, adaptive polyphase downsampling\n(APS-D) was recently proposed to make CNNs perfectly shift invariant. However,\nin networks used for image reconstruction tasks, it can not by itself restore\nshift equivariance. We address this problem by proposing adaptive polyphase\nupsampling (APS-U), a non-linear extension of conventional upsampling, which\nallows CNNs to exhibit perfect shift equivariance. With MRI and CT\nreconstruction experiments, we show that networks containing APS-D/U layers\nexhibit state of the art equivariance performance without sacrificing on image\nreconstruction quality. In addition, unlike prior methods like data\naugmentation and anti-aliasing, the gains in equivariance obtained from APS-D/U\nalso extend to images outside the training distribution.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 22:33:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chaman", "Anadi", ""], ["Dokmani\u0107", "Ivan", ""]]}, {"id": "2105.04066", "submitter": "Bin Zhao", "authors": "Bin Zhao, Haopeng Li, Xiaoqiang Lu, Xuelong Li", "title": "Reconstructive Sequence-Graph Network for Video Summarization", "comments": "Accepted by IEEE TPAMI 2021", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3072117", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the inner-shot and inter-shot dependencies is essential for\nkey-shot based video summarization. Current approaches mainly devote to\nmodeling the video as a frame sequence by recurrent neural networks. However,\none potential limitation of the sequence models is that they focus on capturing\nlocal neighborhood dependencies while the high-order dependencies in long\ndistance are not fully exploited. In general, the frames in each shot record a\ncertain activity and vary smoothly over time, but the multi-hop relationships\noccur frequently among shots. In this case, both the local and global\ndependencies are important for understanding the video content. Motivated by\nthis point, we propose a Reconstructive Sequence-Graph Network (RSGN) to encode\nthe frames and shots as sequence and graph hierarchically, where the\nframe-level dependencies are encoded by Long Short-Term Memory (LSTM), and the\nshot-level dependencies are captured by the Graph Convolutional Network (GCN).\nThen, the videos are summarized by exploiting both the local and global\ndependencies among shots. Besides, a reconstructor is developed to reward the\nsummary generator, so that the generator can be optimized in an unsupervised\nmanner, which can avert the lack of annotated data in video summarization.\nFurthermore, under the guidance of reconstruction loss, the predicted summary\ncan better preserve the main video content and shot-level dependencies.\nPractically, the experimental results on three popular datasets i.e., SumMe,\nTVsum and VTW) have demonstrated the superiority of our proposed approach to\nthe summarization task.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 01:47:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhao", "Bin", ""], ["Li", "Haopeng", ""], ["Lu", "Xiaoqiang", ""], ["Li", "Xuelong", ""]]}, {"id": "2105.04070", "submitter": "Shuo Wang", "authors": "Shuo Wang, Lingjuan Lyu, Surya Nepal, Carsten Rudolph, Marthie\n  Grobler, Kristen Moore", "title": "Robust Training Using Natural Transformation", "comments": "arXiv admin note: text overlap with arXiv:1912.03192,\n  arXiv:2004.02546 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous robustness approaches for deep learning models such as data\naugmentation techniques via data transformation or adversarial training cannot\ncapture real-world variations that preserve the semantics of the input, such as\na change in lighting conditions. To bridge this gap, we present NaTra, an\nadversarial training scheme that is designed to improve the robustness of image\nclassification algorithms. We target attributes of the input images that are\nindependent of the class identification, and manipulate those attributes to\nmimic real-world natural transformations (NaTra) of the inputs, which are then\nused to augment the training dataset of the image classifier. Specifically, we\napply \\textit{Batch Inverse Encoding and Shifting} to map a batch of given\nimages to corresponding disentangled latent codes of well-trained generative\nmodels. \\textit{Latent Codes Expansion} is used to boost image reconstruction\nquality through the incorporation of extended feature maps.\n\\textit{Unsupervised Attribute Directing and Manipulation} enables\nidentification of the latent directions that correspond to specific attribute\nchanges, and then produce interpretable manipulations of those attributes,\nthereby generating natural transformations to the input data. We demonstrate\nthe efficacy of our scheme by utilizing the disentangled latent representations\nderived from well-trained GANs to mimic transformations of an image that are\nsimilar to real-world natural variations (such as lighting conditions or\nhairstyle), and train models to be invariant to these natural transformations.\nExtensive experiments show that our method improves generalization of\nclassification models and increases its robustness to various real-world\ndistortions\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 01:56:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Shuo", ""], ["Lyu", "Lingjuan", ""], ["Nepal", "Surya", ""], ["Rudolph", "Carsten", ""], ["Grobler", "Marthie", ""], ["Moore", "Kristen", ""]]}, {"id": "2105.04075", "submitter": "Ange Lou", "authors": "Ange Lou, Shuyue Guan and Murray Loew", "title": "CFPNet-M: A Light-Weight Encoder-Decoder Based Network for Multimodal\n  Biomedical Image Real-Time Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, developments of deep learning techniques are providing\ninstrumental to identify, classify, and quantify patterns in medical images.\nSegmentation is one of the important applications in medical image analysis. In\nthis regard, U-Net is the predominant approach to medical image segmentation\ntasks. However, we found that those U-Net based models have limitations in\nseveral aspects, for example, millions of parameters in the U-Net consuming\nconsiderable computation resource and memory, lack of global information, and\nmissing some tough objects. Therefore, we applied two modifications to improve\nthe U-Net model: 1) designed and added the dilated channel-wise CNN module, 2)\nsimplified the U shape network. Based on these two modifications, we proposed a\nnovel light-weight architecture -- Channel-wise Feature Pyramid Network for\nMedicine (CFPNet-M). To evaluate our method, we selected five datasets with\ndifferent modalities: thermography, electron microscopy, endoscopy, dermoscopy,\nand digital retinal images. And we compared its performance with several models\nhaving different parameter scales. This paper also involves our previous\nstudies of DC-UNet and some commonly used light-weight neural networks. We\napplied the Tanimoto similarity instead of the Jaccard index for gray-level\nimage measurements. By comparison, CFPNet-M achieves comparable segmentation\nresults on all five medical datasets with only 0.65 million parameters, which\nis about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed can\nreach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 02:29:11 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 15:07:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lou", "Ange", ""], ["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2105.04078", "submitter": "Zhiyu Jiang", "authors": "Can Yao, Yuan Yuan, Zhiyu Jiang", "title": "Self-supervised spectral matching network for hyperspectral target\n  detection", "comments": "IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral target detection is a pixel-level recognition problem. Given a\nfew target samples, it aims to identify the specific target pixels such as\nairplane, vehicle, ship, from the entire hyperspectral image. In general, the\nbackground pixels take the majority of the image and complexly distributed. As\na result, the datasets are weak annotated and extremely imbalanced. To address\nthese problems, a spectral mixing based self-supervised paradigm is designed\nfor hyperspectral data to obtain an effective feature representation. The model\nadopts a spectral similarity based matching network framework. In order to\nlearn more discriminative features, a pair-based loss is adopted to minimize\nthe distance between target pixels while maximizing the distances between\ntarget and background. Furthermore, through a background separated step, the\ncomplex unlabeled spectra are downsampled into different sub-categories. The\nexperimental results on three real hyperspectral datasets demonstrate that the\nproposed framework achieves better results compared with the existing\ndetectors.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 02:32:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yao", "Can", ""], ["Yuan", "Yuan", ""], ["Jiang", "Zhiyu", ""]]}, {"id": "2105.04093", "submitter": "Abhishek Aich", "authors": "Abhishek Aich", "title": "Elastic Weight Consolidation (EWC): Nuts and Bolts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we present a theoretical support of the continual learning\nmethod \\textbf{Elastic Weight Consolidation}, introduced in paper titled\n`Overcoming catastrophic forgetting in neural networks'. Being one of the most\ncited paper in regularized methods for continual learning, this report\ndisentangles the underlying concept of the proposed objective function. We\nassume that the reader is aware of the basic terminologies of continual\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:48:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Aich", "Abhishek", ""]]}, {"id": "2105.04102", "submitter": "Zhiyu Jiang", "authors": "Yuejiao Su, Yuan Yuan, Zhiyu Jiang", "title": "Deep feature selection-and-fusion for RGB-D semantic segmentation", "comments": "ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene depth information can help visual information for more accurate\nsemantic segmentation. However, how to effectively integrate multi-modality\ninformation into representative features is still an open problem. Most of the\nexisting work uses DCNNs to implicitly fuse multi-modality information. But as\nthe network deepens, some critical distinguishing features may be lost, which\nreduces the segmentation performance. This work proposes a unified and\nefficient feature selectionand-fusion network (FSFNet), which contains a\nsymmetric cross-modality residual fusion module used for explicit fusion of\nmulti-modality information. Besides, the network includes a detailed feature\npropagation module, which is used to maintain low-level detailed information\nduring the forward process of the network. Compared with the state-of-the-art\nmethods, experimental evaluations demonstrate that the proposed model achieves\ncompetitive performance on two public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:02:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Su", "Yuejiao", ""], ["Yuan", "Yuan", ""], ["Jiang", "Zhiyu", ""]]}, {"id": "2105.04113", "submitter": "Yichun Tai", "authors": "Hailin Shi, Dan Zeng, Yichun Tai, Hang Du, Yibo Hu, Tao Mei", "title": "Multi-Agent Semi-Siamese Training for Long-tail and Shallow Face\n  Learning", "comments": "12 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2007.08398", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent development of deep convolutional neural networks and\nlarge-scale datasets, deep face recognition has made remarkable progress and\nbeen widely used in various applications. However, unlike the existing public\nface datasets, in many real-world scenarios of face recognition, the depth of\ntraining dataset is shallow, which means only two face images are available for\neach ID. With the non-uniform increase of samples, such issue is converted to a\nmore general case, a.k.a long-tail face learning, which suffers from data\nimbalance and intra-class diversity dearth simultaneously. These adverse\nconditions damage the training and result in the decline of model performance.\nBased on the Semi-Siamese Training (SST), we introduce an advanced solution,\nnamed Multi-Agent Semi-Siamese Training (MASST), to address these problems.\nMASST includes a probe network and multiple gallery agents, the former aims to\nencode the probe features, and the latter constitutes a stack of networks that\nencode the prototypes (gallery features). For each training iteration, the\ngallery network, which is sequentially rotated from the stack, and the probe\nnetwork form a pair of semi-siamese networks. We give theoretical and empirical\nanalysis that, given the long-tail (or shallow) data and training loss, MASST\nsmooths the loss landscape and satisfies the Lipschitz continuity with the help\nof multiple agents and the updating gallery queue. The proposed method is out\nof extra-dependency, thus can be easily integrated with the existing loss\nfunctions and network architectures. It is worth noting that, although multiple\ngallery agents are employed for training, only the probe network is needed for\ninference, without increasing the inference cost. Extensive experiments and\ncomparisons demonstrate the advantages of MASST for long-tail and shallow face\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:57:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shi", "Hailin", ""], ["Zeng", "Dan", ""], ["Tai", "Yichun", ""], ["Du", "Hang", ""], ["Hu", "Yibo", ""], ["Mei", "Tao", ""]]}, {"id": "2105.04128", "submitter": "Stephen MacDonell", "authors": "Nidhi Gowdra, Roopak Sinha and Stephen MacDonell", "title": "Examining and Mitigating Kernel Saturation in Convolutional Neural\n  Networks using Negative Images", "comments": "Conference paper, 6 pages, 3 figures, 1 table", "journal-ref": "Proceedings of the 46th Annual Conference of the IEEE Industrial\n  Electronics Society (IECON2020). IEEE Computer Society Press, pp.465-470", "doi": "10.1109/IECON43393.2020.9255147", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural saturation in Deep Neural Networks (DNNs) has been studied\nextensively, but remains relatively unexplored in Convolutional Neural Networks\n(CNNs). Understanding and alleviating the effects of convolutional kernel\nsaturation is critical for enhancing CNN models classification accuracies. In\nthis paper, we analyze the effect of convolutional kernel saturation in CNNs\nand propose a simple data augmentation technique to mitigate saturation and\nincrease classification accuracy, by supplementing negative images to the\ntraining dataset. We hypothesize that greater semantic feature information can\nbe extracted using negative images since they have the same structural\ninformation as standard images but differ in their data representations. Varied\ndata representations decrease the probability of kernel saturation and thus\nincrease the effectiveness of kernel weight updates. The two datasets selected\nto evaluate our hypothesis were CIFAR- 10 and STL-10 as they have similar image\nclasses but differ in image resolutions thus making for a better understanding\nof the saturation phenomenon. MNIST dataset was used to highlight the\nineffectiveness of the technique for linearly separable data. The ResNet CNN\narchitecture was chosen since the skip connections in the network ensure the\nmost important features contributing the most to classification accuracy are\nretained. Our results show that CNNs are indeed susceptible to convolutional\nkernel saturation and that supplementing negative images to the training\ndataset can offer a statistically significant increase in classification\naccuracies when compared against models trained on the original datasets. Our\nresults present accuracy increases of 6.98% and 3.16% on the STL-10 and\nCIFAR-10 datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:06:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gowdra", "Nidhi", ""], ["Sinha", "Roopak", ""], ["MacDonell", "Stephen", ""]]}, {"id": "2105.04132", "submitter": "Xuan Yang", "authors": "Xuan Yang, Shanshan Li, Zhengchao Chen, Jocelyn Chanussot, Xiuping\n  Jia, Bing Zhang, Baipeng Li, Pan Chen", "title": "An Attention-Fused Network for Semantic Segmentation of\n  Very-High-Resolution Remote Sensing Imagery", "comments": "35 pages. Published by ISPRS Journal of Photogrammetry and Remote\n  Sensing", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 177: 238-262,\n  2021", "doi": "10.1016/j.isprsjprs.2021.05.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an essential part of deep learning. In recent years,\nwith the development of remote sensing big data, semantic segmentation has been\nincreasingly used in remote sensing. Deep convolutional neural networks (DCNNs)\nface the challenge of feature fusion: very-high-resolution remote sensing image\nmultisource data fusion can increase the network's learnable information, which\nis conducive to correctly classifying target objects by DCNNs; simultaneously,\nthe fusion of high-level abstract features and low-level spatial features can\nimprove the classification accuracy at the border between target objects. In\nthis paper, we propose a multipath encoder structure to extract features of\nmultipath inputs, a multipath attention-fused block module to fuse multipath\nfeatures, and a refinement attention-fused block module to fuse high-level\nabstract features and low-level spatial features. Furthermore, we propose a\nnovel convolutional neural network architecture, named attention-fused network\n(AFNet). Based on our AFNet, we achieve state-of-the-art performance with an\noverall accuracy of 91.7% and a mean F1 score of 90.96% on the ISPRS Vaihingen\n2D dataset and an overall accuracy of 92.1% and a mean F1 score of 93.44% on\nthe ISPRS Potsdam 2D dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:23:27 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 11:21:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yang", "Xuan", ""], ["Li", "Shanshan", ""], ["Chen", "Zhengchao", ""], ["Chanussot", "Jocelyn", ""], ["Jia", "Xiuping", ""], ["Zhang", "Bing", ""], ["Li", "Baipeng", ""], ["Chen", "Pan", ""]]}, {"id": "2105.04133", "submitter": "Yu Yao", "authors": "Yu Yao, Ella Atkins, Matthew Johnson Roberson, Ram Vasudevan, Xiaoxiao\n  Du", "title": "Coupling Intent and Action for Pedestrian Crossing Behavior Prediction", "comments": "7pages, 4 figures, 3 tables. Accepted to IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of pedestrian crossing behaviors by autonomous vehicles\ncan significantly improve traffic safety. Existing approaches often model\npedestrian behaviors using trajectories or poses but do not offer a deeper\nsemantic interpretation of a person's actions or how actions influence a\npedestrian's intention to cross in the future. In this work, we follow the\nneuroscience and psychological literature to define pedestrian crossing\nbehavior as a combination of an unobserved inner will (a probabilistic\nrepresentation of binary intent of crossing vs. not crossing) and a set of\nmulti-class actions (e.g., walking, standing, etc.). Intent generates actions,\nand the future actions in turn reflect the intent. We present a novel\nmulti-task network that predicts future pedestrian actions and uses predicted\nfuture action as a prior to detect the present intent and action of the\npedestrian. We also designed an attention relation network to incorporate\nexternal environmental contexts thus further improve intent and action\ndetection performance. We evaluated our approach on two naturalistic driving\ndatasets, PIE and JAAD, and extensive experiments show significantly improved\nand more explainable results for both intent detection and action prediction\nover state-of-the-art approaches. Our code is available at:\nhttps://github.com/umautobots/pedestrian_intent_action_detection.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:26:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yao", "Yu", ""], ["Atkins", "Ella", ""], ["Roberson", "Matthew Johnson", ""], ["Vasudevan", "Ram", ""], ["Du", "Xiaoxiao", ""]]}, {"id": "2105.04143", "submitter": "Dandan Guo", "authors": "Dandan Guo, Ruiying Lu, Bo Chen, Zequn Zeng, Mingyuan Zhou", "title": "Matching Visual Features to Hierarchical Semantic Topics for Image\n  Paragraph Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing a set of images and their corresponding paragraph-captions, a\nchallenging task is to learn how to produce a semantically coherent paragraph\nto describe the visual content of an image. Inspired by recent successes in\nintegrating semantic topics into this task, this paper develops a plug-and-play\nhierarchical-topic-guided image paragraph generation framework, which couples a\nvisual extractor with a deep topic model to guide the learning of a language\nmodel. To capture the correlations between the image and text at multiple\nlevels of abstraction and learn the semantic topics from images, we design a\nvariational inference network to build the mapping from image features to\ntextual captions. To guide the paragraph generation, the learned hierarchical\ntopics and visual features are integrated into the language model, including\nLong Short-Term Memory (LSTM) and Transformer, and jointly optimized.\nExperiments on public dataset demonstrate that the proposed models, which are\ncompetitive with many state-of-the-art approaches in terms of standard\nevaluation metrics, can be used to both distill interpretable multi-layer\ntopics and generate diverse and coherent captions.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:55:39 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Guo", "Dandan", ""], ["Lu", "Ruiying", ""], ["Chen", "Bo", ""], ["Zeng", "Zequn", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2105.04154", "submitter": "Luca Schmidtke", "authors": "Luca Schmidtke, Athanasios Vlontzos, Simon Ellershaw, Anna Lukens,\n  Tomoki Arichi, Bernhard Kainz", "title": "Unsupervised Human Pose Estimation through Transforming Shape Templates", "comments": "CVPR 2021 (poster). Project page: https://infantmotion.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is a major computer vision problem with applications\nranging from augmented reality and video capture to surveillance and movement\ntracking. In the medical context, the latter may be an important biomarker for\nneurological impairments in infants. Whilst many methods exist, their\napplication has been limited by the need for well annotated large datasets and\nthe inability to generalize to humans of different shapes and body\ncompositions, e.g. children and infants. In this paper we present a novel\nmethod for learning pose estimators for human adults and infants in an\nunsupervised fashion. We approach this as a learnable template matching problem\nfacilitated by deep feature extractors. Human-interpretable landmarks are\nestimated by transforming a template consisting of predefined body parts that\nare characterized by 2D Gaussian distributions. Enforcing a connectivity prior\nguides our model to meaningful human shape representations. We demonstrate the\neffectiveness of our approach on two different datasets including adults and\ninfants.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:15:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Schmidtke", "Luca", ""], ["Vlontzos", "Athanasios", ""], ["Ellershaw", "Simon", ""], ["Lukens", "Anna", ""], ["Arichi", "Tomoki", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2105.04165", "submitter": "Pan Lu", "authors": "Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan\n  Liang, Song-Chun Zhu", "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language\n  and Symbolic Reasoning", "comments": "Accepted to ACL 2021, 13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geometry problem solving has attracted much attention in the NLP community\nrecently. The task is challenging as it requires abstract problem understanding\nand symbolic reasoning with axiomatic knowledge. However, current datasets are\neither small in scale or not publicly available. Thus, we construct a new\nlarge-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with\ndense annotation in formal language. We further propose a novel geometry\nsolving approach with formal language and symbolic reasoning, called\nInterpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the\nproblem text and diagram into formal language automatically via rule-based text\nparsing and neural object detecting, respectively. Unlike implicit learning in\nexisting methods, Inter-GPS incorporates theorem knowledge as conditional rules\nand performs symbolic reasoning step by step. Also, a theorem predictor is\ndesigned to infer the theorem application sequence fed to the symbolic solver\nfor the more efficient and reasonable searching path. Extensive experiments on\nthe Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves\nsignificant improvements over existing methods. The project with code and data\nis available at https://lupantech.github.io/inter-gps.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:46:55 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:28:02 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 23:22:27 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lu", "Pan", ""], ["Gong", "Ran", ""], ["Jiang", "Shibiao", ""], ["Qiu", "Liang", ""], ["Huang", "Siyuan", ""], ["Liang", "Xiaodan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2105.04169", "submitter": "Juncong Fei", "authors": "Juncong Fei, Kunyu Peng, Philipp Heidenreich, Frank Bieder and\n  Christoph Stiller", "title": "PillarSegNet: Pillar-based Semantic Grid Map Estimation using Sparse\n  LiDAR Data", "comments": "Accepted to present in the 2021 IEEE Intelligent Vehicles Symposium\n  (IV21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic understanding of the surrounding environment is essential for\nautomated vehicles. The recent publication of the SemanticKITTI dataset\nstimulates the research on semantic segmentation of LiDAR point clouds in urban\nscenarios. While most existing approaches predict sparse pointwise semantic\nclasses for the sparse input LiDAR scan, we propose PillarSegNet to be able to\noutput a dense semantic grid map. In contrast to a previously proposed grid map\nmethod, PillarSegNet uses PointNet to learn features directly from the 3D point\ncloud and then conducts 2D semantic segmentation in the top view. To train and\nevaluate our approach, we use both sparse and dense ground truth, where the\ndense ground truth is obtained from multiple superimposed scans. Experimental\nresults on the SemanticKITTI dataset show that PillarSegNet achieves a\nperformance gain of about 10% mIoU over the state-of-the-art grid map method.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:03:11 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 21:24:22 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Fei", "Juncong", ""], ["Peng", "Kunyu", ""], ["Heidenreich", "Philipp", ""], ["Bieder", "Frank", ""], ["Stiller", "Christoph", ""]]}, {"id": "2105.04181", "submitter": "Mengqi Xue", "authors": "Mengqi Xue, Jie Song, Xinchao Wang, Ying Chen, Xingen Wang, Mingli\n  Song", "title": "KDExplainer: A Task-oriented Attention Model for Explaining Knowledge\n  Distillation", "comments": "7 pages, 4 figures, accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) has recently emerged as an efficacious scheme for\nlearning compact deep neural networks (DNNs). Despite the promising results\nachieved, the rationale that interprets the behavior of KD has yet remained\nlargely understudied. In this paper, we introduce a novel task-oriented\nattention model, termed as KDExplainer, to shed light on the working mechanism\nunderlying the vanilla KD. At the heart of KDExplainer is a Hierarchical\nMixture of Experts (HME), in which a multi-class classification is reformulated\nas a multi-task binary one. Through distilling knowledge from a free-form\npre-trained DNN to KDExplainer, we observe that KD implicitly modulates the\nknowledge conflicts between different subtasks, and in reality has much more to\noffer than label smoothing. Based on such findings, we further introduce a\nportable tool, dubbed as virtual attention module (VAM), that can be seamlessly\nintegrated with various DNNs to enhance their performance under KD.\nExperimental results demonstrate that with a negligible additional cost,\nstudent models equipped with VAM consistently outperform their non-VAM\ncounterparts across different benchmarks. Furthermore, when combined with other\nKD methods, VAM remains competent in promoting results, even though it is only\nmotivated by vanilla KD. The code is available at\nhttps://github.com/zju-vipa/KDExplainer.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:15:26 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 11:54:17 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xue", "Mengqi", ""], ["Song", "Jie", ""], ["Wang", "Xinchao", ""], ["Chen", "Ying", ""], ["Wang", "Xingen", ""], ["Song", "Mingli", ""]]}, {"id": "2105.04194", "submitter": "Ayush Bhandari", "authors": "Matthias Beckmann, Ayush Bhandari and Felix Krahmer", "title": "The Modulo Radon Transform: Theory, Algorithms and Applications", "comments": "32 pages, submitted for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, experiments have been reported where researchers were able to\nperform high dynamic range (HDR) tomography in a heuristic fashion, by fusing\nmultiple tomographic projections. This approach to HDR tomography has been\ninspired by HDR photography and inherits the same disadvantages. Taking a\ncomputational imaging approach to the HDR tomography problem, we here suggest a\nnew model based on the Modulo Radon Transform (MRT), which we rigorously\nintroduce and analyze. By harnessing a joint design between hardware and\nalgorithms, we present a single-shot HDR tomography approach, which to our\nknowledge, is the only approach that is backed by mathematical guarantees.\n  On the hardware front, instead of recording the Radon Transform projections\nthat my potentially saturate, we propose to measure modulo values of the same.\nThis ensures that the HDR measurements are folded into a lower dynamic range.\nOn the algorithmic front, our recovery algorithms reconstruct the HDR images\nfrom folded measurements. Beyond mathematical aspects such as injectivity and\ninversion of the MRT for different scenarios including band-limited and\napproximately compactly supported images, we also provide a first\nproof-of-concept demonstration. To do so, we implement MRT by experimentally\nfolding tomographic measurements available as an open source data set using our\ncustom designed modulo hardware. Our reconstruction clearly shows the\nadvantages of our approach for experimental data. In this way, our MRT based\nsolution paves a path for HDR acquisition in a number of related imaging\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:38:48 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Beckmann", "Matthias", ""], ["Bhandari", "Ayush", ""], ["Krahmer", "Felix", ""]]}, {"id": "2105.04206", "submitter": "Chien-Yao Wang", "authors": "Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao", "title": "You Only Learn One Representation: Unified Network for Multiple Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People ``understand'' the world via vision, hearing, tactile, and also the\npast experience. Human experience can be learned through normal learning (we\ncall it explicit knowledge), or subconsciously (we call it implicit knowledge).\nThese experiences learned through normal learning or subconsciously will be\nencoded and stored in the brain. Using these abundant experience as a huge\ndatabase, human beings can effectively process data, even they were unseen\nbeforehand. In this paper, we propose a unified network to encode implicit\nknowledge and explicit knowledge together, just like the human brain can learn\nknowledge from normal learning as well as subconsciousness learning. The\nunified network can generate a unified representation to simultaneously serve\nvarious tasks. We can perform kernel space alignment, prediction refinement,\nand multi-task learning in a convolutional neural network. The results\ndemonstrate that when implicit knowledge is introduced into the neural network,\nit benefits the performance of all tasks. We further analyze the implicit\nrepresentation learnt from the proposed unified network, and it shows great\ncapability on catching the physical meaning of different tasks. The source code\nof this work is at : https://github.com/WongKinYiu/yolor.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:03:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Chien-Yao", ""], ["Yeh", "I-Hau", ""], ["Liao", "Hong-Yuan Mark", ""]]}, {"id": "2105.04208", "submitter": "Haichao Shi", "authors": "Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Xinchu Shi", "title": "Action Shuffling for Weakly Supervised Temporal Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised action localization is a challenging task with extensive\napplications, which aims to identify actions and the corresponding temporal\nintervals with only video-level annotations available. This paper analyzes the\norder-sensitive and location-insensitive properties of actions, and embodies\nthem into a self-augmented learning framework to improve the weakly supervised\naction localization performance. To be specific, we propose a novel two-branch\nnetwork architecture with intra/inter-action shuffling, referred to as\nActShufNet. The intra-action shuffling branch lays out a self-supervised order\nprediction task to augment the video representation with inner-video relevance,\nwhereas the inter-action shuffling branch imposes a reorganizing strategy on\nthe existing action contents to augment the training set without resorting to\nany external resources. Furthermore, the global-local adversarial training is\npresented to enhance the model's robustness to irrelevant noises. Extensive\nexperiments are conducted on three benchmark datasets, and the results clearly\ndemonstrate the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:05:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Xiao-Yu", ""], ["Shi", "Haichao", ""], ["Li", "Changsheng", ""], ["Shi", "Xinchu", ""]]}, {"id": "2105.04213", "submitter": "Qinyao Chang", "authors": "Qinyao Chang, Shiping Zhu, Lanyun Zhu", "title": "Temporal-Spatial Feature Pyramid for Video Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a 3D fully convolutional encoder-decoder\narchitecture for video saliency detection, which combines scale, space and time\ninformation for video saliency modeling. The encoder extracts multi-scale\ntemporal-spatial features from the input continuous video frames, and then\nconstructs temporal-spatial feature pyramid through temporal-spatial\nconvolution and top-down feature integration. The decoder performs hierarchical\ndecoding of temporal-spatial features from different scales, and finally\nproduces a saliency map from the integration of multiple video frames. Our\nmodel is simple yet effective, and can run in real time. We perform abundant\nexperiments, and the results indicate that the well-designed structure can\nimprove the precision of video saliency detection significantly. Experimental\nresults on three purely visual video saliency benchmarks and six audio-video\nsaliency benchmarks demonstrate that our method achieves state-of-theart\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:14:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chang", "Qinyao", ""], ["Zhu", "Shiping", ""], ["Zhu", "Lanyun", ""]]}, {"id": "2105.04216", "submitter": "Vignesh Ramanathan", "authors": "Lakshmi Annamalai, Vignesh Ramanathan, Chetan Singh Thakur", "title": "Event-LSTM: An Unsupervised and Asynchronous Learning-based\n  Representation for Event-based Data", "comments": "7 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event cameras are activity-driven bio-inspired vision sensors, thereby\nresulting in advantages such as sparsity,high temporal resolution, low latency,\nand power consumption. Given the different sensing modality of event camera and\nhigh quality of conventional vision paradigm, event processing is predominantly\nsolved by transforming the sparse and asynchronous events into 2D grid and\nsubsequently applying standard vision pipelines. Despite the promising results\ndisplayed by supervised learning approaches in 2D grid generation, these\napproaches treat the task in supervised manner. Labeled task specific ground\ntruth event data is challenging to acquire. To overcome this limitation, we\npropose Event-LSTM, an unsupervised Auto-Encoder architecture made up of LSTM\nlayers as a promising alternative to learn 2D grid representation from event\nsequence. Compared to competing supervised approaches, ours is a task-agnostic\napproach ideally suited for the event domain, where task specific labeled data\nis scarce. We also tailor the proposed solution to exploit asynchronous nature\nof event stream, which gives it desirable charateristics such as speed\ninvariant and energy-efficient 2D grid generation. Besides, we also push\nstate-of-the-art event de-noising forward by introducing memory into the\nde-noising process. Evaluations on activity recognition and gesture recognition\ndemonstrate that our approach yields improvement over state-of-the-art\napproaches, while providing the flexibilty to learn from unlabelled data.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:18:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Annamalai", "Lakshmi", ""], ["Ramanathan", "Vignesh", ""], ["Thakur", "Chetan Singh", ""]]}, {"id": "2105.04232", "submitter": "Andreas B{\\ae}rentzen", "authors": "Martin O. Elingaard, Niels Aage, J. Andreas B{\\ae}rentzen, Ole Sigmund", "title": "De-homogenization using Convolutional Neural Networks", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning-based de-homogenization method for\nstructural compliance minimization. By using a convolutional neural network to\nparameterize the mapping from a set of lamination parameters on a coarse mesh\nto a one-scale design on a fine mesh, we avoid solving the least square\nproblems associated with traditional de-homogenization approaches and save time\ncorrespondingly. To train the neural network, a two-step custom loss function\nhas been developed which ensures a periodic output field that follows the local\nlamination orientations. A key feature of the proposed method is that the\ntraining is carried out without any use of or reference to the underlying\nstructural optimization problem, which renders the proposed method robust and\ninsensitive wrt. domain size, boundary conditions, and loading. A\npost-processing procedure utilizing a distance transform on the output field\nskeleton is used to project the desired lamination widths onto the output field\nwhile ensuring a predefined minimum length-scale and volume fraction. To\ndemonstrate that the deep learning approach has excellent generalization\nproperties, numerical examples are shown for several different load and\nboundary conditions. For an appropriate choice of parameters, the\nde-homogenized designs perform within $7-25\\%$ of the homogenization-based\nsolution at a fraction of the computational cost. With several options for\nfurther improvements, the scheme may provide the basis for future interactive\nhigh-resolution topology optimization.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:50:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Elingaard", "Martin O.", ""], ["Aage", "Niels", ""], ["B\u00e6rentzen", "J. Andreas", ""], ["Sigmund", "Ole", ""]]}, {"id": "2105.04242", "submitter": "Mikolaj Wieczorek", "authors": "Barbara Rychalska, Mikolaj Wieczorek, Jacek Dabrowski", "title": "T-EMDE: Sketching-based global similarity for cross-modal retrieval", "comments": "10 pages,5 figures, 4 tables, 1 code snippet", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in cross-modal retrieval is to find similarities between\nobjects represented with different modalities, such as image and text. However,\neach modality embeddings stem from non-related feature spaces, which causes the\nnotorious 'heterogeneity gap'. Currently, many cross-modal systems try to\nbridge the gap with self-attention. However, self-attention has been widely\ncriticized for its quadratic complexity, which prevents many real-life\napplications. In response to this, we propose T-EMDE - a neural density\nestimator inspired by the recently introduced Efficient Manifold Density\nEstimator (EMDE) from the area of recommender systems. EMDE operates on\nsketches - representations especially suitable for multimodal operations.\nHowever, EMDE is non-differentiable and ingests precomputed, static embeddings.\nWith T-EMDE we introduce a trainable version of EMDE which allows full\nend-to-end training. In contrast to self-attention, the complexity of our\nsolution is linear to the number of tokens/segments. As such, T-EMDE is a\ndrop-in replacement for the self-attention module, with beneficial influence on\nboth speed and metric performance in cross-modal settings. It facilitates\ncommunication between modalities, as each global text/image representation is\nexpressed with a standardized sketch histogram which represents the same\nmanifold structures irrespective of the underlying modality. We evaluate T-EMDE\nby introducing it into two recent cross-modal SOTA models and achieving new\nstate-of-the-art results on multiple datasets and decreasing model latency by\nup to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:14:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rychalska", "Barbara", ""], ["Wieczorek", "Mikolaj", ""], ["Dabrowski", "Jacek", ""]]}, {"id": "2105.04244", "submitter": "Timm Haucke", "authors": "Timm Haucke, Hjalmar S. K\\\"uhl, Jacqueline Hoyer, Volker Steinhage", "title": "Overcoming the Distance Estimation Bottleneck in Camera Trap Distance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biodiversity crisis is still accelerating. Estimating animal abundance is of\ncritical importance to assess, for example, the consequences of land-use change\nand invasive species on species composition, or the effectiveness of\nconservation interventions. Camera trap distance sampling (CTDS) is a recently\ndeveloped monitoring method providing reliable estimates of wildlife population\ndensity and abundance. However, in current applications of CTDS, the required\ncamera-to-animal distance measurements are derived by laborious, manual and\nsubjective estimation methods. To overcome this distance estimation bottleneck\nin CTDS, this study proposes a completely automatized workflow utilizing\nstate-of-the-art methods of image processing and pattern recognition.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:17:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Haucke", "Timm", ""], ["K\u00fchl", "Hjalmar S.", ""], ["Hoyer", "Jacqueline", ""], ["Steinhage", "Volker", ""]]}, {"id": "2105.04246", "submitter": "Marios Fournarakis", "authors": "Marios Fournarakis, Markus Nagel", "title": "In-Hindsight Quantization Range Estimation for Quantized Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantization techniques applied to the inference of deep neural networks have\nenabled fast and efficient execution on resource-constraint devices. The\nsuccess of quantization during inference has motivated the academic community\nto explore fully quantized training, i.e. quantizing back-propagation as well.\nHowever, effective gradient quantization is still an open problem. Gradients\nare unbounded and their distribution changes significantly during training,\nwhich leads to the need for dynamic quantization. As we show, dynamic\nquantization can lead to significant memory overhead and additional data\ntraffic slowing down training. We propose a simple alternative to dynamic\nquantization, in-hindsight range estimation, that uses the quantization ranges\nestimated on previous iterations to quantize the present. Our approach enables\nfast static quantization of gradients and activations while requiring only\nminimal hardware support from the neural network accelerator to keep track of\noutput statistics in an online fashion. It is intended as a drop-in replacement\nfor estimating quantization ranges and can be used in conjunction with other\nadvances in quantized training. We compare our method to existing methods for\nrange estimation from the quantized training literature and demonstrate its\neffectiveness with a range of architectures, including MobileNetV2, on image\nclassification benchmarks (Tiny ImageNet & ImageNet).\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:25:28 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fournarakis", "Marios", ""], ["Nagel", "Markus", ""]]}, {"id": "2105.04269", "submitter": "Marvin Lerousseau", "authors": "Marvin Lerousseau and Marion Classe and Enzo Battistella and Th\\'eo\n  Estienne and Th\\'eophraste Henry and Amaury Leroy and Roger Sun and Maria\n  Vakalopoulou and Jean-Yves Scoazec and Eric Deutsch and Nikos Paragios", "title": "Weakly supervised pan-cancer segmentation tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The vast majority of semantic segmentation approaches rely on pixel-level\nannotations that are tedious and time consuming to obtain and suffer from\nsignificant inter and intra-expert variability. To address these issues, recent\napproaches have leveraged categorical annotations at the slide-level, that in\ngeneral suffer from robustness and generalization. In this paper, we propose a\nnovel weakly supervised multi-instance learning approach that deciphers\nquantitative slide-level annotations which are fast to obtain and regularly\npresent in clinical routine. The extreme potentials of the proposed approach\nare demonstrated for tumor segmentation of solid cancer subtypes. The proposed\napproach achieves superior performance in out-of-distribution, out-of-location,\nand out-of-domain testing sets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:11:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lerousseau", "Marvin", ""], ["Classe", "Marion", ""], ["Battistella", "Enzo", ""], ["Estienne", "Th\u00e9o", ""], ["Henry", "Th\u00e9ophraste", ""], ["Leroy", "Amaury", ""], ["Sun", "Roger", ""], ["Vakalopoulou", "Maria", ""], ["Scoazec", "Jean-Yves", ""], ["Deutsch", "Eric", ""], ["Paragios", "Nikos", ""]]}, {"id": "2105.04281", "submitter": "Ye Du", "authors": "Ye Du, Zehua Fu, Qingjie Liu, Yunhong Wang", "title": "Visual Grounding with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a transformer based approach for visual grounding.\nUnlike previous proposal-and-rank frameworks that rely heavily on pretrained\nobject detectors or proposal-free frameworks that upgrade an off-the-shelf\none-stage detector by fusing textual embeddings, our approach is built on top\nof a transformer encoder-decoder and is independent of any pretrained detectors\nor word embedding models. Termed VGTR -- Visual Grounding with TRansformers,\nour approach is designed to learn semantic-discriminative visual features under\nthe guidance of the textual description without harming their location ability.\nThis information flow enables our VGTR to have a strong capability in capturing\ncontext-level semantics of both vision and language modalities, rendering us to\naggregate accurate visual clues implied by the description to locate the\ninterested object instance. Experiments show that our method outperforms\nstate-of-the-art proposal-free approaches by a considerable margin on five\nbenchmarks while maintaining fast inference speed.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:46:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Du", "Ye", ""], ["Fu", "Zehua", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "2105.04286", "submitter": "Ruijie Yan", "authors": "Ruijie Yan, Liangrui Peng, Shanyu Xiao, Gang Yao", "title": "Primitive Representation Learning for Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scene text recognition is a challenging task due to diverse variations of\ntext instances in natural scene images. Conventional methods based on\nCNN-RNN-CTC or encoder-decoder with attention mechanism may not fully\ninvestigate stable and efficient feature representations for multi-oriented\nscene texts. In this paper, we propose a primitive representation learning\nmethod that aims to exploit intrinsic representations of scene text images. We\nmodel elements in feature maps as the nodes of an undirected graph. A pooling\naggregator and a weighted aggregator are proposed to learn primitive\nrepresentations, which are transformed into high-level visual text\nrepresentations by graph convolutional networks. A Primitive REpresentation\nlearning Network (PREN) is constructed to use the visual text representations\nfor parallel decoding. Furthermore, by integrating visual text representations\ninto an encoder-decoder model with the 2D attention mechanism, we propose a\nframework called PREN2D to alleviate the misalignment problem in\nattention-based methods. Experimental results on both English and Chinese scene\ntext recognition tasks demonstrate that PREN keeps a balance between accuracy\nand efficiency, while PREN2D achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:54:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yan", "Ruijie", ""], ["Peng", "Liangrui", ""], ["Xiao", "Shanyu", ""], ["Yao", "Gang", ""]]}, {"id": "2105.04302", "submitter": "Hongyong Wang", "authors": "Hongyong Wang, Xinjian Zhang, Su Yang, Weishan Zhang", "title": "Video Anomaly Detection By The Duality Of Normality-Granted Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection is a challenging task because of diverse abnormal\nevents. To this task, methods based on reconstruction and prediction are wildly\nused in recent works, which are built on the assumption that learning on normal\ndata, anomalies cannot be reconstructed or predicated as good as normal\npatterns, namely the anomaly result with more errors. In this paper, we propose\nto discriminate anomalies from normal ones by the duality of normality-granted\noptical flow, which is conducive to predict normal frames but adverse to\nabnormal frames. The normality-granted optical flow is predicted from a single\nframe, to keep the motion knowledge focused on normal patterns. Meanwhile, We\nextend the appearance-motion correspondence scheme from frame reconstruction to\nprediction, which not only helps to learn the knowledge about object\nappearances and correlated motion, but also meets the fact that motion is the\ntransformation between appearances. We also introduce a margin loss to enhance\nthe learning of frame prediction. Experiments on standard benchmark datasets\ndemonstrate the impressive performance of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:25:00 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Hongyong", ""], ["Zhang", "Xinjian", ""], ["Yang", "Su", ""], ["Zhang", "Weishan", ""]]}, {"id": "2105.04313", "submitter": "Marius Lehne", "authors": "Shachar Klaiman and Marius Lehne", "title": "DocReader: Bounding-Box Free Training of a Document Information\n  Extraction Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction from documents is a ubiquitous first step in many\nbusiness applications. During this step, the entries of various fields must\nfirst be read from the images of scanned documents before being further\nprocessed and inserted into the corresponding databases. While many different\nmethods have been developed over the past years in order to automate the above\nextraction step, they all share the requirement of bounding-box or text segment\nannotations of their training documents. In this work we present DocReader, an\nend-to-end neural-network-based information extraction solution which can be\ntrained using solely the images and the target values that need to be read. The\nDocReader can thus leverage existing historical extraction data, completely\neliminating the need for any additional annotations beyond what is naturally\navailable in existing human-operated service centres. We demonstrate that the\nDocReader can reach and surpass other methods which require bounding-boxes for\ntraining, as well as provide a clear path for continual learning during its\ndeployment in production.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:48:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Klaiman", "Shachar", ""], ["Lehne", "Marius", ""]]}, {"id": "2105.04322", "submitter": "En Yu", "authors": "En Yu, Zhuoling Li, Shoudong Han and Hongwei Wang", "title": "RelationTrack: Relation-aware Multiple Object Tracking with Decoupled\n  Representation", "comments": "11 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing online multiple object tracking (MOT) algorithms often consist of\ntwo subtasks, detection and re-identification (ReID). In order to enhance the\ninference speed and reduce the complexity, current methods commonly integrate\nthese double subtasks into a unified framework. Nevertheless, detection and\nReID demand diverse features. This issue would result in an optimization\ncontradiction during the training procedure. With the target of alleviating\nthis contradiction, we devise a module named Global Context Disentangling (GCD)\nthat decouples the learned representation into detection-specific and\nReID-specific embeddings. As such, this module provides an implicit manner to\nbalance the different requirements of these two subtasks. Moreover, we observe\nthat preceding MOT methods typically leverage local information to associate\nthe detected targets and neglect to consider the global semantic relation. To\nresolve this restriction, we develop a module, referred to as Guided\nTransformer Encoder (GTE), by combining the powerful reasoning ability of\nTransformer encoder and deformable attention. Unlike previous works, GTE avoids\nanalyzing all the pixels and only attends to capture the relation between query\nnodes and a few self-adaptively selected key samples. Therefore, it is\ncomputationally efficient. Extensive experiments have been conducted on the\nMOT16, MOT17 and MOT20 benchmarks to demonstrate the superiority of the\nproposed MOT framework, namely RelationTrack. The experimental results indicate\nthat RelationTrack has surpassed preceding methods significantly and\nestablished a new state-of-the-art performance, e.g., IDF1 of 70.5% and MOTA of\n67.2% on MOT20.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:00:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "En", ""], ["Li", "Zhuoling", ""], ["Han", "Shoudong", ""], ["Wang", "Hongwei", ""]]}, {"id": "2105.04328", "submitter": "Indrajit Kurmi", "authors": "D.C. Schedl, I. Kurmi, and O. Bimber", "title": "An Autonomous Drone for Search and Rescue in Forests using Airborne\n  Optical Sectioning", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Drones will play an essential role in human-machine teaming in future search\nand rescue (SAR) missions. We present a first prototype that finds people fully\nautonomously in densely occluded forests. In the course of 17 field experiments\nconducted over various forest types and under different flying conditions, our\ndrone found 38 out of 42 hidden persons; average precision was 86% for\npredefined flight paths, while adaptive path planning (where potential findings\nare double-checked) increased confidence by 15%. Image processing,\nclassification, and dynamic flight-path adaptation are computed onboard in\nreal-time and while flying. Our finding that deep-learning-based person\nclassification is unaffected by sparse and error-prone sampling within\none-dimensional synthetic apertures allows flights to be shortened and reduces\nrecording requirements to one-tenth of the number of images needed for sampling\nusing two-dimensional synthetic apertures. The goal of our adaptive path\nplanning is to find people as reliably and quickly as possible, which is\nessential in time-critical applications, such as SAR. Our drone enables SAR\noperations in remote areas without stable network coverage, as it transmits to\nthe rescue team only classification results that indicate detections and can\nthus operate with intermittent minimal-bandwidth connections (e.g., by\nsatellite). Once received, these results can be visually enhanced for\ninterpretation on remote mobile devices.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:05:22 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Schedl", "D. C.", ""], ["Kurmi", "I.", ""], ["Bimber", "O.", ""]]}, {"id": "2105.04349", "submitter": "Neel Dey", "authors": "Neel Dey, Mengwei Ren, Adrian V. Dalca, Guido Gerig", "title": "Generative Adversarial Registration for Improved Conditional Deformable\n  Templates", "comments": "24 pages, 15 figures. Code is available at\n  https://github.com/neel-dey/Atlas-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable templates are essential to large-scale medical image registration,\nsegmentation, and population analysis. Current conventional and deep\nnetwork-based methods for template construction use only regularized\nregistration objectives and often yield templates with blurry and/or\nanatomically implausible appearance, confounding downstream biomedical\ninterpretation. We reformulate deformable registration and conditional template\nestimation as an adversarial game wherein we encourage realism in the moved\ntemplates with a generative adversarial registration framework conditioned on\nflexible image covariates. The resulting templates exhibit significant gain in\nspecificity to attributes such as age and disease, better fit underlying\ngroup-wise spatiotemporal trends, and achieve improved sharpness and\ncentrality. These improvements enable more accurate population modeling with\ndiverse covariates for standardized downstream analyses and easier anatomical\ndelineation for structures of interest.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:06:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dey", "Neel", ""], ["Ren", "Mengwei", ""], ["Dalca", "Adrian V.", ""], ["Gerig", "Guido", ""]]}, {"id": "2105.04354", "submitter": "Xinglin Pan", "authors": "Xinglin Pan, Jing Xu, Yu Pan, liangjian Wen, WenXiang Lin, Kun Bai,\n  Zenglin Xu", "title": "AFINet: Attentive Feature Integration Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved tremendous success in a\nnumber of learning tasks including image classification. Recent advanced models\nin CNNs, such as ResNets, mainly focus on the skip connection to avoid gradient\nvanishing. DenseNet designs suggest creating additional bypasses to transfer\nfeatures as an alternative strategy in network design. In this paper, we design\nAttentive Feature Integration (AFI) modules, which are widely applicable to\nmost recent network architectures, leading to new architectures named AFI-Nets.\nAFI-Nets explicitly model the correlations among different levels of features\nand selectively transfer features with a little overhead.AFI-ResNet-152 obtains\na 1.24% relative improvement on the ImageNet dataset while decreases the FLOPs\nby about 10% and the number of parameters by about 9.2% compared to ResNet-152.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:40:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pan", "Xinglin", ""], ["Xu", "Jing", ""], ["Pan", "Yu", ""], ["Wen", "liangjian", ""], ["Lin", "WenXiang", ""], ["Bai", "Kun", ""], ["Xu", "Zenglin", ""]]}, {"id": "2105.04356", "submitter": "Hazrat Ali", "authors": "Muhammad Shakaib Iqbal, Hazrat Ali, Son N. Tran, Talha Iqbal", "title": "Coconut trees detection and segmentation in aerial imagery using mask\n  region-based convolution neural network", "comments": "Published in IET Computer Vision, 09 April 2021", "journal-ref": null, "doi": "10.1049/cvi2.12028", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Food resources face severe damages under extraordinary situations of\ncatastrophes such as earthquakes, cyclones, and tsunamis. Under such scenarios,\nspeedy assessment of food resources from agricultural land is critical as it\nsupports aid activity in the disaster hit areas. In this article, a deep\nlearning approach is presented for the detection and segmentation of coconut\ntress in aerial imagery provided through the AI competition organized by the\nWorld Bank in collaboration with OpenAerialMap and WeRobotics. Maked\nRegion-based Convolutional Neural Network approach was used identification and\nsegmentation of coconut trees. For the segmentation task, Mask R-CNN model with\nResNet50 and ResNet1010 based architectures was used. Several experiments with\ndifferent configuration parameters were performed and the best configuration\nfor the detection of coconut trees with more than 90% confidence factor was\nreported. For the purpose of evaluation, Microsoft COCO dataset evaluation\nmetric namely mean average precision (mAP) was used. An overall 91% mean\naverage precision for coconut trees detection was achieved.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:42:19 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Iqbal", "Muhammad Shakaib", ""], ["Ali", "Hazrat", ""], ["Tran", "Son N.", ""], ["Iqbal", "Talha", ""]]}, {"id": "2105.04383", "submitter": "Franz Wotawa", "authors": "Franz Wotawa and Lorenz Klampfl and Ledio Jahaj", "title": "A framework for the automation of testing computer vision systems", "comments": "4 pages, Submission version, Accepted at the 2nd ACM/IEEE\n  International Conference on Automation of Software Test AST 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision systems, i.e., systems that allow to detect and track objects in\nimages, have gained substantial importance over the past decades. They are used\nin quality assurance applications, e.g., for finding surface defects in\nproducts during manufacturing, surveillance, but also automated driving,\nrequiring reliable behavior. Interestingly, there is only little work on\nquality assurance and especially testing of vision systems in general. In this\npaper, we contribute to the area of testing vision software, and present a\nframework for the automated generation of tests for systems based on vision and\nimage recognition. The framework makes use of existing libraries allowing to\nmodify original images and to obtain similarities between the original and\nmodified images. We show how such a framework can be used for testing a\nparticular industrial application on identifying defects on riblet surfaces and\npresent preliminary results from the image classification domain.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:02:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wotawa", "Franz", ""], ["Klampfl", "Lorenz", ""], ["Jahaj", "Ledio", ""]]}, {"id": "2105.04402", "submitter": "Yihao Luo", "authors": "Yihao Luo and Ailing Yang and Fupeng Sun and Huafei Sun", "title": "AWCD: An Efficient Point Cloud Processing Approach via Wasserstein\n  Curvature", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the adaptive Wasserstein curvature denoising\n(AWCD), an original processing approach for point cloud data. By collecting\ncurvatures information from Wasserstein distance, AWCD consider more precise\nstructures of data and preserves stability and effectiveness even for data with\nnoise in high density. This paper contains some theoretical analysis about the\nWasserstein curvature and the complete algorithm of AWCD. In addition, we\ndesign digital experiments to show the denoising effect of AWCD. According to\ncomparison results, we present the advantages of AWCD against traditional\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:33:05 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 08:04:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Luo", "Yihao", ""], ["Yang", "Ailing", ""], ["Sun", "Fupeng", ""], ["Sun", "Huafei", ""]]}, {"id": "2105.04430", "submitter": "Wadii Boulila Prof.", "authors": "Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa, Nesrine\n  Atitallah, Henda Ben Gh\\'ezala", "title": "An Enhanced Randomly Initialized Convolutional Neural Network for\n  Columnar Cactus Recognition in Unmanned Aerial Vehicle Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) have made a great performance\nfor remote sensing image classification. Plant recognition using CNNs is one of\nthe active deep learning research topics due to its added-value in different\nrelated fields, especially environmental conservation and natural areas\npreservation. Automatic recognition of plants in protected areas helps in the\nsurveillance process of these zones and ensures the sustainability of their\necosystems. In this work, we propose an Enhanced Randomly Initialized\nConvolutional Neural Network (ERI-CNN) for the recognition of columnar cactus,\nwhich is an endemic plant that exists in the Tehuac\\'an-Cuicatl\\'an Valley in\nsoutheastern Mexico. We used a public dataset created by a group of researchers\nthat consists of more than 20000 remote sensing images. The experimental\nresults confirm the effectiveness of the proposed model compared to other\nmodels reported in the literature like InceptionV3 and the modified LeNet-5\nCNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall,\n97.5% as f1-score, and 0.056 loss.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:41:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Atitallah", "Safa Ben", ""], ["Driss", "Maha", ""], ["Boulila", "Wadii", ""], ["Koubaa", "Anis", ""], ["Atitallah", "Nesrine", ""], ["Gh\u00e9zala", "Henda Ben", ""]]}, {"id": "2105.04431", "submitter": "Yuchi Liu", "authors": "Yuchi Liu, Hailin Shi, Hang Du, Rui Zhu, Jun Wang, Liang Zheng, and\n  Tao Mei", "title": "Boosting Semi-Supervised Face Recognition with Noise Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep face recognition benefits significantly from large-scale\ntraining data, a current bottleneck is the labelling cost. A feasible solution\nto this problem is semi-supervised learning, exploiting a small portion of\nlabelled data and large amounts of unlabelled data. The major challenge,\nhowever, is the accumulated label errors through auto-labelling, compromising\nthe training. This paper presents an effective solution to semi-supervised face\nrecognition that is robust to the label noise aroused by the auto-labelling.\nSpecifically, we introduce a multi-agent method, named GroupNet (GN), to endow\nour solution with the ability to identify the wrongly labelled samples and\npreserve the clean samples. We show that GN alone achieves the leading accuracy\nin traditional supervised face recognition even when the noisy labels take over\n50\\% of the training data. Further, we develop a semi-supervised face\nrecognition solution, named Noise Robust Learning-Labelling (NRoLL), which is\nbased on the robust training ability empowered by GN. It starts with a small\namount of labelled data and consequently conducts high-confidence labelling on\na large amount of unlabelled data to boost further training. The more data is\nlabelled by NRoLL, the higher confidence is with the label in the dataset. To\nevaluate the competitiveness of our method, we run NRoLL with a rough condition\nthat only one-fifth of the labelled MSCeleb is available and the rest is used\nas unlabelled data. On a wide range of benchmarks, our method compares\nfavorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:43:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Yuchi", ""], ["Shi", "Hailin", ""], ["Du", "Hang", ""], ["Zhu", "Rui", ""], ["Wang", "Jun", ""], ["Zheng", "Liang", ""], ["Mei", "Tao", ""]]}, {"id": "2105.04447", "submitter": "Bing Li", "authors": "Bing Li, Cheng Zheng, Silvio Giancola, Bernard Ghanem", "title": "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel scene flow estimation approach to capture and infer 3D\nmotions from point clouds. Estimating 3D motions for point clouds is\nchallenging, since a point cloud is unordered and its density is significantly\nnon-uniform. Such unstructured data poses difficulties in matching\ncorresponding points between point clouds, leading to inaccurate flow\nestimation. We propose a novel architecture named Sparse\nConvolution-Transformer Network (SCTN) that equips the sparse convolution with\nthe transformer. Specifically, by leveraging the sparse convolution, SCTN\ntransfers irregular point cloud into locally consistent flow features for\nestimating continuous and consistent motions within an object/local object\npart. We further propose to explicitly learn point relations using a point\ntransformer module, different from exiting methods. We show that the learned\nrelation-based contextual information is rich and helpful for matching\ncorresponding points, benefiting scene flow estimation. In addition, a novel\nloss function is proposed to adaptively encourage flow consistency according to\nfeature similarity. Extensive experiments demonstrate that our proposed\napproach achieves a new state of the art in scene flow estimation. Our approach\nachieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene\nFlow respectively, which significantly outperforms previous methods by large\nmargins.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:16:14 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 23:42:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Bing", ""], ["Zheng", "Cheng", ""], ["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2105.04459", "submitter": "Thomas Greer Iv", "authors": "Hastings Greer, Roland Kwitt, Francois-Xavier Vialard, Marc Niethammer", "title": "ICON: Learning Regular Maps Through Inverse Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning maps between data samples is fundamental. Applications range from\nrepresentation learning, image translation and generative modeling, to the\nestimation of spatial deformations. Such maps relate feature vectors, or map\nbetween feature spaces. Well-behaved maps should be regular, which can be\nimposed explicitly or may emanate from the data itself. We explore what induces\nregularity for spatial transformations, e.g., when computing image\nregistrations. Classical optimization-based models compute maps between pairs\nof samples and rely on an appropriate regularizer for well-posedness. Recent\ndeep learning approaches have attempted to avoid using such regularizers\naltogether by relying on the sample population instead. We explore if it is\npossible to obtain spatial regularity using an inverse consistency loss only\nand elucidate what explains map regularity in such a context. We find that deep\nnetworks combined with an inverse consistency loss and randomized off-grid\ninterpolation yield well behaved, approximately diffeomorphic, spatial\ntransformations. Despite the simplicity of this approach, our experiments\npresent compelling evidence, on both synthetic and real data, that regular maps\ncan be obtained without carefully tuned explicit regularizers, while achieving\ncompetitive registration performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:52:12 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 01:53:00 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 16:39:52 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Greer", "Hastings", ""], ["Kwitt", "Roland", ""], ["Vialard", "Francois-Xavier", ""], ["Niethammer", "Marc", ""]]}, {"id": "2105.04460", "submitter": "Margaret Regan", "authors": "Timothy Duff, Viktor Korotynskiy, Tomas Pajdla, Margaret H. Regan", "title": "Galois/monodromy groups for decomposing minimal problems in 3D\n  reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Galois/monodromy groups arising in computer vision applications,\nwith a view towards building more efficient polynomial solvers. The\nGalois/monodromy group allows us to decide when a given problem decomposes into\nalgebraic subproblems, and whether or not it has any symmetries. Tools from\nnumerical algebraic geometry and computational group theory allow us to apply\nthis framework to classical and novel reconstruction problems. We consider\nthree classical cases--3-point absolute pose, 5-point relative pose, and\n4-point homography estimation for calibrated cameras--where the decomposition\nand symmetries may be naturally understood in terms of the Galois/monodromy\ngroup. We then show how our framework can be applied to novel problems from\nabsolute and relative pose estimation. For instance, we discover new symmetries\nfor absolute pose problems involving mixtures of point and line features. We\nalso describe a problem of estimating a pair of calibrated homographies between\nthree images. For this problem of degree 64, we can reduce the degree to 16;\nthe latter better reflecting the intrinsic difficulty of algebraically solving\nthe problem. As a byproduct, we obtain new constraints on compatible\nhomographies, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:55:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Duff", "Timothy", ""], ["Korotynskiy", "Viktor", ""], ["Pajdla", "Tomas", ""], ["Regan", "Margaret H.", ""]]}, {"id": "2105.04489", "submitter": "SouYoung Jin", "authors": "Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio\n  Feris, James Glass, Aude Oliva", "title": "Spoken Moments: Learning Joint Audio-Visual Representations from Video\n  Descriptions", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people observe events, they are able to abstract key information and\nbuild concise summaries of what is happening. These summaries include\ncontextual and semantic information describing the important high-level details\n(what, where, who and how) of the observed event and exclude background\ninformation that is deemed unimportant to the observer. With this in mind, the\ndescriptions people generate for videos of different dynamic events can greatly\nimprove our understanding of the key information of interest in each video.\nThese descriptions can be captured in captions that provide expanded attributes\nfor video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing\nus to gain new insight into what people find important or necessary to\nsummarize specific events. Existing caption datasets for video understanding\nare either small in scale or restricted to a specific domain. To address this,\nwe present the Spoken Moments (S-MiT) dataset of 500k spoken captions each\nattributed to a unique short video depicting a broad range of different events.\nWe collect our descriptions using audio recordings to ensure that they remain\nas natural and concise as possible while allowing us to scale the size of a\nlarge classification dataset. In order to utilize our proposed dataset, we\npresent a novel Adaptive Mean Margin (AMM) approach to contrastive learning and\nevaluate our models on video/caption retrieval on multiple datasets. We show\nthat our AMM approach consistently improves our results and that models trained\non our Spoken Moments dataset generalize better than those trained on other\nvideo-caption datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:30:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Monfort", "Mathew", ""], ["Jin", "SouYoung", ""], ["Liu", "Alexander", ""], ["Harwath", "David", ""], ["Feris", "Rogerio", ""], ["Glass", "James", ""], ["Oliva", "Aude", ""]]}, {"id": "2105.04508", "submitter": "Rutu Gandhi", "authors": "Rutu Gandhi and Yi Hong", "title": "MDA-Net: Multi-Dimensional Attention-Based Neural Network for 3D Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmenting an entire 3D image often has high computational complexity and\nrequires large memory consumption; by contrast, performing volumetric\nsegmentation in a slice-by-slice manner is efficient but does not fully\nleverage the 3D data. To address this challenge, we propose a multi-dimensional\nattention network (MDA-Net) to efficiently integrate slice-wise, spatial, and\nchannel-wise attention into a U-Net based network, which results in high\nsegmentation accuracy with a low computational cost. We evaluate our model on\nthe MICCAI iSeg and IBSR datasets, and the experimental results demonstrate\nconsistent improvements over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:58:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gandhi", "Rutu", ""], ["Hong", "Yi", ""]]}, {"id": "2105.04515", "submitter": "Julian D. Gilbey", "authors": "Julian D. Gilbey, Carola-Bibiane Sch\\\"onlieb", "title": "An end-to-end Optical Character Recognition approach for\n  ultra-low-resolution printed text images", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Some historical and more recent printed documents have been scanned or stored\nat very low resolutions, such as 60 dpi. Though such scans are relatively easy\nfor humans to read, they still present significant challenges for optical\ncharacter recognition (OCR) systems. The current state-of-the art is to use\nsuper-resolution to reconstruct an approximation of the original\nhigh-resolution image and to feed this into a standard OCR system. Our novel\nend-to-end method bypasses the super-resolution step and produces better OCR\nresults. This approach is inspired from our understanding of the human visual\nsystem, and builds on established neural networks for performing OCR.\n  Our experiments have shown that it is possible to perform OCR on 60 dpi\nscanned images of English text, which is a significantly lower resolution than\nthe state-of-the-art, and we achieved a mean character level accuracy (CLA) of\n99.7% and word level accuracy (WLA) of 98.9% across a set of about 1000 pages\nof 60 dpi text in a wide range of fonts. For 75 dpi images, the mean CLA was\n99.9% and the mean WLA was 99.4% on the same sample of texts. We make our code\nand data (including a set of low-resolution images with their ground truths)\npublicly available as a benchmark for future work in this field.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:08:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gilbey", "Julian D.", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2105.04522", "submitter": "Erik Englesson", "authors": "Erik Englesson, Hossein Azizpour", "title": "Generalized Jensen-Shannon Divergence Loss for Learning with Noisy\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior works have found it beneficial to combine provably noise-robust loss\nfunctions e.g., mean absolute error (MAE) with standard categorical loss\nfunction e.g. cross entropy (CE) to improve their learnability. Here, we\npropose to use Jensen-Shannon divergence as a noise-robust loss function and\nshow that it interestingly interpolate between CE and MAE with a controllable\nmixing parameter. Furthermore, we make a crucial observation that CE exhibit\nlower consistency around noisy data points. Based on this observation, we adopt\na generalized version of the Jensen-Shannon divergence for multiple\ndistributions to encourage consistency around data points. Using this loss\nfunction, we show state-of-the-art results on both synthetic (CIFAR), and\nreal-world (WebVision) noise with varying noise rates.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:19:38 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:07:03 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 08:57:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Englesson", "Erik", ""], ["Azizpour", "Hossein", ""]]}, {"id": "2105.04532", "submitter": "Omer Demirel", "authors": "Omer Burak Demirel, Burhaneddin Yaman, Logan Dowdle, Steen Moeller,\n  Luca Vizioli, Essa Yacoub, John Strupp, Cheryl A. Olman, K\\^amil U\\u{g}urbil\n  and Mehmet Ak\\c{c}akaya", "title": "Improved Simultaneous Multi-Slice Functional MRI Using Self-supervised\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional MRI (fMRI) is commonly used for interpreting neural activities\nacross the brain. Numerous accelerated fMRI techniques aim to provide improved\nspatiotemporal resolutions. Among these, simultaneous multi-slice (SMS) imaging\nhas emerged as a powerful strategy, becoming a part of large-scale studies,\nsuch as the Human Connectome Project. However, when SMS imaging is combined\nwith in-plane acceleration for higher acceleration rates, conventional SMS\nreconstruction methods may suffer from noise amplification and other artifacts.\nRecently, deep learning (DL) techniques have gained interest for improving MRI\nreconstruction. However, these methods are typically trained in a supervised\nmanner that necessitates fully-sampled reference data, which is not feasible in\nhighly-accelerated fMRI acquisitions. Self-supervised learning that does not\nrequire fully-sampled data has recently been proposed and has shown similar\nperformance to supervised learning. However, it has only been applied for\nin-plane acceleration. Furthermore the effect of DL reconstruction on\nsubsequent fMRI analysis remains unclear. In this work, we extend\nself-supervised DL reconstruction to SMS imaging. Our results on prospectively\n10-fold accelerated 7T fMRI data show that self-supervised DL reduces\nreconstruction noise and suppresses residual artifacts. Subsequent fMRI\nanalysis remains unaltered by DL processing, while the improved temporal\nsignal-to-noise ratio produces higher coherence estimates between task runs.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:36:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Demirel", "Omer Burak", ""], ["Yaman", "Burhaneddin", ""], ["Dowdle", "Logan", ""], ["Moeller", "Steen", ""], ["Vizioli", "Luca", ""], ["Yacoub", "Essa", ""], ["Strupp", "John", ""], ["Olman", "Cheryl A.", ""], ["U\u011furbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2105.04538", "submitter": "Yufan Zhou", "authors": "Yufan Zhou, Changyou Chen, Jinhui Xu", "title": "Learning High-Dimensional Distributions with Latent Neural Fokker-Planck\n  Kernels", "comments": "code will be updated at https://github.com/drboog/FPK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning high-dimensional distributions is an important yet challenging\nproblem in machine learning with applications in various domains. In this\npaper, we introduce new techniques to formulate the problem as solving\nFokker-Planck equation in a lower-dimensional latent space, aiming to mitigate\nchallenges in high-dimensional data space. Our proposed model consists of\nlatent-distribution morphing, a generator and a parameterized Fokker-Planck\nkernel function. One fascinating property of our model is that it can be\ntrained with arbitrary steps of latent distribution morphing or even without\nmorphing, which makes it flexible and as efficient as Generative Adversarial\nNetworks (GANs). Furthermore, this property also makes our latent-distribution\nmorphing an efficient plug-and-play scheme, thus can be used to improve\narbitrary GANs, and more interestingly, can effectively correct failure cases\nof the GAN models. Extensive experiments illustrate the advantages of our\nproposed method over existing models.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:42:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Yufan", ""], ["Chen", "Changyou", ""], ["Xu", "Jinhui", ""]]}, {"id": "2105.04550", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, Kenji Kawaguchi", "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip\n  Connections and More Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been studied through the lens of expressive\npower and generalization. However, their optimization properties are less well\nunderstood. We take the first step towards analyzing GNN training by studying\nthe gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that\ndespite the non-convexity of training, convergence to a global minimum at a\nlinear rate is guaranteed under mild assumptions that we validate on real-world\ngraphs. Second, we study what may affect the GNNs' training speed. Our results\nshow that the training of GNNs is implicitly accelerated by skip connections,\nmore depth, and/or a good label distribution. Empirical results confirm that\nour theoretical results for linearized GNNs align with the training behavior of\nnonlinear GNNs. Our results provide the first theoretical support for the\nsuccess of GNNs with skip connections in terms of optimization, and suggest\nthat deep GNNs with skip connections would be promising in practice.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:59:01 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 05:55:42 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Xu", "Keyulu", ""], ["Zhang", "Mozhi", ""], ["Jegelka", "Stefanie", ""], ["Kawaguchi", "Kenji", ""]]}, {"id": "2105.04551", "submitter": "Michael Dorkenwald", "authors": "Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach,\n  Konstantinos G. Derpanis, Bj\\\"orn Ommer", "title": "Stochastic Image-to-Video Synthesis using cINNs", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding calls for a model to learn the characteristic interplay\nbetween static scene content and its dynamics: Given an image, the model must\nbe able to predict a future progression of the portrayed scene and, conversely,\na video should be explained in terms of its static image content and all the\nremaining characteristics not present in the initial frame. This naturally\nsuggests a bijective mapping between the video domain and the static content as\nwell as residual information. In contrast to common stochastic image-to-video\nsynthesis, such a model does not merely generate arbitrary videos progressing\nthe initial image. Given this image, it rather provides a one-to-one mapping\nbetween the residual vectors and the video with stochastic outcomes when\nsampling. The approach is naturally implemented using a conditional invertible\nneural network (cINN) that can explain videos by independently modelling static\nand other video characteristics, thus laying the basis for controlled video\nsynthesis. Experiments on four diverse video datasets demonstrate the\neffectiveness of our approach in terms of both the quality and diversity of the\nsynthesized results. Our project page is available at https://bit.ly/3t66bnU.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:59:09 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 13:09:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dorkenwald", "Michael", ""], ["Milbich", "Timo", ""], ["Blattmann", "Andreas", ""], ["Rombach", "Robin", ""], ["Derpanis", "Konstantinos G.", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2105.04553", "submitter": "Han Hu", "authors": "Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao,\n  Han Hu", "title": "Self-Supervised Learning with Swin Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are witnessing a modeling shift from CNN to Transformers in computer\nvision. In this work, we present a self-supervised learning approach called\nMoBY, with Vision Transformers as its backbone architecture. The approach\nbasically has no new inventions, which is combined from MoCo v2 and BYOL and\ntuned to achieve reasonably high accuracy on ImageNet-1K linear evaluation:\n72.8% and 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by\n300-epoch training. The performance is slightly better than recent works of\nMoCo v3 and DINO which adopt DeiT as the backbone, but with much lighter\ntricks.\n  More importantly, the general-purpose Swin Transformer backbone enables us to\nalso evaluate the learnt representations on downstream tasks such as object\ndetection and semantic segmentation, in contrast to a few recent approaches\nbuilt on ViT/DeiT which only report linear evaluation results on ImageNet-1K\ndue to ViT/DeiT not tamed for these dense prediction tasks. We hope our results\ncan facilitate more comprehensive evaluation of self-supervised learning\nmethods designed for Transformer architectures. Our code and models are\navailable at https://github.com/SwinTransformer/Transformer-SSL, which will be\ncontinually enriched.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:59:45 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 17:28:00 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xie", "Zhenda", ""], ["Lin", "Yutong", ""], ["Yao", "Zhuliang", ""], ["Zhang", "Zheng", ""], ["Dai", "Qi", ""], ["Cao", "Yue", ""], ["Hu", "Han", ""]]}, {"id": "2105.04580", "submitter": "Saksham Suri", "authors": "Sharath Girish, Saksham Suri, Saketh Rambhatla, Abhinav Shrivastava", "title": "Towards Discovery and Attribution of Open-world GAN Generated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent progress in Generative Adversarial Networks (GANs), it is\nimperative for media and visual forensics to develop detectors which can\nidentify and attribute images to the model generating them. Existing works have\nshown to attribute images to their corresponding GAN sources with high\naccuracy. However, these works are limited to a closed set scenario, failing to\ngeneralize to GANs unseen during train time and are therefore, not scalable\nwith a steady influx of new GANs. We present an iterative algorithm for\ndiscovering images generated from previously unseen GANs by exploiting the fact\nthat all GANs leave distinct fingerprints on their generated images. Our\nalgorithm consists of multiple components including network training,\nout-of-distribution detection, clustering, merge and refine steps. Through\nextensive experiments, we show that our algorithm discovers unseen GANs with\nhigh accuracy and also generalizes to GANs trained on unseen real datasets. We\nadditionally apply our algorithm to attribution and discovery of GANs in an\nonline fashion as well as to the more standard task of real/fake detection. Our\nexperiments demonstrate the effectiveness of our approach to discover new GANs\nand can be used in an open-world setup.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:00:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Girish", "Sharath", ""], ["Suri", "Saksham", ""], ["Rambhatla", "Saketh", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2105.04605", "submitter": "Xinyu Yi", "authors": "Xinyu Yi, Yuxiao Zhou, Feng Xu", "title": "TransPose: Real-time 3D Human Translation and Pose Estimation with Six\n  Inertial Sensors", "comments": "Accepted by SIGGRAPH 2021. Project page:\n  https://xinyu-yi.github.io/TransPose/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion capture is facing some new possibilities brought by the inertial\nsensing technologies which do not suffer from occlusion or wide-range\nrecordings as vision-based solutions do. However, as the recorded signals are\nsparse and quite noisy, online performance and global translation estimation\nturn out to be two key difficulties. In this paper, we present TransPose, a\nDNN-based approach to perform full motion capture (with both global\ntranslations and body poses) from only 6 Inertial Measurement Units (IMUs) at\nover 90 fps. For body pose estimation, we propose a multi-stage network that\nestimates leaf-to-full joint positions as intermediate results. This design\nmakes the pose estimation much easier, and thus achieves both better accuracy\nand lower computation cost. For global translation estimation, we propose a\nsupporting-foot-based method and an RNN-based method to robustly solve for the\nglobal translations with a confidence-based fusion technique. Quantitative and\nqualitative comparisons show that our method outperforms the state-of-the-art\nlearning- and optimization-based methods with a large margin in both accuracy\nand efficiency. As a purely inertial sensor-based approach, our method is not\nlimited by environmental settings (e.g., fixed cameras), making the capture\nfree from common difficulties such as wide-range motion space and strong\nocclusion.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:41:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Yi", "Xinyu", ""], ["Zhou", "Yuxiao", ""], ["Xu", "Feng", ""]]}, {"id": "2105.04619", "submitter": "Stephan R Richter", "authors": "Stephan R. Richter and Hassan Abu AlHaija and Vladlen Koltun", "title": "Enhancing Photorealism Enhancement", "comments": "Code and data available at\n  https://github.com/intel-isl/PhotorealismEnhancement Video available at\n  https://youtu.be/P1IcaBn3ej0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to enhancing the realism of synthetic images. The\nimages are enhanced by a convolutional network that leverages intermediate\nrepresentations produced by conventional rendering pipelines. The network is\ntrained via a novel adversarial objective, which provides strong supervision at\nmultiple perceptual levels. We analyze scene layout distributions in commonly\nused datasets and find that they differ in important ways. We hypothesize that\nthis is one of the causes of strong artifacts that can be observed in the\nresults of many prior methods. To address this we propose a new strategy for\nsampling image patches during training. We also introduce multiple\narchitectural improvements in the deep network modules used for photorealism\nenhancement. We confirm the benefits of our contributions in controlled\nexperiments and report substantial gains in stability and realism in comparison\nto recent image-to-image translation methods and a variety of other baselines.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:00:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Richter", "Stephan R.", ""], ["AlHaija", "Hassan Abu", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2105.04637", "submitter": "Hafez Farazi", "authors": "Hafez Farazi, Jan Nogga, Sven Behnke", "title": "Local Frequency Domain Transformer Networks for Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction is commonly referred to as forecasting future frames of a\nvideo sequence provided several past frames thereof. It remains a challenging\ndomain as visual scenes evolve according to complex underlying dynamics, such\nas the camera's egocentric motion or the distinct motility per individual\nobject viewed. These are mostly hidden from the observer and manifest as often\nhighly non-linear transformations between consecutive video frames. Therefore,\nvideo prediction is of interest not only in anticipating visual changes in the\nreal world but has, above all, emerged as an unsupervised learning rule\ntargeting the formation and dynamics of the observed environment. Many of the\ndeep learning-based state-of-the-art models for video prediction utilize some\nform of recurrent layers like Long Short-Term Memory (LSTMs) or Gated Recurrent\nUnits (GRUs) at the core of their models. Although these models can predict the\nfuture frames, they rely entirely on these recurrent structures to\nsimultaneously perform three distinct tasks: extracting transformations,\nprojecting them into the future, and transforming the current frame. In order\nto completely interpret the formed internal representations, it is crucial to\ndisentangle these tasks. This paper proposes a fully differentiable building\nblock that can perform all of those tasks separately while maintaining\ninterpretability. We derive the relevant theoretical foundations and showcase\nresults on synthetic as well as real data. We demonstrate that our method is\nreadily extended to perform motion segmentation and account for the scene's\ncomposition, and learns to produce reliable predictions in an entirely\ninterpretable manner by only observing unlabeled video data.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:48:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Farazi", "Hafez", ""], ["Nogga", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "2105.04642", "submitter": "Yutong Ban", "authors": "Yutong Ban and Guy Rosman and Thomas Ward and Daniel Hashimoto and\n  Taisei Kondo and Hidekazu Iwaki and Ozanan Meireles and Daniela Rus", "title": "SUrgical PRediction GAN for Events Anticipation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Comprehension of surgical workflow is the foundation upon which computers\nbuild the understanding of surgery. In this work, we moved beyond just the\nidentification of surgical phases to predict future surgical phases and the\ntransitions between them. We used a novel GAN formulation that sampled the\nfuture surgical phases trajectory conditioned, on past laparoscopic video\nframes, and compared it to state-of-the-art approaches for surgical video\nanalysis and alternative prediction methods.\n  We demonstrated its effectiveness in inferring and predicting the progress of\nlaparoscopic cholecystectomy videos. We quantified the horizon-accuracy\ntrade-off and explored average performance as well as the performance on the\nmore difficult, and clinically important, transitions between phases. Lastly,\nwe surveyed surgeons to evaluate the plausibility of these predicted\ntrajectories.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:56:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ban", "Yutong", ""], ["Rosman", "Guy", ""], ["Ward", "Thomas", ""], ["Hashimoto", "Daniel", ""], ["Kondo", "Taisei", ""], ["Iwaki", "Hidekazu", ""], ["Meireles", "Ozanan", ""], ["Rus", "Daniela", ""]]}, {"id": "2105.04668", "submitter": "Davis Rempe", "authors": "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath\n  Sridhar, Leonidas J. Guibas", "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal\npose and shape. Though substantial progress has been made in estimating 3D\nhuman motion and shape from dynamic observations, recovering plausible pose\nsequences in the presence of noise and occlusions remains a challenge. For this\npurpose, we propose an expressive generative model in the form of a conditional\nvariational autoencoder, which learns a distribution of the change in pose at\neach step of a motion sequence. Furthermore, we introduce a flexible\noptimization-based approach that leverages HuMoR as a motion prior to robustly\nestimate plausible pose and shape from ambiguous observations. Through\nextensive evaluations, we demonstrate that our model generalizes to diverse\nmotions and body shapes after training on a large motion capture dataset, and\nenables motion reconstruction from multiple input modalities including 3D\nkeypoints and RGB(-D) videos.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:04:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rempe", "Davis", ""], ["Birdal", "Tolga", ""], ["Hertzmann", "Aaron", ""], ["Yang", "Jimei", ""], ["Sridhar", "Srinath", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2105.04678", "submitter": "Bishwo Adhikari Mr.", "authors": "Bishwo Adhikari, Esa Rahtu, Heikki Huttunen", "title": "Sample selection for efficient image annotation", "comments": "This work has been accepted in EUVIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised object detection has been proven to be successful in many\nbenchmark datasets achieving human-level performances. However, acquiring a\nlarge amount of labeled image samples for supervised detection training is\ntedious, time-consuming, and costly. In this paper, we propose an efficient\nimage selection approach that samples the most informative images from the\nunlabeled dataset and utilizes human-machine collaboration in an iterative\ntrain-annotate loop. Image features are extracted by the CNN network followed\nby the similarity score calculation, Euclidean distance. Unlabeled images are\nthen sampled into different approaches based on the similarity score. The\nproposed approach is straightforward, simple and sampling takes place prior to\nthe network training. Experiments on datasets show that our method can reduce\nup to 80% of manual annotation workload, compared to full manual labeling\nsetting, and performs better than random sampling.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:25:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Adhikari", "Bishwo", ""], ["Rahtu", "Esa", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2105.04714", "submitter": "Jiankang Deng", "authors": "Jia Guo and Jiankang Deng and Alexandros Lattas and Stefanos Zafeiriou", "title": "Sample and Computation Redistribution for Efficient Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although tremendous strides have been made in uncontrolled face detection,\nefficient face detection with a low computation cost as well as high precision\nremains an open challenge. In this paper, we point out that training data\nsampling and computation distribution strategies are the keys to efficient and\naccurate face detection. Motivated by these observations, we introduce two\nsimple but effective methods (1) Sample Redistribution (SR), which augments\ntraining samples for the most needed stages, based on the statistics of\nbenchmark datasets; and (2) Computation Redistribution (CR), which reallocates\nthe computation between the backbone, neck and head of the model, based on a\nmeticulously defined search methodology. Extensive experiments conducted on\nWIDER FACE demonstrate the state-of-the-art efficiency-accuracy trade-off for\nthe proposed \\scrfd family across a wide range of compute regimes. In\nparticular, \\scrfdf{34} outperforms the best competitor, TinaFace, by $3.86\\%$\n(AP at hard set) while being more than \\emph{3$\\times$ faster} on GPUs with\nVGA-resolution images. We also release our code to facilitate future research.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 23:51:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Guo", "Jia", ""], ["Deng", "Jiankang", ""], ["Lattas", "Alexandros", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2105.04729", "submitter": "Huihuang Chen", "authors": "Huihuang Chen, Li Li, Jie Chen, Kuo-Yi Lin", "title": "Unsupervised domain adaptation via double classifiers based on high\n  confidence pseudo label", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to solve the problem of knowledge\ntransfer from labeled source domain to unlabeled target domain. Recently, many\ndomain adaptation (DA) methods use centroid to align the local distribution of\ndifferent domains, that is, to align different classes. This improves the\neffect of domain adaptation, but domain differences exist not only between\nclasses, but also between samples. This work rethinks what is the alignment\nbetween different domains, and studies how to achieve the real alignment\nbetween different domains. Previous DA methods only considered one distribution\nfeature of aligned samples, such as full distribution or local distribution. In\naddition to aligning the global distribution, the real domain adaptation should\nalso align the meso distribution and the micro distribution. Therefore, this\nstudy propose a double classifier method based on high confidence label (DCP).\nBy aligning the centroid and the distribution between centroid and sample of\ndifferent classifiers, the meso and micro distribution alignment of different\ndomains is realized. In addition, in order to reduce the chain error caused by\nerror marking, This study propose a high confidence marking method to reduce\nthe marking error. To verify its versatility, this study evaluates DCP on\ndigital recognition and target recognition data sets. The results show that our\nmethod achieves state-of-the-art results on most of the current domain\nadaptation benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:51:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Huihuang", ""], ["Li", "Li", ""], ["Chen", "Jie", ""], ["Lin", "Kuo-Yi", ""]]}, {"id": "2105.04746", "submitter": "Yang Liu", "authors": "Yang Liu and Saeed Anwar and Zhenyue Qin and Pan Ji and Sabrina\n  Caldwell and Tom Gedeon", "title": "Disentangling Noise from Images: A Flow-Based Image Denoising Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalent convolutional neural network (CNN) based image denoising\nmethods extract features of images to restore the clean ground truth, achieving\nhigh denoising accuracy. However, these methods may ignore the underlying\ndistribution of clean images, inducing distortions or artifacts in denoising\nresults. This paper proposes a new perspective to treat image denoising as a\ndistribution learning and disentangling task. Since the noisy image\ndistribution can be viewed as a joint distribution of clean images and noise,\nthe denoised images can be obtained via manipulating the latent representations\nto the clean counterpart. This paper also provides a distribution learning\nbased denoising framework. Following this framework, we present an invertible\ndenoising network, FDN, without any assumptions on either clean or noise\ndistributions, as well as a distribution disentanglement method. FDN learns the\ndistribution of noisy images, which is different from the previous CNN based\ndiscriminative mapping. Experimental results demonstrate FDN's capacity to\nremove synthetic additive white Gaussian noise (AWGN) on both category-specific\nand remote sensing images. Furthermore, the performance of FDN surpasses that\nof previously published methods in real image denoising with fewer parameters\nand faster speed. Our code is available at:\nhttps://github.com/Yang-Liu1082/FDN.git.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 01:52:26 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Liu", "Yang", ""], ["Anwar", "Saeed", ""], ["Qin", "Zhenyue", ""], ["Ji", "Pan", ""], ["Caldwell", "Sabrina", ""], ["Gedeon", "Tom", ""]]}, {"id": "2105.04776", "submitter": "Xiaobin Liu", "authors": "Xiaobin Liu, Shiliang Zhang", "title": "Graph Consistency Based Mean-Teaching for Unsupervised Domain Adaptive\n  Person Re-Identification", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent works show that mean-teaching is an effective framework for\nunsupervised domain adaptive person re-identification. However, existing\nmethods perform contrastive learning on selected samples between teacher and\nstudent networks, which is sensitive to noises in pseudo labels and neglects\nthe relationship among most samples. Moreover, these methods are not effective\nin cooperation of different teacher networks. To handle these issues, this\npaper proposes a Graph Consistency based Mean-Teaching (GCMT) method with\nconstructing the Graph Consistency Constraint (GCC) between teacher and student\nnetworks. Specifically, given unlabeled training images, we apply teacher\nnetworks to extract corresponding features and further construct a teacher\ngraph for each teacher network to describe the similarity relationships among\ntraining images. To boost the representation learning, different teacher graphs\nare fused to provide the supervise signal for optimizing student networks. GCMT\nfuses similarity relationships predicted by different teacher networks as\nsupervision and effectively optimizes student networks with more sample\nrelationships involved. Experiments on three datasets, i.e., Market-1501,\nDukeMTMCreID, and MSMT17, show that proposed GCMT outperforms state-of-the-art\nmethods by clear margin. Specially, GCMT even outperforms the previous method\nthat uses a deeper backbone. Experimental results also show that GCMT can\neffectively boost the performance with multiple teacher and student networks.\nOur code is available at https://github.com/liu-xb/GCMT .\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 04:09:49 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 05:57:52 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 13:28:39 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 02:11:13 GMT"}, {"version": "v5", "created": "Mon, 31 May 2021 01:02:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Xiaobin", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2105.04780", "submitter": "Zixu Wang", "authors": "Zixu Wang, Yishu Miao, Lucia Specia", "title": "Cross-Modal Generative Augmentation for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation is an approach that can effectively improve the performance\nof multimodal machine learning. This paper introduces a generative model for\ndata augmentation by leveraging the correlations among multiple modalities.\nDifferent from conventional data augmentation approaches that apply low level\noperations with deterministic heuristics, our method proposes to learn an\naugmentation sampler that generates samples of the target modality conditioned\non observed modalities in the variational auto-encoder framework. Additionally,\nthe proposed model is able to quantify the confidence of augmented data by its\ngenerative probability, and can be jointly updated with a downstream pipeline.\nExperiments on Visual Question Answering tasks demonstrate the effectiveness of\nthe proposed generative model, which is able to boost the strong UpDn-based\nmodels to the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 04:51:26 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Zixu", ""], ["Miao", "Yishu", ""], ["Specia", "Lucia", ""]]}, {"id": "2105.04799", "submitter": "Wenkai Liang", "authors": "Wenkai Liang, Yan Wu, Ming Li, Peng Zhang, Yice Cao, Xin Hu", "title": "A Feature Fusion-Net Using Deep Spatial Context Encoder and\n  Nonstationary Joint Statistical Model for High Resolution SAR Image\n  Classification", "comments": "17 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been applied to learn spatial\nfeatures for high-resolution (HR) synthetic aperture radar (SAR) image\nclassification. However, there has been little work on integrating the unique\nstatistical distributions of SAR images which can reveal physical properties of\nterrain objects, into CNNs in a supervised feature learning framework. To\naddress this problem, a novel end-to-end supervised classification method is\nproposed for HR SAR images by considering both spatial context and statistical\nfeatures. First, to extract more effective spatial features from SAR images, a\nnew deep spatial context encoder network (DSCEN) is proposed, which is a\nlightweight structure and can be effectively trained with a small number of\nsamples. Meanwhile, to enhance the diversity of statistics, the nonstationary\njoint statistical model (NS-JSM) is adopted to form the global statistical\nfeatures. Specifically, SAR images are transformed into the Gabor wavelet\ndomain and the produced multi-subbands magnitudes and phases are modeled by the\nlog-normal and uniform distribution. The covariance matrix is further utilized\nto capture the inter-scale and intra-scale nonstationary correlation between\nthe statistical subbands and make the joint statistical features more compact\nand distinguishable. Considering complementary advantages, a feature fusion\nnetwork (Fusion-Net) base on group compression and smooth normalization is\nconstructed to embed the statistical features into the spatial features and\noptimize the fusion feature representation. As a result, our model can learn\nthe discriminative features and improve the final classification performance.\nExperiments on four HR SAR images validate the superiority of the proposed\nmethod over other related algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:20:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Liang", "Wenkai", ""], ["Wu", "Yan", ""], ["Li", "Ming", ""], ["Zhang", "Peng", ""], ["Cao", "Yice", ""], ["Hu", "Xin", ""]]}, {"id": "2105.04807", "submitter": "Oded Cohen", "authors": "Oded Cohen", "title": "ORCEA: Object Recognition by Continuous Evidence Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  ORCEA is a novel object recognition method applicable for objects describable\nby a generative model. The primary goal of ORCEA is to maintain a probability\ndensity distribution of possible matches over the object parameter space, while\ncontinuously updating it with incoming evidence; detection and regression are\nby-products of this process. ORCEA can project primitive evidence of various\ntypes (edge element, area patches etc.) directly on the object parameter space;\nthis made possible by the study phase where ORCEA builds a probabilistic model,\nfor each evidence type, that links evidence and the object-parameters under\nwhich they were created. The detection phase consists of building the joint\ndistribution of possible matches resulting from the set of given evidence,\nincluding possible grouping to signal/noise; no additional algorithmic steps\nare needed, as the resulting PDF encapsulates all knowledge about possible\nsolutions. ORCEA represents the match distribution over the parameter space as\na set of Gaussian distributions, each representing a concrete probabilistic\nhypothesis about the object, which can be used outside its scope as well. ORCEA\nwas tested on synthetic images with varying levels of complexity and noise, and\nshows satisfactory results.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:38:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Cohen", "Oded", ""]]}, {"id": "2105.04823", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Jiale Zhou, Xuming He", "title": "Learning Implicit Temporal Alignment for Few-shot Video Classification", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot video classification aims to learn new video categories with only a\nfew labeled examples, alleviating the burden of costly annotation in real-world\napplications. However, it is particularly challenging to learn a\nclass-invariant spatial-temporal representation in such a setting. To address\nthis, we propose a novel matching-based few-shot learning strategy for video\nsequences in this work. Our main idea is to introduce an implicit temporal\nalignment for a video pair, capable of estimating the similarity between them\nin an accurate and robust manner. Moreover, we design an effective context\nencoding module to incorporate spatial and feature channel context, resulting\nin better modeling of intra-class variations. To train our model, we develop a\nmulti-task loss for learning video matching, leading to video features with\nbetter generalization. Extensive experimental results on two challenging\nbenchmarks, show that our method outperforms the prior arts with a sizable\nmargin on SomethingSomething-V2 and competitive results on Kinetics.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:18:57 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhang", "Songyang", ""], ["Zhou", "Jiale", ""], ["He", "Xuming", ""]]}, {"id": "2105.04826", "submitter": "Yuanlun Xie", "authors": "Wenhong Tian, Yuanlun Xie, Tingsong Ma, Hengxin Zhang", "title": "Uncover Common Facial Expressions in Terracotta Warriors: A Deep\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can advanced deep learning technologies be applied to analyze some ancient\nhumanistic arts? Can deep learning technologies be directly applied to special\nscenes such as facial expression analysis of Terracotta Warriors? The big\nchallenging is that the facial features of the Terracotta Warriors are very\ndifferent from today's people. We found that it is very poor to directly use\nthe models that have been trained on other classic facial expression datasets\nto analyze the facial expressions of the Terracotta Warriors. At the same time,\nthe lack of public high-quality facial expression data of the Terracotta\nWarriors also limits the use of deep learning technologies. Therefore, we\nfirstly use Generative Adversarial Networks (GANs) to generate enough\nhigh-quality facial expression data for subsequent training and recognition. We\nalso verify the effectiveness of this approach. For the first time, this paper\nuses deep learning technologies to find common facial expressions of general\nand postured Terracotta Warriors. These results will provide an updated\ntechnical means for the research of art of the Terracotta Warriors and shine\nlights on the research of other ancient arts.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:28:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tian", "Wenhong", ""], ["Xie", "Yuanlun", ""], ["Ma", "Tingsong", ""], ["Zhang", "Hengxin", ""]]}, {"id": "2105.04834", "submitter": "Guoqiu Wang", "authors": "Guoqiu Wang, Huanqian Yan, Ying Guo, Xingxing Wei", "title": "Improving Adversarial Transferability with Gradient Refining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, which are\ncrafted by adding human-imperceptible perturbations to original images. Most\nexisting adversarial attack methods achieve nearly 100% attack success rates\nunder the white-box setting, but only achieve relatively low attack success\nrates under the black-box setting. To improve the transferability of\nadversarial examples for the black-box setting, several methods have been\nproposed, e.g., input diversity, translation-invariant attack, and\nmomentum-based attack. In this paper, we propose a method named Gradient\nRefining, which can further improve the adversarial transferability by\ncorrecting useless gradients introduced by input diversity through multiple\ntransformations. Our method is generally applicable to many gradient-based\nattack methods combined with input diversity. Extensive experiments are\nconducted on the ImageNet dataset and our method can achieve an average\ntransfer success rate of 82.07% for three different models under single-model\nsetting, which outperforms the other state-of-the-art methods by a large margin\nof 6.0% averagely. And we have applied the proposed method to the competition\nCVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and\nwon the second place in attack success rates among 1558 teams.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:44:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 13:01:02 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wang", "Guoqiu", ""], ["Yan", "Huanqian", ""], ["Guo", "Ying", ""], ["Wei", "Xingxing", ""]]}, {"id": "2105.04836", "submitter": "Aisha Urooj", "authors": "Aisha Urooj Khan, Hilde Kuehne, Kevin Duarte, Chuang Gan, Niels Lobo,\n  Mubarak Shah", "title": "Found a Reason for me? Weakly-supervised Grounded Visual Question\n  Answering using Capsules", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of grounding VQA tasks has seen an increased attention in the\nresearch community recently, with most attempts usually focusing on solving\nthis task by using pretrained object detectors. However, pre-trained object\ndetectors require bounding box annotations for detecting relevant objects in\nthe vocabulary, which may not always be feasible for real-life large-scale\napplications. In this paper, we focus on a more relaxed setting: the grounding\nof relevant visual entities in a weakly supervised manner by training on the\nVQA task alone. To address this problem, we propose a visual capsule module\nwith a query-based selection mechanism of capsule features, that allows the\nmodel to focus on relevant regions based on the textual cues about visual\ninformation in the question. We show that integrating the proposed capsule\nmodule in existing VQA systems significantly improves their performance on the\nweakly supervised grounding task. Overall, we demonstrate the effectiveness of\nour approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the\nCLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with\nground truth bounding boxes for objects that are relevant for the correct\nanswer, as well as on GQA, a real world VQA dataset with compositional\nquestions. We show that the systems with the proposed capsule module\nconsistently outperform the respective baseline systems in terms of answer\ngrounding, while achieving comparable performance on VQA task.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:45:32 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Khan", "Aisha Urooj", ""], ["Kuehne", "Hilde", ""], ["Duarte", "Kevin", ""], ["Gan", "Chuang", ""], ["Lobo", "Niels", ""], ["Shah", "Mubarak", ""]]}, {"id": "2105.04839", "submitter": "GuiYu Tian", "authors": "Guiyu Tian, Wenhao Jiang, Wei Liu, Yadong Mu", "title": "Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Poisoning MorphNet, the first backdoor attack method on\npoint clouds. Conventional adversarial attack takes place in the inference\nstage, often fooling a model by perturbing samples. In contrast, backdoor\nattack aims to implant triggers into a model during the training stage, such\nthat the victim model acts normally on the clean data unless a trigger is\npresent in a sample. This work follows a typical setting of clean-label\nbackdoor attack, where a few poisoned samples (with their content tampered yet\nlabels unchanged) are injected into the training set. The unique contributions\nof MorphNet are two-fold. First, it is key to ensure the implanted triggers\nboth visually imperceptible to humans and lead to high attack success rate on\nthe point clouds. To this end, MorphNet jointly optimizes two objectives for\nsample-adaptive poisoning: a reconstruction loss that preserves the visual\nsimilarity between benign / poisoned point clouds, and a classification loss\nthat enforces a modern recognition model of point clouds tends to mis-classify\nthe poisoned sample to a pre-specified target category. This implicitly\nconducts spectral separation over point clouds, hiding sample-adaptive triggers\nin fine-grained high-frequency details. Secondly, existing backdoor attack\nmethods are mainly designed for image data, easily defended by some point cloud\nspecific operations (such as denoising). We propose a third loss in MorphNet\nfor suppressing isolated points, leading to improved resistance to\ndenoising-based defense. Comprehensive evaluations are conducted on ModelNet40\nand ShapeNetcorev2. Our proposed Poisoning MorphNet outstrips all previous\nmethods with clear margins.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:48:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tian", "Guiyu", ""], ["Jiang", "Wenhao", ""], ["Liu", "Wei", ""], ["Mu", "Yadong", ""]]}, {"id": "2105.04872", "submitter": "Zeyu Xiao", "authors": "Ruikang Xu, Zeyu Xiao, Jie Huang, Yueyi Zhang, Zhiwei Xiong", "title": "EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration", "comments": "Accepted at NTIRE Workshop, CVPR 2021. Ruikang and Zeyu contribute\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image deblurring has seen a great improvement with the development of deep\nneural networks. In practice, however, blurry images often suffer from\nadditional degradations such as downscaling and compression. To address these\nchallenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image\nrestoration from multiple degradations, by fully exploiting the self- and\ncross-scale similarities in the degraded image.Specifically, we design two\npyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and\nthe pyramid self-attention (PSA) module, as the main components of the proposed\nnetwork. By taking several replicated blurry images as inputs, the PPT module\ntransfers both self- and cross-scale similarity information from the same\ndegraded image in a progressive manner. Then, the PSA module fuses the above\ntransferred features for subsequent restoration using self- and\nspatial-attention mechanisms. Experimental results demonstrate that our method\nsignificantly outperforms existing solutions for blurry image super-resolution\nand blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN\nachieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the\nbest SSIM/LPIPS scores in Track 2 (JPEG Artifacts).\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:50:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Ruikang", ""], ["Xiao", "Zeyu", ""], ["Huang", "Jie", ""], ["Zhang", "Yueyi", ""], ["Xiong", "Zhiwei", ""]]}, {"id": "2105.04875", "submitter": "Maximilian T. Fischer", "authors": "Maximilian T. Fischer, Daniel A. Keim, Manuel Stein", "title": "Video-based Analysis of Soccer Matches", "comments": "9 pages, 4 figures, 1 table, 2nd International Workshop on Multimedia\n  Content Analysis in Sports (MMSports '19)", "journal-ref": "International Workshop on Multimedia Content Analysis in Sports\n  (MMSports), 2019", "doi": "10.1145/3347318.3355515", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasingly detailed investigation of game play and tactics in\ninvasive team sports such as soccer, it becomes ever more important to present\ncauses, actions and findings in a meaningful manner. Visualizations, especially\nwhen augmenting relevant information directly inside a video recording of a\nmatch, can significantly improve and simplify soccer match preparation and\ntactic planning. However, while many visualization techniques for soccer have\nbeen developed in recent years, few have been directly applied to the\nvideo-based analysis of soccer matches. This paper provides a comprehensive\noverview and categorization of the methods developed for the video-based visual\nanalysis of soccer matches. While identifying the advantages and disadvantages\nof the individual approaches, we identify and discuss open research questions,\nsoon enabling analysts to develop winning strategies more efficiently, do rapid\nfailure analysis or identify weaknesses in opposing teams.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:01:02 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fischer", "Maximilian T.", ""], ["Keim", "Daniel A.", ""], ["Stein", "Manuel", ""]]}, {"id": "2105.04880", "submitter": "Yiming Wang", "authors": "Yiming Wang, Dongxia Chang, Zhiqiang Fu and Yao Zhao", "title": "Consistent Multiple Graph Embedding for Multi-View Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based multi-view clustering aiming to obtain a partition of data across\nmultiple views, has received considerable attention in recent years. Although\ngreat efforts have been made for graph-based multi-view clustering, it remains\na challenge to fuse characteristics from various views to learn a common\nrepresentation for clustering. In this paper, we propose a novel Consistent\nMultiple Graph Embedding Clustering framework(CMGEC). Specifically, a multiple\ngraph auto-encoder(M-GAE) is designed to flexibly encode the complementary\ninformation of multi-view data using a multi-graph attention fusion encoder. To\nguide the learned common representation maintaining the similarity of the\nneighboring characteristics in each view, a Multi-view Mutual Information\nMaximization module(MMIM) is introduced. Furthermore, a graph fusion\nnetwork(GFN) is devised to explore the relationship among graphs from different\nviews and provide a common consensus graph needed in M-GAE. By jointly training\nthese models, the common latent representation can be obtained which encodes\nmore complementary information from multiple views and depicts data more\ncomprehensively. Experiments on three types of multi-view datasets demonstrate\nCMGEC outperforms the state-of-the-art clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:08:22 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Yiming", ""], ["Chang", "Dongxia", ""], ["Fu", "Zhiqiang", ""], ["Zhao", "Yao", ""]]}, {"id": "2105.04881", "submitter": "Roohallah Alizadehsani Dr", "authors": "Afshin Shoeibi, Marjane Khodatars, Mahboobeh Jafari, Parisa Moridian,\n  Mitra Rezaei, Roohallah Alizadehsani, Fahime Khozeimeh, Juan Manuel Gorriz,\n  J\\'onathan Heras, Maryam Panahiazar, Saeid Nahavandi, U. Rajendra Acharya", "title": "Applications of Deep Learning Techniques for Automated Multiple\n  Sclerosis Detection Using Magnetic Resonance Imaging: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Sclerosis (MS) is a type of brain disease which causes visual,\nsensory, and motor problems for people with a detrimental effect on the\nfunctioning of the nervous system. In order to diagnose MS, multiple screening\nmethods have been proposed so far; among them, magnetic resonance imaging (MRI)\nhas received considerable attention among physicians. MRI modalities provide\nphysicians with fundamental information about the structure and function of the\nbrain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS\nusing MRI is time-consuming, tedious, and prone to manual errors. Hence,\ncomputer aided diagnosis systems (CADS) based on artificial intelligence (AI)\nmethods have been proposed in recent years for accurate diagnosis of MS using\nMRI neuroimaging modalities. In the AI field, automated MS diagnosis is being\nconducted using (i) conventional machine learning and (ii) deep learning (DL)\ntechniques. The conventional machine learning approach is based on feature\nextraction and selection by trial and error. In DL, these steps are performed\nby the DL model itself. In this paper, a complete review of automated MS\ndiagnosis methods performed using DL techniques with MRI neuroimaging\nmodalities are discussed. Also, each work is thoroughly reviewed and discussed.\nFinally, the most important challenges and future directions in the automated\nMS diagnosis using DL techniques coupled with MRI modalities are presented in\ndetail.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:08:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Shoeibi", "Afshin", ""], ["Khodatars", "Marjane", ""], ["Jafari", "Mahboobeh", ""], ["Moridian", "Parisa", ""], ["Rezaei", "Mitra", ""], ["Alizadehsani", "Roohallah", ""], ["Khozeimeh", "Fahime", ""], ["Gorriz", "Juan Manuel", ""], ["Heras", "J\u00f3nathan", ""], ["Panahiazar", "Maryam", ""], ["Nahavandi", "Saeid", ""], ["Acharya", "U. Rajendra", ""]]}, {"id": "2105.04885", "submitter": "Michail Chatzianastasis", "authors": "Michail Chatzianastasis, George Dasoulas, Georgios Siolas, Michalis\n  Vazirgiannis", "title": "Operation Embeddings for Neural Architecture Search", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Architecture Search (NAS) has recently gained increased attention, as\na class of approaches that automatically searches in an input space of network\narchitectures. A crucial part of the NAS pipeline is the encoding of the\narchitecture that consists of the applied computational blocks, namely the\noperations and the links between them. Most of the existing approaches either\nfail to capture the structural properties of the architectures or use a\nhand-engineered vector to encode the operator information. In this paper, we\npropose the replacement of fixed operator encoding with learnable\nrepresentations in the optimization process. This approach, which effectively\ncaptures the relations of different operations, leads to smoother and more\naccurate representations of the architectures and consequently to improved\nperformance of the end task. Our extensive evaluation in ENAS benchmark\ndemonstrates the effectiveness of the proposed operation embeddings to the\ngeneration of highly accurate models, achieving state-of-the-art performance.\nFinally, our method produces top-performing architectures that share similar\noperation and graph patterns, highlighting a strong correlation between\narchitecture's structural properties and performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:17:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chatzianastasis", "Michail", ""], ["Dasoulas", "George", ""], ["Siolas", "Georgios", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2105.04891", "submitter": "\\`Oscar Lorente", "authors": "\\`Oscar Lorente, Ian Riera, Shauryadeep Chaudhuri, Oriol Catalan,\n  V\\'ictor Casales", "title": "Museum Painting Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To retrieve images based on their content is one of the most studied topics\nin the field of computer vision. Nowadays, this problem can be addressed using\nmodern techniques such as feature extraction using machine learning, but over\nthe years different classical methods have been developed. In this paper, we\nimplement a query by example retrieval system for finding paintings in a museum\nimage collection using classic computer vision techniques. Specifically, we\nstudy the performance of the color, texture, text and feature descriptors in\ndatasets with different perturbations in the images: noise, overlapping text\nboxes, color corruption and rotation. We evaluate each of the cases using the\nMean Average Precision (MAP) metric, and we obtain results that vary between\n0.5 and 1.0 depending on the problem conditions.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:28:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lorente", "\u00d2scar", ""], ["Riera", "Ian", ""], ["Chaudhuri", "Shauryadeep", ""], ["Catalan", "Oriol", ""], ["Casales", "V\u00edctor", ""]]}, {"id": "2105.04895", "submitter": "\\`Oscar Lorente", "authors": "\\`Oscar Lorente, Ian Riera, Aditya Rana", "title": "Image Classification with Classic and Deep Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To classify images based on their content is one of the most studied topics\nin the field of computer vision. Nowadays, this problem can be addressed using\nmodern techniques such as Convolutional Neural Networks (CNN), but over the\nyears different classical methods have been developed. In this report, we\nimplement an image classifier using both classic computer vision and deep\nlearning techniques. Specifically, we study the performance of a Bag of Visual\nWords classifier using Support Vector Machines, a Multilayer Perceptron, an\nexisting architecture named InceptionV3 and our own CNN, TinyNet, designed from\nscratch. We evaluate each of the cases in terms of accuracy and loss, and we\nobtain results that vary between 0.6 and 0.96 depending on the model and\nconfiguration used.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:32:38 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lorente", "\u00d2scar", ""], ["Riera", "Ian", ""], ["Rana", "Aditya", ""]]}, {"id": "2105.04905", "submitter": "\\`Oscar Lorente", "authors": "\\`Oscar Lorente, Ian Riera, Aditya Rana", "title": "Scene Understanding for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To detect and segment objects in images based on their content is one of the\nmost active topics in the field of computer vision. Nowadays, this problem can\nbe addressed using Deep Learning architectures such as Faster R-CNN or YOLO,\namong others. In this paper, we study the behaviour of different configurations\nof RetinaNet, Faster R-CNN and Mask R-CNN presented in Detectron2. First, we\nevaluate qualitatively and quantitatively (AP) the performance of the\npre-trained models on KITTI-MOTS and MOTSChallenge datasets. We observe a\nsignificant improvement in performance after fine-tuning these models on the\ndatasets of interest and optimizing hyperparameters. Finally, we run inference\nin unusual situations using out of context datasets, and present interesting\nresults that help us understanding better the networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:50:05 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lorente", "\u00d2scar", ""], ["Riera", "Ian", ""], ["Rana", "Aditya", ""]]}, {"id": "2105.04906", "submitter": "Adrien Bardes", "authors": "Adrien Bardes and Jean Ponce and Yann LeCun", "title": "VICReg: Variance-Invariance-Covariance Regularization for\n  Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised methods for image representation learning are based on\nmaximizing the agreement between embedding vectors from different views of the\nsame image. A trivial solution is obtained when the encoder outputs constant\nvectors. This collapse problem is often avoided through implicit biases in the\nlearning architecture, that often lack a clear justification or interpretation.\nIn this paper, we introduce VICReg (Variance-Invariance-Covariance\nRegularization), a method that explicitly avoids the collapse problem with a\nsimple regularization term on the variance of the embeddings along each\ndimension individually. VICReg combines the variance term with a decorrelation\nmechanism based on redundancy reduction and covariance regularization, and\nachieves results on par with the state of the art on several downstream tasks.\nIn addition, we show that incorporating our new variance term into other\nmethods helps stabilize the training and leads to performance improvements.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:53:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bardes", "Adrien", ""], ["Ponce", "Jean", ""], ["LeCun", "Yann", ""]]}, {"id": "2105.04908", "submitter": "\\`Oscar Lorente", "authors": "Pol Albacar, \\`Oscar Lorente, Eduard Mainou, Ian Riera", "title": "Video Surveillance for Road Traffic Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the learned techniques during the Video Analysis Module\nof the Master in Computer Vision from the Universitat Aut\\`onoma de Barcelona,\nused to solve the third track of the AI-City Challenge. This challenge aims to\ntrack vehicles across multiple cameras placed in multiple intersections spread\nout over a city. The methodology followed focuses first in solving\nmulti-tracking in a single camera and then extending it to multiple cameras.\nThe qualitative results of the implemented techniques are presented using\nstandard metrics for video analysis such as mAP for object detection and IDF1\nfor tracking. The source code is publicly available at:\nhttps://github.com/mcv-m6-video/mcv-m6-2021-team4.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:54:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Albacar", "Pol", ""], ["Lorente", "\u00d2scar", ""], ["Mainou", "Eduard", ""], ["Riera", "Ian", ""]]}, {"id": "2105.04916", "submitter": "Yanqi Chen", "authors": "Yanqi Chen, Zhaofei Yu, Wei Fang, Tiejun Huang and Yonghong Tian", "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring", "comments": "9 pages, 7 figures, 4 tables. To appear in the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:05:53 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:35:21 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 08:38:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chen", "Yanqi", ""], ["Yu", "Zhaofei", ""], ["Fang", "Wei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "2105.04932", "submitter": "Yuhao Zhu", "authors": "Yuhao Zhu, Qi Li, Jian Wang, Chengzhong Xu, Zhenan Sun", "title": "One Shot Face Swapping on Megapixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face swapping has both positive applications such as entertainment,\nhuman-computer interaction, etc., and negative applications such as DeepFake\nthreats to politics, economics, etc. Nevertheless, it is necessary to\nunderstand the scheme of advanced methods for high-quality face swapping and\ngenerate enough and representative face swapping images to train DeepFake\ndetection algorithms. This paper proposes the first Megapixel level method for\none shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face\nrepresentation hierarchically by the proposed Hierarchical Representation Face\nEncoder (HieRFE) in an extended latent space to maintain more facial details,\nrather than compressed representation in previous face swapping methods.\nSecondly, a carefully designed Face Transfer Module (FTM) is proposed to\ntransfer the identity from a source image to the target by a non-linear\ntrajectory without explicit feature disentanglement. Finally, the swapped faces\ncan be synthesized by StyleGAN2 with the benefits of its training stability and\npowerful generative capability. Each part of MegaFS can be trained separately\nso the requirement of our model for GPU memory can be satisfied for megapixel\nface swapping. In summary, complete face representation, stable training, and\nlimited memory usage are the three novel contributions to the success of our\nmethod. Extensive experiments demonstrate the superiority of MegaFS and the\nfirst megapixel level face swapping database is released for research on\nDeepFake detection and face image editing in the public domain. The dataset is\nat this link.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:41:47 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhu", "Yuhao", ""], ["Li", "Qi", ""], ["Wang", "Jian", ""], ["Xu", "Chengzhong", ""], ["Sun", "Zhenan", ""]]}, {"id": "2105.04951", "submitter": "Zhiyu Jiang", "authors": "Zhinan Cai, Zhiyu Jiang, Yuan Yuan", "title": "Task-Related Self-Supervised Learning for Remote Sensing Image Change\n  Detection", "comments": "IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection for remote sensing images is widely applied for urban change\ndetection, disaster assessment and other fields. However, most of the existing\nCNN-based change detection methods still suffer from the problem of inadequate\npseudo-changes suppression and insufficient feature representation. In this\nwork, an unsupervised change detection method based on Task-related\nSelf-supervised Learning Change Detection network with smooth mechanism(TSLCD)\nis proposed to eliminate it. The main contributions include: (1) the\ntask-related self-supervised learning module is introduced to extract spatial\nfeatures more effectively. (2) a hard-sample-mining loss function is applied to\npay more attention to the hard-to-classify samples. (3) a smooth mechanism is\nutilized to remove some of pseudo-changes and noise. Experiments on four remote\nsensing change detection datasets reveal that the proposed TSLCD method\nachieves the state-of-the-art for change detection task.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:44:04 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 06:41:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cai", "Zhinan", ""], ["Jiang", "Zhiyu", ""], ["Yuan", "Yuan", ""]]}, {"id": "2105.04967", "submitter": "Zhiyu Jiang", "authors": "Xinxing He, Yuan Yuan, Zhiyu Jiang", "title": "Open Set Domain Recognition via Attention-Based GCN and Semantic\n  Matching Optimization", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open set domain recognition has got the attention in recent years. The task\naims to specifically classify each sample in the practical unlabeled target\ndomain, which consists of all known classes in the manually labeled source\ndomain and target-specific unknown categories. The absence of annotated\ntraining data or auxiliary attribute information for unknown categories makes\nthis task especially difficult. Moreover, exiting domain discrepancy in label\nspace and data distribution further distracts the knowledge transferred from\nknown classes to unknown classes. To address these issues, this work presents\nan end-to-end model based on attention-based GCN and semantic matching\noptimization, which first employs the attention mechanism to enable the central\nnode to learn more discriminating representations from its neighbors in the\nknowledge graph. Moreover, a coarse-to-fine semantic matching optimization\napproach is proposed to progressively bridge the domain gap. Experimental\nresults validate that the proposed model not only has superiority on\nrecognizing the images of known and unknown classes, but also can adapt to\nvarious openness of the target domain.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:05:36 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 02:16:33 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["He", "Xinxing", ""], ["Yuan", "Yuan", ""], ["Jiang", "Zhiyu", ""]]}, {"id": "2105.04984", "submitter": "Jan-Peter Kucklick", "authors": "Jan-Peter Kucklick and Oliver M\\\"uller", "title": "A Comparison of Multi-View Learning Strategies for Satellite Image-Based\n  Real Estate Appraisal", "comments": "Presented at: The AAAI-21 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the house credit process, banks and lenders rely on a fast and accurate\nestimation of a real estate price to determine the maximum loan value. Real\nestate appraisal is often based on relational data, capturing the hard facts of\nthe property. Yet, models benefit strongly from including image data, capturing\nadditional soft factors. The combination of the different data types requires a\nmulti-view learning method. Therefore, the question arises which strengths and\nweaknesses different multi-view learning strategies have. In our study, we test\nmulti-kernel learning, multi-view concatenation and multi-view neural networks\non real estate data and satellite images from Asheville, NC. Our results\nsuggest that multi-view learning increases the predictive performance up to 13%\nin MAE. Multi-view neural networks perform best, however result in\nintransparent black-box models. For users seeking interpretability, hybrid\nmulti-view neural networks or a boosting strategy are a suitable alternative.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:41:22 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kucklick", "Jan-Peter", ""], ["M\u00fcller", "Oliver", ""]]}, {"id": "2105.04990", "submitter": "Zhiyu Jiang", "authors": "Chenlu Wei, Zhiyu Jiang, Yuan Yuan", "title": "Weighted Hierarchical Sparse Representation for Hyperspectral Target\n  Detection", "comments": "IGARSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral target detection has been widely studied in the field of remote\nsensing. However, background dictionary building issue and the correlation\nanalysis of target and background dictionary issue have not been well studied.\nTo tackle these issues, a \\emph{Weighted Hierarchical Sparse Representation}\nfor hyperspectral target detection is proposed. The main contributions of this\nwork are listed as follows. 1) Considering the insufficient representation of\nthe traditional background dictionary building by dual concentric window\nstructure, a hierarchical background dictionary is built considering the local\nand global spectral information simultaneously. 2) To reduce the impureness\nimpact of background dictionary, target scores from target dictionary and\nbackground dictionary are weighted considered according to the dictionary\nquality. Three hyperspectral target detection data sets are utilized to verify\nthe effectiveness of the proposed method. And the experimental results show a\nbetter performance when compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:50:16 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wei", "Chenlu", ""], ["Jiang", "Zhiyu", ""], ["Yuan", "Yuan", ""]]}, {"id": "2105.04996", "submitter": "Zhiyu Jiang", "authors": "Chengze Wang, Zhiyu Jiang, Yuan Yuan", "title": "Instance-aware Remote Sensing Image Captioning with Cross-hierarchy\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial attention is a straightforward approach to enhance the\nperformance for remote sensing image captioning. However, conventional spatial\nattention approaches consider only the attention distribution on one fixed\ncoarse grid, resulting in the semantics of tiny objects can be easily ignored\nor disturbed during the visual feature extraction. Worse still, the fixed\nsemantic level of conventional spatial attention limits the image understanding\nin different levels and perspectives, which is critical for tackling the huge\ndiversity in remote sensing images. To address these issues, we propose a\nremote sensing image caption generator with instance-awareness and\ncross-hierarchy attention. 1) The instances awareness is achieved by\nintroducing a multi-level feature architecture that contains the visual\ninformation of multi-level instance-possible regions and their surroundings. 2)\nMoreover, based on this multi-level feature extraction, a cross-hierarchy\nattention mechanism is proposed to prompt the decoder to dynamically focus on\ndifferent semantic hierarchies and instances at each time step. The\nexperimental results on public datasets demonstrate the superiority of proposed\napproach over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:59:07 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Chengze", ""], ["Jiang", "Zhiyu", ""], ["Yuan", "Yuan", ""]]}, {"id": "2105.05003", "submitter": "Lizhe Liu", "authors": "Lizhe Liu, Xiaohao Chen, Siyu Zhu, Ping Tan", "title": "CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional\n  Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep-learning-based lane detection methods are successful in most\nscenarios but struggling for lane lines with complex topologies. In this work,\nwe propose CondLaneNet, a novel top-to-down lane detection framework that\ndetects the lane instances first and then dynamically predicts the line shape\nfor each instance. Aiming to resolve lane instance-level discrimination\nproblem, we introduce a conditional lane detection strategy based on\nconditional convolution and row-wise formulation. Further, we design the\nRecurrent Instance Module(RIM) to overcome the problem of detecting lane lines\nwith complex topologies such as dense lines and fork lines. Benefit from the\nend-to-end pipeline which requires little post-process, our method has\nreal-time efficiency. We extensively evaluate our method on three benchmarks of\nlane detection. Results show that our method achieves state-of-the-art\nperformance on all three benchmark datasets. Moreover, our method has the\ncoexistence of accuracy and efficiency, e.g. a 78.14 F1 score and 220 FPS on\nCULane. Our code is available at\nhttps://github.com/aliyun/conditional-lane-detection.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:10:34 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 11:44:17 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liu", "Lizhe", ""], ["Chen", "Xiaohao", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2105.05011", "submitter": "Lan Fu", "authors": "Lan Fu, Hongkai Yu, Felix Juefei-Xu, Jinlong Li, Qing Guo, and Song\n  Wang", "title": "Let There be Light: Improved Traffic Surveillance via Detail Preserving\n  Night-to-Day Transfer", "comments": "to appear in TCSVT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, image and video surveillance have made considerable\nprogresses to the Intelligent Transportation Systems (ITS) with the help of\ndeep Convolutional Neural Networks (CNNs). As one of the state-of-the-art\nperception approaches, detecting the interested objects in each frame of video\nsurveillance is widely desired by ITS. Currently, object detection shows\nremarkable efficiency and reliability in standard scenarios such as daytime\nscenes with favorable illumination conditions. However, in face of adverse\nconditions such as the nighttime, object detection loses its accuracy\nsignificantly. One of the main causes of the problem is the lack of sufficient\nannotated detection datasets of nighttime scenes. In this paper, we propose a\nframework to alleviate the accuracy decline when object detection is taken to\nadverse conditions by using image translation method. We propose to utilize\nstyle translation based StyleMix method to acquire pairs of day time image and\nnighttime image as training data for following nighttime to daytime image\ntranslation. To alleviate the detail corruptions caused by Generative\nAdversarial Networks (GANs), we propose to utilize Kernel Prediction Network\n(KPN) based method to refine the nighttime to daytime image translation. The\nKPN network is trained with object detection task together to adapt the trained\ndaytime model to nighttime vehicle detection directly. Experiments on vehicle\ndetection verified the accuracy and effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:18:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fu", "Lan", ""], ["Yu", "Hongkai", ""], ["Juefei-Xu", "Felix", ""], ["Li", "Jinlong", ""], ["Guo", "Qing", ""], ["Wang", "Song", ""]]}, {"id": "2105.05013", "submitter": "Binhui Xie", "authors": "Shuang Li, Binhui Xie, Bin Zang, Chi Harold Liu, Xinjing Cheng,\n  Ruigang Yang and Guoren Wang", "title": "Semantic Distribution-aware Contrastive Adaptation for Semantic\n  Segmentation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive semantic segmentation refers to making predictions on a\ncertain target domain with only annotations of a specific source domain.\nCurrent state-of-the-art works suggest that performing category alignment can\nalleviate domain shift reasonably. However, they are mainly based on\nimage-to-image adversarial training and little consideration is given to\nsemantic variations of an object among images, failing to capture a\ncomprehensive picture of different categories. This motivates us to explore a\nholistic representative, the semantic distribution from each category in source\ndomain, to mitigate the problem above. In this paper, we present semantic\ndistribution-aware contrastive adaptation algorithm that enables pixel-wise\nrepresentation alignment under the guidance of semantic distributions.\nSpecifically, we first design a pixel-wise contrastive loss by considering the\ncorrespondences between semantic distributions and pixel-wise representations\nfrom both domains. Essentially, clusters of pixel representations from the same\ncategory should cluster together and those from different categories should\nspread out. Next, an upper bound on this formulation is derived by involving\nthe learning of an infinite number of (dis)similar pairs, making it efficient.\nFinally, we verify that SDCA can further improve segmentation accuracy when\nintegrated with the self-supervised learning. We evaluate SDCA on multiple\nbenchmarks, achieving considerable improvements over existing algorithms.The\ncode is publicly available at https://github.com/BIT-DA/SDCA\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:21:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Binhui", ""], ["Zang", "Bin", ""], ["Liu", "Chi Harold", ""], ["Cheng", "Xinjing", ""], ["Yang", "Ruigang", ""], ["Wang", "Guoren", ""]]}, {"id": "2105.05037", "submitter": "Zhongping Ji", "authors": "Zhongping Ji", "title": "BikNN: Anomaly Estimation in Bilateral Domains with k-Nearest Neighbors", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel framework for anomaly estimation is proposed. The\nbasic idea behind our method is to reduce the data into a two-dimensional space\nand then rank each data point in the reduced space. We attempt to estimate the\ndegree of anomaly in both spatial and density domains. Specifically, we\ntransform the data points into a density space and measure the distances in\ndensity domain between each point and its k-Nearest Neighbors in spatial\ndomain. Then, an anomaly coordinate system is built by collecting two\nunilateral anomalies from k-nearest neighbors of each point. Further more, we\nintroduce two schemes to model their correlation and combine them to get the\nfinal anomaly score. Experiments performed on the synthetic and real world\ndatasets demonstrate that the proposed method performs well and achieve highest\naverage performance. We also show that the proposed method can provide\nvisualization and classification of the anomalies in a simple manner. Due to\nthe complexity of the anomaly, none of the existing methods can perform best on\nall benchmark datasets. Our method takes into account both the spatial domain\nand the density domain and can be adapted to different datasets by adjusting a\nfew parameters manually.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:45:29 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ji", "Zhongping", ""]]}, {"id": "2105.05061", "submitter": "Ujjal Kr Dutta", "authors": "Ujjal Kr Dutta, Mehrtash Harandi, Chellu Chandra Sekhar", "title": "Semi-Supervised Metric Learning: A Deep Resurrection", "comments": "In AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Distance Metric Learning (DML) seeks to learn a discriminative embedding\nwhere similar examples are closer, and dissimilar examples are apart. In this\npaper, we address the problem of Semi-Supervised DML (SSDML) that tries to\nlearn a metric using a few labeled examples, and abundantly available unlabeled\nexamples. SSDML is important because it is infeasible to manually annotate all\nthe examples present in a large dataset. Surprisingly, with the exception of a\nfew classical approaches that learn a linear Mahalanobis metric, SSDML has not\nbeen studied in the recent years, and lacks approaches in the deep SSDML\nscenario. In this paper, we address this challenging problem, and revamp SSDML\nwith respect to deep learning. In particular, we propose a stochastic,\ngraph-based approach that first propagates the affinities between the pairs of\nexamples from labeled data, to that of the unlabeled pairs. The propagated\naffinities are used to mine triplet based constraints for metric learning. We\nimpose orthogonality constraint on the metric parameters, as it leads to a\nbetter performance by avoiding a model collapse.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:28:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dutta", "Ujjal Kr", ""], ["Harandi", "Mehrtash", ""], ["Sekhar", "Chellu Chandra", ""]]}, {"id": "2105.05066", "submitter": "Hacer Yalim Keles", "authors": "Ozge Mercanoglu Sincan, Julio C. S. Jacques Junior, Sergio Escalera,\n  Hacer Yalim Keles", "title": "ChaLearn LAP Large Scale Signer Independent Isolated Sign Language\n  Recognition Challenge: Design, Results and Future Research", "comments": "Preprint of the accepted paper at ChaLearn Looking at People Sign\n  Language Recognition in the Wild Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The performances of Sign Language Recognition (SLR) systems have improved\nconsiderably in recent years. However, several open challenges still need to be\nsolved to allow SLR to be useful in practice. The research in the field is in\nits infancy in regards to the robustness of the models to a large diversity of\nsigns and signers, and to fairness of the models to performers from different\ndemographics. This work summarises the ChaLearn LAP Large Scale Signer\nIndependent Isolated SLR Challenge, organised at CVPR 2021 with the goal of\novercoming some of the aforementioned challenges. We analyse and discuss the\nchallenge design, top winning solutions and suggestions for future research.\nThe challenge attracted 132 participants in the RGB track and 59 in the\nRGB+Depth track, receiving more than 1.5K submissions in total. Participants\nwere evaluated using a new large-scale multi-modal Turkish Sign Language\n(AUTSL) dataset, consisting of 226 sign labels and 36,302 isolated sign video\nsamples performed by 43 different signers. Winning teams achieved more than 96%\nrecognition rate, and their approaches benefited from pose/hand/face\nestimation, transfer learning, external data, fusion/ensemble of modalities and\ndifferent strategies to model spatio-temporal information. However, methods\nstill fail to distinguish among very similar signs, in particular those sharing\nsimilar hand trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:17:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Sincan", "Ozge Mercanoglu", ""], ["Junior", "Julio C. S. Jacques", ""], ["Escalera", "Sergio", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "2105.05090", "submitter": "Guangmiao Zeng", "authors": "Guangmiao Zeng, Wanneng Yu, Rongjie Wang and Anhui Lin", "title": "Research on Mosaic Image Data Enhancement for Overlapping Ship Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of overlapping occlusion in target recognition has been a\ndifficult research problem, and the situation of mutual occlusion of ship\ntargets in narrow waters still exists. In this paper, an improved mosaic data\nenhancement method is proposed, which optimizes the reading method of the data\nset, strengthens the learning ability of the detection algorithm for local\nfeatures, improves the recognition accuracy of overlapping targets while\nkeeping the test speed unchanged, reduces the decay rate of recognition ability\nunder different resolutions, and strengthens the robustness of the algorithm.\nThe real test experiments prove that, relative to the original algorithm, the\nimproved algorithm improves the recognition accuracy of overlapping targets by\n2.5%, reduces the target loss time by 17%, and improves the recognition\nstability under different video resolutions by 27.01%.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:44:03 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zeng", "Guangmiao", ""], ["Yu", "Wanneng", ""], ["Wang", "Rongjie", ""], ["Lin", "Anhui", ""]]}, {"id": "2105.05092", "submitter": "Gihan Jayatilaka", "authors": "Vu Tran, Gihan Jayatilaka, Ashwin Ashok, Archan Misra", "title": "DeepLight: Robust & Unobtrusive Real-time Screen-Camera Communication\n  for Real-World Displays", "comments": "Accepted for IPSN 2021 (ACM/IEEE International Conference on\n  Information Processing in Sensor Networks 2021)", "journal-ref": null, "doi": "10.1145/3412382.3458269", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a novel, holistic approach for robust Screen-Camera\nCommunication (SCC), where video content on a screen is visually encoded in a\nhuman-imperceptible fashion and decoded by a camera capturing images of such\nscreen content. We first show that state-of-the-art SCC techniques have two key\nlimitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly\nunder even modest screen extraction errors from the captured images, and (b)\nthey generate perceptible flickers on common refresh rate screens even with\nminimal modulation of pixel intensity. To overcome these challenges, we\nintroduce DeepLight, a system that incorporates machine learning (ML) models in\nthe decoding pipeline to achieve humanly-imperceptible, moderately high SCC\nrates under diverse real-world conditions. Deep-Light's key innovation is the\ndesign of a Deep Neural Network (DNN) based decoder that collectively decodes\nall the bits spatially encoded in a display frame, without attempting to\nprecisely isolate the pixels associated with each encoded bit. In addition,\nDeepLight supports imperceptible encoding by selectively modulating the\nintensity of only the Blue channel, and provides reasonably accurate screen\nextraction (IoU values >= 83%) by using state-of-the-art object detection DNN\npipelines. We show that a fully functional DeepLight system is able to robustly\nachieve high decoding accuracy (frame error rate < 0.2) and moderately-high\ndata goodput (>=0.95Kbps) using a human-held smartphone camera, even over\nlarger screen-camera distances (approx =2m).\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:44:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tran", "Vu", ""], ["Jayatilaka", "Gihan", ""], ["Ashok", "Ashwin", ""], ["Misra", "Archan", ""]]}, {"id": "2105.05137", "submitter": "Mohammad Haft-Javaherian", "authors": "Mohammad Haft-Javaherian, Martin Villiger, Kenichiro Otsuka, Joost\n  Daemen, Peter Libby, Polina Golland, and Brett E. Bouma", "title": "Segmentation of Anatomical Layers and Artifacts in Intravascular\n  Polarization Sensitive Optical Coherence Tomography Using Attending Physician\n  and Boundary Cardinality Lost Terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiovascular diseases are the leading cause of death and require a spectrum\nof diagnostic procedures as well as invasive interventions. Medical imaging is\na vital part of the healthcare system, facilitating both diagnosis and guidance\nfor intervention. Intravascular ultrasound and optical coherence tomography are\nwidely available for characterizing coronary stenoses and provide critical\nvessel parameters to optimize percutaneous intervention. Intravascular\npolarization-sensitive optical coherence tomography (PS-OCT) can simultaneously\nprovide high-resolution cross-sectional images of vascular structures while\nalso revealing preponderant tissue components such as collagen and smooth\nmuscle and thereby enhance plaque characterization. Automated interpretation of\nthese features would facilitate the objective clinical investigation of the\nnatural history and significance of coronary atheromas. Here, we propose a\nconvolutional neural network model and optimize its performance using a new\nmulti-term loss function to classify the lumen, intima, and media layers in\naddition to the guidewire and plaque artifacts. Our multi-class classification\nmodel outperforms the state-of-the-art methods in detecting the anatomical\nlayers based on accuracy, Dice coefficient, and average boundary error.\nFurthermore, the proposed model segments two classes of major artifacts and\ndetects the anatomical layers within the thickened vessel wall regions, which\nwere excluded from analysis by other studies. The source code and the trained\nmodel are publicly available at https://github.com/mhaft/OCTseg .\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:52:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Haft-Javaherian", "Mohammad", ""], ["Villiger", "Martin", ""], ["Otsuka", "Kenichiro", ""], ["Daemen", "Joost", ""], ["Libby", "Peter", ""], ["Golland", "Polina", ""], ["Bouma", "Brett E.", ""]]}, {"id": "2105.05145", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, Hod Lipson", "title": "Visual Perspective Taking for Opponent Behavior Modeling", "comments": "ICRA 2021. Website: http://www.cs.columbia.edu/~bchen/vpttob/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to engage in complex social interaction, humans learn at a young age\nto infer what others see and cannot see from a different point-of-view, and\nlearn to predict others' plans and behaviors. These abilities have been mostly\nlacking in robots, sometimes making them appear awkward and socially inept.\nHere we propose an end-to-end long-term visual prediction framework for robots\nto begin to acquire both these critical cognitive skills, known as Visual\nPerspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our\napproach in the context of visual hide-and-seek - a game that represents a\ncognitive milestone in human development. Unlike traditional visual predictive\nmodel that generates new frames from immediate past frames, our agent can\ndirectly predict to multiple future timestamps (25s), extrapolating by 175%\nbeyond the training horizon. We suggest that visual behavior modeling and\nperspective taking skills will play a critical role in the ability of physical\nrobots to fully integrate into real-world multi-agent activities. Our website\nis at http://www.cs.columbia.edu/~bchen/vpttob/.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:02:32 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Boyuan", ""], ["Hu", "Yuhang", ""], ["Kwiatkowski", "Robert", ""], ["Song", "Shuran", ""], ["Lipson", "Hod", ""]]}, {"id": "2105.05165", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Chun-Fu Chen, Quanfu Fan, Ximeng Sun, Kate Saenko,\n  Aude Oliva, Rogerio Feris", "title": "AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal learning, which focuses on utilizing various modalities to\nimprove the performance of a model, is widely used in video recognition. While\ntraditional multi-modal learning offers excellent recognition results, its\ncomputational expense limits its impact for many real-world applications. In\nthis paper, we propose an adaptive multi-modal learning framework, called\nAdaMML, that selects on-the-fly the optimal modalities for each segment\nconditioned on the input for efficient video recognition. Specifically, given a\nvideo segment, a multi-modal policy network is used to decide what modalities\nshould be used for processing by the recognition model, with the goal of\nimproving both accuracy and efficiency. We efficiently train the policy network\njointly with the recognition model using standard back-propagation. Extensive\nexperiments on four challenging diverse datasets demonstrate that our proposed\nadaptive approach yields 35%-55% reduction in computation when compared to the\ntraditional baseline that simply uses all the modalities irrespective of the\ninput, while also achieving consistent improvements in accuracy over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:19:07 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:49:10 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Panda", "Rameswar", ""], ["Chen", "Chun-Fu", ""], ["Fan", "Quanfu", ""], ["Sun", "Ximeng", ""], ["Saenko", "Kate", ""], ["Oliva", "Aude", ""], ["Feris", "Rogerio", ""]]}, {"id": "2105.05204", "submitter": "Marc Boubnovski Martell", "authors": "Marc Boubnovski Martell, Mitchell Chen, Kristofer Linton-Reid, Joram\n  M. Posma, Susan J Copley, Eric O. Aboagye", "title": "Development of a Multi-Task Learning V-Net for Pulmonary Lobar\n  Segmentation on Computed Tomography and Application to Diseased Lungs", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated lobar segmentation allows regional evaluation of lung disease and\nis important for diagnosis and therapy planning. Advanced statistical workflows\npermitting such evaluation is a needed area within respiratory medicine; their\nadoption remains slow, with poor workflow accuracy. Diseased lung regions often\nproduce high-density zones on CT images, limiting an algorithm's execution to\nspecify damaged lobes due to oblique or lacking fissures. This impact motivated\ndeveloping an improved machine learning method to segment lung lobes that\nutilises tracheobronchial tree information to enhance segmentation accuracy\nthrough the algorithm's spatial familiarity to define lobar extent more\naccurately. The method undertakes parallel segmentation of lobes and auxiliary\ntissues simultaneously by employing multi-task learning (MTL) in conjunction\nwith V-Net-attention, a popular convolutional neural network in the imaging\nrealm. In keeping with the model's adeptness for better generalisation, high\nperformance was retained in an external dataset of patients with four distinct\ndiseases: severe lung cancer, COVID-19 pneumonitis, collapsed lungs and Chronic\nObstructive Pulmonary Disease (COPD), even though the training data included\nnone of these cases. The benefit of our external validation test is\nspecifically relevant since our choice includes those patients who have\ndiagnosed lung disease with associated radiological abnormalities. To ensure\nequal rank is given to all segmentations in the main task we report the\nfollowing performance (Dice score) on a per-segment basis: normal lungs 0.97,\nCOPD 0.94, lung cancer 0.94, COVID-19 pneumonitis 0.94 and collapsed lung 0.92,\nall at p<0.05. Even segmenting lobes with large deformations on CT images, the\nmodel maintained high accuracy. The approach can be readily adopted in the\nclinical setting as a robust tool for radiologists.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:10:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Martell", "Marc Boubnovski", ""], ["Chen", "Mitchell", ""], ["Linton-Reid", "Kristofer", ""], ["Posma", "Joram M.", ""], ["Copley", "Susan J", ""], ["Aboagye", "Eric O.", ""]]}, {"id": "2105.05207", "submitter": "Yizhou Wang", "authors": "Yizhou Wang, Gaoang Wang, Hung-Min Hsu, Hui Liu, Jenq-Neng Hwang", "title": "Rethinking of Radar's Role: A Camera-Radar Dataset and Systematic\n  Annotator via Coordinate Alignment", "comments": "10 pages, 7 figures, 6 tables, CVPR 2021 Workshop on Autonomous\n  Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar has long been a common sensor on autonomous vehicles for obstacle\nranging and speed estimation. However, as a robust sensor to all-weather\nconditions, radar's capability has not been well-exploited, compared with\ncamera or LiDAR. Instead of just serving as a supplementary sensor, radar's\nrich information hidden in the radio frequencies can potentially provide useful\nclues to achieve more complicated tasks, like object classification and\ndetection. In this paper, we propose a new dataset, named CRUW, with a\nsystematic annotator and performance evaluation system to address the radar\nobject detection (ROD) task, which aims to classify and localize the objects in\n3D purely from radar's radio frequency (RF) images. To the best of our\nknowledge, CRUW is the first public large-scale dataset with a systematic\nannotation and evaluation system, which involves camera RGB images and radar RF\nimages, collected in various driving scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:13:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Yizhou", ""], ["Wang", "Gaoang", ""], ["Hsu", "Hung-Min", ""], ["Liu", "Hui", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "2105.05216", "submitter": "Ionut Mironica", "authors": "Andreea Birhala and Ionut Mironica", "title": "ReflectNet -- A Generative Adversarial Method for Single Image\n  Reflection Suppression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking pictures through glass windows almost always produces undesired\nreflections that degrade the quality of the photo. The ill-posed nature of the\nreflection removal problem reached the attention of many researchers for more\nthan decades. The main challenge of this problem is the lack of real training\ndata and the necessity of generating realistic synthetic data. In this paper,\nwe proposed a single image reflection removal method based on context\nunderstanding modules and adversarial training to efficiently restore the\ntransmission layer without reflection. We also propose a complex data\ngeneration model in order to create a large training set with various type of\nreflections. Our proposed reflection removal method outperforms\nstate-of-the-art methods in terms of PSNR and SSIM on the SIR benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:33:40 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Birhala", "Andreea", ""], ["Mironica", "Ionut", ""]]}, {"id": "2105.05217", "submitter": "Isma Hadji", "authors": "Isma Hadji, Konstantinos G. Derpanis, Allan D. Jepson", "title": "Representation Learning via Global Temporal Alignment and\n  Cycle-Consistency", "comments": "accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a weakly supervised method for representation learning based on\naligning temporal sequences (e.g., videos) of the same process (e.g., human\naction). The main idea is to use the global temporal ordering of latent\ncorrespondences across sequence pairs as a supervisory signal. In particular,\nwe propose a loss based on scoring the optimal sequence alignment to train an\nembedding network. Our loss is based on a novel probabilistic path finding view\nof dynamic time warping (DTW) that contains the following three key features:\n(i) the local path routing decisions are contrastive and differentiable, (ii)\npairwise distances are cast as probabilities that are contrastive as well, and\n(iii) our formulation naturally admits a global cycle consistency loss that\nverifies correspondences. For evaluation, we consider the tasks of fine-grained\naction classification, few shot learning, and video synchronization. We report\nsignificant performance increases over previous methods. In addition, we report\ntwo applications of our temporal alignment framework, namely 3D pose\nreconstruction and fine-grained audio/visual retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:34:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hadji", "Isma", ""], ["Derpanis", "Konstantinos G.", ""], ["Jepson", "Allan D.", ""]]}, {"id": "2105.05226", "submitter": "Nishant Rai", "authors": "Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka,\n  Shun Ishizaka, Ehsan Adeli, Juan Carlos Niebles", "title": "Home Action Genome: Cooperative Compositional Action Understanding", "comments": "CVPR '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing research on action recognition treats activities as monolithic\nevents occurring in videos. Recently, the benefits of formulating actions as a\ncombination of atomic-actions have shown promise in improving action\nunderstanding with the emergence of datasets containing such annotations,\nallowing us to learn representations capturing this information. However, there\nremains a lack of studies that extend action composition and leverage multiple\nviewpoints and multiple modalities of data for representation learning. To\npromote research in this direction, we introduce Home Action Genome (HOMAGE): a\nmulti-view action dataset with multiple modalities and view-points supplemented\nwith hierarchical activity and atomic action labels together with dense scene\ncomposition labels. Leveraging rich multi-modal and multi-view settings, we\npropose Cooperative Compositional Action Understanding (CCAU), a cooperative\nlearning framework for hierarchical action recognition that is aware of\ncompositional action elements. CCAU shows consistent performance improvements\nacross all modalities. Furthermore, we demonstrate the utility of co-learning\ncompositions in few-shot action recognition by achieving 28.6% mAP with just a\nsingle sample.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:42:47 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rai", "Nishant", ""], ["Chen", "Haofeng", ""], ["Ji", "Jingwei", ""], ["Desai", "Rishi", ""], ["Kozuka", "Kazuki", ""], ["Ishizaka", "Shun", ""], ["Adeli", "Ehsan", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "2105.05233", "submitter": "Prafulla Dhariwal", "authors": "Prafulla Dhariwal, Alex Nichol", "title": "Diffusion Models Beat GANs on Image Synthesis", "comments": "Added compute requirements, ImageNet 256$\\times$256 upsampling FID\n  and samples, DDIM guided sampler, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that diffusion models can achieve image sample quality superior to\nthe current state-of-the-art generative models. We achieve this on\nunconditional image synthesis by finding a better architecture through a series\nof ablations. For conditional image synthesis, we further improve sample\nquality with classifier guidance: a simple, compute-efficient method for\ntrading off diversity for fidelity using gradients from a classifier. We\nachieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet\n256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep\neven with as few as 25 forward passes per sample, all while maintaining better\ncoverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our\ncode at https://github.com/openai/guided-diffusion\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:50:24 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:57:59 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 17:57:08 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 17:49:49 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Dhariwal", "Prafulla", ""], ["Nichol", "Alex", ""]]}, {"id": "2105.05300", "submitter": "Mohamed Ali Souibgui", "authors": "Mohamed Ali Souibgui, Ali Furkan Biten, Sounak Dey, Alicia Forn\\'es,\n  Yousri Kessentini, Lluis Gomez, Dimosthenis Karatzas, Josep Llad\\'os", "title": "One-shot Compositional Data Generation for Low Resource Handwritten Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low resource Handwritten Text Recognition (HTR) is a hard problem due to the\nscarce annotated data and the very limited linguistic information (dictionaries\nand language models). This appears, for example, in the case of historical\nciphered manuscripts, which are usually written with invented alphabets to hide\nthe content. Thus, in this paper we address this problem through a data\ngeneration technique based on Bayesian Program Learning (BPL). Contrary to\ntraditional generation approaches, which require a huge amount of annotated\nimages, our method is able to generate human-like handwriting using only one\nsample of each symbol from the desired alphabet. After generating symbols, we\ncreate synthetic lines to train state-of-the-art HTR architectures in a\nsegmentation free fashion. Quantitative and qualitative analyses were carried\nout and confirm the effectiveness of the proposed method, achieving competitive\nresults compared to the usage of real annotated data.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:53:01 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Souibgui", "Mohamed Ali", ""], ["Biten", "Ali Furkan", ""], ["Dey", "Sounak", ""], ["Forn\u00e9s", "Alicia", ""], ["Kessentini", "Yousri", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""], ["Llad\u00f3s", "Josep", ""]]}, {"id": "2105.05301", "submitter": "Vasileios Choutas", "authors": "Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, Michael\n  J. Black", "title": "Collaborative Regression of Expressive Bodies using Moderation", "comments": "20 pages. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering expressive humans from images is essential for understanding human\nbehavior. Methods that estimate 3D bodies, faces, or hands have progressed\nsignificantly, yet separately. Face methods recover accurate 3D shape and\ngeometric details, but need a tight crop and struggle with extreme views and\nlow resolution. Whole-body methods are robust to a wide range of poses and\nresolutions, but provide only a rough 3D face shape without details like\nwrinkles. To get the best of both worlds, we introduce PIXIE, which produces\nanimatable, whole-body 3D avatars from a single image, with realistic facial\ndetail. To get accurate whole bodies, PIXIE uses two key observations. First,\nbody parts are correlated, but existing work combines independent estimates\nfrom body, face, and hand experts, by trusting them equally. PIXIE introduces a\nnovel moderator that merges the features of the experts, weighted by their\nconfidence. Uniquely, part experts can contribute to the whole, using SMPL-X's\nshared shape space across all body parts. Second, human shape is highly\ncorrelated with gender, but existing work ignores this. We label training\nimages as male, female, or non-binary, and train PIXIE to infer \"gendered\" 3D\nbody shapes with a novel shape loss. In addition to 3D body pose and shape\nparameters, PIXIE estimates expression, illumination, albedo and 3D surface\ndisplacements for the face. Quantitative and qualitative evaluation shows that\nPIXIE estimates 3D humans with a more accurate whole-body shape and detailed\nface shape than the state of the art. Our models and code are available for\nresearch at https://pixie.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:55:59 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Feng", "Yao", ""], ["Choutas", "Vasileios", ""], ["Bolkart", "Timo", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Michael J.", ""]]}, {"id": "2105.05312", "submitter": "Dan Ganea", "authors": "Dan Andrei Ganea, Bas Boom and Ronald Poppe", "title": "Incremental Few-Shot Instance Segmentation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot instance segmentation methods are promising when labeled training\ndata for novel classes is scarce. However, current approaches do not facilitate\nflexible addition of novel classes. They also require that examples of each\nclass are provided at train and test time, which is memory intensive. In this\npaper, we address these limitations by presenting the first incremental\napproach to few-shot instance segmentation: iMTFA. We learn discriminative\nembeddings for object instances that are merged into class representatives.\nStoring embedding vectors rather than images effectively solves the memory\noverhead problem. We match these class embeddings at the RoI-level using cosine\nsimilarity. This allows us to add new classes without the need for further\ntraining or access to previous training data. In a series of experiments, we\nconsistently outperform the current state-of-the-art. Moreover, the reduced\nmemory requirements allow us to evaluate, for the first time, few-shot instance\nsegmentation performance on all classes in COCO jointly.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:15:02 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ganea", "Dan Andrei", ""], ["Boom", "Bas", ""], ["Poppe", "Ronald", ""]]}, {"id": "2105.05318", "submitter": "Youssef Skandarani", "authors": "Youssef Skandarani, Pierre-Marc Jodoin, Alain Lalande", "title": "GANs for Medical Image Synthesis: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n  Results reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:21:39 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 11:51:58 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Skandarani", "Youssef", ""], ["Jodoin", "Pierre-Marc", ""], ["Lalande", "Alain", ""]]}, {"id": "2105.05320", "submitter": "Yiming Wang", "authors": "Yiming Wang, Dongxia Chang, Zhiqian Fu, and Yao Zhao", "title": "Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been considerable research interest in graph clustering\naimed at data partition using the graph information. However, one limitation of\nthe most of graph-based methods is that they assume the graph structure to\noperate is fixed and reliable. And there are inevitably some edges in the graph\nthat are not conducive to graph clustering, which we call spurious edges. This\npaper is the first attempt to employ graph pooling technique for node\nclustering and we propose a novel dual graph embedding network (DGEN), which is\ndesigned as a two-step graph encoder connected by a graph pooling layer to\nlearn the graph embedding. In our model, it is assumed that if a node and its\nnearest neighboring node are close to the same clustering center, this node is\nan informative node and this edge can be considered as a cluster-friendly edge.\nBased on this assumption, the neighbor cluster pooling (NCPool) is devised to\nselect the most informative subset of nodes and the corresponding edges based\non the distance of nodes and their nearest neighbors to the cluster centers.\nThis can effectively alleviate the impact of the spurious edges on the\nclustering. Finally, to obtain the clustering assignment of all nodes, a\nclassifier is trained using the clustering results of the selected nodes.\nExperiments on five benchmark graph datasets demonstrate the superiority of the\nproposed method over state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:51:51 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 02:51:53 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Yiming", ""], ["Chang", "Dongxia", ""], ["Fu", "Zhiqian", ""], ["Zhao", "Yao", ""]]}, {"id": "2105.05332", "submitter": "Ryan Szeto", "authors": "Ryan Szeto, Jason J. Corso", "title": "The DEVIL is in the Details: A Diagnostic Evaluation Benchmark for Video\n  Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative evaluation has increased dramatically among recent video\ninpainting work, but the video and mask content used to gauge performance has\nreceived relatively little attention. Although attributes such as camera and\nbackground scene motion inherently change the difficulty of the task and affect\nmethods differently, existing evaluation schemes fail to control for them,\nthereby providing minimal insight into inpainting failure modes. To address\nthis gap, we propose the Diagnostic Evaluation of Video Inpainting on\nLandscapes (DEVIL) benchmark, which consists of two contributions: (i) a novel\ndataset of videos and masks labeled according to several key inpainting failure\nmodes, and (ii) an evaluation scheme that samples slices of the dataset\ncharacterized by a fixed content attribute, and scores performance on each\nslice according to reconstruction, realism, and temporal consistency quality.\nBy revealing systematic changes in performance induced by particular\ncharacteristics of the input content, our challenging benchmark enables more\ninsightful analysis into video inpainting methods and serves as an invaluable\ndiagnostic tool for the field. Our code is available at\nhttps://github.com/MichiganCOG/devil .\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 20:13:53 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Szeto", "Ryan", ""], ["Corso", "Jason J.", ""]]}, {"id": "2105.05345", "submitter": "Jacob Carse Mr", "authors": "Jacob Carse, Frank Carey, Stephen McKenna", "title": "Unsupervised Representation Learning from Pathology Images with\n  Multi-directional Contrastive Predictive Coding", "comments": "5 pages, 4 figures, presented at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology tasks have benefited greatly from modern deep learning\nalgorithms. However, their need for large quantities of annotated data has been\nidentified as a key challenge. This need for data can be countered by using\nunsupervised learning in situations where data are abundant but access to\nannotations is limited. Feature representations learned from unannotated data\nusing contrastive predictive coding (CPC) have been shown to enable classifiers\nto obtain state of the art performance from relatively small amounts of\nannotated computer vision data. We present a modification to the CPC framework\nfor use with digital pathology patches. This is achieved by introducing an\nalternative mask for building the latent context and using a multi-directional\nPixelCNN autoregressor. To demonstrate our proposed method we learn feature\nrepresentations from the Patch Camelyon histology dataset. We show that our\nproposed modification can yield improved deep classification of histology\npatches.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:17:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Carse", "Jacob", ""], ["Carey", "Frank", ""], ["McKenna", "Stephen", ""]]}, {"id": "2105.05348", "submitter": "Xiangyu Chen", "authors": "Xiangyu Chen and Guanghui Wang", "title": "Few-Shot Learning by Integrating Spatial and Frequency Representation", "comments": "Accepted to CRV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings can recognize new objects with only a few labeled examples,\nhowever, few-shot learning remains a challenging problem for machine learning\nsystems. Most previous algorithms in few-shot learning only utilize spatial\ninformation of the images. In this paper, we propose to integrate the frequency\ninformation into the learning model to boost the discrimination ability of the\nsystem. We employ Discrete Cosine Transformation (DCT) to generate the\nfrequency representation, then, integrate the features from both the spatial\ndomain and frequency domain for classification. The proposed strategy and its\neffectiveness are validated with different backbones, datasets, and algorithms.\nExtensive experiments demonstrate that the frequency information is\ncomplementary to the spatial representations in few-shot classification. The\nclassification accuracy is boosted significantly by integrating features from\nboth the spatial and frequency domains in different few-shot learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:44:31 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 00:17:17 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chen", "Xiangyu", ""], ["Wang", "Guanghui", ""]]}, {"id": "2105.05353", "submitter": "Meng Cao", "authors": "Xi Li, Meng Cao, Yingying Tang, Scott Johnston, Zhendong Hong, Huimin\n  Ma, Jiulong Shan", "title": "Video Frame Interpolation via Structure-Motion based Iterative Fusion", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video Frame Interpolation synthesizes non-existent images between adjacent\nframes, with the aim of providing a smooth and consistent visual experience.\nTwo approaches for solving this challenging task are optical flow based and\nkernel-based methods. In existing works, optical flow based methods can provide\naccurate point-to-point motion description, however, they lack constraints on\nobject structure. On the contrary, kernel-based methods focus on structural\nalignment, which relies on semantic and apparent features, but tends to blur\nresults. Based on these observations, we propose a structure-motion based\niterative fusion method. The framework is an end-to-end learnable structure\nwith two stages. First, interpolated frames are synthesized by structure-based\nand motion-based learning branches respectively, then, an iterative refinement\nmodule is established via spatial and temporal feature integration. Inspired by\nthe observation that audiences have different visual preferences on foreground\nand background objects, we for the first time propose to use saliency masks in\nthe evaluation processes of the task of video frame interpolation. Experimental\nresults on three typical benchmarks show that the proposed method achieves\nsuperior performance on all evaluation metrics over the state-of-the-art\nmethods, even when our models are trained with only one-tenth of the data other\nmethods use.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 22:11:17 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Xi", ""], ["Cao", "Meng", ""], ["Tang", "Yingying", ""], ["Johnston", "Scott", ""], ["Hong", "Zhendong", ""], ["Ma", "Huimin", ""], ["Shan", "Jiulong", ""]]}, {"id": "2105.05403", "submitter": "Jinming Su", "authors": "Jinming Su, Chao Chen, Ke Zhang, Junfeng Luo, Xiaoming Wei and Xiaolin\n  Wei", "title": "Structure Guided Lane Detection", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, lane detection has made great progress with the rapid development\nof deep neural networks and autonomous driving. However, there exist three\nmainly problems including characterizing lanes, modeling the structural\nrelationship between scenes and lanes, and supporting more attributes (e.g.,\ninstance and type) of lanes. In this paper, we propose a novel structure guided\nframework to solve these problems simultaneously. In the framework, we first\nintroduce a new lane representation to characterize each instance. Then a\ntopdown vanishing point guided anchoring mechanism is proposed to produce\nintensive anchors, which efficiently capture various lanes. Next, multi-level\nstructural constraints are used to improve the perception of lanes. In the\nprocess, pixel-level perception with binary segmentation is introduced to\npromote features around anchors and restore lane details from bottom up, a\nlane-level relation is put forward to model structures (i.e., parallel) around\nlanes, and an image-level attention is used to adaptively attend different\nregions of the image from the perspective of scenes. With the help of\nstructural guidance, anchors are effectively classified and regressed to obtain\nprecise locations and shapes. Extensive experiments on public benchmark\ndatasets show that the proposed approach outperforms state-of-the-art methods\nwith 117 FPS on a single GPU.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 02:35:00 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 06:05:40 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Su", "Jinming", ""], ["Chen", "Chao", ""], ["Zhang", "Ke", ""], ["Luo", "Junfeng", ""], ["Wei", "Xiaoming", ""], ["Wei", "Xiaolin", ""]]}, {"id": "2105.05409", "submitter": "Xiongwei Wu", "authors": "Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven C.H. Hoi, Qianru\n  Sun", "title": "A Large-Scale Benchmark for Food Image Segmentation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food image segmentation is a critical and indispensible task for developing\nhealth-related applications such as estimating food calories and nutrients.\nExisting food image segmentation models are underperforming due to two reasons:\n(1) there is a lack of high quality food image datasets with fine-grained\ningredient labels and pixel-wise location masks -- the existing datasets either\ncarry coarse ingredient labels or are small in size; and (2) the complex\nappearance of food makes it difficult to localize and recognize ingredients in\nfood images, e.g., the ingredients may overlap one another in the same image,\nand the identical ingredient may appear distinctly in different food images. In\nthis work, we build a new food image dataset FoodSeg103 (and its extension\nFoodSeg154) containing 9,490 images. We annotate these images with 154\ningredient classes and each image has an average of 6 ingredient labels and\npixel-wise masks. In addition, we propose a multi-modality pre-training\napproach called ReLeM that explicitly equips a segmentation model with rich and\nsemantic food knowledge. In experiments, we use three popular semantic\nsegmentation methods (i.e., Dilated Convolution based, Feature Pyramid based,\nand Vision Transformer based) as baselines, and evaluate them as well as ReLeM\non our new datasets. We believe that the FoodSeg103 (and its extension\nFoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to\nfacilitate future works on fine-grained food image understanding. We make all\nthese datasets and methods public at\n\\url{https://xiongweiwu.github.io/foodseg103.html}.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 03:00:07 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wu", "Xiongwei", ""], ["Fu", "Xin", ""], ["Liu", "Ying", ""], ["Lim", "Ee-Peng", ""], ["Hoi", "Steven C. H.", ""], ["Sun", "Qianru", ""]]}, {"id": "2105.05455", "submitter": "Chuang Yang", "authors": "Chuang Yang, Mulin Chen, Yuan Yuan (Senior Member, IEEE), and Qi Wang\n  (Senior Member, IEEE)", "title": "MT: Multi-Perspective Feature Learning Network for Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection, the key technology for understanding scene text, has become\nan attractive research topic. For detecting various scene texts, researchers\npropose plenty of detectors with different advantages: detection-based models\nenjoy fast detection speed, and segmentation-based algorithms are not limited\nby text shapes. However, for most intelligent systems, the detector needs to\ndetect arbitrary-shaped texts with high speed and accuracy simultaneously.\nThus, in this study, we design an efficient pipeline named as MT, which can\ndetect adhesive arbitrary-shaped texts with only a single binary mask in the\ninference stage. This paper presents the contributions on three aspects: (1) a\nlight-weight detection framework is designed to speed up the inference process\nwhile keeping high detection accuracy; (2) a multi-perspective feature module\nis proposed to learn more discriminative representations to segment the mask\naccurately; (3) a multi-factor constraints IoU minimization loss is introduced\nfor training the proposed model. The effectiveness of MT is evaluated on four\nreal-world scene text datasets, and it surpasses all the state-of-the-art\ncompetitors to a large extent.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:41:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Yang", "Chuang", "", "Senior Member, IEEE"], ["Chen", "Mulin", "", "Senior Member, IEEE"], ["Yuan", "Yuan", "", "Senior Member, IEEE"], ["Wang", "Qi", "", "Senior Member, IEEE"]]}, {"id": "2105.05486", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba,\n  Tal Hassner", "title": "TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped\n  scene text", "comments": "To appear in CVPR 2021. 15 pages, 7 figures. First three authors\n  contributed equally. More info at https://textvqa.org/textocr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A crucial component for the scene text based reasoning required for TextVQA\nand TextCaps datasets involve detecting and recognizing text present in the\nimages using an optical character recognition (OCR) system. The current systems\nare crippled by the unavailability of ground truth text annotations for these\ndatasets as well as lack of scene text detection and recognition datasets on\nreal images disallowing the progress in the field of OCR and evaluation of\nscene text based reasoning in isolation from OCR systems. In this work, we\npropose TextOCR, an arbitrary-shaped scene text detection and recognition with\n900k annotated words collected on real images from TextVQA dataset. We show\nthat current state-of-the-art text-recognition (OCR) models fail to perform\nwell on TextOCR and that training on TextOCR helps achieve state-of-the-art\nperformance on multiple other OCR datasets as well. We use a TextOCR trained\nOCR model to create PixelM4C model which can do scene text based reasoning on\nan image in an end-to-end fashion, allowing us to revisit several design\nchoices to achieve new state-of-the-art performance on TextVQA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:50:42 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Singh", "Amanpreet", ""], ["Pang", "Guan", ""], ["Toh", "Mandy", ""], ["Huang", "Jing", ""], ["Galuba", "Wojciech", ""], ["Hassner", "Tal", ""]]}, {"id": "2105.05489", "submitter": "Shumao Zhang", "authors": "Shumao Zhang, Pengchuan Zhang, Thomas Y. Hou", "title": "Multiscale Invertible Generative Networks for High-Dimensional Bayesian\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multiscale Invertible Generative Network (MsIGN) and associated\ntraining algorithm that leverages multiscale structure to solve\nhigh-dimensional Bayesian inference. To address the curse of dimensionality,\nMsIGN exploits the low-dimensional nature of the posterior, and generates\nsamples from coarse to fine scale (low to high dimension) by iteratively\nupsampling and refining samples. MsIGN is trained in a multi-stage manner to\nminimize the Jeffreys divergence, which avoids mode dropping in\nhigh-dimensional cases. On two high-dimensional Bayesian inverse problems, we\nshow superior performance of MsIGN over previous approaches in posterior\napproximation and multiple mode capture. On the natural image synthesis task,\nMsIGN achieves superior performance in bits-per-dimension over baseline models\nand yields great interpret-ability of its neurons in intermediate layers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:51:47 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Shumao", ""], ["Zhang", "Pengchuan", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "2105.05496", "submitter": "Mahdyar Ravanbakhsh", "authors": "Ahmet Kerem Aksoy, Mahdyar Ravanbakhsh, Tristan Kreuziger, Begum Demir", "title": "A Consensual Collaborative Learning Method for Remote Sensing Image\n  Classification Under Noisy Multi-Labels", "comments": "Accepted in ICIP 2021. Our code is available at\n  https://noisy-labels-in-rs.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collecting a large number of reliable training images annotated by multiple\nland-cover class labels in the framework of multi-label classification is\ntime-consuming and costly in remote sensing (RS). To address this problem,\npublicly available thematic products are often used for annotating RS images\nwith zero-labeling-cost. However, such an approach may result in constructing a\ntraining set with noisy multi-labels, distorting the learning process. To\naddress this problem, we propose a Consensual Collaborative Multi-Label\nLearning (CCML) method. The proposed CCML identifies, ranks and corrects\ntraining images with noisy multi-labels through four main modules: 1)\ndiscrepancy module; 2) group lasso module; 3) flipping module; and 4) swap\nmodule. The discrepancy module ensures that the two networks learn diverse\nfeatures, while obtaining the same predictions. The group lasso module detects\nthe potentially noisy labels by estimating the label uncertainty based on the\naggregation of two collaborative networks. The flipping module corrects the\nidentified noisy labels, whereas the swap module exchanges the ranking\ninformation between the two networks. The experimental results confirm the\nsuccess of the proposed CCML under high (synthetically added) multi-label noise\nrates. The code of the proposed method is publicly available at\nhttps://noisy-labels-in-rs.org\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:06:14 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 10:18:22 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Aksoy", "Ahmet Kerem", ""], ["Ravanbakhsh", "Mahdyar", ""], ["Kreuziger", "Tristan", ""], ["Demir", "Begum", ""]]}, {"id": "2105.05497", "submitter": "Fan Yang", "authors": "Fan Yang, Guosheng Lin", "title": "CT-Net: Complementary Transfering Network for Garment Transfer with\n  Arbitrary Geometric Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Garment transfer shows great potential in realistic applications with the\ngoal of transfering outfits across different people images. However, garment\ntransfer between images with heavy misalignments or severe occlusions still\nremains as a challenge. In this work, we propose Complementary Transfering\nNetwork (CT-Net) to adaptively model different levels of geometric changes and\ntransfer outfits between different people. In specific, CT-Net consists of\nthree modules: 1) A complementary warping module first estimates two\ncomplementary warpings to transfer the desired clothes in different\ngranularities. 2) A layout prediction module is proposed to predict the target\nlayout, which guides the preservation or generation of the body parts in the\nsynthesized images. 3) A dynamic fusion module adaptively combines the\nadvantages of the complementary warpings to render the garment transfer\nresults. Extensive experiments conducted on DeepFashion dataset demonstrate\nthat our network synthesizes high-quality garment transfer images and\nsignificantly outperforms the state-of-art methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:07:07 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Yang", "Fan", ""], ["Lin", "Guosheng", ""]]}, {"id": "2105.05501", "submitter": "Binod Bhattarai", "authors": "Suman Sapkota, Bidur Khanal, Binod Bhattarai, Bishesh Khanal, Tae-Kyun\n  Kim", "title": "Label Geometry Aware Discriminator for Conditional Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-domain image-to-image translation with conditional Generative\nAdversarial Networks (GANs) can generate highly photo realistic images with\ndesired target classes, yet these synthetic images have not always been helpful\nto improve downstream supervised tasks such as image classification. Improving\ndownstream tasks with synthetic examples requires generating images with high\nfidelity to the unknown conditional distribution of the target class, which\nmany labeled conditional GANs attempt to achieve by adding soft-max\ncross-entropy loss based auxiliary classifier in the discriminator. As recent\nstudies suggest that the soft-max loss in Euclidean space of deep feature does\nnot leverage their intrinsic angular distribution, we propose to replace this\nloss in auxiliary classifier with an additive angular margin (AAM) loss that\ntakes benefit of the intrinsic angular distribution, and promotes intra-class\ncompactness and inter-class separation to help generator synthesize high\nfidelity images.\n  We validate our method on RaFD and CIFAR-100, two challenging face expression\nand natural image classification data set. Our method outperforms\nstate-of-the-art methods in several different evaluation criteria including\nrecently proposed GAN-train and GAN-test metrics designed to assess the impact\nof synthetic data on downstream classification task, assessing the usefulness\nin data augmentation for supervised tasks with prediction accuracy score and\naverage confidence score, and the well known FID metric.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:17:25 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sapkota", "Suman", ""], ["Khanal", "Bidur", ""], ["Bhattarai", "Binod", ""], ["Khanal", "Bishesh", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2105.05515", "submitter": "Polychronis Charitidis", "authors": "Polychronis Charitidis, Giorgos Kordopatis-Zilos, Symeon Papadopoulos,\n  Ioannis Kompatsiaris", "title": "Operation-wise Attention Network for Tampering Localization Fusion", "comments": "6 pages, 3 figures, cbmi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a deep learning-based approach for image tampering\nlocalization fusion. This approach is designed to combine the outcomes of\nmultiple image forensics algorithms and provides a fused tampering localization\nmap, which requires no expert knowledge and is easier to interpret by end\nusers. Our fusion framework includes a set of five individual tampering\nlocalization methods for splicing localization on JPEG images. The proposed\ndeep learning fusion model is an adapted architecture, initially proposed for\nthe image restoration task, that performs multiple operations in parallel,\nweighted by an attention mechanism to enable the selection of proper operations\ndepending on the input signals. This weighting process can be very beneficial\nfor cases where the input signal is very diverse, as in our case where the\noutput signals of multiple image forensics algorithms are combined. Evaluation\nin three publicly available forensics datasets demonstrates that the\nperformance of the proposed approach is competitive, outperforming the\nindividual forensics techniques as well as another recently proposed fusion\nframework in the majority of cases.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:50:59 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 10:01:46 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Charitidis", "Polychronis", ""], ["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2105.05516", "submitter": "Svetlana Illarionova", "authors": "Svetlana Illarionova, Sergey Nesteruk, Dmitrii Shadrin, Vladimir\n  Ignatiev, Mariia Pukalchik, Ivan Oseledets", "title": "Object-Based Augmentation Improves Quality of Remote SensingSemantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today deep convolutional neural networks (CNNs) push the limits for most\ncomputer vision problems, define trends, and set state-of-the-art results. In\nremote sensing tasks such as object detection and semantic segmentation, CNNs\nreach the SotA performance. However, for precise performance, CNNs require much\nhigh-quality training data. Rare objects and the variability of environmental\nconditions strongly affect prediction stability and accuracy. To overcome these\ndata restrictions, it is common to consider various approaches including data\naugmentation techniques. This study focuses on the development and testing of\nobject-based augmentation. The practical usefulness of the developed\naugmentation technique is shown in the remote sensing domain, being one of the\nmost demanded ineffective augmentation techniques. We propose a novel pipeline\nfor georeferenced image augmentation that enables a significant increase in the\nnumber of training samples. The presented pipeline is called object-based\naugmentation (OBA) and exploits objects' segmentation masks to produce new\nrealistic training scenes using target objects and various label-free\nbackgrounds. We test the approach on the buildings segmentation dataset with\nsix different CNN architectures and show that the proposed method benefits for\nall the tested models. We also show that further augmentation strategy\noptimization can improve the results. The proposed method leads to the\nmeaningful improvement of U-Net model predictions from 0.78 to 0.83 F1-score.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:54:55 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Illarionova", "Svetlana", ""], ["Nesteruk", "Sergey", ""], ["Shadrin", "Dmitrii", ""], ["Ignatiev", "Vladimir", ""], ["Pukalchik", "Mariia", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2105.05521", "submitter": "Deng Li", "authors": "Deng Li, Yue Wu and Yicong Zhou", "title": "SauvolaNet: Learning Adaptive Sauvola Network for Degraded Document\n  Binarization", "comments": "Submitted to 16th International Conference on Document Analysis and\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the classic Sauvola local image thresholding approach, we\nsystematically study it from the deep neural network (DNN) perspective and\npropose a new solution called SauvolaNet for degraded document binarization\n(DDB). It is composed of three explainable modules, namely, Multi-Window\nSauvola (MWS), Pixelwise Window Attention (PWA), and Adaptive Sauolva Threshold\n(AST). The MWS module honestly reflects the classic Sauvola but with trainable\nparameters and multi-window settings. The PWA module estimates the preferred\nwindow sizes for each pixel location. The AST module further consolidates the\noutputs from MWS and PWA and predicts the final adaptive threshold for each\npixel location. As a result, SauvolaNet becomes end-to-end trainable and\nsignificantly reduces the number of required network parameters to 40K -- it is\nonly 1\\% of MobileNetV2. In the meantime, it achieves the State-of-The-Art\n(SoTA) performance for the DDB task -- SauvolaNet is at least comparable to, if\nnot better than, SoTA binarization solutions in our extensive studies on the 13\npublic document binarization datasets. Our source code is available at\nhttps://github.com/Leedeng/SauvolaNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:56:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Deng", ""], ["Wu", "Yue", ""], ["Zhou", "Yicong", ""]]}, {"id": "2105.05528", "submitter": "Ioan-Adrian Cosma Mr.", "authors": "Adrian Cosma, Emilian Radoi", "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams", "comments": "9 pages, 7 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the hard challenges of real-world scenarios in which camera feeds\ncapture multiple people, who in most cases pass in front of the camera only\nonce. We address privacy concerns by using only the motion information of\nwalking individuals, with no identifiable appearance-based information. As\nsuch, we propose a novel weakly supervised learning framework, WildGait, which\nconsists of training a Spatio-Temporal Graph Convolutional Network on a large\nnumber of automatically annotated skeleton sequences obtained from raw,\nreal-world, surveillance streams to learn useful gait signatures. Our results\nshow that, with fine-tuning, we surpass in terms of recognition accuracy the\ncurrent state-of-the-art pose-based gait recognition solutions. Our proposed\nmethod is reliable in training gait recognition methods in unconstrained\nenvironments, especially in settings with scarce amounts of annotated data.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:11:32 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 08:20:01 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 15:20:03 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 15:28:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cosma", "Adrian", ""], ["Radoi", "Emilian", ""]]}, {"id": "2105.05530", "submitter": "Wenshuo Li", "authors": "Wenshuo Li, Hanting Chen, Mingqiang Huang, Xinghao Chen, Chunjing Xu,\n  Yunhe Wang", "title": "Winograd Algorithm for AdderNet", "comments": "9 pages, accepted by ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adder neural network (AdderNet) is a new kind of deep model that replaces the\noriginal massive multiplications in convolutions by additions while preserving\nthe high performance. Since the hardware complexity of additions is much lower\nthan that of multiplications, the overall energy consumption is thus reduced\nsignificantly. To further optimize the hardware overhead of using AdderNet,\nthis paper studies the winograd algorithm, which is a widely used fast\nalgorithm for accelerating convolution and saving the computational costs.\nUnfortunately, the conventional Winograd algorithm cannot be directly applied\nto AdderNets since the distributive law in multiplication is not valid for the\nl1-norm. Therefore, we replace the element-wise multiplication in the Winograd\nequation by additions and then develop a new set of transform matrixes that can\nenhance the representation ability of output features to maintain the\nperformance. Moreover, we propose the l2-to-l1 training strategy to mitigate\nthe negative impacts caused by formal inconsistency. Experimental results on\nboth FPGA and benchmarks show that the new method can further reduce the energy\nconsumption without affecting the accuracy of the original AdderNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:13:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Wenshuo", ""], ["Chen", "Hanting", ""], ["Huang", "Mingqiang", ""], ["Chen", "Xinghao", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2105.05537", "submitter": "Hu Cao", "authors": "Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi\n  Tian, Manning Wang", "title": "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation", "comments": "a drafted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, convolutional neural networks (CNNs) have achieved\nmilestones in medical image analysis. Especially, the deep neural networks\nbased on U-shaped architecture and skip-connections have been widely applied in\na variety of medical image tasks. However, although CNN has achieved excellent\nperformance, it cannot learn global and long-range semantic information\ninteraction well due to the locality of the convolution operation. In this\npaper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical\nimage segmentation. The tokenized image patches are fed into the\nTransformer-based U-shaped Encoder-Decoder architecture with skip-connections\nfor local-global semantic feature learning. Specifically, we use hierarchical\nSwin Transformer with shifted windows as the encoder to extract context\nfeatures. And a symmetric Swin Transformer-based decoder with patch expanding\nlayer is designed to perform the up-sampling operation to restore the spatial\nresolution of the feature maps. Under the direct down-sampling and up-sampling\nof the inputs and outputs by 4x, experiments on multi-organ and cardiac\nsegmentation tasks demonstrate that the pure Transformer-based U-shaped\nEncoder-Decoder network outperforms those methods with full-convolution or the\ncombination of transformer and convolution. The codes and trained models will\nbe publicly available at https://github.com/HuCaoFighting/Swin-Unet.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:30:26 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cao", "Hu", ""], ["Wang", "Yueyue", ""], ["Chen", "Joy", ""], ["Jiang", "Dongsheng", ""], ["Zhang", "Xiaopeng", ""], ["Tian", "Qi", ""], ["Wang", "Manning", ""]]}, {"id": "2105.05558", "submitter": "Qing Guo", "authors": "Binyu Tian and Felix Juefei-Xu and Qing Guo and Xiaofei Xie and\n  Xiaohong Li and Yang Liu", "title": "AVA: Adversarial Vignetting Attack against Visual Recognition", "comments": "This work has been accepted to IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vignetting is an inherited imaging phenomenon within almost all optical\nsystems, showing as a radial intensity darkening toward the corners of an\nimage. Since it is a common effect for photography and usually appears as a\nslight intensity variation, people usually regard it as a part of a photo and\nwould not even want to post-process it. Due to this natural advantage, in this\nwork, we study vignetting from a new viewpoint, i.e., adversarial vignetting\nattack (AVA), which aims to embed intentionally misleading information into\nvignetting and produce a natural adversarial example without noise patterns.\nThis example can fool the state-of-the-art deep convolutional neural networks\n(CNNs) but is imperceptible to humans. To this end, we first propose the\nradial-isotropic adversarial vignetting attack (RI-AVA) based on the physical\nmodel of vignetting, where the physical parameters (e.g., illumination factor\nand focal length) are tuned through the guidance of target CNN models. To\nachieve higher transferability across different CNNs, we further propose\nradial-anisotropic adversarial vignetting attack (RA-AVA) by allowing the\neffective regions of vignetting to be radial-anisotropic and shape-free.\nMoreover, we propose the geometry-aware level-set optimization method to solve\nthe adversarial vignetting regions and physical parameters jointly. We validate\nthe proposed methods on three popular datasets, i.e., DEV, CIFAR10, and Tiny\nImageNet, by attacking four CNNs, e.g., ResNet50, EfficientNet-B0, DenseNet121,\nand MobileNet-V2, demonstrating the advantages of our methods over baseline\nmethods on both transferability and image quality.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:18:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tian", "Binyu", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Li", "Xiaohong", ""], ["Liu", "Yang", ""]]}, {"id": "2105.05592", "submitter": "Shimon Ullman", "authors": "Shimon Ullman, Liav Assif, Alona Strugatski, Ben-Zion Vatashsky, Hila\n  Levy, Aviv Netanyahu, Adam Yaari", "title": "Image interpretation by iterative bottom-up top-down processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene understanding requires the extraction and representation of scene\ncomponents together with their properties and inter-relations. We describe a\nmodel in which meaningful scene structures are extracted from the image by an\niterative process, combining bottom-up (BU) and top-down (TD) networks,\ninteracting through a symmetric bi-directional communication between them\n(counter-streams structure). The model constructs a scene representation by the\niterative use of three components. The first model component is a BU stream\nthat extracts selected scene elements, properties and relations. The second\ncomponent (cognitive augmentation) augments the extracted visual representation\nbased on relevant non-visual stored representations. It also provides input to\nthe third component, the TD stream, in the form of a TD instruction,\ninstructing the model what task to perform next. The TD stream then guides the\nBU visual stream to perform the selected task in the next cycle. During this\nprocess, the visual representations extracted from the image can be combined\nwith relevant non-visual representations, so that the final scene\nrepresentation is based on both visual information extracted from the scene and\nrelevant stored knowledge of the world. We describe how a sequence of\nTD-instructions is used to extract from the scene structures of interest,\nincluding an algorithm to automatically select the next TD-instruction in the\nsequence. The extraction process is shown to have favorable properties in terms\nof combinatorial generalization, generalizing well to novel scene structures\nand new combinations of objects, properties and relations not seen during\ntraining. Finally, we compare the model with relevant aspects of the human\nvision, and suggest directions for using the BU-TD scheme for integrating\nvisual and cognitive components in the process of scene understanding.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:10:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ullman", "Shimon", ""], ["Assif", "Liav", ""], ["Strugatski", "Alona", ""], ["Vatashsky", "Ben-Zion", ""], ["Levy", "Hila", ""], ["Netanyahu", "Aviv", ""], ["Yaari", "Adam", ""]]}, {"id": "2105.05600", "submitter": "Jiazhao Zhang", "authors": "Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, Kai Xu", "title": "ROSEFusion: Random Optimization for Online Dense Reconstruction under\n  Fast Camera Motion", "comments": "Accepted by SIGGRAPH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online reconstruction based on RGB-D sequences has thus far been restrained\nto relatively slow camera motions (<1m/s). Under very fast camera motion (e.g.,\n3m/s), the reconstruction can easily crumble even for the state-of-the-art\nmethods. Fast motion brings two challenges to depth fusion: 1) the high\nnonlinearity of camera pose optimization due to large inter-frame rotations and\n2) the lack of reliably trackable features due to motion blur. We propose to\ntackle the difficulties of fast-motion camera tracking in the absence of\ninertial measurements using random optimization, in particular, the Particle\nFilter Optimization (PFO). To surmount the computation-intensive particle\nsampling and update in standard PFO, we propose to accelerate the randomized\nsearch via updating a particle swarm template (PST). PST is a set of particles\npre-sampled uniformly within the unit sphere in the 6D space of camera pose.\nThrough moving and rescaling the pre-sampled PST guided by swarm intelligence,\nour method is able to drive tens of thousands of particles to locate and cover\na good local optimum extremely fast and robustly. The particles, representing\ncandidate poses, are evaluated with a fitness function defined based on\ndepth-model conformance. Therefore, our method, being depth-only and\ncorrespondence-free, mitigates the motion blur impediment as ToF-based depths\nare often resilient to motion blur. Thanks to the efficient template-based\nparticle set evolution and the effective fitness function, our method attains\ngood quality pose tracking under fast camera motion (up to 4m/s) in a realtime\nframerate without including loop closure or global pose optimization. Through\nextensive evaluations on public datasets of RGB-D sequences, especially on a\nnewly proposed benchmark of fast camera motion, we demonstrate the significant\nadvantage of our method over the state of the arts.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:37:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Jiazhao", ""], ["Zhu", "Chenyang", ""], ["Zheng", "Lintao", ""], ["Xu", "Kai", ""]]}, {"id": "2105.05609", "submitter": "Jos\\'e Mennesson", "authors": "Sami Barchid, Jos\\'e Mennesson, Chaabane Dj\\'eraba", "title": "Deep Spiking Convolutional Neural Network for Single Object Localization\n  Based On Deep Continuous Local Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of neuromorphic hardware, spiking neural networks can be a\ngood energy-efficient alternative to artificial neural networks. However, the\nuse of spiking neural networks to perform computer vision tasks remains\nlimited, mainly focusing on simple tasks such as digit recognition. It remains\nhard to deal with more complex tasks (e.g. segmentation, object detection) due\nto the small number of works on deep spiking neural networks for these tasks.\nThe objective of this paper is to make the first step towards modern computer\nvision with supervised spiking neural networks. We propose a deep convolutional\nspiking neural network for the localization of a single object in a grayscale\nimage. We propose a network based on DECOLLE, a spiking model that enables\nlocal surrogate gradient-based learning. The encouraging results reported on\nOxford-IIIT-Pet validates the exploitation of spiking neural networks with a\nsupervised learning approach for more elaborate vision tasks in the future.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:02:05 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Barchid", "Sami", ""], ["Mennesson", "Jos\u00e9", ""], ["Dj\u00e9raba", "Chaabane", ""]]}, {"id": "2105.05612", "submitter": "Damien Teney", "authors": "Damien Teney, Ehsan Abbasnejad, Simon Lucey, Anton van den Hengel", "title": "Evading the Simplicity Bias: Training a Diverse Set of Models Discovers\n  Solutions with Superior OOD Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks trained with SGD were recently shown to rely preferentially\non linearly-predictive features and can ignore complex, equally-predictive\nones. This simplicity bias can explain their lack of robustness out of\ndistribution (OOD). The more complex the task to learn, the more likely it is\nthat statistical artifacts (i.e. selection biases, spurious correlations) are\nsimpler than the mechanisms to learn. We demonstrate that the simplicity bias\ncan be mitigated and OOD generalization improved. We train a set of similar\nmodels to fit the data in different ways using a penalty on the alignment of\ntheir input gradients. We show theoretically and empirically that this induces\nthe learning of more complex predictive patterns. OOD generalization\nfundamentally requires information beyond i.i.d. examples, such as multiple\ntraining environments, counterfactual examples, or other side information. Our\napproach shows that we can defer this requirement to an independent model\nselection stage. We obtain SOTA results in visual recognition on biased data\nand generalization across visual domains. The method - the first to evade the\nsimplicity bias - highlights the need for a better understanding and control of\ninductive biases in deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:12:24 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 08:49:35 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Teney", "Damien", ""], ["Abbasnejad", "Ehsan", ""], ["Lucey", "Simon", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2105.05631", "submitter": "Maysam Behmanesh", "authors": "Maysam Behmanesh, Peyman Adibi, Jocelyn Chanussot, Sayyed Mohammad\n  Saeed Ehsani", "title": "Cross-Modal and Multimodal Data Analysis Based on Functional Mapping of\n  Spectral Descriptors and Manifold Regularization", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal manifold modeling methods extend the spectral geometry-aware data\nanalysis to learning from several related and complementary modalities. Most of\nthese methods work based on two major assumptions: 1) there are the same number\nof homogeneous data samples in each modality, and 2) at least partial\ncorrespondences between modalities are given in advance as prior knowledge.\nThis work proposes two new multimodal modeling methods. The first method\nestablishes a general analyzing framework to deal with the multimodal\ninformation problem for heterogeneous data without any specific prior\nknowledge. For this purpose, first, we identify the localities of each manifold\nby extracting local descriptors via spectral graph wavelet signatures (SGWS).\nThen, we propose a manifold regularization framework based on the functional\nmapping between SGWS descriptors (FMBSD) for finding the pointwise\ncorrespondences. The second method is a manifold regularized multimodal\nclassification based on pointwise correspondences (M$^2$CPC) used for the\nproblem of multiclass classification of multimodal heterogeneous, which the\ncorrespondences between modalities are determined based on the FMBSD method.\nThe experimental results of evaluating the FMBSD method on three common\ncross-modal retrieval datasets and evaluating the (M$^2$CPC) method on three\nbenchmark multimodal multiclass classification datasets indicate their\neffectiveness and superiority over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:00:33 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Behmanesh", "Maysam", ""], ["Adibi", "Peyman", ""], ["Chanussot", "Jocelyn", ""], ["Ehsani", "Sayyed Mohammad Saeed", ""]]}, {"id": "2105.05633", "submitter": "Robin Strudel", "authors": "Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid", "title": "Segmenter: Transformer for Semantic Segmentation", "comments": "Code available at https://github.com/rstrudel/segmenter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution based approaches, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on the challenging\nADE20K dataset and performs on-par on Pascal Context and Cityscapes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:01:44 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Strudel", "Robin", ""], ["Garcia", "Ricardo", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2105.05636", "submitter": "Wenbo Ma", "authors": "Wenbo Ma, Long Chen, Hanwang Zhang, Jian Shao, Yueting Zhuang, Jun\n  Xiao", "title": "VL-NMS: Breaking Proposal Bottlenecks in Two-Stage Visual-Language\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevailing framework for matching multimodal inputs is based on a\ntwo-stage process: 1) detecting proposals with an object detector and 2)\nmatching text queries with proposals. Existing two-stage solutions mostly focus\non the matching step. In this paper, we argue that these methods overlook an\nobvious \\emph{mismatch} between the roles of proposals in the two stages: they\ngenerate proposals solely based on the detection confidence (i.e.,\nquery-agnostic), hoping that the proposals contain all instances mentioned in\nthe text query (i.e., query-aware). Due to this mismatch, chances are that\nproposals relevant to the text query are suppressed during the filtering\nprocess, which in turn bounds the matching performance. To this end, we propose\nVL-NMS, which is the first method to yield query-aware proposals at the first\nstage. VL-NMS regards all mentioned instances as critical objects, and\nintroduces a lightweight module to predict a score for aligning each proposal\nwith a critical object. These scores can guide the NMS operation to filter out\nproposals irrelevant to the text query, increasing the recall of critical\nobjects, resulting in a significantly improved matching performance. Since\nVL-NMS is agnostic to the matching step, it can be easily integrated into any\nstate-of-the-art two-stage matching methods. We validate the effectiveness of\nVL-NMS on two multimodal matching tasks, namely referring expression grounding\nand image-text matching. Extensive ablation studies on several baselines and\nbenchmarks consistently demonstrate the superiority of VL-NMS.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:05:25 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ma", "Wenbo", ""], ["Chen", "Long", ""], ["Zhang", "Hanwang", ""], ["Shao", "Jian", ""], ["Zhuang", "Yueting", ""], ["Xiao", "Jun", ""]]}, {"id": "2105.05639", "submitter": "Xingyang Ni", "authors": "Xingyang Ni, Esa Rahtu", "title": "FlipReID: Closing the Gap between Training and Inference in Person\n  Re-Identification", "comments": "First Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since neural networks are data-hungry, incorporating data augmentation in\ntraining is a widely adopted technique that enlarges datasets and improves\ngeneralization. On the other hand, aggregating predictions of multiple\naugmented samples (i.e., test-time augmentation) could boost performance even\nfurther. In the context of person re-identification models, it is common\npractice to extract embeddings for both the original images and their\nhorizontally flipped variants. The final representation is the mean of the\naforementioned feature vectors. However, such scheme results in a gap between\ntraining and inference, i.e., the mean feature vectors calculated in inference\nare not part of the training pipeline. In this study, we devise the FlipReID\nstructure with the flipping loss to address this issue. More specifically,\nmodels using the FlipReID structure are trained on the original images and the\nflipped images simultaneously, and incorporating the flipping loss minimizes\nthe mean squared error between feature vectors of corresponding image pairs.\nExtensive experiments show that our method brings consistent improvements. In\nparticular, we set a new record for MSMT17 which is the largest person\nre-identification dataset. The source code is available at\nhttps://github.com/nixingyang/FlipReID.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:14:01 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ni", "Xingyang", ""], ["Rahtu", "Esa", ""]]}, {"id": "2105.05640", "submitter": "Jiayi Lin", "authors": "Jiayi Lin, Yan Huang, Liang Wang", "title": "FDAN: Flow-guided Deformable Alignment Network for Video\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Video Super-Resolution (VSR) methods enhance a video reference frame by\naligning its neighboring frames and mining information on these frames.\nRecently, deformable alignment has drawn extensive attention in VSR community\nfor its remarkable performance, which can adaptively align neighboring frames\nwith the reference one. However, we experimentally find that deformable\nalignment methods still suffer from fast motion due to locally loss-driven\noffset prediction and lack explicit motion constraints. Hence, we propose a\nMatching-based Flow Estimation (MFE) module to conduct global semantic feature\nmatching and estimate optical flow as coarse offset for each location. And a\nFlow-guided Deformable Module (FDM) is proposed to integrate optical flow into\ndeformable convolution. The FDM uses the optical flow to warp the neighboring\nframes at first. And then, the warped neighboring frames and the reference one\nare used to predict a set of fine offsets for each coarse offset. In general,\nwe propose an end-to-end deep network called Flow-guided Deformable Alignment\nNetwork (FDAN), which reaches the state-of-the-art performance on two benchmark\ndatasets while is still competitive in computation and memory consumption.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:18:36 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Lin", "Jiayi", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""]]}, {"id": "2105.05643", "submitter": "Yang Xiao", "authors": "Yang Xiao, Yuming Du, Renaud Marlet", "title": "PoseContrast: Class-Agnostic Object Viewpoint Estimation in the Wild\n  with Pose-Aware Contrastive Learning", "comments": "Under Review. See project webpage\n  http://imagine.enpc.fr/~xiaoy/PoseContrast/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need of estimating the pose (viewpoint) of arbitrary objects\nin the wild, which is only covered by scarce and small datasets, we consider\nthe challenging problem of class-agnostic 3D object pose estimation, with no 3D\nshape knowledge. The idea is to leverage features learned on seen classes to\nestimate the pose for classes that are unseen, yet that share similar\ngeometries and canonical frames with seen classes. For this, we train a direct\npose estimator in a class-agnostic way by sharing weights across all object\nclasses, and we introduce a contrastive learning method that has three main\ningredients: (i) the use of pre-trained, self-supervised, contrast-based\nfeatures; (ii) pose-aware data augmentations; (iii) a pose-aware contrastive\nloss. We experimented on Pascal3D+ and ObjectNet3D, as well as Pix3D in a\ncross-dataset fashion, with both seen and unseen classes. We report\nstate-of-the-art results, including against methods that use additional shape\ninformation, and also when we use detected bounding boxes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:21:24 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xiao", "Yang", ""], ["Du", "Yuming", ""], ["Marlet", "Renaud", ""]]}, {"id": "2105.05708", "submitter": "Walid Hariri", "authors": "Walid Hariri, Nadir Farah, Dinesh Kumar Vishwakarma", "title": "Deep and Shallow Covariance Feature Quantization for 3D Facial\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial expressions recognition (FER) of 3D face scans has received a\nsignificant amount of attention in recent years. Most of the facial expression\nrecognition methods have been proposed using mainly 2D images. These methods\nsuffer from several issues like illumination changes and pose variations.\nMoreover, 2D mapping from 3D images may lack some geometric and topological\ncharacteristics of the face. Hence, to overcome this problem, a multi-modal 2D\n+ 3D feature-based method is proposed. We extract shallow features from the 3D\nimages, and deep features using Convolutional Neural Networks (CNN) from the\ntransformed 2D images. Combining these features into a compact representation\nuses covariance matrices as descriptors for both features instead of\nsingle-handedly descriptors. A covariance matrix learning is used as a manifold\nlayer to reduce the deep covariance matrices size and enhance their\ndiscrimination power while preserving their manifold structure. We then use the\nBag-of-Features (BoF) paradigm to quantize the covariance matrices after\nflattening. Accordingly, we obtained two codebooks using shallow and deep\nfeatures. The global codebook is then used to feed an SVM classifier. High\nclassification performances have been achieved on the BU-3DFE and Bosphorus\ndatasets compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:48:39 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Hariri", "Walid", ""], ["Farah", "Nadir", ""], ["Vishwakarma", "Dinesh Kumar", ""]]}, {"id": "2105.05712", "submitter": "Shradha Agrawal", "authors": "Shradha Agrawal, Shankar Venkitachalam, Dhanya Raghu, Deepak Pai", "title": "Directional GAN: A Novel Conditioning Strategy for Generative Networks", "comments": "Accepted to AICC workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image content is a predominant factor in marketing campaigns, websites and\nbanners. Today, marketers and designers spend considerable time and money in\ngenerating such professional quality content. We take a step towards\nsimplifying this process using Generative Adversarial Networks (GANs). We\npropose a simple and novel conditioning strategy which allows generation of\nimages conditioned on given semantic attributes using a generator trained for\nan unconditional image generation task. Our approach is based on modifying\nlatent vectors, using directional vectors of relevant semantic attributes in\nlatent space. Our method is designed to work with both discrete (binary and\nmulti-class) and continuous image attributes. We show the applicability of our\nproposed approach, named Directional GAN, on multiple public datasets, with an\naverage accuracy of 86.4% across different attributes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:02:41 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 22:04:31 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Agrawal", "Shradha", ""], ["Venkitachalam", "Shankar", ""], ["Raghu", "Dhanya", ""], ["Pai", "Deepak", ""]]}, {"id": "2105.05758", "submitter": "Mohammadsadegh Saberian", "authors": "M.Sadegh Saberian, Kathleen P. Moriarty, Andrea D. Olmstead, Ivan R.\n  Nabi, Fran\\c{c}ois Jean, Maxwell W. Libbrecht, Ghassan Hamarneh", "title": "DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell\n  Morphology with Deep multiple instance learning", "comments": "Supplementary material is appended to the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Drug repurposing can accelerate the identification of effective compounds for\nclinical use against SARS-CoV-2, with the advantage of pre-existing clinical\nsafety data and an established supply chain. RNA viruses such as SARS-CoV-2\nmanipulate cellular pathways and induce reorganization of subcellular\nstructures to support their life cycle. These morphological changes can be\nquantified using bioimaging techniques. In this work, we developed DEEMD: a\ncomputational pipeline using deep neural network models within a multiple\ninstance learning (MIL) framework, to identify putative treatments effective\nagainst SARS-CoV-2 based on morphological analysis of the publicly available\nRxRx19a dataset. This dataset consists of fluorescence microscopy images of\nSARS-CoV-2 non-infected cells and infected cells, with and without drug\ntreatment. DEEMD first extracts discriminative morphological features to\ngenerate cell morphological profiles from the non-infected and infected cells.\nThese morphological profiles are then used in a statistical model to estimate\nthe applied treatment efficacy on infected cells based on similarities to\nnon-infected cells. DEEMD is capable of localizing infected cells via weak\nsupervision without any expensive pixel-level annotations. DEEMD identifies\nknown SARS-CoV-2 inhibitors, such as Remdesivir and Aloxistatin, supporting the\nvalidity of our approach. DEEMD is scalable to process and screen thousands of\ntreatments in parallel and can be explored for other emerging viruses and\ndatasets to rapidly identify candidate antiviral treatments in the future.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:38:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Saberian", "M. Sadegh", ""], ["Moriarty", "Kathleen P.", ""], ["Olmstead", "Andrea D.", ""], ["Nabi", "Ivan R.", ""], ["Jean", "Fran\u00e7ois", ""], ["Libbrecht", "Maxwell W.", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2105.05787", "submitter": "Ionut Mironica", "authors": "Ionut Mironica and Andrei Zugravu", "title": "A Fast Deep Learning Network for Automatic Image Auto-Straightening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rectifying the orientation of images represents a daily task for every\nphotographer. This task may be complicated even for the human eye, especially\nwhen the horizon or other horizontal and vertical lines in the image are\nmissing. In this paper we address this problem and propose a new deep learning\nnetwork specially adapted for image rotation correction: we introduce the\nrectangle-shaped depthwise convolutions which are specialized in detecting long\nlines from the image and a new adapted loss function that addresses the problem\nof orientation errors.\n  Compared to other methods that are able to detect rotation errors only on few\nimage categories, like man-made structures, the proposed method can be used on\na larger variety of photographs e.g., portraits, landscapes, sport, night\nphotos etc. Moreover, the model is adapted to mobile devices and can be run in\nreal time, both for pictures and for videos. An extensive evaluation of our\nmodel on different datasets shows that it remarkably generalizes, not being\ndependent on any particular type of image. Finally, we significantly outperform\nthe state-of-the-art methods, providing superior results.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:01:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Mironica", "Ionut", ""], ["Zugravu", "Andrei", ""]]}, {"id": "2105.05794", "submitter": "Tiago Roxo", "authors": "Tiago Roxo and Hugo Proen\\c{c}a", "title": "Is Gender \"In-the-Wild\" Inference Really a Solved Problem?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Soft biometrics analysis is seen as an important research topic, given its\nrelevance to various applications. However, even though it is frequently seen\nas a solved task, it can still be very hard to perform in wild conditions,\nunder varying image conditions, uncooperative poses, and occlusions.\nConsidering the gender trait as our topic of study, we report an extensive\nanalysis of the feasibility of its inference regarding image (resolution,\nluminosity, and blurriness) and subject-based features (face and body keypoints\nconfidence). Using three state-of-the-art datasets (PETA, PA-100K, RAP) and\nfive Person Attribute Recognition models, we correlate feature analysis with\ngender inference accuracy using the Shapley value, enabling us to perceive the\nimportance of each image/subject-based feature. Furthermore, we analyze\nface-based gender inference and assess the pose effect on it. Our results\nsuggest that: 1) image-based features are more influential for low-quality\ndata; 2) an increase in image quality translates into higher subject-based\nfeature importance; 3) face-based gender inference accuracy correlates with\nimage quality increase; and 4) subjects' frontal pose promotes an implicit\nattention towards the face. The reported results are seen as a basis for\nsubsequent developments of inference approaches in uncontrolled outdoor\nenvironments, which typically correspond to visual surveillance conditions.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:05:03 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Roxo", "Tiago", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2105.05824", "submitter": "Juiwen Ting", "authors": "Juiwen Ting, Xuesong Wu, Kangkang Hu, Hong Zhang", "title": "Deep Snapshot HDR Reconstruction Based on the Polarization Camera", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent development of the on-chip micro-polarizer technology has made it\npossible to acquire four spatially aligned and temporally synchronized\npolarization images with the same ease of operation as a conventional camera.\nIn this paper, we investigate the use of this sensor technology in\nhigh-dynamic-range (HDR) imaging. Specifically, observing that natural light\ncan be attenuated differently by varying the orientation of the polarization\nfilter, we treat the multiple images captured by the polarization camera as a\nset captured under different exposure times. In our approach, we first study\nthe relationship among polarizer orientation, degree and angle of polarization\nof light to the exposure time of a pixel in the polarization image.\nSubsequently, we propose a deep snapshot HDR reconstruction framework to\nrecover an HDR image using the polarization images. A polarized HDR dataset is\ncreated to train and evaluate our approach. We demonstrate that our approach\nperforms favorably against state-of-the-art HDR reconstruction algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:35:10 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ting", "Juiwen", ""], ["Wu", "Xuesong", ""], ["Hu", "Kangkang", ""], ["Zhang", "Hong", ""]]}, {"id": "2105.05827", "submitter": "Omer Demirel", "authors": "Omer Burak Demirel, Burhaneddin Yaman, Logan Dowdle, Steen Moeller,\n  Luca Vizioli, Essa Yacoub, John Strupp, Cheryl A. Olman, K\\^amil U\\u{g}urbil\n  and Mehmet Ak\\c{c}akaya", "title": "20-fold Accelerated 7T fMRI Using Referenceless Self-Supervised Deep\n  Learning Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High spatial and temporal resolution across the whole brain is essential to\naccurately resolve neural activities in fMRI. Therefore, accelerated imaging\ntechniques target improved coverage with high spatio-temporal resolution.\nSimultaneous multi-slice (SMS) imaging combined with in-plane acceleration are\nused in large studies that involve ultrahigh field fMRI, such as the Human\nConnectome Project. However, for even higher acceleration rates, these methods\ncannot be reliably utilized due to aliasing and noise artifacts. Deep learning\n(DL) reconstruction techniques have recently gained substantial interest for\nimproving highly-accelerated MRI. Supervised learning of DL reconstructions\ngenerally requires fully-sampled training datasets, which is not available for\nhigh-resolution fMRI studies. To tackle this challenge, self-supervised\nlearning has been proposed for training of DL reconstruction with only\nundersampled datasets, showing similar performance to supervised learning. In\nthis study, we utilize a self-supervised physics-guided DL reconstruction on a\n5-fold SMS and 4-fold in-plane accelerated 7T fMRI data. Our results show that\nour self-supervised DL reconstruction produce high-quality images at this\n20-fold acceleration, substantially improving on existing methods, while\nshowing similar functional precision and temporal effects in the subsequent\nanalysis compared to a standard 10-fold accelerated acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:39:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Demirel", "Omer Burak", ""], ["Yaman", "Burhaneddin", ""], ["Dowdle", "Logan", ""], ["Moeller", "Steen", ""], ["Vizioli", "Luca", ""], ["Yacoub", "Essa", ""], ["Strupp", "John", ""], ["Olman", "Cheryl A.", ""], ["U\u011furbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2105.05837", "submitter": "Elijah Cole", "authors": "Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, Serge\n  Belongie", "title": "When Does Contrastive Visual Representation Learning Work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised representation learning techniques have largely closed\nthe gap between supervised and unsupervised learning on ImageNet\nclassification. While the particulars of pretraining on ImageNet are now\nrelatively well understood, the field still lacks widely accepted best\npractices for replicating this success on other datasets. As a first step in\nthis direction, we study contrastive self-supervised learning on four diverse\nlarge-scale datasets. By looking through the lenses of data quantity, data\ndomain, data quality, and task granularity, we provide new insights into the\nnecessary conditions for successful self-supervised learning. Our key findings\ninclude observations such as: (i) the benefit of additional pretraining data\nbeyond 500k images is modest, (ii) adding pretraining images from another\ndomain does not lead to more general representations, (iii) corrupted\npretraining images have a disparate impact on supervised and self-supervised\npretraining, and (iv) contrastive learning lags far behind supervised learning\non fine-grained visual classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:52:42 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cole", "Elijah", ""], ["Yang", "Xuan", ""], ["Wilber", "Kimberly", ""], ["Mac Aodha", "Oisin", ""], ["Belongie", "Serge", ""]]}, {"id": "2105.05838", "submitter": "Yansong Tang", "authors": "Yansong Tang, Zhenyu Jiang, Zhenda Xie, Yue Cao, Zheng Zhang, Philip\n  H. S. Torr, Han Hu", "title": "Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for\n  Video Correspondence Learning", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous cycle-consistency correspondence learning methods usually leverage\nimage patches for training. In this paper, we present a fully convolutional\nmethod, which is simpler and more coherent to the inference process. While\ndirectly applying fully convolutional training results in model collapse, we\nstudy the underline reason behind this collapse phenomenon, indicating that the\nabsolute positions of pixels provide a shortcut to easily accomplish\ncycle-consistence, which hinders the learning of meaningful visual\nrepresentations. To break this absolute position shortcut, we propose to apply\ndifferent crops for forward and backward frames, and adopt feature warping to\nestablish correspondence between two crops of a same frame. The former\ntechnique enforces the corresponding pixels at forward and back tracks to have\ndifferent absolute positions, and the latter effectively blocks the shortcuts\ngoing between forward and back tracks. In three label propagation benchmarks\nfor pose tracking, face landmark tracking and video object segmentation, our\nmethod largely improves the results of vanilla fully convolutional\ncycle-consistency method, achieving very competitive performance compared with\nthe self-supervised state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:52:45 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tang", "Yansong", ""], ["Jiang", "Zhenyu", ""], ["Xie", "Zhenda", ""], ["Cao", "Yue", ""], ["Zhang", "Zheng", ""], ["Torr", "Philip H. S.", ""], ["Hu", "Han", ""]]}, {"id": "2105.05847", "submitter": "Vadim Sushko", "authors": "Vadim Sushko, Juergen Gall, Anna Khoreva", "title": "Learning to Generate Novel Scene Compositions from Single Images and\n  Videos", "comments": "The AI for Content Creation (AICC) workshop at CVPR 2021. The full\n  8-page version of this submission is available at arXiv:2103.13389", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training GANs in low-data regimes remains a challenge, as overfitting often\nleads to memorization or training divergence. In this work, we introduce\nOne-Shot GAN that can learn to generate samples from a training set as little\nas one image or one video. We propose a two-branch discriminator, with content\nand layout branches designed to judge the internal content separately from the\nscene layout realism. This allows synthesis of visually plausible, novel\ncompositions of a scene, with varying content and layout, while preserving the\ncontext of the original sample. Compared to previous single-image GAN models,\nOne-Shot GAN achieves higher diversity and quality of synthesis. It is also not\nrestricted to the single image setting, successfully learning in the introduced\nsetting of a single video.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:59:45 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sushko", "Vadim", ""], ["Gall", "Juergen", ""], ["Khoreva", "Anna", ""]]}, {"id": "2105.05873", "submitter": "Roberto Bigazzi", "authors": "Roberto Bigazzi, Federico Landi, Marcella Cornia, Silvia Cascianelli,\n  Lorenzo Baraldi and Rita Cucchiara", "title": "Out of the Box: Embodied Navigation in the Real World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The research field of Embodied AI has witnessed substantial progress in\nvisual navigation and exploration thanks to powerful simulating platforms and\nthe availability of 3D data of indoor and photorealistic environments. These\ntwo factors have opened the doors to a new generation of intelligent agents\ncapable of achieving nearly perfect PointGoal Navigation. However, such\narchitectures are commonly trained with millions, if not billions, of frames\nand tested in simulation. Together with great enthusiasm, these results yield a\nquestion: how many researchers will effectively benefit from these advances? In\nthis work, we detail how to transfer the knowledge acquired in simulation into\nthe real world. To that end, we describe the architectural discrepancies that\ndamage the Sim2Real adaptation ability of models trained on the Habitat\nsimulator and propose a novel solution tailored towards the deployment in\nreal-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot\nequipped with a single Intel RealSense camera. Different from previous work,\nour testing scene is unavailable to the agent in simulation. The environment is\nalso inaccessible to the agent beforehand, so it cannot count on scene-specific\nsemantic priors. In this way, we reproduce a setting in which a research group\n(potentially from other fields) needs to employ the agent visual navigation\ncapabilities as-a-Service. Our experiments indicate that it is possible to\nachieve satisfying results when deploying the obtained model in the real world.\nOur code and models are available at https://github.com/aimagelab/LoCoNav.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:00:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bigazzi", "Roberto", ""], ["Landi", "Federico", ""], ["Cornia", "Marcella", ""], ["Cascianelli", "Silvia", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2105.05874", "submitter": "Sarthak Pati", "authors": "Sarthak Pati, Ujjwal Baid, Maximilian Zenk, Brandon Edwards, Micah\n  Sheller, G. Anthony Reina, Patrick Foley, Alexey Gruzdev, Jason Martin, Shadi\n  Albarqouni, Yong Chen, Russell Taki Shinohara, Annika Reinke, David Zimmerer,\n  John B. Freymann, Justin S. Kirby, Christos Davatzikos, Rivka R. Colen,\n  Aikaterini Kotrotsou, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Hassan\n  Fathallah-Shaykh, Roland Wiest, Andras Jakab, Marc-Andre Weber, Abhishek\n  Mahajan, Lena Maier-Hein, Jens Kleesiek, Bjoern Menze, Klaus Maier-Hein,\n  Spyridon Bakas", "title": "The Federated Tumor Segmentation (FeTS) Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript describes the first challenge on Federated Learning, namely\nthe Federated Tumor Segmentation (FeTS) challenge 2021. International\nchallenges have become the standard for validation of biomedical image analysis\nmethods. However, the actual performance of participating (even the winning)\nalgorithms on \"real-world\" clinical data often remains unclear, as the data\nincluded in challenges are usually acquired in very controlled settings at few\ninstitutions. The seemingly obvious solution of just collecting increasingly\nmore data from more institutions in such challenges does not scale well due to\nprivacy and ownership hurdles. Towards alleviating these concerns, we are\nproposing the FeTS challenge 2021 to cater towards both the development and the\nevaluation of models for the segmentation of intrinsically heterogeneous (in\nappearance, shape, and histology) brain tumors, namely gliomas. Specifically,\nthe FeTS 2021 challenge uses clinically acquired, multi-institutional magnetic\nresonance imaging (MRI) scans from the BraTS 2020 challenge, as well as from\nvarious remote independent institutions included in the collaborative network\nof a real-world federation (https://www.fets.ai/). The goals of the FeTS\nchallenge are directly represented by the two included tasks: 1) the\nidentification of the optimal weight aggregation approach towards the training\nof a consensus model that has gained knowledge via federated learning from\nmultiple geographically distinct institutions, while their data are always\nretained within each institution, and 2) the federated evaluation of the\ngeneralizability of brain tumor segmentation models \"in the wild\", i.e. on data\nfrom institutional distributions that were not part of the training datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:00:20 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 00:54:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Pati", "Sarthak", ""], ["Baid", "Ujjwal", ""], ["Zenk", "Maximilian", ""], ["Edwards", "Brandon", ""], ["Sheller", "Micah", ""], ["Reina", "G. Anthony", ""], ["Foley", "Patrick", ""], ["Gruzdev", "Alexey", ""], ["Martin", "Jason", ""], ["Albarqouni", "Shadi", ""], ["Chen", "Yong", ""], ["Shinohara", "Russell Taki", ""], ["Reinke", "Annika", ""], ["Zimmerer", "David", ""], ["Freymann", "John B.", ""], ["Kirby", "Justin S.", ""], ["Davatzikos", "Christos", ""], ["Colen", "Rivka R.", ""], ["Kotrotsou", "Aikaterini", ""], ["Marcus", "Daniel", ""], ["Milchenko", "Mikhail", ""], ["Nazeri", "Arash", ""], ["Fathallah-Shaykh", "Hassan", ""], ["Wiest", "Roland", ""], ["Jakab", "Andras", ""], ["Weber", "Marc-Andre", ""], ["Mahajan", "Abhishek", ""], ["Maier-Hein", "Lena", ""], ["Kleesiek", "Jens", ""], ["Menze", "Bjoern", ""], ["Maier-Hein", "Klaus", ""], ["Bakas", "Spyridon", ""]]}, {"id": "2105.05891", "submitter": "Kimmo K\\\"arkk\\\"ainen", "authors": "Kimmo K\\\"arkk\\\"ainen, Shayan Fazeli, Majid Sarrafzadeh", "title": "Unsupervised Acute Intracranial Hemorrhage Segmentation with Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial hemorrhage occurs when blood vessels rupture or leak within the\nbrain tissue or elsewhere inside the skull. It can be caused by physical trauma\nor by various medical conditions and in many cases leads to death. The\ntreatment must be started as soon as possible, and therefore the hemorrhage\nshould be diagnosed accurately and quickly. The diagnosis is usually performed\nby a radiologist who analyses a Computed Tomography (CT) scan containing a\nlarge number of cross-sectional images throughout the brain. Analysing each\nimage manually can be very time-consuming, but automated techniques can help\nspeed up the process. While much of the recent research has focused on solving\nthis problem by using supervised machine learning algorithms,\npublicly-available training data remains scarce due to privacy concerns. This\nproblem can be alleviated by unsupervised algorithms. In this paper, we propose\na fully-unsupervised algorithm which is based on the mixture models. Our\nalgorithm utilizes the fact that the properties of hemorrhage and healthy\ntissues follow different distributions, and therefore an appropriate\nformulation of these distributions allows us to separate them through an\nExpectation-Maximization process. In addition, our algorithm is able to\nadaptively determine the number of clusters such that all the hemorrhage\nregions can be found without including noisy voxels. We demonstrate the results\nof our algorithm on publicly-available datasets that contain all different\nhemorrhage types in various sizes and intensities, and our results are compared\nto earlier unsupervised and supervised algorithms. The results show that our\nalgorithm can outperform the other algorithms with most hemorrhage types.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:26:00 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Kimmo", ""], ["Fazeli", "Shayan", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "2105.05902", "submitter": "Samuele Pino", "authors": "Samuele Pino, Mark James Carman, Paolo Bestagini", "title": "What's wrong with this video? Comparing Explainers for Deepfake\n  Detection", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deepfakes are computer manipulated videos where the face of an individual has\nbeen replaced with that of another. Software for creating such forgeries is\neasy to use and ever more popular, causing serious threats to personal\nreputation and public security. The quality of classifiers for detecting\ndeepfakes has improved with the releasing of ever larger datasets, but the\nunderstanding of why a particular video has been labelled as fake has not kept\npace.\n  In this work we develop, extend and compare white-box, black-box and\nmodel-specific techniques for explaining the labelling of real and fake videos.\nIn particular, we adapt SHAP, GradCAM and self-attention models to the task of\nexplaining the predictions of state-of-the-art detectors based on EfficientNet,\ntrained on the Deepfake Detection Challenge (DFDC) dataset. We compare the\nobtained explanations, proposing metrics to quantify their visual features and\ndesirable characteristics, and also perform a user survey collecting users'\nopinions regarding the usefulness of the explainers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:44:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pino", "Samuele", ""], ["Carman", "Mark James", ""], ["Bestagini", "Paolo", ""]]}, {"id": "2105.05916", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yue Bai, Yun Fu", "title": "Dynamical Isometry: The Missing Ingredient for Neural Network Pruning", "comments": "8 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works [40, 24] observed an interesting phenomenon in neural\nnetwork pruning: A larger finetuning learning rate can improve the final\nperformance significantly. Unfortunately, the reason behind it remains elusive\nup to date. This paper is meant to explain it through the lens of dynamical\nisometry [42]. Specifically, we examine neural network pruning from an unusual\nperspective: pruning as initialization for finetuning, and ask whether the\ninherited weights serve as a good initialization for the finetuning? The\ninsights from dynamical isometry suggest a negative answer. Despite its\ncritical role, this issue has not been well-recognized by the community so far.\nIn this paper, we will show the understanding of this problem is very important\n-- on top of explaining the aforementioned mystery about the larger finetuning\nrate, it also unveils the mystery about the value of pruning [5, 30]. Besides a\nclearer theoretical understanding of pruning, resolving the problem can also\nbring us considerable performance benefits in practice.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:20:09 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Bai", "Yue", ""], ["Fu", "Yun", ""]]}, {"id": "2105.05926", "submitter": "Avi Ben-Cohen", "authors": "Avi Ben-Cohen, Nadav Zamir, Emanuel Ben Baruch, Itamar Friedman, Lihi\n  Zelnik-Manor", "title": "Semantic Diversity Learning for Zero-Shot Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a neural network model for recognizing multiple labels associated\nwith an image, including identifying unseen labels, is challenging, especially\nfor images that portray numerous semantically diverse labels. As challenging as\nthis task is, it is an essential task to tackle since it represents many\nreal-world cases, such as image retrieval of natural images. We argue that\nusing a single embedding vector to represent an image, as commonly practiced,\nis not sufficient to rank both relevant seen and unseen labels accurately. This\nstudy introduces an end-to-end model training for multi-label zero-shot\nlearning that supports semantic diversity of the images and labels. We propose\nto use an embedding matrix having principal embedding vectors trained using a\ntailored loss function. In addition, during training, we suggest up-weighting\nin the loss function image samples presenting higher semantic diversity to\nencourage the diversity of the embedding matrix. Extensive experiments show\nthat our proposed method improves the zero-shot model's quality in tag-based\nimage retrieval achieving SoTA results on several common datasets (NUS-Wide,\nCOCO, Open Images).\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:39:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ben-Cohen", "Avi", ""], ["Zamir", "Nadav", ""], ["Baruch", "Emanuel Ben", ""], ["Friedman", "Itamar", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "2105.05964", "submitter": "Zihang Meng", "authors": "Zihang Meng, Licheng Yu, Ning Zhang, Tamara Berg, Babak Damavandi,\n  Vikas Singh, Amy Bearman", "title": "Connecting What to Say With Where to Look by Modeling Human Attention\n  Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a unified framework to jointly model images, text, and human\nattention traces. Our work is built on top of the recent Localized Narratives\nannotation framework [30], where each word of a given caption is paired with a\nmouse trace segment. We propose two novel tasks: (1) predict a trace given an\nimage and caption (i.e., visual grounding), and (2) predict a caption and a\ntrace given only an image. Learning the grounding of each word is challenging,\ndue to noise in the human-provided traces and the presence of words that cannot\nbe meaningfully visually grounded. We present a novel model architecture that\nis jointly trained on dual tasks (controlled trace generation and controlled\ncaption generation). To evaluate the quality of the generated traces, we\npropose a local bipartite matching (LBM) distance metric which allows the\ncomparison of two traces of different lengths. Extensive experiments show our\nmodel is robust to the imperfect training data and outperforms the baselines by\na clear margin. Moreover, we demonstrate that our model pre-trained on the\nproposed tasks can be also beneficial to the downstream task of COCO's guided\nimage captioning. Our code and project page are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:53:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Meng", "Zihang", ""], ["Yu", "Licheng", ""], ["Zhang", "Ning", ""], ["Berg", "Tamara", ""], ["Damavandi", "Babak", ""], ["Singh", "Vikas", ""], ["Bearman", "Amy", ""]]}, {"id": "2105.05973", "submitter": "Henry Chopp", "authors": "Henry H. Chopp, Srutarshi Banerjee, Oliver Cossairt, Aggelos K.\n  Katsaggelos", "title": "Removing Blocking Artifacts in Video Streams Using Event Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose EveRestNet, a convolutional neural network designed\nto remove blocking artifacts in videostreams using events from neuromorphic\nsensors. We first degrade the video frame using a quadtree structure to produce\nthe blocking artifacts to simulate transmitting a video under a heavily\nconstrained bandwidth. Events from the neuromorphic sensor are also simulated,\nbut are transmitted in full. Using the distorted frames and the event stream,\nEveRestNet is able to improve the image quality.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:19:54 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chopp", "Henry H.", ""], ["Banerjee", "Srutarshi", ""], ["Cossairt", "Oliver", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "2105.05980", "submitter": "Chun-Mei Feng", "authors": "Chun-Mei Feng, Zhanyuan Yang, Huazhu Fu, Yong Xu, Jian Yang, Ling Shao", "title": "DONet: Dual-Octave Network for Fast MR Image Reconstruction", "comments": "arXiv admin note: substantial text overlap with arXiv:2104.05345", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic resonance (MR) image acquisition is an inherently prolonged process,\nwhose acceleration has long been the subject of research. This is commonly\nachieved by obtaining multiple undersampled images, simultaneously, through\nparallel imaging. In this paper, we propose the Dual-Octave Network (DONet),\nwhich is capable of learning multi-scale spatial-frequency features from both\nthe real and imaginary components of MR data, for fast parallel MR image\nreconstruction. More specifically, our DONet consists of a series of\nDual-Octave convolutions (Dual-OctConv), which are connected in a dense manner\nfor better reuse of features. In each Dual-OctConv, the input feature maps and\nconvolutional kernels are first split into two components (ie, real and\nimaginary), and then divided into four groups according to their spatial\nfrequencies. Then, our Dual-OctConv conducts intra-group information updating\nand inter-group information exchange to aggregate the contextual information\nacross different groups. Our framework provides three appealing benefits: (i)\nIt encourages information interaction and fusion between the real and imaginary\ncomponents at various spatial frequencies to achieve richer representational\ncapacity. (ii) The dense connections between the real and imaginary groups in\neach Dual-OctConv make the propagation of features more efficient by feature\nreuse. (iii) DONet enlarges the receptive field by learning multiple\nspatial-frequency features of both the real and imaginary components. Extensive\nexperiments on two popular datasets (ie, clinical knee and fastMRI), under\ndifferent undersampling patterns and acceleration factors, demonstrate the\nsuperiority of our model in accelerated parallel MR image reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:41:02 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 10:50:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Yang", "Zhanyuan", ""], ["Fu", "Huazhu", ""], ["Xu", "Yong", ""], ["Yang", "Jian", ""], ["Shao", "Ling", ""]]}, {"id": "2105.05994", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang, Ben Eckart, Simon Lucey, Orazio Gallo", "title": "Neural Trajectory Fields for Dynamic Novel View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches to render photorealistic views from a limited set of\nphotographs have pushed the boundaries of our interactions with pictures of\nstatic scenes. The ability to recreate moments, that is, time-varying\nsequences, is perhaps an even more interesting scenario, but it remains largely\nunsolved. We introduce DCT-NeRF, a coordinatebased neural representation for\ndynamic scenes. DCTNeRF learns smooth and stable trajectories over the input\nsequence for each point in space. This allows us to enforce consistency between\nany two frames in the sequence, which results in high quality reconstruction,\nparticularly in dynamic regions.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 22:38:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Chaoyang", ""], ["Eckart", "Ben", ""], ["Lucey", "Simon", ""], ["Gallo", "Orazio", ""]]}, {"id": "2105.06033", "submitter": "Nilesh Pandey", "authors": "Nilesh Pandey, Andreas Savakis", "title": "Extreme Face Inpainting with Sketch-Guided Conditional GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recovering badly damaged face images is a useful yet challenging task,\nespecially in extreme cases where the masked or damaged region is very large.\nOne of the major challenges is the ability of the system to generalize on faces\noutside the training dataset. We propose to tackle this extreme inpainting task\nwith a conditional Generative Adversarial Network (GAN) that utilizes\nstructural information, such as edges, as a prior condition. Edge information\ncan be obtained from the partially masked image and a structurally similar\nimage or a hand drawing. In our proposed conditional GAN, we pass the\nconditional input in every layer of the encoder while maintaining consistency\nin the distributions between the learned weights and the incoming conditional\ninput. We demonstrate the effectiveness of our method with badly damaged face\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:45:45 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pandey", "Nilesh", ""], ["Savakis", "Andreas", ""]]}, {"id": "2105.06047", "submitter": "Rahul Duggal", "authors": "Rahul Duggal, Hao Zhou, Shuo Yang, Yuanjun Xiong, Wei Xia, Zhuowen Tu,\n  Stefano Soatto", "title": "Compatibility-aware Heterogeneous Visual Search", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of visual search under resource constraints. Existing\nsystems use the same embedding model to compute representations (embeddings)\nfor the query and gallery images. Such systems inherently face a hard\naccuracy-efficiency trade-off: the embedding model needs to be large enough to\nensure high accuracy, yet small enough to enable query-embedding computation on\nresource-constrained platforms. This trade-off could be mitigated if gallery\nembeddings are generated from a large model and query embeddings are extracted\nusing a compact model. The key to building such a system is to ensure\nrepresentation compatibility between the query and gallery models. In this\npaper, we address two forms of compatibility: One enforced by modifying the\nparameters of each model that computes the embeddings. The other by modifying\nthe architectures that compute the embeddings, leading to compatibility-aware\nneural architecture search (CMP-NAS). We test CMP-NAS on challenging retrieval\ntasks for fashion images (DeepFashion2), and face images (IJB-C). Compared to\nordinary (homogeneous) visual search using the largest embedding model\n(paragon), CMP-NAS achieves 80-fold and 23-fold cost reduction while\nmaintaining accuracy within 0.3% and 1.6% of the paragon on DeepFashion2 and\nIJB-C respectively.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 02:30:50 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Duggal", "Rahul", ""], ["Zhou", "Hao", ""], ["Yang", "Shuo", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Tu", "Zhuowen", ""], ["Soatto", "Stefano", ""]]}, {"id": "2105.06049", "submitter": "Fan Wang", "authors": "Fan Wang, Saarthak Kapse, Steven Liu, Prateek Prasanna, Chao Chen", "title": "TopoTxR: A Topological Biomarker for Predicting Treatment Response in\n  Breast Cancer", "comments": "12 pages, 5 figures, 2 tables, accepted to International Conference\n  on Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Characterization of breast parenchyma on dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI) is a challenging task owing to the complexity of\nunderlying tissue structures. Current quantitative approaches, including\nradiomics and deep learning models, do not explicitly capture the complex and\nsubtle parenchymal structures, such as fibroglandular tissue. In this paper, we\npropose a novel method to direct a neural network's attention to a dedicated\nset of voxels surrounding biologically relevant tissue structures. By\nextracting multi-dimensional topological structures with high saliency, we\nbuild a topology-derived biomarker, TopoTxR. We demonstrate the efficacy of\nTopoTxR in predicting response to neoadjuvant chemotherapy in breast cancer.\nOur qualitative and quantitative results suggest differential topological\nbehavior of breast tissue on treatment-na\\\"ive imaging, in patients who respond\nfavorably to therapy versus those who do not.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 02:38:48 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Fan", ""], ["Kapse", "Saarthak", ""], ["Liu", "Steven", ""], ["Prasanna", "Prateek", ""], ["Chen", "Chao", ""]]}, {"id": "2105.06052", "submitter": "Zidu Wang", "authors": "Zidu Wang, Xuexin Liu, Long Huang, Yunqing Chen, Yufei Zhang, Zhikang\n  Lin, Rui Wang", "title": "Model Pruning Based on Quantified Similarity of Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A high-accuracy CNN is often accompanied by huge parameters, which are\nusually stored in the high-dimensional tensors. However, there are few methods\ncan figure out the redundant information of the parameters stored in the\nhigh-dimensional tensors, which leads to the lack of theoretical guidance for\nthe compression of CNNs. In this paper, we propose a novel theory to find\nredundant information in three dimensional tensors, namely Quantified\nSimilarity of Feature Maps (QSFM), and use this theory to prune convolutional\nneural networks to enhance the inference speed. Our method belongs to filter\npruning, which can be implemented without using any special libraries. We\nperform our method not only on common convolution layers but also on special\nconvolution layers, such as depthwise separable convolution layers. The\nexperiments prove that QSFM can find the redundant information in the neural\nnetwork effectively. Without any fine-tuning operation, QSFM can compress\nResNet-56 on CIFAR-10 significantly (48.27% FLOPs and 57.90% parameters\nreduction) with only a loss of 0.54% in the top-1 accuracy. QSFM also prunes\nResNet-56, VGG-16 and MobileNetV2 with fine-tuning operation, which also shows\nexcellent results.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 02:57:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Zidu", ""], ["Liu", "Xuexin", ""], ["Huang", "Long", ""], ["Chen", "Yunqing", ""], ["Zhang", "Yufei", ""], ["Lin", "Zhikang", ""], ["Wang", "Rui", ""]]}, {"id": "2105.06070", "submitter": "Tao Yang", "authors": "Tao Yang (1), Peiran Ren (1), Xuansong Xie (1) and Lei Zhang (1 and 2)\n  ((1) DAMO Academy, Alibaba Group, (2) Department of Computing, The Hong Kong\n  Polytechnic University)", "title": "GAN Prior Embedded Network for Blind Face Restoration in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind face restoration (BFR) from severely degraded face images in the wild\nis a very challenging problem. Due to the high illness of the problem and the\ncomplex unknown degradation, directly training a deep neural network (DNN)\nusually cannot lead to acceptable results. Existing generative adversarial\nnetwork (GAN) based methods can produce better results but tend to generate\nover-smoothed restorations. In this work, we propose a new method by first\nlearning a GAN for high-quality face image generation and embedding it into a\nU-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN\nwith a set of synthesized low-quality face images. The GAN blocks are designed\nto ensure that the latent code and noise input to the GAN can be respectively\ngenerated from the deep and shallow features of the DNN, controlling the global\nface structure, local face details and background of the reconstructed image.\nThe proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can\ngenerate visually photo-realistic results. Our experiments demonstrated that\nthe proposed GPEN achieves significantly superior results to state-of-the-art\nBFR methods both quantitatively and qualitatively, especially for the\nrestoration of severely degraded face images in the wild. The source code and\nmodels can be found at https://github.com/yangxy/GPEN.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 04:14:00 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yang", "Tao", "", "1 and 2"], ["Ren", "Peiran", "", "1 and 2"], ["Xie", "Xuansong", "", "1 and 2"], ["Zhang", "Lei", "", "1 and 2"]]}, {"id": "2105.06084", "submitter": "Hung Tuan Nguyen Dr.", "authors": "Thanh-Nghia Truong, Hung Tuan Nguyen, Cuong Tuan Nguyen and Masaki\n  Nakagawa", "title": "Learning symbol relation tree for online mathematical expression\n  recognition", "comments": "13 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for recognizing online handwritten mathematical\nexpressions (OnHME) by building a symbol relation tree (SRT) directly from a\nsequence of strokes. A bidirectional recurrent neural network learns from\nmultiple derived paths of SRT to predict both symbols and spatial relations\nbetween symbols using global context. The recognition system has two parts: a\ntemporal classifier and a tree connector. The temporal classifier produces an\nSRT by recognizing an OnHME pattern. The tree connector splits the SRT into\nseveral sub-SRTs. The final SRT is formed by looking up the best combination\namong those sub-SRTs. Besides, we adopt a tree sorting method to deal with\nvarious stroke orders. Recognition experiments indicate that the proposed OnHME\nrecognition system is competitive to other methods. The recognition system\nachieves 44.12% and 41.76% expression recognition rates on the Competition on\nRecognition of Online Handwritten Mathematical Expressions (CROHME) 2014 and\n2016 testing sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 05:18:17 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Truong", "Thanh-Nghia", ""], ["Nguyen", "Hung Tuan", ""], ["Nguyen", "Cuong Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2105.06086", "submitter": "Liangyu Chen", "authors": "Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, Chengpeng Chen", "title": "HINet: Half Instance Normalization Network for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the role of Instance Normalization in low-level\nvision tasks. Specifically, we present a novel block: Half Instance\nNormalization Block (HIN Block), to boost the performance of image restoration\nnetworks. Based on HIN Block, we design a simple and powerful multi-stage\nnetwork named HINet, which consists of two subnetworks. With the help of HIN\nBlock, HINet surpasses the state-of-the-art (SOTA) on various image restoration\ntasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD\ndataset, with only 7.5% and 30% of its multiplier-accumulator operations\n(MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we\nget comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS\nand GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the\naverage result of multiple datasets with 1.4 times speedup. With HINet, we won\n1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG\nArtifacts, with a PSNR of 29.70. The code is available at\nhttps://github.com/megvii-model/HINet.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 05:25:01 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chen", "Liangyu", ""], ["Lu", "Xin", ""], ["Zhang", "Jie", ""], ["Chu", "Xiaojie", ""], ["Chen", "Chengpeng", ""]]}, {"id": "2105.06091", "submitter": "Ankit Sonthalia", "authors": "Weng Fei Low, Ankit Sonthalia, Zhi Gao, Andr\\'e van Schaik, Bharath\n  Ramesh", "title": "Superevents: Towards Native Semantic Segmentation for Event-based\n  Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most successful computer vision models transform low-level features, such as\nGabor filter responses, into richer representations of intermediate or\nmid-level complexity for downstream visual tasks. These mid-level\nrepresentations have not been explored for event cameras, although it is\nespecially relevant to the visually sparse and often disjoint spatial\ninformation in the event stream. By making use of locally consistent\nintermediate representations, termed as superevents, numerous visual tasks\nranging from semantic segmentation, visual tracking, depth estimation shall\nbenefit. In essence, superevents are perceptually consistent local units that\ndelineate parts of an object in a scene. Inspired by recent deep learning\narchitectures, we present a novel method that employs lifetime augmentation for\nobtaining an event stream representation that is fed to a fully convolutional\nnetwork to extract superevents. Our qualitative and quantitative experimental\nresults on several sequences of a benchmark dataset highlights the significant\npotential for event-based downstream applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 05:49:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Low", "Weng Fei", ""], ["Sonthalia", "Ankit", ""], ["Gao", "Zhi", ""], ["van Schaik", "Andr\u00e9", ""], ["Ramesh", "Bharath", ""]]}, {"id": "2105.06117", "submitter": "Sangyup Lee Mr.", "authors": "Sangyup Lee, Shahroz Tariq, Junyaup Kim, and Simon S. Woo", "title": "TAR: Generalized Forensic Framework to Detect Deepfakes using Weakly\n  Supervised Learning", "comments": "16 pages, 3 figures, to be published in IFIP-SEC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deepfakes have become a critical social problem, and detecting them is of\nutmost importance. Also, deepfake generation methods are advancing, and it is\nbecoming harder to detect. While many deepfake detection models can detect\ndifferent types of deepfakes separately, they perform poorly on generalizing\nthe detection performance over multiple types of deepfake. This motivates us to\ndevelop a generalized model to detect different types of deepfakes. Therefore,\nin this work, we introduce a practical digital forensic tool to detect\ndifferent types of deepfakes simultaneously and propose Transfer learning-based\nAutoencoder with Residuals (TAR). The ultimate goal of our work is to develop a\nunified model to detect various types of deepfake videos with high accuracy,\nwith only a small number of training samples that can work well in real-world\nsettings. We develop an autoencoder-based detection model with Residual blocks\nand sequentially perform transfer learning to detect different types of\ndeepfakes simultaneously. Our approach achieves a much higher generalized\ndetection performance than the state-of-the-art methods on the FaceForensics++\ndataset. In addition, we evaluate our model on 200 real-world\nDeepfake-in-the-Wild (DW) videos of 50 celebrities available on the Internet\nand achieve 89.49% zero-shot accuracy, which is significantly higher than the\nbest baseline model (gaining 10.77%), demonstrating and validating the\npracticability of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 07:31:08 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lee", "Sangyup", ""], ["Tariq", "Shahroz", ""], ["Kim", "Junyaup", ""], ["Woo", "Simon S.", ""]]}, {"id": "2105.06125", "submitter": "Xiao Luo", "authors": "Xiao Luo, Zeyu Ma, Daqing Wu, Huasong Zhong, Chong Chen, Jinwen Ma,\n  Minghua Deng", "title": "Deep Unsupervised Hashing by Distilled Smooth Guidance", "comments": "7 pages, 3 figures", "journal-ref": "ICME 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hashing has been widely used in approximate nearest neighbor search for its\nstorage and computational efficiency. Deep supervised hashing methods are not\nwidely used because of the lack of labeled data, especially when the domain is\ntransferred. Meanwhile, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable similarity signals. To\ntackle this problem, we propose a novel deep unsupervised hashing method,\nnamely Distilled Smooth Guidance (DSG), which can learn a distilled dataset\nconsisting of similarity signals as well as smooth confidence signals. To be\nspecific, we obtain the similarity confidence weights based on the initial\nnoisy similarity signals learned from local structures and construct a priority\nloss function for smooth similarity-preserving learning. Besides, global\ninformation based on clustering is utilized to distill the image pairs by\nremoving contradictory similarity signals. Extensive experiments on three\nwidely used benchmark datasets show that the proposed DSG consistently\noutperforms the state-of-the-art search methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 07:59:57 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Luo", "Xiao", ""], ["Ma", "Zeyu", ""], ["Wu", "Daqing", ""], ["Zhong", "Huasong", ""], ["Chen", "Chong", ""], ["Ma", "Jinwen", ""], ["Deng", "Minghua", ""]]}, {"id": "2105.06129", "submitter": "Aaditya Singh", "authors": "Aaditya Singh, Shreeshail Hingane, Xinyu Gong, Zhangyang Wang", "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance\n  Normalization", "comments": "Accepted at ICME 2021, 5 Pages + 1 Page (references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:01:01 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 05:44:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Singh", "Aaditya", ""], ["Hingane", "Shreeshail", ""], ["Gong", "Xinyu", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2105.06138", "submitter": "Zexuan Qiu", "authors": "Zexuan Qiu, Qinliang Su, Zijing Ou, Jianxing Yu and Changyou Chen", "title": "Unsupervised Hashing with Contrastive Information Bottleneck", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many unsupervised hashing methods are implicitly established on the idea of\nreconstructing the input data, which basically encourages the hashing codes to\nretain as much information of original data as possible. However, this\nrequirement may force the models spending lots of their effort on\nreconstructing the unuseful background information, while ignoring to preserve\nthe discriminative semantic information that is more important for the hashing\ntask. To tackle this problem, inspired by the recent success of contrastive\nlearning in learning continuous representations, we propose to adapt this\nframework to learn binary hashing codes. Specifically, we first propose to\nmodify the objective function to meet the specific requirement of hashing and\nthen introduce a probabilistic binary representation layer into the model to\nfacilitate end-to-end training of the entire model. We further prove the strong\nconnection between the proposed contrastive-learning-based hashing method and\nthe mutual information, and show that the proposed model can be considered\nunder the broader framework of the information bottleneck (IB). Under this\nperspective, a more general hashing model is naturally obtained. Extensive\nexperimental results on three benchmark image datasets demonstrate that the\nproposed hashing method significantly outperforms existing baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:30:16 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:57:51 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Qiu", "Zexuan", ""], ["Su", "Qinliang", ""], ["Ou", "Zijing", ""], ["Yu", "Jianxing", ""], ["Chen", "Changyou", ""]]}, {"id": "2105.06141", "submitter": "Matteo Chieregato", "authors": "Matteo Chieregato, Fabio Frangiamore, Mauro Morassi, Claudia Baresi,\n  Stefania Nici, Chiara Bassetti, Claudio Bn\\`a and Marco Galelli", "title": "A hybrid machine learning/deep learning COVID-19 severity predictive\n  model from CT images and clinical data", "comments": "16 pages, 10 figures, 2 supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 clinical presentation and prognosis are highly variable, ranging\nfrom asymptomatic and paucisymptomatic cases to acute respiratory distress\nsyndrome and multi-organ involvement. We developed a hybrid machine\nlearning/deep learning model to classify patients in two outcome categories,\nnon-ICU and ICU (intensive care admission or death), using 558 patients\nadmitted in a northern Italy hospital in February/May of 2020. A fully 3D\npatient-level CNN classifier on baseline CT images is used as feature\nextractor. Features extracted, alongside with laboratory and clinical data, are\nfed for selection in a Boruta algorithm with SHAP game theoretical values. A\nclassifier is built on the reduced feature space using CatBoost gradient\nboosting algorithm and reaching a probabilistic AUC of 0.949 on holdout test\nset. The model aims to provide clinical decision support to medical doctors,\nwith the probability score of belonging to an outcome class and with case-based\nSHAP interpretation of features importance.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:39:56 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chieregato", "Matteo", ""], ["Frangiamore", "Fabio", ""], ["Morassi", "Mauro", ""], ["Baresi", "Claudia", ""], ["Nici", "Stefania", ""], ["Bassetti", "Chiara", ""], ["Bn\u00e0", "Claudio", ""], ["Galelli", "Marco", ""]]}, {"id": "2105.06143", "submitter": "Junjie Hu", "authors": "Junjie Hu, Chenyou Fan, Hualie Jiang, Xiyue Guo, Xiangyong Lu, and Tin\n  Lun Lam", "title": "Boosting Light-Weight Depth Estimation Via Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advanced performance of depth estimation is achieved by the employment of\nlarge and complex neural networks. While the performance has still been\ncontinuously improved, we argue that the depth estimation has to be accurate\nand efficient. It's a preliminary requirement for real-world applications.\nHowever, fast depth estimation tends to lower the performance as the trade-off\nbetween the model's capacity and accuracy. In this paper, we attempt to archive\nhighly accurate depth estimation with a light-weight network. To this end, we\nfirst introduce a compact network that can estimate a depth map in real-time.\nWe then technically show two complementary and necessary strategies to improve\nthe performance of the light-weight network. As the number of real-world scenes\nis infinite, the first is the employment of auxiliary data that increases the\ndiversity of training data. The second is the use of knowledge distillation to\nfurther boost the performance. Through extensive and rigorous experiments, we\nshow that our method outperforms previous light-weight methods in terms of\ninference accuracy, computational efficiency and generalization. We can achieve\ncomparable performance compared to state-of-the-of-art methods with only 1%\nparameters, on the other hand, our method outperforms other light-weight\nmethods by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:42:42 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Hu", "Junjie", ""], ["Fan", "Chenyou", ""], ["Jiang", "Hualie", ""], ["Guo", "Xiyue", ""], ["Lu", "Xiangyong", ""], ["Lam", "Tin Lun", ""]]}, {"id": "2105.06152", "submitter": "Jiahang Wang", "authors": "Jiahang Wang, Sheng Jin, Wentao Liu, Weizhong Liu, Chen Qian, Ping Luo", "title": "When Human Pose Estimation Meets Robustness: Adversarial Algorithms and\n  Benchmarks", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is a fundamental yet challenging task in computer\nvision, which aims at localizing human anatomical keypoints. However, unlike\nhuman vision that is robust to various data corruptions such as blur and\npixelation, current pose estimators are easily confused by these corruptions.\nThis work comprehensively studies and addresses this problem by building\nrigorous robust benchmarks, termed COCO-C, MPII-C, and OCHuman-C, to evaluate\nthe weaknesses of current advanced pose estimators, and a new algorithm termed\nAdvMix is proposed to improve their robustness in different corruptions. Our\nwork has several unique benefits. (1) AdvMix is model-agnostic and capable in a\nwide-spectrum of pose estimation models. (2) AdvMix consists of adversarial\naugmentation and knowledge distillation. Adversarial augmentation contains two\nneural network modules that are trained jointly and competitively in an\nadversarial manner, where a generator network mixes different corrupted images\nto confuse a pose estimator, improving the robustness of the pose estimator by\nlearning from harder samples. To compensate for the noise patterns by\nadversarial augmentation, knowledge distillation is applied to transfer clean\npose structure knowledge to the target pose estimator. (3) Extensive\nexperiments show that AdvMix significantly increases the robustness of pose\nestimations across a wide range of corruptions, while maintaining accuracy on\nclean data in various challenging benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 09:09:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Jiahang", ""], ["Jin", "Sheng", ""], ["Liu", "Wentao", ""], ["Liu", "Weizhong", ""], ["Qian", "Chen", ""], ["Luo", "Ping", ""]]}, {"id": "2105.06160", "submitter": "Fang Tao Li", "authors": "Fangtao Li, Ting Bai, Chenyu Cao, Zihe Liu, Chenghao Yan, Bin Wu", "title": "Relation-aware Hierarchical Attention Framework for Video Question\n  Answering", "comments": "9 pages, This paper is accepted by ICMR 2021", "journal-ref": null, "doi": "10.1145/3460426.3463635", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering (VideoQA) is a challenging video understanding task\nsince it requires a deep understanding of both question and video. Previous\nstudies mainly focus on extracting sophisticated visual and language\nembeddings, fusing them by delicate hand-crafted networks. However, the\nrelevance of different frames, objects, and modalities to the question are\nvaried along with the time, which is ignored in most of existing methods.\nLacking understanding of the the dynamic relationships and interactions among\nobjects brings a great challenge to VideoQA task. To address this problem, we\npropose a novel Relation-aware Hierarchical Attention (RHA) framework to learn\nboth the static and dynamic relations of the objects in videos. In particular,\nvideos and questions are embedded by pre-trained models firstly to obtain the\nvisual and textual features. Then a graph-based relation encoder is utilized to\nextract the static relationship between visual objects. To capture the dynamic\nchanges of multimodal objects in different video frames, we consider the\ntemporal, spatial, and semantic relations, and fuse the multimodal features by\nhierarchical attention mechanism to predict the answer. We conduct extensive\nexperiments on a large scale VideoQA dataset, and the experimental results\ndemonstrate that our RHA outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 09:35:42 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 02:34:56 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Li", "Fangtao", ""], ["Bai", "Ting", ""], ["Cao", "Chenyu", ""], ["Liu", "Zihe", ""], ["Yan", "Chenghao", ""], ["Wu", "Bin", ""]]}, {"id": "2105.06182", "submitter": "Etienne David", "authors": "Etienne David, Franklin Ogidi, Wei Guo, Frederic Baret, Ian Stavness", "title": "Global Wheat Challenge 2020: Analysis of the competition design and\n  winning models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data competitions have become a popular approach to crowdsource new data\nanalysis methods for general and specialized data science problems. In plant\nphenotyping, data competitions have a rich history, and new outdoor field\ndatasets have potential for new data competitions. We developed the Global\nWheat Challenge as a generalization competition to see if solutions for wheat\nhead detection from field images would work in different regions around the\nworld. In this paper, we analyze the winning challenge solutions in terms of\ntheir robustness and the relative importance of model and data augmentation\ndesign decisions. We found that the design of the competition influence the\nselection of winning solutions and provide recommendations for future\ncompetitions in an attempt to garner more robust winning solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 10:41:09 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["David", "Etienne", ""], ["Ogidi", "Franklin", ""], ["Guo", "Wei", ""], ["Baret", "Frederic", ""], ["Stavness", "Ian", ""]]}, {"id": "2105.06183", "submitter": "Luca Mocerino", "authors": "Luca Mocerino, Roberto G. Rizzo, Valentino Peluso, Andrea Calimera,\n  Enrico Macii", "title": "Adaptive Test-Time Augmentation for Low-Power CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) are trained offline using the few\navailable data and may therefore suffer from substantial accuracy loss when\nported on the field, where unseen input patterns received under unpredictable\nexternal conditions can mislead the model. Test-Time Augmentation (TTA)\ntechniques aim to alleviate such common side effect at inference-time, first\nrunning multiple feed-forward passes on a set of altered versions of the same\ninput sample, and then computing the main outcome through a consensus of the\naggregated predictions. Unfortunately, the implementation of TTA on embedded\nCPUs introduces latency penalties that limit its adoption on edge applications.\nTo tackle this issue, we propose AdapTTA, an adaptive implementation of TTA\nthat controls the number of feed-forward passes dynamically, depending on the\ncomplexity of the input. Experimental results on state-of-the-art ConvNets for\nimage classification deployed on a commercial ARM Cortex-A CPU demonstrate\nAdapTTA reaches remarkable latency savings, from 1.49X to 2.21X, and hence a\nhigher frame rate compared to static TTA, still preserving the same accuracy\ngain.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 10:50:13 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Mocerino", "Luca", ""], ["Rizzo", "Roberto G.", ""], ["Peluso", "Valentino", ""], ["Calimera", "Andrea", ""], ["Macii", "Enrico", ""]]}, {"id": "2105.06188", "submitter": "Xiaofei Li", "authors": "Xiaofei Li, Zhong Dong", "title": "SizeNet: Object Recognition via Object Real Size-based Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the conclusion that humans choose the visual cortex regions\ncorresponding to the real size of an object to analyze its features when\nidentifying objects in the real world, this paper presents a framework,\nSizeNet, which is based on both the real sizes and features of objects to solve\nobject recognition problems. SizeNet was used for object recognition\nexperiments on the homemade Rsize dataset, and was compared with the\nstate-of-the-art methods AlexNet, VGG-16, Inception V3, Resnet-18, and\nDenseNet-121. The results showed that SizeNet provides much higher accuracy\nrates for object recognition than the other algorithms. SizeNet can solve the\ntwo problems of correctly recognizing objects with highly similar features but\nreal sizes that are obviously different from each other, and correctly\ndistinguishing a target object from interference objects whose real sizes are\nobviously different from the target object. This is because SizeNet recognizes\nobjects based not only on their features, but also on their real size. The real\nsize of an object can help exclude the interference object's categories whose\nreal size ranges do not match the real size of the object, which greatly\nreduces the object's categories' number in the label set used for the\ndownstream object recognition based on object features. SizeNet is of great\nsignificance for studying the interpretable computer vision. Our code and\ndataset will thus be made public.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 11:03:24 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 02:32:13 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Li", "Xiaofei", ""], ["Dong", "Zhong", ""]]}, {"id": "2105.06194", "submitter": "Vincenzo Ciancia", "authors": "Nick Bezhanishvili and Vincenzo Ciancia and David Gabelaia and\n  Gianluca Grilletti and Diego Latella and Mieke Massink", "title": "Geometric Model Checking of Continuous Space", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Spatial Model Checking is a recent paradigm that combines Model\nChecking with the topological interpretation of Modal Logic. The Spatial Logic\nof Closure Spaces, SLCS, extends Modal Logic with reachability connectives\nthat, in turn, can be used for expressing interesting spatial properties, such\nas \"being near to\" or \"being surrounded by\". SLCS constitutes the kernel of a\nsolid logical framework for reasoning about discrete space, such as graphs and\ndigital images, interpreted as quasi discrete closure spaces. In particular,\nthe spatial model checker VoxLogicA, that uses an extended version of SLCS, has\nbeen used successfully in the domain of medical imaging. However, SLCS is not\nrestricted to discrete space. Following a recently developed geometric\nsemantics of Modal Logic, we show that it is possible to assign an\ninterpretation to SLCS in continuous space, admitting a model checking\nprocedure, by resorting to models based on polyhedra. In medical imaging such\nrepresentations of space are increasingly relevant, due to recent developments\nof 3D scanning and visualisation techniques that exploit mesh processing. We\ndemonstrate feasibility of our approach via a new tool, PolyLogicA, aimed at\nefficient verification of SLCS formulas on polyhedra, while inheriting some\nwell-established optimization techniques already adopted in VoxLogicA. Finally,\nwe cater for a geometric definition of bisimilarity, proving that it\ncharacterises logical equivalence.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 11:25:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bezhanishvili", "Nick", ""], ["Ciancia", "Vincenzo", ""], ["Gabelaia", "David", ""], ["Grilletti", "Gianluca", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "2105.06211", "submitter": "Kartheek Kumar Reddy Nareddy", "authors": "Kartheek Kumar Reddy Nareddy, Mani Madhoolika Bulusu, Praveen Kumar\n  Pokala, Chandra Sekhar Seelamantula", "title": "Quantized Proximal Averaging Network for Analysis Sparse Coding", "comments": "8 pages + references, 7 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We solve the analysis sparse coding problem considering a combination of\nconvex and non-convex sparsity promoting penalties. The multi-penalty\nformulation results in an iterative algorithm involving proximal-averaging. We\nthen unfold the iterative algorithm into a trainable network that facilitates\nlearning the sparsity prior. We also consider quantization of the network\nweights. Quantization makes neural networks efficient both in terms of memory\nand computation during inference, and also renders them compatible for\nlow-precision hardware deployment. Our learning algorithm is based on a variant\nof the ADAM optimizer in which the quantizer is part of the forward pass and\nthe gradients of the loss function are evaluated corresponding to the quantized\nweights while doing a book-keeping of the high-precision weights. We\ndemonstrate applications to compressed image recovery and magnetic resonance\nimage reconstruction. The proposed approach offers superior reconstruction\naccuracy and quality than state-of-the-art unfolding techniques and the\nperformance degradation is minimal even when the weights are subjected to\nextreme quantization.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:05:35 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Nareddy", "Kartheek Kumar Reddy", ""], ["Bulusu", "Mani Madhoolika", ""], ["Pokala", "Praveen Kumar", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "2105.06219", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Hector Laria Mantecon, Joost van de Weijer, Laura\n  Lopez-Fuentes, Bogdan Raducanu", "title": "TransferI2I: Transfer Learning for Image-to-Image Translation from Small\n  Datasets", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image (I2I) translation has matured in recent years and is able to\ngenerate high-quality realistic images. However, despite current success, it\nstill faces important challenges when applied to small domains. Existing\nmethods use transfer learning for I2I translation, but they still require the\nlearning of millions of parameters from scratch. This drawback severely limits\nits application on small domains. In this paper, we propose a new transfer\nlearning for I2I translation (TransferI2I). We decouple our learning process\ninto the image generation step and the I2I translation step. In the first step\nwe propose two novel techniques: source-target initialization and\nself-initialization of the adaptor layer. The former finetunes the pretrained\ngenerative model (e.g., StyleGAN) on source and target data. The latter allows\nto initialize all non-pretrained network parameters without the need of any\ndata. These techniques provide a better initialization for the I2I translation\nstep. In addition, we introduce an auxiliary GAN that further facilitates the\ntraining of deep I2I systems even from small datasets. In extensive experiments\non three datasets, (Animal faces, Birds, and Foods), we show that we outperform\nexisting methods and that mFID improves on several datasets with over 25\npoints.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:19:36 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 07:14:12 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Yaxing", ""], ["Mantecon", "Hector Laria", ""], ["van de Weijer", "Joost", ""], ["Lopez-Fuentes", "Laura", ""], ["Raducanu", "Bogdan", ""]]}, {"id": "2105.06220", "submitter": "Zhanzhan Cheng", "authors": "Peng Zhang and Can Li and Liang Qiao and Zhanzhan Cheng and Shiliang\n  Pu and Yi Niu and Fei Wu", "title": "VSR: A Unified Framework for Document Layout Analysis combining Vision,\n  Semantics and Relations", "comments": "Accepted by ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Document layout analysis is crucial for understanding document structures. On\nthis task, vision and semantics of documents, and relations between layout\ncomponents contribute to the understanding process. Though many works have been\nproposed to exploit the above information, they show unsatisfactory results.\nNLP-based methods model layout analysis as a sequence labeling task and show\ninsufficient capabilities in layout modeling. CV-based methods model layout\nanalysis as a detection or segmentation task, but bear limitations of\ninefficient modality fusion and lack of relation modeling between layout\ncomponents. To address the above limitations, we propose a unified framework\nVSR for document layout analysis, combining vision, semantics and relations.\nVSR supports both NLP-based and CV-based methods. Specifically, we first\nintroduce vision through document image and semantics through text embedding\nmaps. Then, modality-specific visual and semantic features are extracted using\na two-stream network, which are adaptively fused to make full use of\ncomplementary information. Finally, given component candidates, a relation\nmodule based on graph neural network is incorported to model relations between\ncomponents and output final results. On three popular benchmarks, VSR\noutperforms previous models by large margins. Code will be released soon.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:20:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhang", "Peng", ""], ["Li", "Can", ""], ["Qiao", "Liang", ""], ["Cheng", "Zhanzhan", ""], ["Pu", "Shiliang", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2105.06224", "submitter": "Zhanzhan Cheng", "authors": "Liang Qiao and Zaisheng Li and Zhanzhan Cheng and Peng Zhang and\n  Shiliang Pu and Yi Niu and Wenqi Ren and Wenming Tan and Fei Wu", "title": "LGPMA: Complicated Table Structure Recognition with Local and Global\n  Pyramid Mask Alignment", "comments": "Accepted by ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Table structure recognition is a challenging task due to the various\nstructures and complicated cell spanning relations. Previous methods handled\nthe problem starting from elements in different granularities (rows/columns,\ntext regions), which somehow fell into the issues like lossy heuristic rules or\nneglect of empty cell division. Based on table structure characteristics, we\nfind that obtaining the aligned bounding boxes of text region can effectively\nmaintain the entire relevant range of different cells. However, the aligned\nbounding boxes are hard to be accurately predicted due to the visual\nambiguities. In this paper, we aim to obtain more reliable aligned bounding\nboxes by fully utilizing the visual information from both text regions in\nproposed local features and cell relations in global features. Specifically, we\npropose the framework of Local and Global Pyramid Mask Alignment, which adopts\nthe soft pyramid mask learning mechanism in both the local and global feature\nmaps. It allows the predicted boundaries of bounding boxes to break through the\nlimitation of original proposals. A pyramid mask re-scoring module is then\nintegrated to compromise the local and global information and refine the\npredicted boundaries. Finally, we propose a robust table structure recovery\npipeline to obtain the final structure, in which we also effectively solve the\nproblems of empty cells locating and division. Experimental results show that\nthe proposed method achieves competitive and even new state-of-the-art\nperformance on several public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:24:12 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Qiao", "Liang", ""], ["Li", "Zaisheng", ""], ["Cheng", "Zhanzhan", ""], ["Zhang", "Peng", ""], ["Pu", "Shiliang", ""], ["Niu", "Yi", ""], ["Ren", "Wenqi", ""], ["Tan", "Wenming", ""], ["Wu", "Fei", ""]]}, {"id": "2105.06229", "submitter": "Zhanzhan Cheng", "authors": "Hui Jiang and Yunlu Xu and Zhanzhan Cheng and Shiliang Pu and Yi Niu\n  and Wenqi Ren and Fei Wu and Wenming Tan", "title": "Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene\n  Text Recognition", "comments": "Accepted by ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text recognition is a popular topic for its broad applications. In this work,\nwe excavate the implicit task, character counting within the traditional text\nrecognition, without additional labor annotation cost. The implicit task plays\nas an auxiliary branch for complementing the sequential recognition. We design\na two-branch reciprocal feature learning framework in order to adequately\nutilize the features from both the tasks. Through exploiting the complementary\neffect between explicit and implicit tasks, the feature is reliably enhanced.\nExtensive experiments on 7 benchmarks show the advantages of the proposed\nmethods in both text recognition and the new-built character counting tasks. In\naddition, it is convenient yet effective to equip with variable networks and\ntasks. We offer abundant ablation studies, generalizing experiments with deeper\nunderstanding on the tasks. Code is available.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:27:35 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jiang", "Hui", ""], ["Xu", "Yunlu", ""], ["Cheng", "Zhanzhan", ""], ["Pu", "Shiliang", ""], ["Niu", "Yi", ""], ["Ren", "Wenqi", ""], ["Wu", "Fei", ""], ["Tan", "Wenming", ""]]}, {"id": "2105.06238", "submitter": "Reza Azad", "authors": "Afshin Bozorgpour, Reza Azad, Eman Showkatian, Alaa Sulaiman", "title": "Multi-scale Regional Attention Deeplab3+: Multiple Myeloma Plasma Cells\n  Segmentation in Microscopic Images", "comments": "10 pages, 5 figures, presented at ISBI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple myeloma cancer is a type of blood cancer that happens when the\ngrowth of abnormal plasma cells becomes out of control in the bone marrow.\nThere are various ways to diagnose multiple myeloma in bone marrow such as\ncomplete blood count test (CBC) or counting myeloma plasma cell in aspirate\nslide images using manual visualization or through image processing technique.\nIn this work, an automatic deep learning method for the detection and\nsegmentation of multiple myeloma plasma cell have been explored. To this end, a\ntwo-stage deep learning method is designed. In the first stage, the nucleus\ndetection network is utilized to extract each instance of a cell of interest.\nThe extracted instance is then fed to the multi-scale function to generate a\nmulti-scale representation. The objective of the multi-scale function is to\ncapture the shape variation and reduce the effect of object scale on the\ncytoplasm segmentation network. The generated scales are then fed into a\npyramid of cytoplasm networks to learn the segmentation map in various scales.\nOn top of the cytoplasm segmentation network, we included a scale aggregation\nfunction to refine and generate a final prediction. The proposed approach has\nbeen evaluated on the SegPC2021 grand-challenge and ranked second on the final\ntest phase among all teams.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:42:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bozorgpour", "Afshin", ""], ["Azad", "Reza", ""], ["Showkatian", "Eman", ""], ["Sulaiman", "Alaa", ""]]}, {"id": "2105.06247", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi\n  Zhou, Rick Siow Mong Goh", "title": "Video Corpus Moment Retrieval with Contrastive Learning", "comments": "11 pages, 7 figures and 6 tables. Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462874", "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of untrimmed and unsegmented videos, video corpus moment\nretrieval (VCMR) is to retrieve a temporal moment (i.e., a fraction of a video)\nthat semantically corresponds to a given text query. As video and text are from\ntwo distinct feature spaces, there are two general approaches to address VCMR:\n(i) to separately encode each modality representations, then align the two\nmodality representations for query processing, and (ii) to adopt fine-grained\ncross-modal interaction to learn multi-modal representations for query\nprocessing. While the second approach often leads to better retrieval accuracy,\nthe first approach is far more efficient. In this paper, we propose a Retrieval\nand Localization Network with Contrastive Learning (ReLoCLNet) for VCMR. We\nadopt the first approach and introduce two contrastive learning objectives to\nrefine video encoder and text encoder to learn video and text representations\nseparately but with better alignment for VCMR. The video contrastive learning\n(VideoCL) is to maximize mutual information between query and candidate video\nat video-level. The frame contrastive learning (FrameCL) aims to highlight the\nmoment region corresponds to the query at frame-level, within a video.\nExperimental results show that, although ReLoCLNet encodes text and video\nseparately for efficiency, its retrieval accuracy is comparable with baselines\nadopting cross-modal interaction learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:54:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Nan", "Guoshun", ""], ["Zhen", "Liangli", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2105.06292", "submitter": "Eric Postma", "authors": "Koko Visser and Bas Bosma and Eric Postma", "title": "A one-armed CNN for exoplanet detection from light curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Genesis, a one-armed simplified Convolutional Neural Network\n(CNN)for exoplanet detection, and compare it to the more complex, two-armed CNN\ncalled Astronet. Furthermore, we examine how Monte Carlo cross-validation\naffects the estimation of the exoplanet detection performance. Finally, we\nincrease the input resolution twofold to assess its effect on performance. The\nexperiments reveal that (i)the reduced complexity of Genesis, i.e., a more than\n95% reduction in the number of free parameters, incurs a small performance cost\nof about 0.5% compared to Astronet, (ii) Monte Carlo cross-validation provides\na more realistic performance estimate that is almost 0.7% below the original\nestimate, and (iii) the twofold increase in input resolution decreases the\naverage performance by about 0.5%. We conclude by arguing that further\nexploration of shallower CNN architectures may be beneficial in order to\nimprove the generalizability of CNN-based exoplanet detection across surveys.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:00:22 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Visser", "Koko", ""], ["Bosma", "Bas", ""], ["Postma", "Eric", ""]]}, {"id": "2105.06325", "submitter": "Jiaqi Jiang", "authors": "Jiaqi Jiang, Guanqun Cao, Daniel Fernandes Gomes and Shan Luo", "title": "Vision-Guided Active Tactile Perception for Crack Detection and\n  Reconstruction", "comments": "7 pages, accepted by Mediterranean Conference on Control and\n  Automation 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crack detection is of great significance for monitoring the integrity and\nwell-being of the infrastructure such as bridges and underground pipelines,\nwhich are harsh environments for people to access. In recent years, computer\nvision techniques have been applied in detecting cracks in concrete structures.\nHowever, they suffer from variances in light conditions and shadows, lacking\nrobustness and resulting in many false positives. To address the uncertainty in\nvision, human inspectors actively touch the surface of the structures, guided\nby vision, which has not been explored in autonomous crack detection. In this\npaper, we propose a novel approach to detect and reconstruct cracks in concrete\nstructures using vision-guided active tactile perception. Given an RGB-D image\nof a structure, the rough profile of the crack in the structure surface will\nfirst be segmented with a fine-tuned Deep Convolutional Neural Networks, and a\nset of contact points are generated to guide the collection of tactile images\nby a camera-based optical tactile sensor. When contacts are made, a pixel-wise\nmask of the crack can be obtained from the tactile images and therefore the\nprofile of the crack can be refined by aligning the RGB-D image and the tactile\nimages. Extensive experiment results have shown that the proposed method\nimproves the effectiveness and robustness of crack detection and reconstruction\nsignificantly, compared to crack detection with vision only, and has the\npotential to enable robots to help humans with the inspection and repair of the\nconcrete infrastructure.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:25:08 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jiang", "Jiaqi", ""], ["Cao", "Guanqun", ""], ["Gomes", "Daniel Fernandes", ""], ["Luo", "Shan", ""]]}, {"id": "2105.06340", "submitter": "Chuin Hong Yap", "authors": "Chuin Hong Yap, Moi Hoon Yap, Adrian K. Davison, Ryan Cunningham", "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video\n  Sequences using Temporal Oriented Reference Frame", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression spotting is the preliminary step for micro- and\nmacro-expression analysis. The task of reliably spotting such expressions in\nvideo sequences is currently unsolved. The current best systems depend upon\noptical flow methods to extract regional motion features, before categorisation\nof that motion into a specific class of facial movement. Optical flow is\nsusceptible to drift error, which introduces a serious problem for motions with\nlong-term dependencies, such as high frame-rate macro-expression. We propose a\npurely deep learning solution which, rather than track frame differential\nmotion, compares via a convolutional model, each frame with two temporally\nlocal reference frames. Reference frames are sampled according to calculated\nmicro- and macro-expression durations. We show that our solution achieves\nstate-of-the-art performance (F1-score of 0.126) in a dataset of high\nframe-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low\nframe-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep\nlearning model and parameters, including how we use local contrast\nnormalisation, which we show is critical for optimal results. We surpass a\nlimitation in existing methods, and advance the state of deep learning in the\ndomain of facial expression spotting.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:55:06 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:39:31 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Yap", "Chuin Hong", ""], ["Yap", "Moi Hoon", ""], ["Davison", "Adrian K.", ""], ["Cunningham", "Ryan", ""]]}, {"id": "2105.06361", "submitter": "Ziyue Xiang", "authors": "Ziyue Xiang, J\\'anos Horv\\'ath, Sriram Baireddy, Paolo Bestagini,\n  Stefano Tubaro, Edward J. Delp", "title": "Forensic Analysis of Video Files Using Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The unprecedented ease and ability to manipulate video content has led to a\nrapid spread of manipulated media. The availability of video editing tools\ngreatly increased in recent years, allowing one to easily generate\nphoto-realistic alterations. Such manipulations can leave traces in the\nmetadata embedded in video files. This metadata information can be used to\ndetermine video manipulations, brand of video recording device, the type of\nvideo editing tool, and other important evidence. In this paper, we focus on\nthe metadata contained in the popular MP4 video wrapper/container. We describe\nour method for metadata extractor that uses the MP4's tree structure. Our\napproach for analyzing the video metadata produces a more compact\nrepresentation. We will describe how we construct features from the metadata\nand then use dimensionality reduction and nearest neighbor classification for\nforensic analysis of a video file. Our approach allows one to visually inspect\nthe distribution of metadata features and make decisions. The experimental\nresults confirm that the performance of our approach surpasses other methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:40:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xiang", "Ziyue", ""], ["Horv\u00e1th", "J\u00e1nos", ""], ["Baireddy", "Sriram", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""], ["Delp", "Edward J.", ""]]}, {"id": "2105.06369", "submitter": "Xiaofang Wang", "authors": "Xiaofang Wang, Shengcao Cao, Mengtian Li, Kris M. Kitani", "title": "Neighborhood-Aware Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural architecture search (NAS) methods often return an\narchitecture with good search performance but generalizes poorly to the test\nsetting. To achieve better generalization, we propose a novel\nneighborhood-aware NAS formulation to identify flat-minima architectures in the\nsearch space, with the assumption that flat minima generalize better than sharp\nminima. The phrase \"flat-minima architecture\" refers to architectures whose\nperformance is stable under small perturbations in the architecture (e.g.,\nreplacing a convolution with a skip connection). Our formulation takes the\n\"flatness\" of an architecture into account by aggregating the performance over\nthe neighborhood of this architecture. We demonstrate a principled way to apply\nour formulation to existing search algorithms, including sampling-based\nalgorithms and gradient-based algorithms. To facilitate the application to\ngradient-based algorithms, we also propose a differentiable representation for\nthe neighborhood of architectures. Based on our formulation, we propose\nneighborhood-aware random search (NA-RS) and neighborhood-aware differentiable\narchitecture search (NA-DARTS). Notably, by simply augmenting DARTS with our\nformulation, NA-DARTS finds architectures that perform better or on par with\nthose found by state-of-the-art NAS methods on established benchmarks,\nincluding CIFAR-10, CIFAR-100 and ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:56:52 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Xiaofang", ""], ["Cao", "Shengcao", ""], ["Li", "Mengtian", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2105.06399", "submitter": "Ali Jazayeri", "authors": "Ali Jazayeri and Christopher C. Yang", "title": "Frequent Pattern Mining in Continuous-time Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are used as highly expressive tools in different disciplines. In\nrecent years, the analysis and mining of temporal networks have attracted\nsubstantial attention. Frequent pattern mining is considered an essential task\nin the network science literature. In addition to the numerous applications,\nthe investigation of frequent pattern mining in networks directly impacts other\nanalytical approaches, such as clustering, quasi-clique and clique mining, and\nlink prediction. In nearly all the algorithms proposed for frequent pattern\nmining in temporal networks, the networks are represented as sequences of\nstatic networks. Then, the inter- or intra-network patterns are mined. This\ntype of representation imposes a computation-expressiveness trade-off to the\nmining problem. In this paper, we propose a novel representation that can\npreserve the temporal aspects of the network losslessly. Then, we introduce the\nconcept of constrained interval graphs (CIGs). Next, we develop a series of\nalgorithms for mining the complete set of frequent temporal patterns in a\ntemporal network data set. We also consider four different definitions of\nisomorphism to allow noise tolerance in temporal data collection. Implementing\nthe algorithm for three real-world data sets proves the practicality of the\nproposed algorithm and its capability to discover unknown patterns in various\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 02:47:24 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jazayeri", "Ali", ""], ["Yang", "Christopher C.", ""]]}, {"id": "2105.06405", "submitter": "Gurvan Lecuyer", "authors": "Anne Mergy, Gurvan Lecuyer, Dawa Derksen, Dario Izzo", "title": "Vision-based Neural Scene Representations for Spacecraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In advanced mission concepts with high levels of autonomy, spacecraft need to\ninternally model the pose and shape of nearby orbiting objects. Recent works in\nneural scene representations show promising results for inferring generic\nthree-dimensional scenes from optical images. Neural Radiance Fields (NeRF)\nhave shown success in rendering highly specular surfaces using a large number\nof images and their pose. More recently, Generative Radiance Fields (GRAF)\nachieved full volumetric reconstruction of a scene from unposed images only,\nthanks to the use of an adversarial framework to train a NeRF. In this paper,\nwe compare and evaluate the potential of NeRF and GRAF to render novel views\nand extract the 3D shape of two different spacecraft, the Soil Moisture and\nOcean Salinity satellite of ESA's Living Planet Programme and a generic cube\nsat. Considering the best performances of both models, we observe that NeRF has\nthe ability to render more accurate images regarding the material specularity\nof the spacecraft and its pose. For its part, GRAF generates precise novel\nviews with accurate details even when parts of the satellites are shadowed\nwhile having the significant advantage of not needing any information about the\nrelative pose.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:35:05 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Mergy", "Anne", ""], ["Lecuyer", "Gurvan", ""], ["Derksen", "Dawa", ""], ["Izzo", "Dario", ""]]}, {"id": "2105.06407", "submitter": "Robin Kips", "authors": "Robin Kips, Ruowei Jiang, Sileye Ba, Edmund Phung, Parham Aarabi,\n  Pietro Gori, Matthieu Perrot, Isabelle Bloch", "title": "Deep Graphics Encoder for Real-Time Video Makeup Synthesis from Example", "comments": "CVPR 2021 Workshop AI for Content Creation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While makeup virtual-try-on is now widespread, parametrizing a computer\ngraphics rendering engine for synthesizing images of a given cosmetics product\nremains a challenging task. In this paper, we introduce an inverse computer\ngraphics method for automatic makeup synthesis from a reference image, by\nlearning a model that maps an example portrait image with makeup to the space\nof rendering parameters. This method can be used by artists to automatically\ncreate realistic virtual cosmetics image samples, or by consumers, to virtually\ntry-on a makeup extracted from their favorite reference image.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:28:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Kips", "Robin", ""], ["Jiang", "Ruowei", ""], ["Ba", "Sileye", ""], ["Phung", "Edmund", ""], ["Aarabi", "Parham", ""], ["Gori", "Pietro", ""], ["Perrot", "Matthieu", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2105.06409", "submitter": "Fred Bertsch", "authors": "Trung Le, Ryan Poplin, Fred Bertsch, Andeep Singh Toor, Margaret L. Oh", "title": "SyntheticFur dataset for neural rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new dataset called SyntheticFur built specifically for machine\nlearning training. The dataset consists of ray traced synthetic fur renders\nwith corresponding rasterized input buffers and simulation data files. We\nprocedurally generated approximately 140,000 images and 15 simulations with\nHoudini. The images consist of fur groomed with different skin primitives and\nmove with various motions in a predefined set of lighting environments. We also\ndemonstrated how the dataset could be used with neural rendering to\nsignificantly improve fur graphics using inexpensive input buffers by training\na conditional generative adversarial network with perceptual loss. We hope the\navailability of such high fidelity fur renders will encourage new advances with\nneural rendering for a variety of applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:31:15 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Le", "Trung", ""], ["Poplin", "Ryan", ""], ["Bertsch", "Fred", ""], ["Toor", "Andeep Singh", ""], ["Oh", "Margaret L.", ""]]}, {"id": "2105.06421", "submitter": "Mahdi Pourmirzaei", "authors": "Mahdi Pourmirzaei, Farzaneh Esmaili, Gholam Ali Montazer", "title": "Using Self-Supervised Co-Training to Improve Facial Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, at first, the impact of ImageNet pre-training on Facial\nExpression Recognition (FER) was tested under different augmentation levels. It\ncould be seen from the results that training from scratch could reach better\nperformance compared to ImageNet fine-tuning at stronger augmentation levels.\nAfter that, a framework was proposed for standard Supervised Learning (SL),\ncalled Hybrid Learning (HL) which used Self-Supervised co-training with SL in\nMulti-Task Learning (MTL) manner. Leveraging Self-Supervised Learning (SSL)\ncould gain additional information from input data like spatial information from\nfaces which helped the main SL task. It is been investigated how this method\ncould be used for FER problems with self-supervised pre-tasks such as Jigsaw\npuzzling and in-painting. The supervised head (SH) was helped by these two\nmethods to lower the error rate under different augmentations and low data\nregime in the same training settings. The state-of-the-art was reached on\nAffectNet via two completely different HL methods, without utilizing additional\ndatasets. Moreover, HL's effect was shown on two different facial-related\nproblem, head poses estimation and gender recognition, which concluded to\nreduce in error rate by up to 9% and 1% respectively. Also, we saw that the HL\nmethods prevented the model from reaching overfitting.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:56:36 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pourmirzaei", "Mahdi", ""], ["Esmaili", "Farzaneh", ""], ["Montazer", "Gholam Ali", ""]]}, {"id": "2105.06441", "submitter": "Mona Zehni", "authors": "Safa Messaoud, Ismini Lourentzou, Assma Boughoula, Mona Zehni, Zhizhen\n  Zhao, Chengxiang Zhai, Alexander G. Schwing", "title": "DeepQAMVS: Query-Aware Hierarchical Pointer Networks for Multi-Video\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent growth of web video sharing platforms has increased the demand for\nsystems that can efficiently browse, retrieve and summarize video content.\nQuery-aware multi-video summarization is a promising technique that caters to\nthis demand. In this work, we introduce a novel Query-Aware Hierarchical\nPointer Network for Multi-Video Summarization, termed DeepQAMVS, that jointly\noptimizes multiple criteria: (1) conciseness, (2) representativeness of\nimportant query-relevant events and (3) chronological soundness. We design a\nhierarchical attention model that factorizes over three distributions, each\ncollecting evidence from a different modality, followed by a pointer network\nthat selects frames to include in the summary. DeepQAMVS is trained with\nreinforcement learning, incorporating rewards that capture representativeness,\ndiversity, query-adaptability and temporal coherence. We achieve\nstate-of-the-art results on the MVS1K dataset, with inference time scaling\nlinearly with the number of input video frames.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:33:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Messaoud", "Safa", ""], ["Lourentzou", "Ismini", ""], ["Boughoula", "Assma", ""], ["Zehni", "Mona", ""], ["Zhao", "Zhizhen", ""], ["Zhai", "Chengxiang", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2105.06453", "submitter": "Chen Sun", "authors": "Alexander Pashevich and Cordelia Schmid and Chen Sun", "title": "Episodic Transformer for Vision-and-Language Navigation", "comments": "Code available at https://github.com/alexpashevich/E.T.; 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction and navigation defined by natural language instructions in\ndynamic environments pose significant challenges for neural agents. This paper\nfocuses on addressing two challenges: handling long sequence of subtasks, and\nunderstanding complex human instructions. We propose Episodic Transformer\n(E.T.), a multimodal transformer that encodes language inputs and the full\nepisode history of visual observations and actions. To improve training, we\nleverage synthetic instructions as an intermediate representation that\ndecouples understanding the visual appearance of an environment from the\nvariations of natural language instructions. We demonstrate that encoding the\nhistory with a transformer is critical to solve compositional tasks, and that\npretraining and joint training with synthetic instructions further improve the\nperformance. Our approach sets a new state of the art on the challenging ALFRED\nbenchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test\nsplits.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:51:46 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pashevich", "Alexander", ""], ["Schmid", "Cordelia", ""], ["Sun", "Chen", ""]]}, {"id": "2105.06458", "submitter": "Robin Rombach", "authors": "Manuel Jahn and Robin Rombach and Bj\\\"orn Ommer", "title": "High-Resolution Complex Scene Synthesis with Transformers", "comments": "AI for Content Creation Workshop, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of coarse-grained layouts for controllable synthesis of complex scene\nimages via deep generative models has recently gained popularity. However,\nresults of current approaches still fall short of their promise of\nhigh-resolution synthesis. We hypothesize that this is mostly due to the highly\nengineered nature of these approaches which often rely on auxiliary losses and\nintermediate steps such as mask generators. In this note, we present an\northogonal approach to this task, where the generative model is based on pure\nlikelihood training without additional objectives. To do so, we first optimize\na powerful compression model with adversarial training which learns to\nreconstruct its inputs via a discrete latent bottleneck and thereby effectively\nstrips the latent representation of high-frequency details such as texture.\nSubsequently, we train an autoregressive transformer model to learn the\ndistribution of the discrete image representations conditioned on a tokenized\nversion of the layouts. Our experiments show that the resulting system is able\nto synthesize high-quality images consistent with the given layouts. In\nparticular, we improve the state-of-the-art FID score on COCO-Stuff and on\nVisual Genome by up to 19% and 53% and demonstrate the synthesis of images up\nto 512 x 512 px on COCO and Open Images.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:56:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jahn", "Manuel", ""], ["Rombach", "Robin", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2105.06460", "submitter": "Tianwei Yin", "authors": "Tianwei Yin, Zihui Wu, He Sun, Adrian V. Dalca, Yisong Yue, Katherine\n  L. Bouman", "title": "End-to-End Sequential Sampling and Reconstruction for MR Imaging", "comments": "Code and supplementary materials are available at\n  http://imaging.cms.caltech.edu/seq-mri", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated MRI shortens acquisition time by subsampling in the measurement\nk-space. Recovering a high-fidelity anatomical image from subsampled\nmeasurements requires close cooperation between two components: (1) a sampler\nthat chooses the subsampling pattern and (2) a reconstructor that recovers\nimages from incomplete measurements. In this paper, we leverage the sequential\nnature of MRI measurements, and propose a fully differentiable framework that\njointly learns a sequential sampling policy simultaneously with a\nreconstruction strategy. This co-designed framework is able to adapt during\nacquisition in order to capture the most informative measurements for a\nparticular target (Figure 1). Experimental results on the fastMRI knee dataset\ndemonstrate that the proposed approach successfully utilizes intermediate\ninformation during the sampling process to boost reconstruction performance. In\nparticular, our proposed method outperforms the current state-of-the-art\nlearned k-space sampling baseline on up to 96.96% of test samples. We also\ninvestigate the individual and collective benefits of the sequential sampling\nand co-design strategies. Code and more visualizations are available at\nhttp://imaging.cms.caltech.edu/seq-mri\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:56:18 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yin", "Tianwei", ""], ["Wu", "Zihui", ""], ["Sun", "He", ""], ["Dalca", "Adrian V.", ""], ["Yue", "Yisong", ""], ["Bouman", "Katherine L.", ""]]}, {"id": "2105.06461", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Ishan Misra, Alexander G. Schwing, and Rohit Girdhar", "title": "3D Spatial Recognition without Spatially Labeled 3D", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition,\nrequiring only scene-level class tags as supervision. WyPR jointly addresses\nthree core 3D recognition tasks: point-level semantic segmentation, 3D proposal\ngeneration, and 3D object detection, coupling their predictions through self\nand cross-task consistency losses. We show that in conjunction with standard\nmultiple-instance learning objectives, WyPR can detect and segment objects in\npoint cloud data without access to any spatial labels at training time. We\ndemonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming\nprior state of the art on weakly-supervised segmentation by more than 6% mIoU.\nIn addition, we set up the first benchmark for weakly-supervised 3D object\ndetection on both datasets, where WyPR outperforms standard approaches and\nestablishes strong baselines for future work.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:58:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Misra", "Ishan", ""], ["Schwing", "Alexander G.", ""], ["Girdhar", "Rohit", ""]]}, {"id": "2105.06462", "submitter": "Dan Casas", "authors": "Igor Santesteban, Nils Thuerey, Miguel A. Otaduy, Dan Casas", "title": "Self-Supervised Collision Handling via Generative 3D Garment Models for\n  Virtual Try-On", "comments": "Accepted to CVPR 2021. Project website\n  http://mslab.es/projects/SelfSupervisedGarmentCollisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new generative model for 3D garment deformations that enables us\nto learn, for the first time, a data-driven method for virtual try-on that\neffectively addresses garment-body collisions. In contrast to existing methods\nthat require an undesirable postprocessing step to fix garment-body\ninterpenetrations at test time, our approach directly outputs 3D garment\nconfigurations that do not collide with the underlying body. Key to our success\nis a new canonical space for garments that removes pose-and-shape deformations\nalready captured by a new diffused human body model, which extrapolates body\nsurface properties such as skinning weights and blendshapes to any 3D point. We\nleverage this representation to train a generative model with a novel\nself-supervised collision term that learns to reliably solve garment-body\ninterpenetrations. We extensively evaluate and compare our results with\nrecently proposed data-driven methods, and show that our method is the first to\nsuccessfully address garment-body contact in unseen body shapes and motions,\nwithout compromising realism and detail.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:58:20 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Santesteban", "Igor", ""], ["Thuerey", "Nils", ""], ["Otaduy", "Miguel A.", ""], ["Casas", "Dan", ""]]}, {"id": "2105.06463", "submitter": "Haiping Wu", "authors": "Haiping Wu, Xiaolong Wang", "title": "Contrastive Learning of Image Representations with Cross-Video\n  Cycle-Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have advanced the performance of self-supervised representation\nlearning by a large margin. The core among these methods is intra-image\ninvariance learning. Two different transformations of one image instance are\nconsidered as a positive sample pair, where various tasks are designed to learn\ninvariant representations by comparing the pair. Analogically, for video data,\nrepresentations of frames from the same video are trained to be closer than\nframes from other videos, i.e. intra-video invariance. However, cross-video\nrelation has barely been explored for visual representation learning. Unlike\nintra-video invariance, ground-truth labels of cross-video relation is usually\nunavailable without human labors. In this paper, we propose a novel contrastive\nlearning method which explores the cross-video relation by using\ncycle-consistency for general image representation learning. This allows to\ncollect positive sample pairs across different video instances, which we\nhypothesize will lead to higher-level semantics. We validate our method by\ntransferring our image representation to multiple downstream tasks including\nvisual object tracking, image classification, and action recognition. We show\nsignificant improvement over state-of-the-art contrastive learning methods.\nProject page is available at https://happywu.github.io/cycle_contrast_video.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:11 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wu", "Haiping", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2105.06464", "submitter": "Zhiding Yu", "authors": "Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Radhakrishnan,\n  Guilin Liu, Yuke Zhu, Larry S. Davis, Anima Anandkumar", "title": "DiscoBox: Weakly Supervised Instance Segmentation and Semantic\n  Correspondence from Box Supervision", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DiscoBox, a novel framework that jointly learns instance\nsegmentation and semantic correspondence using bounding box supervision.\nSpecifically, we propose a self-ensembling framework where instance\nsegmentation and semantic correspondence are jointly guided by a structured\nteacher in addition to the bounding box supervision. The teacher is a\nstructured energy model incorporating a pairwise potential and a cross-image\npotential to model the pairwise pixel relationships both within and across the\nboxes. Minimizing the teacher energy simultaneously yields refined object masks\nand dense correspondences between intra-class objects, which are taken as\npseudo-labels to supervise the task network and provide positive/negative\ncorrespondence pairs for dense constrastive learning. We show a symbiotic\nrelationship where the two tasks mutually benefit from each other. Our best\nmodel achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly\nsupervised methods and is competitive to supervised methods. We also obtain\nstate of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with\nreal-time inference.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:41 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 23:19:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lan", "Shiyi", ""], ["Yu", "Zhiding", ""], ["Choy", "Christopher", ""], ["Radhakrishnan", "Subhashree", ""], ["Liu", "Guilin", ""], ["Zhu", "Yuke", ""], ["Davis", "Larry S.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2105.06466", "submitter": "Steven Liu", "authors": "Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu,\n  Bryan Russell", "title": "Editing Conditional Radiance Fields", "comments": "Code: https://github.com/stevliu/editnerf Website:\n  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional\n  details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A neural radiance field (NeRF) is a scene model supporting high-quality view\nsynthesis, optimized per scene. In this paper, we explore enabling user editing\nof a category-level NeRF - also known as a conditional radiance field - trained\non a shape category. Specifically, we introduce a method for propagating coarse\n2D user scribbles to the 3D space, to modify the color or shape of a local\nregion. First, we propose a conditional radiance field that incorporates new\nmodular network components, including a shape branch that is shared across\nobject instances. Observing multiple instances of the same category, our model\nlearns underlying part semantics without any supervision, thereby allowing the\npropagation of coarse 2D user scribbles to the entire 3D region (e.g., chair\nseat). Next, we propose a hybrid network update strategy that targets specific\nnetwork components, which balances efficiency and accuracy. During user\ninteraction, we formulate an optimization problem that both satisfies the\nuser's constraints and preserves the original object structure. We demonstrate\nour approach on various editing tasks over three shape datasets and show that\nit outperforms prior neural editing approaches. Finally, we edit the appearance\nand shape of a real photograph and show that the edit propagates to\nextrapolated novel views.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:48 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 16:30:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liu", "Steven", ""], ["Zhang", "Xiuming", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""], ["Russell", "Bryan", ""]]}, {"id": "2105.06468", "submitter": "Chen Gao", "authors": "Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang", "title": "Dynamic View Synthesis from Dynamic Monocular Video", "comments": "Project webpage: https://free-view-video.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an algorithm for generating novel views at arbitrary viewpoints\nand any input time step given a monocular video of a dynamic scene. Our work\nbuilds upon recent advances in neural implicit representation and uses\ncontinuous and differentiable functions for modeling the time-varying structure\nand the appearance of the scene. We jointly train a time-invariant static NeRF\nand a time-varying dynamic NeRF, and learn how to blend the results in an\nunsupervised manner. However, learning this implicit function from a single\nvideo is highly ill-posed (with infinitely many solutions that match the input\nvideo). To resolve the ambiguity, we introduce regularization losses to\nencourage a more physically plausible solution. We show extensive quantitative\nand qualitative results of dynamic view synthesis from casually captured\nvideos.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:50 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Gao", "Chen", ""], ["Saraf", "Ayush", ""], ["Kopf", "Johannes", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2105.06496", "submitter": "Matt Groh", "authors": "Matthew Groh, Ziv Epstein, Chaz Firestone, Rosalind Picard", "title": "Comparing Human and Machine Deepfake Detection with Affective and\n  Holistic Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent emergence of deepfake videos leads to an important societal\nquestion: how can we know if a video that we watch is real or fake? In three\nonline studies with 15,016 participants, we present authentic videos and\ndeepfakes and ask participants to identify which is which. We compare the\nperformance of ordinary participants against the leading computer vision\ndeepfake detection model and find them similarly accurate while making\ndifferent kinds of mistakes. Together, participants with access to the model's\nprediction are more accurate than either alone, but inaccurate model\npredictions often decrease participants' accuracy. We embed randomized\nexperiments and find: incidental anger decreases participants' performance and\nobstructing holistic visual processing of faces also hinders participants'\nperformance while mostly not affecting the model's. These results suggest that\nconsidering emotional influences and harnessing specialized, holistic visual\nprocessing of ordinary people could be promising defenses against\nmachine-manipulated media.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:22:16 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Groh", "Matthew", ""], ["Epstein", "Ziv", ""], ["Firestone", "Chaz", ""], ["Picard", "Rosalind", ""]]}, {"id": "2105.06508", "submitter": "Shailesh Arya", "authors": "Shailesh Arya", "title": "Internet of Things (IoT) Based Video Analytics: a use case of Smart\n  Doorbell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vision of the internet of things (IoT) is a reality now. IoT devices are\ngetting cheaper, smaller. They are becoming more and more computationally and\nenergy-efficient. The global market of IoT-based video analytics has seen\nsignificant growth in recent years and it is expected to be a growing market\nsegment. For any IoT-based video analytics application, few key points\nrequired, such as cost-effectiveness, widespread use, flexible design, accurate\nscene detection, reusability of the framework. Video-based smart doorbell\nsystem is one such application domain for video analytics where many commercial\nofferings are available in the consumer market. However, such existing\nofferings are costly, monolithic, and proprietary. Also, there will be a\ntrade-off between accuracy and portability. To address the foreseen problems,\nI'm proposing a distributed framework for video analytics with a use case of a\nsmart doorbell system. The proposed framework uses AWS cloud services as a base\nplatform and to meet the price affordability constraint, the system was\nimplemented on affordable Raspberry Pi. The smart doorbell will be able to\nrecognize the known/unknown person with at most accuracy. The smart doorbell\nsystem is also having additional detection functionalities such as harmful\nweapon detection, noteworthy vehicle detection, animal/pet detection. An iOS\napplication is specifically developed for this implementation which can receive\nthe notification from the smart doorbell in real-time. Finally, the paper also\nmentions the classical approaches for video analytics, their feasibility in\nimplementing with this use-case, and comparative analysis in terms of accuracy\nand time required to detect an object in the frame is carried out. Results\nconclude that AWS cloud-based approach is worthy for this smart doorbell use\ncase.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:48:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Arya", "Shailesh", ""]]}, {"id": "2105.06524", "submitter": "Hongpeng Guo", "authors": "Hongpeng Guo, Shuochao Yao, Zhe Yang, Qian Zhou, Klara Nahrstedt", "title": "CrossRoI: Cross-camera Region of Interest Optimization for Efficient\n  Real Time Video Analytics at Scale", "comments": "accepted in 12th ACM Multimedia Systems Conference (MMsys 21')", "journal-ref": null, "doi": "10.1145/3458305.3463381", "report-no": null, "categories": "cs.DC cs.CV cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video cameras are pervasively deployed in city scale for public good or\ncommunity safety (i.e. traffic monitoring or suspected person tracking).\nHowever, analyzing large scale video feeds in real time is data intensive and\nposes severe challenges to network and computation systems today. We present\nCrossRoI, a resource-efficient system that enables real time video analytics at\nscale via harnessing the videos content associations and redundancy across a\nfleet of cameras. CrossRoI exploits the intrinsic physical correlations of\ncross-camera viewing fields to drastically reduce the communication and\ncomputation costs. CrossRoI removes the repentant appearances of same objects\nin multiple cameras without harming comprehensive coverage of the scene.\nCrossRoI operates in two phases - an offline phase to establish cross-camera\ncorrelations, and an efficient online phase for real time video inference.\nExperiments on real-world video feeds show that CrossRoI achieves 42% - 65%\nreduction for network overhead and 25% - 34% reduction for response delay in\nreal time video analytics applications with more than 99% query accuracy, when\ncompared to baseline methods. If integrated with SotA frame filtering systems,\nthe performance gains of CrossRoI reach 50% - 80% (network overhead) and 33% -\n61% (end-to-end delay).\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:29:14 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Guo", "Hongpeng", ""], ["Yao", "Shuochao", ""], ["Yang", "Zhe", ""], ["Zhou", "Qian", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2105.06528", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla, Hamid Reza Vaezi Joze, and Vishal M Patel", "title": "Network Architecture Search for Face Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various factors such as ambient lighting conditions, noise, motion blur, etc.\naffect the quality of captured face images. Poor quality face images often\nreduce the performance of face analysis and recognition systems. Hence, it is\nimportant to enhance the quality of face images collected in such conditions.\nWe present a multi-task face restoration network, called Network Architecture\nSearch for Face Enhancement (NASFE), which can enhance poor quality face images\ncontaining a single degradation (i.e. noise or blur) or multiple degradations\n(noise+blur+low-light). During training, NASFE uses clean face images of a\nperson present in the degraded image to extract the identity information in\nterms of features for restoring the image. Furthermore, the network is guided\nby an identity-loss so that the identity in-formation is maintained in the\nrestored image. Additionally, we propose a network architecture search-based\nfusion network in NASFE which fuses the task-specific features that are\nextracted using the task-specific encoders. We introduce FFT-op and deveiling\noperators in the fusion network to efficiently fuse the task-specific features.\nComprehensive experiments on synthetic and real images demonstrate that the\nproposed method outperforms many recent state-of-the-art face restoration and\nenhancement methods in terms of quantitative and visual performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:46:05 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yasarla", "Rajeev", ""], ["Joze", "Hamid Reza Vaezi", ""], ["Patel", "Vishal M", ""]]}, {"id": "2105.06544", "submitter": "Chuanlong Li", "authors": "Chuanlong Li", "title": "Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets", "comments": "Update the segmentation examples figure (Fig. 7); Add the\n  implementation link; Language related issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cerebrovascular accident, or commonly known as stroke, is an acute disease\nwith extreme impact on patients and healthcare systems and is the second\nlargest cause of death worldwide. Fast and precise stroke lesion detection and\nlocation is an extreme important process with regards to stroke diagnosis,\ntreatment, and prognosis. Except from the manual segmentation approach, machine\nlearning based segmentation methods are the most promising ones when\nconsidering efficiency and accuracy, and convolutional neural network based\nmodels are the first of its kind. However, most of these neural network models\ndo not really align with the brain anatomical structures. Intuitively, this\nwork presents a more brain alike model which mimics the anatomical structure of\nthe human visual cortex. Through the preliminary experiments on the stroke\nlesion segmentation task, the proposed model is found to be able to perform\nequally well or better to the de-facto standard U-Net. Part of the\nimplementation will be made available at https://github.com/DarkoBomer/VCA-Net.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:39:29 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 07:01:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Chuanlong", ""]]}, {"id": "2105.06562", "submitter": "Chethan M Parameshwara", "authors": "Chethan M. Parameshwara, Simin Li, Cornelia Ferm\\\"uller, Nitin J.\n  Sanket, Matthew S. Evanusa, Yiannis Aloimonos", "title": "SpikeMS: Deep Spiking Neural Network for Motion Segmentation", "comments": "7 pages, 6 figures, 3 tables, Under review IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking Neural Networks (SNN) are the so-called third generation of neural\nnetworks which attempt to more closely match the functioning of the biological\nbrain. They inherently encode temporal data, allowing for training with less\nenergy usage and can be extremely energy efficient when coded on neuromorphic\nhardware. In addition, they are well suited for tasks involving event-based\nsensors, which match the event-based nature of the SNN. However, SNNs have not\nbeen as effectively applied to real-world, large-scale tasks as standard\nArtificial Neural Networks (ANNs) due to the algorithmic and training\ncomplexity. To exacerbate the situation further, the input representation is\nunconventional and requires careful analysis and deep understanding. In this\npaper, we propose \\textit{SpikeMS}, the first deep encoder-decoder SNN\narchitecture for the real-world large-scale problem of motion segmentation\nusing the event-based DVS camera as input. To accomplish this, we introduce a\nnovel spatio-temporal loss formulation that includes both spike counts and\nclassification labels in conjunction with the use of new techniques for SNN\nbackpropagation. In addition, we show that \\textit{SpikeMS} is capable of\n\\textit{incremental predictions}, or predictions from smaller amounts of test\ndata than it is trained on. This is invaluable for providing outputs even with\npartial input data for low-latency applications and those requiring fast\npredictions. We evaluated \\textit{SpikeMS} on challenging synthetic and\nreal-world sequences from EV-IMO, EED and MOD datasets and achieving results on\na par with a comparable ANN method, but using potentially 50 times less power.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 21:34:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Parameshwara", "Chethan M.", ""], ["Li", "Simin", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Sanket", "Nitin J.", ""], ["Evanusa", "Matthew S.", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2105.06582", "submitter": "Derek Prijatelj", "authors": "Derek S. Prijatelj (1), Samuel Grieggs (1), Futoshi Yumoto (2), Eric\n  Robertson (2), Walter J. Scheirer (1) ((1) University of Notre Dame, (2) PAR\n  Government)", "title": "Handwriting Recognition with Novelty", "comments": "16 pages, 3 Figures, 2 Tables, To be published in ICDAR 2021.\n  Camera-ready version 1. Supplementary Material 22 pages, 4 Figures, 18\n  Tables. Moved novelty type examples from supp mat to main. Added brief\n  explanation of usefulness of formalization. Added comment on joint\n  information between transcription and style tasks in CRNN's encoding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an agent-centric approach to handle novelty in the\nvisual recognition domain of handwriting recognition (HWR). An ideal\ntranscription agent would rival or surpass human perception, being able to\nrecognize known and new characters in an image, and detect any stylistic\nchanges that may occur within or across documents. A key confound is the\npresence of novelty, which has continued to stymie even the best machine\nlearning-based algorithms for these tasks. In handwritten documents, novelty\ncan be a change in writer, character attributes, writing attributes, or overall\ndocument appearance, among other things. Instead of looking at each aspect\nindependently, we suggest that an integrated agent that can process known\ncharacters and novelties simultaneously is a better strategy. This paper\nformalizes the domain of handwriting recognition with novelty, describes a\nbaseline agent, introduces an evaluation protocol with benchmark data, and\nprovides experimentation to set the state-of-the-art. Results show feasibility\nfor the agent-centric approach, but more work is needed to approach\nhuman-levels of reading ability, giving the HWR community a formal basis to\nbuild upon as they solve this challenging problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:01:07 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 20:14:41 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Prijatelj", "Derek S.", ""], ["Grieggs", "Samuel", ""], ["Yumoto", "Futoshi", ""], ["Robertson", "Eric", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "2105.06599", "submitter": "Mohsen Gholami", "authors": "Mohsen Gholami, Ahmad Rezaei, Helge Rhodin, Rabab Ward and Z. Jane\n  Wang", "title": "TriPose: A Weakly-Supervised 3D Human Pose Estimation via Triangulation\n  from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D human poses from video is a challenging problem. The lack of 3D\nhuman pose annotations is a major obstacle for supervised training and for\ngeneralization to unseen datasets. In this work, we address this problem by\nproposing a weakly-supervised training scheme that does not require 3D\nannotations or calibrated cameras. The proposed method relies on temporal\ninformation and triangulation. Using 2D poses from multiple views as the input,\nwe first estimate the relative camera orientations and then generate 3D poses\nvia triangulation. The triangulation is only applied to the views with high 2D\nhuman joint confidence. The generated 3D poses are then used to train a\nrecurrent lifting network (RLN) that estimates 3D poses from 2D poses. We\nfurther apply a multi-view re-projection loss to the estimated 3D poses and\nenforce the 3D poses estimated from multi-views to be consistent. Therefore,\nour method relaxes the constraints in practice, only multi-view videos are\nrequired for training, and is thus convenient for in-the-wild settings. At\ninference, RLN merely requires single-view videos. The proposed method\noutperforms previous works on two challenging datasets, Human3.6M and\nMPI-INF-3DHP. Codes and pretrained models will be publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 00:46:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gholami", "Mohsen", ""], ["Rezaei", "Ahmad", ""], ["Rhodin", "Helge", ""], ["Ward", "Rabab", ""], ["Wang", "Z. Jane", ""]]}, {"id": "2105.06620", "submitter": "Yong Li", "authors": "Yong Li, Shiguang Shan", "title": "Meta Auxiliary Learning for Facial Action Unit Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep neural networks on facial action unit (AU)\ndetection, better performance depends on a large number of training images with\naccurate AU annotations. However, labeling AU is time-consuming, expensive, and\nerror-prone. Considering AU detection and facial expression recognition (FER)\nare two highly correlated tasks, and facial expression (FE) is relatively easy\nto annotate, we consider learning AU detection and FER in a multi-task manner.\nHowever, the performance of the AU detection task cannot be always enhanced due\nto the negative transfer in the multi-task scenario. To alleviate this issue,\nwe propose a Meta Auxiliary Learning method (MAL) that automatically selects\nhighly related FE samples by learning adaptative weights for the training FE\nsamples in a meta learning manner. The learned sample weights alleviate the\nnegative transfer from two aspects: 1) balance the loss of each task\nautomatically, and 2) suppress the weights of FE samples that have large\nuncertainties. Experimental results on several popular AU datasets demonstrate\nMAL consistently improves the AU detection performance compared with the\nstate-of-the-art multi-task and auxiliary learning methods. MAL automatically\nestimates adaptive weights for the auxiliary FE samples according to their\nsemantic relevance with the primary AU detection task.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 02:28:40 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Li", "Yong", ""], ["Shan", "Shiguang", ""]]}, {"id": "2105.06623", "submitter": "Chong Liu", "authors": "Chong Liu and Yuqi Zhang and Hao Luo and Jiasheng Tang and Weihua Chen\n  and Xianzhe Xu and Fan Wang and Hao Li and Yi-Dong Shen", "title": "City-Scale Multi-Camera Vehicle Tracking Guided by Crossroad Zones", "comments": "CVPR 2021 AI CITY CHALLENGE City-Scale Multi-Camera Vehicle Tracking\n  Top 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-Target Multi-Camera Tracking has a wide range of applications and is\nthe basis for many advanced inferences and predictions. This paper describes\nour solution to the Track 3 multi-camera vehicle tracking task in 2021 AI City\nChallenge (AICITY21). This paper proposes a multi-target multi-camera vehicle\ntracking framework guided by the crossroad zones. The framework includes: (1)\nUse mature detection and vehicle re-identification models to extract targets\nand appearance features. (2) Use modified JDETracker (without detection module)\nto track single-camera vehicles and generate single-camera tracklets. (3)\nAccording to the characteristics of the crossroad, the Tracklet Filter Strategy\nand the Direction Based Temporal Mask are proposed. (4) Propose Sub-clustering\nin Adjacent Cameras for multi-camera tracklets matching. Through the above\ntechniques, our method obtained an IDF1 score of 0.8095, ranking first on the\nleaderboard. The code have released: https://github.com/LCFractal/AIC21-MTMC.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 03:01:17 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Liu", "Chong", ""], ["Zhang", "Yuqi", ""], ["Luo", "Hao", ""], ["Tang", "Jiasheng", ""], ["Chen", "Weihua", ""], ["Xu", "Xianzhe", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""], ["Shen", "Yi-Dong", ""]]}, {"id": "2105.06625", "submitter": "Joshua Engelsma", "authors": "Anil K. Jain, Debayan Deb and Joshua J. Engelsma", "title": "Biometrics: Trust, but Verify", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, biometric recognition has exploded into a plethora\nof different applications around the globe. This proliferation can be\nattributed to the high levels of authentication accuracy and user convenience\nthat biometric recognition systems afford end-users. However, in-spite of the\nsuccess of biometric recognition systems, there are a number of outstanding\nproblems and concerns pertaining to the various sub-modules of biometric\nrecognition systems that create an element of mistrust in their use - both by\nthe scientific community and also the public at large. Some of these problems\ninclude: i) questions related to system recognition performance, ii) security\n(spoof attacks, adversarial attacks, template reconstruction attacks and\ndemographic information leakage), iii) uncertainty over the bias and fairness\nof the systems to all users, iv) explainability of the seemingly black-box\ndecisions made by most recognition systems, and v) concerns over data\ncentralization and user privacy. In this paper, we provide an overview of each\nof the aforementioned open-ended challenges. We survey work that has been\nconducted to address each of these concerns and highlight the issues requiring\nfurther attention. Finally, we provide insights into how the biometric\ncommunity can address core biometric recognition systems design issues to\nbetter instill trust, fairness, and security for all.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 03:07:25 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 16:02:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jain", "Anil K.", ""], ["Deb", "Debayan", ""], ["Engelsma", "Joshua J.", ""]]}, {"id": "2105.06640", "submitter": "Alexander Wong", "authors": "Maya Pavlova, Naomi Terhljan, Audrey G. Chung, Andy Zhao, Siddharth\n  Surana, Hossein Aboutalebi, Hayden Gunraj, Ali Sabri, Amer Alaref, and\n  Alexander Wong", "title": "COVID-Net CXR-2: An Enhanced Deep Convolutional Neural Network Design\n  for Detection of COVID-19 Cases from Chest X-ray Images", "comments": "12 pages. arXiv admin note: text overlap with arXiv:2105.00256", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 pandemic continues to devastate globally, the use of chest\nX-ray (CXR) imaging as a complimentary screening strategy to RT-PCR testing\ncontinues to grow given its routine clinical use for respiratory complaint. As\npart of the COVID-Net open source initiative, we introduce COVID-Net CXR-2, an\nenhanced deep convolutional neural network design for COVID-19 detection from\nCXR images built using a greater quantity and diversity of patients than the\noriginal COVID-Net. To facilitate this, we also introduce a new benchmark\ndataset composed of 19,203 CXR images from a multinational cohort of 16,656\npatients from at least 51 countries, making it the largest, most diverse\nCOVID-19 CXR dataset in open access form. The COVID-Net CXR-2 network achieves\nsensitivity and positive predictive value of 95.5%/97.0%, respectively, and was\naudited in a transparent and responsible manner. Explainability-driven\nperformance validation was used during auditing to gain deeper insights in its\ndecision-making behaviour and to ensure clinically relevant factors are\nleveraged for improving trust in its usage. Radiologist validation was also\nconducted, where select cases were reviewed and reported on by two\nboard-certified radiologists with over 10 and 19 years of experience,\nrespectively, and showed that the critical factors leveraged by COVID-Net CXR-2\nare consistent with radiologist interpretations. While not a production-ready\nsolution, we hope the open-source, open-access release of COVID-Net CXR-2 and\nthe respective CXR benchmark dataset will encourage researchers, clinical\nscientists, and citizen scientists to accelerate advancements and innovations\nin the fight against the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 04:29:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Pavlova", "Maya", ""], ["Terhljan", "Naomi", ""], ["Chung", "Audrey G.", ""], ["Zhao", "Andy", ""], ["Surana", "Siddharth", ""], ["Aboutalebi", "Hossein", ""], ["Gunraj", "Hayden", ""], ["Sabri", "Ali", ""], ["Alaref", "Amer", ""], ["Wong", "Alexander", ""]]}, {"id": "2105.06653", "submitter": "Yi Zhang", "authors": "Zhiwen Wang, Wenjun Xia, Zexin Lu, Yongqiang Huang, Yan Liu, Hu Chen,\n  Jiliu Zhou, and Yi Zhang", "title": "One Network to Solve Them All: A Sequential Multi-Task Joint Learning\n  Network Framework for MR Imaging Pipeline", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) acquisition, reconstruction, and\nsegmentation are usually processed independently in the conventional practice\nof MRI workflow. It is easy to notice that there are significant relevances\namong these tasks and this procedure artificially cuts off these potential\nconnections, which may lead to losing clinically important information for the\nfinal diagnosis. To involve these potential relations for further performance\nimprovement, a sequential multi-task joint learning network model is proposed\nto train a combined end-to-end pipeline in a differentiable way, aiming at\nexploring the mutual influence among those tasks simultaneously. Our design\nconsists of three cascaded modules: 1) deep sampling pattern learning module\noptimizes the $k$-space sampling pattern with predetermined sampling rate; 2)\ndeep reconstruction module is dedicated to reconstructing MR images from the\nundersampled data using the learned sampling pattern; 3) deep segmentation\nmodule encodes MR images reconstructed from the previous module to segment the\ninterested tissues. The proposed model retrieves the latently interactive and\ncyclic relations among those tasks, from which each task will be mutually\nbeneficial. The proposed framework is verified on MRB dataset, which achieves\nsuperior performance on other SOTA methods in terms of both reconstruction and\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 05:55:27 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Zhiwen", ""], ["Xia", "Wenjun", ""], ["Lu", "Zexin", ""], ["Huang", "Yongqiang", ""], ["Liu", "Yan", ""], ["Chen", "Hu", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "2105.06663", "submitter": "Yuan-Chen Guo", "authors": "Song-Hai Zhang, Yuan-Chen Guo, Qing-Wen Gu", "title": "Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the problem of generating 3D meshes from single free-hand\nsketches, aiming at fast 3D modeling for novice users. It can be regarded as a\nsingle-view reconstruction problem, but with unique challenges, brought by the\nvariation and conciseness of sketches. Ambiguities in poorly-drawn sketches\ncould make it hard to determine how the sketched object is posed. In this\npaper, we address the importance of viewpoint specification for overcoming such\nambiguities, and propose a novel view-aware generation approach. By explicitly\nconditioning the generation process on a given viewpoint, our method can\ngenerate plausible shapes automatically with predicted viewpoints, or with\nspecified viewpoints to help users better express their intentions. Extensive\nevaluations on various datasets demonstrate the effectiveness of our view-aware\ndesign in solving sketch ambiguities and improving reconstruction quality.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 06:27:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Song-Hai", ""], ["Guo", "Yuan-Chen", ""], ["Gu", "Qing-Wen", ""]]}, {"id": "2105.06668", "submitter": "Haoliang Sun", "authors": "Haoliang Sun, Xiankai Lu, Haochen Wang, Yilong Yin, Xiantong Zhen,\n  Cees G. M. Snoek, and Ling Shao", "title": "Attentional Prototype Inference for Few-Shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to address few-shot semantic segmentation. While existing\nprototype-based methods have achieved considerable success, they suffer from\nuncertainty and ambiguity caused by limited labelled examples. In this work, we\npropose attentional prototype inference (API), a probabilistic latent variable\nframework for few-shot semantic segmentation. We define a global latent\nvariable to represent the prototype of each object category, which we model as\na probabilistic distribution. The probabilistic modeling of the prototype\nenhances the model's generalization ability by handling the inherent\nuncertainty caused by limited data and intra-class variations of objects. To\nfurther enhance the model, we introduce a local latent variable to represent\nthe attention map of each query image, which enables the model to attend to\nforeground objects while suppressing background. The optimization of the\nproposed model is formulated as a variational Bayesian inference problem, which\nis established by amortized inference networks.We conduct extensive experiments\non three benchmarks, where our proposal obtains at least competitive and often\nbetter performance than state-of-the-art methods. We also provide comprehensive\nanalyses and ablation studies to gain insight into the effectiveness of our\nmethod for few-shot semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 06:58:44 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sun", "Haoliang", ""], ["Lu", "Xiankai", ""], ["Wang", "Haochen", ""], ["Yin", "Yilong", ""], ["Zhen", "Xiantong", ""], ["Snoek", "Cees G. M.", ""], ["Shao", "Ling", ""]]}, {"id": "2105.06677", "submitter": "Sebastian Palacio", "authors": "Sebastian Palacio, Adriano Lucieri, Mohsin Munir, J\\\"orn Hees, Sheraz\n  Ahmed, Andreas Dengel", "title": "XAI Handbook: Towards a Unified Framework for Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The field of explainable AI (XAI) has quickly become a thriving and prolific\ncommunity. However, a silent, recurrent and acknowledged issue in this area is\nthe lack of consensus regarding its terminology. In particular, each new\ncontribution seems to rely on its own (and often intuitive) version of terms\nlike \"explanation\" and \"interpretation\". Such disarray encumbers the\nconsolidation of advances in the field towards the fulfillment of scientific\nand regulatory demands e.g., when comparing methods or establishing their\ncompliance with respect to biases and fairness constraints. We propose a\ntheoretical framework that not only provides concrete definitions for these\nterms, but it also outlines all steps necessary to produce explanations and\ninterpretations. The framework also allows for existing contributions to be\nre-contextualized such that their scope can be measured, thus making them\ncomparable to other methods. We show that this framework is compliant with\ndesiderata on explanations, on interpretability and on evaluation metrics. We\npresent a use-case showing how the framework can be used to compare LIME, SHAP\nand MDNet, establishing their advantages and shortcomings. Finally, we discuss\nrelevant trends in XAI as well as recommendations for future work, all from the\nstandpoint of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:28:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Palacio", "Sebastian", ""], ["Lucieri", "Adriano", ""], ["Munir", "Mohsin", ""], ["Hees", "J\u00f6rn", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "2105.06711", "submitter": "Bruno Degardin", "authors": "Bruno Degardin, Vasco Lopes and Hugo Proen\\c{c}a", "title": "REGINA - Reasoning Graph Convolutional Networks in Human Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is known that the kinematics of the human body skeleton reveals valuable\ninformation in action recognition. Recently, modeling skeletons as\nspatio-temporal graphs with Graph Convolutional Networks (GCNs) has been\nreported to solidly advance the state-of-the-art performance. However,\nGCN-based approaches exclusively learn from raw skeleton data, and are expected\nto extract the inherent structural information on their own. This paper\ndescribes REGINA, introducing a novel way to REasoning Graph convolutional\nnetworks IN Human Action recognition. The rationale is to provide to the GCNs\nadditional knowledge about the skeleton data, obtained by handcrafted features,\nin order to facilitate the learning process, while guaranteeing that it remains\nfully trainable in an end-to-end manner. The challenge is to capture\ncomplementary information over the dynamics between consecutive frames, which\nis the key information extracted by state-of-the-art GCN techniques. Moreover,\nthe proposed strategy can be easily integrated in the existing GCN-based\nmethods, which we also regard positively. Our experiments were carried out in\nwell known action recognition datasets and enabled to conclude that REGINA\ncontributes for solid improvements in performance when incorporated to other\nGCN-based approaches, without any other adjustment regarding the original\nmethod. For reproducibility, the REGINA code and all the experiments carried\nout will be publicly available at https://github.com/DegardinBruno.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:46:42 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Degardin", "Bruno", ""], ["Lopes", "Vasco", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2105.06714", "submitter": "Peijia Chen", "authors": "Peijia Chen, Jianhuang Lai, Guangcong Wang, Huajun Zhou", "title": "Confidence-guided Adaptive Gate and Dual Differential Enhancement for\n  Video Salient Object Detection", "comments": "Accepted by ICME2021 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video salient object detection (VSOD) aims to locate and segment the most\nattractive object by exploiting both spatial cues and temporal cues hidden in\nvideo sequences. However, spatial and temporal cues are often unreliable in\nreal-world scenarios, such as low-contrast foreground, fast motion, and\nmultiple moving objects. To address these problems, we propose a new framework\nto adaptively capture available information from spatial and temporal cues,\nwhich contains Confidence-guided Adaptive Gate (CAG) modules and Dual\nDifferential Enhancement (DDE) modules. For both RGB features and optical flow\nfeatures, CAG estimates confidence scores supervised by the IoU between\npredictions and the ground truths to re-calibrate the information with a gate\nmechanism. DDE captures the differential feature representation to enrich the\nspatial and temporal information and generate the fused features. Experimental\nresults on four widely used datasets demonstrate the effectiveness of the\nproposed method against thirteen state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:49:37 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Chen", "Peijia", ""], ["Lai", "Jianhuang", ""], ["Wang", "Guangcong", ""], ["Zhou", "Huajun", ""]]}, {"id": "2105.06727", "submitter": "Gesina Schwalbe", "authors": "Gesina Schwalbe", "title": "Verification of Size Invariance in DNN Activations using Concept\n  Embeddings", "comments": "12 pages, 7 figures; Camera-ready version for AIAI2021", "journal-ref": null, "doi": "10.1007/978-3-030-79150-6_30", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The benefits of deep neural networks (DNNs) have become of interest for\nsafety critical applications like medical ones or automated driving. Here,\nhowever, quantitative insights into the DNN inner representations are\nmandatory. One approach to this is concept analysis, which aims to establish a\nmapping between the internal representation of a DNN and intuitive semantic\nconcepts. Such can be sub-objects like human body parts that are valuable for\nvalidation of pedestrian detection. To our knowledge, concept analysis has not\nyet been applied to large object detectors, specifically not for sub-parts.\nTherefore, this work first suggests a substantially improved version of the\nNet2Vec approach (arXiv:1801.03454) for post-hoc segmentation of sub-objects.\nIts practical applicability is then demonstrated on a new concept dataset by\ntwo exemplary assessments of three standard networks, including the larger Mask\nR-CNN model (arXiv:1703.06870): (1) the consistency of body part similarity,\nand (2) the invariance of internal representations of body parts with respect\nto the size in pixels of the depicted person. The findings show that the\nrepresentation of body parts is mostly size invariant, which may suggest an\nearly intelligent fusion of information in different size categories.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:25:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Schwalbe", "Gesina", ""]]}, {"id": "2105.06738", "submitter": "Johann Briffa", "authors": "Marc Tanti, Camille Berruyer, Paul Tafforeau, Adrian Muscat, Reuben\n  Farrugia, Kenneth Scerri, Gianluca Valentino, V. Armando Sol\\'e and Johann A.\n  Briffa", "title": "Automated segmentation of microtomography imaging of Egyptian mummies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propagation Phase Contrast Synchrotron Microtomography (PPC-SR${\\mu}$CT) is\nthe gold standard for non-invasive and non-destructive access to internal\nstructures of archaeological remains. In this analysis, the virtual specimen\nneeds to be segmented to separate different parts or materials, a process that\nnormally requires considerable human effort. In the Automated SEgmentation of\nMicrotomography Imaging (ASEMI) project, we developed a tool to automatically\nsegment these volumetric images, using manually segmented samples to tune and\ntrain a machine learning model. For a set of four specimens of ancient Egyptian\nanimal mummies we achieve an overall accuracy of 94-98% when compared with\nmanually segmented slices, approaching the results of off-the-shelf commercial\nsoftware using deep learning (97-99%) at much lower complexity. A qualitative\nanalysis of the segmented output shows that our results are close in term of\nusability to those from deep learning, justifying the use of these techniques.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:56:13 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Tanti", "Marc", ""], ["Berruyer", "Camille", ""], ["Tafforeau", "Paul", ""], ["Muscat", "Adrian", ""], ["Farrugia", "Reuben", ""], ["Scerri", "Kenneth", ""], ["Valentino", "Gianluca", ""], ["Sol\u00e9", "V. Armando", ""], ["Briffa", "Johann A.", ""]]}, {"id": "2105.06746", "submitter": "Christian Venner{\\o}d", "authors": "Adrian Kj{\\ae}rran and Christian Bakke Venner{\\o}d and Erling Stray\n  Bugge", "title": "Facial Age Estimation using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is a part of a student project in Machine Learning at the\nNorwegian University of Science and Technology. In this paper, a deep\nconvolutional neural network with five convolutional layers and three\nfully-connected layers is presented to estimate the ages of individuals based\non images. The model is in its entirety trained from scratch, where a\ncombination of three different datasets is used as training data. These\ndatasets are the APPA dataset, UTK dataset, and the IMDB dataset. The images\nwere preprocessed using a proprietary face-recognition software. Our model is\nevaluated on both a held-out test set, and on the Adience benchmark. On the\ntest set, our model achieves a categorical accuracy of 52%. On the Adience\nbenchmark, our model proves inferior compared with other leading models, with\nan exact accuray of 30%, and an one-off accuracy of 46%. Furthermore, a script\nwas created, allowing users to estimate their age directly using their web\ncamera. The script, alongside all other code, is located in our GitHub\nrepository: AgeNet.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:09:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Kj\u00e6rran", "Adrian", ""], ["Venner\u00f8d", "Christian Bakke", ""], ["Bugge", "Erling Stray", ""]]}, {"id": "2105.06747", "submitter": "Wang Zhihua", "authors": "Zhihua Wang and Haotao Wang and Tianlong Chen and Zhangyang Wang and\n  Kede Ma", "title": "Troubleshooting Blind Image Quality Models in the Wild", "comments": "7 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the group maximum differentiation competition (gMAD) has been used\nto improve blind image quality assessment (BIQA) models, with the help of\nfull-reference metrics. When applying this type of approach to troubleshoot\n\"best-performing\" BIQA models in the wild, we are faced with a practical\nchallenge: it is highly nontrivial to obtain stronger competing models for\nefficient failure-spotting. Inspired by recent findings that difficult samples\nof deep models may be exposed through network pruning, we construct a set of\n\"self-competitors,\" as random ensembles of pruned versions of the target model\nto be improved. Diverse failures can then be efficiently identified via\nself-gMAD competition. Next, we fine-tune both the target and its pruned\nvariants on the human-rated gMAD set. This allows all models to learn from\ntheir respective failures, preparing themselves for the next round of self-gMAD\ncompetition. Experimental results demonstrate that our method efficiently\ntroubleshoots BIQA models in the wild with improved generalizability.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:10:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Zhihua", ""], ["Wang", "Haotao", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""], ["Ma", "Kede", ""]]}, {"id": "2105.06754", "submitter": "Lorenzo Seidenari", "authors": "Fabio Zappardino and Tiberio Uricchio and Lorenzo Seidenari and\n  Alberto Del Bimbo", "title": "Learning Group Activities from Skeletons without Individual Action\n  Labels", "comments": "ICPR 2020", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9413195", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand human behavior we must not just recognize individual actions\nbut model possibly complex group activity and interactions. Hierarchical models\nobtain the best results in group activity recognition but require fine grained\nindividual action annotations at the actor level. In this paper we show that\nusing only skeletal data we can train a state-of-the art end-to-end system\nusing only group activity labels at the sequence level. Our experiments show\nthat models trained without individual action supervision perform poorly. On\nthe other hand we show that pseudo-labels can be computed from any pre-trained\nfeature extractor with comparable final performance. Finally our carefully\ndesigned lean pose only architecture shows highly competitive results versus\nmore complex multimodal approaches even in the self-supervised variant.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:31:32 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zappardino", "Fabio", ""], ["Uricchio", "Tiberio", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2105.06775", "submitter": "Shaoqi Yu", "authors": "Shaoqi Yu, Xiaorun Li, Shuhan Chen, Liaoying Zhao", "title": "Exploring the Intrinsic Probability Distribution for Hyperspectral\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural network-based anomaly detection methods have\nattracted considerable attention in the hyperspectral remote sensing domain due\nto the powerful reconstruction ability compared with traditional methods.\nHowever, actual probability distribution statistics hidden in the latent space\nare not discovered by exploiting the reconstruction error because the\nprobability distribution of anomalies is not explicitly modeled. To address the\nissue, we propose a novel probability distribution representation detector\n(PDRD) that explores the intrinsic distribution of both the background and the\nanomalies in original data for hyperspectral anomaly detection in this paper.\nFirst, we represent the hyperspectral data with multivariate Gaussian\ndistributions from a probabilistic perspective. Then, we combine the local\nstatistics with the obtained distributions to leverage the spatial information.\nFinally, the difference between the corresponding distributions of the test\npixel and the average expectation of the pixels in the Chebyshev neighborhood\nis measured by computing the modified Wasserstein distance to acquire the\ndetection map. We conduct the experiments on four real data sets to evaluate\nthe performance of our proposed method. Experimental results demonstrate the\naccuracy and efficiency of our proposed method compared to the state-of-the-art\ndetection methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 11:42:09 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yu", "Shaoqi", ""], ["Li", "Xiaorun", ""], ["Chen", "Shuhan", ""], ["Zhao", "Liaoying", ""]]}, {"id": "2105.06779", "submitter": "Jun Shi", "authors": "Jun Shi, Huite Yi, Xiaoyu Hao, Hong An, Wei Wei", "title": "Dual-Attention Residual Network for Automatic Diagnosis of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ongoing global pandemic of Coronavirus Disease 2019 (COVID-19) has posed\nserious threat to public health and the economy. Rapid and accurate diagnosis\nof COVID-19 is crucial to prevent the further spread of the disease and reduce\nits mortality. Chest computed tomography (CT) is an effective tool for the\nearly diagnosis of lung diseases including pneumonia. However, detecting\nCOVID-19 from CT is demanding and prone to human errors as some early-stage\npatients may have negative findings on images. In this study, we propose a\nnovel residual network to automatically identify COVID-19 from other common\npneumonia and normal people using CT images. Specifically, we employ the\nmodified 3D ResNet18 as the backbone network, which is equipped with both\nchannel-wise attention (CA) and depth-wise attention (DA) modules to further\nimprove the diagnostic performance. Experimental results on the large\nopen-source dataset show that our method can differentiate COVID-19 from the\nother two classes with 94.7% accuracy, 93.73% sensitivity, 98.28% specificity,\n95.26% F1-score, and an area under the receiver operating characteristic curve\n(AUC) of 0.99, outperforming baseline methods. These results demonstrate that\nthe proposed method could potentially assist the clinicians in performing a\nquick diagnosis to fight COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 11:59:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Shi", "Jun", ""], ["Yi", "Huite", ""], ["Hao", "Xiaoyu", ""], ["An", "Hong", ""], ["Wei", "Wei", ""]]}, {"id": "2105.06807", "submitter": "Ruoxi Chen", "authors": "Jinyin Chen, Ruoxi Chen, Haibin Zheng, Zhaoyan Ming, Wenrong Jiang and\n  Chen Cui", "title": "Salient Feature Extractor for Adversarial Defense on Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed unprecedented success achieved by deep learning\nmodels in the field of computer vision. However, their vulnerability towards\ncarefully crafted adversarial examples has also attracted the increasing\nattention of researchers. Motivated by the observation that adversarial\nexamples are due to the non-robust feature learned from the original dataset by\nmodels, we propose the concepts of salient feature(SF) and trivial feature(TF).\nThe former represents the class-related feature, while the latter is usually\nadopted to mislead the model. We extract these two features with coupled\ngenerative adversarial network model and put forward a novel detection and\ndefense method named salient feature extractor (SFE) to defend against\nadversarial attacks. Concretely, detection is realized by separating and\ncomparing the difference between SF and TF of the input. At the same time,\ncorrect labels are obtained by re-identifying SF to reach the purpose of\ndefense. Extensive experiments are carried out on MNIST, CIFAR-10, and ImageNet\ndatasets where SFE shows state-of-the-art results in effectiveness and\nefficiency compared with baselines. Furthermore, we provide an interpretable\nunderstanding of the defense and detection process.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 12:56:06 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Chen", "Jinyin", ""], ["Chen", "Ruoxi", ""], ["Zheng", "Haibin", ""], ["Ming", "Zhaoyan", ""], ["Jiang", "Wenrong", ""], ["Cui", "Chen", ""]]}, {"id": "2105.06808", "submitter": "Sylwia Majchrowska Ms.", "authors": "Sylwia Majchrowska, Agnieszka Miko{\\l}ajczyk, Maria Ferlin, Zuzanna\n  Klawikowska, Marta A. Plantykow, Arkadiusz Kwasigroch, Karol Majek", "title": "Waste detection in Pomerania: non-profit project for detecting waste in\n  environment", "comments": "Litter detection, Waste detection, Object detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Waste pollution is one of the most significant environmental issues in the\nmodern world. The importance of recycling is well known, either for economic or\necological reasons, and the industry demands high efficiency. Our team\nconducted comprehensive research on Artificial Intelligence usage in waste\ndetection and classification to fight the world's waste pollution problem. As a\nresult an open-source framework that enables the detection and classification\nof litter was developed. The final pipeline consists of two neural networks:\none that detects litter and a second responsible for litter classification.\nWaste is classified into seven categories: bio, glass, metal and plastic,\nnon-recyclable, other, paper and unknown. Our approach achieves up to 70% of\naverage precision in waste detection and around 75% of classification accuracy\non the test dataset. The code used in the studies is publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:33:22 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Majchrowska", "Sylwia", ""], ["Miko\u0142ajczyk", "Agnieszka", ""], ["Ferlin", "Maria", ""], ["Klawikowska", "Zuzanna", ""], ["Plantykow", "Marta A.", ""], ["Kwasigroch", "Arkadiusz", ""], ["Majek", "Karol", ""]]}, {"id": "2105.06818", "submitter": "Tianrui Hui", "authors": "Tianrui Hui, Shaofei Huang, Si Liu, Zihan Ding, Guanbin Li, Wenguan\n  Wang, Jizhong Han, Fei Wang", "title": "Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor\n  Segmentation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-queried video actor segmentation aims to predict the pixel-level\nmask of the actor which performs the actions described by a natural language\nquery in the target frames. Existing methods adopt 3D CNNs over the video clip\nas a general encoder to extract a mixed spatio-temporal feature for the target\nframe. Though 3D convolutions are amenable to recognizing which actor is\nperforming the queried actions, it also inevitably introduces misaligned\nspatial information from adjacent frames, which confuses features of the target\nframe and yields inaccurate segmentation. Therefore, we propose a collaborative\nspatial-temporal encoder-decoder framework which contains a 3D temporal encoder\nover the video clip to recognize the queried actions, and a 2D spatial encoder\nover the target frame to accurately segment the queried actors. In the decoder,\na Language-Guided Feature Selection (LGFS) module is proposed to flexibly\nintegrate spatial and temporal features from the two encoders. We also propose\na Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine\nspatial- and temporal-relevant linguistic features for multimodal feature\ninteraction in each stage of the two encoders. Our method achieves new\nstate-of-the-art performance on two popular benchmarks with less computational\noverhead than previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:27:53 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Hui", "Tianrui", ""], ["Huang", "Shaofei", ""], ["Liu", "Si", ""], ["Ding", "Zihan", ""], ["Li", "Guanbin", ""], ["Wang", "Wenguan", ""], ["Han", "Jizhong", ""], ["Wang", "Fei", ""]]}, {"id": "2105.06820", "submitter": "Charalambos Poullis", "authors": "Farhan Rahman Wasee, Alen Joy, Charalambos Poullis", "title": "Predicting Surface Reflectance Properties of Outdoor Scenes Under\n  Unknown Natural Illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Estimating and modelling the appearance of an object under outdoor\nillumination conditions is a complex process. Although there have been several\nstudies on illumination estimation and relighting, very few of them focus on\nestimating the reflectance properties of outdoor objects and scenes. This paper\naddresses this problem and proposes a complete framework to predict surface\nreflectance properties of outdoor scenes under unknown natural illumination.\nUniquely, we recast the problem into its two constituent components involving\nthe BRDF incoming light and outgoing view directions: (i) surface points'\nradiance captured in the images, and outgoing view directions are aggregated\nand encoded into reflectance maps, and (ii) a neural network trained on\nreflectance maps of renders of a unit sphere under arbitrary light directions\ninfers a low-parameter reflection model representing the reflectance properties\nat each surface in the scene. Our model is based on a combination of\nphenomenological and physics-based scattering models and can relight the scenes\nfrom novel viewpoints. We present experiments that show that rendering with the\npredicted reflectance properties results in a visually similar appearance to\nusing textures that cannot otherwise be disentangled from the reflectance\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:31:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wasee", "Farhan Rahman", ""], ["Joy", "Alen", ""], ["Poullis", "Charalambos", ""]]}, {"id": "2105.06822", "submitter": "Hao Du", "authors": "Hao Du, Melissa Min-Szu Yao, Liangyu Chen, Wing P. Chan, and Mengling\n  Feng", "title": "Multi-task Graph Convolutional Neural Network for Calcification\n  Morphology and Distribution Analysis in Mammograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphology and distribution of microcalcifications in a cluster are the\nmost important characteristics for radiologists to diagnose breast cancer.\nHowever, it is time-consuming and difficult for radiologists to identify these\ncharacteristics, and there also lacks of effective solutions for automatic\ncharacterization. In this study, we proposed a multi-task deep graph\nconvolutional network (GCN) method for the automatic characterization of\nmorphology and distribution of microcalcifications in mammograms. Our proposed\nmethod transforms morphology and distribution characterization into node and\ngraph classification problem and learns the representations concurrently.\nThrough extensive experiments, we demonstrate significant improvements with the\nproposed multi-task GCN comparing to the baselines. Moreover, the achieved\nimprovements can be related to and enhance clinical understandings. We explore,\nfor the first time, the application of GCNs in microcalcification\ncharacterization that suggests the potential of graph learning for more robust\nunderstanding of medical images.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:32:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Du", "Hao", ""], ["Yao", "Melissa Min-Szu", ""], ["Chen", "Liangyu", ""], ["Chan", "Wing P.", ""], ["Feng", "Mengling", ""]]}, {"id": "2105.06825", "submitter": "Pablo Gil Dr.", "authors": "Victor De Gea and Santiago T. Puente and Pablo Gil", "title": "Domestic waste detection and grasping points for robotic picking up", "comments": "2 pages, 3 figures, accepted as poster for presentation in ICRA 2021\n  Workshop: Emerging paradigms for robotic manipulation: from the lab to the\n  productive world", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an AI system applied to location and robotic grasping.\nExperimental setup is based on a parameter study to train a deep-learning\nnetwork based on Mask-RCNN to perform waste location in indoor and outdoor\nenvironment, using five different classes and generating a new waste dataset.\nInitially the AI system obtain the RGBD data of the environment, followed by\nthe detection of objects using the neural network. Later, the 3D object shape\nis computed using the network result and the depth channel. Finally, the shape\nis used to compute grasping for a robot arm with a two-finger gripper. The\nobjective is to classify the waste in groups to improve a recycling strategy.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:37:33 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["De Gea", "Victor", ""], ["Puente", "Santiago T.", ""], ["Gil", "Pablo", ""]]}, {"id": "2105.06830", "submitter": "Minshan Xie", "authors": "Minshan Xie, Menghan Xia, Tien-Tsin Wong", "title": "Exploiting Aliasing for Manga Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a popular entertainment art form, manga enriches the line drawings details\nwith bitonal screentones. However, manga resources over the Internet usually\nshow screentone artifacts because of inappropriate scanning/rescaling\nresolution. In this paper, we propose an innovative two-stage method to restore\nquality bitonal manga from degraded ones. Our key observation is that the\naliasing induced by downsampling bitonal screentones can be utilized as\ninformative clues to infer the original resolution and screentones. First, we\npredict the target resolution from the degraded manga via the Scale Estimation\nNetwork (SE-Net) with spatial voting scheme. Then, at the target resolution, we\nrestore the region-wise bitonal screentones via the Manga Restoration Network\n(MR-Net) discriminatively, depending on the degradation degree. Specifically,\nthe original screentones are directly restored in pattern-identifiable regions,\nand visually plausible screentones are synthesized in pattern-agnostic regions.\nQuantitative evaluation on synthetic data and visual assessment on real-world\ncases illustrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:47:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Xie", "Minshan", ""], ["Xia", "Menghan", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "2105.06858", "submitter": "Andrea Raffo", "authors": "Chiara Romanengo, Andrea Raffo, Yifan Qie, Nabil Anwer, Bianca\n  Falcidieno", "title": "Fit4CAD: A point cloud benchmark for fitting simple geometric primitives\n  in CAD models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Fit4CAD, a benchmark for the evaluation and comparison of methods\nfor fitting simple geometric primitives in point clouds representing CAD\nmodels. This benchmark is meant to help both method developers and those who\nwant to identify the best performing tools. The Fit4CAD dataset is composed by\n225 high quality point clouds, each of which has been obtained by sampling a\nCAD model. The way these elements were created by using existing platforms and\ndatasets makes the benchmark easily expandable. The dataset is already split\ninto a training set and a test set. To assess performance and accuracy of the\ndifferent primitive fitting methods, various measures are defined. To\ndemonstrate the effective use of Fit4CAD, we have tested it on two methods\nbelonging to two different categories of approaches to the primitive fitting\nproblem: a clustering method based on a primitive growing framework and a\nparametric method based on the Hough transform.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:32:08 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 11:55:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Romanengo", "Chiara", ""], ["Raffo", "Andrea", ""], ["Qie", "Yifan", ""], ["Anwer", "Nabil", ""], ["Falcidieno", "Bianca", ""]]}, {"id": "2105.06861", "submitter": "Felix Gonda", "authors": "Felix Gonda, Xueying Wang, Johanna Beyer, Markus Hadwiger, Jeff W.\n  Lichtman, and Hanspeter Pfister", "title": "VICE: Visual Identification and Correction of Neural Circuit Errors", "comments": "This paper has been accepted for publication and presentation at the\n  23rd EG Conference on Visualization (EuroVis 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A connectivity graph of neurons at the resolution of single synapses provides\nscientists with a tool for understanding the nervous system in health and\ndisease. Recent advances in automatic image segmentation and synapse prediction\nin electron microscopy (EM) datasets of the brain have made reconstructions of\nneurons possible at the nanometer scale. However, automatic segmentation\nsometimes struggles to segment large neurons correctly, requiring human effort\nto proofread its output. General proofreading involves inspecting large volumes\nto correct segmentation errors at the pixel level, a visually intensive and\ntime-consuming process. This paper presents the design and implementation of an\nanalytics framework that streamlines proofreading, focusing on\nconnectivity-related errors. We accomplish this with automated likely-error\ndetection and synapse clustering that drives the proofreading effort with\nhighly interactive 3D visualizations. In particular, our strategy centers on\nproofreading the local circuit of a single cell to ensure a basic level of\ncompleteness. We demonstrate our framework's utility with a user study and\nreport quantitative and subjective feedback from our users. Overall, users find\nthe framework more efficient for proofreading, understanding evolving graphs,\nand sharing error correction strategies.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:34:58 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gonda", "Felix", ""], ["Wang", "Xueying", ""], ["Beyer", "Johanna", ""], ["Hadwiger", "Markus", ""], ["Lichtman", "Jeff W.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2105.06878", "submitter": "Zhengxiong Luo", "authors": "Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang and Tieniu Tan", "title": "End-to-end Alternating Optimization for Blind Super Resolution", "comments": "Submited to PAMI. arXiv admin note: substantial text overlap with\n  arXiv:2010.02631", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous methods decompose the blind super-resolution (SR) problem into two\nsequential steps: \\textit{i}) estimating the blur kernel from given\nlow-resolution (LR) image and \\textit{ii}) restoring the SR image based on the\nestimated kernel. This two-step solution involves two independently trained\nmodels, which may not be well compatible with each other. A small estimation\nerror of the first step could cause a severe performance drop of the second\none. While on the other hand, the first step can only utilize limited\ninformation from the LR image, which makes it difficult to predict a highly\naccurate blur kernel. Towards these issues, instead of considering these two\nsteps separately, we adopt an alternating optimization algorithm, which can\nestimate the blur kernel and restore the SR image in a single model.\nSpecifically, we design two convolutional neural modules, namely\n\\textit{Restorer} and \\textit{Estimator}. \\textit{Restorer} restores the SR\nimage based on the predicted kernel, and \\textit{Estimator} estimates the blur\nkernel with the help of the restored SR image. We alternate these two modules\nrepeatedly and unfold this process to form an end-to-end trainable network. In\nthis way, \\textit{Estimator} utilizes information from both LR and SR images,\nwhich makes the estimation of the blur kernel easier. More importantly,\n\\textit{Restorer} is trained with the kernel estimated by \\textit{Estimator},\ninstead of the ground-truth kernel, thus \\textit{Restorer} could be more\ntolerant to the estimation error of \\textit{Estimator}. Extensive experiments\non synthetic datasets and real-world images show that our model can largely\noutperform state-of-the-art methods and produce more visually favorable results\nat a much higher speed. The source code is available at\n\\url{https://github.com/greatlog/DAN.git}.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:05:30 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Luo", "Zhengxiong", ""], ["Huang", "Yan", ""], ["Li", "Shang", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "2105.06887", "submitter": "Qing Ma", "authors": "Qing Ma, Jae Chul Koh, WonSook Lee", "title": "A Frequency Domain Constraint for Synthetic X-ray Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic X-ray images can be helpful for image guiding systems and VR\nsimulations. However, it is difficult to produce high-quality arbitrary view\nsynthetic X-ray images in real-time due to limited CT scanning resolution, high\ncomputation resource demand or algorithm complexity. Our goal is to generate\nhigh-resolution synthetic X-ray images in real-time by upsampling\nlow-resolution im-ages. Reference-based Super Resolution (RefSR) has been well\nstudied in recent years and has been proven to be more powerful than\ntraditional Single Image Su-per-Resolution (SISR). RefSR can produce fine\ndetails by utilizing the reference image but it still inevitably generates some\nartifacts and noise. In this paper, we propose texture transformer\nsuper-resolution with frequency domain (TTSR-FD). We introduce frequency domain\nloss as a constraint to further improve the quality of the RefSR results with\nfine details and without obvious artifacts. This makes a real-time synthetic\nX-ray image-guided procedure VR simulation system possible. To the best of our\nknowledge, this is the first paper utilizing the frequency domain as part of\nthe loss functions in the field of super-resolution. We evaluated TTSR-FD on\nour synthetic X-ray image dataset and achieved state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:17:27 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ma", "Qing", ""], ["Koh", "Jae Chul", ""], ["Lee", "WonSook", ""]]}, {"id": "2105.06967", "submitter": "Gabriel Salomon", "authors": "Gabriel Salomon, Alceu Britto, Rafael H. Vareto, William R. Schwartz,\n  David Menotti", "title": "Open-set Face Recognition for Small Galleries Using Siamese Networks", "comments": null, "journal-ref": "2020 International Conference on Systems, Signals and Image\n  Processing (IWSSIP), 2020, pp. 161-166", "doi": "10.1109/IWSSIP48289.2020.9145245", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition has been one of the most relevant and explored fields of\nBiometrics. In real-world applications, face recognition methods usually must\ndeal with scenarios where not all probe individuals were seen during the\ntraining phase (open-set scenarios). Therefore, open-set face recognition is a\nsubject of increasing interest as it deals with identifying individuals in a\nspace where not all faces are known in advance. This is useful in several\napplications, such as access authentication, on which only a few individuals\nthat have been previously enrolled in a gallery are allowed. The present work\nintroduces a novel approach towards open-set face recognition focusing on small\ngalleries and in enrollment detection, not identity retrieval. A Siamese\nNetwork architecture is proposed to learn a model to detect if a face probe is\nenrolled in the gallery based on a verification-like approach. Promising\nresults were achieved for small galleries on experiments carried out on\nPubfig83, FRGCv1 and LFW datasets. State-of-the-art methods like HFCN and HPLS\nwere outperformed on FRGCv1. Besides, a new evaluation protocol is introduced\nfor experiments in small galleries on LFW.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:16:37 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Salomon", "Gabriel", ""], ["Britto", "Alceu", ""], ["Vareto", "Rafael H.", ""], ["Schwartz", "William R.", ""], ["Menotti", "David", ""]]}, {"id": "2105.06986", "submitter": "Fernando Navarro", "authors": "Fernando Navarro, Christopher Watanabe, Suprosanna Shit, Anjany\n  Sekuboyina, Jan C. Peeken, Stephanie E. Combs and Bjoern H. Menze", "title": "Evaluating the Robustness of Self-Supervised Learning in Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervision has demonstrated to be an effective learning strategy when\ntraining target tasks on small annotated data-sets. While current research\nfocuses on creating novel pretext tasks to learn meaningful and reusable\nrepresentations for the target task, these efforts obtain marginal performance\ngains compared to fully-supervised learning. Meanwhile, little attention has\nbeen given to study the robustness of networks trained in a self-supervised\nmanner. In this work, we demonstrate that networks trained via self-supervised\nlearning have superior robustness and generalizability compared to\nfully-supervised learning in the context of medical imaging. Our experiments on\npneumonia detection in X-rays and multi-organ segmentation in CT yield\nconsistent results exposing the hidden benefits of self-supervision for\nlearning robust feature representations.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:49:52 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Navarro", "Fernando", ""], ["Watanabe", "Christopher", ""], ["Shit", "Suprosanna", ""], ["Sekuboyina", "Anjany", ""], ["Peeken", "Jan C.", ""], ["Combs", "Stephanie E.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2105.06988", "submitter": "Nathan Frey", "authors": "Nathan Frey, Peggy Chi, Weilong Yang, Irfan Essa", "title": "Automatic Non-Linear Video Editing Transfer", "comments": "Published to AI for Content Creation Workshop at CVPR 2021", "journal-ref": "AI for Content Creation Workshop at CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic approach that extracts editing styles in a source\nvideo and applies the edits to matched footage for video creation. Our Computer\nVision based techniques considers framing, content type, playback speed, and\nlighting of each input video segment. By applying a combination of these\nfeatures, we demonstrate an effective method that automatically transfers the\nvisual and temporal styles from professionally edited videos to unseen raw\nfootage. We evaluated our approach with real-world videos that contained a\ntotal of 3872 video shots of a variety of editing styles, including different\nsubjects, camera motions, and lighting. We reported feedback from survey\nparticipants who reviewed a set of our results.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:52:25 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Frey", "Nathan", ""], ["Chi", "Peggy", ""], ["Yang", "Weilong", ""], ["Essa", "Irfan", ""]]}, {"id": "2105.06993", "submitter": "Erika Lu", "authors": "Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T.\n  Freeman, Michael Rubinstein", "title": "Omnimatte: Associating Objects and Their Effects in Video", "comments": "Accepted to CVPR 2021 Oral. Project webpage:\n  https://omnimatte.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer vision is increasingly effective at segmenting objects in images and\nvideos; however, scene effects related to the objects---shadows, reflections,\ngenerated smoke, etc---are typically overlooked. Identifying such scene effects\nand associating them with the objects producing them is important for improving\nour fundamental understanding of visual scenes, and can also assist a variety\nof applications such as removing, duplicating, or enhancing objects in video.\nIn this work, we take a step towards solving this novel problem of\nautomatically associating objects with their effects in video. Given an\nordinary video and a rough segmentation mask over time of one or more subjects\nof interest, we estimate an omnimatte for each subject---an alpha matte and\ncolor image that includes the subject along with all its related time-varying\nscene elements. Our model is trained only on the input video in a\nself-supervised manner, without any manual labels, and is generic---it produces\nomnimattes automatically for arbitrary objects and a variety of effects. We\nshow results on real-world videos containing interactions between different\ntypes of subjects (cars, animals, people) and complex effects, ranging from\nsemi-transparent elements such as smoke and reflections, to fully opaque\neffects such as objects attached to the subject.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:57:08 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lu", "Erika", ""], ["Cole", "Forrester", ""], ["Dekel", "Tali", ""], ["Zisserman", "Andrew", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""]]}, {"id": "2105.07014", "submitter": "Rico Jonschkowski", "authors": "Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, Rico\n  Jonschkowski", "title": "SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image\n  Warping", "comments": "Accepted at CVPR 2021, all code available at\n  https://github.com/google-research/google-research/tree/master/smurf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present SMURF, a method for unsupervised learning of optical flow that\nimproves state of the art on all benchmarks by $36\\%$ to $40\\%$ (over the prior\nbest method UFlow) and even outperforms several supervised approaches such as\nPWC-Net and FlowNet2. Our method integrates architecture improvements from\nsupervised optical flow, i.e. the RAFT model, with new ideas for unsupervised\nlearning that include a sequence-aware self-supervision loss, a technique for\nhandling out-of-frame motion, and an approach for learning effectively from\nmulti-frame video data while still only requiring two frames for inference.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:02:50 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Stone", "Austin", ""], ["Maurer", "Daniel", ""], ["Ayvaci", "Alper", ""], ["Angelova", "Anelia", ""], ["Jonschkowski", "Rico", ""]]}, {"id": "2105.07029", "submitter": "Eleni Triantafillou", "authors": "Eleni Triantafillou, Hugo Larochelle, Richard Zemel and Vincent\n  Dumoulin", "title": "Learning a Universal Template for Few-shot Dataset Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot dataset generalization is a challenging variant of the well-studied\nfew-shot classification problem where a diverse training set of several\ndatasets is given, for the purpose of training an adaptable model that can then\nlearn classes from new datasets using only a few examples. To this end, we\npropose to utilize the diverse training set to construct a universal template:\na partial model that can define a wide array of dataset-specialized models, by\nplugging in appropriate components. For each new few-shot classification\nproblem, our approach therefore only requires inferring a small number of\nparameters to insert into the universal template. We design a separate network\nthat produces an initialization of those parameters for each given task, and we\nthen fine-tune its proposed initialization via a few steps of gradient descent.\nOur approach is more parameter-efficient, scalable and adaptable compared to\nprevious methods, and achieves the state-of-the-art on the challenging\nMeta-Dataset benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:46:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 15:31:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Triantafillou", "Eleni", ""], ["Larochelle", "Hugo", ""], ["Zemel", "Richard", ""], ["Dumoulin", "Vincent", ""]]}, {"id": "2105.07044", "submitter": "Hajar Emami Gohari", "authors": "Hajar Emami, Ming Dong, Siamak Nejad-Davarani, and Carri Glide-Hurst", "title": "SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical image synthesis, model training could be challenging due to the\ninconsistencies between images of different modalities even with the same\npatient, typically caused by internal status/tissue changes as different\nmodalities are usually obtained at a different time. This paper proposes a\nnovel deep learning method, Structure-aware Generative Adversarial Network\n(SA-GAN), that preserves the shapes and locations of in-consistent structures\nwhen generating medical images. SA-GAN is employed to generate synthetic\ncomputed tomography (synCT) images from magnetic resonance imaging (MRI) with\ntwo parallel streams: the global stream translates the input from the MRI to\nthe CT domain while the local stream automatically segments the inconsistent\norgans, maintains their locations and shapes in MRI, and translates the organ\nintensities to CT. Through extensive experiments on a pelvic dataset, we\ndemonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs\nand organ segmentation and supports MR-only treatment planning in disease sites\nwith internal organ status changes.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 19:34:23 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 19:36:46 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Emami", "Hajar", ""], ["Dong", "Ming", ""], ["Nejad-Davarani", "Siamak", ""], ["Glide-Hurst", "Carri", ""]]}, {"id": "2105.07054", "submitter": "Matheus Diniz", "authors": "Matheus Alves Diniz and William Robson Schwartz", "title": "Face Attributes as Cues for Deep Face Recognition Understanding", "comments": "7 pages, 5 figures, published at automatic face and gesture\n  recognition 2020", "journal-ref": null, "doi": "10.1109/FG47880.2020.00088", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deeply learned representations are the state-of-the-art descriptors for face\nrecognition methods. These representations encode latent features that are\ndifficult to explain, compromising the confidence and interpretability of their\npredictions. Most attempts to explain deep features are visualization\ntechniques that are often open to interpretation. Instead of relying only on\nvisualizations, we use the outputs of hidden layers to predict face attributes.\nThe obtained performance is an indicator of how well the attribute is\nimplicitly learned in that layer of the network. Using a variable selection\ntechnique, we also analyze how these semantic concepts are distributed inside\neach layer, establishing the precise location of relevant neurons for each\nattribute. According to our experiments, gender, eyeglasses and hat usage can\nbe predicted with over 96% accuracy even when only a single neural output is\nused to predict each attribute. These performances are less than 3 percentage\npoints lower than the ones achieved by deep supervised face attribute networks.\nIn summary, our experiments show that, inside DCNNs optimized for face\nidentification, there exists latent neurons encoding face attributes almost as\naccurately as DCNNs optimized for these attributes.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 19:54:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Diniz", "Matheus Alves", ""], ["Schwartz", "William Robson", ""]]}, {"id": "2105.07059", "submitter": "Chenyu You", "authors": "Chenyu You, Ruihan Zhao, Lawrence Staib, James S. Duncan", "title": "Momentum Contrastive Voxel-wise Representation Learning for\n  Semi-supervised Volumetric Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, manually annotating\nmedical data is often laborious, and most existing learning-based approaches\nfail to accurately delineate object boundaries without effective geometric\nconstraints. Contrastive learning, a sub-area of self-supervised learning, has\nrecently been noted as a promising direction in multiple application fields. In\nthis work, we present a novel Contrastive Voxel-wise Representation Learning\n(CVRL) method with geometric constraints to learn global-local visual\nrepresentations for volumetric medical image segmentation with limited\nannotations. Our framework can effectively learn global and local features by\ncapturing 3D spatial context and rich anatomical information. Specifically, we\nintroduce a voxel-to-volume contrastive algorithm to learn global information\nfrom 3D images, and propose to perform local voxel-to-voxel contrast to\nexplicitly make use of local cues in the embedding space. Moreover, we\nintegrate an elastic interaction-based active contour model as a geometric\nregularization term to enable fast and reliable object delineations in an\nend-to-end learning manner. Results on the Atrial Segmentation Challenge\ndataset demonstrate superiority of our proposed scheme, especially in a setting\nwith a very limited number of annotated data.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:27:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["You", "Chenyu", ""], ["Zhao", "Ruihan", ""], ["Staib", "Lawrence", ""], ["Duncan", "James S.", ""]]}, {"id": "2105.07078", "submitter": "Siyue Wang", "authors": "Siyue Wang, Xiao Wang, Pin-Yu Chen, Pu Zhao and Xue Lin", "title": "High-Robustness, Low-Transferability Fingerprinting of Neural Networks", "comments": "ICLR 2021 Workshop on Security and Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes Characteristic Examples for effectively fingerprinting\ndeep neural networks, featuring high-robustness to the base model against model\npruning as well as low-transferability to unassociated models. This is the\nfirst work taking both robustness and transferability into consideration for\ngenerating realistic fingerprints, whereas current methods lack practical\nassumptions and may incur large false positive rates. To achieve better\ntrade-off between robustness and transferability, we propose three kinds of\ncharacteristic examples: vanilla C-examples, RC-examples, and LTRC-example, to\nderive fingerprints from the original base model. To fairly characterize the\ntrade-off between robustness and transferability, we propose Uniqueness Score,\na comprehensive metric that measures the difference between robustness and\ntransferability, which also serves as an indicator to the false alarm problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 21:48:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Siyue", ""], ["Wang", "Xiao", ""], ["Chen", "Pin-Yu", ""], ["Zhao", "Pu", ""], ["Lin", "Xue", ""]]}, {"id": "2105.07085", "submitter": "Taojiannan Yang", "authors": "Taojiannan Yang, Sijie Zhu, Matias Mendieta, Pu Wang, Ravikumar\n  Balakrishnan, Minwoo Lee, Tao Han, Mubarak Shah, Chen Chen", "title": "MutualNet: Adaptive ConvNet via Mutual Learning from Different Model\n  Configurations", "comments": "Extended version of arXiv:1909.12978. More experiments on 3D networks\n  (SlowFast, X3D) and analyses on training cost", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing deep neural networks are static, which means they can only do\ninference at a fixed complexity. But the resource budget can vary substantially\nacross different devices. Even on a single device, the affordable budget can\nchange with different scenarios, and repeatedly training networks for each\nrequired budget would be incredibly expensive. Therefore, in this work, we\npropose a general method called MutualNet to train a single network that can\nrun at a diverse set of resource constraints. Our method trains a cohort of\nmodel configurations with various network widths and input resolutions. This\nmutual learning scheme not only allows the model to run at different\nwidth-resolution configurations but also transfers the unique knowledge among\nthese configurations, helping the model to learn stronger representations\noverall. MutualNet is a general training methodology that can be applied to\nvarious network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks:\nSlowFast, X3D) and various tasks (e.g., image classification, object detection,\nsegmentation, and action recognition), and is demonstrated to achieve\nconsistent improvements on a variety of datasets. Since we only train the model\nonce, it also greatly reduces the training cost compared to independently\ntraining several models. Surprisingly, MutualNet can also be used to\nsignificantly boost the performance of a single network, if dynamic resource\nconstraint is not a concern. In summary, MutualNet is a unified method for both\nstatic and adaptive, 2D and 3D networks. Codes and pre-trained models are\navailable at \\url{https://github.com/taoyang1122/MutualNet}.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 22:30:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Mendieta", "Matias", ""], ["Wang", "Pu", ""], ["Balakrishnan", "Ravikumar", ""], ["Lee", "Minwoo", ""], ["Han", "Tao", ""], ["Shah", "Mubarak", ""], ["Chen", "Chen", ""]]}, {"id": "2105.07112", "submitter": "Celong Liu", "authors": "Celong Liu, Zhong Li, Junsong Yuan, Yi Xu", "title": "NeLF: Practical Novel View Synthesis with Neural Light Field", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a 4D parameterization of the light field, where each ray is\ncharacterized by a 4D parameter. We then formulate the light field as a 4D\nfunction that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Our method achieves state-of-the-art novel view synthesis results while\nmaintaining an interactive frame rate.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:20:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 11:39:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Celong", ""], ["Li", "Zhong", ""], ["Yuan", "Junsong", ""], ["Xu", "Yi", ""]]}, {"id": "2105.07113", "submitter": "Christian Mejia-Escobar", "authors": "Christian Mejia-Escobar, Miguel Cazorla, Ester Martinez-Martin", "title": "A Large Visual, Qualitative and Quantitative Dataset of Web Pages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The World Wide Web is not only one of the most important platforms of\ncommunication and information at present, but also an area of growing interest\nfor scientific research. This motivates a lot of work and projects that require\nlarge amounts of data. However, there is no dataset that integrates the\nparameters and visual appearance of Web pages, because its collection is a\ncostly task in terms of time and effort. With the support of various computer\ntools and programming scripts, we have created a large dataset of 49,438 Web\npages. It consists of visual, textual and numerical data types, includes all\ncountries worldwide, and considers a broad range of topics such as art,\nentertainment, economy, business, education, government, news, media, science,\nand environment, covering different cultural characteristics and varied design\npreferences. In this paper, we describe the process of collecting, debugging\nand publishing the final product, which is freely available. To demonstrate the\nusefulness of our dataset, we expose a binary classification model for\ndetecting error Web pages, and a multi-class Web subject-based categorization,\nboth problems using convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:31:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mejia-Escobar", "Christian", ""], ["Cazorla", "Miguel", ""], ["Martinez-Martin", "Ester", ""]]}, {"id": "2105.07116", "submitter": "Mohammadreza Mohseni", "authors": "Mohammadreza Mohseni, Jordan Yap, William Yolland, Arash Koochek and M\n  Stella Atkins", "title": "Can self-training identify suspicious ugly duckling lesions?", "comments": "Accepted at Sixth ISIC Skin Image Analysis Workshop @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One commonly used clinical approach towards detecting melanomas recognises\nthe existence of Ugly Duckling nevi, or skin lesions which look different from\nthe other lesions on the same patient. An automatic method of detecting and\nanalysing these lesions would help to standardize studies, compared with manual\nscreening methods. However, it is difficult to obtain expertly-labelled images\nfor ugly duckling lesions. We therefore propose to use self-supervised machine\nlearning to automatically detect outlier lesions. We first automatically detect\nand extract all the lesions from a wide-field skin image, and calculate an\nembedding for each detected lesion in a patient image, based on automatically\nidentified features. These embeddings are then used to calculate the L2\ndistances as a way to measure dissimilarity. Using this deep learning method,\nUgly Ducklings are identified as outliers which should deserve more attention\nfrom the examining physician. We evaluate through comparison with\ndermatologists, and achieve a sensitivity rate of 72.1% and diagnostic accuracy\nof 94.2% on the held-out test set.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 02:01:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mohseni", "Mohammadreza", ""], ["Yap", "Jordan", ""], ["Yolland", "William", ""], ["Koochek", "Arash", ""], ["Atkins", "M Stella", ""]]}, {"id": "2105.07128", "submitter": "Xin Liu Dr.", "authors": "Xin Liu, Xingzhi Wang and Yiu-ming Cheung", "title": "FDDH: Fast Discriminative Discrete Hashing for Large-Scale Cross-Modal\n  Retrieval", "comments": "16 pages, 7 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2021", "doi": "10.1109/TNNLS.2021.3076684", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cross-modal hashing, favored for its effectiveness and efficiency, has\nreceived wide attention to facilitating efficient retrieval across different\nmodalities. Nevertheless, most existing methods do not sufficiently exploit the\ndiscriminative power of semantic information when learning the hash codes,\nwhile often involving time-consuming training procedure for handling the\nlarge-scale dataset. To tackle these issues, we formulate the learning of\nsimilarity-preserving hash codes in terms of orthogonally rotating the semantic\ndata so as to minimize the quantization loss of mapping such data to hamming\nspace, and propose an efficient Fast Discriminative Discrete Hashing (FDDH)\napproach for large-scale cross-modal retrieval. More specifically, FDDH\nintroduces an orthogonal basis to regress the targeted hash codes of training\nexamples to their corresponding semantic labels, and utilizes \"-dragging\ntechnique to provide provable large semantic margins. Accordingly, the\ndiscriminative power of semantic information can be explicitly captured and\nmaximized. Moreover, an orthogonal transformation scheme is further proposed to\nmap the nonlinear embedding data into the semantic subspace, which can well\nguarantee the semantic consistency between the data feature and its semantic\nrepresentation. Consequently, an efficient closed form solution is derived for\ndiscriminative hash code learning, which is very computationally efficient. In\naddition, an effective and stable online learning strategy is presented for\noptimizing modality-specific projection functions, featuring adaptivity to\ndifferent training sizes and streaming data. The proposed FDDH approach\ntheoretically approximates the bi-Lipschitz continuity, runs sufficiently fast,\nand also significantly improves the retrieval performance over the\nstate-of-the-art methods. The source code is released at:\nhttps://github.com/starxliu/FDDH.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 03:53:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Xin", ""], ["Wang", "Xingzhi", ""], ["Cheung", "Yiu-ming", ""]]}, {"id": "2105.07129", "submitter": "Wen Lu", "authors": "Hongwei Chen and Wen Lu", "title": "Regularized Deep Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a non-linear extension of the classic Linear Discriminant Analysis(LDA),\nDeep Linear Discriminant Analysis(DLDA) replaces the original Categorical Cross\nEntropy(CCE) loss function with eigenvalue-based loss function to make a deep\nneural network(DNN) able to learn linearly separable hidden representations. In\nthis paper, we first point out DLDA focuses on training the cooperative\ndiscriminative ability of all the dimensions in the latent subspace, while put\nless emphasis on training the separable capacity of single dimension. To\nimprove DLDA, a regularization method on within-class scatter matrix is\nproposed to strengthen the discriminative ability of each dimension, and also\nkeep them complement each other. Experiment results on STL-10, CIFAR-10 and\nPediatric Pneumonic Chest X-ray Dataset showed that our proposed regularization\nmethod Regularized Deep Linear Discriminant Analysis(RDLDA) outperformed DLDA\nand conventional neural network with CCE as objective. To further improve the\ndiscriminative ability of RDLDA in the local space, an algorithm named Subclass\nRDLDA is also proposed.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 03:54:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Hongwei", ""], ["Lu", "Wen", ""]]}, {"id": "2105.07139", "submitter": "Wei Zhou", "authors": "Wei Zhou, Zhou Wang, Zhibo Chen", "title": "Image Super-Resolution Quality Assessment: Structural Fidelity Versus\n  Statistical Naturalness", "comments": "Accepted by QoMEX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) algorithms reconstruct high-resolution\n(HR) images with their low-resolution (LR) counterparts. It is desirable to\ndevelop image quality assessment (IQA) methods that can not only evaluate and\ncompare SISR algorithms, but also guide their future development. In this\npaper, we assess the quality of SISR generated images in a two-dimensional (2D)\nspace of structural fidelity versus statistical naturalness. This allows us to\nobserve the behaviors of different SISR algorithms as a tradeoff in the 2D\nspace. Specifically, SISR methods are traditionally designed to achieve high\nstructural fidelity but often sacrifice statistical naturalness, while recent\ngenerative adversarial network (GAN) based algorithms tend to create more\nnatural-looking results but lose significantly on structural fidelity.\nFurthermore, such a 2D evaluation can be easily fused to a scalar quality\nprediction. Interestingly, we find that a simple linear combination of a\nstraightforward local structural fidelity and a global statistical naturalness\nmeasures produce surprisingly accurate predictions of SISR image quality when\ntested using public subject-rated SISR image datasets. Code of the proposed\nSFSN model is publicly available at \\url{https://github.com/weizhou-geek/SFSN}.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:31:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhou", "Wei", ""], ["Wang", "Zhou", ""], ["Chen", "Zhibo", ""]]}, {"id": "2105.07140", "submitter": "Zijin Gu", "authors": "Zijin Gu, Keith W. Jamison, Meenakshi Khosla, Emily J. Allen, Yihan\n  Wu, Thomas Naselaris, Kendrick Kay, Mert R. Sabuncu, Amy Kuceyeski", "title": "NeuroGen: activation optimized image synthesis for discovery\n  neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Functional MRI (fMRI) is a powerful technique that has allowed us to\ncharacterize visual cortex responses to stimuli, yet such experiments are by\nnature constructed based on a priori hypotheses, limited to the set of images\npresented to the individual while they are in the scanner, are subject to noise\nin the observed brain responses, and may vary widely across individuals. In\nthis work, we propose a novel computational strategy, which we call NeuroGen,\nto overcome these limitations and develop a powerful tool for human vision\nneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model\nof human vision with a deep generative network to synthesize images predicted\nto achieve a target pattern of macro-scale brain activation. We demonstrate\nthat the reduction of noise that the encoding model provides, coupled with the\ngenerative network's ability to produce images of high fidelity, results in a\nrobust discovery architecture for visual neuroscience. By using only a small\nnumber of synthetic images created by NeuroGen, we demonstrate that we can\ndetect and amplify differences in regional and individual human brain response\npatterns to visual stimuli. We then verify that these discoveries are reflected\nin the several thousand observed image responses measured with fMRI. We further\ndemonstrate that NeuroGen can create synthetic images predicted to achieve\nregional response patterns not achievable by the best-matching natural images.\nThe NeuroGen framework extends the utility of brain encoding models and opens\nup a new avenue for exploring, and possibly precisely controlling, the human\nvisual system.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:36:39 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gu", "Zijin", ""], ["Jamison", "Keith W.", ""], ["Khosla", "Meenakshi", ""], ["Allen", "Emily J.", ""], ["Wu", "Yihan", ""], ["Naselaris", "Thomas", ""], ["Kay", "Kendrick", ""], ["Sabuncu", "Mert R.", ""], ["Kuceyeski", "Amy", ""]]}, {"id": "2105.07141", "submitter": "Nihar Shrikant Bendre", "authors": "Nihar Bendre, Kevin Desai and Peyman Najafirad", "title": "Show Why the Answer is Correct! Towards Explainable AI using\n  Compositional Temporal Attention", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) models have achieved significant success in\nrecent times. Despite the success of VQA models, they are mostly black-box\nmodels providing no reasoning about the predicted answer, thus raising\nquestions for their applicability in safety-critical such as autonomous systems\nand cyber-security. Current state of the art fail to better complex questions\nand thus are unable to exploit compositionality. To minimize the black-box\neffect of these models and also to make them better exploit compositionality,\nwe propose a Dynamic Neural Network (DMN), which can understand a particular\nquestion and then dynamically assemble various relatively shallow deep learning\nmodules from a pool of modules to form a network. We incorporate compositional\ntemporal attention to these deep learning based modules to increase\ncompositionality exploitation. This results in achieving better understanding\nof complex questions and also provides reasoning as to why the module predicts\na particular answer. Experimental analysis on the two benchmark datasets,\nVQA2.0 and CLEVR, depicts that our model outperforms the previous approaches\nfor Visual Question Answering task as well as provides better reasoning, thus\nmaking it reliable for mission critical applications like safety and security.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:51:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bendre", "Nihar", ""], ["Desai", "Kevin", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2105.07142", "submitter": "Sagnik Majumder", "authors": "Sagnik Majumder, Ziad Al-Halah, Kristen Grauman", "title": "Move2Hear: Active Audio-Visual Source Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the active audio-visual source separation problem, where an\nagent must move intelligently in order to better isolate the sounds coming from\nan object of interest in its environment. The agent hears multiple audio\nsources simultaneously (e.g., a person speaking down the hall in a noisy\nhousehold) and must use its eyes and ears to automatically separate out the\nsounds originating from the target object within a limited time budget. Towards\nthis goal, we introduce a reinforcement learning approach that trains movement\npolicies controlling the agent's camera and microphone placement over time,\nguided by the improvement in predicted audio separation quality. We demonstrate\nour approach in scenarios motivated by both augmented reality (system is\nalready co-located with the target object) and mobile robotics (agent begins\narbitrarily far from the target object). Using state-of-the-art realistic\naudio-visual simulations in 3D environments, we demonstrate our model's ability\nto find minimal movement sequences with maximal payoff for audio source\nseparation. Project: http://vision.cs.utexas.edu/projects/move2hear.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:58:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Majumder", "Sagnik", ""], ["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2105.07143", "submitter": "Monu Verma", "authors": "Monu Verma, Ayushi Gupta, santosh kumar Vipparthi", "title": "One for All: An End-to-End Compact Solution for Hand Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The HGR is a quite challenging task as its performance is influenced by\nvarious aspects such as illumination variations, cluttered backgrounds,\nspontaneous capture, etc. The conventional CNN networks for HGR are following\ntwo stage pipeline to deal with the various challenges: complex signs,\nillumination variations, complex and cluttered backgrounds. The existing\napproaches needs expert expertise as well as auxiliary computation at stage 1\nto remove the complexities from the input images. Therefore, in this paper, we\nproposes an novel end-to-end compact CNN framework: fine grained feature\nattentive network for hand gesture recognition (Fit-Hand) to solve the\nchallenges as discussed above. The pipeline of the proposed architecture\nconsists of two main units: FineFeat module and dilated convolutional (Conv)\nlayer. The FineFeat module extracts fine grained feature maps by employing\nattention mechanism over multiscale receptive fields. The attention mechanism\nis introduced to capture effective features by enlarging the average behaviour\nof multi-scale responses. Moreover, dilated convolution provides global\nfeatures of hand gestures through a larger receptive field. In addition,\nintegrated layer is also utilized to combine the features of FineFeat module\nand dilated layer which enhances the discriminability of the network by\ncapturing complementary context information of hand postures. The effectiveness\nof Fit- Hand is evaluated by using subject dependent (SD) and subject\nindependent (SI) validation setup over seven benchmark datasets: MUGD-I,\nMUGD-II, MUGD-III, MUGD-IV, MUGD-V, Finger Spelling and OUHANDS, respectively.\nFurthermore, to investigate the deep insights of the proposed Fit-Hand\nframework, we performed ten ablation study.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 05:10:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Verma", "Monu", ""], ["Gupta", "Ayushi", ""], ["Vipparthi", "santosh kumar", ""]]}, {"id": "2105.07146", "submitter": "Kecheng Chen", "authors": "Kecheng Chen, Jiayu Sun, Jiang Shen, Jixiang Luo, Xinyu Zhang, Xuelin\n  Pan, Dongsheng Wu, Yue Zhao, Miguel Bento, Yazhou Ren and Xiaorong Pu", "title": "RIDnet: Radiologist-Inspired Deep Neural Network for Low-dose CT\n  Denoising", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being low-level radiation exposure and less harmful to health, low-dose\ncomputed tomography (LDCT) has been widely adopted in the early screening of\nlung cancer and COVID-19. LDCT images inevitably suffer from the degradation\nproblem caused by complex noises. It was reported that, compared with\ncommercial iterative reconstruction methods, deep learning (DL)-based LDCT\ndenoising methods using convolutional neural network (CNN) achieved competitive\nperformance. Most existing DL-based methods focus on the local information\nextracted by CNN, while ignoring both explicit non-local and context\ninformation (which are leveraged by radiologists). To address this issue, we\npropose a novel deep learning model named radiologist-inspired deep denoising\nnetwork (RIDnet) to imitate the workflow of a radiologist reading LDCT images.\nConcretely, the proposed model explicitly integrates all the local, non-local\nand context information rather than local information only. Our\nradiologist-inspired model is potentially favoured by radiologists as a\nfamiliar workflow. A double-blind reader study on a public clinical dataset\nshows that, compared with state-of-the-art methods, our proposed model achieves\nthe most impressive performance in terms of the structural fidelity, the noise\nsuppression and the overall score. As a physicians-inspired model, RIDnet gives\na new research roadmap that takes into account the behavior of physicians when\ndesigning decision support tools for assisting clinical diagnosis. Models and\ncode are available at https://github.com/tonyckc/RIDnet_demo.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 05:59:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Kecheng", ""], ["Sun", "Jiayu", ""], ["Shen", "Jiang", ""], ["Luo", "Jixiang", ""], ["Zhang", "Xinyu", ""], ["Pan", "Xuelin", ""], ["Wu", "Dongsheng", ""], ["Zhao", "Yue", ""], ["Bento", "Miguel", ""], ["Ren", "Yazhou", ""], ["Pu", "Xiaorong", ""]]}, {"id": "2105.07147", "submitter": "Zhiwen Fan", "authors": "Zhiwen Fan, Lingjie Zhu, Honghua Li, Xiaohao Chen, Siyu Zhu, Ping Tan", "title": "FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol\n  Spotting", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to large and diverse computer-aided design (CAD) drawings is critical\nfor developing symbol spotting algorithms. In this paper, we present\nFloorPlanCAD, a large-scale real-world CAD drawing dataset containing over\n10,000 floor plans, ranging from residential to commercial buildings. CAD\ndrawings in the dataset are all represented as vector graphics, which enable us\nto provide line-grained annotations of 30 object categories. Equipped by such\nannotations, we introduce the task of panoptic symbol spotting, which requires\nto spot not only instances of countable things, but also the semantic of\nuncountable stuff. Aiming to solve this task, we propose a novel method by\ncombining Graph Convolutional Networks (GCNs) with Convolutional Neural\nNetworks (CNNs), which captures both non-Euclidean and Euclidean features and\ncan be trained end-to-end. The proposed CNN-GCN method achieved\nstate-of-the-art (SOTA) performance on the task of semantic symbol spotting,\nand help us build a baseline network for the panoptic symbol spotting task. Our\ncontributions are three-fold: 1) to the best of our knowledge, the presented\nCAD drawing dataset is the first of its kind; 2) the panoptic symbol spotting\ntask considers the spotting of both thing instances and stuff semantic as one\nrecognition problem; and 3) we presented a baseline solution to the panoptic\nsymbol spotting task based on a novel CNN-GCN method, which achieved SOTA\nperformance on semantic symbol spotting. We believe that these contributions\nwill boost research in related areas.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 06:01:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Fan", "Zhiwen", ""], ["Zhu", "Lingjie", ""], ["Li", "Honghua", ""], ["Chen", "Xiaohao", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2105.07153", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Ayaan Haque, Adam Wang, Abdullah-Al-Zubaer Imran", "title": "Window-Level is a Strong Denoising Surrogate", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  CT image quality is heavily reliant on radiation dose, which causes a\ntrade-off between radiation dose and image quality that affects the subsequent\nimage-based diagnostic performance. However, high radiation can be harmful to\nboth patients and operators. Several (deep learning-based) approaches have been\nattempted to denoise low dose images. However, those approaches require access\nto large training sets, specifically the full dose CT images for reference,\nwhich can often be difficult to obtain. Self-supervised learning is an emerging\nalternative for lowering the reference data requirement facilitating\nunsupervised learning. Currently available self-supervised CT denoising works\nare either dependent on foreign domain or pretexts are not very task-relevant.\nTo tackle the aforementioned challenges, we propose a novel self-supervised\nlearning approach, namely Self-Supervised Window-Leveling for Image DeNoising\n(SSWL-IDN), leveraging an innovative, task-relevant, simple, yet effective\nsurrogate -- prediction of the window-leveled equivalent. SSWL-IDN leverages\nresidual learning and a hybrid loss combining perceptual loss and MSE, all\nincorporated in a VAE framework. Our extensive (in- and cross-domain)\nexperimentation demonstrates the effectiveness of SSWL-IDN in aggressive\ndenoising of CT (abdomen and chest) images acquired at 5\\% dose level only.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 07:01:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Haque", "Ayaan", ""], ["Wang", "Adam", ""], ["Imran", "Abdullah-Al-Zubaer", ""]]}, {"id": "2105.07174", "submitter": "Saikat Dutta", "authors": "Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah, Anil Kumar Tiwari", "title": "Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect\n  Rendering from a Single Image", "comments": "Accepted to MAI workshop, CVPR 2021. Code and models:\n  https://github.com/saikatdutta/Stacked_DMSHN_bokeh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Bokeh Effect is one of the most desirable effects in photography for\nrendering artistic and aesthetic photos. Usually, it requires a DSLR camera\nwith different aperture and shutter settings and certain photography skills to\ngenerate this effect. In smartphones, computational methods and additional\nsensors are used to overcome the physical lens and sensor limitations to\nachieve such effect. Most of the existing methods utilized additional sensor's\ndata or pretrained network for fine depth estimation of the scene and sometimes\nuse portrait segmentation pretrained network module to segment salient objects\nin the image. Because of these reasons, networks have many parameters, become\nruntime intensive and unable to run in mid-range devices. In this paper, we\nused an end-to-end Deep Multi-Scale Hierarchical Network (DMSHN) model for\ndirect Bokeh effect rendering of images captured from the monocular camera. To\nfurther improve the perceptual quality of such effect, a stacked model\nconsisting of two DMSHN modules is also proposed. Our model does not rely on\nany pretrained network module for Monocular Depth Estimation or Saliency\nDetection, thus significantly reducing the size of model and run time. Stacked\nDMSHN achieves state-of-the-art results on a large scale EBB! dataset with\naround 6x less runtime compared to the current state-of-the-art model in\nprocessing HD quality images.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 08:45:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dutta", "Saikat", ""], ["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Tiwari", "Anil Kumar", ""]]}, {"id": "2105.07175", "submitter": "Tianrui Hui", "authors": "Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, Guanbin Li", "title": "Cross-Modal Progressive Comprehension for Referring Segmentation", "comments": "Accepted by TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a natural language expression and an image/video, the goal of referring\nsegmentation is to produce the pixel-level masks of the entities described by\nthe subject of the expression. Previous approaches tackle this problem by\nimplicit feature interaction and fusion between visual and linguistic\nmodalities in a one-stage manner. However, human tends to solve the referring\nproblem in a progressive manner based on informative words in the expression,\ni.e., first roughly locating candidate entities and then distinguishing the\ntarget one. In this paper, we propose a Cross-Modal Progressive Comprehension\n(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I\n(Image) module and a CMPC-V (Video) module to improve referring image and video\nsegmentation models. For image data, our CMPC-I module first employs entity and\nattribute words to perceive all the related entities that might be considered\nby the expression. Then, the relational words are adopted to highlight the\ntarget entity as well as suppress other irrelevant ones by spatial graph\nreasoning. For video data, our CMPC-V module further exploits action words\nbased on CMPC-I to highlight the correct entity matched with the action cues by\ntemporal graph reasoning. In addition to the CMPC, we also introduce a simple\nyet effective Text-Guided Feature Exchange (TGFE) module to integrate the\nreasoned multimodal features corresponding to different levels in the visual\nbackbone under the guidance of textual information. In this way, multi-level\nfeatures can communicate with each other and be mutually refined based on the\ntextual context. Combining CMPC-I or CMPC-V with TGFE can form our image or\nvideo version referring segmentation frameworks and our frameworks achieve new\nstate-of-the-art performances on four referring image segmentation benchmarks\nand three referring video segmentation benchmarks respectively.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 08:55:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Si", ""], ["Hui", "Tianrui", ""], ["Huang", "Shaofei", ""], ["Wei", "Yunchao", ""], ["Li", "Bo", ""], ["Li", "Guanbin", ""]]}, {"id": "2105.07193", "submitter": "Vishal Kumar", "authors": "Vishal Kumar and Sinnu Susan Thomas", "title": "Make Bipedal Robots Learn How to Imitate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bipedal robots do not perform well as humans since they do not learn to walk\nlike we do. In this paper we propose a method to train a bipedal robot to\nperform some basic movements with the help of imitation learning (IL) in which\nan instructor will perform the movement and the robot will try to mimic the\ninstructor movement. To the best of our knowledge, this is the first time we\ntrain the robot to perform movements with a single video of the instructor and\nas the training is done based on joint angles the robot will keep its joint\nangles always in physical limits which in return help in faster training. The\njoints of the robot are identified by OpenPose architecture and then joint\nangle data is extracted with the help of angle between three points resulting\nin a noisy solution. We smooth the data using Savitzky-Golay filter and\npreserve the Simulatore data anatomy. An ingeniously written Deep Q Network\n(DQN) is trained with experience replay to make the robot learn to perform the\nmovements as similar as the instructor. The implementation of the paper is made\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:06:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kumar", "Vishal", ""], ["Thomas", "Sinnu Susan", ""]]}, {"id": "2105.07197", "submitter": "Shikhar Tuli", "authors": "Shikhar Tuli, Ishita Dasgupta, Erin Grant, Thomas L. Griffiths", "title": "Are Convolutional Neural Networks or Transformers more like human\n  vision?", "comments": "Accepted at CogSci 2021. Source code and fine-tuned models are\n  available at https://github.com/shikhartuli/cnn_txf_bias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:33:35 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 11:55:07 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tuli", "Shikhar", ""], ["Dasgupta", "Ishita", ""], ["Grant", "Erin", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "2105.07200", "submitter": "Gang Yu", "authors": "Yanhua Gao (1), Ting Xie (2), Xun Wang (2), Qingqing Yang (2), Le Chen\n  (2), Kai Sun (2), Youmin Guo (1), Gang Yu (2), Kuansong Wang (3) ((1)\n  Department of Medical Imaging, The First Affiliated Hospital of Xi'an\n  Jiaotong University, 277 Yanta West Road, Xi'an, 710061, China. (2)\n  Department of Biomedical Engineering, School of Basic Medical Sciences,\n  Central South University, 172 Tongzipo Road, Changsha, 410013, China. (3)\n  Department of Pathology, School of Basic Medical Sciences, Central South\n  University, 172 Tongzipo Road, Changsha, 410013, China.)", "title": "Multi-scale super-resolution generation of low-resolution scanned\n  pathological images", "comments": "27 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology slide is easy to store and manage, convenient to browse and\ntransmit. However, because of the high-resolution scan for example 40 times\nmagnification(40X) during the digitization, the file size of each whole slide\nimage exceeds 1Gigabyte, which eventually leads to huge storage capacity and\nvery slow network transmission. We design a strategy to scan slides with low\nresolution (5X) and a super-resolution method is proposed to restore the image\ndetails when in diagnosis. The method is based on a multi-scale generative\nadversarial network, which sequentially generate three high-resolution images\nsuch as 10X, 20X and 40X. The perceived loss, generator loss of the generated\nimages and real images are compared on three image resolutions, and a\ndiscriminator is used to evaluate the difference of highest-resolution\ngenerated image and real image. A dataset consisting of 100,000 pathological\nimages from 10 types of human tissues is performed for training and testing the\nnetwork. The generated images have high peak-signal-to-noise-ratio (PSNR) and\nstructural-similarity-index (SSIM). The PSNR of 10X to 40X image are 24.16,\n22.27 and 20.44, and the SSIM are 0.845, 0.680 and 0.512, which are better than\nother super-resolution networks such as DBPN, ESPCN, RDN, EDSR and MDSR.\nMoreover, visual inspections show that the generated high-resolution images by\nour network have enough details for diagnosis, good color reproduction and\nclose to real images, while other five networks are severely blurred, local\ndeformation or miss important details. Moreover, no significant differences can\nbe found on pathological diagnosis based on the generated and real images. The\nproposed multi-scale network can generate good high-resolution pathological\nimages, and will provide a low-cost storage (about 15MB/image on 5X), faster\nimage sharing method for digital pathology.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 11:09:05 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gao", "Yanhua", ""], ["Xie", "Ting", ""], ["Wang", "Xun", ""], ["Yang", "Qingqing", ""], ["Chen", "Le", ""], ["Sun", "Kai", ""], ["Guo", "Youmin", ""], ["Yu", "Gang", ""], ["Wang", "Kuansong", ""]]}, {"id": "2105.07205", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, Yuexian Zou", "title": "Rethinking Skip Connection with Layer Normalization in Transformers and\n  ResNets", "comments": "Accepted by COLING2020 (The 28th International Conference on\n  Computational Linguistics (COLING 2020))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skip connection, is a widely-used technique to improve the performance and\nthe convergence of deep neural networks, which is believed to relieve the\ndifficulty in optimization due to non-linearity by propagating a linear\ncomponent through the neural network layers. However, from another point of\nview, it can also be seen as a modulating mechanism between the input and the\noutput, with the input scaled by a pre-defined value one. In this work, we\ninvestigate how the scale factors in the effectiveness of the skip connection\nand reveal that a trivial adjustment of the scale will lead to spurious\ngradient exploding or vanishing in line with the deepness of the models, which\ncould be addressed by normalization, in particular, layer normalization, which\ninduces consistent improvements over the plain skip connection. Inspired by the\nfindings, we further propose to adaptively adjust the scale of the input by\nrecursively applying skip connection with layer normalization, which promotes\nthe performance substantially and generalizes well across diverse tasks\nincluding both machine translation and image classification datasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 11:44:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Fenglin", ""], ["Ren", "Xuancheng", ""], ["Zhang", "Zhiyuan", ""], ["Sun", "Xu", ""], ["Zou", "Yuexian", ""]]}, {"id": "2105.07209", "submitter": "Kailun Yang", "authors": "Lei Sun, Jia Wang, Kailun Yang, Kaikai Wu, Xiangdong Zhou, Kaiwei\n  Wang, Jian Bai", "title": "Aerial-PASS: Panoramic Annular Scene Segmentation in Drone Videos", "comments": "Our dataset will be made publicly available at:\n  http://wangkaiwei.org/downloadeg.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial pixel-wise scene perception of the surrounding environment is an\nimportant task for UAVs (Unmanned Aerial Vehicles). Previous research works\nmainly adopt conventional pinhole cameras or fisheye cameras as the imaging\ndevice. However, these imaging systems cannot achieve large Field of View\n(FoV), small size, and lightweight at the same time. To this end, we design a\nUAV system with a Panoramic Annular Lens (PAL), which has the characteristics\nof small size, low weight, and a 360-degree annular FoV. A lightweight\npanoramic annular semantic segmentation neural network model is designed to\nachieve high-accuracy and real-time scene parsing. In addition, we present the\nfirst drone-perspective panoramic scene segmentation dataset Aerial-PASS, with\nannotated labels of track, field, and others. A comprehensive variety of\nexperiments shows that the designed system performs satisfactorily in aerial\npanoramic scene parsing. In particular, our proposed model strikes an excellent\ntrade-off between segmentation performance and inference speed suitable,\nvalidated on both public street-scene and our established aerial-scene\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 12:01:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sun", "Lei", ""], ["Wang", "Jia", ""], ["Yang", "Kailun", ""], ["Wu", "Kaikai", ""], ["Zhou", "Xiangdong", ""], ["Wang", "Kaiwei", ""], ["Bai", "Jian", ""]]}, {"id": "2105.07237", "submitter": "Angad Wadhwa", "authors": "Pinaki Roy Chowdhury, Angad Wadhwa, Antariksha Kar and Nikhil Tyagi", "title": "Brain Inspired Object Recognition System", "comments": "24 Pages, 26 Tables, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a new proposal of an efficient computational model of\nface and object recognition which uses cues from the distributed face and\nobject recognition mechanism of the brain, and by gathering engineering\nequivalent of these cues from existing literature. Three distinct and widely\nused features, Histogram of Oriented Gradients, Local Binary Patterns, and\nPrincipal components extracted from target images are used in a manner which is\nsimple, and yet effective. Our model uses multi-layer perceptrons (MLP) to\nclassify these three features and fuse them at the decision level using sum\nrule. A computational theory is first developed by using concepts from the\ninformation processing mechanism of the brain. Extensive experiments are\ncarried out using fifteen publicly available datasets to validate the\nperformance of our proposed model in recognizing faces and objects with extreme\nvariation of illumination, pose angle, expression, and background. Results\nobtained are extremely promising when compared with other face and object\nrecognition algorithms including CNN and deep learning based methods. This\nhighlights that simple computational processes, if clubbed properly, can\nproduce competing performance with best algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 14:42:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chowdhury", "Pinaki Roy", ""], ["Wadhwa", "Angad", ""], ["Kar", "Antariksha", ""], ["Tyagi", "Nikhil", ""]]}, {"id": "2105.07239", "submitter": "Zhizhong Huang", "authors": "Zhizhong Huang, Shouzhen Chen, Junping Zhang, Hongming Shan", "title": "AgeFlow: Conditional Age Progression and Regression with Normalizing\n  Flows", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age progression and regression aim to synthesize photorealistic appearance of\na given face image with aging and rejuvenation effects, respectively. Existing\ngenerative adversarial networks (GANs) based methods suffer from the following\nthree major issues: 1) unstable training introducing strong ghost artifacts in\nthe generated faces, 2) unpaired training leading to unexpected changes in\nfacial attributes such as genders and races, and 3) non-bijective age mappings\nincreasing the uncertainty in the face transformation. To overcome these\nissues, this paper proposes a novel framework, termed AgeFlow, to integrate the\nadvantages of both flow-based models and GANs. The proposed AgeFlow contains\nthree parts: an encoder that maps a given face to a latent space through an\ninvertible neural network, a novel invertible conditional translation module\n(ICTM) that translates the source latent vector to target one, and a decoder\nthat reconstructs the generated face from the target latent vector using the\nsame encoder network; all parts are invertible achieving bijective age\nmappings. The novelties of ICTM are two-fold. First, we propose an\nattribute-aware knowledge distillation to learn the manipulation direction of\nage progression while keeping other unrelated attributes unchanged, alleviating\nunexpected changes in facial attributes. Second, we propose to use GANs in the\nlatent space to ensure the learned latent vector indistinguishable from the\nreal ones, which is much easier than traditional use of GANs in the image\ndomain. Experimental results demonstrate superior performance over existing\nGANs-based methods on two benchmarked datasets. The source code is available at\nhttps://github.com/Hzzone/AgeFlow.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 15:02:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Huang", "Zhizhong", ""], ["Chen", "Shouzhen", ""], ["Zhang", "Junping", ""], ["Shan", "Hongming", ""]]}, {"id": "2105.07245", "submitter": "Zifan Chen", "authors": "ZiFan Chen, Xin Qin, Chao Yang, Li Zhang", "title": "Composite Localization for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing human pose estimation methods are confronted with inaccurate\nlong-distance regression or high computational cost due to the complex learning\nobjectives. This work proposes a novel deep learning framework for human pose\nestimation called composite localization to divide the complex learning\nobjective into two simpler ones: a sparse heatmap to find the keypoint's\napproximate location and two short-distance offsetmaps to obtain its final\nprecise coordinates. To realize the framework, we construct two types of\ncomposite localization networks: CLNet-ResNet and CLNet-Hourglass. We evaluate\nthe networks on three benchmark datasets, including the Leeds Sports Pose\ndataset, the MPII Human Pose dataset, and the COCO keypoints detection dataset.\nThe experimental results show that our CLNet-ResNet50 outperforms\nSimpleBaseline by 1.14% with about 1/2 GFLOPs. Our CLNet-Hourglass outperforms\nthe original stacked-hourglass by 4.45% on COCO.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 15:22:27 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "ZiFan", ""], ["Qin", "Xin", ""], ["Yang", "Chao", ""], ["Zhang", "Li", ""]]}, {"id": "2105.07264", "submitter": "Rajat Talak", "authors": "Rajat Talak, Siyi Hu, Lisa Peng, and Luca Carlone", "title": "Neural Trees for Learning on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach\nfor learning over graphs. Despite this success, existing GNNs are constrained\nby their local message-passing architecture and are provably limited in their\nexpressive power. In this work, we propose a new GNN architecture -- the Neural\nTree. The neural tree architecture does not perform message passing on the\ninput graph but on a tree-structured graph, called the H-tree, that is\nconstructed from the input graph. Nodes in the H-tree correspond to subgraphs\nin the input graph, and they are reorganized in a hierarchical manner such that\na parent-node of a node in the H-tree always corresponds to a larger subgraph\nin the input graph. We show that the neural tree architecture can approximate\nany smooth probability distribution function over an undirected graph, as well\nas emulate the junction tree algorithm. We also prove that the number of\nparameters needed to achieve an $\\epsilon$-approximation of the distribution\nfunction is exponential in the treewidth of the input graph, but linear in its\nsize. We apply the neural tree to semi-supervised node classification in 3D\nscene graphs, and show that these theoretical properties translate into\nsignificant gains in prediction accuracy, over the more traditional GNN\narchitectures.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 17:08:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Talak", "Rajat", ""], ["Hu", "Siyi", ""], ["Peng", "Lisa", ""], ["Carlone", "Luca", ""]]}, {"id": "2105.07269", "submitter": "Ajinkya Tejankar", "authors": "Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash", "title": "Mean Shift for Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most recent self-supervised learning (SSL) algorithms learn features by\ncontrasting between instances of images or by clustering the images and then\ncontrasting between the image clusters. We introduce a simple mean-shift\nalgorithm that learns representations by grouping images together without\ncontrasting between them or adopting much of prior on the structure of the\nclusters. We simply \"shift\" the embedding of each image to be close to the\n\"mean\" of its neighbors. Since in our setting, the closest neighbor is always\nanother augmentation of the same image, our model will be identical to BYOL\nwhen using only one nearest neighbor instead of 5 as used in our experiments.\nOur model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200\nepochs outperforming BYOL. Our code is available here:\nhttps://github.com/UMBCvision/MSF\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 17:42:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Koohpayegani", "Soroush Abbasi", ""], ["Tejankar", "Ajinkya", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "2105.07273", "submitter": "Mengyu Yang", "authors": "Mengyu Yang, David Rokeby, Xavier Snelgrove", "title": "Mask-Guided Discovery of Semantic Manifolds in Generative Models", "comments": "In the 4th Workshop on Machine Learning for Creativity and Design at\n  NeurIPS 2020, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in the realm of Generative Adversarial Networks (GANs) have led to\narchitectures capable of producing amazingly realistic images such as\nStyleGAN2, which, when trained on the FFHQ dataset, generates images of human\nfaces from random vectors in a lower-dimensional latent space. Unfortunately,\nthis space is entangled - translating a latent vector along its axes does not\ncorrespond to a meaningful transformation in the output space (e.g., smiling\nmouth, squinting eyes). The model behaves as a black box, providing neither\ncontrol over its output nor insight into the structures it has learned from the\ndata. We present a method to explore the manifolds of changes of spatially\nlocalized regions of the face. Our method discovers smoothly varying sequences\nof latent vectors along these manifolds suitable for creating animations.\nUnlike existing disentanglement methods that either require labelled data or\nexplicitly alter internal model parameters, our method is an optimization-based\napproach guided by a custom loss function and manually defined region of\nchange. Our code is open-sourced, which can be found, along with supplementary\nresults, on our project page: https://github.com/bmolab/masked-gan-manifold\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 18:06:38 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Mengyu", ""], ["Rokeby", "David", ""], ["Snelgrove", "Xavier", ""]]}, {"id": "2105.07299", "submitter": "Eyvind Niklasson", "authors": "Alexander Mordvintsev, Eyvind Niklasson, Ettore Randazzo", "title": "Texture Generation with Neural Cellular Automata", "comments": "AI for Content Creation Workshop, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural Cellular Automata (NCA) have shown a remarkable ability to learn the\nrequired rules to \"grow\" images, classify morphologies, segment images, as well\nas to do general computation such as path-finding. We believe the inductive\nprior they introduce lends itself to the generation of textures. Textures in\nthe natural world are often generated by variants of locally interacting\nreaction-diffusion systems. Human-made textures are likewise often generated in\na local manner (textile weaving, for instance) or using rules with local\ndependencies (regular grids or geometric patterns). We demonstrate learning a\ntexture generator from a single template image, with the generation method\nbeing embarrassingly parallel, exhibiting quick convergence and high fidelity\nof output, and requiring only some minimal assumptions around the underlying\nstate manifold. Furthermore, we investigate properties of the learned models\nthat are both useful and interesting, such as non-stationary dynamics and an\ninherent robustness to damage. Finally, we make qualitative claims that the\nbehaviour exhibited by the NCA model is a learned, distributed, local algorithm\nto generate a texture, setting our method apart from existing work on texture\ngeneration. We discuss the advantages of such a paradigm.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 22:05:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mordvintsev", "Alexander", ""], ["Niklasson", "Eyvind", ""], ["Randazzo", "Ettore", ""]]}, {"id": "2105.07322", "submitter": "Max Ehrlich", "authors": "Arthita Ghosh, Max Ehrlich, Larry Davis, Rama Chellappa", "title": "Unsupervised Super-Resolution of Satellite Imagery for High Fidelity\n  Material Label Transfer", "comments": "Published in the proceedings of the 2019 IEEE International\n  Geoscience and Remote Sensing Symposium", "journal-ref": "IGARSS (2019), 5144-5147", "doi": "10.1109/IGARSS.2019.8900639", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban material recognition in remote sensing imagery is a highly relevant,\nyet extremely challenging problem due to the difficulty of obtaining human\nannotations, especially on low resolution satellite images. To this end, we\npropose an unsupervised domain adaptation based approach using adversarial\nlearning. We aim to harvest information from smaller quantities of high\nresolution data (source domain) and utilize the same to super-resolve low\nresolution imagery (target domain). This can potentially aid in semantic as\nwell as material label transfer from a richly annotated source to a target\ndomain.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 00:57:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ghosh", "Arthita", ""], ["Ehrlich", "Max", ""], ["Davis", "Larry", ""], ["Chellappa", "Rama", ""]]}, {"id": "2105.07331", "submitter": "Haichao Yu", "authors": "Haichao Yu, Linjie Yang, Humphrey Shi", "title": "Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain\n  Calibration for Network Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Post-training quantization methods use a set of calibration data to compute\nquantization ranges for network parameters and activations. The calibration\ndata usually comes from the training dataset which could be inaccessible due to\nsensitivity of the data. In this work, we want to study such a problem: can we\nuse out-of-domain data to calibrate the trained networks without knowledge of\nthe original dataset? Specifically, we go beyond the domain of natural images\nto include drastically different domains such as X-ray images, satellite images\nand ultrasound images. We find cross-domain calibration leads to surprisingly\nstable performance of quantized models on 10 tasks in different image domains\nwith 13 different calibration datasets. We also find that the performance of\nquantized models is correlated with the similarity of the Gram matrices between\nthe source and calibration domains, which can be used as a criterion to choose\ncalibration set for better performance. We believe our research opens the door\nto borrow cross-domain knowledge for network quantization and compression.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 02:07:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yu", "Haichao", ""], ["Yang", "Linjie", ""], ["Shi", "Humphrey", ""]]}, {"id": "2105.07334", "submitter": "Kenneth Co", "authors": "Kenneth T. Co, Luis Mu\\~noz-Gonz\\'alez, Leslie Kanthan, Emil C. Lupu", "title": "Real-time Detection of Practical Universal Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Universal Adversarial Perturbations (UAPs) are a prominent class of\nadversarial examples that exploit the systemic vulnerabilities and enable\nphysically realizable and robust attacks against Deep Neural Networks (DNNs).\nUAPs generalize across many different inputs; this leads to realistic and\neffective attacks that can be applied at scale. In this paper we propose\nHyperNeuron, an efficient and scalable algorithm that allows for the real-time\ndetection of UAPs by identifying suspicious neuron hyper-activations. Our\nresults show the effectiveness of HyperNeuron on multiple tasks (image\nclassification, object detection), against a wide variety of universal attacks,\nand in realistic scenarios, like perceptual ad-blocking and adversarial\npatches. HyperNeuron is able to simultaneously detect both adversarial mask and\npatch UAPs with comparable or better performance than existing UAP defenses\nwhilst introducing a significantly reduced latency of only 0.86 milliseconds\nper image. This suggests that many realistic and practical universal attacks\ncan be reliably mitigated in real-time, which shows promise for the robust\ndeployment of machine learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 03:01:29 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 23:33:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Co", "Kenneth T.", ""], ["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Kanthan", "Leslie", ""], ["Lupu", "Emil C.", ""]]}, {"id": "2105.07345", "submitter": "Shijie Yu", "authors": "Shijie Yu and Dapeng Chen and Rui Zhao and Haobin Chen and Yu Qiao", "title": "Neighbourhood-guided Feature Reconstruction for Occluded Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person images captured by surveillance cameras are often occluded by various\nobstacles, which lead to defective feature representation and harm person\nre-identification (Re-ID) performance. To tackle this challenge, we propose to\nreconstruct the feature representation of occluded parts by fully exploiting\nthe information of its neighborhood in a gallery image set. Specifically, we\nfirst introduce a visible part-based feature by body mask for each person\nimage. Then we identify its neighboring samples using the visible features and\nreconstruct the representation of the full body by an outlier-removable graph\nneural network with all the neighboring samples as input. Extensive experiments\nshow that the proposed approach obtains significant improvements. In the\nlarge-scale Occluded-DukeMTMC benchmark, our approach achieves 64.2% mAP and\n67.6% rank-1 accuracy which outperforms the state-of-the-art approaches by\nlarge margins, i.e.,20.4% and 12.5%, respectively, indicating the effectiveness\nof our method on occluded Re-ID problem.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 03:53:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yu", "Shijie", ""], ["Chen", "Dapeng", ""], ["Zhao", "Rui", ""], ["Chen", "Haobin", ""], ["Qiao", "Yu", ""]]}, {"id": "2105.07350", "submitter": "Zicheng Zhang", "authors": "ZiCheng Zhang, CongYing Han, TianDe Guo", "title": "ExSinGAN: Learning an Explainable Generative Model from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images from a single sample, as a newly developing branch of image\nsynthesis, has attracted extensive attention. In this paper, we formulate this\nproblem as sampling from the conditional distribution of a single image, and\npropose a hierarchical framework that simplifies the learning of the intricate\nconditional distributions through the successive learning of the distributions\nabout structure, semantics and texture, making the process of learning and\ngeneration comprehensible. On this basis, we design ExSinGAN composed of three\ncascaded GANs for learning an explainable generative model from a given image,\nwhere the cascaded GANs model the distributions about structure, semantics and\ntexture successively. ExSinGAN is learned not only from the internal patches of\nthe given image as the previous works did, but also from the external prior\nobtained by the GAN inversion technique. Benefiting from the appropriate\ncombination of internal and external information, ExSinGAN has a more powerful\ncapability of generation and competitive generalization ability for the image\nmanipulation tasks compared with prior works.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 04:38:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "ZiCheng", ""], ["Han", "CongYing", ""], ["Guo", "TianDe", ""]]}, {"id": "2105.07364", "submitter": "Yu Shen", "authors": "Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen, Delu Pan, Jianyu Chen,\n  Liang Xiao, Qian Du", "title": "BDANet: Multiscale Convolutional Neural Network with Cross-directional\n  Attention for Building Damage Assessment from Satellite Images", "comments": "arXiv admin note: text overlap with arXiv:2010.14014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective responses are required when a natural disaster (e.g.,\nearthquake, hurricane, etc.) strikes. Building damage assessment from satellite\nimagery is critical before relief effort is deployed. With a pair of pre- and\npost-disaster satellite images, building damage assessment aims at predicting\nthe extent of damage to buildings. With the powerful ability of feature\nrepresentation, deep neural networks have been successfully applied to building\ndamage assessment. Most existing works simply concatenate pre- and\npost-disaster images as input of a deep neural network without considering\ntheir correlations. In this paper, we propose a novel two-stage convolutional\nneural network for Building Damage Assessment, called BDANet. In the first\nstage, a U-Net is used to extract the locations of buildings. Then the network\nweights from the first stage are shared in the second stage for building damage\nassessment. In the second stage, a two-branch multi-scale U-Net is employed as\nbackbone, where pre- and post-disaster images are fed into the network\nseparately. A cross-directional attention module is proposed to explore the\ncorrelations between pre- and post-disaster images. Moreover, CutMix data\naugmentation is exploited to tackle the challenge of difficult classes. The\nproposed method achieves state-of-the-art performance on a large-scale dataset\n-- xBD. The code is available at\nhttps://github.com/ShaneShen/BDANet-Building-Damage-Assessment.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 06:13:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shen", "Yu", ""], ["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Pan", "Delu", ""], ["Chen", "Jianyu", ""], ["Xiao", "Liang", ""], ["Du", "Qian", ""]]}, {"id": "2105.07371", "submitter": "Gesina Schwalbe", "authors": "Johannes Rabold, Gesina Schwalbe, Ute Schmid", "title": "Expressive Explanations of DNNs by Combining Concept Analysis with ILP", "comments": "14 pages, 4 figures; Camera-ready submission to KI2020; The final\n  authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-58285-2_11; code available at\n  https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020", "journal-ref": null, "doi": "10.1007/978-3-030-58285-2_11", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable AI has emerged to be a key component for black-box machine\nlearning approaches in domains with a high demand for reliability or\ntransparency. Examples are medical assistant systems, and applications\nconcerned with the General Data Protection Regulation of the European Union,\nwhich features transparency as a cornerstone. Such demands require the ability\nto audit the rationale behind a classifier's decision. While visualizations are\nthe de facto standard of explanations, they come short in terms of\nexpressiveness in many ways: They cannot distinguish between different\nattribute manifestations of visual features (e.g. eye open vs. closed), and\nthey cannot accurately describe the influence of absence of, and relations\nbetween features. An alternative would be more expressive symbolic surrogate\nmodels. However, these require symbolic inputs, which are not readily available\nin most computer vision tasks. In this paper we investigate how to overcome\nthis: We use inherent features learned by the network to build a global,\nexpressive, verbal explanation of the rationale of a feed-forward convolutional\ndeep neural network (DNN). The semantics of the features are mined by a concept\nanalysis approach trained on a set of human understandable visual concepts. The\nexplanation is found by an Inductive Logic Programming (ILP) method and\npresented as first-order rules. We show that our explanation is faithful to the\noriginal black-box model.\n  The code for our experiments is available at\nhttps://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 07:00:27 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rabold", "Johannes", ""], ["Schwalbe", "Gesina", ""], ["Schmid", "Ute", ""]]}, {"id": "2105.07387", "submitter": "Yuhang Zhang", "authors": "Yuhang Zhang and Xiaopeng Zhang and Robert.C.Qiu and Jie Li and\n  Haohang Xu and Qi Tian", "title": "Semi-supervised Contrastive Learning with Similarity Co-calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised learning acts as an effective way to leverage massive\nunlabeled data. In this paper, we propose a novel training strategy, termed as\nSemi-supervised Contrastive Learning (SsCL), which combines the well-known\ncontrastive loss in self-supervised learning with the cross entropy loss in\nsemi-supervised learning, and jointly optimizes the two objectives in an\nend-to-end way. The highlight is that different from self-training based\nsemi-supervised learning that conducts prediction and retraining over the same\nmodel weights, SsCL interchanges the predictions over the unlabeled data\nbetween the two branches, and thus formulates a co-calibration procedure, which\nwe find is beneficial for better prediction and avoid being trapped in local\nminimum. Towards this goal, the contrastive loss branch models pairwise\nsimilarities among samples, using the nearest neighborhood generated from the\ncross entropy branch, and in turn calibrates the prediction distribution of the\ncross entropy branch with the contrastive similarity. We show that SsCL\nproduces more discriminative representation and is beneficial to few shot\nlearning. Notably, on ImageNet with ResNet50 as the backbone, SsCL achieves\n60.2% and 72.1% top-1 accuracy with 1% and 10% labeled samples, respectively,\nwhich significantly outperforms the baseline, and is better than previous\nsemi-supervised and self-supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 09:13:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Yuhang", ""], ["Zhang", "Xiaopeng", ""], ["Qiu", "Robert. C.", ""], ["Li", "Jie", ""], ["Xu", "Haohang", ""], ["Tian", "Qi", ""]]}, {"id": "2105.07391", "submitter": "Kazuya Ueki", "authors": "Kazuya Ueki", "title": "Survey of Visual-Semantic Embedding Methods for Zero-Shot Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-semantic embedding is an interesting research topic because it is\nuseful for various tasks, such as visual question answering (VQA), image-text\nretrieval, image captioning, and scene graph generation. In this paper, we\nfocus on zero-shot image retrieval using sentences as queries and present a\nsurvey of the technological trends in this area. First, we provide a\ncomprehensive overview of the history of the technology, starting with a\ndiscussion of the early studies of image-to-text matching and how the\ntechnology has evolved over time. In addition, a description of the datasets\ncommonly used in experiments and a comparison of the evaluation results of each\nmethod are presented. We also introduce the implementation available on github\nfor use in confirming the accuracy of experiments and for further improvement.\nWe hope that this survey paper will encourage researchers to further develop\ntheir research on bridging images and languages.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 09:43:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ueki", "Kazuya", ""]]}, {"id": "2105.07392", "submitter": "Wangbin Ding", "authors": "Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang", "title": "Unsupervised Registration Method based on Deep Neural Network:\n  Application to cardiac and liver MR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality medical images can provide relevant or complementary\ninformation for a target (organ, tumor or tissue). Registering multi-modality\nimages to a common space can fuse these comprehensive information, and bring\nconvenience for clinical application. Recently, neural networks have been\nwidely investigated to boost registration methods. However, it is still\nchallenging to develop a multi-modality registration network due to the lack of\nrobust criteria for network training. In this work, we propose a multi-modality\nregistration network (MMRegNet), which can perform registration between\nmulti-modality images. Meanwhile, we present spatially encoded gradient\ninformation to train MMRegNet in an unsupervised manner. The proposed network\nwas evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising\nperformance for left ventricle cardiac registration tasks. Meanwhile, to\ndemonstrate the versatility of MMRegNet, we further evaluate the method with a\nliver dataset from CHAOS 2019. Source code will be released\npublicly\\footnote{https://github.com/NanYoMy/mmregnet} once the manuscript is\naccepted.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 09:47:42 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 03:36:09 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ding", "Wangbin", ""], ["Li", "Lei", ""], ["Zhuang", "Xiahai", ""], ["Huang", "Liqin", ""]]}, {"id": "2105.07402", "submitter": "Wanli Liu", "authors": "Wanli Liu, Chen Li, Hongzan Sun, Weiming Hu, Haoyuan Chen, Changhao\n  Sun, Marcin Grzegorzek", "title": "Is Image Size Important? A Robustness Comparison of Deep Learning\n  Methods for Multi-scale Cell Image Classification Tasks: from Convolutional\n  Neural Networks to Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical cancer is a very common and fatal cancer in women, but it can be\nprevented through early examination and treatment. Cytopathology images are\noften used to screen for cancer. Then, because of the possibility of artificial\nerrors due to the large number of this method, the computer-aided diagnosis\nsystem based on deep learning is developed. The image input required by the\ndeep learning method is usually consistent, but the size of the clinical\nmedical image is inconsistent. The internal information is lost after resizing\nthe image directly, so it is unreasonable. A lot of research is to directly\nresize the image, and the results are still robust. In order to find a\nreasonable explanation, 22 deep learning models are used to process images of\ndifferent scales, and experiments are conducted on the SIPaKMeD dataset. The\nconclusion is that the deep learning method is very robust to the size changes\nof images. This conclusion is also validated on the Herlev dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:37:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Wanli", ""], ["Li", "Chen", ""], ["Sun", "Hongzan", ""], ["Hu", "Weiming", ""], ["Chen", "Haoyuan", ""], ["Sun", "Changhao", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2105.07404", "submitter": "Yixuan Li", "authors": "Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, Limin Wang", "title": "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized\n  Sports Actions", "comments": "One track of DeeperAction Workshop@ICCV2021. HomePage:\n  https://deeperaction.github.io/multisports/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal action detection is an important and challenging problem in\nvideo understanding. The existing action detection benchmarks are limited in\naspects of small numbers of instances in a trimmed video or relatively\nlow-level atomic actions. This paper aims to present a new multi-person dataset\nof spatio-temporal localized sports actions, coined as MultiSports. We first\nanalyze the important ingredients of constructing a realistic and challenging\ndataset for spatio-temporal action detection by proposing three criteria: (1)\nmotion dependent identification, (2) with well-defined boundaries, (3)\nrelatively high-level classes. Based on these guidelines, we build the dataset\nof Multi-Sports v1.0 by selecting 4 sports classes, collecting around 3200\nvideo clips, and annotating around 37790 action instances with 907k bounding\nboxes. Our datasets are characterized with important properties of strong\ndiversity, detailed annotation, and high quality. Our MultiSports, with its\nrealistic setting and dense annotations, exposes the intrinsic challenge of\naction localization. To benchmark this, we adapt several representative methods\nto our dataset and give an in-depth analysis on the difficulty of action\nlocalization in our dataset. We hope our MultiSports can serve as a standard\nbenchmark for spatio-temporal action detection in the future. Our dataset\nwebsite is at https://deeperaction.github.io/multisports/.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:40:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Yixuan", ""], ["Chen", "Lei", ""], ["He", "Runyu", ""], ["Wang", "Zhenzhi", ""], ["Wu", "Gangshan", ""], ["Wang", "Limin", ""]]}, {"id": "2105.07451", "submitter": "Debesh Jha", "authors": "Abhishek Srivastava, Debesh Jha, Sukalpa Chanda, Umapada Pal,\n  H{\\aa}vard D. Johansen, Dag Johansen, Michael A. Riegler, Sharib Ali, P{\\aa}l\n  Halvorsen", "title": "MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods based on convolutional neural networks have improved the performance\nof biomedical image segmentation. However, most of these methods cannot\nefficiently segment objects of variable sizes and train on small and biased\ndatasets, which are common in biomedical use cases. While methods exist that\nincorporate multi-scale fusion approaches to address the challenges arising\nwith variable sizes, they usually use complex models that are more suitable for\ngeneral semantic segmentation computer vision problems. In this paper, we\npropose a novel architecture called MSRF-Net, which is specially designed for\nmedical image segmentation tasks. The proposed MSRF-Net is able to exchange\nmulti-scale features of varying receptive fields using a dual-scale dense\nfusion block (DSDF). Our DSDF block can exchange information rigorously across\ntwo different resolution scales, and our MSRF sub-network uses multiple DSDF\nblocks in sequence to perform multi-scale fusion. This allows the preservation\nof resolution, improved information flow, and propagation of both high- and\nlow-level features to obtain accurate segmentation maps. The proposed MSRF-Net\nallows to capture object variabilities and provides improved results on\ndifferent biomedical datasets. Extensive experiments on MSRF-Net demonstrate\nthat the proposed method outperforms most of the cutting-edge medical image\nsegmentation state-of-the-art methods. MSRF-Net advances the performance on\nfour publicly available datasets, and also, MSRF-Net is more generalizable as\ncompared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 15:19:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Srivastava", "Abhishek", ""], ["Jha", "Debesh", ""], ["Chanda", "Sukalpa", ""], ["Pal", "Umapada", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""], ["Riegler", "Michael A.", ""], ["Ali", "Sharib", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2105.07463", "submitter": "Mohammed Daoudi", "authors": "Naima Otberdout, Claudio Ferrari, Mohamed Daoudi, Stefano Berretti,\n  Alberto Del Bimbo", "title": "3D to 4D Facial Expressions Generation Guided by Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep learning-based 3D face generation has made a progress recently,\nthe problem of dynamic 3D (4D) facial expression synthesis is less\ninvestigated. In this paper, we propose a novel solution to the following\nquestion: given one input 3D neutral face, can we generate dynamic 3D (4D)\nfacial expressions from it? To tackle this problem, we first propose a mesh\nencoder-decoder architecture (Expr-ED) that exploits a set of 3D landmarks to\ngenerate an expressive 3D face from its neutral counterpart. Then, we extend it\nto 4D by modeling the temporal dynamics of facial expressions using a\nmanifold-valued GAN capable of generating a sequence of 3D landmarks from an\nexpression label (Motion3DGAN). The generated landmarks are fed into the mesh\nencoder-decoder, ultimately producing a sequence of 3D expressive faces. By\ndecoupling the two steps, we separately address the non-linearity induced by\nthe mesh deformation and motion dynamics. The experimental results on the CoMA\ndataset show that our mesh encoder-decoder guided by landmarks brings a\nsignificant improvement with respect to other landmark-based 3D fitting\napproaches, and that we can generate high quality dynamic facial expressions.\nThis framework further enables the 3D expression intensity to be continuously\nadapted from low to high intensity. Finally, we show our framework can be\napplied to other tasks, such as 2D-3D facial expression transfer.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 15:52:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Otberdout", "Naima", ""], ["Ferrari", "Claudio", ""], ["Daoudi", "Mohamed", ""], ["Berretti", "Stefano", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2105.07467", "submitter": "Michael Yeung", "authors": "Michael Yeung, Evis Sala, Carola-Bibiane Sch\\\"onlieb, Leonardo Rundo", "title": "Focus U-Net: A novel dual attention-gated CNN for polyp segmentation\n  during colonoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Colonoscopy remains the gold-standard screening for colorectal\ncancer. However, significant miss rates for polyps have been reported,\nparticularly when there are multiple small adenomas. This presents an\nopportunity to leverage computer-aided systems to support clinicians and reduce\nthe number of polyps missed.\n  Method: In this work we introduce the Focus U-Net, a novel dual\nattention-gated deep neural network, which combines efficient spatial and\nchannel-based attention into a single Focus Gate module to encourage selective\nlearning of polyp features. The Focus U-Net further incorporates short-range\nskip connections and deep supervision. Furthermore, we introduce the Hybrid\nFocal loss, a new compound loss function based on the Focal loss and Focal\nTversky loss, to handle class-imbalanced image segmentation. For our\nexperiments, we selected five public datasets containing images of polyps\nobtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,\nETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we\nuse the Dice similarity coefficient (DSC) and Intersection over Union (IoU)\nmetrics.\n  Results: Our model achieves state-of-the-art results for both CVC-ClinicDB\nand Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When\nevaluated on a combination of five public polyp datasets, our model similarly\nachieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of\n0.809, a 14% and 15% improvement over the previous state-of-the-art results of\n0.768 and 0.702, respectively.\n  Conclusions: This study shows the potential for deep learning to provide fast\nand accurate polyp segmentation results for use during colonoscopy. The Focus\nU-Net may be adapted for future use in newer non-invasive screening and more\nbroadly to other biomedical image segmentation tasks involving class imbalance\nand requiring efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:10:32 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 11:03:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yeung", "Michael", ""], ["Sala", "Evis", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Rundo", "Leonardo", ""]]}, {"id": "2105.07468", "submitter": "Margarita Grinvald", "authors": "Margarita Grinvald, Federico Tombari, Roland Siegwart, Juan Nieto", "title": "TSDF++: A Multi-Object Formulation for Dynamic Object Tracking and\n  Reconstruction", "comments": "7 pages, 3 figures. To be published in the 2021 IEEE International\n  Conference on Robotics and Automation (ICRA). Code is available at\n  https://github.com/ethz-asl/tsdf-plusplus and the accompanying video material\n  can be found at https://youtu.be/dSJmoeVasI0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to simultaneously track and reconstruct multiple objects moving\nin the scene is of the utmost importance for robotic tasks such as autonomous\nnavigation and interaction. Virtually all of the previous attempts to map\nmultiple dynamic objects have evolved to store individual objects in separate\nreconstruction volumes and track the relative pose between them. While simple\nand intuitive, such formulation does not scale well with respect to the number\nof objects in the scene and introduces the need for an explicit occlusion\nhandling strategy. In contrast, we propose a map representation that allows\nmaintaining a single volume for the entire scene and all the objects therein.\nTo this end, we introduce a novel multi-object TSDF formulation that can encode\nmultiple object surfaces at any given location in the map. In a multiple\ndynamic object tracking and reconstruction scenario, our representation allows\nmaintaining accurate reconstruction of surfaces even while they become\ntemporarily occluded by other objects moving in their proximity. We evaluate\nthe proposed TSDF++ formulation on a public synthetic dataset and demonstrate\nits ability to preserve reconstructions of occluded surfaces when compared to\nthe standard TSDF map representation.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:15:05 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Grinvald", "Margarita", ""], ["Tombari", "Federico", ""], ["Siegwart", "Roland", ""], ["Nieto", "Juan", ""]]}, {"id": "2105.07469", "submitter": "Amirhossein Kardoost", "authors": "Amirhossein Kardoost and Margret Keuper", "title": "Uncertainty in Minimum Cost Multicuts for Image and Motion Segmentation", "comments": "Accepted in the 37th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum cost lifted multicut approach has proven practically good\nperformance in a wide range of applications such as image decomposition, mesh\nsegmentation, multiple object tracking, and motion segmentation. It addresses\nsuch problems in a graph-based model, where real-valued costs are assigned to\nthe edges between entities such that the minimum cut decomposes the graph into\nan optimal number of segments. Driven by a probabilistic formulation of minimum\ncost multicuts, we provide a measure for the uncertainties of the decisions\nmade during the optimization. We argue that access to such uncertainties is\ncrucial for many practical applications and conduct an evaluation by means of\nsparsifications on three different, widely used datasets in the context of\nimage decomposition (BSDS-500) and motion segmentation (DAVIS2016 and FBMS59)\nin terms of variation of information (VI) and Rand index (RI).\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:22:38 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kardoost", "Amirhossein", ""], ["Keuper", "Margret", ""]]}, {"id": "2105.07474", "submitter": "Baris Gecer", "authors": "Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou", "title": "Fast-GANFIT: Generative Adversarial Network for High Fidelity 3D Face\n  Reconstruction", "comments": "TPAMI camera ready (submitted: 05-May-2020); Check project page:\n  https://github.com/barisgecer/GANFit. arXiv admin note: substantial text\n  overlap with arXiv:1902.05978", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A lot of work has been done towards reconstructing the 3D facial structure\nfrom single images by capitalizing on the power of Deep Convolutional Neural\nNetworks (DCNNs). In the recent works, the texture features either correspond\nto components of a linear texture space or are learned by auto-encoders\ndirectly from in-the-wild images. In all cases, the quality of the facial\ntexture reconstruction is still not capable of modeling facial texture with\nhigh-frequency details. In this paper, we take a radically different approach\nand harness the power of Generative Adversarial Networks (GANs) and DCNNs in\norder to reconstruct the facial texture and shape from single images. That is,\nwe utilize GANs to train a very powerful facial texture prior \\edit{from a\nlarge-scale 3D texture dataset}. Then, we revisit the original 3D Morphable\nModels (3DMMs) fitting making use of non-linear optimization to find the\noptimal latent parameters that best reconstruct the test image but under a new\nperspective. In order to be robust towards initialisation and expedite the\nfitting process, we propose a novel self-supervised regression based approach.\nWe demonstrate excellent results in photorealistic and identity preserving 3D\nface reconstructions and achieve for the first time, to the best of our\nknowledge, facial texture reconstruction with high-frequency details.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:35:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gecer", "Baris", ""], ["Ploumpis", "Stylianos", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2105.07484", "submitter": "Ioannis Pikoulis", "authors": "Ioannis Pikoulis, Panagiotis P. Filntisis, Petros Maragos", "title": "Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional\n  Architectures in a Contextual Approach for Video-Based Visual Emotion\n  Recognition in the Wild", "comments": "9 pages, 4 figures, 5 tables, submitted to the 16th IEEE\n  International Conference on Automatic Face and Gesture Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we tackle the task of video-based visual emotion recognition in\nthe wild. Standard methodologies that rely solely on the extraction of bodily\nand facial features often fall short of accurate emotion prediction in cases\nwhere the aforementioned sources of affective information are inaccessible due\nto head/body orientation, low resolution and poor illumination. We aspire to\nalleviate this problem by leveraging visual context in the form of scene\ncharacteristics and attributes, as part of a broader emotion recognition\nframework. Temporal Segment Networks (TSN) constitute the backbone of our\nproposed model. Apart from the RGB input modality, we make use of dense Optical\nFlow, following an intuitive multi-stream approach for a more effective\nencoding of motion. Furthermore, we shift our attention towards skeleton-based\nlearning and leverage action-centric data as means of pre-training a\nSpatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion\nrecognition. Our extensive experiments on the challenging Body Language Dataset\n(BoLD) verify the superiority of our methods over existing approaches, while by\nproperly incorporating all of the aforementioned modules in a network ensemble,\nwe manage to surpass the previous best published recognition scores, by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 17:31:59 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Pikoulis", "Ioannis", ""], ["Filntisis", "Panagiotis P.", ""], ["Maragos", "Petros", ""]]}, {"id": "2105.07512", "submitter": "Xiao Wang", "authors": "Xiao Wang, Wei Jiang, Wei Wang, Shan Liu, Brian Kulis, Peter Chin", "title": "Substitutional Neural Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We describe Substitutional Neural Image Compression (SNIC), a general\napproach for enhancing any neural image compression model, that requires no\ndata or additional tuning of the trained model. It boosts compression\nperformance toward a flexible distortion metric and enables bit-rate control\nusing a single model instance. The key idea is to replace the image to be\ncompressed with a substitutional one that outperforms the original one in a\ndesired way. Finding such a substitute is inherently difficult for conventional\ncodecs, yet surprisingly favorable for neural compression models thanks to\ntheir fully differentiable structures. With gradients of a particular loss\nbackpropogated to the input, a desired substitute can be efficiently crafted\niteratively. We demonstrate the effectiveness of SNIC, when combined with\nvarious neural compression models and target metrics, in improving compression\nquality and performing bit-rate control measured by rate-distortion curves.\nEmpirical results of control precision and generation speed are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 20:53:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xiao", ""], ["Jiang", "Wei", ""], ["Wang", "Wei", ""], ["Liu", "Shan", ""], ["Kulis", "Brian", ""], ["Chin", "Peter", ""]]}, {"id": "2105.07533", "submitter": "Richard Jiang", "authors": "Richard Jiang, Paul Chazot, Danny Crookes, Ahmed Bouridane and M Emre\n  Celebi", "title": "Private Facial Diagnosis as an Edge Service for Parkinson's DBS\n  Treatment Valuation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial phenotyping has recently been successfully exploited for medical\ndiagnosis as a novel way to diagnose a range of diseases, where facial\nbiometrics has been revealed to have rich links to underlying genetic or\nmedical causes. In this paper, taking Parkinson's Diseases (PD) as a case\nstudy, we proposed an Artificial-Intelligence-of-Things (AIoT) edge-oriented\nprivacy-preserving facial diagnosis framework to analyze the treatment of Deep\nBrain Stimulation (DBS) on PD patients. In the proposed framework, a new\nedge-based information theoretically secure framework is proposed to implement\nprivate deep facial diagnosis as a service over a privacy-preserving\nAIoT-oriented multi-party communication scheme, where partial homomorphic\nencryption (PHE) is leveraged to enable privacy-preserving deep facial\ndiagnosis directly on encrypted facial patterns. In our experiments with a\ncollected facial dataset from PD patients, for the first time, we demonstrated\nthat facial patterns could be used to valuate the improvement of PD patients\nundergoing DBS treatment. We further implemented a privacy-preserving deep\nfacial diagnosis framework that can achieve the same accuracy as the\nnon-encrypted one, showing the potential of our privacy-preserving facial\ndiagnosis as an trustworthy edge service for grading the severity of PD in\npatients.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:24:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jiang", "Richard", ""], ["Chazot", "Paul", ""], ["Crookes", "Danny", ""], ["Bouridane", "Ahmed", ""], ["Celebi", "M Emre", ""]]}, {"id": "2105.07537", "submitter": "Wenjin Wang Dr.", "authors": "Wenjin Wang, Albertus C. den Brinker", "title": "Algorithmic Principles of Camera-based Respiratory Motion Extraction", "comments": "Camera based contactless health monitoring", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the respiratory signal from a video based on body motion has been\nproposed and recently matured in products for video health monitoring. The core\nalgorithm for this measurement is the estimation of tiny chest/abdominal\nmotions induced by respiration, and the fundamental challenge is motion\nsensitivity. Though prior arts reported on the validation with real human\nsubjects, there is no thorough/rigorous benchmark to quantify the sensitivities\nand boundary conditions of motion-based core respiratory algorithms that\nmeasure sub-pixel displacement between video frames. In this paper, we designed\na setup with a fully-controllable physical phantom to investigate the essence\nof core algorithms, together with a mathematical model incorporating two motion\nestimation strategies and three spatial representations, leading to six\nalgorithmic combinations for respiratory signal extraction. Their promises and\nlimitations are discussed and clarified via the phantom benchmark. The insights\ngained in this paper are intended to improve the understanding and applications\nof camera-based respiration measurement in health monitoring.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:45:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Wenjin", ""], ["Brinker", "Albertus C. den", ""]]}, {"id": "2105.07540", "submitter": "Yun Liu", "authors": "Sahar Kazemzadeh, Jin Yu, Shahar Jamshy, Rory Pilgrim, Zaid Nabulsi,\n  Christina Chen, Neeral Beladia, Charles Lau, Scott Mayer McKinney, Thad\n  Hughes, Atilla Kiraly, Sreenivasa Raju Kalidindi, Monde Muyoyeta, Jameson\n  Malemela, Ting Shih, Greg S. Corrado, Lily Peng, Katherine Chou, Po-Hsuan\n  Cameron Chen, Yun Liu, Krish Eswaran, Daniel Tse, Shravya Shetty, Shruthi\n  Prabhakara", "title": "Deep learning for detecting pulmonary tuberculosis via chest\n  radiography: an international study across 10 countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO\nrecommends chest radiographs (CXRs) for TB screening, the limited availability\nof CXR interpretation is a barrier. We trained a deep learning system (DLS) to\ndetect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and\nEurope, and utilized large-scale CXR pretraining, attention pooling, and noisy\nstudent semi-supervised learning. Evaluation was on (1) a combined test set\nspanning China, India, US, and Zambia, and (2) an independent mining population\nin South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the\nDLS's operating point was prespecified to favor sensitivity over specificity.\nOn the combined test set, the DLS's ROC curve was above all 9 India-based\nradiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity\n(88%) was higher than the India-based radiologists (75% mean sensitivity),\np<0.001 for superiority; and its specificity (79%) was non-inferior to the\nradiologists (84% mean specificity), p=0.004. Similar trends were observed\nwithin HIV positive and sputum smear positive sub-groups, and in the South\nAfrica test set. We found that 5 US-based radiologists (where TB isn't endemic)\nwere more sensitive and less specific than the India-based radiologists (where\nTB is endemic). The DLS also remained non-inferior to the US-based\nradiologists. In simulations, using the DLS as a prioritization tool for\nconfirmatory testing reduced the cost per positive case detected by 40-80%\ncompared to using confirmatory testing alone. To conclude, our DLS generalized\nto 5 countries, and merits prospective evaluation to assist cost-effective\nscreening efforts in radiologist-limited settings. Operating point flexibility\nmay permit customization of the DLS to account for site-specific factors such\nas TB prevalence, demographics, clinical resources, and customary practice\npatterns.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:56:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kazemzadeh", "Sahar", ""], ["Yu", "Jin", ""], ["Jamshy", "Shahar", ""], ["Pilgrim", "Rory", ""], ["Nabulsi", "Zaid", ""], ["Chen", "Christina", ""], ["Beladia", "Neeral", ""], ["Lau", "Charles", ""], ["McKinney", "Scott Mayer", ""], ["Hughes", "Thad", ""], ["Kiraly", "Atilla", ""], ["Kalidindi", "Sreenivasa Raju", ""], ["Muyoyeta", "Monde", ""], ["Malemela", "Jameson", ""], ["Shih", "Ting", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Chou", "Katherine", ""], ["Chen", "Po-Hsuan Cameron", ""], ["Liu", "Yun", ""], ["Eswaran", "Krish", ""], ["Tse", "Daniel", ""], ["Shetty", "Shravya", ""], ["Prabhakara", "Shruthi", ""]]}, {"id": "2105.07553", "submitter": "Zheng Zhang", "authors": "Xunguang Wang, Zheng Zhang, Baoyuan Wu, Fumin Shen, Guangming Lu", "title": "Prototype-supervised Adversarial Network for Targeted Attack of Deep\n  Hashing", "comments": "This paper has been accepted by CVPR 2021, and the related codes\n  could be available at https://github.com/xunguangwang/ProS-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its powerful capability of representation learning and high-efficiency\ncomputation, deep hashing has made significant progress in large-scale image\nretrieval. However, deep hashing networks are vulnerable to adversarial\nexamples, which is a practical secure problem but seldom studied in\nhashing-based retrieval field. In this paper, we propose a novel\nprototype-supervised adversarial network (ProS-GAN), which formulates a\nflexible generative architecture for efficient and effective targeted hashing\nattack. To the best of our knowledge, this is the first generation-based method\nto attack deep hashing networks. Generally, our proposed framework consists of\nthree parts, i.e., a PrototypeNet, a generator, and a discriminator.\nSpecifically, the designed PrototypeNet embeds the target label into the\nsemantic representation and learns the prototype code as the category-level\nrepresentative of the target label. Moreover, the semantic representation and\nthe original image are jointly fed into the generator for a flexible targeted\nattack. Particularly, the prototype code is adopted to supervise the generator\nto construct the targeted adversarial example by minimizing the Hamming\ndistance between the hash code of the adversarial example and the prototype\ncode. Furthermore, the generator is against the discriminator to simultaneously\nencourage the adversarial examples visually realistic and the semantic\nrepresentation informative. Extensive experiments verify that the proposed\nframework can efficiently produce adversarial examples with better targeted\nattack performance and transferability over state-of-the-art targeted attack\nmethods of deep hashing. The related codes could be available at\nhttps://github.com/xunguangwang/ProS-GAN .\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 00:31:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xunguang", ""], ["Zhang", "Zheng", ""], ["Wu", "Baoyuan", ""], ["Shen", "Fumin", ""], ["Lu", "Guangming", ""]]}, {"id": "2105.07561", "submitter": "Shixiang Tang", "authors": "Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu and Wanli Ouyang", "title": "Layerwise Optimization by Gradient Decomposition for Continual Learning", "comments": "cvpr2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve state-of-the-art and sometimes super-human\nperformance across various domains. However, when learning tasks sequentially,\nthe networks easily forget the knowledge of previous tasks, known as\n\"catastrophic forgetting\". To achieve the consistencies between the old tasks\nand the new task, one effective solution is to modify the gradient for update.\nPrevious methods enforce independent gradient constraints for different tasks,\nwhile we consider these gradients contain complex information, and propose to\nleverage inter-task information by gradient decomposition. In particular, the\ngradient of an old task is decomposed into a part shared by all old tasks and a\npart specific to that task. The gradient for update should be close to the\ngradient of the new task, consistent with the gradients shared by all old\ntasks, and orthogonal to the space spanned by the gradients specific to the old\ntasks. In this way, our approach encourages common knowledge consolidation\nwithout impairing the task-specific knowledge. Furthermore, the optimization is\nperformed for the gradients of each layer separately rather than the\nconcatenation of all gradients as in previous works. This effectively avoids\nthe influence of the magnitude variation of the gradients in different layers.\nExtensive experiments validate the effectiveness of both gradient-decomposed\noptimization and layer-wise updates. Our proposed method achieves\nstate-of-the-art results on various benchmarks of continual learning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:15:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tang", "Shixiang", ""], ["Chen", "Dapeng", ""], ["Zhu", "Jinguo", ""], ["Yu", "Shijie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2105.07576", "submitter": "Yuxin Wu", "authors": "Yuxin Wu, Justin Johnson", "title": "Rethinking \"Batch\" in BatchNorm", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BatchNorm is a critical building block in modern convolutional neural\nnetworks. Its unique property of operating on \"batches\" instead of individual\nsamples introduces significantly different behaviors from most other operations\nin deep learning. As a result, it leads to many hidden caveats that can\nnegatively impact model's performance in subtle ways. This paper thoroughly\nreviews such problems in visual recognition tasks, and shows that a key to\naddress them is to rethink different choices in the concept of \"batch\" in\nBatchNorm. By presenting these caveats and their mitigations, we hope this\nreview can help researchers use BatchNorm more effectively.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:58:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wu", "Yuxin", ""], ["Johnson", "Justin", ""]]}, {"id": "2105.07581", "submitter": "Pin-Yu Chen", "authors": "Sayak Paul and Pin-Yu Chen", "title": "Vision Transformers are Robust Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers, composed of multiple self-attention layers, hold strong\npromises toward a generic learning primitive applicable to different data\nmodalities, including the recent breakthroughs in computer vision achieving\nstate-of-the-art (SOTA) standard accuracy with better parameter efficiency.\nSince self-attention helps a model systematically align different components\npresent inside the input data, it leaves grounds to investigate its performance\nunder model robustness benchmarks. In this work, we study the robustness of the\nVision Transformer (ViT) against common corruptions and perturbations,\ndistribution shifts, and natural adversarial examples. We use six different\ndiverse ImageNet datasets concerning robust classification to conduct a\ncomprehensive performance comparison of ViT models and SOTA convolutional\nneural networks (CNNs), Big-Transfer. Through a series of six systematically\ndesigned experiments, we then present analyses that provide both quantitative\nand qualitative indications to explain why ViTs are indeed more robust\nlearners. For example, with fewer parameters and similar dataset and\npre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A\nwhich is 4.3x higher than a comparable variant of BiT. Our analyses on image\nmasking, Fourier spectrum sensitivity, and spread on discrete cosine energy\nspectrum reveal intriguing properties of ViT attributing to improved\nrobustness. Code for reproducing our experiments is available here:\nhttps://git.io/J3VO0.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 02:39:22 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 04:02:06 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Paul", "Sayak", ""], ["Chen", "Pin-Yu", ""]]}, {"id": "2105.07592", "submitter": "Yutong Li", "authors": "Yutong Li, Ruoqing Zhu, Annie Qu and Mike Yeh", "title": "Dermoscopic Image Classification with Neural Style Transfer", "comments": "32 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skin cancer, the most commonly found human malignancy, is primarily diagnosed\nvisually via dermoscopic analysis, biopsy, and histopathological examination.\nHowever, unlike other types of cancer, automated image classification of skin\nlesions is deemed more challenging due to the irregularity and variability in\nthe lesions' appearances. In this work, we propose an adaptation of the Neural\nStyle Transfer (NST) as a novel image pre-processing step for skin lesion\nclassification problems. We represent each dermoscopic image as the style image\nand transfer the style of the lesion onto a homogeneous content image. This\ntransfers the main variability of each lesion onto the same localized region,\nwhich allows us to integrate the generated images together and extract latent,\nlow-rank style features via tensor decomposition. We train and cross-validate\nour model on a dermoscopic data set collected and preprocessed from the\nInternational Skin Imaging Collaboration (ISIC) database. We show that the\nclassification performance based on the extracted tensor features using the\nstyle-transferred images significantly outperforms that of the raw images by\nmore than 10%, and is also competitive with well-studied, pre-trained CNN\nmodels through transfer learning. Additionally, the tensor decomposition\nfurther identifies latent style clusters, which may provide clinical\ninterpretation and insights.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:50:51 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 04:42:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yutong", ""], ["Zhu", "Ruoqing", ""], ["Qu", "Annie", ""], ["Yeh", "Mike", ""]]}, {"id": "2105.07593", "submitter": "Peter Karkus", "authors": "Peter Karkus, Shaojun Cai, David Hsu", "title": "Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation", "comments": "CVPR 2021, extended results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous localization and mapping (SLAM) remains challenging for a number\nof downstream applications, such as visual robot navigation, because of rapid\nturns, featureless walls, and poor camera quality. We introduce the\nDifferentiable SLAM Network (SLAM-net) along with a navigation architecture to\nenable planar robot navigation in previously unseen indoor environments.\nSLAM-net encodes a particle filter based SLAM algorithm in a differentiable\ncomputation graph, and learns task-oriented neural network components by\nbackpropagating through the SLAM algorithm. Because it can optimize all model\ncomponents jointly for the end-objective, SLAM-net learns to be robust in\nchallenging conditions. We run experiments in the Habitat platform with\ndifferent real-world RGB and RGB-D datasets. SLAM-net significantly outperforms\nthe widely adapted ORB-SLAM in noisy conditions. Our navigation architecture\nwith SLAM-net improves the state-of-the-art for the Habitat Challenge 2020\nPointNav task by a large margin (37% to 64% success). Project website:\nhttp://sites.google.com/view/slamnet\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:54:34 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 14:12:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Karkus", "Peter", ""], ["Cai", "Shaojun", ""], ["Hsu", "David", ""]]}, {"id": "2105.07599", "submitter": "Feng Bao", "authors": "Feng Bao", "title": "Disentangled Variational Information Bottleneck for Multiview\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiview data contain information from multiple modalities and have\npotentials to provide more comprehensive features for diverse machine learning\ntasks. A fundamental question in multiview analysis is what is the additional\ninformation brought by additional views and can quantitatively identify this\nadditional information. In this work, we try to tackle this challenge by\ndecomposing the entangled multiview features into shared latent representations\nthat are common across all views and private representations that are specific\nto each single view. We formulate this feature disentanglement in the framework\nof information bottleneck and propose disentangled variational information\nbottleneck (DVIB). DVIB explicitly defines the properties of shared and private\nrepresentations using constrains from mutual information. By deriving\nvariational upper and lower bounds of mutual information terms, representations\nare efficiently optimized. We demonstrate the shared and private\nrepresentations learned by DVIB well preserve the common labels shared between\ntwo views and unique labels corresponding to each single view, respectively.\nDVIB also shows comparable performance in classification task on images with\ncorruptions. DVIB implementation is available at\nhttps://github.com/feng-bao-ucsf/DVIB.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:03:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bao", "Feng", ""]]}, {"id": "2105.07606", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang,\n  Shuai Yi", "title": "Towards Unsupervised Domain Adaptation for Deep Face Recognition under\n  Privacy Constraints via Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation has been widely adopted to generalize models\nfor unlabeled data in a target domain, given labeled data in a source domain,\nwhose data distributions differ from the target domain. However, existing works\nare inapplicable to face recognition under privacy constraints because they\nrequire sharing sensitive face images between two domains. To address this\nproblem, we propose a novel unsupervised federated face recognition approach\n(FedFR). FedFR improves the performance in the target domain by iteratively\naggregating knowledge from the source domain through federated learning. It\nprotects data privacy by transferring models instead of raw data between\ndomains. Besides, we propose a new domain constraint loss (DCL) to regularize\nsource domain training. DCL suppresses the data volume dominance of the source\ndomain. We also enhance a hierarchical clustering algorithm to predict pseudo\nlabels for the unlabeled target domain accurately. To this end, FedFR forms an\nend-to-end training pipeline: (1) pre-train in the source domain; (2) predict\npseudo labels by clustering in the target domain; (3) conduct\ndomain-constrained federated learning across two domains. Extensive experiments\nand analysis on two newly constructed benchmarks demonstrate the effectiveness\nof FedFR. It outperforms the baseline and classic methods in the target domain\nby over 4% on the more realistic benchmark. We believe that FedFR will shed\nlight on applying federated learning to more computer vision tasks under\nprivacy constraints.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:24:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhuang", "Weiming", ""], ["Gan", "Xin", ""], ["Wen", "Yonggang", ""], ["Zhang", "Xuesen", ""], ["Zhang", "Shuai", ""], ["Yi", "Shuai", ""]]}, {"id": "2105.07621", "submitter": "Sho Inoue", "authors": "Sho Inoue and Tad Gonsalves", "title": "Style-Restricted GAN: Multi-Modal Translation with Style Restriction\n  Using Generative Adversarial Networks", "comments": "20 pages, 11 figures, 6 tables; Our implementation is available at\n  https://github.com/shinshoji01/Style-Restricted_GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Unpaired image-to-image translation using Generative Adversarial Networks\n(GAN) is successful in converting images among multiple domains. Moreover,\nrecent studies have shown a way to diversify the outputs of the generator.\nHowever, since there are no restrictions on how the generator diversifies the\nresults, it is likely to translate some unexpected features. In this paper, we\npropose Style-Restricted GAN (SRGAN) to demonstrate the importance of\ncontrolling the encoded features used in style diversifying process. More\nspecifically, instead of KL divergence loss, we adopt three new losses to\nrestrict the distribution of the encoded features: batch KL divergence loss,\ncorrelation loss, and histogram imitation loss. Further, the encoder is\npre-trained with classification tasks before being used in translation process.\nThe study reports quantitative as well as qualitative results with Precision,\nRecall, Density, and Coverage. The proposed three losses lead to the\nenhancement of the level of diversity compared to the conventional KL loss. In\nparticular, SRGAN is found to be successful in translating with higher\ndiversity and without changing the class-unrelated features in the CelebA face\ndataset. To conclude, the importance of the encoded features being\nwell-regulated was proven with two experiments. Our implementation is available\nat https://github.com/shinshoji01/Style-Restricted_GAN.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 05:58:33 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 06:57:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Inoue", "Sho", ""], ["Gonsalves", "Tad", ""]]}, {"id": "2105.07625", "submitter": "Kamala Gajurel", "authors": "Kamala Gajurel, Cuncong Zhong and Guanghui Wang", "title": "A Fine-Grained Visual Attention Approach for Fingerspelling Recognition\n  in the Wild", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fingerspelling in sign language has been the means of communicating technical\nterms and proper nouns when they do not have dedicated sign language gestures.\nAutomatic recognition of fingerspelling can help resolve communication barriers\nwhen interacting with deaf people. The main challenges prevalent in\nfingerspelling recognition are the ambiguity in the gestures and strong\narticulation of the hands. The automatic recognition model should address high\ninter-class visual similarity and high intra-class variation in the gestures.\nMost of the existing research in fingerspelling recognition has focused on the\ndataset collected in a controlled environment. The recent collection of a\nlarge-scale annotated fingerspelling dataset in the wild, from social media and\nonline platforms, captures the challenges in a real-world scenario. In this\nwork, we propose a fine-grained visual attention mechanism using the\nTransformer model for the sequence-to-sequence prediction task in the wild\ndataset. The fine-grained attention is achieved by utilizing the change in\nmotion of the video frames (optical flow) in sequential context-based attention\nalong with a Transformer encoder model. The unsegmented continuous video\ndataset is jointly trained by balancing the Connectionist Temporal\nClassification (CTC) loss and the maximum-entropy loss. The proposed approach\ncan capture better fine-grained attention in a single iteration. Experiment\nevaluations show that it outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:15:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gajurel", "Kamala", ""], ["Zhong", "Cuncong", ""], ["Wang", "Guanghui", ""]]}, {"id": "2105.07627", "submitter": "Subhankar Ghosh", "authors": "Subhankar Ghosh", "title": "Shared and Private VAEs with Generative Replay for Continual Learning", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning tries to learn new tasks without forgetting previously\nlearned ones. In reality, most of the existing artificial neural network(ANN)\nmodels fail, while humans do the same by remembering previous works throughout\ntheir life. Although simply storing all past data can alleviate the problem, it\nneeds large memory and often infeasible in real-world applications where last\ndata access is limited. We hypothesize that the model that learns to solve each\ntask continually has some task-specific properties and some task-invariant\ncharacteristics. We propose a hybrid continual learning model that is more\nsuitable in real case scenarios to address the issues that has a task-invariant\nshared variational autoencoder and T task-specific variational autoencoders.\nOur model combines generative replay and architectural growth to prevent\ncatastrophic forgetting. We show our hybrid model effectively avoids forgetting\nand achieves state-of-the-art results on visual continual learning benchmarks\nsuch as MNIST, Permuted MNIST(QMNIST), CIFAR100, and miniImageNet datasets. We\ndiscuss results on a few more datasets, such as SVHN, Fashion-MNIST, EMNIST,\nand CIFAR10.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:18:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ghosh", "Subhankar", ""]]}, {"id": "2105.07635", "submitter": "Lakshman Balasubramanian", "authors": "Lakshman Balasubramanian, Friedrich Kruber, Michael Botsch and Ke Deng", "title": "Open-set Recognition based on the Combination of Deep Learning and\n  Ensemble Method for Detecting Unknown Traffic Scenarios", "comments": "Accepted for IEEE Intelligent Vehicles 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An understanding and classification of driving scenarios are important for\ntesting and development of autonomous driving functionalities. Machine learning\nmodels are useful for scenario classification but most of them assume that data\nreceived during the testing are from one of the classes used in the training.\nThis assumption is not true always because of the open environment where\nvehicles operate. This is addressed by a new machine learning paradigm called\nopen-set recognition. Open-set recognition is the problem of assigning test\nsamples to one of the classes used in training or to an unknown class. This\nwork proposes a combination of Convolutional Neural Networks (CNN) and Random\nForest (RF) for open set recognition of traffic scenarios. CNNs are used for\nthe feature generation and the RF algorithm along with extreme value theory for\nthe detection of known and unknown classes. The proposed solution is featured\nby exploring the vote patterns of trees in RF instead of just majority voting.\nBy inheriting the ensemble nature of RF, the vote pattern of all trees combined\nwith extreme value theory is shown to be well suited for detecting unknown\nclasses. The proposed method has been tested on the highD and OpenTraffic\ndatasets and has demonstrated superior performance in various aspects compared\nto existing solutions.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:48:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Balasubramanian", "Lakshman", ""], ["Kruber", "Friedrich", ""], ["Botsch", "Michael", ""], ["Deng", "Ke", ""]]}, {"id": "2105.07636", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Bernardo Gonzalez Torres", "title": "DOC3-Deep One Class Classification using Contradictions", "comments": "Deep Learning, Anomaly Detection, Visual Inspection, Learning from\n  Contradictions, Outlier Exposure, 18 pages, 14 tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces the notion of learning from contradictions (a.k.a\nUniversum learning) for deep one class classification problems. We formalize\nthis notion for the widely adopted one class large-margin loss, and propose the\nDeep One Class Classification using Contradictions (DOC3) algorithm. We show\nthat learning from contradictions incurs lower generalization error by\ncomparing the Empirical Radamacher Complexity (ERC) of DOC3 against its\ntraditional inductive learning counterpart. Our empirical results demonstrate\nthe efficacy of DOC3 algorithm achieving > 30% for CIFAR-10 and >50% for MV-Tec\nAD data sets in test AUCs compared to its inductive learning counterpart and in\nmany cases improving the state-of-the-art in anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:48:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dhar", "Sauptik", ""], ["Torres", "Bernardo Gonzalez", ""]]}, {"id": "2105.07637", "submitter": "Pengyang Li", "authors": "Pengyang Li, Yanan Li and Donghui Wang", "title": "Class-Incremental Few-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional detection networks usually need abundant labeled training\nsamples, while humans can learn new concepts incrementally with just a few\nexamples. This paper focuses on a more challenging but realistic\nclass-incremental few-shot object detection problem (iFSD). It aims to\nincrementally transfer the model for novel objects from only a few annotated\nsamples without catastrophically forgetting the previously learned ones. To\ntackle this problem, we propose a novel method LEAST, which can transfer with\nLess forgetting, fEwer training resources, And Stronger Transfer capability.\nSpecifically, we first present the transfer strategy to reduce unnecessary\nweight adaptation and improve the transfer capability for iFSD. On this basis,\nwe then integrate the knowledge distillation technique using a less\nresource-consuming approach to alleviate forgetting and propose a novel\nclustering-based exemplar selection process to preserve more discriminative\nfeatures previously learned. Being a generic and effective method, LEAST can\nlargely improve the iFSD performance on various benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:49:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Pengyang", ""], ["Li", "Yanan", ""], ["Wang", "Donghui", ""]]}, {"id": "2105.07639", "submitter": "Lakshman Balasubramanian", "authors": "Lakshman Balasubramanian, Jonas Wurst, Michael Botsch and Ke Deng", "title": "Traffic Scenario Clustering by Iterative Optimisation of Self-Supervised\n  Networks Using a Random Forest Activation Pattern Similarity", "comments": "Accepted for IEEE Intelligent Vehicles 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic scenario categorisation is an essential component of automated\ndriving, for e.\\,g., in motion planning algorithms and their validation.\nFinding new relevant scenarios without handcrafted steps reduce the required\nresources for the development of autonomous driving dramatically. In this work,\na method is proposed to address this challenge by introducing a clustering\ntechnique based on a novel data-adaptive similarity measure, called Random\nForest Activation Pattern (RFAP) similarity. The RFAP similarity is generated\nusing a tree encoding scheme in a Random Forest algorithm. The clustering\nmethod proposed in this work takes into account that there are labelled\nscenarios available and the information from the labelled scenarios can help to\nguide the clustering of unlabelled scenarios. It consists of three steps.\nFirst, a self-supervised Convolutional Neural Network~(CNN) is trained on all\navailable traffic scenarios using a defined self-supervised objective. Second,\nthe CNN is fine-tuned for classification of the labelled scenarios. Third,\nusing the labelled and unlabelled scenarios an iterative optimisation procedure\nis performed for clustering. In the third step at each epoch of the iterative\noptimisation, the CNN is used as a feature generator for an unsupervised Random\nForest. The trained forest, in turn, provides the RFAP similarity to adapt\niteratively the feature generation process implemented by the CNN. Extensive\nexperiments and ablation studies have been done on the highD dataset. The\nproposed method shows superior performance compared to baseline clustering\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:54:59 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 09:26:17 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Balasubramanian", "Lakshman", ""], ["Wurst", "Jonas", ""], ["Botsch", "Michael", ""], ["Deng", "Ke", ""]]}, {"id": "2105.07645", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Giorgos Kordopatis-Zilos, Panagiotis Galopoulos, Symeon Papadopoulos,\n  Ioannis Kompatsiaris", "title": "Leveraging EfficientNet and Contrastive Learning for Accurate\n  Global-scale Location Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of global-scale image geolocation,\nproposing a mixed classification-retrieval scheme. Unlike other methods that\nstrictly tackle the problem as a classification or retrieval task, we combine\nthe two practices in a unified solution leveraging the advantages of each\napproach with two different modules. The first leverages the EfficientNet\narchitecture to assign images to a specific geographic cell in a robust way.\nThe second introduces a new residual architecture that is trained with\ncontrastive learning to map input images to an embedding space that minimizes\nthe pairwise geodesic distance of same-location images. For the final location\nestimation, the two modules are combined with a search-within-cell scheme,\nwhere the locations of most similar images from the predicted geographic cell\nare aggregated based on a spatial clustering scheme. Our approach demonstrates\nvery competitive performance on four public datasets, achieving new\nstate-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km\nrange on Im2GPS3k.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 07:18:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Galopoulos", "Panagiotis", ""], ["Papadopoulos", "Symeon", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2105.07647", "submitter": "Yi Wei", "authors": "Yi Wei, Shang Su, Jiwen Lu, Jie Zhou", "title": "FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle\n  Detection", "comments": "Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of weakly supervised 3D vehicle\ndetection. Conventional methods for 3D object detection need vast amounts of\nmanually labelled 3D data as supervision signals. However, annotating large\ndatasets requires huge human efforts, especially for 3D area. To tackle this\nproblem, we propose frustum-aware geometric reasoning (FGR) to detect vehicles\nin point clouds without any 3D annotations. Our method consists of two stages:\ncoarse 3D segmentation and 3D bounding box estimation. For the first stage, a\ncontext-aware adaptive region growing algorithm is designed to segment objects\nbased on 2D bounding boxes. Leveraging predicted segmentation masks, we develop\nan anti-noise approach to estimate 3D bounding boxes in the second stage.\nFinally 3D pseudo labels generated by our method are utilized to train a 3D\ndetector. Independent of any 3D groundtruth, FGR reaches comparable performance\nwith fully supervised methods on the KITTI dataset. The findings indicate that\nit is able to accurately detect objects in 3D space with only 2D bounding boxes\nand sparse point clouds.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 07:29:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wei", "Yi", ""], ["Su", "Shang", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.07660", "submitter": "Etienne David", "authors": "Etienne David, Mario Serouart, Daniel Smith, Simon Madec, Kaaviya\n  Velumani, Shouyang Liu, Xu Wang, Francisco Pinto Espinosa, Shahameh Shafiee,\n  Izzat S. A. Tahir, Hisashi Tsujimoto, Shuhei Nasuda, Bangyou Zheng, Norbert\n  Kichgessner, Helge Aasen, Andreas Hund, Pouria Sadhegi-Tehran, Koichi\n  Nagasawa, Goro Ishikawa, S\\'ebastien Dandrifosse, Alexis Carlier, Benoit\n  Mercatoris, Ken Kuroki, Haozhou Wang, Masanori Ishii, Minhajul A. Badhon,\n  Curtis Pozniak, David Shaner LeBauer, Morten Lilimo, Jesse Poland, Scott\n  Chapman, Benoit de Solan, Fr\\'ed\\'eric Baret, Ian Stavness, Wei Guo", "title": "Global Wheat Head Dataset 2021: more diversity to improve the\n  benchmarking of wheat head localization methods", "comments": "8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Global Wheat Head Detection (GWHD) dataset was created in 2020 and has\nassembled 193,634 labelled wheat heads from 4,700 RGB images acquired from\nvarious acquisition platforms and 7 countries/institutions. With an associated\ncompetition hosted in Kaggle, GWHD has successfully attracted attention from\nboth the computer vision and agricultural science communities. From this first\nexperience in 2020, a few avenues for improvements have been identified,\nespecially from the perspective of data size, head diversity and label\nreliability. To address these issues, the 2020 dataset has been reexamined,\nrelabeled, and augmented by adding 1,722 images from 5 additional countries,\nallowing for 81,553 additional wheat heads to be added. We now release a new\nversion of the Global Wheat Head Detection (GWHD) dataset in 2021, which is\nbigger, more diverse, and less noisy than the 2020 version. The GWHD 2021 is\nnow publicly available at http://www.global-wheat.com/ and a new data challenge\nhas been organized on AIcrowd to make use of this updated dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:18:30 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 15:32:53 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["David", "Etienne", ""], ["Serouart", "Mario", ""], ["Smith", "Daniel", ""], ["Madec", "Simon", ""], ["Velumani", "Kaaviya", ""], ["Liu", "Shouyang", ""], ["Wang", "Xu", ""], ["Espinosa", "Francisco Pinto", ""], ["Shafiee", "Shahameh", ""], ["Tahir", "Izzat S. A.", ""], ["Tsujimoto", "Hisashi", ""], ["Nasuda", "Shuhei", ""], ["Zheng", "Bangyou", ""], ["Kichgessner", "Norbert", ""], ["Aasen", "Helge", ""], ["Hund", "Andreas", ""], ["Sadhegi-Tehran", "Pouria", ""], ["Nagasawa", "Koichi", ""], ["Ishikawa", "Goro", ""], ["Dandrifosse", "S\u00e9bastien", ""], ["Carlier", "Alexis", ""], ["Mercatoris", "Benoit", ""], ["Kuroki", "Ken", ""], ["Wang", "Haozhou", ""], ["Ishii", "Masanori", ""], ["Badhon", "Minhajul A.", ""], ["Pozniak", "Curtis", ""], ["LeBauer", "David Shaner", ""], ["Lilimo", "Morten", ""], ["Poland", "Jesse", ""], ["Chapman", "Scott", ""], ["de Solan", "Benoit", ""], ["Baret", "Fr\u00e9d\u00e9ric", ""], ["Stavness", "Ian", ""], ["Guo", "Wei", ""]]}, {"id": "2105.07667", "submitter": "Bin Zhao", "authors": "Bin Zhao, Maoguo Gong, Xuelong Li", "title": "AudioVisual Video Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio and vision are two main modalities in video data. Multimodal learning,\nespecially for audiovisual learning, has drawn considerable attention recently,\nwhich can boost the performance of various computer vision tasks. However, in\nvideo summarization, existing approaches just exploit the visual information\nwhile neglect the audio information. In this paper, we argue that the audio\nmodality can assist vision modality to better understand the video content and\nstructure, and further benefit the summarization process. Motivated by this, we\npropose to jointly exploit the audio and visual information for the video\nsummarization task, and develop an AudioVisual Recurrent Network (AVRN) to\nachieve this. Specifically, the proposed AVRN can be separated into three\nparts: 1) the two-stream LSTM is utilized to encode the audio and visual\nfeature sequentially by capturing their temporal dependency. 2) the audiovisual\nfusion LSTM is employed to fuse the two modalities by exploring the latent\nconsistency between them. 3) the self-attention video encoder is adopted to\ncapture the global dependency in the video. Finally, the fused audiovisual\ninformation, and the integrated temporal and global dependencies are jointly\nused to predict the video summary. Practically, the experimental results on the\ntwo benchmarks, \\emph{i.e.,} SumMe and TVsum, have demonstrated the\neffectiveness of each part, and the superiority of AVRN compared to those\napproaches just exploiting visual information for video summarization.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:36:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhao", "Bin", ""], ["Gong", "Maoguo", ""], ["Li", "Xuelong", ""]]}, {"id": "2105.07672", "submitter": "Chae Eun Lee", "authors": "Chae Eun Lee, Minyoung Chung, Yeong-Gil Shin", "title": "Voxel-level Siamese Representation Learning for Abdominal Multi-Organ\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in medical image segmentation have actively explored various\ndeep learning architectures or objective functions to encode high-level\nfeatures from volumetric data owing to limited image annotations. However, most\nexisting approaches tend to ignore cross-volume global context and define\ncontext relations in the decision space. In this work, we propose a novel\nvoxel-level Siamese representation learning method for abdominal multi-organ\nsegmentation to improve representation space. The proposed method enforces\nvoxel-wise feature relations in the representation space for leveraging limited\ndatasets more comprehensively to achieve better performance. Inspired by recent\nprogress in contrastive learning, we suppressed voxel-wise relations from the\nsame class to be projected to the same point without using negative samples.\nMoreover, we introduce a multi-resolution context aggregation method that\naggregates features from multiple hidden layers, which encodes both the global\nand local contexts for segmentation. Our experiments on the multi-organ dataset\noutperformed the existing approaches by 2% in Dice score coefficient. The\nqualitative visualizations of the representation spaces demonstrate that the\nimprovements were gained primarily by a disentangled feature space.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:42:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lee", "Chae Eun", ""], ["Chung", "Minyoung", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "2105.07673", "submitter": "Bin Zhao", "authors": "Bin Zhao and Xuelong Li", "title": "EA-Net: Edge-Aware Network for Flow-based Video Frame Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation can up-convert the frame rate and enhance the video\nquality. In recent years, although the interpolation performance has achieved\ngreat success, image blur usually occurs at the object boundaries owing to the\nlarge motion. It has been a long-standing problem, and has not been addressed\nyet. In this paper, we propose to reduce the image blur and get the clear shape\nof objects by preserving the edges in the interpolated frames. To this end, the\nproposed Edge-Aware Network (EA-Net) integrates the edge information into the\nframe interpolation task. It follows an end-to-end architecture and can be\nseparated into two stages, \\emph{i.e.}, edge-guided flow estimation and\nedge-protected frame synthesis. Specifically, in the flow estimation stage,\nthree edge-aware mechanisms are developed to emphasize the frame edges in\nestimating flow maps, so that the edge-maps are taken as the auxiliary\ninformation to provide more guidance to boost the flow accuracy. In the frame\nsynthesis stage, the flow refinement module is designed to refine the flow map,\nand the attention module is carried out to adaptively focus on the\nbidirectional flow maps when synthesizing the intermediate frames. Furthermore,\nthe frame and edge discriminators are adopted to conduct the adversarial\ntraining strategy, so as to enhance the reality and clarity of synthesized\nframes. Experiments on three benchmarks, including Vimeo90k, UCF101 for\nsingle-frame interpolation and Adobe240-fps for multi-frame interpolation, have\ndemonstrated the superiority of the proposed EA-Net for the video frame\ninterpolation task.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:44:34 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhao", "Bin", ""], ["Li", "Xuelong", ""]]}, {"id": "2105.07715", "submitter": "Kelei He", "authors": "Kelei He, Wen Ji, Tao Zhou, Zhuoyuan Li, Jing Huo, Xin Zhang, Yang\n  Gao, Dinggang Shen, Bing Zhang, and Junfeng Zhang", "title": "Cross-Modality Brain Tumor Segmentation via Bidirectional\n  Global-to-Local Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate segmentation of brain tumors from multi-modal Magnetic Resonance\n(MR) images is essential in brain tumor diagnosis and treatment. However, due\nto the existence of domain shifts among different modalities, the performance\nof networks decreases dramatically when training on one modality and performing\non another, e.g., train on T1 image while performing on T2 image, which is\noften required in clinical applications. This also prohibits a network from\nbeing trained on labeled data and then transferred to unlabeled data from a\ndifferent domain. To overcome this, unsupervised domain adaptation (UDA)\nmethods provide effective solutions to alleviate the domain shift between\nlabeled source data and unlabeled target data. In this paper, we propose a\nnovel Bidirectional Global-to-Local (BiGL) adaptation framework under a UDA\nscheme. Specifically, a bidirectional image synthesis and segmentation module\nis proposed to segment the brain tumor using the intermediate data\ndistributions generated for the two domains, which includes an image-to-image\ntranslator and a shared-weighted segmentation network. Further, a\nglobal-to-local consistency learning module is proposed to build robust\nrepresentation alignments in an integrated way. Extensive experiments on a\nmulti-modal brain MR benchmark dataset demonstrate that the proposed method\noutperforms several state-of-the-art unsupervised domain adaptation methods by\na large margin, while a comprehensive ablation study validates the\neffectiveness of each key component. The implementation code of our method will\nbe released at \\url{https://github.com/KeleiHe/BiGL}.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 10:11:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["He", "Kelei", ""], ["Ji", "Wen", ""], ["Zhou", "Tao", ""], ["Li", "Zhuoyuan", ""], ["Huo", "Jing", ""], ["Zhang", "Xin", ""], ["Gao", "Yang", ""], ["Shen", "Dinggang", ""], ["Zhang", "Bing", ""], ["Zhang", "Junfeng", ""]]}, {"id": "2105.07751", "submitter": "Ruibo Li", "authors": "Ruibo Li, Guosheng Lin, Tong He, Fayao Liu, Chunhua Shen", "title": "HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs\n  and Position-aware Flow Embedding", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow in 3D point clouds plays an important role in understanding\ndynamic environments. Although significant advances have been made by deep\nneural networks, the performance is far from satisfactory as only per-point\ntranslational motion is considered, neglecting the constraints of the rigid\nmotion in local regions. To address the issue, we propose to introduce the\nmotion consistency to force the smoothness among neighboring points. In\naddition, constraints on the rigidity of the local transformation are also\nadded by sharing unique rigid motion parameters for all points within each\nlocal region. To this end, a high-order CRFs based relation module (Con-HCRFs)\nis deployed to explore both point-wise smoothness and region-wise rigidity. To\nempower the CRFs to have a discriminative unary term, we also introduce a\nposition-aware flow estimation module to be incorporated into the Con-HCRFs.\nComprehensive experiments on FlyingThings3D and KITTI show that our proposed\nframework (HCRF-Flow) achieves state-of-the-art performance and significantly\noutperforms previous approaches substantially.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:53:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Ruibo", ""], ["Lin", "Guosheng", ""], ["He", "Tong", ""], ["Liu", "Fayao", ""], ["Shen", "Chunhua", ""]]}, {"id": "2105.07763", "submitter": "Bill Cassidy", "authors": "Bill Cassidy, Neil D. Reeves, Joseph M. Pappachan, Naseer Ahmad,\n  Samantha Haycocks, David Gillespie, Moi Hoon Yap", "title": "A Cloud-based Deep Learning Framework for Remote Detection of Diabetic\n  Foot Ulcers", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a mobile and cloud-based framework for the automatic\ndetection of diabetic foot ulcers and conducts an investigation of its\nperformance. The system uses a cross-platform mobile framework which enables\nthe deployment of mobile apps to multiple platforms using a single TypeScript\ncode base. A deep convolutional neural network was deployed to a cloud-based\nplatform where the mobile app could send photographs of patient's feet for\ninference to detect the presence of diabetic foot ulcers. The functionality and\nusability of the system were tested in two clinical settings: Salford Royal NHS\nFoundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. The\nbenefits of the system, such as the potential use of the app by patients to\nidentify and monitor their condition are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:15:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cassidy", "Bill", ""], ["Reeves", "Neil D.", ""], ["Pappachan", "Joseph M.", ""], ["Ahmad", "Naseer", ""], ["Haycocks", "Samantha", ""], ["Gillespie", "David", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2105.07789", "submitter": "Lukas Drees", "authors": "Lukas Drees, Laura Verena Junker-Frohn, Jana Kierdorf, Ribana Roscher", "title": "Temporal Prediction and Evaluation of Brassica Growth in the Field using\n  Conditional Generative Adversarial Networks", "comments": "38 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Farmers frequently assess plant growth and performance as basis for making\ndecisions when to take action in the field, such as fertilization, weed\ncontrol, or harvesting. The prediction of plant growth is a major challenge, as\nit is affected by numerous and highly variable environmental factors. This\npaper proposes a novel monitoring approach that comprises high-throughput\nimaging sensor measurements and their automatic analysis to predict future\nplant growth. Our approach's core is a novel machine learning-based growth\nmodel based on conditional generative adversarial networks, which is able to\npredict the future appearance of individual plants. In experiments with RGB\ntime-series images of laboratory-grown Arabidopsis thaliana and field-grown\ncauliflower plants, we show that our approach produces realistic, reliable, and\nreasonable images of future growth stages. The automatic interpretation of the\ngenerated images through neural network-based instance segmentation allows the\nderivation of various phenotypic traits that describe plant growth.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:00:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Drees", "Lukas", ""], ["Junker-Frohn", "Laura Verena", ""], ["Kierdorf", "Jana", ""], ["Roscher", "Ribana", ""]]}, {"id": "2105.07795", "submitter": "Gopi Ramena", "authors": "Rachit S Munjal, Arun D Prabhu, Nikhil Arora, Sukumar Moharana, Gopi\n  Ramena", "title": "STRIDE : Scene Text Recognition In-Device", "comments": "accepted in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Optical Character Recognition (OCR) systems have been widely used in various\napplications for extracting semantic information from images. To give the user\nmore control over their privacy, an on-device solution is needed. The current\nstate-of-the-art models are too heavy and complex to be deployed on-device. We\ndevelop an efficient lightweight scene text recognition (STR) system, which has\nonly 0.88M parameters and performs real-time text recognition. Attention\nmodules tend to boost the accuracy of STR networks but are generally slow and\nnot optimized for device inference. So, we propose the use of convolution\nattention modules to the text recognition networks, which aims to provide\nchannel and spatial attention information to the LSTM module by adding very\nminimal computational cost. It boosts our word accuracy on ICDAR 13 dataset by\nalmost 2\\%. We also introduce a novel orientation classifier module, to support\nthe simultaneous recognition of both horizontal and vertical text. The proposed\nmodel surpasses on-device metrics of inference time and memory footprint and\nachieves comparable accuracy when compared to the leading commercial and other\nopen-source OCR engines. We deploy the system on-device with an inference speed\nof 2.44 ms per word on the Exynos 990 chipset device and achieve an accuracy of\n88.4\\% on ICDAR-13 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:06:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Munjal", "Rachit S", ""], ["Prabhu", "Arun D", ""], ["Arora", "Nikhil", ""], ["Moharana", "Sukumar", ""], ["Ramena", "Gopi", ""]]}, {"id": "2105.07797", "submitter": "Taro Langner", "authors": "Taro Langner, Robin Strand, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Deep regression for uncertainty-aware and interpretable analysis of\n  large-scale body MRI", "comments": "Presented at the Swedish Symposium on Deep Learning 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale medical studies such as the UK Biobank examine thousands of\nvolunteer participants with medical imaging techniques. Combined with the vast\namount of collected metadata, anatomical information from these images has the\npotential for medical analyses at unprecedented scale. However, their\nevaluation often requires manual input and long processing times, limiting the\namount of reference values for biomarkers and other measurements available for\nresearch. Recent approaches with convolutional neural networks for regression\ncan perform these evaluations automatically. On magnetic resonance imaging\n(MRI) data of more than 40,000 UK Biobank subjects, these systems can estimate\nhuman age, body composition and more. This style of analysis is almost entirely\ndata-driven and no manual intervention or guidance with manually segmented\nground truth images is required. The networks often closely emulate the\nreference method that provided their training data and can reach levels of\nagreement comparable to the expected variability between established medical\ngold standard techniques. The risk of silent failure can be individually\nquantified by predictive uncertainty obtained from a mean-variance criterion\nand ensembling. Saliency analysis furthermore enables an interpretation of the\nunderlying relevant image features and showed that the networks learned to\ncorrectly target specific organs, limbs, and regions of interest.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:12:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Langner", "Taro", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2105.07800", "submitter": "Lin Wu", "authors": "Lin Wu, Teng Wang, Changyin Sun", "title": "Multi-modal Visual Place Recognition in Dynamics-Invariant Perception\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual place recognition is one of the essential and challenging problems in\nthe fields of robotics. In this letter, we for the first time explore the use\nof multi-modal fusion of semantic and visual modalities in dynamics-invariant\nspace to improve place recognition in dynamic environments. We achieve this by\nfirst designing a novel deep learning architecture to generate the static\nsemantic segmentation and recover the static image directly from the\ncorresponding dynamic image. We then innovatively leverage the\nspatial-pyramid-matching model to encode the static semantic segmentation into\nfeature vectors. In parallel, the static image is encoded using the popular\nBag-of-words model. On the basis of the above multi-modal features, we finally\nmeasure the similarity between the query image and target landmark by the joint\nsimilarity of their semantic and visual codes. Extensive experiments\ndemonstrate the effectiveness and robustness of the proposed approach for place\nrecognition in dynamic environments.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:14:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Teng", ""], ["Sun", "Changyin", ""]]}, {"id": "2105.07809", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Cheng-Ming Chiang, Hsien-Kai Kuo, Anastasia Sycheva,\n  Radu Timofte, Min-Hung Chen, Man-Yu Lee, Yu-Syuan Xu, Yu Tseng, Shusong Xu,\n  Jin Guo, Chao-Hung Chen, Ming-Chun Hsyu, Wen-Chia Tsai, Chao-Wei Chen,\n  Grigory Malivenko, Minsu Kwon, Myungje Lee, Jaeyoon Yoo, Changbeom Kang,\n  Shinjo Wang, Zheng Shaolong, Hao Dejun, Xie Fen, Feng Zhuang, Yipeng Ma,\n  Jingyang Peng, Tao Wang, Fenglong Song, Chih-Chung Hsu, Kwan-Lin Chen,\n  Mei-Hsuang Wu, Vishal Chudasama, Kalpesh Prajapati, Heena Patel, Anjali\n  Sarvaiya, Kishor Upla, Kiran Raja, Raghavendra Ramachandra, Christoph Busch,\n  Etienne de Stoutz", "title": "Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021\n  Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the quality of mobile cameras starts to play a crucial role in modern\nsmartphones, more and more attention is now being paid to ISP algorithms used\nto improve various perceptual aspects of mobile photos. In this Mobile AI\nchallenge, the target was to develop an end-to-end deep learning-based image\nsignal processing (ISP) pipeline that can replace classical hand-crafted ISPs\nand achieve nearly real-time performance on smartphone NPUs. For this, the\nparticipants were provided with a novel learned ISP dataset consisting of\nRAW-RGB image pairs captured with the Sony IMX586 Quad Bayer mobile sensor and\na professional 102-megapixel medium format camera. The runtime of all models\nwas evaluated on the MediaTek Dimensity 1000+ platform with a dedicated AI\nprocessing unit capable of accelerating both floating-point and quantized\nneural networks. The proposed solutions are fully compatible with the above NPU\nand are capable of processing Full HD photos under 60-100 milliseconds while\nachieving high fidelity results. A detailed description of all models developed\nin this challenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:20:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ignatov", "Andrey", ""], ["Chiang", "Cheng-Ming", ""], ["Kuo", "Hsien-Kai", ""], ["Sycheva", "Anastasia", ""], ["Timofte", "Radu", ""], ["Chen", "Min-Hung", ""], ["Lee", "Man-Yu", ""], ["Xu", "Yu-Syuan", ""], ["Tseng", "Yu", ""], ["Xu", "Shusong", ""], ["Guo", "Jin", ""], ["Chen", "Chao-Hung", ""], ["Hsyu", "Ming-Chun", ""], ["Tsai", "Wen-Chia", ""], ["Chen", "Chao-Wei", ""], ["Malivenko", "Grigory", ""], ["Kwon", "Minsu", ""], ["Lee", "Myungje", ""], ["Yoo", "Jaeyoon", ""], ["Kang", "Changbeom", ""], ["Wang", "Shinjo", ""], ["Shaolong", "Zheng", ""], ["Dejun", "Hao", ""], ["Fen", "Xie", ""], ["Zhuang", "Feng", ""], ["Ma", "Yipeng", ""], ["Peng", "Jingyang", ""], ["Wang", "Tao", ""], ["Song", "Fenglong", ""], ["Hsu", "Chih-Chung", ""], ["Chen", "Kwan-Lin", ""], ["Wu", "Mei-Hsuang", ""], ["Chudasama", "Vishal", ""], ["Prajapati", "Kalpesh", ""], ["Patel", "Heena", ""], ["Sarvaiya", "Anjali", ""], ["Upla", "Kishor", ""], ["Raja", "Kiran", ""], ["Ramachandra", "Raghavendra", ""], ["Busch", "Christoph", ""], ["de Stoutz", "Etienne", ""]]}, {"id": "2105.07825", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Radu Timofte, Maurizio Denna, Abdel Younes, Andrew\n  Lek, Mustafa Ayazoglu, Jie Liu, Zongcai Du, Jiaming Guo, Xueyi Zhou, Hao Jia,\n  Youliang Yan, Zexin Zhang, Yixin Chen, Yunbo Peng, Yue Lin, Xindong Zhang,\n  Hui Zeng, Kun Zeng, Peirong Li, Zhihuang Liu, Shiqi Xue, Shengpeng Wang", "title": "Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI\n  2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution is one of the most popular computer vision problems\nwith many important applications to mobile devices. While many solutions have\nbeen proposed for this task, they are usually not optimized even for common\nsmartphone AI hardware, not to mention more constrained smart TV platforms that\nare often supporting INT8 inference only. To address this problem, we introduce\nthe first Mobile AI challenge, where the target is to develop an end-to-end\ndeep learning-based image super-resolution solutions that can demonstrate a\nreal-time performance on mobile or edge NPUs. For this, the participants were\nprovided with the DIV2K dataset and trained quantized models to do an efficient\n3X image upscaling. The runtime of all models was evaluated on the Synaptics\nVS680 Smart Home board with a dedicated NPU capable of accelerating quantized\nneural networks. The proposed solutions are fully compatible with all major\nmobile AI accelerators and are capable of reconstructing Full HD images under\n40-60 ms while achieving high fidelity results. A detailed description of all\nmodels developed in the challenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:34:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Denna", "Maurizio", ""], ["Younes", "Abdel", ""], ["Lek", "Andrew", ""], ["Ayazoglu", "Mustafa", ""], ["Liu", "Jie", ""], ["Du", "Zongcai", ""], ["Guo", "Jiaming", ""], ["Zhou", "Xueyi", ""], ["Jia", "Hao", ""], ["Yan", "Youliang", ""], ["Zhang", "Zexin", ""], ["Chen", "Yixin", ""], ["Peng", "Yunbo", ""], ["Lin", "Yue", ""], ["Zhang", "Xindong", ""], ["Zeng", "Hui", ""], ["Zeng", "Kun", ""], ["Li", "Peirong", ""], ["Liu", "Zhihuang", ""], ["Xue", "Shiqi", ""], ["Wang", "Shengpeng", ""]]}, {"id": "2105.07830", "submitter": "Suman Saha", "authors": "Suman Saha, Anton Obukhov, Danda Pani Paudel, Menelaos Kanakis, Yuhua\n  Chen, Stamatios Georgoulis, Luc Van Gool", "title": "Learning to Relate Depth and Semantics for Unsupervised Domain\n  Adaptation", "comments": "Accepted at CVPR 2021; updated results according to the released\n  source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an approach for encoding visual task relationships to improve\nmodel performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic\nsegmentation and monocular depth estimation are shown to be complementary\ntasks; in a multi-task learning setting, a proper encoding of their\nrelationships can further improve performance on both tasks. Motivated by this\nobservation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes\ntask dependencies between the semantic and depth predictions. To capture the\ncross-task relationships, we propose a neural network architecture that\ncontains task-specific and cross-task refinement heads. Furthermore, we propose\nan Iterative Self-Learning (ISL) training scheme, which exploits semantic\npseudo-labels to provide extra supervision on the target domain. We\nexperimentally observe improvements in both tasks' performance because the\ncomplementary information present in these tasks is better captured.\nSpecifically, we show that: (1) our approach improves performance on all tasks\nwhen they are complementary and mutually dependent; (2) the CTRL helps to\nimprove both semantic segmentation and depth estimation tasks performance in\nthe challenging UDA setting; (3) the proposed ISL training scheme further\nimproves the semantic segmentation performance. The implementation is available\nat https://github.com/susaha/ctrl-uda.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:42:09 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 09:27:00 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Saha", "Suman", ""], ["Obukhov", "Anton", ""], ["Paudel", "Danda Pani", ""], ["Kanakis", "Menelaos", ""], ["Chen", "Yuhua", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2105.07869", "submitter": "Andrey Ignatov", "authors": "Angeline Pouget, Sidharth Ramesh, Maximilian Giang, Ramithan\n  Chandrapalan, Toni Tanner, Moritz Prussing, Radu Timofte, Andrey Ignatov", "title": "Fast and Accurate Camera Scene Detection on Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-powered automatic camera scene detection mode is nowadays available in\nnearly any modern smartphone, though the problem of accurate scene prediction\nhas not yet been addressed by the research community. This paper for the first\ntime carefully defines this problem and proposes a novel Camera Scene Detection\nDataset (CamSDD) containing more than 11K manually crawled images belonging to\n30 different scene categories. We propose an efficient and NPU-friendly CNN\nmodel for this task that demonstrates a top-3 accuracy of 99.5% on this dataset\nand achieves more than 200 FPS on the recent mobile SoCs. An additional\nin-the-wild evaluation of the obtained solution is performed to analyze its\nperformance and limitation in the real-world scenarios. The dataset and\npre-trained models used in this paper are available on the project website.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:06:21 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Pouget", "Angeline", ""], ["Ramesh", "Sidharth", ""], ["Giang", "Maximilian", ""], ["Chandrapalan", "Ramithan", ""], ["Tanner", "Toni", ""], ["Prussing", "Moritz", ""], ["Timofte", "Radu", ""], ["Ignatov", "Andrey", ""]]}, {"id": "2105.07878", "submitter": "Gargi Joshi Miss", "authors": "Gargi Joshi, Rahee Walambe, Ketan Kotecha", "title": "A Review on Explainability in Multimodal Deep Neural Nets", "comments": "24 pages 6 figures", "journal-ref": "in IEEE Access, vol. 9, pp. 59800-59821, 2021", "doi": "10.1109/ACCESS.2021.3070212.", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence techniques powered by deep neural nets have achieved\nmuch success in several application domains, most significantly and notably in\nthe Computer Vision applications and Natural Language Processing tasks.\nSurpassing human-level performance propelled the research in the applications\nwhere different modalities amongst language, vision, sensory, text play an\nimportant role in accurate predictions and identification. Several multimodal\nfusion methods employing deep learning models are proposed in the literature.\nDespite their outstanding performance, the complex, opaque and black-box nature\nof the deep neural nets limits their social acceptance and usability. This has\ngiven rise to the quest for model interpretability and explainability, more so\nin the complex tasks involving multimodal AI methods. This paper extensively\nreviews the present literature to present a comprehensive survey and commentary\non the explainability in multimodal deep neural nets, especially for the vision\nand language tasks. Several topics on multimodal AI and its applications for\ngeneric domains have been covered in this paper, including the significance,\ndatasets, fundamental building blocks of the methods and techniques,\nchallenges, applications, and future trends in this domain\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:17:49 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 11:53:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Joshi", "Gargi", ""], ["Walambe", "Rahee", ""], ["Kotecha", "Ketan", ""]]}, {"id": "2105.07901", "submitter": "Nanyang Yang", "authors": "Nanyang Yang, Yi Wang and Lap-Pui Chau", "title": "Multi-object Tracking with Tracked Object Bounding Box Association", "comments": "6 pages, accepted paper at ICME workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CenterTrack tracking algorithm achieves state-of-the-art tracking\nperformance using a simple detection model and single-frame spatial offsets to\nlocalize objects and predict their associations in a single network. However,\nthis joint detection and tracking method still suffers from high identity\nswitches due to the inferior association method. To reduce the high number of\nidentity switches and improve the tracking accuracy, in this paper, we propose\nto incorporate a simple tracked object bounding box and overlapping prediction\nbased on the current frame onto the CenterTrack algorithm. Specifically, we\npropose an Intersection over Union (IOU) distance cost matrix in the\nassociation step instead of simple point displacement distance. We evaluate our\nproposed tracker on the MOT17 test dataset, showing that our proposed method\ncan reduce identity switches significantly by 22.6% and obtain a notable\nimprovement of 1.5% in IDF1 compared to the original CenterTrack's under the\nsame tracklet lifetime. The source code is released at\nhttps://github.com/Nanyangny/CenterTrack-IOU.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:32:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Nanyang", ""], ["Wang", "Yi", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2105.07914", "submitter": "Yan Bai", "authors": "Weiquan Huang, Yan Bai, Qiuyu Ren, Xinbo Zhao, Ming Feng and Yin Wang", "title": "Large-Scale Unsupervised Person Re-Identification with Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing public person Re-Identification~(ReID) datasets are small in modern\nterms because of labeling difficulty. Although unlabeled surveillance video is\nabundant and relatively easy to obtain, it is unclear how to leverage these\nfootage to learn meaningful ReID representations. In particular, most existing\nunsupervised and domain adaptation ReID methods utilize only the public\ndatasets in their experiments, with labels removed. In addition, due to small\ndata sizes, these methods usually rely on fine tuning by the unlabeled training\ndata in the testing domain to achieve good performance. Inspired by the recent\nprogress of large-scale self-supervised image classification using contrastive\nlearning, we propose to learn ReID representation from large-scale unlabeled\nsurveillance video alone. Assisted by off-the-shelf pedestrian detection tools,\nwe apply the contrastive loss at both the image and the tracklet levels.\nTogether with a principal component analysis step using camera labels freely\navailable, our evaluation using a large-scale unlabeled dataset shows far\nsuperior performance among unsupervised methods that do not use any training\ndata in the testing domain. Furthermore, the accuracy improves with the data\nsize and therefore our method has great potential with even larger and more\ndiversified datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:55:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Huang", "Weiquan", ""], ["Bai", "Yan", ""], ["Ren", "Qiuyu", ""], ["Zhao", "Xinbo", ""], ["Feng", "Ming", ""], ["Wang", "Yin", ""]]}, {"id": "2105.07917", "submitter": "Giulia Cisotto", "authors": "Alberto Zancanaro, Giulia Cisotto, Jo\\~ao Ruivo Paulo, Gabriel Pires,\n  and Urbano J. Nunes", "title": "CNN-based Approaches For Cross-Subject Classification in Motor Imagery:\n  From The State-of-The-Art to DynamicNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery (MI)-based brain-computer interface (BCI) systems are being\nincreasingly employed to provide alternative means of communication and control\nfor people suffering from neuro-motor impairments, with a special effort to\nbring these systems out of the controlled lab environments. Hence, accurately\nclassifying MI from brain signals, e.g., from electroencephalography (EEG), is\nessential to obtain reliable BCI systems. However, MI classification is still a\nchallenging task, because the signals are characterized by poor SNR, high\nintra-subject and cross-subject variability. Deep learning approaches have\nstarted to emerge as valid alternatives to standard machine learning\ntechniques, e.g., filter bank common spatial pattern (FBCSP), to extract\nsubject-independent features and to increase the cross-subject classification\nperformance of MI BCI systems. In this paper, we first present a review of the\nmost recent studies using deep learning for MI classification, with particular\nattention to their cross-subject performance. Second, we propose DynamicNet, a\nPython-based tool for quick and flexible implementations of deep learning\nmodels based on convolutional neural networks. We show-case the potentiality of\nDynamicNet by implementing EEGNet, a well-established architecture for\neffective EEG classification. Finally, we compare its performance with FBCSP in\na 4-class MI classification over public datasets. To explore its cross-subject\nclassification ability, we applied three different cross-validation schemes.\nFrom our results, we demonstrate that DynamicNet-implemented EEGNet outperforms\nFBCSP by about 25%, with a statistically significant difference when\ncross-subject validation schemes are applied.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:57:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zancanaro", "Alberto", ""], ["Cisotto", "Giulia", ""], ["Paulo", "Jo\u00e3o Ruivo", ""], ["Pires", "Gabriel", ""], ["Nunes", "Urbano J.", ""]]}, {"id": "2105.07921", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul, Arne de Wall, Tristan Kreuziger, Filipe Marcelino, Hugo\n  Costa, Pedro Benevides, M\\'ario Caetano, Beg\\\"um Demir, Volker Markl", "title": "BigEarthNet-MM: A Large Scale Multi-Modal Multi-Label Benchmark Archive\n  for Remote Sensing Image Classification and Retrieval", "comments": "Accepted at the IEEE Geoscience and Remote Sensing Magazine. Our code\n  is available online at\n  https://git.tu-berlin.de/rsim/BigEarthNet-MM_19-classes_models. arXiv admin\n  note: substantial text overlap with arXiv:2001.06372", "journal-ref": null, "doi": "10.1109/MGRS.2021.3089174", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the multi-modal BigEarthNet (BigEarthNet-MM) benchmark\narchive made up of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches to\nsupport the deep learning (DL) studies in multi-modal multi-label remote\nsensing (RS) image retrieval and classification. Each pair of patches in\nBigEarthNet-MM is annotated with multi-labels provided by the CORINE Land Cover\n(CLC) map of 2018 based on its thematically most detailed Level-3 class\nnomenclature. Our initial research demonstrates that some CLC classes are\nchallenging to be accurately described by only considering (single-date)\nBigEarthNet-MM images. In this paper, we also introduce an alternative\nclass-nomenclature as an evolution of the original CLC labels to address this\nproblem. This is achieved by interpreting and arranging the CLC Level-3\nnomenclature based on the properties of BigEarthNet-MM images in a new\nnomenclature of 19 classes. In our experiments, we show the potential of\nBigEarthNet-MM for multi-modal multi-label image retrieval and classification\nproblems by considering several state-of-the-art DL models. We also demonstrate\nthat the DL models trained from scratch on BigEarthNet-MM outperform those\npre-trained on ImageNet, especially in relation to some complex classes,\nincluding agriculture and other vegetated and natural environments. We make all\nthe data and the DL models publicly available at https://bigearth.net, offering\nan important resource to support studies on multi-modal image scene\nclassification and retrieval problems in RS.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:00:31 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 15:11:47 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Sumbul", "Gencer", ""], ["de Wall", "Arne", ""], ["Kreuziger", "Tristan", ""], ["Marcelino", "Filipe", ""], ["Costa", "Hugo", ""], ["Benevides", "Pedro", ""], ["Caetano", "M\u00e1rio", ""], ["Demir", "Beg\u00fcm", ""], ["Markl", "Volker", ""]]}, {"id": "2105.07926", "submitter": "YueFeng Chen", "authors": "Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai\n  Ye, Yuan He, Hui Xue", "title": "Towards Robust Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on Vision Transformer (ViT) and its improved variants have\nshown that self-attention-based networks surpass traditional Convolutional\nNeural Networks (CNNs) in most vision tasks. However, existing ViTs focus on\nthe standard accuracy and computation cost, lacking the investigation of the\nintrinsic influence on model robustness and generalization. In this work, we\nconduct systematic evaluation on components of ViTs in terms of their impact on\nrobustness to adversarial examples, common corruptions and distribution shifts.\nWe find some components can be harmful to robustness. By using and combining\nrobust components as building blocks of ViTs, we propose Robust Vision\nTransformer (RVT), which is a new vision transformer and has superior\nperformance with strong robustness. We further propose two new plug-and-play\ntechniques called position-aware attention scaling and patch-wise augmentation\nto augment our RVT, which we abbreviate as RVT*. The experimental results on\nImageNet and six robustness benchmarks show the advanced robustness and\ngeneralization ability of RVT compared with previous ViTs and state-of-the-art\nCNNs. Furthermore, RVT-S* also achieves Top-1 rank on multiple robustness\nleaderboards including ImageNet-C and ImageNet-Sketch. The code will be\navailable at \\url{https://git.io/Jswdk}.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:04:15 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 15:02:55 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 09:01:06 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mao", "Xiaofeng", ""], ["Qi", "Gege", ""], ["Chen", "Yuefeng", ""], ["Li", "Xiaodan", ""], ["Duan", "Ranjie", ""], ["Ye", "Shaokai", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "2105.07930", "submitter": "Senthil Yogamani", "authors": "Michal Uricar, Ganesh Sistu, Lucie Yahiaoui and Senthil Yogamani", "title": "Ensemble-based Semi-supervised Learning to Improve Noisy Soiling\n  Annotations in Autonomous Driving", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual annotation of soiling on surround view cameras is a very challenging\nand expensive task. The unclear boundary for various soiling categories like\nwater drops or mud particles usually results in a large variance in the\nannotation quality. As a result, the models trained on such poorly annotated\ndata are far from being optimal. In this paper, we focus on handling such noisy\nannotations via pseudo-label driven ensemble model which allow us to quickly\nspot problematic annotations and in most cases also sufficiently fixing them.\nWe train a soiling segmentation model on both noisy and refined labels and\ndemonstrate significant improvements using the refined annotations. It also\nillustrates that it is possible to effectively refine lower cost coarse\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:10:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 17:49:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Uricar", "Michal", ""], ["Sistu", "Ganesh", ""], ["Yahiaoui", "Lucie", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2105.07961", "submitter": "Alan Wang", "authors": "Alan Q. Wang, Aaron K. LaViolette, Leo Moon, Chris Xu, and Mert R.\n  Sabuncu", "title": "Joint Optimization of Hadamard Sensing and Reconstruction in Compressed\n  Sensing Fluorescence Microscopy", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby\nless measurements are collected during sensing and reconstruction is performed\nto recover the image. Much work has gone into optimizing the sensing and\nreconstruction portions separately. We propose a method of jointly optimizing\nboth sensing and reconstruction end-to-end under a total measurement\nconstraint, enabling learning of the optimal sensing scheme concurrently with\nthe parameters of a neural network-based reconstruction network. We train our\nmodel on a rich dataset of confocal, two-photon, and wide-field microscopy\nimages comprising of a variety of biological samples. We show that our method\noutperforms several baseline sensing schemes and a regularized regression\nreconstruction algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:42:28 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 01:09:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Alan Q.", ""], ["LaViolette", "Aaron K.", ""], ["Moon", "Leo", ""], ["Xu", "Chris", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2105.07962", "submitter": "Hritam Basak", "authors": "Hritam Basak, Rukhshanda Hussain, Ajay Rana", "title": "DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI\n  Segmentation", "comments": "Submitted at SN Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid increment of morbidity of brain stroke in the last few years have\nbeen a driving force towards fast and accurate segmentation of stroke lesions\nfrom brain MRI images. With the recent development of deep-learning,\ncomputer-aided and segmentation methods of ischemic stroke lesions have been\nuseful for clinicians in early diagnosis and treatment planning. However, most\nof these methods suffer from inaccurate and unreliable segmentation results\nbecause of their inability to capture sufficient contextual features from the\nMRI volumes. To meet these requirements, 3D convolutional neural networks have\nbeen proposed, which, however, suffer from huge computational requirements. To\nmitigate these problems, we propose a novel Dimension Fusion Edge-guided\nnetwork (DFENet) that can meet both of these requirements by fusing the\nfeatures of 2D and 3D CNNs. Unlike other methods, our proposed network uses a\nparallel partial decoder (PPD) module for aggregating and upsampling selected\nfeatures, rich in important contextual information. Additionally, we use an\nedge-guidance and enhanced mixing loss for constantly supervising and\nimprovising the learning process of the network. The proposed method is\nevaluated on publicly available Anatomical Tracings of Lesions After Stroke\n(ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of\n0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to\nother state-of-the-art methods, outperforms them by a significant margin.\nTherefore, the proposed model is robust, accurate, superior to the existing\nmethods, and can be relied upon for biomedical applications.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:43:59 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 19:24:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Basak", "Hritam", ""], ["Hussain", "Rukhshanda", ""], ["Rana", "Ajay", ""]]}, {"id": "2105.07983", "submitter": "Ayantha Randika Ponnamperuma Arachchige", "authors": "Ayantha Randika, Nilanjan Ray, Xiao Xiao, Allegra Latimer", "title": "Unknown-box Approximation to Improve Optical Character Recognition\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical character recognition (OCR) is a widely used pattern recognition\napplication in numerous domains. There are several feature-rich,\ngeneral-purpose OCR solutions available for consumers, which can provide\nmoderate to excellent accuracy levels. However, accuracy can diminish with\ndifficult and uncommon document domains. Preprocessing of document images can\nbe used to minimize the effect of domain shift. In this paper, a novel approach\nis presented for creating a customized preprocessor for a given OCR engine.\nUnlike the previous OCR agnostic preprocessing techniques, the proposed\napproach approximates the gradient of a particular OCR engine to train a\npreprocessor module. Experiments with two datasets and two OCR engines show\nthat the presented preprocessor is able to improve the accuracy of the OCR up\nto 46% from the baseline by applying pixel-level manipulations to the document\nimage. The implementation of the proposed method and the enhanced public\ndatasets are available for download.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:09:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Randika", "Ayantha", ""], ["Ray", "Nilanjan", ""], ["Xiao", "Xiao", ""], ["Latimer", "Allegra", ""]]}, {"id": "2105.07986", "submitter": "David Montero", "authors": "J. Javier Yebes, David Montero, Ignacio Arriola", "title": "Learning to Automatically Catch Potholes in Worldwide Road Scene Images", "comments": "in IEEE Intelligent Transportation Systems Magazine", "journal-ref": null, "doi": "10.1109/MITS.2019.2926370", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Among several road hazards that are present in any paved way in the world,\npotholes are one of the most annoying and also involving higher maintenance\ncosts. There exists an increasing interest on the automated detection of these\nhazards enabled by technological and research progress. Our research work\ntackled the challenge of pothole detection from images of real world road\nscenes. The main novelty resides on the application of the latest progress in\nAI to learn the visual appearance of potholes. We built a large dataset of\nimages with pothole annotations. They contained road scenes from different\ncities in the world, taken with different cameras, vehicles and viewpoints\nunder varied environmental conditions. Then, we fine-tuned four different\nobject detection models based on Faster R-CNN and SSD deep neural networks. We\nachieved high average precision and the pothole detector was tested on the\nNvidia DrivePX2 platform with GPGPU capability, which can be embedded on\nvehicles. Moreover, it was deployed on a real vehicle to notify the detected\npotholes to a given IoT platform as part of AUTOPILOT H2020 project.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:10:58 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 07:15:56 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yebes", "J. Javier", ""], ["Montero", "David", ""], ["Arriola", "Ignacio", ""]]}, {"id": "2105.08016", "submitter": "Ge Zhang", "authors": "Ge Zhang, Or Litany, Srinath Sridhar, Leonidas Guibas", "title": "StrobeNet: Category-Level Multiview Reconstruction of Articulated\n  Objects", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present StrobeNet, a method for category-level 3D reconstruction of\narticulating objects from one or more unposed RGB images. Reconstructing\ngeneral articulating object categories % has important applications, but is\nchallenging since objects can have wide variation in shape, articulation,\nappearance and topology. We address this by building on the idea of\ncategory-level articulation canonicalization -- mapping observations to a\ncanonical articulation which enables correspondence-free multiview aggregation.\nOur end-to-end trainable neural network estimates feature-enriched canonical 3D\npoint clouds, articulation joints, and part segmentation from one or more\nunposed images of an object. These intermediate estimates are used to generate\na final implicit 3D reconstruction.Our approach reconstructs objects even when\nthey are observed in different articulations in images with large baselines,\nand animation of reconstructed shapes. Quantitative and qualitative evaluations\non different object categories show that our method is able to achieve high\nreconstruction accuracy, especially as more views are added.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:05:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Ge", ""], ["Litany", "Or", ""], ["Sridhar", "Srinath", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2105.08040", "submitter": "Burhaneddin Yaman", "authors": "Mehmet Ak\\c{c}akaya, Burhaneddin Yaman, Hyungjin Chung, Jong Chul Ye", "title": "Unsupervised Deep Learning Methods for Biological Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches have become the main research frontier for\nbiological image reconstruction problems thanks to their high performance,\nalong with their ultra-fast reconstruction times. However, due to the\ndifficulty of obtaining matched reference data for supervised learning, there\nhas been increasing interest in unsupervised learning approaches that do not\nneed paired reference data. In particular, self-supervised learning and\ngenerative models have been successfully used for various biological imaging\napplications. In this paper, we overview these approaches from a coherent\nperspective in the context of classical inverse problems, and discuss their\napplications to biological imaging.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:43:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ak\u00e7akaya", "Mehmet", ""], ["Yaman", "Burhaneddin", ""], ["Chung", "Hyungjin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2105.08050", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le", "title": "Pay Attention to MLPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have become one of the most important architectural innovations\nin deep learning and have enabled many breakthroughs over the past few years.\nHere we propose a simple network architecture, gMLP, based on MLPs with gating,\nand show that it can perform as well as Transformers in key language and vision\napplications. Our comparisons show that self-attention is not critical for\nVision Transformers, as gMLP can achieve the same accuracy. For BERT, our model\nachieves parity with Transformers on pretraining perplexity and is better on\nsome downstream NLP tasks. On finetuning tasks where gMLP performs worse,\nmaking the gMLP model substantially larger can close the gap with Transformers.\nIn general, our experiments show that gMLP can scale as well as Transformers\nover increased data and compute.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:55:04 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 20:24:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Hanxiao", ""], ["Dai", "Zihang", ""], ["So", "David R.", ""], ["Le", "Quoc V.", ""]]}, {"id": "2105.08051", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Brian Curless, Ira Kemelmacher-Shlizerman, Steve\n  Seitz", "title": "A Light Stage on Every Desk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every time you sit in front of a TV or monitor, your face is actively\nilluminated by time-varying patterns of light. This paper proposes to use this\ntime-varying illumination for synthetic relighting of your face with any new\nillumination condition. In doing so, we take inspiration from the light stage\nwork of Debevec et al., who first demonstrated the ability to relight people\ncaptured in a controlled lighting environment. Whereas existing light stages\nrequire expensive, room-scale spherical capture gantries and exist in only a\nfew labs in the world, we demonstrate how to acquire useful data from a normal\nTV or desktop monitor. Instead of subjecting the user to uncomfortable rapidly\nflashing light patterns, we operate on images of the user watching a YouTube\nvideo or other standard content. We train a deep network on images plus monitor\npatterns of a given user and learn to predict images of that user under any\ntarget illumination (monitor pattern). Experimental evaluation shows that our\nmethod produces realistic relighting results. Video results are available at\nhttp://grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:56:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Curless", "Brian", ""], ["Kemelmacher-Shlizerman", "Ira", ""], ["Seitz", "Steve", ""]]}, {"id": "2105.08052", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Mia Chiquier, Hod Lipson, Carl Vondrick", "title": "The Boombox: Visual Reconstruction from Acoustic Vibrations", "comments": "Website: boombox.cs.columbia.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce The Boombox, a container that uses acoustic vibrations to\nreconstruct an image of its inside contents. When an object interacts with the\ncontainer, they produce small acoustic vibrations. The exact vibration\ncharacteristics depend on the physical properties of the box and the object. We\ndemonstrate how to use this incidental signal in order to predict visual\nstructure. After learning, our approach remains effective even when a camera\ncannot view inside the box. Although we use low-cost and low-power contact\nmicrophones to detect the vibrations, our results show that learning from\nmulti-modal data enables us to transform cheap acoustic sensors into rich\nvisual sensors. Due to the ubiquity of containers, we believe integrating\nperception capabilities into them will enable new applications in\nhuman-computer interaction and robotics. Our project website is at:\nboombox.cs.columbia.edu\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:58:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Boyuan", ""], ["Chiquier", "Mia", ""], ["Lipson", "Hod", ""], ["Vondrick", "Carl", ""]]}, {"id": "2105.08054", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Olivier J. Henaff, Aaron van den Oord", "title": "Divide and Contrast: Self-supervised Learning from Uncurated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning holds promise in leveraging large amounts of\nunlabeled data, however much of its progress has thus far been limited to\nhighly curated pre-training data such as ImageNet. We explore the effects of\ncontrastive learning from larger, less-curated image datasets such as YFCC, and\nfind there is indeed a large difference in the resulting representation\nquality. We hypothesize that this curation gap is due to a shift in the\ndistribution of image classes -- which is more diverse and heavy-tailed --\nresulting in less relevant negative samples to learn from. We test this\nhypothesis with a new approach, Divide and Contrast (DnC), which alternates\nbetween contrastive learning and clustering-based hard negative mining. When\npretrained on less curated datasets, DnC greatly improves the performance of\nself-supervised learning on downstream tasks, while remaining competitive with\nthe current state-of-the-art on curated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:59:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tian", "Yonglong", ""], ["Henaff", "Olivier J.", ""], ["Oord", "Aaron van den", ""]]}, {"id": "2105.08058", "submitter": "Francesco Guzzi", "authors": "Francesco Guzzi, George Kourousias, Fulvio Bill\\`e, Roberto Pugliese,\n  Alessandra Gianoncelli and Sergio Carrato", "title": "A parameter refinement method for Ptychography based on Deep Learning\n  concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  X-ray Ptychography is an advanced computational microscopy technique which is\ndelivering exceptionally detailed quantitative imaging of biological and\nnanotechnology specimens. However coarse parametrisation in propagation\ndistance, position errors and partial coherence frequently menaces the\nexperiment viability. In this work we formally introduced these actors, solving\nthe whole reconstruction as an optimisation problem. A modern Deep Learning\nframework is used to correct autonomously the setup incoherences, thus\nimproving the quality of a ptychography reconstruction. Automatic procedures\nare indeed crucial to reduce the time for a reliable analysis, which has a\nsignificant impact on all the fields that use this kind of microscopy. We\nimplemented our algorithm in our software framework, SciComPty, releasing it as\nopen-source. We tested our system on both synthetic datasets and also on real\ndata acquired at the TwinMic beamline of the Elettra synchrotron facility.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 10:15:17 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Guzzi", "Francesco", ""], ["Kourousias", "George", ""], ["Bill\u00e8", "Fulvio", ""], ["Pugliese", "Roberto", ""], ["Gianoncelli", "Alessandra", ""], ["Carrato", "Sergio", ""]]}, {"id": "2105.08059", "submitter": "Salman Ul Hassan Dar", "authors": "Yilmaz Korkmaz, Salman UH Dar, Mahmut Yurt, Muzaffer \\\"Ozbey, Tolga\n  \\c{C}ukur", "title": "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning has swiftly become a workhorse for accelerated MRI\nin recent years, offering state-of-the-art performance in image reconstruction\nfrom undersampled acquisitions. Training deep supervised models requires large\ndatasets of undersampled and fully-sampled acquisitions typically from a\nmatching set of subjects. Given scarce access to large medical datasets, this\nlimitation has sparked interest in unsupervised methods that reduce reliance on\nfully-sampled ground-truth data. A common framework is based on the deep image\nprior, where network-driven regularization is enforced directly during\ninference on undersampled acquisitions. Yet, canonical convolutional\narchitectures are suboptimal in capturing long-range relationships, and\nrandomly initialized networks may hamper convergence. To address these\nlimitations, here we introduce a novel unsupervised MRI reconstruction method\nbased on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a\ndeep adversarial network with cross-attention transformer blocks to map noise\nand latent variables onto MR images. This unconditional network learns a\nhigh-quality MRI prior in a self-supervised encoding task. A zero-shot\nreconstruction is performed on undersampled test data, where inference is\nperformed by optimizing network parameters, latent and noise variables to\nensure maximal consistency to multi-coil MRI data. Comprehensive experiments on\nbrain MRI datasets clearly demonstrate the superior performance of SLATER\nagainst several state-of-the-art unsupervised methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 02:01:21 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:37:20 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Korkmaz", "Yilmaz", ""], ["Dar", "Salman UH", ""], ["Yurt", "Mahmut", ""], ["\u00d6zbey", "Muzaffer", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2105.08127", "submitter": "Luke Melas-Kyriazi", "authors": "Luke Melas-Kyriazi and Christian Rupprecht and Iro Laina and Andrea\n  Vedaldi", "title": "Finding an Unsupervised Image Segmenter in Each of Your Deep Generative\n  Models", "comments": "Project page and GitHub link:\n  https://lukemelas.github.io/unsupervised-image-segmentation &\n  https://github.com/lukemelas/unsupervised-image-segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that numerous human-interpretable directions exist\nin the latent space of GANs. In this paper, we develop an automatic procedure\nfor finding directions that lead to foreground-background image separation, and\nwe use these directions to train an image segmentation model without human\nsupervision. Our method is generator-agnostic, producing strong segmentation\nresults with a wide range of different GAN architectures. Furthermore, by\nleveraging GANs pretrained on large datasets such as ImageNet, we are able to\nsegment images from a range of domains without further training or finetuning.\nEvaluating our method on image segmentation benchmarks, we compare favorably to\nprior work while using neither human supervision nor access to the training\ndata. Broadly, our results demonstrate that automatically extracting\nforeground-background structure from pretrained deep generative models can\nserve as a remarkably effective substitute for human supervision.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:34:24 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Melas-Kyriazi", "Luke", ""], ["Rupprecht", "Christian", ""], ["Laina", "Iro", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2105.08128", "submitter": "Luke Melas-Kyriazi", "authors": "Luke Melas-Kyriazi and Arjun K. Manrai", "title": "PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency\n  Training", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is a promising technique for semantic\nsegmentation and other computer vision tasks for which large-scale data\nannotation is costly and time-consuming. In semantic segmentation, it is\nattractive to train models on annotated images from a simulated (source) domain\nand deploy them on real (target) domains. In this work, we present a novel\nframework for unsupervised domain adaptation based on the notion of\ntarget-domain consistency training. Intuitively, our work is based on the idea\nthat in order to perform well on the target domain, a model's output should be\nconsistent with respect to small perturbations of inputs in the target domain.\nSpecifically, we introduce a new loss term to enforce pixelwise consistency\nbetween the model's predictions on a target image and a perturbed version of\nthe same image. In comparison to popular adversarial adaptation methods, our\napproach is simpler, easier to implement, and more memory-efficient during\ntraining. Experiments and extensive ablation studies demonstrate that our\nsimple approach achieves remarkably strong results on two challenging\nsynthetic-to-real benchmarks, GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes.\n  Code is available at: https://github.com/lukemelas/pixmatch\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:36:28 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Melas-Kyriazi", "Luke", ""], ["Manrai", "Arjun K.", ""]]}, {"id": "2105.08141", "submitter": "Srijan Das", "authors": "Srijan Das, Rui Dai, Di Yang, Francois Bremond", "title": "VPN++: Rethinking Video-Pose embeddings for understanding Activities of\n  Daily Living", "comments": "submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many attempts have been made towards combining RGB and 3D poses for the\nrecognition of Activities of Daily Living (ADL). ADL may look very similar and\noften necessitate to model fine-grained details to distinguish them. Because\nthe recent 3D ConvNets are too rigid to capture the subtle visual patterns\nacross an action, this research direction is dominated by methods combining RGB\nand 3D Poses. But the cost of computing 3D poses from RGB stream is high in the\nabsence of appropriate sensors. This limits the usage of aforementioned\napproaches in real-world applications requiring low latency. Then, how to best\ntake advantage of 3D Poses for recognizing ADL? To this end, we propose an\nextension of a pose driven attention mechanism: Video-Pose Network (VPN),\nexploring two distinct directions. One is to transfer the Pose knowledge into\nRGB through a feature-level distillation and the other towards mimicking pose\ndriven attention through an attention-level distillation. Finally, these two\napproaches are integrated into a single model, we call VPN++. We show that\nVPN++ is not only effective but also provides a high speed up and high\nresilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the\nrepresentative baselines on 4 public datasets. Code is available at\nhttps://github.com/srijandas07/vpnplusplus.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:19:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Das", "Srijan", ""], ["Dai", "Rui", ""], ["Yang", "Di", ""], ["Bremond", "Francois", ""]]}, {"id": "2105.08147", "submitter": "Vignav Ramesh", "authors": "Vignav Ramesh, Blaine Rister, Daniel L. Rubin", "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN\n  on Chest X-rays Automatically Computed from Volumetric CTs", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:27:32 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 00:36:21 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ramesh", "Vignav", ""], ["Rister", "Blaine", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "2105.08149", "submitter": "Xiaochen Yang", "authors": "Xiaoxu Li, Xiaochen Yang, Zhanyu Ma, Jing-Hao Xue", "title": "Deep Metric Learning for Few-Shot Image Classification: A Selective\n  Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot image classification is a challenging problem which aims to achieve\nthe human level of recognition based only on a small number of images. Deep\nlearning algorithms such as meta-learning, transfer learning, and metric\nlearning have been employed recently and achieved the state-of-the-art\nperformance. In this survey, we review representative deep metric learning\nmethods for few-shot classification, and categorize them into three groups\naccording to the major problems and novelties they focus on. We conclude this\nreview with a discussion on current challenges and future trends in few-shot\nimage classification.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:27:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Xiaoxu", ""], ["Yang", "Xiaochen", ""], ["Ma", "Zhanyu", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "2105.08157", "submitter": "Eric Chen", "authors": "Eric Z. Chen, Xiao Chen, Jingyuan Lyu, Qi Liu, Zhongqi Zhang, Yu Ding,\n  Shuheng Zhang, Terrence Chen, Jian Xu, and Shanhui Sun", "title": "Cardiac Functional Analysis with Cine MRI via Deep Learning\n  Reconstruction", "comments": "Presented at ISMRM 2021 as the digital poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrospectively gated cine (retro-cine) MRI is the clinical standard for\ncardiac functional analysis. Deep learning (DL) based methods have been\nproposed for the reconstruction of highly undersampled MRI data and show\nsuperior image quality and magnitude faster reconstruction time than CS-based\nmethods. Nevertheless, it remains unclear whether DL reconstruction is suitable\nfor cardiac function analysis. To address this question, in this study we\nevaluate and compare the cardiac functional values (EDV, ESV and EF for LV and\nRV, respectively) obtained from highly accelerated MRI acquisition using DL\nbased reconstruction algorithm (DL-cine) with values from CS-cine and\nconventional retro-cine. To the best of our knowledge, this is the first work\nto evaluate the cine MRI with deep learning reconstruction for cardiac function\nanalysis and compare it with other conventional methods. The cardiac functional\nvalues obtained from cine MRI with deep learning reconstruction are consistent\nwith values from clinical standard retro-cine MRI.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:53:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Eric Z.", ""], ["Chen", "Xiao", ""], ["Lyu", "Jingyuan", ""], ["Liu", "Qi", ""], ["Zhang", "Zhongqi", ""], ["Ding", "Yu", ""], ["Zhang", "Shuheng", ""], ["Chen", "Terrence", ""], ["Xu", "Jian", ""], ["Sun", "Shanhui", ""]]}, {"id": "2105.08175", "submitter": "Guang Yang A", "authors": "Jun Lv, Guangyuan Li, Xiangrong Tong, Weibo Chen, Jiahao Huang,\n  Chengyan Wang, Guang Yang", "title": "Transfer Learning Enhanced Generative Adversarial Networks for\n  Multi-Channel MRI Reconstruction", "comments": "29 pages, 11 figures, accepted by CBM journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based generative adversarial networks (GAN) can effectively\nperform image reconstruction with under-sampled MR data. In general, a large\nnumber of training samples are required to improve the reconstruction\nperformance of a certain model. However, in real clinical applications, it is\ndifficult to obtain tens of thousands of raw patient data to train the model\nsince saving k-space data is not in the routine clinical flow. Therefore,\nenhancing the generalizability of a network based on small samples is urgently\nneeded. In this study, three novel applications were explored based on parallel\nimaging combined with the GAN model (PI-GAN) and transfer learning. The model\nwas pre-trained with public Calgary brain images and then fine-tuned for use in\n(1) patients with tumors in our center; (2) different anatomies, including knee\nand liver; (3) different k-space sampling masks with acceleration factors (AFs)\nof 2 and 6. As for the brain tumor dataset, the transfer learning results could\nremove the artifacts found in PI-GAN and yield smoother brain edges. The\ntransfer learning results for the knee and liver were superior to those of the\nPI-GAN model trained with its own dataset using a smaller number of training\ncases. However, the learning procedure converged more slowly in the knee\ndatasets compared to the learning in the brain tumor datasets. The\nreconstruction performance was improved by transfer learning both in the models\nwith AFs of 2 and 6. Of these two models, the one with AF=2 showed better\nresults. The results also showed that transfer learning with the pre-trained\nmodel could solve the problem of inconsistency between the training and test\ndatasets and facilitate generalization to unseen data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:28:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lv", "Jun", ""], ["Li", "Guangyuan", ""], ["Tong", "Xiangrong", ""], ["Chen", "Weibo", ""], ["Huang", "Jiahao", ""], ["Wang", "Chengyan", ""], ["Yang", "Guang", ""]]}, {"id": "2105.08190", "submitter": "Athanasios Efthymiou", "authors": "Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Marcel Worring,\n  Nachoem Wijnberg", "title": "Graph Neural Networks for Knowledge Enhanced Visual Representation of\n  Paintings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose ArtSAGENet, a novel multimodal architecture that integrates Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), to jointly\nlearn visual and semantic-based artistic representations. First, we illustrate\nthe significant advantages of multi-task learning for fine art analysis and\nargue that it is conceptually a much more appropriate setting in the fine art\ndomain than the single-task alternatives. We further demonstrate that several\nGNN architectures can outperform strong CNN baselines in a range of fine art\nanalysis tasks, such as style classification, artist attribution, creation\nperiod estimation, and tag prediction, while training them requires an order of\nmagnitude less computational time and only a small amount of labeled data.\nFinally, through extensive experimentation we show that our proposed ArtSAGENet\ncaptures and encodes valuable relational dependencies between the artists and\nthe artworks, surpassing the performance of traditional methods that rely\nsolely on the analysis of visual content. Our findings underline a great\npotential of integrating visual content and semantics for fine art analysis and\ncuration.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:05:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Efthymiou", "Athanasios", ""], ["Rudinac", "Stevan", ""], ["Kackovic", "Monika", ""], ["Worring", "Marcel", ""], ["Wijnberg", "Nachoem", ""]]}, {"id": "2105.08194", "submitter": "Brian Davis", "authors": "Brian Davis, Bryan Morse\u007f, Brian Price\u007f, Chris Tensmeyer\u007f, Curtis\n  Wiginton", "title": "Visual FUDGE: Form Understanding via Dynamic Graph Editing", "comments": "Accepted at ICDAR 2021, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of form understanding: finding text entities and the\nrelationships/links between them in form images. The proposed FUDGE model\nformulates this problem on a graph of text elements (the vertices) and uses a\nGraph Convolutional Network to predict changes to the graph. The initial\nvertices are detected text lines and do not necessarily correspond to the final\ntext entities, which can span multiple lines. Also, initial edges contain many\nfalse-positive relationships. FUDGE edits the graph structure by combining text\nsegments (graph vertices) and pruning edges in an iterative fashion to obtain\nthe final text entities and relationships. While recent work in this area has\nfocused on leveraging large-scale pre-trained Language Models (LM), FUDGE\nachieves almost the same level of entity linking performance on the FUNSD\ndataset by learning only visual features from the (small) provided training\nset. FUDGE can be applied on forms where text recognition is difficult (e.g.\ndegraded or historical forms) and on forms in resource-poor languages where\npre-training such LMs is challenging. FUDGE is state-of-the-art on the\nhistorical NAF dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:18:39 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 17:50:23 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Davis", "Brian", ""], ["Morse\u007f", "Bryan", ""], ["Price\u007f", "Brian", ""], ["Tensmeyer\u007f", "Chris", ""], ["Wiginton", "Curtis", ""]]}, {"id": "2105.08196", "submitter": "Aditya Vaidya", "authors": "Akarsh Kumar (1), Aditya R. Vaidya (1), Alexander G. Huth (1) ((1) The\n  University of Texas at Austin)", "title": "Physically Plausible Pose Refinement using Fully Differentiable Forces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  All hand-object interaction is controlled by forces that the two bodies exert\non each other, but little work has been done in modeling these underlying\nforces when doing pose and contact estimation from RGB/RGB-D data. Given the\npose of the hand and object from any pose estimation system, we propose an\nend-to-end differentiable model that refines pose estimates by learning the\nforces experienced by the object at each vertex in its mesh. By matching the\nlearned net force to an estimate of net force based on finite differences of\nposition, this model is able to find forces that accurately describe the\nmovement of the object, while resolving issues like mesh interpenetration and\nlack of contact. Evaluating on the ContactPose dataset, we show this model\nsuccessfully corrects poses and finds contact maps that better match the ground\ntruth, despite not using any RGB or depth image data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:33:04 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Kumar", "Akarsh", ""], ["Vaidya", "Aditya R.", ""], ["Huth", "Alexander G.", ""]]}, {"id": "2105.08199", "submitter": "Wadii Boulila Prof.", "authors": "Safa Ben Atitallah, Maha Driss, Wadii Boulila, Henda Ben Gh\\'ezala", "title": "Randomly Initialized Convolutional Neural Network for the Recognition of\n  COVID-19 using X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By the start of 2020, the novel coronavirus disease (COVID-19) has been\ndeclared a worldwide pandemic. Because of the severity of this infectious\ndisease, several kinds of research have focused on combatting its ongoing\nspread. One potential solution to detect COVID-19 is by analyzing the chest\nX-ray images using Deep Learning (DL) models. In this context, Convolutional\nNeural Networks (CNNs) are presented as efficient techniques for early\ndiagnosis. In this study, we propose a novel randomly initialized CNN\narchitecture for the recognition of COVID-19. This network consists of a set of\ndifferent-sized hidden layers created from scratch. The performance of this\nnetwork is evaluated through two public datasets, which are the COVIDx and the\nenhanced COVID-19 datasets. Both of these datasets consist of 3 different\nclasses of images: COVID19, pneumonia, and normal chest X-ray images. The\nproposed CNN model yields encouraging results with 94% and 99% of accuracy for\nCOVIDx and enhanced COVID-19 dataset, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:40:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Atitallah", "Safa Ben", ""], ["Driss", "Maha", ""], ["Boulila", "Wadii", ""], ["Gh\u00e9zala", "Henda Ben", ""]]}, {"id": "2105.08205", "submitter": "Sidi Lu", "authors": "Sidi Lu, Xin Yuan, Aggelos K Katsaggelos, Weisong Shi", "title": "Reinforcement Learning for Adaptive Video Compressive Sensing", "comments": "12 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply reinforcement learning to video compressive sensing to adapt the\ncompression ratio. Specifically, video snapshot compressive imaging (SCI),\nwhich captures high-speed video using a low-speed camera is considered in this\nwork, in which multiple (B) video frames can be reconstructed from a snapshot\nmeasurement. One research gap in previous studies is how to adapt B in the\nvideo SCI system for different scenes. In this paper, we fill this gap\nutilizing reinforcement learning (RL). An RL model, as well as various\nconvolutional neural networks for reconstruction, are learned to achieve\nadaptive sensing of video SCI systems. Furthermore, the performance of an\nobject detection network using directly the video SCI measurements without\nreconstruction is also used to perform RL-based adaptive video compressive\nsensing. Our proposed adaptive SCI method can thus be implemented in low cost\nand real time. Our work takes the technology one step further towards real\napplications of video SCI.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 00:01:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lu", "Sidi", ""], ["Yuan", "Xin", ""], ["Katsaggelos", "Aggelos K", ""], ["Shi", "Weisong", ""]]}, {"id": "2105.08222", "submitter": "Chen Zhang", "authors": "Chen Zhang, Yinghao Xu, Yujun Shen", "title": "Decorating Your Own Bedroom: Locally Controlling Image Generation with\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have made great success in\nsynthesizing high-quality images. However, how to steer the generation process\nof a well-trained GAN model and customize the output image is much less\nexplored. It has been recently found that modulating the input latent code used\nin GANs can reasonably alter some variation factors in the output image, but\nsuch manipulation usually presents to change the entire image as a whole. In\nthis work, we propose an effective approach, termed as LoGAN, to support local\nediting of the output image. Concretely, we introduce two operators, i.e.,\ncontent modulation and style modulation, together with a priority mask to\nfacilitate the precise control of the intermediate generative features. Taking\nbedroom synthesis as an instance, we are able to seamlessly remove, insert,\nshift, and rotate the individual objects inside a room. Furthermore, our method\ncan completely clear out a room and then refurnish it with customized furniture\nand styles. Experimental results show the great potentials of steering the\nimage generation of pre-trained GANs for versatile image editing.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 01:31:49 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhang", "Chen", ""], ["Xu", "Yinghao", ""], ["Shen", "Yujun", ""]]}, {"id": "2105.08229", "submitter": "Myron Brown", "authors": "Gordon Christie, Kevin Foster, Shea Hagstrom, Gregory D. Hager, Myron\n  Z. Brown", "title": "Single View Geocentric Pose in the Wild", "comments": "To be published in the proceedings of the CVPR 2021 EarthVision\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for Earth observation tasks such as semantic mapping, map\nalignment, and change detection rely on near-nadir images; however, often the\nfirst available images in response to dynamic world events such as natural\ndisasters are oblique. These tasks are much more difficult for oblique images\ndue to observed object parallax. There has been recent success in learning to\nregress geocentric pose, defined as height above ground and orientation with\nrespect to gravity, by training with airborne lidar registered to satellite\nimages. We present a model for this novel task that exploits affine invariance\nproperties to outperform state of the art performance by a wide margin. We also\naddress practical issues required to deploy this method in the wild for\nreal-world applications. Our data and code are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 01:55:15 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Christie", "Gordon", ""], ["Foster", "Kevin", ""], ["Hagstrom", "Shea", ""], ["Hager", "Gregory D.", ""], ["Brown", "Myron Z.", ""]]}, {"id": "2105.08237", "submitter": "Conghui Hu", "authors": "Conghui Hu, Yongxin Yang, Yunpeng Li, Timothy M. Hospedales, Yi-Zhe\n  Song", "title": "Towards Unsupervised Sketch-based Image Retrieval", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current supervised sketch-based image retrieval (SBIR) methods achieve\nexcellent performance. However, the cost of data collection and labeling\nimposes an intractable barrier to practical deployment of real applications. In\nthis paper, we present the first attempt at unsupervised SBIR to remove the\nlabeling cost (category annotations and sketch-photo pairings) that is\nconventionally needed for training. Existing single-domain unsupervised\nrepresentation learning methods perform poorly in this application, due to the\nunique cross-domain (sketch and photo) nature of the problem. We therefore\nintroduce a novel framework that simultaneously performs unsupervised\nrepresentation learning and sketch-photo domain alignment. Technically this is\nunderpinned by exploiting joint distribution optimal transport (JDOT) to align\ndata from different domains during representation learning, which we extend\nwith trainable cluster prototypes and feature memory banks to further improve\nscalability and efficacy. Extensive experiments show that our framework\nachieves excellent performance in the new unsupervised setting, and performs\ncomparably or better than state-of-the-art in the zero-shot setting.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 02:38:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Hu", "Conghui", ""], ["Yang", "Yongxin", ""], ["Li", "Yunpeng", ""], ["Hospedales", "Timothy M.", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2105.08248", "submitter": "Ruibo Li", "authors": "Ruibo Li, Guosheng Lin, Lihua Xie", "title": "Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds\n  with Optimal Transport and Random Walk", "comments": "Accepted to CVPR2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the scarcity of annotated scene flow data, self-supervised scene flow\nlearning in point clouds has attracted increasing attention. In the\nself-supervised manner, establishing correspondences between two point clouds\nto approximate scene flow is an effective approach. Previous methods often\nobtain correspondences by applying point-wise matching that only takes the\ndistance on 3D point coordinates into account, introducing two critical issues:\n(1) it overlooks other discriminative measures, such as color and surface\nnormal, which often bring fruitful clues for accurate matching; and (2) it\noften generates sub-par performance, as the matching is operated in an\nunconstrained situation, where multiple points can be ended up with the same\ncorresponding point. To address the issues, we formulate this matching task as\nan optimal transport problem. The output optimal assignment matrix can be\nutilized to guide the generation of pseudo ground truth. In this optimal\ntransport, we design the transport cost by considering multiple descriptors and\nencourage one-to-one matching by mass equality constraints. Also, constructing\na graph on the points, a random walk module is introduced to encourage the\nlocal consistency of the pseudo labels. Comprehensive experiments on\nFlyingThings3D and KITTI show that our method achieves state-of-the-art\nperformance among self-supervised learning methods. Our self-supervised method\neven performs on par with some supervised learning approaches, although we do\nnot need any ground truth flow for training.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 03:12:42 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Ruibo", ""], ["Lin", "Guosheng", ""], ["Xie", "Lihua", ""]]}, {"id": "2105.08252", "submitter": "Bofeng Wu", "authors": "Bofeng Wu, Guocheng Niu, Jun Yu, Xinyan Xiao, Jian Zhang and Hua Wu", "title": "Weakly Supervised Dense Video Captioning via Jointly Usage of Knowledge\n  Distillation and Cross-modal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to Dense Video Captioning (DVC) without\npairwise event-sentence annotation. First, we adopt the knowledge distilled\nfrom relevant and well solved tasks to generate high-quality event proposals.\nThen we incorporate contrastive loss and cycle-consistency loss typically\napplied to cross-modal retrieval tasks to build semantic matching between the\nproposals and sentences, which are eventually used to train the caption\ngeneration module. In addition, the parameters of matching module are\ninitialized via pre-training based on annotated images to improve the matching\nperformance. Extensive experiments on ActivityNet-Caption dataset reveal the\nsignificance of distillation-based event proposal generation and cross-modal\nretrieval-based semantic matching to weakly supervised DVC, and demonstrate the\nsuperiority of our method to existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 03:21:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wu", "Bofeng", ""], ["Niu", "Guocheng", ""], ["Yu", "Jun", ""], ["Xiao", "Xinyan", ""], ["Zhang", "Jian", ""], ["Wu", "Hua", ""]]}, {"id": "2105.08253", "submitter": "Ryota Yoshihashi", "authors": "Ryota Yoshihashi, Rei Kawakami, Shaodi You, Tu Tuan Trinh, Makoto\n  Iida, Takeshi Naemura", "title": "Finding a Needle in a Haystack: Tiny Flying Object Detection in 4K\n  Videos using a Joint Detection-and-Tracking Approach", "comments": "arXiv admin note: text overlap with arXiv:1709.04666", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting tiny objects in a high-resolution video is challenging because the\nvisual information is little and unreliable. Specifically, the challenge\nincludes very low resolution of the objects, MPEG artifacts due to compression\nand a large searching area with many hard negatives. Tracking is equally\ndifficult because of the unreliable appearance, and the unreliable motion\nestimation. Luckily, we found that by combining this two challenging tasks\ntogether, there will be mutual benefits. Following the idea, in this paper, we\npresent a neural network model called the Recurrent Correlational Network,\nwhere detection and tracking are jointly performed over a multi-frame\nrepresentation learned through a single, trainable, and end-to-end network. The\nframework exploits a convolutional long short-term memory network for learning\ninformative appearance changes for detection, while the learned representation\nis shared in tracking for enhancing its performance. In experiments with\ndatasets containing images of scenes with small flying objects, such as birds\nand unmanned aerial vehicles, the proposed method yielded consistent\nimprovements in detection performance over deep single-frame detectors and\nexisting motion-based detectors. Furthermore, our network performs as well as\nstate-of-the-art generic object trackers when it was evaluated as a tracker on\na bird image dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 03:22:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yoshihashi", "Ryota", ""], ["Kawakami", "Rei", ""], ["You", "Shaodi", ""], ["Trinh", "Tu Tuan", ""], ["Iida", "Makoto", ""], ["Naemura", "Takeshi", ""]]}, {"id": "2105.08267", "submitter": "Tianchen Wang", "authors": "Tianchen Wang, Zhihe Li, Meiping Huang, Jian Zhuang, Shanshan Bi,\n  Jiawei Zhang, Yiyu Shi, Hongwen Fei, Xiaowei Xu", "title": "EchoCP: An Echocardiography Dataset in Contrast Transthoracic\n  Echocardiography for Patent Foramen Ovale Diagnosis", "comments": "Accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patent foramen ovale (PFO) is a potential separation between the septum,\nprimum and septum secundum located in the anterosuperior portion of the atrial\nseptum. PFO is one of the main factors causing cryptogenic stroke which is the\nfifth leading cause of death in the United States. For PFO diagnosis, contrast\ntransthoracic echocardiography (cTTE) is preferred as being a more robust\nmethod compared with others. However, the current PFO diagnosis through cTTE is\nextremely slow as it is proceeded manually by sonographers on echocardiography\nvideos. Currently there is no publicly available dataset for this important\ntopic in the community. In this paper, we present EchoCP, as the first\nechocardiography dataset in cTTE targeting PFO diagnosis.\n  EchoCP consists of 30 patients with both rest and Valsalva maneuver videos\nwhich covers various PFO grades. We further establish an automated baseline\nmethod for PFO diagnosis based on the state-of-the-art cardiac chamber\nsegmentation technique, which achieves 0.89 average mean Dice score, but only\n0.70/0.67 mean accuracies for PFO diagnosis, leaving large room for\nimprovement. We hope that the challenging EchoCP dataset can stimulate further\nresearch and lead to innovative and generic solutions that would have an impact\nin multiple domains. Our dataset is released.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:24:53 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wang", "Tianchen", ""], ["Li", "Zhihe", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""], ["Bi", "Shanshan", ""], ["Zhang", "Jiawei", ""], ["Shi", "Yiyu", ""], ["Fei", "Hongwen", ""], ["Xu", "Xiaowei", ""]]}, {"id": "2105.08269", "submitter": "Felix Juefei-Xu", "authors": "Qing Guo, Felix Juefei-Xu, Changqing Zhou, Yang Liu, Song Wang", "title": "Sparta: Spatially Attentive and Adversarially Robust Activation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is one of the most effective ways for improving the\nrobustness of deep convolution neural networks (CNNs). Just like common network\ntraining, the effectiveness of AT relies on the design of basic network\ncomponents. In this paper, we conduct an in-depth study on the role of the\nbasic ReLU activation component in AT for robust CNNs. We find that the\nspatially-shared and input-independent properties of ReLU activation make CNNs\nless robust to white-box adversarial attacks with either standard or\nadversarial training. To address this problem, we extend ReLU to a novel Sparta\nactivation function (Spatially attentive and Adversarially Robust Activation),\nwhich enables CNNs to achieve both higher robustness, i.e., lower error rate on\nadversarial examples, and higher accuracy, i.e., lower error rate on clean\nexamples, than the existing state-of-the-art (SOTA) activation functions. We\nfurther study the relationship between Sparta and the SOTA activation\nfunctions, providing more insights about the advantages of our method. With\ncomprehensive experiments, we also find that the proposed method exhibits\nsuperior cross-CNN and cross-dataset transferability. For the former, the\nadversarially trained Sparta function for one CNN (e.g., ResNet-18) can be\nfixed and directly used to train another adversarially robust CNN (e.g.,\nResNet-34). For the latter, the Sparta function trained on one dataset (e.g.,\nCIFAR-10) can be employed to train adversarially robust CNNs on another dataset\n(e.g., SVHN). In both cases, Sparta leads to CNNs with higher robustness than\nthe vanilla ReLU, verifying the flexibility and versatility of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:36:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Zhou", "Changqing", ""], ["Liu", "Yang", ""], ["Wang", "Song", ""]]}, {"id": "2105.08276", "submitter": "Junbin Xiao", "authors": "Junbin Xiao, Xindi Shang, Angela Yao and Tat-Seng Chua", "title": "NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions", "comments": "To appear at CVPR2021.(Receive one 'Strong Accept'. Review comments:\n  The benchmark will be beneficial for an important video understanding\n  problem. The analysis is comprehensive and provides meaningful insights.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce NExT-QA, a rigorously designed video question answering\n(VideoQA) benchmark to advance video understanding from describing to\nexplaining the temporal actions. Based on the dataset, we set up multi-choice\nand open-ended QA tasks targeting causal action reasoning, temporal action\nreasoning, and common scene comprehension. Through extensive analysis of\nbaselines and established VideoQA techniques, we find that top-performing\nmethods excel at shallow scene descriptions but are weak in causal and temporal\naction reasoning. Furthermore, the models that are effective on multi-choice\nQA, when adapted to open-ended QA, still struggle in generalizing the answers.\nThis raises doubt on the ability of these models to reason and highlights\npossibilities for improvement. With detailed results for different question\ntypes and heuristic observations for future works, we hope NExT-QA will guide\nthe next generation of VQA research to go beyond superficial scene description\ntowards a deeper understanding of videos. (The dataset and related resources\nare available at https://github.com/doc-doc/NExT-QA.git)\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:56:46 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 09:37:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xiao", "Junbin", ""], ["Shang", "Xindi", ""], ["Yao", "Angela", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2105.08286", "submitter": "Jinming Su", "authors": "Jinming Su, Changqun Xia, and Jia Li", "title": "Exploring Driving-aware Salient Object Detection via Knowledge Transfer", "comments": "Accepted by ICME 2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, general salient object detection (SOD) has made great progress with\nthe rapid development of deep neural networks. However, task-aware SOD has\nhardly been studied due to the lack of task-specific datasets. In this paper,\nwe construct a driving task-oriented dataset where pixel-level masks of salient\nobjects have been annotated. Comparing with general SOD datasets, we find that\nthe cross-domain knowledge difference and task-specific scene gap are two main\nchallenges to focus the salient objects when driving. Inspired by these\nfindings, we proposed a baseline model for the driving task-aware SOD via a\nknowledge transfer convolutional neural network. In this network, we construct\nan attentionbased knowledge transfer module to make up the knowledge\ndifference. In addition, an efficient boundary-aware feature decoding module is\nintroduced to perform fine feature decoding for objects in the complex\ntask-specific scenes. The whole network integrates the knowledge transfer and\nfeature decoding modules in a progressive manner. Experiments show that the\nproposed dataset is very challenging, and the proposed method outperforms 12\nstate-of-the-art methods on the dataset, which facilitates the development of\ntask-aware SOD.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 05:24:21 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Su", "Jinming", ""], ["Xia", "Changqun", ""], ["Li", "Jia", ""]]}, {"id": "2105.08336", "submitter": "Jaedong Hwang", "authors": "Jaedong Hwang, Seoung Wug Oh, Joon-Young Lee, Bohyung Han", "title": "Exemplar-Based Open-Set Panoptic Segmentation Network", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend panoptic segmentation to the open-world and introduce an open-set\npanoptic segmentation (OPS) task. This task requires performing panoptic\nsegmentation for not only known classes but also unknown ones that have not\nbeen acknowledged during training. We investigate the practical challenges of\nthe task and construct a benchmark on top of an existing dataset, COCO. In\naddition, we propose a novel exemplar-based open-set panoptic segmentation\nnetwork (EOPSN) inspired by exemplar theory. Our approach identifies a new\nclass based on exemplars, which are identified by clustering and employed as\npseudo-ground-truths. The size of each class increases by mining new exemplars\nbased on the similarities to the existing ones associated with the class. We\nevaluate EOPSN on the proposed benchmark and demonstrate the effectiveness of\nour proposals. The primary goal of our work is to draw the attention of the\ncommunity to the recognition in the open-world scenarios. The implementation of\nour algorithm is available on the project webpage:\nhttps://cv.snu.ac.kr/research/EOPSN.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 07:59:21 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 00:38:26 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Hwang", "Jaedong", ""], ["Oh", "Seoung Wug", ""], ["Lee", "Joon-Young", ""], ["Han", "Bohyung", ""]]}, {"id": "2105.08382", "submitter": "Mahyar Bolhassani", "authors": "Mahyar Bolhassani", "title": "Transfer learning approach to Classify the X-ray image that corresponds\n  to corona disease Using ResNet50 pretrained by ChexNet", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus adversely has affected people worldwide. There are common\nsymptoms between the Covid19 virus disease and other respiratory diseases like\npneumonia or Influenza. Therefore, diagnosing it fast is crucial not only to\nsave patients but also to prevent it from spreading. One of the most reliant\nmethods of diagnosis is through X-ray images of a lung. With the help of deep\nlearning approaches, we can teach the deep model to learn the condition of an\naffected lung. Therefore, it can classify the new sample as if it is a Covid19\ninfected patient or not. In this project, we train a deep model based on\nResNet50 pretrained by ImageNet dataset and CheXNet dataset. Based on the\nimbalanced CoronaHack Chest X-Ray dataset introducing by Kaggle we applied both\nbinary and multi-class classification. Also, we compare the results when using\nFocal loss and Cross entropy loss.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:12:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bolhassani", "Mahyar", ""]]}, {"id": "2105.08383", "submitter": "Chuhui Xue", "authors": "Chuhui Xue, Shijian Lu, Song Bai, Wenqing Zhang, Changhu Wang", "title": "I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Leveraging the advances of natural language processing, most recent scene\ntext recognizers adopt an encoder-decoder architecture where text images are\nfirst converted to representative features and then a sequence of characters\nvia `direct decoding'. However, scene text images suffer from rich noises of\ndifferent sources such as complex background and geometric distortions which\noften confuse the decoder and lead to incorrect alignment of visual features at\nnoisy decoding time steps. This paper presents I2C2W, a novel scene text\nrecognizer that is accurate and tolerant to various noises in scenes. I2C2W\nconsists of an image-to-character module (I2C) and a character-to-word module\n(C2W) which are complementary and can be trained end-to-end. I2C detects\ncharacters and predicts their relative positions in a word. It strives to\ndetect all possible characters including incorrect and redundant ones based on\ndifferent alignments of visual features without the restriction of time steps.\nTaking the detected characters as input, C2W learns from character semantics\nand their positions to filter out incorrect and redundant detection and produce\nthe final word recognition. Extensive experiments over seven public datasets\nshow that I2C2W achieves superior recognition performances and outperforms the\nstate-of-the-art by large margins on challenging irregular scene text datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:20:58 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Xue", "Chuhui", ""], ["Lu", "Shijian", ""], ["Bai", "Song", ""], ["Zhang", "Wenqing", ""], ["Wang", "Changhu", ""]]}, {"id": "2105.08416", "submitter": "Iv\\'an Garc\\'ia Aguilar", "authors": "Iv\\'an Garc\\'ia, Rafael Marcos Luque and Ezequiel L\\'opez", "title": "Improved detection of small objects in road network sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vast number of existing IP cameras in current road networks is an\nopportunity to take advantage of the captured data and analyze the video and\ndetect any significant events. For this purpose, it is necessary to detect\nmoving vehicles, a task that was carried out using classical artificial vision\ntechniques until a few years ago. Nowadays, significant improvements have been\nobtained by deep learning networks. Still, object detection is considered one\nof the leading open issues within computer vision.\n  The current scenario is constantly evolving, and new models and techniques\nare appearing trying to improve this field. In particular, new problems and\ndrawbacks appear regarding detecting small objects, which correspond mainly to\nthe vehicles that appear in the road scenes. All this means that new solutions\nthat try to improve the low detection rate of small elements are essential.\nAmong the different emerging research lines, this work focuses on the detection\nof small objects. In particular, our proposal aims to vehicle detection from\nimages captured by video surveillance cameras.\n  In this work, we propose a new procedure for detecting small-scale objects by\napplying super-resolution processes based on detections performed by\nconvolutional neural networks \\emph{(CNN)}. The neural network is integrated\nwith processes that are in charge of increasing the resolution of the images to\nimprove the object detection performance. This solution has been tested for a\nset of traffic images containing elements of different scales to test the\nefficiency according to the detections obtained by the model, thus\ndemonstrating that our proposal achieves good results in a wide range of\nsituations.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 10:13:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Garc\u00eda", "Iv\u00e1n", ""], ["Luque", "Rafael Marcos", ""], ["L\u00f3pez", "Ezequiel", ""]]}, {"id": "2105.08447", "submitter": "Parastoo Akbari Moghaddam", "authors": "Parastoo Akbari, Atefeh Ziaei, and Hamed Azarnoush", "title": "Deep Active Contours Using Locally Controlled Distance Vector Flow", "comments": "22 pages with 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active contours Model (ACM) has been extensively used in computer vision and\nimage processing. In recent studies, Convolutional Neural Networks (CNNs) have\nbeen combined with active contours replacing the user in the process of contour\nevolution and image segmentation to eliminate limitations associated with ACM's\ndependence on parameters of the energy functional and initialization. However,\nprior works did not aim for automatic initialization which is addressed here.\nIn addition to manual initialization, current methods are highly sensitive to\ninitial location and fail to delineate borders accurately. We propose a fully\nautomatic image segmentation method to address problems of manual\ninitialization, insufficient capture range, and poor convergence to boundaries,\nin addition to the problem of assignment of energy functional parameters. We\ntrain two CNNs, which predict active contour weighting parameters and generate\na ground truth mask to extract Distance Transform (DT) and an initialization\ncircle. Distance transform is used to form a vector field pointing from each\npixel of the image towards the closest point on the boundary, the size of which\nis equal to the Euclidean distance map. We evaluate our method on four publicly\navailable datasets including two building instance segmentation datasets,\nVaihingen and Bing huts, and two mammography image datasets, INBreast and\nDDSM-BCRP. Our approach outperforms latest research by 0.59 ans 2.39 percent in\nmean Intersection-over-Union (mIoU), 7.38 and 8.62 percent in Boundary F-score\n(BoundF) for Vaihingen and Bing huts datasets, respectively. Dice similarity\ncoefficient for the INBreast and DDSM-BCRP datasets is 94.23% and 90.89%,\nrespectively indicating our method is comparable to state-of-the-art\nframeworks.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:38:01 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Akbari", "Parastoo", ""], ["Ziaei", "Atefeh", ""], ["Azarnoush", "Hamed", ""]]}, {"id": "2105.08463", "submitter": "Ankush Panwar", "authors": "Ankush Panwar, Pratyush Singh, Suman Saha, Danda Pani Paudel and Luc\n  Van Gool", "title": "Unsupervised Compound Domain Adaptation for Face Anti-Spoofing", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of face anti-spoofing which aims to make the face\nverification systems robust in the real world settings. The context of\ndetecting live vs. spoofed face images may differ significantly in the target\ndomain, when compared to that of labeled source domain where the model is\ntrained. Such difference may be caused due to new and unknown spoof types,\nillumination conditions, scene backgrounds, among many others. These varieties\nof differences make the target a compound domain, thus calling for the problem\nof the unsupervised compound domain adaptation. We demonstrate the\neffectiveness of the compound domain assumption for the task of face\nanti-spoofing, for the first time in this work. To this end, we propose a\nmemory augmentation method for adapting the source model to the target domain\nin a domain aware manner. The adaptation process is further improved by using\nthe curriculum learning and the domain agnostic source network training\napproaches. The proposed method successfully adapts to the compound target\ndomain consisting multiple new spoof types. Our experiments on multiple\nbenchmark datasets demonstrate the superiority of the proposed method over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:08:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Panwar", "Ankush", ""], ["Singh", "Pratyush", ""], ["Saha", "Suman", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "2105.08468", "submitter": "Ge-Peng Ji", "authors": "Ge-Peng Ji, Yu-Cheng Chou, Deng-Ping Fan, Geng Chen, Huazhu Fu, Debesh\n  Jha, and Ling Shao", "title": "Progressively Normalized Self-Attention Network for Video Polyp\n  Segmentation", "comments": "MICCAI 2021 (Provisional accept); Code:\n  https://github.com/GewelsJI/PNS-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing video polyp segmentation (VPS) models typically employ convolutional\nneural networks (CNNs) to extract features. However, due to their limited\nreceptive fields, CNNs can not fully exploit the global temporal and spatial\ninformation in successive video frames, resulting in false-positive\nsegmentation results. In this paper, we propose the novel PNS-Net\n(Progressively Normalized Self-attention Network), which can efficiently learn\nrepresentations from polyp videos with real-time speed (~140fps) on a single\nRTX 2080 GPU and no post-processing. Our PNS-Net is based solely on a basic\nnormalized self-attention block, equipping with recurrence and CNNs entirely.\nExperiments on challenging VPS datasets demonstrate that the proposed PNS-Net\nachieves state-of-the-art performance. We also conduct extensive experiments to\nstudy the effectiveness of the channel split, soft-attention, and progressive\nlearning strategy. We find that our PNS-Net works well under different\nsettings, making it a promising solution to the VPS task.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:20:00 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 06:31:00 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ji", "Ge-Peng", ""], ["Chou", "Yu-Cheng", ""], ["Fan", "Deng-Ping", ""], ["Chen", "Geng", ""], ["Fu", "Huazhu", ""], ["Jha", "Debesh", ""], ["Shao", "Ling", ""]]}, {"id": "2105.08470", "submitter": "Lorenz K. Muller", "authors": "Lorenz K. Muller", "title": "Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast\n  Neural Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks can enhance images taken with small mobile\ncamera sensors and excel at tasks like demoisaicing, denoising and\nsuper-resolution. However, for practical use on mobile devices these networks\noften require too many FLOPs and reducing the FLOPs of a convolution layer,\nalso reduces its parameter count. This is problematic in view of the recent\nfinding that heavily over-parameterized neural networks are often the ones that\ngeneralize best. In this paper we propose to use HyperNetworks to break the\nfixed ratio of FLOPs to parameters of standard convolutions. This allows us to\nexceed previous state-of-the-art architectures in SSIM and MS-SSIM on the\nZurich RAW- to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we\nfurther observe generalization curves consistent with 'double-descent' behavior\nat fixed FLOP-count, in the large image limit. Finally we demonstrate the same\ntechnique can be applied to an existing network (VDN) to reduce its\ncomputational cost while maintaining fidelity on the Smartphone Image Denoising\nDataset (SIDD). Code for key functions is given in the appendix.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:27:05 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Muller", "Lorenz K.", ""]]}, {"id": "2105.08481", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Liangli Zhen, Joey Tianyi Zhou, Rick\n  Siow Mong Goh", "title": "Parallel Attention Network with Sequence Matching for Video Grounding", "comments": "15 pages, 10 figures, 7 tables, Findings at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video, video grounding aims to retrieve a temporal moment that\nsemantically corresponds to a language query. In this work, we propose a\nParallel Attention Network with Sequence matching (SeqPAN) to address the\nchallenges in this task: multi-modal representation learning, and target moment\nboundary prediction. We design a self-guided parallel attention module to\neffectively capture self-modal contexts and cross-modal attentive information\nbetween video and text. Inspired by sequence labeling tasks in natural language\nprocessing, we split the ground truth moment into begin, inside, and end\nregions. We then propose a sequence matching strategy to guide start/end\nboundary predictions using region labels. Experimental results on three\ndatasets show that SeqPAN is superior to state-of-the-art methods. Furthermore,\nthe effectiveness of the self-guided parallel attention module and the sequence\nmatching module is verified.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:43:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Zhen", "Liangli", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2105.08488", "submitter": "Daniele Meli", "authors": "Daniele Meli, Paolo Fiorini", "title": "Unsupervised identification of surgical robotic actions from small non\n  homogeneous datasets", "comments": "Submitted to IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted surgery is an established clinical practice. The automatic\nidentification of surgical actions is needed for a range of applications,\nincluding performance assessment of trainees and surgical process modeling for\nautonomous execution and monitoring. However, supervised action identification\nis not feasible, due to the burden of manually annotating recordings of\npotentially complex and long surgical executions. Moreover, often few example\nexecutions of a surgical procedure can be recorded. This paper proposes a novel\nalgorithm for unsupervised identification of surgical actions in a standard\nsurgical training task, the ring transfer, executed with da Vinci Research Kit.\nExploiting kinematic and semantic visual features automatically extracted from\na very limited dataset of executions, we are able to significantly outperform\nthe state-of-the-art results for a similar application, improving the quality\nof segmentation (88% vs. 82% matching score) and clustering (67% vs. 54%\nF1-score) even in the presence of noise, short actions and non homogeneous\nworkflows, i.e. non repetitive action sequences. Full action identification on\nhardware with standard commercial specifications is performed in less than 1 s\nfor single execution.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:06:32 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Meli", "Daniele", ""], ["Fiorini", "Paolo", ""]]}, {"id": "2105.08499", "submitter": "Filip Biljecki", "authors": "Koichi Ito, Filip Biljecki", "title": "Assessing bikeability with street view imagery and computer vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies evaluating bikeability usually compute spatial indicators shaping\ncycling conditions and conflate them in a quantitative index. Much research\ninvolves site visits or conventional geospatial approaches, and few studies\nhave leveraged street view imagery (SVI) for conducting virtual audits. These\nhave assessed a limited range of aspects, and not all have been automated using\ncomputer vision (CV). Furthermore, studies have not yet zeroed in on gauging\nthe usability of these technologies thoroughly. We investigate, with\nexperiments at a fine spatial scale and across multiple geographies (Singapore\nand Tokyo), whether we can use SVI and CV to assess bikeability\ncomprehensively. Extending related work, we develop an exhaustive index of\nbikeability composed of 34 indicators. The results suggest that SVI and CV are\nadequate to evaluate bikeability in cities comprehensively. As they\noutperformed non-SVI counterparts by a wide margin, SVI indicators are also\nfound to be superior in assessing urban bikeability, and potentially can be\nused independently, replacing traditional techniques. However, the paper\nexposes some limitations, suggesting that the best way forward is combining\nboth SVI and non-SVI approaches. The new bikeability index presents a\ncontribution in transportation and urban analytics, and it is scalable to\nassess cycling appeal widely.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:08:58 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 01:56:59 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ito", "Koichi", ""], ["Biljecki", "Filip", ""]]}, {"id": "2105.08501", "submitter": "Yuxing Chen", "authors": "Yuxing Chen", "title": "Multi-view Contrastive Coding of Remote Sensing Images at Pixel-level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our planet is viewed by satellites through multiple sensors (e.g.,\nmulti-spectral, Lidar and SAR) and at different times. Multi-view observations\nbring us complementary information than the single one. Alternatively, there\nare common features shared between different views, such as geometry and\nsemantics. Recently, contrastive learning methods have been proposed for the\nalignment of multi-view remote sensing images and improving the feature\nrepresentation of single sensor images by modeling view-invariant factors.\nHowever, these methods are based on the pretraining of the predefined tasks or\njust focus on image-level classification. Moreover, these methods lack research\non uncertainty estimation. In this work, a pixel-wise contrastive approach\nbased on an unlabeled multi-view setting is proposed to overcome this\nlimitation. This is achieved by the use of contrastive loss in the feature\nalignment and uniformity between multi-view images. In this approach, a\npseudo-Siamese ResUnet is trained to learn a representation that aims to align\nfeatures from the shifted positive pairs and uniform the induced distribution\nof the features on the hypersphere. The learned features of multi-view remote\nsensing images are evaluated on a liner protocol evaluation and an unsupervised\nchange detection task. We analyze key properties of the approach that make it\nwork, finding that the requirement of shift equivariance ensured the success of\nthe proposed approach and the uncertainty estimation of representations leads\nto performance improvements. Moreover, the performance of multi-view\ncontrastive learning is affected by the choice of different sensors. Results\ndemonstrate both improvements in efficiency and accuracy over the\nstate-of-the-art multi-view contrastive methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:28:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Yuxing", ""]]}, {"id": "2105.08506", "submitter": "Sara Atito", "authors": "Sara Atito Ali Ahmed and Mehmet Can Yavuz and Mehmet Umut Sen and\n  Fatih Gulsen and Onur Tutar and Bora Korkmazer and Cesur Samanci and Sabri\n  Sirolu and Rauf Hamid and Ali Ergun Eryurekli and Toghrul Mammadov and Berrin\n  Yanikoglu", "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D\n  Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 20:12:02 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 08:47:45 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ahmed", "Sara Atito Ali", ""], ["Yavuz", "Mehmet Can", ""], ["Sen", "Mehmet Umut", ""], ["Gulsen", "Fatih", ""], ["Tutar", "Onur", ""], ["Korkmazer", "Bora", ""], ["Samanci", "Cesur", ""], ["Sirolu", "Sabri", ""], ["Hamid", "Rauf", ""], ["Eryurekli", "Ali Ergun", ""], ["Mammadov", "Toghrul", ""], ["Yanikoglu", "Berrin", ""]]}, {"id": "2105.08568", "submitter": "Auguste Lehuger", "authors": "Auguste Lehuger, Matthew Crosby", "title": "Fixed $\\beta$-VAE Encoding for Curious Exploration in Complex 3D\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Curiosity is a general method for augmenting an environment reward with an\nintrinsic reward, which encourages exploration and is especially useful in\nsparse reward settings. As curiosity is calculated using next state prediction\nerror, the type of state encoding used has a large impact on performance.\nRandom features and inverse-dynamics features are generally preferred over VAEs\nbased on previous results from Atari and other mostly 2D environments. However,\nunlike VAEs, they may not encode sufficient information for optimal behaviour,\nwhich becomes increasingly important as environments become more complex. In\nthis paper, we use the sparse reward 3D physics environment Animal-AI, to\ndemonstrate how a fixed $\\beta$-VAE encoding can be used effectively with\ncuriosity. We combine this with curriculum learning to solve the previously\nunsolved exploration intensive detour tasks while achieving 22\\% gain in sample\nefficiency on the training curriculum against the next best encoding. We also\ncorroborate the results on Atari Breakout, with our custom encoding\noutperforming random features and inverse-dynamics features.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:52:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lehuger", "Auguste", ""], ["Crosby", "Matthew", ""]]}, {"id": "2105.08573", "submitter": "Wenqing Chen", "authors": "Wenqing Chen, Jidong Tian, Caoyun Fan, Hao He, and Yaohui Jin", "title": "Dependent Multi-Task Learning with Causal Intervention for Image\n  Captioning", "comments": "To be published in IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work for image captioning mainly followed an extract-then-generate\nparadigm, pre-extracting a sequence of object-based features and then\nformulating image captioning as a single sequence-to-sequence task. Although\npromising, we observed two problems in generated captions: 1) content\ninconsistency where models would generate contradicting facts; 2) not\ninformative enough where models would miss parts of important information. From\na causal perspective, the reason is that models have captured spurious\nstatistical correlations between visual features and certain expressions (e.g.,\nvisual features of \"long hair\" and \"woman\"). In this paper, we propose a\ndependent multi-task learning framework with the causal intervention (DMTCI).\nFirstly, we involve an intermediate task, bag-of-categories generation, before\nthe final task, image captioning. The intermediate task would help the model\nbetter understand the visual features and thus alleviate the content\ninconsistency problem. Secondly, we apply Pearl's do-calculus on the model,\ncutting off the link between the visual features and possible confounders and\nthus letting models focus on the causal visual features. Specifically, the\nhigh-frequency concept set is considered as the proxy confounders where the\nreal confounders are inferred in the continuous space. Finally, we use a\nmulti-agent reinforcement learning (MARL) strategy to enable end-to-end\ntraining and reduce the inter-task error accumulations. The extensive\nexperiments show that our model outperforms the baseline models and achieves\ncompetitive performance with state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:57:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Wenqing", ""], ["Tian", "Jidong", ""], ["Fan", "Caoyun", ""], ["He", "Hao", ""], ["Jin", "Yaohui", ""]]}, {"id": "2105.08582", "submitter": "Rowel Atienza", "authors": "Rowel Atienza", "title": "Vision Transformer for Fast and Efficient Scene Text Recognition", "comments": "To appear at ICDAR2021 Springer Lecture Notes in Computer Science\n  series", "journal-ref": "ICDAR2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene text recognition (STR) enables computers to read text in natural scenes\nsuch as object labels, road signs and instructions. STR helps machines perform\ninformed decisions such as what object to pick, which direction to go, and what\nis the next step of action. In the body of work on STR, the focus has always\nbeen on recognition accuracy. There is little emphasis placed on speed and\ncomputational efficiency which are equally important especially for\nenergy-constrained mobile machines. In this paper we propose ViTSTR, an STR\nwith a simple single stage model architecture built on a compute and parameter\nefficient vision transformer (ViT). On a comparable strong baseline method such\nas TRBA with accuracy of 84.3%, our small ViTSTR achieves a competitive\naccuracy of 82.6% (84.2% with data augmentation) at 2.4x speed up, using only\n43.4% of the number of parameters and 42.2% FLOPS. The tiny version of ViTSTR\nachieves 80.3% accuracy (82.1% with data augmentation), at 2.5x the speed,\nrequiring only 10.9% of the number of parameters and 11.9% FLOPS. With data\naugmentation, our base ViTSTR outperforms TRBA at 85.2% accuracy (83.7% without\naugmentation) at 2.3x the speed but requires 73.2% more parameters and 61.5%\nmore FLOPS. In terms of trade-offs, nearly all ViTSTR configurations are at or\nnear the frontiers to maximize accuracy, speed and computational efficiency all\nat the same time.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:08:54 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Atienza", "Rowel", ""]]}, {"id": "2105.08584", "submitter": "Gongfan Fang", "authors": "Gongfan Fang, Jie Song, Xinchao Wang, Chengchao Shen, Xingen Wang,\n  Mingli Song", "title": "Contrastive Model Inversion for Data-Free Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model inversion, whose goal is to recover training data from a pre-trained\nmodel, has been recently proved feasible. However, existing inversion methods\nusually suffer from the mode collapse problem, where the synthesized instances\nare highly similar to each other and thus show limited effectiveness for\ndownstream tasks, such as knowledge distillation. In this paper, we propose\nContrastive Model Inversion~(CMI), where the data diversity is explicitly\nmodeled as an optimizable objective, to alleviate the mode collapse issue. Our\nmain observation is that, under the constraint of the same amount of data,\nhigher data diversity usually indicates stronger instance discrimination. To\nthis end, we introduce in CMI a contrastive learning objective that encourages\nthe synthesizing instances to be distinguishable from the already synthesized\nones in previous batches. Experiments of pre-trained models on CIFAR-10,\nCIFAR-100, and Tiny-ImageNet demonstrate that CMI not only generates more\nvisually plausible instances than the state of the arts, but also achieves\nsignificantly superior performance when the generated data are used for\nknowledge distillation. Code is available at\n\\url{https://github.com/zju-vipa/DataFree}.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:13:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fang", "Gongfan", ""], ["Song", "Jie", ""], ["Wang", "Xinchao", ""], ["Shen", "Chengchao", ""], ["Wang", "Xingen", ""], ["Song", "Mingli", ""]]}, {"id": "2105.08590", "submitter": "Moloud Abdar", "authors": "Moloud Abdar, Soorena Salari, Sina Qahremani, Hak-Keung Lam, Fakhri\n  Karray, Sadiq Hussain, Abbas Khosravi, U. Rajendra Acharya, Saeid Nahavandi", "title": "UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion\n  with Ensemble Monte Carlo Dropout for COVID-19 Detection", "comments": "16 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 (Coronavirus disease 2019) has infected more than 151 million\npeople and caused approximately 3.17 million deaths around the world up to the\npresent. The rapid spread of COVID-19 is continuing to threaten human's life\nand health. Therefore, the development of computer-aided detection (CAD)\nsystems based on machine and deep learning methods which are able to accurately\ndifferentiate COVID-19 from other diseases using chest computed tomography (CT)\nand X-Ray datasets is essential and of immediate priority. Different from most\nof the previous studies which used either one of CT or X-ray images, we\nemployed both data types with sufficient samples in implementation. On the\nother hand, due to the extreme sensitivity of this pervasive virus, model\nuncertainty should be considered, while most previous studies have overlooked\nit. Therefore, we propose a novel powerful fusion model named\n$UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte\nCarlo (EMC) dropout. The obtained results prove the effectiveness of our\nproposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also,\nour proposed $UncertaintyFuseNet$ model is significantly robust to noise and\nperforms well with the previously unseen data. The source codes and models of\nthis study are available at:\nhttps://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:20:34 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 05:25:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Abdar", "Moloud", ""], ["Salari", "Soorena", ""], ["Qahremani", "Sina", ""], ["Lam", "Hak-Keung", ""], ["Karray", "Fakhri", ""], ["Hussain", "Sadiq", ""], ["Khosravi", "Abbas", ""], ["Acharya", "U. Rajendra", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2105.08595", "submitter": "Kai Wang", "authors": "Kai Wang, Luis Herranz, Joost van de Weijer", "title": "ACAE-REMIND for Online Continual Learning with Compressed Feature Replay", "comments": "Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online continual learning aims to learn from a non-IID stream of data from a\nnumber of different tasks, where the learner is only allowed to consider data\nonce. Methods are typically allowed to use a limited buffer to store some of\nthe images in the stream. Recently, it was found that feature replay, where an\nintermediate layer representation of the image is stored (or generated) leads\nto superior results than image replay, while requiring less memory. Quantized\nexemplars can further reduce the memory usage. However, a drawback of these\nmethods is that they use a fixed (or very intransigent) backbone network. This\nsignificantly limits the learning of representations that can discriminate\nbetween all tasks. To address this problem, we propose an auxiliary classifier\nauto-encoder (ACAE) module for feature replay at intermediate layers with high\ncompression rates. The reduced memory footprint per image allows us to save\nmore exemplars for replay. In our experiments, we conduct task-agnostic\nevaluation under online continual learning setting and get state-of-the-art\nperformance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:27:51 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 15:19:08 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Kai", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2105.08604", "submitter": "Darshan Bryner", "authors": "Darshan Bryner and Anuj Srivastava", "title": "Shape Analysis of Functional Data with Elastic Partial Matching", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Elastic Riemannian metrics have been used successfully in the past for\nstatistical treatments of functional and curve shape data. However, this usage\nhas suffered from an important restriction: the function boundaries are assumed\nfixed and matched. Functional data exhibiting unmatched boundaries typically\narise from dynamical systems with variable evolution rates such as COVID-19\ninfection rate curves associated with different geographical regions. In this\ncase, it is more natural to model such data with sliding boundaries and use\npartial matching, i.e., only a part of a function is matched to another\nfunction. Here, we develop a comprehensive Riemannian framework that allows for\npartial matching, comparing, and clustering of functions under both phase\nvariability and uncertain boundaries. We extend past work by: (1) Forming a\njoint action of the time-warping and time-scaling groups; (2) Introducing a\nmetric that is invariant to this joint action, allowing for a gradient-based\napproach to elastic partial matching; and (3) Presenting a modification that,\nwhile losing the metric property, allows one to control relative influence of\nthe two groups. This framework is illustrated for registering and clustering\nshapes of COVID-19 rate curves, identifying essential patterns, minimizing\nmismatch errors, and reducing variability within clusters compared to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:36:51 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bryner", "Darshan", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2105.08612", "submitter": "Yuan-Ting Hu", "authors": "Yuan-Ting Hu, Jiahong Wang, Raymond A. Yeh, Alexander G. Schwing", "title": "SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and\n  3D Mesh Reconstruction from Video Data", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting detailed 3D information of objects from video data is an important\ngoal for holistic scene understanding. While recent methods have shown\nimpressive results when reconstructing meshes of objects from a single image,\nresults often remain ambiguous as part of the object is unobserved. Moreover,\nexisting image-based datasets for mesh reconstruction don't permit to study\nmodels which integrate temporal information. To alleviate both concerns we\npresent SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh\nannotations which extends SAIL-VOS. We also develop first baselines for\nreconstruction of 3D meshes from video data via temporal models. We demonstrate\nefficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that\ntemporal information improves reconstruction quality. Resources and additional\ninformation are available at http://sailvos.web.illinois.edu.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:42:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Wang", "Jiahong", ""], ["Yeh", "Raymond A.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2105.08620", "submitter": "Yao Li", "authors": "Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee", "title": "Detecting Adversarial Examples with Bayesian Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new framework to detect adversarial examples\nmotivated by the observations that random components can improve the smoothness\nof predictors and make it easier to simulate output distribution of deep neural\nnetwork. With these observations, we propose a novel Bayesian adversarial\nexample detector, short for BATer, to improve the performance of adversarial\nexample detection. In specific, we study the distributional difference of\nhidden layer output between natural and adversarial examples, and propose to\nuse the randomness of Bayesian neural network (BNN) to simulate hidden layer\noutput distribution and leverage the distribution dispersion to detect\nadversarial examples. The advantage of BNN is that the output is stochastic\nwhile neural networks without random components do not have such\ncharacteristics. Empirical results on several benchmark datasets against\npopular attacks show that the proposed BATer outperforms the state-of-the-art\ndetectors in adversarial example detection.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:51:24 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:04:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yao", ""], ["Tang", "Tongyi", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "2105.08629", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Kim Byeoung-su, Radu Timofte, Angeline Pouget,\n  Fenglong Song, Cheng Li, Shuai Xiao, Zhongqian Fu, Matteo Maggioni, Yibin\n  Huang, Shen Cheng, Xin Lu, Yifeng Zhou, Liangyu Chen, Donghao Liu, Xiangyu\n  Zhang, Haoqiang Fan, Jian Sun, Shuaicheng Liu, Minsu Kwon, Myungje Lee,\n  Jaeyoon Yoo, Changbeom Kang, Shinjo Wang, Bin Huang, Tianbao Zhou, Shuai Liu,\n  Lei Lei, Chaoyu Feng, Liguang Huang, Zhikun Lei, Feifei Chen", "title": "Fast Camera Image Denoising on Mobile GPUs with Deep Learning, Mobile AI\n  2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial\n  text overlap with arXiv:2105.07809, arXiv:2105.07825", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is one of the most critical problems in mobile photo\nprocessing. While many solutions have been proposed for this task, they are\nusually working with synthetic data and are too computationally expensive to\nrun on mobile devices. To address this problem, we introduce the first Mobile\nAI challenge, where the target is to develop an end-to-end deep learning-based\nimage denoising solution that can demonstrate high efficiency on smartphone\nGPUs. For this, the participants were provided with a novel large-scale dataset\nconsisting of noisy-clean image pairs captured in the wild. The runtime of all\nmodels was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali\nGPU capable of accelerating floating-point and quantized neural networks. The\nproposed solutions are fully compatible with any mobile GPU and are capable of\nprocessing 480p resolution images under 40-80 ms while achieving high fidelity\nresults. A detailed description of all models developed in the challenge is\nprovided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:27:56 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ignatov", "Andrey", ""], ["Byeoung-su", "Kim", ""], ["Timofte", "Radu", ""], ["Pouget", "Angeline", ""], ["Song", "Fenglong", ""], ["Li", "Cheng", ""], ["Xiao", "Shuai", ""], ["Fu", "Zhongqian", ""], ["Maggioni", "Matteo", ""], ["Huang", "Yibin", ""], ["Cheng", "Shen", ""], ["Lu", "Xin", ""], ["Zhou", "Yifeng", ""], ["Chen", "Liangyu", ""], ["Liu", "Donghao", ""], ["Zhang", "Xiangyu", ""], ["Fan", "Haoqiang", ""], ["Sun", "Jian", ""], ["Liu", "Shuaicheng", ""], ["Kwon", "Minsu", ""], ["Lee", "Myungje", ""], ["Yoo", "Jaeyoon", ""], ["Kang", "Changbeom", ""], ["Wang", "Shinjo", ""], ["Huang", "Bin", ""], ["Zhou", "Tianbao", ""], ["Liu", "Shuai", ""], ["Lei", "Lei", ""], ["Feng", "Chaoyu", ""], ["Huang", "Liguang", ""], ["Lei", "Zhikun", ""], ["Chen", "Feifei", ""]]}, {"id": "2105.08630", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Grigory Malivenko, David Plowman, Samarth Shukla, Radu\n  Timofte, Ziyu Zhang, Yicheng Wang, Zilong Huang, Guozhong Luo, Gang Yu, Bin\n  Fu, Yiran Wang, Xingyi Li, Min Shi, Ke Xian, Zhiguo Cao, Jin-Hua Du, Pei-Lin\n  Wu, Chao Ge, Jiaoyang Yao, Fangwen Tu, Bo Li, Jung Eun Yoo, Kwanggyoon Seo,\n  Jialei Xu, Zhenyu Li, Xianming Liu, Junjun Jiang, Wei-Chi Chen, Shayan Joya,\n  Huanhuan Fan, Zhaobing Kang, Ang Li, Tianpeng Feng, Yang Liu, Chuannan Sheng,\n  Jian Yin, Fausto T. Benavide", "title": "Fast and Accurate Single-Image Depth Estimation on Mobile Devices,\n  Mobile AI 2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: text overlap\n  with arXiv:2105.07809", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is an important computer vision problem with many practical\napplications to mobile devices. While many solutions have been proposed for\nthis task, they are usually very computationally expensive and thus are not\napplicable for on-device inference. To address this problem, we introduce the\nfirst Mobile AI challenge, where the target is to develop an end-to-end deep\nlearning-based depth estimation solutions that can demonstrate a nearly\nreal-time performance on smartphones and IoT platforms. For this, the\nparticipants were provided with a new large-scale dataset containing RGB-depth\nimage pairs obtained with a dedicated stereo ZED camera producing\nhigh-resolution depth maps for objects located at up to 50 meters. The runtime\nof all models was evaluated on the popular Raspberry Pi 4 platform with a\nmobile ARM-based Broadcom chipset. The proposed solutions can generate VGA\nresolution depth maps at up to 10 FPS on the Raspberry Pi 4 while achieving\nhigh fidelity results, and are compatible with any Android or Linux-based\nmobile devices. A detailed description of all models developed in the challenge\nis provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:49:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ignatov", "Andrey", ""], ["Malivenko", "Grigory", ""], ["Plowman", "David", ""], ["Shukla", "Samarth", ""], ["Timofte", "Radu", ""], ["Zhang", "Ziyu", ""], ["Wang", "Yicheng", ""], ["Huang", "Zilong", ""], ["Luo", "Guozhong", ""], ["Yu", "Gang", ""], ["Fu", "Bin", ""], ["Wang", "Yiran", ""], ["Li", "Xingyi", ""], ["Shi", "Min", ""], ["Xian", "Ke", ""], ["Cao", "Zhiguo", ""], ["Du", "Jin-Hua", ""], ["Wu", "Pei-Lin", ""], ["Ge", "Chao", ""], ["Yao", "Jiaoyang", ""], ["Tu", "Fangwen", ""], ["Li", "Bo", ""], ["Yoo", "Jung Eun", ""], ["Seo", "Kwanggyoon", ""], ["Xu", "Jialei", ""], ["Li", "Zhenyu", ""], ["Liu", "Xianming", ""], ["Jiang", "Junjun", ""], ["Chen", "Wei-Chi", ""], ["Joya", "Shayan", ""], ["Fan", "Huanhuan", ""], ["Kang", "Zhaobing", ""], ["Li", "Ang", ""], ["Feng", "Tianpeng", ""], ["Liu", "Yang", ""], ["Sheng", "Chuannan", ""], ["Yin", "Jian", ""], ["Benavide", "Fausto T.", ""]]}, {"id": "2105.08635", "submitter": "Sina Khajehabdollahi", "authors": "Sina Khajehabdollahi, Georg Martius, Anna Levina", "title": "Assessing aesthetics of generated abstract images using correlation\n  structure", "comments": null, "journal-ref": "2019 IEEE Symposium Series on Computational Intelligence (SSCI),\n  306-313", "doi": "10.1109/SSCI44817.2019.9002779", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can we generate abstract aesthetic images without bias from natural or human\nselected image corpi? Are aesthetic images singled out in their correlation\nfunctions? In this paper we give answers to these and more questions. We\ngenerate images using compositional pattern-producing networks with random\nweights and varying architecture. We demonstrate that even with the randomly\nselected weights the correlation functions remain largely determined by the\nnetwork architecture. In a controlled experiment, human subjects picked\naesthetic images out of a large dataset of all generated images. Statistical\nanalysis reveals that the correlation function is indeed different for\naesthetic images.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:05:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Khajehabdollahi", "Sina", ""], ["Martius", "Georg", ""], ["Levina", "Anna", ""]]}, {"id": "2105.08647", "submitter": "Javier Lorenzo D\\'iaz", "authors": "J. Lorenzo, I. Parra and M. A. Sotelo", "title": "IntFormer: Predicting pedestrian intention with the aid of the\n  Transformer architecture", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding pedestrian crossing behavior is an essential goal in\nintelligent vehicle development, leading to an improvement in their security\nand traffic flow. In this paper, we developed a method called IntFormer. It is\nbased on transformer architecture and a novel convolutional video\nclassification model called RubiksNet. Following the evaluation procedure in a\nrecent benchmark, we show that our model reaches state-of-the-art results with\ngood performance ($\\approx 40$ seq. per second) and size ($8\\times $smaller\nthan the best performing model), making it suitable for real-time usage. We\nalso explore each of the input features, finding that ego-vehicle speed is the\nmost important variable, possibly due to the similarity in crossing cases in\nPIE dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:23:15 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lorenzo", "J.", ""], ["Parra", "I.", ""], ["Sotelo", "M. A.", ""]]}, {"id": "2105.08655", "submitter": "Abhiraj Tiwari", "authors": "Sahil Khose, Abhiraj Tiwari, Ankita Ghosh", "title": "Semi-Supervised Classification and Segmentation on High Resolution\n  Aerial Images", "comments": "5 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FloodNet is a high-resolution image dataset acquired by a small UAV platform,\nDJI Mavic Pro quadcopters, after Hurricane Harvey. The dataset presents a\nunique challenge of advancing the damage assessment process for post-disaster\nscenarios using unlabeled and limited labeled dataset. We propose a solution to\naddress their classification and semantic segmentation challenge. We approach\nthis problem by generating pseudo labels for both classification and\nsegmentation during training and slowly incrementing the amount by which the\npseudo label loss affects the final loss. Using this semi-supervised method of\ntraining helped us improve our baseline supervised loss by a huge margin for\nclassification, allowing the model to generalize and perform better on the\nvalidation and test splits of the dataset. In this paper, we compare and\ncontrast the various methods and models for image classification and semantic\nsegmentation on the FloodNet dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 09:30:03 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 17:45:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Khose", "Sahil", ""], ["Tiwari", "Abhiraj", ""], ["Ghosh", "Ankita", ""]]}, {"id": "2105.08665", "submitter": "Ambareesh Ravi", "authors": "Ambareesh Ravi, Amith Nandakumar", "title": "A multimodal deep learning framework for scalable content based visual\n  media retrieval", "comments": "Paper pertaining to a course project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel, efficient, modular and scalable framework for content\nbased visual media retrieval systems by leveraging the power of Deep Learning\nwhich is flexible to work both for images and videos conjointly and we also\nintroduce an efficient comparison and filtering metric for retrieval. We put\nforward our findings from critical performance tests comparing our method to\nthe predominant conventional approach to demonstrate the feasibility and\nefficiency of the proposed solution with best practices, possible improvements\nthat may further augment the ability of retrieval architectures.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:49:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ravi", "Ambareesh", ""], ["Nandakumar", "Amith", ""]]}, {"id": "2105.08667", "submitter": "Uthaipon Tantipongpipat", "authors": "Kyra Yee, Uthaipon Tantipongpipat, Shubhanshu Mishra", "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the\n  Importance of Representation, Design, and Agency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities. However, we demonstrate that\nformalized fairness metrics and quantitative analysis on their own are\ninsufficient for capturing the risk of representational harm in automatic\ncropping. We suggest the removal of saliency-based cropping in favor of a\nsolution that better preserves user agency. For developing a new solution that\nsufficiently address concerns related to representational harm, our critique\nmotivates a combination of quantitative and qualitative methods that include\nhuman-centered design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:50:50 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yee", "Kyra", ""], ["Tantipongpipat", "Uthaipon", ""], ["Mishra", "Shubhanshu", ""]]}, {"id": "2105.08704", "submitter": "Artem Savkin", "authors": "Mert Keser, Artem Savkin, Federico Tombari", "title": "Content Disentanglement for Semantically Consistent Synthetic-to-Real\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data generation is an appealing approach to generate novel traffic\nscenarios in autonomous driving. However, deep learning techniques trained\nsolely on synthetic data encounter dramatic performance drops when they are\ntested on real data. Such performance drop is commonly attributed to the domain\ngap between real and synthetic data. Domain adaptation methods have been\napplied to mitigate the aforementioned domain gap. These methods achieve\nvisually appealing results, but the translated samples usually introduce\nsemantic inconsistencies. In this work, we propose a new, unsupervised,\nend-to-end domain adaptation network architecture that enables semantically\nconsistent domain adaptation between synthetic and real data. We evaluate our\narchitecture on the downstream task of semantic segmentation and show that our\nmethod achieves superior performance compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:42:26 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 12:48:14 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 17:50:07 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Keser", "Mert", ""], ["Savkin", "Artem", ""], ["Tombari", "Federico", ""]]}, {"id": "2105.08714", "submitter": "Evan Shelhamer", "authors": "Dequan Wang, An Ju, Evan Shelhamer, David Wagner, Trevor Darrell", "title": "Fighting Gradients with Gradients: Dynamic Defenses against Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks optimize against models to defeat defenses. Existing\ndefenses are static, and stay the same once trained, even while attacks change.\nWe argue that models should fight back, and optimize their defenses against\nattacks at test time. We propose dynamic defenses, to adapt the model and input\nduring testing, by defensive entropy minimization (dent). Dent alters testing,\nbut not training, for compatibility with existing models and train-time\ndefenses. Dent improves the robustness of adversarially-trained defenses and\nnominally-trained models against white-box, black-box, and adaptive attacks on\nCIFAR-10/100 and ImageNet. In particular, dent boosts state-of-the-art defenses\nby 20+ points absolute against AutoAttack on CIFAR-10 at $\\epsilon_\\infty$ =\n8/255.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:55:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wang", "Dequan", ""], ["Ju", "An", ""], ["Shelhamer", "Evan", ""], ["Wagner", "David", ""], ["Darrell", "Trevor", ""]]}, {"id": "2105.08715", "submitter": "Mohammed Daoudi", "authors": "Baptiste Chopin, Naima Otberdout, Mohamed Daoudi, Angela Bartolo", "title": "Human Motion Prediction Using Manifold-Aware Wasserstein GAN", "comments": "IEEE International Conference on Automatic Face and Gesture\n  Recognition 2021 Jodhpur, India December 15 - 18, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human motion prediction aims to forecast future human poses given a prior\npose sequence. The discontinuity of the predicted motion and the performance\ndeterioration in long-term horizons are still the main challenges encountered\nin current literature. In this work, we tackle these issues by using a compact\nmanifold-valued representation of human motion. Specifically, we model the\ntemporal evolution of the 3D human poses as trajectory, what allows us to map\nhuman motions to single points on a sphere manifold. To learn these\nnon-Euclidean representations, we build a manifold-aware Wasserstein generative\nadversarial model that captures the temporal and spatial dependencies of human\nmotion through different losses. Extensive experiments show that our approach\noutperforms the state-of-the-art on CMU MoCap and Human 3.6M datasets. Our\nqualitative results show the smoothness of the predicted motions.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:56:10 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 21:16:53 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chopin", "Baptiste", ""], ["Otberdout", "Naima", ""], ["Daoudi", "Mohamed", ""], ["Bartolo", "Angela", ""]]}, {"id": "2105.08756", "submitter": "Jing Yu Koh", "authors": "Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson", "title": "Pathdreamer: A World Model for Indoor Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People navigating in unfamiliar buildings take advantage of myriad visual,\nspatial and semantic cues to efficiently achieve their navigation goals.\nTowards equipping computational agents with similar capabilities, we introduce\nPathdreamer, a visual world model for agents navigating in novel indoor\nenvironments. Given one or more previous visual observations, Pathdreamer\ngenerates plausible high-resolution 360 visual observations (RGB, semantic\nsegmentation and depth) for viewpoints that have not been visited, in buildings\nnot seen during training. In regions of high uncertainty (e.g. predicting\naround corners, imagining the contents of an unseen room), Pathdreamer can\npredict diverse scenes, allowing an agent to sample multiple realistic outcomes\nfor a given trajectory. We demonstrate that Pathdreamer encodes useful and\naccessible visual, spatial and semantic knowledge about human environments by\nusing it in the downstream task of Vision-and-Language Navigation (VLN).\nSpecifically, we show that planning ahead with Pathdreamer brings about half\nthe benefit of looking ahead at actual observations from unobserved parts of\nthe environment. We hope that Pathdreamer will help unlock model-based\napproaches to challenging embodied navigation tasks such as navigating to\nspecified objects and VLN.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:13:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Koh", "Jing Yu", ""], ["Lee", "Honglak", ""], ["Yang", "Yinfei", ""], ["Baldridge", "Jason", ""], ["Anderson", "Peter", ""]]}, {"id": "2105.08788", "submitter": "Muhammad Maaz Mr", "authors": "Muhammad Maaz, Hanoona Abdul Rasheed, Dhanalaxmi Gaddam", "title": "Self-Supervised Learning for Fine-Grained Visual Categorization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research in self-supervised learning (SSL) has shown its capability in\nlearning useful semantic representations from images for classification tasks.\nThrough our work, we study the usefulness of SSL for Fine-Grained Visual\nCategorization (FGVC). FGVC aims to distinguish objects of visually similar sub\ncategories within a general category. The small inter-class, but large\nintra-class variations within the dataset makes it a challenging task. The\nlimited availability of annotated labels for such a fine-grained data\nencourages the need for SSL, where additional supervision can boost learning\nwithout the cost of extra annotations. Our baseline achieves $86.36\\%$ top-1\nclassification accuracy on CUB-200-2011 dataset by utilizing random crop\naugmentation during training and center crop augmentation during testing. In\nthis work, we explore the usefulness of various pretext tasks, specifically,\nrotation, pretext invariant representation learning (PIRL), and deconstruction\nand construction learning (DCL) for FGVC. Rotation as an auxiliary task\npromotes the model to learn global features, and diverts it from focusing on\nthe subtle details. PIRL that uses jigsaw patches attempts to focus on\ndiscriminative local regions, but struggles to accurately localize them. DCL\nhelps in learning local discriminating features and outperforms the baseline by\nachieving $87.41\\%$ top-1 accuracy. The deconstruction learning forces the\nmodel to focus on local object parts, while reconstruction learning helps in\nlearning the correlation between the parts. We perform extensive experiments to\nreason our findings. Our code is available at\nhttps://github.com/mmaaz60/ssl_for_fgvc.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:16:05 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Maaz", "Muhammad", ""], ["Rasheed", "Hanoona Abdul", ""], ["Gaddam", "Dhanalaxmi", ""]]}, {"id": "2105.08793", "submitter": "Hyunsoo Cho", "authors": "Hyunsoo Cho, Jinseok Seol, Sang-goo Lee", "title": "Masked Contrastive Learning for Anomaly Detection", "comments": "Accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalies is one fundamental aspect of a safety-critical software\nsystem, however, it remains a long-standing problem. Numerous branches of works\nhave been proposed to alleviate the complication and have demonstrated their\nefficiencies. In particular, self-supervised learning based methods are\nspurring interest due to their capability of learning diverse representations\nwithout additional labels. Among self-supervised learning tactics, contrastive\nlearning is one specific framework validating their superiority in various\nfields, including anomaly detection. However, the primary objective of\ncontrastive learning is to learn task-agnostic features without any labels,\nwhich is not entirely suited to discern anomalies. In this paper, we propose a\ntask-specific variant of contrastive learning named masked contrastive\nlearning, which is more befitted for anomaly detection. Moreover, we propose a\nnew inference method dubbed self-ensemble inference that further boosts\nperformance by leveraging the ability learned through auxiliary\nself-supervision tasks. By combining our models, we can outperform previous\nstate-of-the-art methods by a significant margin on various benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:27:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Cho", "Hyunsoo", ""], ["Seol", "Jinseok", ""], ["Lee", "Sang-goo", ""]]}, {"id": "2105.08796", "submitter": "Aleksei Zhuchkov", "authors": "Aleksei Zhuchkov", "title": "Analyzing the effectiveness of image augmentations for face recognition\n  from limited data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an analysis of the efficiency of image augmentations for\nthe face recognition problem from limited data. We considered basic\nmanipulations, generative methods, and their combinations for augmentations.\nOur results show that augmentations, in general, can considerably improve the\nquality of face recognition systems and the combination of generative and basic\napproaches performs better than the other tested techniques.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:33:17 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhuchkov", "Aleksei", ""]]}, {"id": "2105.08808", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Correlated Adversarial Joint Discrepancy Adaptation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Domain adaptation aims to mitigate the domain shift problem when transferring\nknowledge from one domain into another similar but different domain. However,\nmost existing works rely on extracting marginal features without considering\nclass labels. Moreover, some methods name their model as so-called unsupervised\ndomain adaptation while tuning the parameters using the target domain label. To\naddress these issues, we propose a novel approach called correlated adversarial\njoint discrepancy adaptation network (CAJNet), which minimizes the joint\ndiscrepancy of two domains and achieves competitive performance with tuning\nparameters using the correlated label. By training the joint features, we can\nalign the marginal and conditional distributions between the two domains. In\naddition, we introduce a probability-based top-$\\mathcal{K}$ correlated label\n($\\mathcal{K}$-label), which is a powerful indicator of the target domain and\neffective metric to tune parameters to aid predictions. Extensive experiments\non benchmark datasets demonstrate significant improvements in classification\naccuracy over the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:52:08 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2105.08809", "submitter": "Fatma Abdeo", "authors": "Fatma S. Abousaleh, Wen-Huang Cheng, Neng-Hao Yu, and Yu Tsao", "title": "Multimodal Deep Learning Framework for Image Popularity Prediction on\n  Social Media", "comments": "14 pages, 11 figures, 7 tables", "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems. 2020 Nov\n  9", "doi": "10.1109/TCDS.2020.3036690", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Billions of photos are uploaded to the web daily through various types of\nsocial networks. Some of these images receive millions of views and become\npopular, whereas others remain completely unnoticed. This raises the problem of\npredicting image popularity on social media. The popularity of an image can be\naffected by several factors, such as visual content, aesthetic quality, user,\npost metadata, and time. Thus, considering all these factors is essential for\naccurately predicting image popularity. In addition, the efficiency of the\npredictive model also plays a crucial role. In this study, motivated by\nmultimodal learning, which uses information from various modalities, and the\ncurrent success of convolutional neural networks (CNNs) in various fields, we\npropose a deep learning model, called visual-social convolutional neural\nnetwork (VSCNN), which predicts the popularity of a posted image by\nincorporating various types of visual and social features into a unified\nnetwork model. VSCNN first learns to extract high-level representations from\nthe input visual and social features by utilizing two individual CNNs. The\noutputs of these two networks are then fused into a joint network to estimate\nthe popularity score in the output layer. We assess the performance of the\nproposed method by conducting extensive experiments on a dataset of\napproximately 432K images posted on Flickr. The simulation results demonstrate\nthat the proposed VSCNN model significantly outperforms state-of-the-art\nmodels, with a relative improvement of greater than 2.33%, 7.59%, and 14.16% in\nterms of Spearman's Rho, mean absolute error, and mean squared error,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:58:58 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Abousaleh", "Fatma S.", ""], ["Cheng", "Wen-Huang", ""], ["Yu", "Neng-Hao", ""], ["Tsao", "Yu", ""]]}, {"id": "2105.08819", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Grigory Malivenko, Radu Timofte, Sheng Chen, Xin Xia,\n  Zhaoyan Liu, Yuwei Zhang, Feng Zhu, Jiashi Li, Xuefeng Xiao, Yuan Tian,\n  Xinglong Wu, Christos Kyrkou, Yixin Chen, Zexin Zhang, Yunbo Peng, Yue Lin,\n  Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah, Himanshu Kumar, Chao Ge,\n  Pei-Lin Wu, Jin-Hua Du, Andrew Batutin, Juan Pablo Federico, Konrad Lyda,\n  Levon Khojoyan, Abhishek Thanki, Sayak Paul, Shahid Siddiqui", "title": "Fast and Accurate Quantized Camera Scene Detection on Smartphones,\n  Mobile AI 2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial\n  text overlap with arXiv:2105.08630; text overlap with arXiv:2105.07825,\n  arXiv:2105.07809, arXiv:2105.08629", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera scene detection is among the most popular computer vision problem on\nsmartphones. While many custom solutions were developed for this task by phone\nvendors, none of the designed models were available publicly up until now. To\naddress this problem, we introduce the first Mobile AI challenge, where the\ntarget is to develop quantized deep learning-based camera scene classification\nsolutions that can demonstrate a real-time performance on smartphones and IoT\nplatforms. For this, the participants were provided with a large-scale CamSDD\ndataset consisting of more than 11K images belonging to the 30 most important\nscene categories. The runtime of all models was evaluated on the popular Apple\nBionic A11 platform that can be found in many iOS devices. The proposed\nsolutions are fully compatible with all major mobile AI accelerators and can\ndemonstrate more than 100-200 FPS on the majority of recent smartphone\nplatforms while achieving a top-3 accuracy of more than 98%. A detailed\ndescription of all models developed in the challenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:55:38 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ignatov", "Andrey", ""], ["Malivenko", "Grigory", ""], ["Timofte", "Radu", ""], ["Chen", "Sheng", ""], ["Xia", "Xin", ""], ["Liu", "Zhaoyan", ""], ["Zhang", "Yuwei", ""], ["Zhu", "Feng", ""], ["Li", "Jiashi", ""], ["Xiao", "Xuefeng", ""], ["Tian", "Yuan", ""], ["Wu", "Xinglong", ""], ["Kyrkou", "Christos", ""], ["Chen", "Yixin", ""], ["Zhang", "Zexin", ""], ["Peng", "Yunbo", ""], ["Lin", "Yue", ""], ["Dutta", "Saikat", ""], ["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Kumar", "Himanshu", ""], ["Ge", "Chao", ""], ["Wu", "Pei-Lin", ""], ["Du", "Jin-Hua", ""], ["Batutin", "Andrew", ""], ["Federico", "Juan Pablo", ""], ["Lyda", "Konrad", ""], ["Khojoyan", "Levon", ""], ["Thanki", "Abhishek", ""], ["Paul", "Sayak", ""], ["Siddiqui", "Shahid", ""]]}, {"id": "2105.08822", "submitter": "Ruijing Yang", "authors": "Ruijing Yang, Ziyu Guan, Zitong Yu, Guoying Zhao, Xiaoyi Feng, Jinye\n  Peng", "title": "Non-contact Pain Recognition from Video Sequences with Remote\n  Physiological Measurements Prediction", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic pain recognition is paramount for medical diagnosis and treatment.\nThe existing works fall into three categories: assessing facial appearance\nchanges, exploiting physiological cues, or fusing them in a multi-modal manner.\nHowever, (1) appearance changes are easily affected by subjective factors which\nimpedes objective pain recognition. Besides, the appearance-based approaches\nignore long-range spatial-temporal dependencies that are important for modeling\nexpressions over time; (2) the physiological cues are obtained by attaching\nsensors on human body, which is inconvenient and uncomfortable. In this paper,\nwe present a novel multi-task learning framework which encodes both appearance\nchanges and physiological cues in a non-contact manner for pain recognition.\nThe framework is able to capture both local and long-range dependencies via the\nproposed attention mechanism for the learned appearance representations, which\nare further enriched by temporally attended physiological cues (remote\nphotoplethysmography, rPPG) that are recovered from videos in the auxiliary\ntask. This framework is dubbed rPPG-enriched Spatio-Temporal Attention Network\n(rSTAN) and allows us to establish the state-of-the-art performance of\nnon-contact pain recognition on publicly available pain databases. It\ndemonstrates that rPPG predictions can be used as an auxiliary task to\nfacilitate non-contact automatic pain recognition.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:47:45 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yang", "Ruijing", ""], ["Guan", "Ziyu", ""], ["Yu", "Zitong", ""], ["Zhao", "Guoying", ""], ["Feng", "Xiaoyi", ""], ["Peng", "Jinye", ""]]}, {"id": "2105.08825", "submitter": "Xiaoyu Bie", "authors": "Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, Francesc Moreno-Noguer", "title": "Multi-Person Extreme Motion Prediction with Cross-Interaction Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction aims to forecast future human poses given a sequence\nof past 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper we explore this problem from a novel perspective, involving humans\nperforming collaborative tasks. We assume that the input of our system are two\nsequences of past skeletons for two interacting persons, and we aim to predict\nthe future motion for each of them. For this purpose, we devise a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons and learns to predict cross dependencies between self poses and the\nposes of the other person in spite of their spatial or temporal distance. Since\nno dataset to train such interactive situations is available, we have captured\nExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of\nprofessional dancers performing acrobatics. ExPI contains 115 sequences with\n30k frames and 60k instances with annotated 3D body poses and shapes. We\nthoroughly evaluate our cross-interaction network on this dataset and show that\nboth in short-term and long-term predictions, it consistently outperforms\nbaselines that independently reason for each person. We plan to release our\ncode jointly with the dataset and the train/test splits to spur future research\non the topic.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:52:05 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 11:46:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Guo", "Wen", ""], ["Bie", "Xiaoyu", ""], ["Alameda-Pineda", "Xavier", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2105.08826", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Andres Romero, Heewon Kim, Radu Timofte, Chiu Man Ho,\n  Zibo Meng, Kyoung Mu Lee, Yuxiang Chen, Yutong Wang, Zeyu Long, Chenhao Wang,\n  Yifei Chen, Boshen Xu, Shuhang Gu, Lixin Duan, Wen Li, Wang Bofei, Zhang\n  Diankai, Zheng Chengjian, Liu Shaoli, Gao Si, Zhang Xiaofeng, Lu Kaidi, Xu\n  Tianyu, Zheng Hui, Xinbo Gao, Xiumei Wang, Jiaming Guo, Xueyi Zhou, Hao Jia,\n  Youliang Yan", "title": "Real-Time Video Super-Resolution on Smartphones with Deep Learning,\n  Mobile AI 2021 Challenge: Report", "comments": "Mobile AI 2021 Workshop and Challenges:\n  https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial\n  text overlap with arXiv:2105.07825. substantial text overlap with\n  arXiv:2105.08629, arXiv:2105.07809, arXiv:2105.08630", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution has recently become one of the most important\nmobile-related problems due to the rise of video communication and streaming\nservices. While many solutions have been proposed for this task, the majority\nof them are too computationally expensive to run on portable devices with\nlimited hardware resources. To address this problem, we introduce the first\nMobile AI challenge, where the target is to develop an end-to-end deep\nlearning-based video super-resolution solutions that can achieve a real-time\nperformance on mobile GPUs. The participants were provided with the REDS\ndataset and trained their models to do an efficient 4X video upscaling. The\nruntime of all models was evaluated on the OPPO Find X2 smartphone with the\nSnapdragon 865 SoC capable of accelerating floating-point networks on its\nAdreno GPU. The proposed solutions are fully compatible with any mobile GPU and\ncan upscale videos to HD resolution at up to 80 FPS while demonstrating high\nfidelity results. A detailed description of all models developed in the\nchallenge is provided in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:40:50 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ignatov", "Andrey", ""], ["Romero", "Andres", ""], ["Kim", "Heewon", ""], ["Timofte", "Radu", ""], ["Ho", "Chiu Man", ""], ["Meng", "Zibo", ""], ["Lee", "Kyoung Mu", ""], ["Chen", "Yuxiang", ""], ["Wang", "Yutong", ""], ["Long", "Zeyu", ""], ["Wang", "Chenhao", ""], ["Chen", "Yifei", ""], ["Xu", "Boshen", ""], ["Gu", "Shuhang", ""], ["Duan", "Lixin", ""], ["Li", "Wen", ""], ["Bofei", "Wang", ""], ["Diankai", "Zhang", ""], ["Chengjian", "Zheng", ""], ["Shaoli", "Liu", ""], ["Si", "Gao", ""], ["Xiaofeng", "Zhang", ""], ["Kaidi", "Lu", ""], ["Tianyu", "Xu", ""], ["Hui", "Zheng", ""], ["Gao", "Xinbo", ""], ["Wang", "Xiumei", ""], ["Guo", "Jiaming", ""], ["Zhou", "Xueyi", ""], ["Jia", "Hao", ""], ["Yan", "Youliang", ""]]}, {"id": "2105.08837", "submitter": "Sachini Herath", "authors": "Sachini Herath, Saghar Irandoust, Bowen Chen, Yiming Qian, Pyojin Kim,\n  Yasutaka Furukawa", "title": "Fusion-DHL: WiFi, IMU, and Floorplan Fusion for Dense History of\n  Locations in Indoor Environments", "comments": "To be published in ICRA 2021. Code and data:\n  https://github.com/Sachini/Fusion-DHL", "journal-ref": "ICRA 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes a multi-modal sensor fusion algorithm that fuses WiFi,\nIMU, and floorplan information to infer an accurate and dense location history\nin indoor environments. The algorithm uses 1) an inertial navigation algorithm\nto estimate a relative motion trajectory from IMU sensor data; 2) a WiFi-based\nlocalization API in industry to obtain positional constraints and geo-localize\nthe trajectory; and 3) a convolutional neural network to refine the location\nhistory to be consistent with the floorplan.\n  We have developed a data acquisition app to build a new dataset with WiFi,\nIMU, and floorplan data with ground-truth positions at 4 university buildings\nand 3 shopping malls. Our qualitative and quantitative evaluations demonstrate\nthat the proposed system is able to produce twice as accurate and a few orders\nof magnitude denser location history than the current standard, while requiring\nminimal additional energy consumption. We will publicly share our code, data\nand models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:26:45 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Herath", "Sachini", ""], ["Irandoust", "Saghar", ""], ["Chen", "Bowen", ""], ["Qian", "Yiming", ""], ["Kim", "Pyojin", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "2105.08865", "submitter": "Shikha Gupta", "authors": "Krishan Sharma (1), Shikha Gupta (1), Renu Rameshan (2) ((1) Vehant\n  Technologies Pvt. Ltd., (2) Indian Institute of Technology Mandi, India)", "title": "Learning optimally separated class-specific subspace representations\n  using convolutional autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel convolutional autoencoder based architecture\nto generate subspace specific feature representations that are best suited for\nclassification task. The class-specific data is assumed to lie in low\ndimensional linear subspaces, which could be noisy and not well separated,\ni.e., subspace distance (principal angle) between two classes is very low. The\nproposed network uses a novel class-specific self expressiveness (CSSE) layer\nsandwiched between encoder and decoder networks to generate class-wise subspace\nrepresentations which are well separated. The CSSE layer along with encoder/\ndecoder are trained in such a way that data still lies in subspaces in the\nfeature space with minimum principal angle much higher than that of the input\nspace. To demonstrate the effectiveness of the proposed approach, several\nexperiments have been carried out on state-of-the-art machine learning datasets\nand a significant improvement in classification performance is observed over\nexisting subspace based transformation learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 00:45:34 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sharma", "Krishan", ""], ["Gupta", "Shikha", ""], ["Rameshan", "Renu", ""]]}, {"id": "2105.08876", "submitter": "Yuexin Xiang", "authors": "Yuexin Xiang, Tiantian Li, Wei Ren, Tianqing Zhu, Kim-Kwang Raymond\n  Choo", "title": "A Lightweight Privacy-Preserving Scheme Using Label-based Pixel Block\n  Mixing for Image Classification in Deep Learning", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure the privacy of sensitive data used in the training of deep learning\nmodels, a number of privacy-preserving methods have been designed by the\nresearch community. However, existing schemes are generally designed to work\nwith textual data, or are not efficient when a large number of images is used\nfor training. Hence, in this paper we propose a lightweight and efficient\napproach to preserve image privacy while maintaining the availability of the\ntraining set. Specifically, we design the pixel block mixing algorithm for\nimage classification privacy preservation in deep learning. To evaluate its\nutility, we use the mixed training set to train the ResNet50, VGG16,\nInceptionV3 and DenseNet121 models on the WIKI dataset and the CNBC face\ndataset. Experimental findings on the testing set show that our scheme\npreserves image privacy while maintaining the availability of the training set\nin the deep learning models. Additionally, the experimental results demonstrate\nthat we achieve good performance for the VGG16 model on the WIKI dataset and\nboth ResNet50 and DenseNet121 on the CNBC dataset. The pixel block algorithm\nachieves fairly high efficiency in the mixing of the images, and it is\ncomputationally challenging for the attackers to restore the mixed training set\nto the original training set. Moreover, data augmentation can be applied to the\nmixed training set to improve the training's effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:50:50 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Xiang", "Yuexin", ""], ["Li", "Tiantian", ""], ["Ren", "Wei", ""], ["Zhu", "Tianqing", ""], ["Choo", "Kim-Kwang Raymond", ""]]}, {"id": "2105.08879", "submitter": "Brian Kenji Iwana", "authors": "Taiga Miyazono, Brian Kenji Iwana, Daichi Haraguchi, Seiichi Uchida", "title": "Font Style that Fits an Image -- Font Generation Based on Image Context", "comments": "Accepted to ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fonts are used on documents, they are intentionally selected by\ndesigners. For example, when designing a book cover, the typography of the text\nis an important factor in the overall feel of the book. In addition, it needs\nto be an appropriate font for the rest of the book cover. Thus, we propose a\nmethod of generating a book title image based on its context within a book\ncover. We propose an end-to-end neural network that inputs the book cover, a\ntarget location mask, and a desired book title and outputs stylized text\nsuitable for the cover. The proposed network uses a combination of a\nmulti-input encoder-decoder, a text skeleton prediction network, a perception\nnetwork, and an adversarial discriminator. We demonstrate that the proposed\nmethod can effectively produce desirable and appropriate book cover text\nthrough quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:53:04 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Miyazono", "Taiga", ""], ["Iwana", "Brian Kenji", ""], ["Haraguchi", "Daichi", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2105.08913", "submitter": "Binh Nguyen Xuan", "authors": "Tuong Do, Binh X. Nguyen, Erman Tjiputra, Minh Tran, Quang D. Tran,\n  Anh Nguyen", "title": "Multiple Meta-model Quantifying for Medical Visual Question Answering", "comments": "Provisional accepted in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is an important step to extract meaningful features and\novercome the data limitation in the medical Visual Question Answering (VQA)\ntask. However, most of the existing medical VQA methods rely on external data\nfor transfer learning, while the meta-data within the dataset is not fully\nutilized. In this paper, we present a new multiple meta-model quantifying\nmethod that effectively learns meta-annotation and leverages meaningful\nfeatures to the medical VQA task. Our proposed method is designed to increase\nmeta-data by auto-annotation, deal with noisy labels, and output meta-models\nwhich provide robust features for medical VQA tasks. Extensively experimental\nresults on two public medical VQA datasets show that our approach achieves\nsuperior accuracy in comparison with other state-of-the-art methods, while does\nnot require external data to train meta-models.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:06:05 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 10:49:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Do", "Tuong", ""], ["Nguyen", "Binh X.", ""], ["Tjiputra", "Erman", ""], ["Tran", "Minh", ""], ["Tran", "Quang D.", ""], ["Nguyen", "Anh", ""]]}, {"id": "2105.08919", "submitter": "Taehyeon Kim", "authors": "Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, Se-Young Yun", "title": "Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in\n  Knowledge Distillation", "comments": "Proceedings of International Joint Conference on Artificial\n  Intelligence (IJCAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD), transferring knowledge from a cumbersome teacher\nmodel to a lightweight student model, has been investigated to design efficient\nneural architectures. Generally, the objective function of KD is the\nKullback-Leibler (KL) divergence loss between the softened probability\ndistributions of the teacher model and the student model with the temperature\nscaling hyperparameter tau. Despite its widespread use, few studies have\ndiscussed the influence of such softening on generalization. Here, we\ntheoretically show that the KL divergence loss focuses on the logit matching\nwhen tau increases and the label matching when tau goes to 0 and empirically\nshow that the logit matching is positively correlated to performance\nimprovement in general. From this observation, we consider an intuitive KD loss\nfunction, the mean squared error (MSE) between the logit vectors, so that the\nstudent model can directly learn the logit of the teacher model. The MSE loss\noutperforms the KL divergence loss, explained by the difference in the\npenultimate layer representations between the two losses. Furthermore, we show\nthat sequential distillation can improve performance and that KD, particularly\nwhen using the KL divergence loss with small tau, mitigates the label noise.\nThe code to reproduce the experiments is publicly available online at\nhttps://github.com/jhoon-oh/kd_data/.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:40:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kim", "Taehyeon", ""], ["Oh", "Jaehoon", ""], ["Kim", "NakYil", ""], ["Cho", "Sangwook", ""], ["Yun", "Se-Young", ""]]}, {"id": "2105.08941", "submitter": "Martin Humenberger", "authors": "Donghwan Lee, Soohyun Ryu, Suyong Yeon, Yonghan Lee, Deokhwa Kim,\n  Cheolho Han, Yohann Cabon, Philippe Weinzaepfel, Nicolas Gu\\'erin, Gabriela\n  Csurka, and Martin Humenberger", "title": "Large-scale Localization Datasets in Crowded Indoor Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the precise location of a camera using visual localization enables\ninteresting applications such as augmented reality or robot navigation. This is\nparticularly useful in indoor environments where other localization\ntechnologies, such as GNSS, fail. Indoor spaces impose interesting challenges\non visual localization algorithms: occlusions due to people, textureless\nsurfaces, large viewpoint changes, low light, repetitive textures, etc.\nExisting indoor datasets are either comparably small or do only cover a subset\nof the mentioned challenges. In this paper, we introduce 5 new indoor datasets\nfor visual localization in challenging real-world environments. They were\ncaptured in a large shopping mall and a large metro station in Seoul, South\nKorea, using a dedicated mapping platform consisting of 10 cameras and 2 laser\nscanners. In order to obtain accurate ground truth camera poses, we developed a\nrobust LiDAR SLAM which provides initial poses that are then refined using a\nnovel structure-from-motion based optimization. We present a benchmark of\nmodern visual localization algorithms on these challenging datasets showing\nsuperior performance of structure-based methods using robust image features.\nThe datasets are available at: https://naverlabs.com/datasets\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:20:49 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lee", "Donghwan", ""], ["Ryu", "Soohyun", ""], ["Yeon", "Suyong", ""], ["Lee", "Yonghan", ""], ["Kim", "Deokhwa", ""], ["Han", "Cheolho", ""], ["Cabon", "Yohann", ""], ["Weinzaepfel", "Philippe", ""], ["Gu\u00e9rin", "Nicolas", ""], ["Csurka", "Gabriela", ""], ["Humenberger", "Martin", ""]]}, {"id": "2105.08949", "submitter": "Chun-Mei Feng", "authors": "Chun-Mei Feng, Huazhu Fu, Shuhao Yuan, and Yong Xu", "title": "Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration\n  Network", "comments": "10 pages, 3 figures", "journal-ref": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI2021)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Super-resolution (SR) plays a crucial role in improving the image quality of\nmagnetic resonance imaging (MRI). MRI produces multi-contrast images and can\nprovide a clear display of soft tissues. However, current super-resolution\nmethods only employ a single contrast, or use a simple multi-contrast fusion\nmechanism, ignoring the rich relations among different contrasts, which are\nvaluable for improving SR. In this work, we propose a multi-stage integration\nnetwork (i.e., MINet) for multi-contrast MRI SR, which explicitly models the\ndependencies between multi-contrast images at different stages to guide image\nSR. In particular, our MINet first learns a hierarchical feature representation\nfrom multiple convolutional stages for each of different-contrast image.\nSubsequently, we introduce a multi-stage integration module to mine the\ncomprehensive relations between the representations of the multi-contrast\nimages. Specifically, the module matches each representation with all other\nfeatures, which are integrated in terms of their similarities to obtain an\nenriched representation. Extensive experiments on fastMRI and real-world\nclinical datasets demonstrate that 1) our MINet outperforms state-of-the-art\nmulti-contrast SR methods in terms of various metrics and 2) our multi-stage\nintegration module is able to excavate complex interactions among\nmulti-contrast features at different stages, leading to improved target-image\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:47:31 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 20:25:11 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 19:40:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Fu", "Huazhu", ""], ["Yuan", "Shuhao", ""], ["Xu", "Yong", ""]]}, {"id": "2105.08952", "submitter": "Haoping Bai", "authors": "Haoping Bai, Meng Cao, Ping Huang, Jiulong Shan", "title": "BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the applications of deep learning models on edge devices increase at an\naccelerating pace, fast adaptation to various scenarios with varying resource\nconstraints has become a crucial aspect of model deployment. As a result, model\noptimization strategies with adaptive configuration are becoming increasingly\npopular. While single-shot quantized neural architecture search enjoys\nflexibility in both model architecture and quantization policy, the combined\nsearch space comes with many challenges, including instability when training\nthe weight-sharing supernet and difficulty in navigating the exponentially\ngrowing search space. Existing methods tend to either limit the architecture\nsearch space to a small set of options or limit the quantization policy search\nspace to fixed precision policies. To this end, we propose BatchQuant, a robust\nquantizer formulation that allows fast and stable training of a compact,\nsingle-shot, mixed-precision, weight-sharing supernet. We employ BatchQuant to\ntrain a compact supernet (offering over $10^{76}$ quantized subnets) within\nsubstantially fewer GPU hours than previous methods. Our approach,\nQuantized-for-all (QFA), is the first to seamlessly extend one-shot\nweight-sharing NAS supernet to support subnets with arbitrary ultra-low\nbitwidth mixed-precision quantization policies without retraining. QFA opens up\nnew possibilities in joint hardware-aware neural architecture search and\nquantization. We demonstrate the effectiveness of our method on ImageNet and\nachieve SOTA Top-1 accuracy under a low complexity constraint ($<20$ MFLOPs).\nThe code and models will be made publicly available at\nhttps://github.com/bhpfelix/QFA.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:56:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bai", "Haoping", ""], ["Cao", "Meng", ""], ["Huang", "Ping", ""], ["Shan", "Jiulong", ""]]}, {"id": "2105.08959", "submitter": "Cheng-Yu Tsai", "authors": "Cheng Yu Tsai and Mu-Chun Su", "title": "VSGM -- Enhance robot task understanding ability through visual semantic\n  graph", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, developing AI for robotics has raised much attention. The\ninteraction of vision and language of robots is particularly difficult. We\nconsider that giving robots an understanding of visual semantics and language\nsemantics will improve inference ability. In this paper, we propose a novel\nmethod-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to\nobtain better visual image features, improve the robot's visual understanding\nability. By providing prior knowledge of the robot and detecting the objects in\nthe image, it predicts the correlation between the attributes of the object and\nthe objects and converts them into a graph-based representation; and mapping\nthe object in the image to be a top-down egocentric map. Finally, the important\nobject features of the current task are extracted by Graph Neural Networks. The\nmethod proposed in this paper is verified in the ALFRED (Action Learning From\nRealistic Environments and Directives) dataset. In this dataset, the robot\nneeds to perform daily indoor household tasks following the required language\ninstructions. After the model is added to the VSGM, the task success rate can\nbe improved by 6~10%.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:22:31 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 09:36:36 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Tsai", "Cheng Yu", ""], ["Su", "Mu-Chun", ""]]}, {"id": "2105.08965", "submitter": "Hyunjung Shim Dr.", "authors": "Seungho Lee, Minhyun Lee, Jongwuk Lee and Hyunjung Shim", "title": "Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly\n  Supervised Semantic Segmentation", "comments": "CVPR 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies in weakly-supervised semantic segmentation (WSSS) using\nimage-level weak supervision have several limitations: sparse object coverage,\ninaccurate object boundaries, and co-occurring pixels from non-target objects.\nTo overcome these challenges, we propose a novel framework, namely Explicit\nPseudo-pixel Supervision (EPS), which learns from pixel-level feedback by\ncombining two weak supervisions; the image-level label provides the object\nidentity via the localization map and the saliency map from the off-the-shelf\nsaliency detection model offers rich boundaries. We devise a joint training\nstrategy to fully utilize the complementary relationship between both\ninformation. Our method can obtain accurate object boundaries and discard\nco-occurring pixels, thereby significantly improving the quality of\npseudo-masks. Experimental results show that the proposed method remarkably\noutperforms existing methods by resolving key challenges of WSSS and achieves\nthe new state-of-the-art performance on both PASCAL VOC 2012 and MS COCO 2014\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:31:11 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lee", "Seungho", ""], ["Lee", "Minhyun", ""], ["Lee", "Jongwuk", ""], ["Shim", "Hyunjung", ""]]}, {"id": "2105.08982", "submitter": "Mete Ozay", "authors": "Umberto Michieli and Mete Ozay", "title": "Prototype Guided Federated Learning of Visual Feature Representations", "comments": "11 pages manuscript, 6 pages supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated Learning (FL) is a framework which enables distributed model\ntraining using a large corpus of decentralized training data. Existing methods\naggregate models disregarding their internal representations, which are crucial\nfor training models in vision tasks. System and statistical heterogeneity\n(e.g., highly imbalanced and non-i.i.d. data) further harm model training. To\nthis end, we introduce a method, called FedProto, which computes client\ndeviations using margins of prototypical representations learned on distributed\ndata, and applies them to drive federated optimization via an attention\nmechanism. In addition, we propose three methods to analyse statistical\nproperties of feature representations learned in FL, in order to elucidate the\nrelationship between accuracy, margins and feature discrepancy of FL models. In\nexperimental analyses, FedProto demonstrates state-of-the-art accuracy and\nconvergence rate across image classification and semantic segmentation\nbenchmarks by enabling maximum margin training of FL models. Moreover, FedProto\nreduces uncertainty of predictions of FL models compared to the baseline. To\nour knowledge, this is the first work evaluating FL models in dense prediction\ntasks, such as semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:29:12 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Michieli", "Umberto", ""], ["Ozay", "Mete", ""]]}, {"id": "2105.08993", "submitter": "Junxiao Chen", "authors": "Junxiao Chen, Jia Wei, and Rui Li", "title": "TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality\n  Medical Image Translation", "comments": "10 pages, 3 figures. It has been provisionally accepted for MICCAI\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paired multi-modality medical images, can provide complementary information\nto help physicians make more reasonable decisions than single modality medical\nimages. But they are difficult to generate due to multiple factors in practice\n(e.g., time, cost, radiation dose). To address these problems, multi-modality\nmedical image translation has aroused increasing research interest recently.\nHowever, the existing works mainly focus on translation effect of a whole image\ninstead of a critical target area or Region of Interest (ROI), e.g., organ and\nso on. This leads to poor-quality translation of the localized target area\nwhich becomes blurry, deformed or even with extra unreasonable textures. In\nthis paper, we propose a novel target-aware generative adversarial network\ncalled TarGAN, which is a generic multi-modality medical image translation\nmodel capable of (1) learning multi-modality medical image translation without\nrelying on paired data, (2) enhancing quality of target area generation with\nthe help of target area labels. The generator of TarGAN jointly learns mapping\nat two levels simultaneously - whole image translation mapping and target area\ntranslation mapping. These two mappings are interrelated through a proposed\ncrossing loss. The experiments on both quantitative measures and qualitative\nevaluations demonstrate that TarGAN outperforms the state-of-the-art methods in\nall cases. Subsequent segmentation task is conducted to demonstrate\neffectiveness of synthetic images generated by TarGAN in a real-world\napplication. Our code is available at https://github.com/2165998/TarGAN.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:45:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chen", "Junxiao", ""], ["Wei", "Jia", ""], ["Li", "Rui", ""]]}, {"id": "2105.08994", "submitter": "Ming Sun", "authors": "Ming Sun, Haoxuan Dou, Junjie Yan", "title": "Efficient Transfer Learning via Joint Adaptation of Network Architecture\n  and Weight", "comments": "NAS is one part of transfer learning", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning can boost the performance on the targettask by leveraging\nthe knowledge of the source domain. Recent worksin neural architecture search\n(NAS), especially one-shot NAS, can aidtransfer learning by establishing\nsufficient network search space. How-ever, existing NAS methods tend to\napproximate huge search spaces byexplicitly building giant super-networks with\nmultiple sub-paths, anddiscard super-network weights after a child structure is\nfound. Both thecharacteristics of existing approaches causes repetitive network\ntrainingon source tasks in transfer learning. To remedy the above issues, we\nre-duce the super-network size by randomly dropping connection betweennetwork\nblocks while embedding a larger search space. Moreover, wereuse super-network\nweights to avoid redundant training by proposinga novel framework consisting of\ntwo modules, the neural architecturesearch module for architecture transfer and\nthe neural weight searchmodule for weight transfer. These two modules conduct\nsearch on thetarget task based on a reduced super-networks, so we only need to\ntrainonce on the source task. We experiment our framework on both MS-COCO and\nCUB-200 for the object detection and fine-grained imageclassification tasks,\nand show promising improvements with onlyO(CN)super-network complexity.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:58:04 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sun", "Ming", ""], ["Dou", "Haoxuan", ""], ["Yan", "Junjie", ""]]}, {"id": "2105.08997", "submitter": "Iuliia Pliushch", "authors": "Iuliia Pliushch, Martin Mundt, Nicolas Lupp, Visvanathan Ramesh", "title": "When Deep Classifiers Agree: Analyzing Correlations between Learning\n  Order and Image Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a plethora of architectural variants for deep classification has\nbeen introduced over time, recent works have found empirical evidence towards\nsimilarities in their training process. It has been hypothesized that neural\nnetworks converge not only to similar representations, but also exhibit a\nnotion of empirical agreement on which data instances are learned first.\nFollowing in the latter works$'$ footsteps, we define a metric to quantify the\nrelationship between such classification agreement over time, and posit that\nthe agreement phenomenon can be mapped to core statistics of the investigated\ndataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,\nImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to\nbe independent of specific architectures, training hyper-parameters or labels,\nalbeit follows an ordering according to image statistics.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:03:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pliushch", "Iuliia", ""], ["Mundt", "Martin", ""], ["Lupp", "Nicolas", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "2105.09008", "submitter": "Shyh Yaw Jou", "authors": "Shi-Yao Zhou and Chung-Yen Su", "title": "A Novel lightweight Convolutional Neural Network, ExquisiteNetV2", "comments": "7 pages, 8 figures, 27 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper of ExquisiteNetV1, the ability of classification of\nExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and\nbetter model ExquisiteNetV2. We conduct many experiments to evaluate its\nperformance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known\nmodels on 15 credible datasets under the same condition. According to the\nexperimental results, ExquisiteNetV2 gets the highest classification accuracy\nover half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts\nof parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing\nspeed.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:21:30 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 00:48:49 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:13:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhou", "Shi-Yao", ""], ["Su", "Chung-Yen", ""]]}, {"id": "2105.09034", "submitter": "Keiichiro Shirai", "authors": "Keiichiro Shirai, Tatsuya Baba, Shunsuke Ono, Masahiro Okuda, Yusuke\n  Tatesumi, and Paul Perrotin", "title": "Guided Facial Skin Color Correction", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic image correction method for portrait\nphotographs, which promotes consistency of facial skin color by suppressing\nskin color changes due to background colors. In portrait photographs, skin\ncolor is often distorted due to the lighting environment (e.g., light reflected\nfrom a colored background wall and over-exposure by a camera strobe), and if\nthe photo is artificially combined with another background color, this color\nchange is emphasized, resulting in an unnatural synthesized result. In our\nframework, after roughly extracting the face region and rectifying the skin\ncolor distribution in a color space, we perform color and brightness correction\naround the face in the original image to achieve a proper color balance of the\nfacial image, which is not affected by luminance and background colors. Unlike\nconventional algorithms for color correction, our final result is attained by a\ncolor correction process with a guide image. In particular, our guided image\nfiltering for the color correction does not require a perfectly-aligned guide\nimage required in the original guide image filtering method proposed by He et\nal. Experimental results show that our method generates more natural results\nthan conventional methods on not only headshot photographs but also natural\nscene photographs. We also show automatic yearbook style photo generation as an\nanother application.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:59:55 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Shirai", "Keiichiro", ""], ["Baba", "Tatsuya", ""], ["Ono", "Shunsuke", ""], ["Okuda", "Masahiro", ""], ["Tatesumi", "Yusuke", ""], ["Perrotin", "Paul", ""]]}, {"id": "2105.09063", "submitter": "Hilal Elyousseph", "authors": "Hilal Elyousseph, Majid L Altamimi", "title": "Deep Learning Radio Frequency Signal Classification with Hybrid Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, Deep Learning (DL) has been successfully applied to detect\nand classify Radio Frequency (RF) Signals. A DL approach is especially useful\nsince it identifies the presence of a signal without needing full protocol\ninformation, and can also detect and/or classify non-communication waveforms,\nsuch as radar signals. In this work, we focus on the different pre-processing\nsteps that can be used on the input training data, and test the results on a\nfixed DL architecture. While previous works have mostly focused exclusively on\neither time-domain or frequency domain approaches, we propose a hybrid image\nthat takes advantage of both time and frequency domain information, and tackles\nthe classification as a Computer Vision problem. Our initial results point out\nlimitations to classical pre-processing approaches while also showing that it's\npossible to build a classifier that can leverage the strengths of multiple\nsignal representations.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:12:09 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Elyousseph", "Hilal", ""], ["Altamimi", "Majid L", ""]]}, {"id": "2105.09067", "submitter": "Benjamin Alt", "authors": "Sven Dittus, Benjamin Alt, Andreas Hermann, Darko Katic, Rainer\n  J\\\"akel, J\\\"urgen Fleischer", "title": "Localization and Tracking of User-Defined Points on Deformable Objects\n  for Robotic Manipulation", "comments": "4 pages, 4 figures, accepted at the ICRA 2021 Workshop on\n  Representing and Manipulating Deformable Objects", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an efficient procedure to localize user-defined points\non the surface of deformable objects and track their positions in 3D space over\ntime. To cope with a deformable object's infinite number of DOF, we propose a\ndiscretized deformation field, which is estimated during runtime using a\nmulti-step non-linear solver pipeline. The resulting high-dimensional energy\nminimization problem describes the deviation between an offline-defined\nreference model and a pre-processed camera image. An additional regularization\nterm allows for assumptions about the object's hidden areas and increases the\nsolver's numerical stability. Our approach is capable of solving the\nlocalization problem online in a data-parallel manner, making it ideally\nsuitable for the perception of non-rigid objects in industrial manufacturing\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:25:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dittus", "Sven", ""], ["Alt", "Benjamin", ""], ["Hermann", "Andreas", ""], ["Katic", "Darko", ""], ["J\u00e4kel", "Rainer", ""], ["Fleischer", "J\u00fcrgen", ""]]}, {"id": "2105.09076", "submitter": "Soumyadeep Dey", "authors": "Soumyadeep Dey, Pratik Jawanpuria", "title": "Light-weight Document Image Cleanup using Perceptual Loss", "comments": "Accepted in 16th International Conference on Document Analysis and\n  Recognition 2021 (ICDAR 21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones have enabled effortless capturing and sharing of documents in\ndigital form. The documents, however, often undergo various types of\ndegradation due to aging, stains, or shortcoming of capturing environment such\nas shadow, non-uniform lighting, etc., which reduces the comprehensibility of\nthe document images. In this work, we consider the problem of document image\ncleanup on embedded applications such as smartphone apps, which usually have\nmemory, energy, and latency limitations due to the device and/or for best human\nuser experience. We propose a light-weight encoder decoder based convolutional\nneural network architecture for removing the noisy elements from document\nimages. To compensate for generalization performance with a low network\ncapacity, we incorporate the perceptual loss for knowledge transfer from\npre-trained deep CNN network in our loss function. In terms of the number of\nparameters and product-sum operations, our models are 65-1030 and 3-27 times,\nrespectively, smaller than existing state-of-the-art document enhancement\nmodels. Overall, the proposed models offer a favorable resource versus accuracy\ntrade-off and we empirically illustrate the efficacy of our approach on several\nreal-world benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:54:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dey", "Soumyadeep", ""], ["Jawanpuria", "Pratik", ""]]}, {"id": "2105.09090", "submitter": "Yiming Sun", "authors": "Yiming Sun, Feng Chen, Zhiyu Chen, Mingjie Wang, Ruonan Li", "title": "Local Aggressive Adversarial Attacks on 3D Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are found to be prone to adversarial examples which\ncould deliberately fool the model to make mistakes. Recently, a few of works\nexpand this task from 2D image to 3D point cloud by using global point cloud\noptimization. However, the perturbations of global point are not effective for\nmisleading the victim model. First, not all points are important in\noptimization toward misleading. Abundant points account considerable distortion\nbudget but contribute trivially to attack. Second, the multi-label optimization\nis suboptimal for adversarial attack, since it consumes extra energy in finding\nmulti-label victim model collapse and causes instance transformation to be\ndissimilar to any particular instance. Third, the independent adversarial and\nperceptibility losses, caring misclassification and dissimilarity separately,\ntreat the updating of each point equally without a focus. Therefore, once\nperceptibility loss approaches its budget threshold, all points would be stock\nin the surface of hypersphere and attack would be locked in local optimality.\nTherefore, we propose a local aggressive adversarial attacks (L3A) to solve\nabove issues. Technically, we select a bunch of salient points, the high-score\nsubset of point cloud according to gradient, to perturb. Then a flow of\naggressive optimization strategies are developed to reinforce the unperceptive\ngeneration of adversarial examples toward misleading victim models. Extensive\nexperiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art\nperformance of our method against existing adversarial attack methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 12:22:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sun", "Yiming", ""], ["Chen", "Feng", ""], ["Chen", "Zhiyu", ""], ["Wang", "Mingjie", ""], ["Li", "Ruonan", ""]]}, {"id": "2105.09103", "submitter": "Guo-Wei Yang", "authors": "Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu,\n  Shi-Min Hu", "title": "Recursive-NeRF: An Efficient and Dynamically Growing NeRF", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View synthesis methods using implicit continuous shape representations\nlearned from a set of images, such as the Neural Radiance Field (NeRF) method,\nhave gained increasing attention due to their high quality imagery and\nscalability to high resolution. However, the heavy computation required by its\nvolumetric approach prevents NeRF from being useful in practice; minutes are\ntaken to render a single image of a few megapixels. Now, an image of a scene\ncan be rendered in a level-of-detail manner, so we posit that a complicated\nregion of the scene should be represented by a large neural network while a\nsmall neural network is capable of encoding a simple region, enabling a balance\nbetween efficiency and quality. Recursive-NeRF is our embodiment of this idea,\nproviding an efficient and adaptive rendering and training approach for NeRF.\nThe core of Recursive-NeRF learns uncertainties for query coordinates,\nrepresenting the quality of the predicted color and volumetric intensity at\neach level. Only query coordinates with high uncertainties are forwarded to the\nnext level to a bigger neural network with a more powerful representational\ncapability. The final rendered image is a composition of results from neural\nnetworks of all levels. Our evaluation on three public datasets shows that\nRecursive-NeRF is more efficient than NeRF while providing state-of-the-art\nquality. The code will be available at https://github.com/Gword/Recursive-NeRF.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 12:51:54 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yang", "Guo-Wei", ""], ["Zhou", "Wen-Yang", ""], ["Peng", "Hao-Yang", ""], ["Liang", "Dun", ""], ["Mu", "Tai-Jiang", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2105.09109", "submitter": "Min Yang", "authors": "Cong Xu, Xiang Li and Min Yang", "title": "An Orthogonal Classifier for Improving the Adversarial Robustness of\n  Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are susceptible to artificially designed adversarial\nperturbations. Recent efforts have shown that imposing certain modifications on\nclassification layer can improve the robustness of the neural networks. In this\npaper, we explicitly construct a dense orthogonal weight matrix whose entries\nhave the same magnitude, thereby leading to a novel robust classifier. The\nproposed classifier avoids the undesired structural redundancy issue in\nprevious work. Applying this classifier in standard training on clean data is\nsufficient to ensure the high accuracy and good robustness of the model.\nMoreover, when extra adversarial samples are used, better robustness can be\nfurther obtained with the help of a special worst-case loss. Experimental\nresults show that our method is efficient and competitive to many\nstate-of-the-art defensive approaches. Our code is available at\n\\url{https://github.com/MTandHJ/roboc}.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:12:14 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Xu", "Cong", ""], ["Li", "Xiang", ""], ["Yang", "Min", ""]]}, {"id": "2105.09121", "submitter": "Arian Bakhtiarnia", "authors": "Arian Bakhtiarnia, Qi Zhang and Alexandros Iosifidis", "title": "Single-Layer Vision Transformers for More Accurate Early Exits with Less\n  Overhead", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:30:34 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bakhtiarnia", "Arian", ""], ["Zhang", "Qi", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2105.09124", "submitter": "Juzheng Miao", "authors": "Guang-Quan Zhou, Juzheng Miao, Xin Yang, Rui Li, En-Ze Huo, Wenlong\n  Shi, Yuhao Huang, Jikuan Qian, Chaoyu Chen, Dong Ni", "title": "Learn Fine-grained Adaptive Loss for Multiple Anatomical Landmark\n  Detection in Medical Images", "comments": "12 pages, 10 figures, accepted by IEEE Journal of Biomedical and\n  Health Informatics", "journal-ref": null, "doi": "10.1109/JBHI.2021.3080703", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and accurate detection of anatomical landmarks is an essential\noperation in medical image analysis with a multitude of applications. Recent\ndeep learning methods have improved results by directly encoding the appearance\nof the captured anatomy with the likelihood maps (i.e., heatmaps). However,\nmost current solutions overlook another essence of heatmap regression, the\nobjective metric for regressing target heatmaps and rely on hand-crafted\nheuristics to set the target precision, thus being usually cumbersome and\ntask-specific. In this paper, we propose a novel learning-to-learn framework\nfor landmark detection to optimize the neural network and the target precision\nsimultaneously. The pivot of this work is to leverage the reinforcement\nlearning (RL) framework to search objective metrics for regressing multiple\nheatmaps dynamically during the training process, thus avoiding setting\nproblem-specific target precision. We also introduce an early-stop strategy for\nactive termination of the RL agent's interaction that adapts the optimal\nprecision for separate targets considering exploration-exploitation tradeoffs.\nThis approach shows better stability in training and improved localization\naccuracy in inference. Extensive experimental results on two different\napplications of landmark localization: 1) our in-house prenatal ultrasound (US)\ndataset and 2) the publicly available dataset of cephalometric X-Ray landmark\ndetection, demonstrate the effectiveness of our proposed method. Our proposed\nframework is general and shows the potential to improve the efficiency of\nanatomical landmark detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:39:18 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhou", "Guang-Quan", ""], ["Miao", "Juzheng", ""], ["Yang", "Xin", ""], ["Li", "Rui", ""], ["Huo", "En-Ze", ""], ["Shi", "Wenlong", ""], ["Huang", "Yuhao", ""], ["Qian", "Jikuan", ""], ["Chen", "Chaoyu", ""], ["Ni", "Dong", ""]]}, {"id": "2105.09128", "submitter": "Feras Almasri", "authors": "Feras Almasri, Jurgen Vandendriessche, Laurent Segers, Bruno da Silva,\n  An Braeken, Kris Steenhaut, Abdellah Touhafi and Olivier Debeir", "title": "XCycles Backprojection Acoustic Super-Resolution", "comments": null, "journal-ref": "Sensors 2021, 21, 3453", "doi": "10.3390/s21103453", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer vision community has paid much attention to the development of\nvisible image super-resolution (SR) using deep neural networks (DNNs) and has\nachieved impressive results. The advancement of non-visible light sensors, such\nas acoustic imaging sensors, has attracted much attention, as they allow people\nto visualize the intensity of sound waves beyond the visible spectrum. However,\nbecause of the limitations imposed on acquiring acoustic data, new methods for\nimproving the resolution of the acoustic images are necessary. At this time,\nthere is no acoustic imaging dataset designed for the SR problem. This work\nproposed a novel backprojection model architecture for the acoustic image\nsuper-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset\n(AMIVU). The dataset provides large simulated and real captured images at\ndifferent resolutions. The proposed XCycles BackProjection model (XCBP), in\ncontrast to the feedforward model approach, fully uses the iterative correction\nprocedure in each cycle to reconstruct the residual error correction for the\nencoded features in both low- and high-resolution space. The proposed approach\nwas evaluated on the dataset and showed high outperformance compared to the\nclassical interpolation operators and to the recent feedforward\nstate-of-the-art models. It also contributed to a drastically reduced\nsub-sampling error produced during the data acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:43:15 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Almasri", "Feras", ""], ["Vandendriessche", "Jurgen", ""], ["Segers", "Laurent", ""], ["da Silva", "Bruno", ""], ["Braeken", "An", ""], ["Steenhaut", "Kris", ""], ["Touhafi", "Abdellah", ""], ["Debeir", "Olivier", ""]]}, {"id": "2105.09137", "submitter": "Saumya Banthia", "authors": "Saumya Banthia, Anantha Sharma, Ravi Mangipudi", "title": "TableZa -- A classical Computer Vision approach to Tabular Extraction", "comments": "14 pages, 16 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer aided Tabular Data Extraction has always been a very challenging and\nerror prone task because it demands both Spectral and Spatial Sanity of data.\nIn this paper we discuss an approach for Tabular Data Extraction in the realm\nof document comprehension. Given the different kinds of the Tabular formats\nthat are often found across various documents, we discuss a novel approach\nusing Computer Vision for extraction of tabular data from images or vector\npdf(s) converted to image(s).\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:55:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Banthia", "Saumya", ""], ["Sharma", "Anantha", ""], ["Mangipudi", "Ravi", ""]]}, {"id": "2105.09143", "submitter": "Jun Fu", "authors": "Jun Fu, Chen Hou, Wei Zhou, Jiahua Xu, Zhibo Chen", "title": "Adaptive Hypergraph Convolutional Network for No-Reference 360-degree\n  Image Quality Assessment", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In no-reference 360-degree image quality assessment (NR 360IQA), graph\nconvolutional networks (GCNs), which model interactions between viewports\nthrough graphs, have achieved impressive performance. However, prevailing\nGCN-based NR 360IQA methods suffer from three main limitations. First, they\nonly use high-level features of the distorted image to regress the quality\nscore, while the human visual system (HVS) scores the image based on\nhierarchical features. Second, they simplify complex high-order interactions\nbetween viewports in a pairwise fashion through graphs. Third, in the graph\nconstruction, they only consider spatial locations of viewports, ignoring its\ncontent characteristics. Accordingly, to address these issues, we propose an\nadaptive hypergraph convolutional network for NR 360IQA, denoted as AHGCN.\nSpecifically, we first design a multi-level viewport descriptor for extracting\nhierarchical representations from viewports. Then, we model interactions\nbetween viewports through hypergraphs, where each hyperedge connects two or\nmore viewports. In the hypergraph construction, we build a location-based\nhyperedge and a content-based hyperedge for each viewport. Experimental results\non two public 360IQA databases demonstrate that our proposed approach has a\nclear advantage over state-of-the-art full-reference and no-reference IQA\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:02:48 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Fu", "Jun", ""], ["Hou", "Chen", ""], ["Zhou", "Wei", ""], ["Xu", "Jiahua", ""], ["Chen", "Zhibo", ""]]}, {"id": "2105.09156", "submitter": "Yongxing Dai", "authors": "Yongxing Dai, Xiaotong Li, Jun Liu, Zekun Tong, Ling-Yu Duan", "title": "Generalizable Person Re-identification with Relevance-aware Mixture of\n  Experts", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalizable (DG) person re-identification (ReID) is a challenging\nproblem because we cannot access any unseen target domain data during training.\nAlmost all the existing DG ReID methods follow the same pipeline where they use\na hybrid dataset from multiple source domains for training, and then directly\napply the trained model to the unseen target domains for testing. These methods\noften neglect individual source domains' discriminative characteristics and\ntheir relevances w.r.t. the unseen target domains, though both of which can be\nleveraged to help the model's generalization. To handle the above two issues,\nwe propose a novel method called the relevance-aware mixture of experts\n(RaMoE), using an effective voting-based mixture mechanism to dynamically\nleverage source domains' diverse characteristics to improve the model's\ngeneralization. Specifically, we propose a decorrelation loss to make the\nsource domain networks (experts) keep the diversity and discriminability of\nindividual domains' characteristics. Besides, we design a voting network to\nadaptively integrate all the experts' features into the more generalizable\naggregated features with domain relevance. Considering the target domains'\ninvisibility during training, we propose a novel learning-to-learn algorithm\ncombined with our relation alignment loss to update the voting network.\nExtensive experiments demonstrate that our proposed RaMoE outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:19:34 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dai", "Yongxing", ""], ["Li", "Xiaotong", ""], ["Liu", "Jun", ""], ["Tong", "Zekun", ""], ["Duan", "Ling-Yu", ""]]}, {"id": "2105.09180", "submitter": "Jie Liang", "authors": "Jie Liang, Hui Zeng, Miaomiao Cui, Xuansong Xie, Lei Zhang", "title": "PPR10K: A Large-Scale Portrait Photo Retouching Dataset with\n  Human-Region Mask and Group-Level Consistency", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Different from general photo retouching tasks, portrait photo retouching\n(PPR), which aims to enhance the visual quality of a collection of flat-looking\nportrait photos, has its special and practical requirements such as\nhuman-region priority (HRP) and group-level consistency (GLC). HRP requires\nthat more attention should be paid to human regions, while GLC requires that a\ngroup of portrait photos should be retouched to a consistent tone. Models\ntrained on existing general photo retouching datasets, however, can hardly meet\nthese requirements of PPR. To facilitate the research on this high-frequency\ntask, we construct a large-scale PPR dataset, namely PPR10K, which is the first\nof its kind to our best knowledge. PPR10K contains $1, 681$ groups and $11,\n161$ high-quality raw portrait photos in total. High-resolution segmentation\nmasks of human regions are provided. Each raw photo is retouched by three\nexperts, while they elaborately adjust each group of photos to have consistent\ntones. We define a set of objective measures to evaluate the performance of PPR\nand propose strategies to learn PPR models with good HRP and GLC performance.\nThe constructed PPR10K dataset provides a good benchmark for studying automatic\nPPR methods, and experiments demonstrate that the proposed learning strategies\nare effective to improve the retouching performance. Datasets and codes are\navailable: https://github.com/csjliang/PPR10K.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:55:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Liang", "Jie", ""], ["Zeng", "Hui", ""], ["Cui", "Miaomiao", ""], ["Xie", "Xuansong", ""], ["Zhang", "Lei", ""]]}, {"id": "2105.09188", "submitter": "Jie Liang", "authors": "Jie Liang, Hui Zeng, Lei Zhang", "title": "High-Resolution Photorealistic Image Translation in Real-Time: A\n  Laplacian Pyramid Translation Network", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing image-to-image translation (I2IT) methods are either constrained to\nlow-resolution images or long inference time due to their heavy computational\nburden on the convolution of high-resolution feature maps. In this paper, we\nfocus on speeding-up the high-resolution photorealistic I2IT tasks based on\nclosed-form Laplacian pyramid decomposition and reconstruction. Specifically,\nwe reveal that the attribute transformations, such as illumination and color\nmanipulation, relate more to the low-frequency component, while the content\ndetails can be adaptively refined on high-frequency components. We consequently\npropose a Laplacian Pyramid Translation Network (LPTN) to simultaneously\nperform these two tasks, where we design a lightweight network for translating\nthe low-frequency component with reduced resolution and a progressive masking\nstrategy to efficiently refine the high-frequency ones. Our model avoids most\nof the heavy computation consumed by processing high-resolution feature maps\nand faithfully preserves the image details. Extensive experimental results on\nvarious tasks demonstrate that the proposed method can translate 4K images in\nreal-time using one normal GPU while achieving comparable transformation\nperformance against existing methods. Datasets and codes are available:\nhttps://github.com/csjliang/LPTN.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:05:22 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Liang", "Jie", ""], ["Zeng", "Hui", ""], ["Zhang", "Lei", ""]]}, {"id": "2105.09207", "submitter": "Hiromu Yakura", "authors": "Hiromu Yakura, Yuki Koyama, Masataka Goto", "title": "Tool- and Domain-Agnostic Parameterization of Style Transfer Effects\n  Leveraging Pretrained Perceptual Metrics", "comments": "To appear in Proceedings of the 30th International Joint Conference\n  on Artificial Intelligence (IJCAI 2021); Project page available at\n  https://yumetaro.info/projects/parametric-transcription/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning techniques for style transfer would not be optimal for\ndesign support since their \"one-shot\" transfer does not fit exploratory design\nprocesses. To overcome this gap, we propose parametric transcription, which\ntranscribes an end-to-end style transfer effect into parameter values of\nspecific transformations available in an existing content editing tool. With\nthis approach, users can imitate the style of a reference sample in the tool\nthat they are familiar with and thus can easily continue further exploration by\nmanipulating the parameters. To enable this, we introduce a framework that\nutilizes an existing pretrained model for style transfer to calculate a\nperceptual style distance to the reference sample and uses black-box\noptimization to find the parameters that minimize this distance. Our\nexperiments with various third-party tools, such as Instagram and Blender, show\nthat our framework can effectively leverage deep learning techniques for\ncomputational design support.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:39:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yakura", "Hiromu", ""], ["Koyama", "Yuki", ""], ["Goto", "Masataka", ""]]}, {"id": "2105.09220", "submitter": "Aniket Pramanik", "authors": "Aniket Pramanik, Xiaodong Wu, Mathews Jacob", "title": "Joint Calibrationless Reconstruction and Segmentation of Parallel MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume estimation of brain regions from MRI data is a key problem in many\nclinical applications, where the acquisition of data at high spatial resolution\nis desirable. While parallel MRI and constrained image reconstruction\nalgorithms can accelerate the scans, image reconstruction artifacts are\ninevitable, especially at high acceleration factors. We introduce a novel image\ndomain deep-learning framework for calibrationless parallel MRI reconstruction,\ncoupled with a segmentation network to improve image quality and to reduce the\nvulnerability of current segmentation algorithms to image artifacts resulting\nfrom acceleration. The combination of the proposed image domain deep\ncalibrationless approach with the segmentation algorithm offers improved image\nquality, while increasing the accuracy of the segmentations. The novel\narchitecture with an encoder shared between the reconstruction and segmentation\ntasks is seen to reduce the need for segmented training datasets. In\nparticular, the proposed few-shot training strategy requires only 10% of\nsegmented datasets to offer good performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:04:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pramanik", "Aniket", ""], ["Wu", "Xiaodong", ""], ["Jacob", "Mathews", ""]]}, {"id": "2105.09253", "submitter": "Vaishali Ingale", "authors": "Vaishali Ingale, Rishabh Singh, Pragati Patwal", "title": "Image to Image Translation : Generating maps from satellite images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generation of maps from satellite images is conventionally done by a range of\ntools. Maps became an important part of life whose conversion from satellite\nimages may be a bit expensive but Generative models can pander to this\nchallenge. These models aims at finding the patterns between the input and\noutput image. Image to image translation is employed to convert satellite image\nto corresponding map. Different techniques for image to image translations like\nGenerative adversarial network, Conditional adversarial networks and\nCo-Variational Auto encoders are used to generate the corresponding\nhuman-readable maps for that region, which takes a satellite image at a given\nzoom level as its input. We are training our model on Conditional Generative\nAdversarial Network which comprises of Generator model which which generates\nfake images while the discriminator tries to classify the image as real or fake\nand both these models are trained synchronously in adversarial manner where\nboth try to fool each other and result in enhancing model performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:58:04 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ingale", "Vaishali", ""], ["Singh", "Rishabh", ""], ["Patwal", "Pragati", ""]]}, {"id": "2105.09270", "submitter": "Zhisheng Xiao", "authors": "Zhisheng Xiao, Qing Yan, Yali Amit", "title": "Do We Really Need to Learn Representations from In-domain Data for\n  Outlier Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised outlier detection, which predicts if a test sample is an outlier\nor not using only the information from unlabelled inlier data, is an important\nbut challenging task. Recently, methods based on the two-stage framework\nachieve state-of-the-art performance on this task. The framework leverages\nself-supervised representation learning algorithms to train a feature extractor\non inlier data, and applies a simple outlier detector in the feature space. In\nthis paper, we explore the possibility of avoiding the high cost of training a\ndistinct representation for each outlier detection task, and instead using a\nsingle pre-trained network as the universal feature extractor regardless of the\nsource of in-domain data. In particular, we replace the task-specific feature\nextractor by one network pre-trained on ImageNet with a self-supervised loss.\nIn experiments, we demonstrate competitive or better performance on a variety\nof outlier detection benchmarks compared with previous two-stage methods,\nsuggesting that learning representations from in-domain data may be unnecessary\nfor outlier detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:30:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Xiao", "Zhisheng", ""], ["Yan", "Qing", ""], ["Amit", "Yali", ""]]}, {"id": "2105.09279", "submitter": "Sascha Hornauer", "authors": "Sascha Hornauer, Ke Li, Stella X. Yu, Shabnam Ghaffarzadegan, Liu Ren", "title": "Unsupervised Discriminative Learning of Sounds for Audio Event\n  Classification", "comments": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP) | 978-1-7281-7605-5/20/$31.00 (c) 2021 IEEE |\n  DOI: 10.1109/ICASSP39728.2021.9413482", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9413482", "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent progress in network-based audio event classification has shown the\nbenefit of pre-training models on visual data such as ImageNet. While this\nprocess allows knowledge transfer across different domains, training a model on\nlarge-scale visual datasets is time consuming. On several audio event\nclassification benchmarks, we show a fast and effective alternative that\npre-trains the model unsupervised, only on audio data and yet delivers on-par\nperformance with ImageNet pre-training. Furthermore, we show that our\ndiscriminative audio learning can be used to transfer knowledge across audio\ndatasets and optionally include ImageNet pre-training.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:42:03 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:51:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hornauer", "Sascha", ""], ["Li", "Ke", ""], ["Yu", "Stella X.", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Ren", "Liu", ""]]}, {"id": "2105.09281", "submitter": "Shahrokh Paravarzar", "authors": "Shahrokh Paravarzar, Javaneh Alavi", "title": "A Decade of Research for Image Compression In Multimedia Laboratory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement of technology, we have supercomputers with high\nprocessing power and affordable prices. In addition, using multimedia expanded\nall around the world. This caused a vast use of images and videos in different\nfields. As this kind of data consists of a large amount of information, there\nis a need to use compression methods to store, manage or transfer them better\nand faster. One effective technique, which was introduced is variable\nresolution. This technique stimulates human vision and divides regions in\npictures into two different parts, including the area of interest that needs\nmore detail and periphery parts with less detail. This results in better\ncompression. The variable resolution was used for image, video, and 3D motion\ndata compression. This paper investigates the mentioned technique and some\nother research in this regard.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:11:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Paravarzar", "Shahrokh", ""], ["Alavi", "Javaneh", ""]]}, {"id": "2105.09356", "submitter": "Seyed Saeed Changiz Rezaei", "authors": "Seyed Saeed Changiz Rezaei, Fred X. Han, Di Niu, Mohammad Salameh,\n  Keith Mills, Shuo Lian, Wei Lu, and Shangling Jui", "title": "Generative Adversarial Neural Architecture Search", "comments": "17 pages, 9 figures, 13 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 18:54:44 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:06:50 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 04:49:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Rezaei", "Seyed Saeed Changiz", ""], ["Han", "Fred X.", ""], ["Niu", "Di", ""], ["Salameh", "Mohammad", ""], ["Mills", "Keith", ""], ["Lian", "Shuo", ""], ["Lu", "Wei", ""], ["Jui", "Shangling", ""]]}, {"id": "2105.09365", "submitter": "Onur Boyar", "authors": "Enes Sadi Uysal, M.\\c{S}afak Bilici, B. Selin Zaza, M. Yi\\u{g}it\n  \\\"Ozgen\\c{c}, Onur Boyar", "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel\n  Segmentation", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal Vessel Segmentation is important for the diagnosis of various\ndiseases. The research on retinal vessel segmentation focuses mainly on the\nimprovement of the segmentation model which is usually based on U-Net\narchitecture. In our study, we use the U-Net architecture and we rely on heavy\ndata augmentation in order to achieve better performance. The success of the\ndata augmentation relies on successfully addressing the problem of input\nimages. By analyzing input images and performing the augmentation accordingly\nwe show that the performance of the U-Net model can be increased dramatically.\nResults are reported using the most widely used retina dataset, DRIVE.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:15:31 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 13:04:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Uysal", "Enes Sadi", ""], ["Bilici", "M. \u015eafak", ""], ["Zaza", "B. Selin", ""], ["\u00d6zgen\u00e7", "M. Yi\u011fit", ""], ["Boyar", "Onur", ""]]}, {"id": "2105.09371", "submitter": "Haresh Karnan", "authors": "Haresh Karnan, Garrett Warnell, Xuesu Xiao, Peter Stone", "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous\n  Navigation", "comments": "Under Submission to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:25:23 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Karnan", "Haresh", ""], ["Warnell", "Garrett", ""], ["Xiao", "Xuesu", ""], ["Stone", "Peter", ""]]}, {"id": "2105.09374", "submitter": "Tavi Halperin", "authors": "Tavi Halperin, Hanit Hakim, Orestis Vantzos, Gershon Hochman, Netai\n  Benaim, Lior Sassy, Michael Kupchik, Ofir Bibi, Ohad Fried", "title": "Endless Loops: Detecting and Animating Periodic Patterns in Still Images", "comments": "SIGGRAPH 2021. Project page:\n  https://pub.res.lightricks.com/endless-loops/ . Video:\n  https://youtu.be/8ZYUvxWuD2Y", "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 142. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459935", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present an algorithm for producing a seamless animated loop from a single\nimage. The algorithm detects periodic structures, such as the windows of a\nbuilding or the steps of a staircase, and generates a non-trivial displacement\nvector field that maps each segment of the structure onto a neighboring segment\nalong a user- or auto-selected main direction of motion. This displacement\nfield is used, together with suitable temporal and spatial smoothing, to warp\nthe image and produce the frames of a continuous animation loop. Our\ncinemagraphs are created in under a second on a mobile device. Over 140,000\nusers downloaded our app and exported over 350,000 cinemagraphs. Moreover, we\nconducted two user studies that show that users prefer our method for creating\nsurreal and structured cinemagraphs compared to more manual approaches and\ncompared to previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:39:58 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Halperin", "Tavi", ""], ["Hakim", "Hanit", ""], ["Vantzos", "Orestis", ""], ["Hochman", "Gershon", ""], ["Benaim", "Netai", ""], ["Sassy", "Lior", ""], ["Kupchik", "Michael", ""], ["Bibi", "Ofir", ""], ["Fried", "Ohad", ""]]}, {"id": "2105.09378", "submitter": "Fasil Gadjimuradov", "authors": "Fasil Gadjimuradov, Thomas Benkert, Marcel Dominik Nickel, Andreas\n  Maier", "title": "Robust partial Fourier reconstruction for diffusion-weighted imaging\n  using a recurrent convolutional neural network", "comments": "Submitted to Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose: To develop an algorithm for robust partial Fourier (PF)\nreconstruction applicable to diffusion-weighted (DW) images with non-smooth\nphase variations.\n  Methods: Based on an unrolled proximal splitting algorithm, a neural network\narchitecture is derived which alternates between data consistency operations\nand regularization implemented by recurrent convolutions. In order to exploit\ncorrelations, multiple repetitions of the same slice are jointly reconstructed\nunder consideration of permutation-equivariance. The proposed method is trained\non DW liver data of 60 volunteers and evaluated on retrospectively and\nprospectively sub-sampled data of different anatomies and resolutions. In\naddition, the benefits of using a recurrent network over other unrolling\nstrategies is investigated.\n  Results: Conventional PF techniques can be significantly outperformed in\nterms of quantitative measures as well as perceptual image quality. The\nproposed method is able to generalize well to brain data with contrasts and\nresolution not present in the training set. The reduction in echo time (TE)\nassociated with prospective PF-sampling enables DW imaging with higher signal.\nAlso, the TE increase in acquisitions with higher resolution can be compensated\nfor. It can be shown that unrolling by means of a recurrent network produced\nbetter results than using a weight-shared network or a cascade of networks.\n  Conclusion: This work demonstrates that robust PF reconstruction of DW data\nis feasible even at strong PF factors in applications with severe phase\nvariations. Since the proposed method does not rely on smoothness priors of the\nphase but uses learned recurrent convolutions instead, artifacts of\nconventional PF methods can be avoided.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:00:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gadjimuradov", "Fasil", ""], ["Benkert", "Thomas", ""], ["Nickel", "Marcel Dominik", ""], ["Maier", "Andreas", ""]]}, {"id": "2105.09396", "submitter": "Yufu Wang", "authors": "Yufu Wang, Nikos Kolotouros, Kostas Daniilidis, Marc Badger", "title": "Birds of a Feather: Capturing Avian Shape Models from Images", "comments": "CVPR 2021. Project website: https://yufu-wang.github.io/aves/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals are diverse in shape, but building a deformable shape model for a new\nspecies is not always possible due to the lack of 3D data. We present a method\nto capture new species using an articulated template and images of that\nspecies. In this work, we focus mainly on birds. Although birds represent\nalmost twice the number of species as mammals, no accurate shape model is\navailable. To capture a novel species, we first fit the articulated template to\neach training sample. By disentangling pose and shape, we learn a shape space\nthat captures variation both among species and within each species from image\nevidence. We learn models of multiple species from the CUB dataset, and\ncontribute new species-specific and multi-species shape models that are useful\nfor downstream reconstruction tasks. Using a low-dimensional embedding, we show\nthat our learned 3D shape space better reflects the phylogenetic relationships\namong birds than learned perceptual features.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:53:48 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Yufu", ""], ["Kolotouros", "Nikos", ""], ["Daniilidis", "Kostas", ""], ["Badger", "Marc", ""]]}, {"id": "2105.09401", "submitter": "Lecheng Zheng", "authors": "Lecheng Zheng, Yada Zhu, Jingrui He, and Jinjun Xiong", "title": "Heterogeneous Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 21:01:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zheng", "Lecheng", ""], ["Zhu", "Yada", ""], ["He", "Jingrui", ""], ["Xiong", "Jinjun", ""]]}, {"id": "2105.09405", "submitter": "Berat Kurar Barakat", "authors": "Berat Kurar Barakat, Ahmad Droby, Raid Saabni, and Jihad El-Sana", "title": "Unsupervised learning of text line segmentation by differentiating\n  coarse patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in the field of supervised deep learning for text\nline segmentation, unsupervised deep learning solutions are beginning to gain\npopularity. In this paper, we present an unsupervised deep learning method that\nembeds document image patches to a compact Euclidean space where distances\ncorrespond to a coarse text line pattern similarity. Once this space has been\nproduced, text line segmentation can be easily implemented using standard\ntechniques with the embedded feature vectors. To train the model, we extract\nrandom pairs of document image patches with the assumption that neighbour\npatches contain a similar coarse trend of text lines, whereas if one of them is\nrotated, they contain different coarse trends of text lines. Doing well on this\ntask requires the model to learn to recognize the text lines and their salient\nparts. The benefit of our approach is zero manual labelling effort. We evaluate\nthe method qualitatively and quantitatively on several variants of text line\nsegmentation datasets to demonstrate its effectivity.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 21:21:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 00:39:27 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Barakat", "Berat Kurar", ""], ["Droby", "Ahmad", ""], ["Saabni", "Raid", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2105.09422", "submitter": "Mayukh Bagchi", "authors": "Fausto Giunchiglia and Mayukh Bagchi", "title": "Classifying concepts via visual properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We assume that substances in the world are represented by two types of\nconcepts, namely substance concepts and classification concepts, the former\ninstrumental to (visual) perception, the latter to (language based)\nclassification. Based on this distinction, we introduce a general methodology\nfor building lexico-semantic hierarchies of substance concepts, where nodes are\nannotated with the media, e.g.,videos or photos, from which substance concepts\nare extracted, and are associated with the corresponding classification\nconcepts. The methodology is based on Ranganathan's original faceted approach,\ncontextualized to the problem of classifying substance concepts. The key\nnovelty is that the hierarchy is built exploiting the visual properties of\nsubstance concepts, while the linguistically defined properties of\nclassification concepts are only used to describe substance concepts. The\nvalidity of the approach is exemplified by providing some highlights of an\nongoing project whose goal is to build a large scale multimedia multilingual\nconcept hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:24:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Giunchiglia", "Fausto", ""], ["Bagchi", "Mayukh", ""]]}, {"id": "2105.09437", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J Gangeh, Marcin Plata, Hamid Motahari, Nigel P Duffy", "title": "End-to-End Unsupervised Document Image Blind Denoising", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Removing noise from scanned pages is a vital step before their submission to\noptical character recognition (OCR) system. Most available image denoising\nmethods are supervised where the pairs of noisy/clean pages are required.\nHowever, this assumption is rarely met in real settings. Besides, there is no\nsingle model that can remove various noise types from documents. Here, we\npropose a unified end-to-end unsupervised deep learning model, for the first\ntime, that can effectively remove multiple types of noise, including salt \\&\npepper noise, blurred and/or faded text, as well as watermarks from documents\nat various levels of intensity. We demonstrate that the proposed model\nsignificantly improves the quality of scanned images and the OCR of the pages\non several test datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 23:55:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gangeh", "Mehrdad J", ""], ["Plata", "Marcin", ""], ["Motahari", "Hamid", ""], ["Duffy", "Nigel P", ""]]}, {"id": "2105.09447", "submitter": "Heming Du", "authors": "Heming Du, Xin Yu, Liang Zheng", "title": "VTNet: Visual Transformer Network for Object Goal Navigation", "comments": "accepted paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object goal navigation aims to steer an agent towards a target object based\non observations of the agent. It is of pivotal importance to design effective\nvisual representations of the observed scene in determining navigation actions.\nIn this paper, we introduce a Visual Transformer Network (VTNet) for learning\ninformative visual representation in navigation. VTNet is a highly effective\nstructure that embodies two key properties for visual representations: First,\nthe relationships among all the object instances in a scene are exploited;\nSecond, the spatial locations of objects and image regions are emphasized so\nthat directional navigation signals can be learned. Furthermore, we also\ndevelop a pre-training scheme to associate the visual representations with\nnavigation signals, and thus facilitate navigation policy learning. In a\nnutshell, VTNet embeds object and region features with their location cues as\nspatial-aware descriptors and then incorporates all the encoded descriptors\nthrough attention operations to achieve informative representation for\nnavigation. Given such visual representations, agents are able to explore the\ncorrelations between visual observations and navigation actions. For example,\nan agent would prioritize \"turning right\" over \"turning left\" when the visual\nrepresentation emphasizes on the right side of activation map. Experiments in\nthe artificial environment AI2-Thor demonstrate that VTNet significantly\noutperforms state-of-the-art methods in unseen testing environments.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:23:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Du", "Heming", ""], ["Yu", "Xin", ""], ["Zheng", "Liang", ""]]}, {"id": "2105.09448", "submitter": "Gunjan Chhablani", "authors": "Gunjan Chhablani, Abheesht Sharma, Harshit Pandey, Tirtharaj Dash", "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision", "comments": "6 pages, 1 figure, Under review at ESANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:25:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chhablani", "Gunjan", ""], ["Sharma", "Abheesht", ""], ["Pandey", "Harshit", ""], ["Dash", "Tirtharaj", ""]]}, {"id": "2105.09451", "submitter": "Trung-Nghia Le", "authors": "Trung-Nghia Le, Tam V. Nguyen, Zhongliang Nie, Minh-Triet Tran,\n  Akihiro Sugimoto", "title": "Anabranch Network for Camouflaged Object Segmentation", "comments": "Published in CVIU 2019. Project page:\n  https://sites.google.com/view/ltnghia/research/camo", "journal-ref": "Computer Vision and Image Understanding 184 (2019) 45-56", "doi": "10.1016/j.cviu.2019.04.006", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camouflaged objects attempt to conceal their texture into the background and\ndiscriminating them from the background is hard even for human beings. The main\nobjective of this paper is to explore the camouflaged object segmentation\nproblem, namely, segmenting the camouflaged object(s) for a given image. This\nproblem has not been well studied in spite of a wide range of potential\napplications including the preservation of wild animals and the discovery of\nnew species, surveillance systems, search-and-rescue missions in the event of\nnatural disasters such as earthquakes, floods or hurricanes. This paper\naddresses a new challenging problem of camouflaged object segmentation. To\naddress this problem, we provide a new image dataset of camouflaged objects for\nbenchmarking purposes. In addition, we propose a general end-to-end network,\ncalled the Anabranch Network, that leverages both classification and\nsegmentation tasks. Different from existing networks for segmentation, our\nproposed network possesses the second branch for classification to predict the\nprobability of containing camouflaged object(s) in an image, which is then\nfused into the main branch for segmentation to boost up the segmentation\naccuracy. Extensive experiments conducted on the newly built dataset\ndemonstrate the effectiveness of our network using various fully convolutional\nnetworks. \\url{https://sites.google.com/view/ltnghia/research/camo}\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:52:44 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Le", "Trung-Nghia", ""], ["Nguyen", "Tam V.", ""], ["Nie", "Zhongliang", ""], ["Tran", "Minh-Triet", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "2105.09464", "submitter": "Yongxiang Gu", "authors": "Yongxiang Gu, Xiaolin Qin, Yuncong Peng, Lu Li", "title": "Content-Augmented Feature Pyramid Network with Light Linear Spatial\n  Transformers for Object Detection", "comments": "13 pages,7 figures,6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As one of the prevalent components, Feature Pyramid Network (FPN) is widely\nused in the current object detection models to improve the performance of\nmulti-scale detection. However, its interaction is still in a local and lossy\nmanner, thus limiting the representation power. In this paper, to simulate a\nglobal view of human vision in object detection and address the inherent\ndefects of interaction mode in FPN, we construct a novel architecture termed\nContent-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN,\nwhich fuses features within a local receptive field, CA-FPN can adaptively\naggregate similar features from a global view. It is equipped with a global\ncontent extraction module and light linear spatial transformers. The former\nallows to extract multi-scale context information and the latter can deeply\ncombine the global content extraction module with the vanilla FPN using the\nlinearized attention function, which is designed to reduce model complexity.\nFurthermore, CA-FPN can be readily plugged into existing FPN-based models.\nExtensive experiments on the challenging COCO and PASCAL VOC object detection\ndatasets demonstrated that our CA-FPN significantly outperforms competitive\nFPN-based detectors without bells and whistles. When plugging CA-FPN into\nCascade R-CNN framework built upon a standard ResNet-50 backbone, our method\ncan achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous\nstate-of-the-art by 1.5 AP, demonstrating the potentiality of application.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 02:31:31 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 09:12:24 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gu", "Yongxiang", ""], ["Qin", "Xiaolin", ""], ["Peng", "Yuncong", ""], ["Li", "Lu", ""]]}, {"id": "2105.09491", "submitter": "Zhibo Fan", "authors": "Zhibo Fan, Yuchen Ma, Zeming Li, Jian Sun", "title": "Generalized Few-Shot Object Detection without Forgetting", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently few-shot object detection is widely adopted to deal with\ndata-limited situations. While most previous works merely focus on the\nperformance on few-shot categories, we claim that detecting all classes is\ncrucial as test samples may contain any instances in realistic applications,\nwhich requires the few-shot detector to learn new concepts without forgetting.\nThrough analysis on transfer learning based methods, some neglected but\nbeneficial properties are utilized to design a simple yet effective few-shot\ndetector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the\npretrained RPN and Re-detector to find few-shot class objects without\nforgetting previous knowledge. Extensive experiments on few-shot detection\nbenchmarks show that Retentive R-CNN significantly outperforms state-of-the-art\nmethods on overall performance among all settings as it can achieve competitive\nresults on few-shot classes and does not degrade the base class performance at\nall. Our approach has demonstrated that the long desired never-forgetting\nlearner is available in object detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:25:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Fan", "Zhibo", ""], ["Ma", "Yuchen", ""], ["Li", "Zeming", ""], ["Sun", "Jian", ""]]}, {"id": "2105.09492", "submitter": "Rundi Wu", "authors": "Rundi Wu, Chang Xiao, Changxi Zheng", "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models", "comments": "14 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:29:18 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wu", "Rundi", ""], ["Xiao", "Chang", ""], ["Zheng", "Changxi", ""]]}, {"id": "2105.09511", "submitter": "Shaohua Li", "authors": "Shaohua Li, Xiuchao Sui, Xiangde Luo, Xinxing Xu, Yong Liu, Rick Goh", "title": "Medical Image Segmentation Using Squeeze-and-Expansion Transformers", "comments": "Camera ready for IJCAI'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is important for computer-aided diagnosis. Good\nsegmentation demands the model to see the big picture and fine details\nsimultaneously, i.e., to learn image features that incorporate large context\nwhile keep high spatial resolutions. To approach this goal, the most widely\nused methods -- U-Net and variants, extract and fuse multi-scale features.\nHowever, the fused features still have small \"effective receptive fields\" with\na focus on local image cues, limiting their performance. In this work, we\npropose Segtran, an alternative segmentation framework based on transformers,\nwhich have unlimited \"effective receptive fields\" even at high feature\nresolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:\na squeezed attention block regularizes the self attention of transformers, and\nan expansion block learns diversified representations. Additionally, we propose\na new positional encoding scheme for transformers, imposing a continuity\ninductive bias for images. Experiments were performed on 2D and 3D medical\nimage segmentation tasks: optic disc/cup segmentation in fundus images\n(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain\ntumor segmentation in MRI scans (BraTS'19 challenge). Compared with\nrepresentative existing methods, Segtran consistently achieved the highest\nsegmentation accuracy, and exhibited good cross-domain generalization\ncapabilities. The source code of Segtran is released at\nhttps://github.com/askerlee/segtran.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:45:47 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 12:11:20 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 02:42:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Li", "Shaohua", ""], ["Sui", "Xiuchao", ""], ["Luo", "Xiangde", ""], ["Xu", "Xinxing", ""], ["Liu", "Yong", ""], ["Goh", "Rick", ""]]}, {"id": "2105.09544", "submitter": "Chao Li", "authors": "Miao Liu, Lingni Ma, Kiran Somasundaram, Yin Li, Kristen Grauman,\n  James M. Rehg and Chao Li", "title": "Egocentric Activity Recognition and Localization on a 3D Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Given a video captured from a first person perspective and recorded in a\nfamiliar environment, can we recognize what the person is doing and identify\nwhere the action occurs in the 3D space? We address this challenging problem of\njointly recognizing and localizing actions of a mobile user on a known 3D map\nfrom egocentric videos. To this end, we propose a novel deep probabilistic\nmodel. Our model takes the inputs of a Hierarchical Volumetric Representation\n(HVR) of the environment and an egocentric video, infers the 3D action location\nas a latent variable, and recognizes the action based on the video and\ncontextual cues surrounding its potential locations. To evaluate our model, we\nconduct extensive experiments on a newly collected egocentric video dataset, in\nwhich both human naturalistic actions and photo-realistic 3D environment\nreconstructions are captured. Our method demonstrates strong results on both\naction recognition and 3D action localization across seen and unseen\nenvironments. We believe our work points to an exciting research direction in\nthe intersection of egocentric vision, and 3D scene understanding.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:58:15 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 00:39:56 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liu", "Miao", ""], ["Ma", "Lingni", ""], ["Somasundaram", "Kiran", ""], ["Li", "Yin", ""], ["Grauman", "Kristen", ""], ["Rehg", "James M.", ""], ["Li", "Chao", ""]]}, {"id": "2105.09548", "submitter": "Dengqiang Jia", "authors": "Dengqiang Jia, Shangqi Gao, Qunlong Chen, Xinzhe Luo, Xiahai Zhuang", "title": "A low-rank representation for unsupervised registration of medical\n  images", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration networks have shown great application potentials in medical\nimage analysis. However, supervised training methods have a great demand for\nlarge and high-quality labeled datasets, which is time-consuming and sometimes\nimpractical due to data sharing issues. Unsupervised image registration\nalgorithms commonly employ intensity-based similarity measures as loss\nfunctions without any manual annotations. These methods estimate the\nparameterized transformations between pairs of moving and fixed images through\nthe optimization of the network parameters during training. However, these\nmethods become less effective when the image quality varies, e.g., some images\nare corrupted by substantial noise or artifacts. In this work, we propose a\nnovel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle\nthe problem. We project noisy images into a noise-free low-rank space, and then\ncompute the similarity between the images. Based on the low-rank similarity\nmeasure, we train the registration network to predict the dense deformation\nfields of noisy image pairs. We highlight that the low-rank projection is\nreformulated in a way that the registration network can successfully update\ngradients. With two tasks, i.e., cardiac and abdominal intra-modality\nregistration, we demonstrate that the low-rank representation can boost the\ngeneralization ability and robustness of models as well as bring significant\nimprovements in noisy data registration scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:04:10 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Jia", "Dengqiang", ""], ["Gao", "Shangqi", ""], ["Chen", "Qunlong", ""], ["Luo", "Xinzhe", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2105.09590", "submitter": "Shijie Fang", "authors": "Shijie Fang, Tong Lin", "title": "Intra-Model Collaborative Learning of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, collaborative learning proposed by Song and Chai has achieved\nremarkable improvements in image classification tasks by simultaneously\ntraining multiple classifier heads. However, huge memory footprints required by\nsuch multi-head structures may hinder the training of large-capacity baseline\nmodels. The natural question is how to achieve collaborative learning within a\nsingle network without duplicating any modules. In this paper, we propose four\nways of collaborative learning among different parts of a single network with\nnegligible engineering efforts. To improve the robustness of the network, we\nleverage the consistency of the output layer and intermediate layers for\ntraining under the collaborative learning framework. Besides, the similarity of\nintermediate representation and convolution kernel is also introduced to reduce\nthe reduce redundant in a neural network. Compared to the method of Song and\nChai, our framework further considers the collaboration inside a single model\nand takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100,\nImageNet32 and STL-10 corroborate the effectiveness of these four ways\nseparately while combining them leads to further improvements. In particular,\ntest errors on the STL-10 dataset are decreased by $9.28\\%$ and $5.45\\%$ for\nResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust\nto label noise with experiments on Cifar-10 dataset. For example, our method\nhas $3.53\\%$ higher performance under $50\\%$ noise ratio setting.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:30:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Fang", "Shijie", ""], ["Lin", "Tong", ""]]}, {"id": "2105.09596", "submitter": "Wei Xiang", "authors": "Li Wang, Wei Xiang, Ruhui Xue, Kaida Zou, Laili Zhu", "title": "AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid\n  network of object detection", "comments": "9 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, the anchor-free object detection model has shown great potential\nfor accuracy and speed to exceed anchor-based object detection. Therefore, two\nissues are mainly studied in this article: (1) How to let the backbone network\nin the anchor-free object detection model learn feature extraction? (2) How to\nmake better use of the feature pyramid network? In order to solve the above\nproblems, Experiments show that our model has a certain improvement in accuracy\ncompared with the current popular detection models on the COCO dataset, the\ndesigned attention mechanism module can capture contextual information well,\nimprove detection accuracy, and use sepc network to help balance abstract and\ndetailed information, and reduce the problem of semantic gap in the feature\npyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN,\nor anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get\n39.5% COCO AP under the background of ResNet50.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:41:02 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Li", ""], ["Xiang", "Wei", ""], ["Xue", "Ruhui", ""], ["Zou", "Kaida", ""], ["Zhu", "Laili", ""]]}, {"id": "2105.09597", "submitter": "Yuxiao Chen", "authors": "Yuxiao Chen, Jianbo Yuan, Long Zhao, Rui Luo, Larry Davis, Dimitris N.\n  Metaxas", "title": "More Than Just Attention: Learning Cross-Modal Attentions with\n  Contrastive Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have been widely applied to cross-modal tasks such as\nimage captioning and information retrieval, and have achieved remarkable\nimprovements due to its capability to learn fine-grained relevance across\ndifferent modalities. However, existing attention models could be sub-optimal\nand lack preciseness because there is no direct supervision involved during\ntraining. In this work, we propose Contrastive Content Re-sourcing (CCR) and\nContrastive Content Swapping (CCS) constraints to address such limitation.\nThese constraints supervise the training of attention models in a contrastive\nlearning manner without requiring explicit attention annotations. Additionally,\nwe introduce three metrics, namely Attention Precision, Recall and F1-Score, to\nquantitatively evaluate the attention quality. We evaluate the proposed\nconstraints with cross-modal retrieval (image-text matching) task. The\nexperiments on both Flickr30k and MS-COCO datasets demonstrate that integrating\nthese attention constraints into two state-of-the-art attention-based models\nimproves the model performance in terms of both retrieval accuracy and\nattention metrics.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:48:10 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chen", "Yuxiao", ""], ["Yuan", "Jianbo", ""], ["Zhao", "Long", ""], ["Luo", "Rui", ""], ["Davis", "Larry", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "2105.09600", "submitter": "Zhihao Hu", "authors": "Zhihao Hu, Guo Lu, Dong Xu", "title": "FVC: A New Framework towards Deep Video Compression in Feature Space", "comments": "CVPR2021(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based video compression attracts increasing attention in the past\nfew years. The previous hybrid coding approaches rely on pixel space operations\nto reduce spatial and temporal redundancy, which may suffer from inaccurate\nmotion estimation or less effective motion compensation. In this work, we\npropose a feature-space video coding network (FVC) by performing all major\noperations (i.e., motion estimation, motion compression, motion compensation\nand residual compression) in the feature space. Specifically, in the proposed\ndeformable compensation module, we first apply motion estimation in the feature\nspace to produce motion information (i.e., the offset maps), which will be\ncompressed by using the auto-encoder style network. Then we perform motion\ncompensation by using deformable convolution and generate the predicted\nfeature. After that, we compress the residual feature between the feature from\nthe current frame and the predicted feature from our deformable compensation\nmodule. For better frame reconstruction, the reference features from multiple\nprevious reconstructed frames are also fused by using the non-local attention\nmechanism in the multi-frame feature fusion module. Comprehensive experimental\nresults demonstrate that the proposed framework achieves the state-of-the-art\nperformance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:55:32 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hu", "Zhihao", ""], ["Lu", "Guo", ""], ["Xu", "Dong", ""]]}, {"id": "2105.09624", "submitter": "Janek Gr\\\"ohl", "authors": "Janek Gr\\\"ohl, Melanie Schellenberg, Kris Dreher, Niklas Holzwarth,\n  Minu D. Tizabi, Alexander Seitel, Lena Maier-Hein", "title": "Semantic segmentation of multispectral photoacoustic images using deep\n  learning", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic imaging has the potential to revolutionise healthcare due to\nthe valuable information on tissue physiology that is contained in\nmultispectral photoacoustic measurements. Clinical translation of the\ntechnology requires conversion of the high-dimensional acquired data into\nclinically relevant and interpretable information. In this work, we present a\ndeep learning-based approach to semantic segmentation of multispectral\nphotoacoustic images to facilitate the interpretability of recorded images.\nManually annotated multispectral photoacoustic imaging data are used as gold\nstandard reference annotations and enable the training of a deep learning-based\nsegmentation algorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data of healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualisations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a processing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 09:33:55 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 14:31:03 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Gr\u00f6hl", "Janek", ""], ["Schellenberg", "Melanie", ""], ["Dreher", "Kris", ""], ["Holzwarth", "Niklas", ""], ["Tizabi", "Minu D.", ""], ["Seitel", "Alexander", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2105.09645", "submitter": "Yukai Shi", "authors": "Yukai Shi, Jinghui Qin", "title": "Content-adaptive Representation Learning for Fast Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have attracted great attention in image\nrestoration and enhancement. Generally, restoration quality has been improved\nby building more and more convolutional block. However, these methods mostly\nlearn a specific model to handle all images and ignore difficulty diversity. In\nother words, an area in the image with high frequency tend to lose more\ninformation during compressing while an area with low frequency tends to lose\nless. In this article, we adrress the efficiency issue in image SR by\nincorporating a patch-wise rolling network(PRN) to content-adaptively recover\nimages according to difficulty levels. In contrast to existing studies that\nignore difficulty diversity, we adopt different stage of a neural network to\nperform image restoration. In addition, we propose a rolling strategy that\nutilizes the parameters of each stage more flexible. Extensive experiments\ndemonstrate that our model not only shows a significant acceleration but also\nmaintain state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:24:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Shi", "Yukai", ""], ["Qin", "Jinghui", ""]]}, {"id": "2105.09658", "submitter": "Tomasz Kryjak", "authors": "Marcin Kowalczyk and Tomasz Kryjak", "title": "A Connected Component Labelling algorithm for multi-pixel per clock\n  cycle video stream", "comments": "Submitted to DSD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the hardware implementation of a connected component\nlabelling (CCL) module in reprogammable logic. The main novelty of the design\nis the \"full\", i.e. without any simplifications, support of a 4 pixel per clock\nformat (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x\n2160 pixels) at 60 frames per second. To achieve this, a special labelling\nmethod was designed and a functionality that stops the input data stream in\norder to process pixel groups which require writing more than one merger into\nthe equivalence table. The proposed module was verified in simulation and in\nhardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation\nboard.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:43:58 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kowalczyk", "Marcin", ""], ["Kryjak", "Tomasz", ""]]}, {"id": "2105.09683", "submitter": "Wei Xiang", "authors": "Bo Cheng, Ruhui Xue, Hang Yang, Laili Zhu, and Wei Xiang", "title": "DPN-SENet:A self-attention mechanism neural network for detection and\n  diagnosis of COVID-19 from chest x-ray images", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background and Objective: The new type of coronavirus is also called\nCOVID-19. It began to spread at the end of 2019 and has now spread across the\nworld. Until October 2020, It has infected around 37 million people and claimed\nabout 1 million lives. We propose a deep learning model that can help\nradiologists and clinicians use chest X-rays to diagnose COVID-19 cases and\nshow the diagnostic features of pneumonia. Methods: The approach in this study\nis: 1) we propose a data enhancement method to increase the diversity of the\ndata set, thereby improving the generalization performance of the model. 2) Our\ndeep convolution neural network model DPN-SE adds a self-attention mechanism to\nthe DPN network. The addition of a self-attention mechanism has greatly\nimproved the performance of the network. 3) Use the Lime interpretable library\nto mark the feature regions on the X-ray medical image that helps doctors more\nquickly diagnose COVID-19 in people. Results: Under the same network model, the\ndata with and without data enhancement is put into the model for training\nrespectively. At last, comparing two experimental results: among the 10 network\nmodels with different structures, 7 network models have improved their effects\nafter using data enhancement, with an average improvement of 1% in recognition\naccuracy. We propose that the accuracy and recall rates of the DPN-SE network\nare 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs.\nnormal). Compared with the original DPN, the respective accuracy is improved by\n2%. Conclusion: The data augmentation method we used has achieved effective\nresults on a small amount of data set, showing that a reasonable data\naugmentation method can improve the recognition accuracy without changing the\nsample size and model structure. Overall, the proposed method and model can\neffectively become a very useful tool for clinical radiologists.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:50:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cheng", "Bo", ""], ["Xue", "Ruhui", ""], ["Yang", "Hang", ""], ["Zhu", "Laili", ""], ["Xiang", "Wei", ""]]}, {"id": "2105.09684", "submitter": "Haoyue Bai", "authors": "Haoyue Bai, Song Wen, S.-H. Gary Chan", "title": "Crowd Counting by Self-supervised Transfer Colorization Learning and\n  Global Prior Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled crowd scene images are expensive and scarce. To significantly reduce\nthe requirement of the labeled images, we propose ColorCount, a novel CNN-based\napproach by combining self-supervised transfer colorization learning and global\nprior classification to leverage the abundantly available unlabeled data. The\nself-supervised colorization branch learns the semantics and surface texture of\nthe image by using its color components as pseudo labels. The classification\nbranch extracts global group priors by learning correlations among image\nclusters. Their fused resultant discriminative features (global priors,\nsemantics and textures) provide ample priors for counting, hence significantly\nreducing the requirement of labeled images. We conduct extensive experiments on\nfour challenging benchmarks. ColorCount achieves much better performance as\ncompared with other unsupervised approaches. Its performance is close to the\nsupervised baseline with substantially less labeled data (10\\% of the original\none).\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:54:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bai", "Haoyue", ""], ["Wen", "Song", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "2105.09685", "submitter": "Jaydeep Borkar", "authors": "Jaydeep Borkar, Pin-Yu Chen", "title": "Simple Transparent Adversarial Examples", "comments": "14 pages, 9 figures, Published at ICLR 2021 Workshop on Security and\n  Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:54:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Borkar", "Jaydeep", ""], ["Chen", "Pin-Yu", ""]]}, {"id": "2105.09701", "submitter": "Hao Luo", "authors": "Hao Luo, Weihua Chen, Xianzhe Xu, Jianyang Gu, Yuqi Zhang, Chong Liu,\n  Yiqi Jiang, Shuting He, Fan Wang, Hao Li", "title": "An Empirical Study of Vehicle Re-Identification on the AI City Challenge", "comments": "CVPR 2021 AI CITY CHALLENGE City-Scale Multi-Camera Vehicle\n  Re-Identification Top 1. arXiv admin note: text overlap with arXiv:2004.10547", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces our solution for the Track2 in AI City Challenge 2021\n(AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the\nreal-world data and synthetic data. We mainly focus on four points, i.e.\ntraining data, unsupervised domain-adaptive (UDA) training, post-processing,\nmodel ensembling in this challenge. (1) Both cropping training data and using\nsynthetic data can help the model learn more discriminative features. (2) Since\nthere is a new scenario in the test set that dose not appear in the training\nset, UDA methods perform well in the challenge. (3) Post-processing techniques\nincluding re-ranking, image-to-track retrieval, inter-camera fusion, etc,\nsignificantly improve final performance. (4) We ensemble CNN-based models and\ntransformer-based models which provide different representation diversity. With\naforementioned techniques, our method finally achieves 0.7445 mAP score,\nyielding the first place in the competition. Codes are available at\nhttps://github.com/michuanhaohao/AICITY2021_Track2_DMT.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:20:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Luo", "Hao", ""], ["Chen", "Weihua", ""], ["Xu", "Xianzhe", ""], ["Gu", "Jianyang", ""], ["Zhang", "Yuqi", ""], ["Liu", "Chong", ""], ["Jiang", "Yiqi", ""], ["He", "Shuting", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""]]}, {"id": "2105.09711", "submitter": "Pengxiang Ding", "authors": "Pengxiang Ding and Jianqin Yin", "title": "An Attractor-Guided Neural Networks for Skeleton-Based Human Motion\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint relation modeling is a curial component in human motion prediction.\nMost existing methods tend to design skeletal-based graphs to build the\nrelations among joints, where local interactions between joint pairs are well\nlearned. However, the global coordination of all joints, which reflects human\nmotion's balance property, is usually weakened because it is learned from part\nto whole progressively and asynchronously. Thus, the final predicted motions\nare sometimes unnatural. To tackle this issue, we learn a medium, called\nbalance attractor (BA), from the spatiotemporal features of motion to\ncharacterize the global motion features, which is subsequently used to build\nnew joint relations. Through the BA, all joints are related synchronously, and\nthus the global coordination of all joints can be better learned. Based on the\nBA, we propose our framework, referred to Attractor-Guided Neural Network,\nmainly including Attractor-Based Joint Relation Extractor (AJRE) and\nMulti-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global\nCoordination Extractor (GCE) and Local Interaction Extractor (LIE). The former\npresents the global coordination of all joints, and the latter encodes local\ninteractions between joint pairs. The MTDE is designed to extract dynamic\ninformation from raw position information for effective prediction. Extensive\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:51:39 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ding", "Pengxiang", ""], ["Yin", "Jianqin", ""]]}, {"id": "2105.09720", "submitter": "Thosini Bamunu Mudiyanselage", "authors": "Thosini Bamunu Mudiyanselage, Nipuna Senanayake, Chunyan Ji, Yi Pan\n  and Yanqing Zhang", "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:13:29 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:38:45 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Mudiyanselage", "Thosini Bamunu", ""], ["Senanayake", "Nipuna", ""], ["Ji", "Chunyan", ""], ["Pan", "Yi", ""], ["Zhang", "Yanqing", ""]]}, {"id": "2105.09737", "submitter": "Kasra Arnavaz", "authors": "Kasra Arnavaz, Oswin Krause, Jelena M. Krivokapic, Silja Heilmann,\n  Jakob Andreas B{\\ae}rentzen, Pia Nyeng, Aasa Feragen", "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from\n  Live Imaging 3D Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:35:44 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Arnavaz", "Kasra", ""], ["Krause", "Oswin", ""], ["Krivokapic", "Jelena M.", ""], ["Heilmann", "Silja", ""], ["B\u00e6rentzen", "Jakob Andreas", ""], ["Nyeng", "Pia", ""], ["Feragen", "Aasa", ""]]}, {"id": "2105.09750", "submitter": "Zongcai Du", "authors": "Zongcai Du, Jie Liu, Jie Tang and Gangshan Wu", "title": "Anchor-based Plain Net for Mobile Image Super-Resolution", "comments": "accepted by CVPR2021 MAI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Along with the rapid development of real-world applications, higher\nrequirements on the accuracy and efficiency of image super-resolution (SR) are\nbrought forward. Though existing methods have achieved remarkable success, the\nmajority of them demand plenty of computational resources and large amount of\nRAM, and thus they can not be well applied to mobile device. In this paper, we\naim at designing efficient architecture for 8-bit quantization and deploy it on\nmobile device. First, we conduct an experiment about meta-node latency by\ndecomposing lightweight SR architectures, which determines the portable\noperations we can utilize. Then, we dig deeper into what kind of architecture\nis beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).\nFinally, we adopt quantization-aware training strategy to further boost the\nperformance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in\nterms of PSNR, while satisfying realistic needs at the same time. Code is\navaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:52:53 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Du", "Zongcai", ""], ["Liu", "Jie", ""], ["Tang", "Jie", ""], ["Wu", "Gangshan", ""]]}, {"id": "2105.09783", "submitter": "Binh Nguyen-Thai", "authors": "Binh Nguyen-Thai, Vuong Le, Catherine Morgan, Nadia Badawi, Truyen\n  Tran, and Svetha Venkatesh", "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment\n  from Videos", "comments": "Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)", "journal-ref": null, "doi": "10.1109/JBHI.2021.3077957", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n  Addressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:31:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nguyen-Thai", "Binh", ""], ["Le", "Vuong", ""], ["Morgan", "Catherine", ""], ["Badawi", "Nadia", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2105.09803", "submitter": "Rakshit Kothari", "authors": "Rakshit Kothari, Shalini De Mello, Umar Iqbal, Wonmin Byeon, Seonwook\n  Park, Jan Kautz", "title": "Weakly-Supervised Physically Unconstrained Gaze Estimation", "comments": "CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for physically unconstrained gaze estimation is acquiring\ntraining data with 3D gaze annotations for in-the-wild and outdoor scenarios.\nIn contrast, videos of human interactions in unconstrained environments are\nabundantly available and can be much more easily annotated with frame-level\nactivity labels. In this work, we tackle the previously unexplored problem of\nweakly-supervised gaze estimation from videos of human interactions. We\nleverage the insight that strong gaze-related geometric constraints exist when\npeople perform the activity of \"looking at each other\" (LAEO). To acquire\nviable 3D gaze supervision from LAEO labels, we propose a training algorithm\nalong with several novel loss functions especially designed for the task. With\nweak supervision from two large scale CMU-Panoptic and AVA-LAEO activity\ndatasets, we show significant improvements in (a) the accuracy of\nsemi-supervised gaze estimation and (b) cross-domain generalization on the\nstate-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation\nbenchmark. We open source our code at\nhttps://github.com/NVlabs/weakly-supervised-gaze.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:58:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kothari", "Rakshit", ""], ["De Mello", "Shalini", ""], ["Iqbal", "Umar", ""], ["Byeon", "Wonmin", ""], ["Park", "Seonwook", ""], ["Kautz", "Jan", ""]]}, {"id": "2105.09830", "submitter": "Tonio Weidler", "authors": "Tonio Weidler, Julian Lehnen, Quinton Denman, D\\'avid Seb\\H{o}k,\n  Gerhard Weiss, Kurt Driessens, Mario Senden", "title": "Biologically Inspired Semantic Lateral Connectivity for Convolutional\n  Neural Networks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lateral connections play an important role for sensory processing in visual\ncortex by supporting discriminable neuronal responses even to highly similar\nfeatures. In the present work, we show that establishing a biologically\ninspired Mexican hat lateral connectivity profile along the filter domain can\nsignificantly improve the classification accuracy of a variety of lightweight\nconvolutional neural networks without the addition of trainable network\nparameters. Moreover, we demonstrate that it is possible to analytically\ndetermine the stationary distribution of modulated filter activations and\nthereby avoid using recurrence for modeling temporal dynamics. We furthermore\nreveal that the Mexican hat connectivity profile has the effect of ordering\nfilters in a sequence resembling the topographic organization of feature\nselectivity in early visual cortex. In an ordered filter sequence, this profile\nthen sharpens the filters' tuning curves.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:24:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Weidler", "Tonio", ""], ["Lehnen", "Julian", ""], ["Denman", "Quinton", ""], ["Seb\u0151k", "D\u00e1vid", ""], ["Weiss", "Gerhard", ""], ["Driessens", "Kurt", ""], ["Senden", "Mario", ""]]}, {"id": "2105.09847", "submitter": "Micha\\\"el Fonder", "authors": "Micha\\\"el Fonder and Damien Ernst and Marc Van Droogenbroeck", "title": "M4Depth: A motion-based approach for monocular depth estimation on video\n  sequences", "comments": "Main paper: 8 pages + references, Appendix: 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Getting the distance to objects is crucial for autonomous vehicles. In\ninstances where depth sensors cannot be used, this distance has to be estimated\nfrom RGB cameras. As opposed to cars, the task of estimating depth from\non-board mounted cameras is made complex on drones because of the lack of\nconstrains on motion during flights. In this paper, we present a method to\nestimate the distance of objects seen by an on-board mounted camera by using\nits RGB video stream and drone motion information. Our method is built upon a\npyramidal convolutional neural network architecture and uses time recurrence in\npair with geometric constraints imposed by motion to produce pixel-wise depth\nmaps. In our architecture, each level of the pyramid is designed to produce its\nown depth estimate based on past observations and information provided by the\nprevious level in the pyramid. We introduce a spatial reprojection layer to\nmaintain the spatio-temporal consistency of the data between the levels. We\nanalyse the performance of our approach on Mid-Air, a public drone dataset\nfeaturing synthetic drone trajectories recorded in a wide variety of\nunstructured outdoor environments. Our experiments show that our network\noutperforms state-of-the-art depth estimation methods and that the use of\nmotion information is the main contributing factor for this improvement. The\ncode of our method is publicly available on GitHub; see\nhttps://github.com/michael-fonder/M4Depth\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:46:02 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 09:13:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Fonder", "Micha\u00ebl", ""], ["Ernst", "Damien", ""], ["Van Droogenbroeck", "Marc", ""]]}, {"id": "2105.09848", "submitter": "Yanli Zhou", "authors": "Yanli Zhou, Brenden M. Lake", "title": "Flexible Compositional Learning of Structured Visual Concepts", "comments": "Please cite as: Zhou, Y. and Lake, B. M. (2021). Flexible\n  compositional learning of structured visual concepts. In Proceedings of the\n  43rd Annual Conference of the Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:48:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhou", "Yanli", ""], ["Lake", "Brenden M.", ""]]}, {"id": "2105.09880", "submitter": "William McNally", "authors": "William McNally, Pascale Walters, Kanav Vats, Alexander Wong, John\n  McPhee", "title": "DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in\n  Darts using a Single Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-camera solutions for automatic scorekeeping in steel-tip darts\nare very expensive and thus inaccessible to most players. Motivated to develop\na more accessible low-cost solution, we present a new approach to keypoint\ndetection and apply it to predict dart scores from a single image taken from\nany camera angle. This problem involves detecting multiple keypoints that may\nbe of the same class and positioned in close proximity to one another. The\nwidely adopted framework for regressing keypoints using heatmaps is not\nwell-suited for this task. To address this issue, we instead propose to model\nkeypoints as objects. We develop a deep convolutional neural network around\nthis idea and use it to predict dart locations and dartboard calibration points\nwithin an overall pipeline for automatic dart scoring, which we call DeepDarts.\nAdditionally, we propose several task-specific data augmentation strategies to\nimprove the generalization of our method. As a proof of concept, two datasets\ncomprising 16k images originating from two different dartboard setups were\nmanually collected and annotated to evaluate the system. In the primary dataset\ncontaining 15k images captured from a face-on view of the dartboard using a\nsmartphone, DeepDarts predicted the total score correctly in 94.7% of the test\nimages. In a second more challenging dataset containing limited training data\n(830 images) and various camera angles, we utilize transfer learning and\nextensive data augmentation to achieve a test accuracy of 84.0%. Because\nDeepDarts relies only on single images, it has the potential to be deployed on\nedge devices, giving anyone with a smartphone access to an automatic dart\nscoring system for steel-tip darts. The code and datasets are available.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:25:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["McNally", "William", ""], ["Walters", "Pascale", ""], ["Vats", "Kanav", ""], ["Wong", "Alexander", ""], ["McPhee", "John", ""]]}, {"id": "2105.09899", "submitter": "Ran Zhu", "authors": "Ran Zhu, Mingkun Yang, Wang Liu, Rujun Song, Bo Yan, Zhuoling Xiao", "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual\n  Odometry", "comments": "17 pages,14 figures, Neurocomputing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:05:31 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhu", "Ran", ""], ["Yang", "Mingkun", ""], ["Liu", "Wang", ""], ["Song", "Rujun", ""], ["Yan", "Bo", ""], ["Xiao", "Zhuoling", ""]]}, {"id": "2105.09903", "submitter": "Abhinav Valada", "authors": "Manav Madan, Peter Jakob, Tobias Schmid-Schirling, Abhinav Valada", "title": "Multi-Perspective Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:07:36 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Madan", "Manav", ""], ["Jakob", "Peter", ""], ["Schmid-Schirling", "Tobias", ""], ["Valada", "Abhinav", ""]]}, {"id": "2105.09906", "submitter": "Aditya Bhattacharya", "authors": "Aditya Bhattacharya, Eshwar Shamanna Girishekar, Padmakar Anil\n  Deshpande", "title": "Empirical Analysis of Image Caption Generation using Deep Learning", "comments": "Withdrawing for further updates to the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated image captioning is one of the applications of Deep Learning which\ninvolves fusion of work done in computer vision and natural language\nprocessing, and it is typically performed using Encoder-Decoder architectures.\nIn this project, we have implemented and experimented with various flavors of\nmulti-modal image captioning networks where ResNet101, DenseNet121 and VGG19\nbased CNN Encoders and Attention based LSTM Decoders were explored. We have\nstudied the effect of beam size and the use of pretrained word embeddings and\ncompared them to baseline CNN encoder and RNN decoder architecture. The goal is\nto analyze the performance of each approach using various evaluation metrics\nincluding BLEU, CIDEr, ROUGE and METEOR. We have also explored model\nexplainability using Visual Attention Maps (VAM) to highlight parts of the\nimages which has maximum contribution for predicting each word of the generated\ncaption.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 05:38:13 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 15:17:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bhattacharya", "Aditya", ""], ["Girishekar", "Eshwar Shamanna", ""], ["Deshpande", "Padmakar Anil", ""]]}, {"id": "2105.09907", "submitter": "Xiaoguang Tu", "authors": "Xiaoguang Tu, Jian Zhao, Qiankun Liu, Wenjie Ai, Guodong Guo, Zhifeng\n  Li, Wei Liu, and Jiashi Feng", "title": "Joint Face Image Restoration and Frontalization for Recognition", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3078517", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world scenarios, many factors may harm face recognition performance,\ne.g., large pose, bad illumination,low resolution, blur and noise. To address\nthese challenges, previous efforts usually first restore the low-quality faces\nto high-quality ones and then perform face recognition. However, most of these\nmethods are stage-wise, which is sub-optimal and deviates from the reality. In\nthis paper, we address all these challenges jointly for unconstrained face\nrecognition. We propose an Multi-Degradation Face Restoration (MDFR) model to\nrestore frontalized high-quality faces from the given low-quality ones under\narbitrary facial poses, with three distinct novelties. First, MDFR is a\nwell-designed encoder-decoder architecture which extracts feature\nrepresentation from an input face image with arbitrary low-quality factors and\nrestores it to a high-quality counterpart. Second, MDFR introduces a pose\nresidual learning strategy along with a 3D-based Pose Normalization Module\n(PNM), which can perceive the pose gap between the input initial pose and its\nreal-frontal pose to guide the face frontalization. Finally, MDFR can generate\nfrontalized high-quality face images by a single unified network, showing a\nstrong capability of preserving face identity. Qualitative and quantitative\nexperiments on both controlled and in-the-wild benchmarks demonstrate the\nsuperiority of MDFR over state-of-the-art methods on both face frontalization\nand face restoration.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 03:52:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Tu", "Xiaoguang", ""], ["Zhao", "Jian", ""], ["Liu", "Qiankun", ""], ["Ai", "Wenjie", ""], ["Guo", "Guodong", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""], ["Feng", "Jiashi", ""]]}, {"id": "2105.09908", "submitter": "Filip Biljecki", "authors": "Wangyang Chen, Abraham Noah Wu, Filip Biljecki", "title": "Classification of Urban Morphology with Deep Learning: Application on\n  Urban Vitality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:53:31 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 13:48:38 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chen", "Wangyang", ""], ["Wu", "Abraham Noah", ""], ["Biljecki", "Filip", ""]]}, {"id": "2105.09909", "submitter": "Dipayan Das", "authors": "Dipayan Das, Saumik Bhattacharya, Umapada Pal, and Sukalpa Chanda", "title": "PLSM: A Parallelized Liquid State Machine for Unintentional Action\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reservoir Computing (RC) offers a viable option to deploy AI algorithms on\nlow-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired\nRC model that mimics the cortical microcircuits and uses spiking neural\nnetworks (SNN) that can be directly realized on neuromorphic hardware. In this\npaper, we present a novel Parallelized LSM (PLSM) architecture that\nincorporates spatio-temporal read-out layer and semantic constraints on model\noutput. To the best of our knowledge, such a formulation has been done for the\nfirst time in literature, and it offers a computationally lighter alternative\nto traditional deep-learning models. Additionally, we also present a\ncomprehensive algorithm for the implementation of parallelizable SNNs and LSMs\nthat are GPU-compatible. We implement the PLSM model to classify\nunintentional/accidental video clips, using the Oops dataset. From the\nexperimental results on detecting unintentional action in video, it can be\nobserved that our proposed model outperforms a self-supervised model and a\nfully supervised traditional deep learning model. All the implemented codes can\nbe found at our repository\nhttps://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:10:35 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Das", "Dipayan", ""], ["Bhattacharya", "Saumik", ""], ["Pal", "Umapada", ""], ["Chanda", "Sukalpa", ""]]}, {"id": "2105.09913", "submitter": "Shehan Perera", "authors": "Shehan Perera, Srikar Adhikari, Alper Yilmaz", "title": "POCFormer: A Lightweight Transformer Architecture for Detection of\n  COVID-19 Using Point of Care Ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid and seemingly endless expansion of COVID-19 can be traced back to\nthe inefficiency and shortage of testing kits that offer accurate results in a\ntimely manner. An emerging popular technique, which adopts improvements made in\nmobile ultrasound technology, allows for healthcare professionals to conduct\nrapid screenings on a large scale. We present an image-based solution that aims\nat automating the testing process which allows for rapid mass testing to be\nconducted with or without a trained medical professional that can be applied to\nrural environments and third world countries. Our contributions towards rapid\nlarge-scale testing include a novel deep learning architecture capable of\nanalyzing ultrasound data that can run in real-time and significantly improve\nthe current state-of-the-art detection accuracies using image-based COVID-19\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:14:01 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Perera", "Shehan", ""], ["Adhikari", "Srikar", ""], ["Yilmaz", "Alper", ""]]}, {"id": "2105.09932", "submitter": "Zhijian Liu", "authors": "Zhijian Liu, Alexander Amini, Sibo Zhu, Sertac Karaman, Song Han,\n  Daniela Rus", "title": "Efficient and Robust LiDAR-Based End-to-End Navigation", "comments": "ICRA 2021. The first two authors contributed equally to this work.\n  Project page: https://le2ed.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been used to demonstrate end-to-end neural network learning\nfor autonomous vehicle control from raw sensory input. While LiDAR sensors\nprovide reliably accurate information, existing end-to-end driving solutions\nare mainly based on cameras since processing 3D data requires a large memory\nfootprint and computation cost. On the other hand, increasing the robustness of\nthese systems is also critical; however, even estimating the model's\nuncertainty is very challenging due to the cost of sampling-based methods. In\nthis paper, we present an efficient and robust LiDAR-based end-to-end\nnavigation framework. We first introduce Fast-LiDARNet that is based on sparse\nconvolution kernel optimization and hardware-aware model design. We then\npropose Hybrid Evidential Fusion that directly estimates the uncertainty of the\nprediction from only a single forward pass and then fuses the control\npredictions intelligently. We evaluate our system on a full-scale vehicle and\ndemonstrate lane-stable as well as navigation capabilities. In the presence of\nout-of-distribution events (e.g., sensor failures), our system significantly\nimproves robustness and reduces the number of takeovers in the real world.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:52:37 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Liu", "Zhijian", ""], ["Amini", "Alexander", ""], ["Zhu", "Sibo", ""], ["Karaman", "Sertac", ""], ["Han", "Song", ""], ["Rus", "Daniela", ""]]}, {"id": "2105.09934", "submitter": "John Tsotsos", "authors": "John K. Tsotsos and Jun Luo", "title": "Probing the Effect of Selection Bias on NN Generalization with a Thought\n  Experiment", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Learned networks in the domain of visual recognition and cognition impress in\npart because even though they are trained with datasets many orders of\nmagnitude smaller than the full population of possible images, they exhibit\nsufficient generalization to be applicable to new and previously unseen data.\nAlthough many have examined issues regarding generalization from several\nperspectives, we wondered If a network is trained with a biased dataset that\nmisses particular samples corresponding to some defining domain attribute, can\nit generalize to the full domain from which that training dataset was\nextracted? It is certainly true that in vision, no current training set fully\ncaptures all visual information and this may lead to Selection Bias. Here, we\ntry a novel approach in the tradition of the Thought Experiment. We run this\nthought experiment on a real domain of visual objects that we can fully\ncharacterize and look at specific gaps in training data and their impact on\nperformance requirements. Our thought experiment points to three conclusions:\nfirst, that generalization behavior is dependent on how sufficiently the\nparticular dimensions of the domain are represented during training; second,\nthat the utility of any generalization is completely dependent on the\nacceptable system error; and third, that specific visual features of objects,\nsuch as pose orientations out of the imaging plane or colours, may not be\nrecoverable if not represented sufficiently in a training set. Any currently\nobserved generalization in modern deep learning networks may be more the result\nof coincidental alignments and whose utility needs to be confirmed with respect\nto a system's performance specification. Our Thought Experiment Probe approach,\ncoupled with the resulting Bias Breakdown can be very informative towards\nunderstanding the impact of biases.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:54:48 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Tsotsos", "John K.", ""], ["Luo", "Jun", ""]]}, {"id": "2105.09936", "submitter": "Henry M. Clever", "authors": "Henry M. Clever, Patrick Grady, Greg Turk, and Charles C. Kemp", "title": "BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth\n  Image", "comments": "19 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Contact pressure between the human body and its surroundings has important\nimplications. For example, it plays a role in comfort, safety, posture, and\nhealth. We present a method that infers contact pressure between a human body\nand a mattress from a depth image. Specifically, we focus on using a depth\nimage from a downward facing camera to infer pressure on a body at rest in bed\noccluded by bedding, which is directly applicable to the prevention of pressure\ninjuries in healthcare. Our approach involves augmenting a real dataset with\nsynthetic data generated via a soft-body physics simulation of a human body, a\nmattress, a pressure sensing mat, and a blanket. We introduce a novel deep\nnetwork that we trained on an augmented dataset and evaluated with real data.\nThe network contains an embedded human body mesh model and uses a white-box\nmodel of depth and pressure image generation. Our network successfully infers\nbody pose, outperforming prior work. It also infers contact pressure across a\n3D mesh model of the human body, which is a novel capability, and does so in\nthe presence of occlusion from blankets.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:55:31 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Clever", "Henry M.", ""], ["Grady", "Patrick", ""], ["Turk", "Greg", ""], ["Kemp", "Charles C.", ""]]}, {"id": "2105.09937", "submitter": "Mehdi Moradi", "authors": "Nkechinyere N. Agu, Joy T. Wu, Hanqing Chao, Ismini Lourentzou, Arjun\n  Sharma, Mehdi Moradi, Pingkun Yan, James Hendler", "title": "AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiologists usually observe anatomical regions of chest X-ray images as well\nas the overall image before making a decision. However, most existing deep\nlearning models only look at the entire X-ray image for classification, failing\nto utilize important anatomical information. In this paper, we propose a novel\nmulti-label chest X-ray classification model that accurately classifies the\nimage finding and also localizes the findings to their correct anatomical\nregions. Specifically, our model consists of two modules, the detection module\nand the anatomical dependency module. The latter utilizes graph convolutional\nnetworks, which enable our model to learn not only the label dependency but\nalso the relationship between the anatomical regions in the chest X-ray. We\nfurther utilize a method to efficiently create an adjacency matrix for the\nanatomical regions using the correlation of the label across the different\nregions. Detailed experiments and analysis of our results show the\neffectiveness of our method when compared to the current state-of-the-art\nmulti-label chest X-ray image classification methods while also providing\naccurate location information.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:58:02 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Agu", "Nkechinyere N.", ""], ["Wu", "Joy T.", ""], ["Chao", "Hanqing", ""], ["Lourentzou", "Ismini", ""], ["Sharma", "Arjun", ""], ["Moradi", "Mehdi", ""], ["Yan", "Pingkun", ""], ["Hendler", "James", ""]]}, {"id": "2105.09939", "submitter": "Andrew Brown", "authors": "Andrew Brown, Vicky Kalogeiton, Andrew Zisserman", "title": "Face, Body, Voice: Video Person-Clustering with Multiple Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is person-clustering in videos -- grouping\ncharacters according to their identity. Previous methods focus on the narrower\ntask of face-clustering, and for the most part ignore other cues such as the\nperson's voice, their overall appearance (hair, clothes, posture), and the\nediting structure of the videos. Similarly, most current datasets evaluate only\nthe task of face-clustering, rather than person-clustering. This limits their\napplicability to downstream applications such as story understanding which\nrequire person-level, rather than only face-level, reasoning. In this paper we\nmake contributions to address both these deficiencies: first, we introduce a\nMulti-Modal High-Precision Clustering algorithm for person-clustering in videos\nusing cues from several modalities (face, body, and voice). Second, we\nintroduce a Video Person-Clustering dataset, for evaluating multi-modal\nperson-clustering. It contains body-tracks for each annotated character,\nface-tracks when visible, and voice-tracks when speaking, with their associated\nfeatures. The dataset is by far the largest of its kind, and covers films and\nTV-shows representing a wide range of demographics. Finally, we show the\neffectiveness of using multiple modalities for person-clustering, explore the\nuse of this new broad task for story understanding through character\nco-occurrences, and achieve a new state of the art on all available datasets\nfor face and person-clustering.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:59:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Brown", "Andrew", ""], ["Kalogeiton", "Vicky", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2105.09967", "submitter": "Boaz Shmueli", "authors": "Boaz Shmueli, Soumya Ray, Lun-Wei Ku", "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on\n  Twitter", "comments": "To be published in ACL 2021. 7 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:01:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Shmueli", "Boaz", ""], ["Ray", "Soumya", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2105.09975", "submitter": "Sara Mousavi", "authors": "Sara Mousavi, Zhenning Yang, Kelley Cross, Dawnie Steadman, Audris\n  Mockus", "title": "Pseudo Pixel-level Labeling for Images with Evolving Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Annotating images for semantic segmentation requires intense manual labor and\nis a time-consuming and expensive task especially for domains with a scarcity\nof experts, such as Forensic Anthropology. We leverage the evolving nature of\nimages depicting the decay process in human decomposition data to design a\nsimple yet effective pseudo-pixel-level label generation technique to reduce\nthe amount of effort for manual annotation of such images. We first identify\nsequences of images with a minimum variation that are most suitable to share\nthe same or similar annotation using an unsupervised approach. Given one\nuser-annotated image in each sequence, we propagate the annotation to the\nremaining images in the sequence by merging it with annotations produced by a\nstate-of-the-art CAM-based pseudo label generation technique. To evaluate the\nquality of our pseudo-pixel-level labels, we train two semantic segmentation\nmodels with VGG and ResNet backbones on images labeled using our pseudo\nlabeling method and those of a state-of-the-art method. The results indicate\nthat using our pseudo-labels instead of those generated using the\nstate-of-the-art method in the training process improves the mean-IoU and the\nfrequency-weighted-IoU of the VGG and ResNet-based semantic segmentation models\nby 3.36%, 2.58%, 10.39%, and 12.91% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:14:19 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Mousavi", "Sara", ""], ["Yang", "Zhenning", ""], ["Cross", "Kelley", ""], ["Steadman", "Dawnie", ""], ["Mockus", "Audris", ""]]}, {"id": "2105.09993", "submitter": "Kai Han", "authors": "Kai Han and Kwan-Yee K. Wong and Miaomiao Liu", "title": "Dense Reconstruction of Transparent Objects by Altering Incident Light\n  Paths Through Refraction", "comments": "International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": "10.1007/s11263-017-1045-3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of reconstructing the surface shape of\ntransparent objects. The difficulty of this problem originates from the\nviewpoint dependent appearance of a transparent object, which quickly makes\nreconstruction methods tailored for diffuse surfaces fail disgracefully. In\nthis paper, we introduce a fixed viewpoint approach to dense surface\nreconstruction of transparent objects based on refraction of light. We present\na simple setup that allows us to alter the incident light paths before light\nrays enter the object by immersing the object partially in a liquid, and\ndevelop a method for recovering the object surface through reconstructing and\ntriangulating such incident light paths. Our proposed approach does not need to\nmodel the complex interactions of light as it travels through the object,\nneither does it assume any parametric form for the object shape nor the exact\nnumber of refractions and reflections taken place along the light paths. It can\ntherefore handle transparent objects with a relatively complex shape and\nstructure, with unknown and inhomogeneous refractive index. We also show that\nfor thin transparent objects, our proposed acquisition setup can be further\nsimplified by adopting a single refraction approximation. Experimental results\non both synthetic and real data demonstrate the feasibility and accuracy of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:01:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Han", "Kai", ""], ["Wong", "Kwan-Yee K.", ""], ["Liu", "Miaomiao", ""]]}, {"id": "2105.09996", "submitter": "Hu Xu", "authors": "Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh,\n  Christoph Feichtenhofer, Florian Metze, Luke Zettlemoyer", "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video\n  Understanding", "comments": "9 pages, ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:13:27 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Hu", ""], ["Ghosh", "Gargi", ""], ["Huang", "Po-Yao", ""], ["Arora", "Prahal", ""], ["Aminzadeh", "Masoumeh", ""], ["Feichtenhofer", "Christoph", ""], ["Metze", "Florian", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2105.10005", "submitter": "C.-H. Huck Yang", "authors": "C.-H. Huck Yang, Mohit Chhabra, Y.-C. Liu, Quan Kong, Tomoaki\n  Yoshinaga, Tomokazu Murakami", "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments", "comments": "Accepted to IEEE ICIP 2021", "journal-ref": "2021 IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:38:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:29:32 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 01:36:22 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 06:52:21 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Chhabra", "Mohit", ""], ["Liu", "Y. -C.", ""], ["Kong", "Quan", ""], ["Yoshinaga", "Tomoaki", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "2105.10013", "submitter": "Hugo Oliveira", "authors": "Marcos Vendramini and Hugo Oliveira and Alexei Machado and Jefersson\n  A. dos Santos", "title": "Opening Deep Neural Networks with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:02:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 20:34:15 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 03:00:04 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Vendramini", "Marcos", ""], ["Oliveira", "Hugo", ""], ["Machado", "Alexei", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "2105.10014", "submitter": "Florence Carton", "authors": "Florence Carton, David Filliat, Jaonary Rabarisoa and Quoc Cuong Pham", "title": "Evaluating Robustness over High Level Driving Instruction for Autonomous\n  Driving", "comments": "Accepted to IV21, 32nd IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have witnessed increasingly high performance in the field\nof autonomous end-to-end driving. In particular, more and more research is\nbeing done on driving in urban environments, where the car has to follow high\nlevel commands to navigate. However, few evaluations are made on the ability of\nthese agents to react in an unexpected situation. Specifically, no evaluations\nare conducted on the robustness of driving agents in the event of a bad\nhigh-level command. We propose here an evaluation method, namely a benchmark\nthat allows to assess the robustness of an agent, and to appreciate its\nunderstanding of the environment through its ability to keep a safe behavior,\nregardless of the instruction.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:10:14 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Carton", "Florence", ""], ["Filliat", "David", ""], ["Rabarisoa", "Jaonary", ""], ["Pham", "Quoc Cuong", ""]]}, {"id": "2105.10026", "submitter": "Adyasha Maharana", "authors": "Adyasha Maharana, Darryl Hannan, Mohit Bansal", "title": "Improving Generation and Evaluation of Visual Stories via Semantic\n  Consistency", "comments": "NAACL 2021 (16 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story visualization is an under-explored task that falls at the intersection\nof many important research directions in both computer vision and natural\nlanguage processing. In this task, given a series of natural language captions\nwhich compose a story, an agent must generate a sequence of images that\ncorrespond to the captions. Prior work has introduced recurrent generative\nmodels which outperform text-to-image synthesis models on this task. However,\nthere is room for improvement of generated images in terms of visual quality,\ncoherence and relevance. We present a number of improvements to prior modeling\napproaches, including (1) the addition of a dual learning framework that\nutilizes video captioning to reinforce the semantic alignment between the story\nand generated images, (2) a copy-transform mechanism for\nsequentially-consistent story visualization, and (3) MART-based transformers to\nmodel complex interactions between frames. We present ablation studies to\ndemonstrate the effect of each of these techniques on the generative power of\nthe model for both individual images as well as the entire narrative.\nFurthermore, due to the complexity and generative nature of the task, standard\nevaluation metrics do not accurately reflect performance. Therefore, we also\nprovide an exploration of evaluation metrics for the model, focused on aspects\nof the generated frames such as the presence/quality of generated characters,\nthe relevance to captions, and the diversity of the generated images. We also\npresent correlation experiments of our proposed automated metrics with human\nevaluations. Code and data available at:\nhttps://github.com/adymaharana/StoryViz\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:42:42 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Maharana", "Adyasha", ""], ["Hannan", "Darryl", ""], ["Bansal", "Mohit", ""]]}, {"id": "2105.10063", "submitter": "Ezequiel Santos", "authors": "Ezequiel Fran\\c{c}a dos Santos, Gabriel Fontenelle", "title": "Uma implementa\\c{c}\\~ao do jogo Pedra, Papel e Tesoura utilizando Visao\n  Computacional", "comments": "14 pages, in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a game, controlled by computer vision, in identification\nof hand gestures (hand-tracking). The proposed work is based on image\nsegmentation and construction of a convex hull with Jarvis Algorithm , and\ndetermination of the pattern based on the extraction of area characteristics in\nthe convex hull.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 23:11:36 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Santos", "Ezequiel Fran\u00e7a dos", ""], ["Fontenelle", "Gabriel", ""]]}, {"id": "2105.10076", "submitter": "Suren Sritharan", "authors": "Harshana Weligampola, Gihan Jayatilaka, Suren Sritharan, Parakrama\n  Ekanayake, Roshan Ragel, Vijitha Herath, Roshan Godaliyadda", "title": "An Optical physics inspired CNN approach for intrinsic image\n  decomposition", "comments": "5 pages, 3 figures, 1 table, ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic Image Decomposition is an open problem of generating the\nconstituents of an image. Generating reflectance and shading from a single\nimage is a challenging task specifically when there is no ground truth. There\nis a lack of unsupervised learning approaches for decomposing an image into\nreflectance and shading using a single image. We propose a neural network\narchitecture capable of this decomposition using physics-based parameters\nderived from the image. Through experimental results, we show that (a) the\nproposed methodology outperforms the existing deep learning-based IID\ntechniques and (b) the derived parameters improve the efficacy significantly.\nWe conclude with a closer analysis of the results (numerical and example\nimages) showing several avenues for improvement.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 00:54:01 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Weligampola", "Harshana", ""], ["Jayatilaka", "Gihan", ""], ["Sritharan", "Suren", ""], ["Ekanayake", "Parakrama", ""], ["Ragel", "Roshan", ""], ["Herath", "Vijitha", ""], ["Godaliyadda", "Roshan", ""]]}, {"id": "2105.10081", "submitter": "Rodina Bassiouny Mrs.", "authors": "Rodina Bassiouny (1), Adel Mohamed (2), Karthi Umapathy (1) and Naimul\n  Khan (1) ((1) Ryerson University, Toronto, Canada, (2) Mount Sinai Hospital,\n  University of Toronto, Toronto, Canada)", "title": "An interpretable object detection based model for the diagnosis of\n  neonatal lung diseases using Ultrasound images", "comments": "7 pages, 8 figures, 4 tables, full paper has been submitted to the\n  2021 43rd Annual International Conference of the IEEE Engineering in Medicine\n  & Biology Society (EMBC) (EMBC 2021) at the Expo Guadalajara, Mexico, on Oct\n  31 - Nov 4, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, Lung Ultrasound (LUS) has been increasingly used\nto diagnose and monitor different lung diseases in neonates. It is a non\ninvasive tool that allows a fast bedside examination while minimally handling\nthe neonate. Acquiring a LUS scan is easy, but understanding the artifacts\nconcerned with each respiratory disease is challenging. Mixed artifact patterns\nfound in different respiratory diseases may limit LUS readability by the\noperator. While machine learning (ML), especially deep learning can assist in\nautomated analysis, simply feeding the ultrasound images to an ML model for\ndiagnosis is not enough to earn the trust of medical professionals. The\nalgorithm should output LUS features that are familiar to the operator instead.\nTherefore, in this paper we present a unique approach for extracting seven\nmeaningful LUS features that can be easily associated with a specific\npathological lung condition: Normal pleura, irregular pleura, thick pleura,\nAlines, Coalescent B-lines, Separate B-lines and Consolidations. These\nartifacts can lead to early prediction of infants developing later respiratory\ndistress symptoms. A single multi-class region proposal-based object detection\nmodel faster-RCNN (fRCNN) was trained on lower posterior lung ultrasound videos\nto detect these LUS features which are further linked to four common neonatal\ndiseases. Our results show that fRCNN surpasses single stage models such as\nRetinaNet and can successfully detect the aforementioned LUS features with a\nmean average precision of 86.4%. Instead of a fully automatic diagnosis from\nimages without any interpretability, detection of such LUS features leave the\nultimate control of diagnosis to the clinician, which can result in a more\ntrustworthy intelligent system.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:12:35 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bassiouny", "Rodina", ""], ["Mohamed", "Adel", ""], ["Umapathy", "Karthi", ""], ["Khan", "Naimul", ""]]}, {"id": "2105.10087", "submitter": "Zhehua Mao", "authors": "Zhehua Mao, Liang Zhao, Shoudong Huang, Yiting Fan, and Alex Pui-Wai\n  Lee", "title": "Direct Simultaneous Multi-Image Registration", "comments": "10 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel algorithm that registers a collection of\nmono-modal 3D images in a simultaneous fashion, named as Direct Simultaneous\nRegistration (DSR). The algorithm optimizes global poses of local frames\ndirectly based on the intensities of images (without extracting features from\nthe images). To obtain the optimal result, we start with formulating a Direct\nBundle Adjustment (DBA) problem which jointly optimizes pose parameters of\nlocal frames and intensities of panoramic image. By proving the independence of\nthe pose from panoramic image in the iterative process, DSR is proposed and\nproved to be able to generate the same optimal poses as DBA, but without\noptimizing the intensities of the panoramic image. The proposed DSR method is\nparticularly suitable in mono-modal registration and in the scenarios where\ndistinct features are not available, such as Transesophageal Echocardiography\n(TEE) images. The proposed method is validated via simulated and in-vivo 3D TEE\nimages. It is shown that the proposed method outperforms conventional\nsequential registration method in terms of accuracy and the obtained results\ncan produce good alignment in in-vivo images.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:42:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Mao", "Zhehua", ""], ["Zhao", "Liang", ""], ["Huang", "Shoudong", ""], ["Fan", "Yiting", ""], ["Lee", "Alex Pui-Wai", ""]]}, {"id": "2105.10104", "submitter": "Lin Xu", "authors": "Leilei Cao, Yao Xiao, and Lin Xu", "title": "EMface: Detecting Hard Faces by Exploring Receptive Field Pyraminds", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale variation is one of the most challenging problems in face detection.\nModern face detectors employ feature pyramids to deal with scale variation.\nHowever, it might break the feature consistency across different scales of\nfaces. In this paper, we propose a simple yet effective method named the\nreceptive field pyramids (RFP) method to enhance the representation ability of\nfeature pyramids. It can learn different receptive fields in each feature map\nadaptively based on the varying scales of detected faces. Empirical results on\ntwo face detection benchmark datasets, i.e., WIDER FACE and UFDD, demonstrate\nthat our proposed method can accelerate the inference rate significantly while\nachieving state-of-the-art performance. The source code of our method is\navailable at \\url{https://github.com/emdata-ailab/EMface}.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 03:01:37 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cao", "Leilei", ""], ["Xiao", "Yao", ""], ["Xu", "Lin", ""]]}, {"id": "2105.10110", "submitter": "Ge-Peng Ji", "authors": "Yingxia Jiao, Xiao Wang, Yu-Cheng Chou, Shouyuan Yang, Ge-Peng Ji,\n  Rong Zhu, Ge Gao", "title": "Guidance and Teaching Network for Video Salient Object Detection", "comments": "Accepted at IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Owing to the difficulties of mining spatial-temporal cues, the existing\napproaches for video salient object detection (VSOD) are limited in\nunderstanding complex and noisy scenarios, and often fail in inferring\nprominent objects. To alleviate such shortcomings, we propose a simple yet\nefficient architecture, termed Guidance and Teaching Network (GTNet), to\nindependently distil effective spatial and temporal cues with implicit guidance\nand explicit teaching at feature- and decision-level, respectively. To be\nspecific, we (a) introduce a temporal modulator to implicitly bridge features\nfrom motion into the appearance branch, which is capable of fusing cross-modal\nfeatures collaboratively, and (b) utilise motion-guided mask to propagate the\nexplicit cues during the feature aggregation. This novel learning strategy\nachieves satisfactory results via decoupling the complex spatial-temporal cues\nand mapping informative cues across different modalities. Extensive experiments\non three challenging benchmarks show that the proposed method can run at ~28\nfps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 03:25:38 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 15:20:21 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 13:55:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jiao", "Yingxia", ""], ["Wang", "Xiao", ""], ["Chou", "Yu-Cheng", ""], ["Yang", "Shouyuan", ""], ["Ji", "Ge-Peng", ""], ["Zhu", "Rong", ""], ["Gao", "Ge", ""]]}, {"id": "2105.10112", "submitter": "Lin Xu", "authors": "Zhiyuan Chen, Guang Yao, Wennan Ma, Lin Xu", "title": "IDEAL: Independent Domain Embedding Augmentation Learning", "comments": "11 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many efforts have been devoted to designing sampling, mining, and weighting\nstrategies in high-level deep metric learning (DML) loss objectives. However,\nlittle attention has been paid to low-level but essential data transformation.\nIn this paper, we develop a novel mechanism, the independent domain embedding\naugmentation learning ({IDEAL}) method. It can simultaneously learn multiple\nindependent embedding spaces for multiple domains generated by predefined data\ntransformations. Our IDEAL is orthogonal to existing DML techniques and can be\nseamlessly combined with prior DML approaches for enhanced performance.\nEmpirical results on visual retrieval tasks demonstrate the superiority of the\nproposed method. For example, the IDEAL improves the performance of MS loss by\na large margin, 84.5\\% $\\rightarrow$ 87.1\\% on Cars-196, and 65.8\\%\n$\\rightarrow$ 69.5\\% on CUB-200 at Recall$@1$. Our IDEAL with MS loss also\nachieves the new state-of-the-art performance on three image retrieval\nbenchmarks, \\ie, \\emph{Cars-196}, \\emph{CUB-200}, and \\emph{SOP}. It\noutperforms the most recent DML approaches, such as Circle loss and XBM,\nsignificantly. The source code and pre-trained models of our method will be\navailable at\\emph{\\url{https://github.com/emdata-ailab/IDEAL}}.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 03:40:24 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Zhiyuan", ""], ["Yao", "Guang", ""], ["Ma", "Wennan", ""], ["Xu", "Lin", ""]]}, {"id": "2105.10123", "submitter": "Aniruddha Saha", "authors": "Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed\n  Pirsiavash", "title": "Backdoor Attacks on Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale unlabeled data has allowed recent progress in self-supervised\nlearning methods that learn rich visual representations. State-of-the-art\nself-supervised methods for learning representations from images (MoCo and\nBYOL) use an inductive bias that different augmentations (e.g. random crops) of\nan image should produce similar embeddings. We show that such methods are\nvulnerable to backdoor attacks where an attacker poisons a part of the\nunlabeled data by adding a small trigger (known to the attacker) to the images.\nThe model performance is good on clean test images but the attacker can\nmanipulate the decision of the model by showing the trigger at test time.\nBackdoor attacks have been studied extensively in supervised learning and to\nthe best of our knowledge, we are the first to study them for self-supervised\nlearning. Backdoor attacks are more practical in self-supervised learning since\nthe unlabeled data is large and as a result, an inspection of the data to avoid\nthe presence of poisoned data is prohibitive. We show that in our targeted\nattack, the attacker can produce many false positives for the target category\nby using the trigger at test time. We also propose a knowledge distillation\nbased defense algorithm that succeeds in neutralizing the attack. Our code is\navailable here: https://github.com/UMBCvision/SSL-Backdoor .\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 04:22:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Saha", "Aniruddha", ""], ["Tejankar", "Ajinkya", ""], ["Koohpayegani", "Soroush Abbasi", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "2105.10129", "submitter": "Abheesht Sharma", "authors": "Mansi Sharma, Abheesht Sharma, Kadvekar Rohit Tushar, Avinash Panneer", "title": "A Novel 3D-UNet Deep Learning Framework Based on High-Dimensional\n  Bilateral Grid for Edge Consistent Single Image Depth Estimation", "comments": "8 pages, 5 figures, accepted at IC3D 2020", "journal-ref": "In 2020 International Conference on 3D Immersion (IC3D), IEEE,\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of predicting smooth and edge-consistent depth maps is notoriously\ndifficult for single image depth estimation. This paper proposes a novel\nBilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that\nparameterizes high dimensional feature space by encoding compact 3D bilateral\ngrids with UNets and infers sharp geometric layout of the scene. Further,\nanother novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for\ninferring an accurate depth map given a single color view. The 3DBGES-UNet\nconcatenates 3DBG-UNet geometry map with the inception network edge\naccentuation map and a spatial object's boundary map obtained by leveraging\nsemantic segmentation and train the UNet model with ResNet backbone. Both\nmodels are designed with a particular attention to explicitly account for edges\nor minute details. Preserving sharp discontinuities at depth edges is critical\nfor many applications such as realistic integration of virtual objects in AR\nvideo or occlusion-aware view synthesis for 3D display applications.The\nproposed depth prediction network achieves state-of-the-art performance in both\nqualitative and quantitative evaluations on the challenging NYUv2-Depth data.\nThe code and corresponding pre-trained weights will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 04:53:14 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sharma", "Mansi", ""], ["Sharma", "Abheesht", ""], ["Tushar", "Kadvekar Rohit", ""], ["Panneer", "Avinash", ""]]}, {"id": "2105.10131", "submitter": "Koji Mineshima", "authors": "Yuri Sato, Koji Mineshima, Kazuhiro Ueda", "title": "Visual representation of negation: Real world data analysis on comic\n  image design", "comments": "To appear in Proceedings of the 43rd Annual Conference of the\n  Cognitive Science Society (CogSci 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a widely held view that visual representations (e.g.,\nphotographs and illustrations) do not depict negation, for example, one that\ncan be expressed by a sentence \"the train is not coming\". This view is\nempirically challenged by analyzing the real-world visual representations of\ncomic (manga) illustrations. In the experiment using image captioning tasks, we\ngave people comic illustrations and asked them to explain what they could read\nfrom them. The collected data showed that some comic illustrations could depict\nnegation without any aid of sequences (multiple panels) or conventional devices\n(special symbols). This type of comic illustrations was subjected to further\nexperiments, classifying images into those containing negation and those not\ncontaining negation. While this image classification was easy for humans, it\nwas difficult for data-driven machines, i.e., deep learning models (CNN), to\nachieve the same high performance. Given the findings, we argue that some comic\nillustrations evoke background knowledge and thus can depict negation with\npurely visual elements.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 04:57:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sato", "Yuri", ""], ["Mineshima", "Koji", ""], ["Ueda", "Kazuhiro", ""]]}, {"id": "2105.10142", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Alois Knoll, Hsuan-Cheng Liao", "title": "Safety Metrics for Semantic Segmentation in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of autonomous driving, safety-related metrics for deep\nneural networks have been widely studied for image classification and object\ndetection. In this paper, we further consider safety-aware correctness and\nrobustness metrics specialized for semantic segmentation. The novelty of our\nproposal is to move beyond pixel-level metrics: Given two images with each\nhaving N pixels being class-flipped, the designed metrics should, depending on\nthe clustering of pixels being class-flipped or the location of occurrence,\nreflect a different level of safety criticality. The result evaluated on an\nautonomous driving dataset demonstrates the validity and practicality of our\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 05:59:49 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["Knoll", "Alois", ""], ["Liao", "Hsuan-Cheng", ""]]}, {"id": "2105.10154", "submitter": "Lumin Xu", "authors": "Lumin Xu, Yingda Guan, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo,\n  Wanli Ouyang, Xiaogang Wang", "title": "ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation has achieved significant progress in recent years.\nHowever, most of the recent methods focus on improving accuracy using\ncomplicated models and ignoring real-time efficiency. To achieve a better\ntrade-off between accuracy and efficiency, we propose a novel neural\narchitecture search (NAS) method, termed ViPNAS, to search networks in both\nspatial and temporal levels for fast online video pose estimation. In the\nspatial level, we carefully design the search space with five different\ndimensions including network depth, width, kernel size, group number, and\nattentions. In the temporal level, we search from a series of temporal feature\nfusions to optimize the total accuracy and speed across multiple video frames.\nTo the best of our knowledge, we are the first to search for the temporal\nfeature fusion and automatic computation allocation in videos. Extensive\nexperiments demonstrate the effectiveness of our approach on the challenging\nCOCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and\nT-ViPNAS, achieve significantly higher inference speed (CPU real-time) without\nsacrificing the accuracy compared to the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:36:40 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Lumin", ""], ["Guan", "Yingda", ""], ["Jin", "Sheng", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Luo", "Ping", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2105.10156", "submitter": "Cuong Tuan Nguyen", "authors": "Cuong Tuan Nguyen, Thanh-Nghia Truong, Hung Tuan Nguyen and Masaki\n  Nakagawa", "title": "Global Context for improving recognition of Online Handwritten\n  Mathematical Expressions", "comments": "16 pages, ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a temporal classification method for all three subtasks\nof symbol segmentation, symbol recognition and relation classification in\nonline handwritten mathematical expressions (HMEs). The classification model is\ntrained by multiple paths of symbols and spatial relations derived from the\nSymbol Relation Tree (SRT) representation of HMEs. The method benefits from\nglobal context of a deep bidirectional Long Short-term Memory network, which\nlearns the temporal classification directly from online handwriting by the\nConnectionist Temporal Classification loss. To recognize an online HME, a\nsymbol-level parse tree with Context-Free Grammar is constructed, where symbols\nand spatial relations are obtained from the temporal classification results. We\nshow the effectiveness of the proposed method on the two latest CROHME\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:39:47 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Nguyen", "Cuong Tuan", ""], ["Truong", "Thanh-Nghia", ""], ["Nguyen", "Hung Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2105.10159", "submitter": "Cuong Tuan Nguyen", "authors": "Huy Quang Ung, Cuong Tuan Nguyen, Hung Tuan Nguyen and Masaki Nakagawa", "title": "GSSF: A Generative Sequence Similarity Function based on a Seq2Seq model\n  for clustering online handwritten mathematical answers", "comments": "16 pages, ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toward a computer-assisted marking for descriptive math questions,this paper\npresents clustering of online handwritten mathematical expressions (OnHMEs) to\nhelp human markers to mark them efficiently and reliably. We propose a\ngenerative sequence similarity function for computing a similarity score of two\nOnHMEs based on a sequence-to-sequence OnHME recognizer. Each OnHME is\nrepresented by a similarity-based representation (SbR) vector. The SbR matrix\nis inputted to the k-means algorithm for clustering OnHMEs. Experiments are\nconducted on an answer dataset (Dset_Mix) of 200 OnHMEs mixed of real patterns\nand synthesized patterns for each of 10 questions and a real online handwritten\nmathematical answer dataset of 122 student answers at most for each of 15\nquestions (NIER_CBT). The best clustering results achieved around 0.916 and\n0.915 for purity, and around 0.556 and 0.702 for the marking cost on Dset_Mix\nand NIER_CBT, respectively. Our method currently outperforms the previous\nmethods for clustering HMEs.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:48:02 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ung", "Huy Quang", ""], ["Nguyen", "Cuong Tuan", ""], ["Nguyen", "Hung Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2105.10160", "submitter": "Chaoqi Chen", "authors": "Yuhang Liu, Fandong Zhang, Chaoqi Chen, Siwen Wang, Yizhou Wang,\n  Yizhou Yu", "title": "Act Like a Radiologist: Towards Reliable Multi-view Correspondence\n  Reasoning for Mammogram Mass Detection", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammogram mass detection is crucial for diagnosing and preventing the breast\ncancers in clinical practice. The complementary effect of multi-view mammogram\nimages provides valuable information about the breast anatomical prior\nstructure and is of great significance in digital mammography interpretation.\nHowever, unlike radiologists who can utilize the natural reasoning ability to\nidentify masses based on multiple mammographic views, how to endow the existing\nobject detection models with the capability of multi-view reasoning is vital\nfor decision-making in clinical diagnosis but remains the boundary to explore.\nIn this paper, we propose an Anatomy-aware Graph convolutional Network (AGN),\nwhich is tailored for mammogram mass detection and endows existing detection\nmethods with multi-view reasoning ability. The proposed AGN consists of three\nsteps. Firstly, we introduce a Bipartite Graph convolutional Network (BGN) to\nmodel the intrinsic geometric and semantic relations of ipsilateral views.\nSecondly, considering that the visual asymmetry of bilateral views is widely\nadopted in clinical practice to assist the diagnosis of breast lesions, we\npropose an Inception Graph convolutional Network (IGN) to model the structural\nsimilarities of bilateral views. Finally, based on the constructed graphs, the\nmulti-view information is propagated through nodes methodically, which equips\nthe features learned from the examined view with multi-view reasoning ability.\nExperiments on two standard benchmarks reveal that AGN significantly exceeds\nthe state-of-the-art performance. Visualization results show that AGN provides\ninterpretable visual cues for clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:48:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Yuhang", ""], ["Zhang", "Fandong", ""], ["Chen", "Chaoqi", ""], ["Wang", "Siwen", ""], ["Wang", "Yizhou", ""], ["Yu", "Yizhou", ""]]}, {"id": "2105.10175", "submitter": "Eva Dokladalova", "authors": "Rosemberg Rodriguez Salas (LIGM), Eva Dokladalova (LIGM), Petr\n  Dokl\\'adal (CMM)", "title": "Rotation invariant CNN using scattering transform for image\n  classification", "comments": null, "journal-ref": "IEEE International Conference on Image Processing (ICIP), Sep\n  2019, Taipei, Taiwan", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks accuracy is heavily impacted by rotations\nof the input data. In this paper, we propose a convolutional predictor that is\ninvariant to rotations in the input. This architecture is capable of predicting\nthe angular orientation without angle-annotated data. Furthermore, the\npredictor maps continuously the random rotation of the input to a circular\nspace of the prediction. For this purpose, we use the roto-translation\nproperties existing in the Scattering Transform Networks with a series of 3D\nConvolutions. We validate the results by training with upright and randomly\nrotated samples. This allows further applications of this work on fields like\nautomatic re-orientation of randomly oriented datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:36:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Salas", "Rosemberg Rodriguez", "", "LIGM"], ["Dokladalova", "Eva", "", "LIGM"], ["Dokl\u00e1dal", "Petr", "", "CMM"]]}, {"id": "2105.10189", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Stanislav Frolov, J\\\"orn Hees, Federico Raue,\n  Franz-Josef Pfreundt, Andreas Dengel, Janis Keupe", "title": "Combining Transformer Generators with Convolutional Discriminators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer models have recently attracted much interest from computer vision\nresearchers and have since been successfully employed for several problems\ntraditionally addressed with convolutional neural networks. At the same time,\nimage synthesis using generative adversarial networks (GANs) has drastically\nimproved over the last few years. The recently proposed TransGAN is the first\nGAN using only transformer-based architectures and achieves competitive results\nwhen compared to convolutional GANs. However, since transformers are\ndata-hungry architectures, TransGAN requires data augmentation, an auxiliary\nsuper-resolution task during training, and a masking prior to guide the\nself-attention mechanism. In this paper, we study the combination of a\ntransformer-based generator and convolutional discriminator and successfully\nremove the need of the aforementioned required design choices. We evaluate our\napproach by conducting a benchmark of well-known CNN discriminators, ablate the\nsize of the transformer-based generator, and show that combining both\narchitectural elements into a hybrid model leads to better results.\nFurthermore, we investigate the frequency spectrum properties of generated\nimages and observe that our model retains the benefits of an attention based\ngenerator.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:56:59 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 10:24:59 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 10:16:47 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Durall", "Ricard", ""], ["Frolov", "Stanislav", ""], ["Hees", "J\u00f6rn", ""], ["Raue", "Federico", ""], ["Pfreundt", "Franz-Josef", ""], ["Dengel", "Andreas", ""], ["Keupe", "Janis", ""]]}, {"id": "2105.10192", "submitter": "Qiyuan Liang", "authors": "Qiyuan Liang, Bin Zhu, Chong-Wah Ngo", "title": "Pyramid Fusion Dark Channel Prior for Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the pyramid fusion dark channel prior (PF-DCP) for\nsingle image dehazing. Based on the well-known Dark Channel Prior (DCP), we\nintroduce an easy yet effective approach PF-DCP by employing the DCP algorithm\nat a pyramid of multi-scale images to alleviate the problem of patch size\nselection. In this case, we obtain the final transmission map by fusing\ntransmission maps at each level to recover a high-quality haze-free image.\nExperiments on RESIDE SOTS show that PF-DCP not only outperforms the\ntraditional prior-based methods with a large margin but also achieves\ncomparable or even better results of state-of-art deep learning approaches.\nFurthermore, the visual quality is also greatly improved with much fewer color\ndistortions and halo artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:01:57 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liang", "Qiyuan", ""], ["Zhu", "Bin", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "2105.10194", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Lianru Gao and Jing Yao and Naoto Yokoya and Jocelyn\n  Chanussot and Uta Heiden and Bing Zhang", "title": "Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning\n  Framework for Self-Supervised Hyperspectral Unmixing", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2021", "doi": "10.1109/TNNLS.2021", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decades, enormous efforts have been made to improve the\nperformance of linear or nonlinear mixing models for hyperspectral unmixing,\nyet their ability to simultaneously generalize various spectral variabilities\nand extract physically meaningful endmembers still remains limited due to the\npoor ability in data fitting and reconstruction and the sensitivity to various\nspectral variabilities. Inspired by the powerful learning ability of deep\nlearning, we attempt to develop a general deep learning approach for\nhyperspectral unmixing, by fully considering the properties of endmembers\nextracted from the hyperspectral imagery, called endmember-guided unmixing\nnetwork (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a\ntwo-stream Siamese deep network, which learns an additional network from the\npure or nearly-pure endmembers to correct the weights of another unmixing\nnetwork by sharing network parameters and adding spectrally meaningful\nconstraints (e.g., non-negativity and sum-to-one) towards a more accurate and\ninterpretable unmixing solution. Furthermore, the resulting general framework\nis not only limited to pixel-wise spectral unmixing but also applicable to\nspatial information modeling with convolutional operators for spatial-spectral\nunmixing. Experimental results conducted on three different datasets with the\nground-truth of abundance maps corresponding to each material demonstrate the\neffectiveness and superiority of the EGU-Net over state-of-the-art unmixing\nalgorithms. The codes will be available from the website:\nhttps://github.com/danfenghong/IEEE_TNNLS_EGU-Net.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:07:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Hong", "Danfeng", ""], ["Gao", "Lianru", ""], ["Yao", "Jing", ""], ["Yokoya", "Naoto", ""], ["Chanussot", "Jocelyn", ""], ["Heiden", "Uta", ""], ["Zhang", "Bing", ""]]}, {"id": "2105.10195", "submitter": "Kun Yan", "authors": "Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, Steven Schockaert", "title": "Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning", "comments": "Accepted by ICMR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) is the task of learning to recognize previously\nunseen categories of images from a small number of training examples. This is a\nchallenging task, as the available examples may not be enough to unambiguously\ndetermine which visual features are most characteristic of the considered\ncategories. To alleviate this issue, we propose a method that additionally\ntakes into account the names of the image classes. While the use of class names\nhas already been explored in previous work, our approach differs in two key\naspects. First, while previous work has aimed to directly predict visual\nprototypes from word embeddings, we found that better results can be obtained\nby treating visual and text-based prototypes separately. Second, we propose a\nsimple strategy for learning class name embeddings using the BERT language\nmodel, which we found to substantially outperform the GloVe vectors that were\nused in previous work. We furthermore propose a strategy for dealing with the\nhigh dimensionality of these vectors, inspired by models for aligning\ncross-lingual word embeddings. We provide experiments on miniImageNet, CUB and\ntieredImageNet, showing that our approach consistently improves the\nstate-of-the-art in metric-based FSL.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:08:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yan", "Kun", ""], ["Bouraoui", "Zied", ""], ["Wang", "Ping", ""], ["Jameel", "Shoaib", ""], ["Schockaert", "Steven", ""]]}, {"id": "2105.10196", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Jingliang Hu and Jing Yao and Jocelyn Chanussot and\n  Xiao Xiang Zhu", "title": "Multimodal Remote Sensing Benchmark Datasets for Land Cover\n  Classification with A Shared and Specific Feature Learning Model", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As remote sensing (RS) data obtained from different sensors become available\nlargely and openly, multimodal data processing and analysis techniques have\nbeen garnering increasing interest in the RS and geoscience community. However,\ndue to the gap between different modalities in terms of imaging sensors,\nresolutions, and contents, embedding their complementary information into a\nconsistent, compact, accurate, and discriminative representation, to a great\nextent, remains challenging. To this end, we propose a shared and specific\nfeature learning (S2FL) model. S2FL is capable of decomposing multimodal RS\ndata into modality-shared and modality-specific components, enabling the\ninformation blending of multi-modalities more effectively, particularly for\nheterogeneous data sources. Moreover, to better assess multimodal baselines and\nthe newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e.,\nHouston2013 -- hyperspectral and multispectral data, Berlin -- hyperspectral\nand synthetic aperture radar (SAR) data, Augsburg -- hyperspectral, SAR, and\ndigital surface model (DSM) data, are released and used for land cover\nclassification. Extensive experiments conducted on the three datasets\ndemonstrate the superiority and advancement of our S2FL model in the task of\nland cover classification in comparison with previously-proposed\nstate-of-the-art baselines. Furthermore, the baseline codes and datasets used\nin this paper will be made available freely at\nhttps://github.com/danfenghong/ISPRS_S2FL.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:14:21 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Hong", "Danfeng", ""], ["Hu", "Jingliang", ""], ["Yao", "Jing", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2105.10201", "submitter": "Jinshuo Zhang", "authors": "Jinshuo Zhang, Zhicheng Wang, Songyan Zhang, Gang Wei", "title": "DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain shift has always been one of the primary issues in video object\nsegmentation (VOS), for which models suffer from degeneration when tested on\nunfamiliar datasets. Recently, many online methods have emerged to narrow the\nperformance gap between training data (source domain) and test data (target\ndomain) by fine-tuning on annotations of test data which are usually in\nshortage. In this paper, we propose a novel method to tackle domain shift by\nfirst introducing adversarial domain adaptation to the VOS task, with\nsupervised training on the source domain and unsupervised training on the\ntarget domain. By fusing appearance and motion features with a convolution\nlayer, and by adding supervision onto the motion branch, our model achieves\nstate-of-the-art performance on DAVIS2016 with 82.6% mean IoU score after\nsupervised training. Meanwhile, our adversarial domain adaptation strategy\nsignificantly raises the performance of the trained model when applied on\nFBMS59 and Youtube-Object, without exploiting extra annotations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:23:51 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 16:00:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Jinshuo", ""], ["Wang", "Zhicheng", ""], ["Zhang", "Songyan", ""], ["Wei", "Gang", ""]]}, {"id": "2105.10203", "submitter": "Yuan Xie", "authors": "Jingyu Gong, Jiachen Xu, Xin Tan, Haichuan Song, Yanyun Qu, Yuan Xie,\n  Lizhuang Ma", "title": "Omni-supervised Point Cloud Segmentation via Gradual Receptive Field\n  Component Reasoning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden features in neural network usually fail to learn informative\nrepresentation for 3D segmentation as supervisions are only given on output\nprediction, while this can be solved by omni-scale supervision on intermediate\nlayers. In this paper, we bring the first omni-scale supervision method to\npoint cloud segmentation via the proposed gradual Receptive Field Component\nReasoning (RFCR), where target Receptive Field Component Codes (RFCCs) are\ndesigned to record categories within receptive fields for hidden units in the\nencoder. Then, target RFCCs will supervise the decoder to gradually infer the\nRFCCs in a coarse-to-fine categories reasoning manner, and finally obtain the\nsemantic labels. Because many hidden features are inactive with tiny magnitude\nand make minor contributions to RFCC prediction, we propose a Feature\nDensification with a centrifugal potential to obtain more unambiguous features,\nand it is in effect equivalent to entropy regularization over features. More\nactive features can further unleash the potential of our omni-supervision\nmethod. We embed our method into four prevailing backbones and test on three\nchallenging benchmarks. Our method can significantly improve the backbones in\nall three datasets. Specifically, our method brings new state-of-the-art\nperformances for S3DIS as well as Semantic3D and ranks the 1st in the ScanNet\nbenchmark among all the point-based methods. Code will be publicly available at\nhttps://github.com/azuki-miho/RFCR.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:32:02 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Gong", "Jingyu", ""], ["Xu", "Jiachen", ""], ["Tan", "Xin", ""], ["Song", "Haichuan", ""], ["Qu", "Yanyun", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2105.10214", "submitter": "Masaki Nakanishi", "authors": "Masaki Nakanishi, Kazuki Sato, Hideo Terada", "title": "Anomaly Detection By Autoencoder Based On Weighted Frequency Domain Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image anomaly detection, Autoencoders are the popular methods that\nreconstruct the input image that might contain anomalies and output a clean\nimage with no abnormalities. These Autoencoder-based methods usually calculate\nthe anomaly score from the reconstruction error, the difference between the\ninput image and the reconstructed image. On the other hand, the accuracy of the\nreconstruction is insufficient in many of these methods, so it leads to\ndegraded accuracy of anomaly detection. To improve the accuracy of the\nreconstruction, we consider defining loss function in the frequency domain. In\ngeneral, we know that natural images contain many low-frequency components and\nfew high-frequency components. Hence, to improve the accuracy of the\nreconstruction of high-frequency components, we introduce a new loss function\nnamed weighted frequency domain loss(WFDL). WFDL provides a sharper\nreconstructed image, which contributes to improving the accuracy of anomaly\ndetection. In this paper, we show our method's superiority over the\nconventional Autoencoder methods by comparing it with AUROC on the MVTec AD\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:10:36 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Nakanishi", "Masaki", ""], ["Sato", "Kazuki", ""], ["Terada", "Hideo", ""]]}, {"id": "2105.10227", "submitter": "Sani Abdullahi", "authors": "Sani M. Abdullahi and Sun Shuifa", "title": "Random Hash Code Generation for Cancelable Fingerprint Templates using\n  Vector Permutation and Shift-order Process", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cancelable biometric techniques have been used to prevent the compromise of\nbiometric data by generating and using their corresponding cancelable templates\nfor user authentication. However, the non-invertible distance preserving\ntransformation methods employed in various schemes are often vulnerable to\ninformation leakage since matching is performed in the transformed domain. In\nthis paper, we propose a non-invertible distance preserving scheme based on\nvector permutation and shift-order process. First, the dimension of feature\nvectors is reduced using kernelized principle component analysis (KPCA) prior\nto randomly permuting the extracted vector features. A shift-order process is\nthen applied to the generated features in order to achieve non-invertibility\nand combat similarity-based attacks. The generated hash codes are resilient to\ndifferent security and privacy attacks whilst fulfilling the major revocability\nand unlinkability requirements. Experimental evaluation conducted on 6 datasets\nof FVC2002 and FVC2004 reveals a high-performance accuracy of the proposed\nscheme better than other existing state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:37:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Abdullahi", "Sani M.", ""], ["Shuifa", "Sun", ""]]}, {"id": "2105.10228", "submitter": "Teruaki Akazawa", "authors": "Teruaki Akazawa, Yuma Kinoshita and Hitoshi Kiya", "title": "Multi-color balance for color constancy", "comments": "\\c{opyright} 2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-color balance adjustment for color\nconstancy. The proposed method, called \"n-color balancing,\" allows us not only\nto perfectly correct n target colors on the basis of corresponding ground truth\ncolors but also to correct colors other than the n colors. In contrast,\nalthough white-balancing can perfectly adjust white, colors other than white\nare not considered in the framework of white-balancing in general. In an\nexperiment, the proposed multi-color balancing is demonstrated to outperform\nboth conventional white and multi-color balance adjustments including\nBradford's model.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:38:56 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Akazawa", "Teruaki", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2105.10233", "submitter": "Markus Juvonen", "authors": "Markus Juvonen, Samuli Siltanen, Fernando Silva de Moura", "title": "Helsinki Deblur Challenge 2021: description of photographic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The photographic dataset collected for the Helsinki Deblur Challenge 2021\n(HDC2021) contains pairs of images taken by two identical cameras of the same\ntarget but with different conditions. One camera is always in focus and\nproduces sharp and low-noise images the other camera produces blurred and noisy\nimages as it is gradually more and more out of focus and has a higher ISO\nsetting. Even though the dataset was designed and captured with the HDC2021 in\nmind it can be used for any testing and benchmarking of image deblurring\nalgorithms. The data is available here: https://doi.org/10.5281/zenodo.477228\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:47:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Juvonen", "Markus", ""], ["Siltanen", "Samuli", ""], ["de Moura", "Fernando Silva", ""]]}, {"id": "2105.10238", "submitter": "Anna Zapaishchykova", "authors": "Anna Zapaishchykova, David Dreizin, Zhaoshuo Li, Jie Ying Wu, Shahrooz\n  Faghih Roohi, Mathias Unberath", "title": "An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pelvic ring disruptions result from blunt injury mechanisms and are often\nfound in patients with multi-system trauma. To grade pelvic fracture severity\nin trauma victims based on whole-body CT, the Tile AO/OTA classification is\nfrequently used. Due to the high volume of whole-body trauma CTs generated in\nbusy trauma centers, an automated approach to Tile classification would provide\nsubstantial value, e.,g., to prioritize the reading queue of the attending\ntrauma radiologist. In such scenario, an automated method should perform\ngrading based on a transparent process and based on interpretable features to\nenable interaction with human readers and lower their workload by offering\ninsights from a first automated read of the scan. This paper introduces an\nautomated yet interpretable pelvic trauma decision support system to assist\nradiologists in fracture detection and Tile grade classification. The method\noperates similarly to human interpretation of CT scans and first detects\ndistinct pelvic fractures on CT with high specificity using a Faster-RCNN model\nthat are then interpreted using a structural causal model based on clinical\nbest practices to infer an initial Tile grade. The Bayesian causal model and\nfinally, the object detector are then queried for likely co-occurring fractures\nthat may have been rejected initially due to the highly specific operating\npoint of the detector, resulting in an updated list of detected fractures and\ncorresponding final Tile grade. Our method is transparent in that it provides\nfinding location and type using the object detector, as well as information on\nimportant counterfactuals that would invalidate the system's recommendation and\nachieves an AUC of 83.3%/85.1% for translational/rotational instability.\nDespite being designed for human-machine teaming, our approach does not\ncompromise on performance compared to previous black-box approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:52:33 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zapaishchykova", "Anna", ""], ["Dreizin", "David", ""], ["Li", "Zhaoshuo", ""], ["Wu", "Jie Ying", ""], ["Roohi", "Shahrooz Faghih", ""], ["Unberath", "Mathias", ""]]}, {"id": "2105.10239", "submitter": "Shiv Ram Dubey", "authors": "Anirudh Ambati, Shiv Ram Dubey", "title": "AC-CovidNet: Attention Guided Contrastive CNN for Recognition of\n  Covid-19 in Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covid-19 global pandemic continues to devastate health care systems across\nthe world. In many countries, the 2nd wave is very severe. Economical and rapid\ntesting, as well as diagnosis, is urgently needed to control the pandemic. At\npresent, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)\ntesting can be the fastest, scalable, and non-invasive method. The existing\nmethods suffer due to the limited CXR samples available from Covid-19. Thus,\ninspired by the limitations of the open-source work in this field, we propose\nattention guided contrastive CNN architecture (AC-CovidNet) for Covid-19\ndetection in CXR images. The proposed method learns the robust and\ndiscriminative features with the help of contrastive loss. Moreover, the\nproposed method gives more importance to the infected regions as guided by the\nattention mechanism. We compute the sensitivity of the proposed method over the\npublicly available Covid-19 dataset. It is observed that the proposed\nAC-CovidNet exhibits very promising performance as compared to the existing\nmethods even with limited training data. It can tackle the bottleneck of CXR\nCovid-19 datasets being faced by the researchers. The code used in this paper\nis released publicly at \\url{https://github.com/shivram1987/AC-CovidNet/}.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:53:07 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ambati", "Anirudh", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2105.10262", "submitter": "Shiv Ram Dubey", "authors": "Satya Rajendra Singh, Shiv Ram Dubey, Shruthi MS, Sairathan\n  Ventrapragada, Saivamshi Salla Dasharatha", "title": "Joint Triplet Autoencoder for Histopathological Colon Cancer Nuclei\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown a great improvement in the performance of visual\ntasks. Image retrieval is the task of extracting the visually similar images\nfrom a database for a query image. The feature matching is performed to rank\nthe images. Various hand-designed features have been derived in past to\nrepresent the images. Nowadays, the power of deep learning is being utilized\nfor automatic feature learning from data in the field of biomedical image\nanalysis. Autoencoder and Siamese networks are two deep learning models to\nlearn the latent space (i.e., features or embedding). Autoencoder works based\non the reconstruction of the image from latent space. Siamese network utilizes\nthe triplets to learn the intra-class similarity and inter-class dissimilarity.\nMoreover, Autoencoder is unsupervised, whereas Siamese network is supervised.\nWe propose a Joint Triplet Autoencoder Network (JTANet) by facilitating the\ntriplet learning in autoencoder framework. A joint supervised learning for\nSiamese network and unsupervised learning for Autoencoder is performed.\nMoreover, the Encoder network of Autoencoder is shared with Siamese network and\nreferred as the Siamcoder network. The features are extracted by using the\ntrained Siamcoder network for retrieval purpose. The experiments are performed\nover Histopathological Routine Colon Cancer dataset. We have observed the\npromising performance using the proposed JTANet model against the Autoencoder\nand Siamese models for colon cancer nuclei retrieval in histopathological\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:31:14 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 04:56:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Singh", "Satya Rajendra", ""], ["Dubey", "Shiv Ram", ""], ["MS", "Shruthi", ""], ["Ventrapragada", "Sairathan", ""], ["Dasharatha", "Saivamshi Salla", ""]]}, {"id": "2105.10288", "submitter": "Mustafa Ayazoglu", "authors": "Mustafa Ayazoglu", "title": "Extremely Lightweight Quantization Robust Real-Time Single-Image Super\n  Resolution for Mobile Devices", "comments": null, "journal-ref": "IEEE Computer Vision Pattern Recognition Workshops (Mobile AI 2021\n  Workshop)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single-Image Super Resolution (SISR) is a classical computer vision problem\nand it has been studied for over decades. With the recent success of deep\nlearning methods, recent work on SISR focuses solutions with deep learning\nmethodologies and achieves state-of-the-art results. However most of the\nstate-of-the-art SISR methods contain millions of parameters and layers, which\nlimits their practical applications. In this paper, we propose a hardware\n(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization\nrobust real-time super resolution network (XLSR). The proposed model's building\nblock is inspired from root modules for Image classification. We successfully\napplied root modules to SISR problem, further more to make the model uint8\nquantization robust we used Clipped ReLU at the last layer of the network and\nachieved great balance between reconstruction quality and runtime. Furthermore,\nalthough the proposed network contains 30x fewer parameters than VDSR its\nperformance surpasses it on Div2K validation set. The network proved itself by\nwinning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 11:29:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ayazoglu", "Mustafa", ""]]}, {"id": "2105.10305", "submitter": "Mark Collier", "authors": "Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton and\n  Jesse Berent", "title": "Correlated Input-Dependent Label Noise in Large-Scale Image\n  Classification", "comments": "Accepted as Oral at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale image classification datasets often contain noisy labels. We take\na principled probabilistic approach to modelling input-dependent, also known as\nheteroscedastic, label noise in these datasets. We place a multivariate Normal\ndistributed latent variable on the final hidden layer of a neural network\nclassifier. The covariance matrix of this latent variable, models the aleatoric\nuncertainty due to label noise. We demonstrate that the learned covariance\nstructure captures known sources of label noise between semantically similar\nand co-occurring classes. Compared to standard neural network training and\nother baselines, we show significantly improved accuracy on Imagenet ILSVRC\n2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a\nnew state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These\ndatasets range from over 1M to over 300M training examples and from 1k classes\nto more than 21k classes. Our method is simple to use, and we provide an\nimplementation that is a drop-in replacement for the final fully-connected\nlayer in a deep classifier.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:30:59 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Collier", "Mark", ""], ["Mustafa", "Basil", ""], ["Kokiopoulou", "Efi", ""], ["Jenatton", "Rodolphe", ""], ["Berent", "Jesse", ""]]}, {"id": "2105.10310", "submitter": "Arnaud Boutillon", "authors": "Arnaud Boutillon, Pierre-Henri Conze, Christelle Pons, Val\\'erie\n  Burdin, Bhushan Borotikar", "title": "Multi-Task, Multi-Domain Deep Segmentation with Shared Representations\n  and Contrastive Regularization for Sparse Pediatric Datasets", "comments": "11 pages, 4 figures, 2 tables, accepted at the 24th International\n  Conference on Medical Image Computing and Computer-Assisted Intervention\n  (MICCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of magnetic resonance (MR) images is crucial for\nmorphological evaluation of the pediatric musculoskeletal system in clinical\npractice. However, the accuracy and generalization performance of individual\nsegmentation models are limited due to the restricted amount of annotated\npediatric data. Hence, we propose to train a segmentation model on multiple\ndatasets, arising from different parts of the anatomy, in a multi-task and\nmulti-domain learning framework. This approach allows to overcome the inherent\nscarcity of pediatric data while benefiting from a more robust shared\nrepresentation. The proposed segmentation network comprises shared\nconvolutional filters, domain-specific batch normalization parameters that\ncompute the respective dataset statistics and a domain-specific segmentation\nlayer. Furthermore, a supervised contrastive regularization is integrated to\nfurther improve generalization capabilities, by promoting intra-domain\nsimilarity and impose inter-domain margins in embedded space. We evaluate our\ncontributions on two pediatric imaging datasets of the ankle and shoulder\njoints for bone segmentation. Results demonstrate that the proposed model\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:26:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Boutillon", "Arnaud", ""], ["Conze", "Pierre-Henri", ""], ["Pons", "Christelle", ""], ["Burdin", "Val\u00e9rie", ""], ["Borotikar", "Bhushan", ""]]}, {"id": "2105.10313", "submitter": "Sofia Broom\\'e", "authors": "Sofia Broom\\'e, Katrina Ask, Maheen Rashid, Pia Haubro Andersen,\n  Hedvig Kjellstr\\\"om", "title": "Sharing Pain: Using Domain Transfer Between Pain Types for Recognition\n  of Sparse Pain Expressions in Horses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthopedic disorders are a common cause for euthanasia among horses, which\noften could have been avoided with earlier detection. These conditions often\ncreate varying degrees of subtle but long-term pain. It is challenging to train\na visual pain recognition method with video data depicting such pain, since the\nresulting pain behavior also is subtle, sparsely appearing, and varying, making\nit challenging for even an expert human labeler to provide accurate\nground-truth for the data. We show that transferring features from a dataset of\nhorses with acute nociceptive pain (where labeling is less ambiguous) can aid\nthe learning to recognize more complex orthopedic pain. Moreover, we present a\nhuman expert baseline for the problem, as well as an extensive empirical study\nof various domain transfer methods and of what is detected by the pain\nrecognition method trained on acute pain in the orthopedic dataset. Finally,\nthis is accompanied with a discussion around the challenges posed by real-world\nanimal behavior datasets and how best practices can be established for similar\nfine-grained action recognition tasks. Our code is available at\nhttps://github.com/sofiabroome/painface-recognition.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:35:00 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 19:17:51 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Broom\u00e9", "Sofia", ""], ["Ask", "Katrina", ""], ["Rashid", "Maheen", ""], ["Andersen", "Pia Haubro", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2105.10316", "submitter": "Illia Oleksiienko", "authors": "Illia Oleksiienko and Alexandros Iosifidis", "title": "Analysis of voxel-based 3D object detection methods efficiency for\n  real-time embedded systems", "comments": "6 pages, 4 figures. Accepted in IEEE ICETCI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-time detection of objects in the 3D scene is one of the tasks an\nautonomous agent needs to perform for understanding its surroundings. While\nrecent Deep Learning-based solutions achieve satisfactory performance, their\nhigh computational cost renders their application in real-life settings in\nwhich computations need to be performed on embedded platforms intractable. In\nthis paper, we analyze the efficiency of two popular voxel-based 3D object\ndetection methods providing a good compromise between high performance and\nspeed based on two aspects, their ability to detect objects located at large\ndistances from the agent and their ability to operate in real time on embedded\nplatforms equipped with high-performance GPUs. Our experiments show that these\nmethods mostly fail to detect distant small objects due to the sparsity of the\ninput point clouds at large distances. Moreover, models trained on near objects\nachieve similar or better performance compared to those trained on all objects\nin the scene. This means that the models learn object appearance\nrepresentations mostly from near objects. Our findings suggest that a\nconsiderable part of the computations of existing methods is focused on\nlocations of the scene that do not contribute with successful detection. This\nmeans that the methods can achieve a speed-up of $40$-$60\\%$ by restricting\noperation to near objects while not sacrificing much in performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:40:59 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Oleksiienko", "Illia", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2105.10325", "submitter": "Jana Kierdorf", "authors": "Jana Kierdorf, Immanuel Weber, Anna Kicherer, Laura Zabawa, Lukas\n  Drees, Ribana Roscher", "title": "Behind the leaves -- Estimation of occluded grapevine berries with\n  conditional generative adversarial networks", "comments": "45 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for accurate yield estimates for viticulture is becoming more\nimportant due to increasing competition in the wine market worldwide. One of\nthe most promising methods to estimate the harvest is berry counting, as it can\nbe approached non-destructively, and its process can be automated. In this\narticle, we present a method that addresses the challenge of occluded berries\nwith leaves to obtain a more accurate estimate of the number of berries that\nwill enable a better estimate of the harvest. We use generative adversarial\nnetworks, a deep learning-based approach that generates a likely scenario\nbehind the leaves exploiting learned patterns from images with non-occluded\nberries. Our experiments show that the estimate of the number of berries after\napplying our method is closer to the manually counted reference. In contrast to\napplying a factor to the berry count, our approach better adapts to local\nconditions by directly involving the appearance of the visible berries.\nFurthermore, we show that our approach can identify which areas in the image\nshould be changed by adding new berries without explicitly requiring\ninformation about hidden areas.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:57:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kierdorf", "Jana", ""], ["Weber", "Immanuel", ""], ["Kicherer", "Anna", ""], ["Zabawa", "Laura", ""], ["Drees", "Lukas", ""], ["Roscher", "Ribana", ""]]}, {"id": "2105.10335", "submitter": "Debasmit Das", "authors": "Debasmit Das, Yash Bhalgat and Fatih Porikli", "title": "Data-driven Weight Initialization with Sylvester Solvers", "comments": "Practical Machine Learning for Developing Countries Workshop,\n  International Conference on Learning Representations, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a data-driven scheme to initialize the parameters of\na deep neural network. This is in contrast to traditional approaches which\nrandomly initialize parameters by sampling from transformed standard\ndistributions. Such methods do not use the training data to produce a more\ninformed initialization. Our method uses a sequential layer-wise approach where\neach layer is initialized using its input activations. The initialization is\ncast as an optimization problem where we minimize a combination of encoding and\ndecoding losses of the input activations, which is further constrained by a\nuser-defined latent code. The optimization problem is then restructured into\nthe well-known Sylvester equation, which has fast and efficient gradient-free\nsolutions. Our data-driven method achieves a boost in performance compared to\nrandom initialization methods, both before start of training and after training\nis over. We show that our proposed method is especially effective in few-shot\nand fine-tuning settings. We conclude this paper with analyses on time\ncomplexity and the effect of different latent codes on the recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 07:33:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Das", "Debasmit", ""], ["Bhalgat", "Yash", ""], ["Porikli", "Fatih", ""]]}, {"id": "2105.10341", "submitter": "Ivan Bajic", "authors": "Lior Bragilevsky and Ivan V. Baji\\'c", "title": "Error Resilient Collaborative Intelligence via Low-Rank Tensor\n  Completion", "comments": "2 pages, 1 figure, extended abstract for a poster at IEEE\n  Communication Theory Workshop (CTW) 2020 (moved to 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the race to bring Artificial Intelligence (AI) to the edge, collaborative\nintelligence has emerged as a promising way to lighten the computation load on\nedge devices that run applications based on Deep Neural Networks (DNNs).\nTypically, a deep model is split at a certain layer into edge and cloud\nsub-models. The deep feature tensor produced by the edge sub-model is\ntransmitted to the cloud, where the remaining computationally intensive\nworkload is performed by the cloud sub-model. The communication channel between\nthe edge and cloud is imperfect, which will result in missing data in the deep\nfeature tensor received at the cloud side. In this study, we examine the\neffectiveness of four low-rank tensor completion methods in recovering missing\ndata in the deep feature tensor. We consider both sparse tensors, such as those\nproduced by the VGG16 model, as well as non-sparse tensors, such as those\nproduced by ResNet34 model. We study tensor completion effectiveness in both\nconplexity-constrained and unconstrained scenario.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:47:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bragilevsky", "Lior", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2105.10369", "submitter": "Ziyuan Zhao", "authors": "Shumeng Li, Ziyuan Zhao, Kaixin Xu, Zeng Zeng, Cuntai Guan", "title": "Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D\n  Left Atrium Segmentation", "comments": "Submitted to EMBC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved promising segmentation performance on 3D left\natrium MR images. However, annotations for segmentation tasks are expensive,\ncostly and difficult to obtain. In this paper, we introduce a novel\nhierarchical consistency regularized mean teacher framework for 3D left atrium\nsegmentation. In each iteration, the student model is optimized by multi-scale\ndeep supervision and hierarchical consistency regularization, concurrently.\nExtensive experiments have shown that our method achieves competitive\nperformance as compared with full annotation, outperforming other\nstateof-the-art semi-supervised segmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:15:45 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Shumeng", ""], ["Zhao", "Ziyuan", ""], ["Xu", "Kaixin", ""], ["Zeng", "Zeng", ""], ["Guan", "Cuntai", ""]]}, {"id": "2105.10375", "submitter": "Kai Wang", "authors": "Kai Wang, Shuo Wang, Zhipeng Zhou, Xiaobo Wang, Xiaojiang Peng, Baigui\n  Sun, Hao Li, Yang You", "title": "An Efficient Training Approach for Very Large Scale Face Recognition", "comments": "This is a very effcient framework for ultra-large-scale\n  classification tasks. Our code is available at\n  https://github.com/tiandunx/FFC. We will keep updating!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has achieved significant progress in deep-learning era due\nto the ultra-large-scale and well-labeled datasets.\n  However, training on ultra-large-scale datasets is time-consuming and takes\nup a lot of hardware resource.\n  Therefore, designing an efficient training approach is crucial and\nindispensable.\n  The heavy computational and memory costs mainly result from the high\ndimensionality of the Fully-Connected (FC) layer.\n  Specifically, the dimensionality is determined by the number of face\nidentities, which can be million-level or even more.\n  To this end, we propose a novel training approach for ultra-large-scale face\ndatasets, termed Faster Face Classification (F$^2$C).\n  In F$^2$C, we first define a Gallery Net and a Probe Net that are used to\ngenerate identities' centers and extract faces' features for face recognition,\nrespectively.\n  Gallery Net has the same structure as Probe Net and inherits the parameters\nfrom Probe Net with a moving average paradigm.\n  After that, to reduce the training time and hardware costs of the FC layer,\nwe propose a Dynamic Class Pool (DCP) that stores the features from Gallery Net\nand calculates the inner product (logits) with positive samples (whose\nidentities are in the DCP) in each mini-batch.\n  DCP can be regarded as a substitute for the FC layer but it is far smaller,\nthus greatly reducing the computational and memory costs.\n  For negative samples (whose identities are not in DCP), we minimize the\ncosine similarities between negative samples and those in DCP.\n  Then, to improve the update efficiency of DCP's parameters, we design a dual\ndata-loader including identity-based and instance-based loaders to generate a\ncertain of identities and samples in mini-batches.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:34:00 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:27:03 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 13:20:29 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 07:50:18 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Kai", ""], ["Wang", "Shuo", ""], ["Zhou", "Zhipeng", ""], ["Wang", "Xiaobo", ""], ["Peng", "Xiaojiang", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""], ["You", "Yang", ""]]}, {"id": "2105.10379", "submitter": "Soubarna Banik", "authors": "Soubarna Banik, Alejandro Mendoza Gracia, Alois Knoll", "title": "3D Human Pose Regression using Graph Convolutional Network", "comments": "Paper accepted in IEEE ICIP 2021, DOI will be updated once published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose estimation is a difficult task, due to challenges such as\noccluded body parts and ambiguous poses. Graph convolutional networks encode\nthe structural information of the human skeleton in the form of an adjacency\nmatrix, which is beneficial for better pose prediction. We propose one such\ngraph convolutional network named PoseGraphNet for 3D human pose regression\nfrom 2D poses. Our network uses an adaptive adjacency matrix and kernels\nspecific to neighbor groups. We evaluate our model on the Human3.6M dataset\nwhich is a standard dataset for 3D pose estimation. Our model's performance is\nclose to the state-of-the-art, but with much fewer parameters. The model learns\ninteresting adjacency relations between joints that have no physical\nconnections, but are behaviorally similar.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:41:31 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Banik", "Soubarna", ""], ["Gracia", "Alejandro Mendoza", ""], ["Knoll", "Alois", ""]]}, {"id": "2105.10382", "submitter": "Fabio Poiesi", "authors": "Fabio Poiesi and Davide Boscaini", "title": "Generalisable and distinctive 3D local deep descriptors for point cloud\n  registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An effective 3D descriptor should be invariant to different geometric\ntransformations, such as scale and rotation, repeatable in the case of\nocclusions and clutter, and generalisable in different contexts when data is\ncaptured with different sensors. We present a simple but yet effective method\nto learn generalisable and distinctive 3D local descriptors that can be used to\nregister point clouds captured in different contexts with different sensors.\nPoint cloud patches are extracted, canonicalised with respect to their local\nreference frame, and encoded into scale and rotation-invariant compact\ndescriptors by a point permutation-invariant deep neural network. Our\ndescriptors can effectively generalise across sensor modalities from locally\nand randomly sampled points. We evaluate and compare our descriptors with\nalternative handcrafted and deep learning-based descriptors on several indoor\nand outdoor datasets reconstructed using both RGBD sensors and laser scanners.\nOur descriptors outperform most recent descriptors by a large margin in terms\nof generalisation, and become the state of the art also in benchmarks where\ntraining and testing are performed in the same scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:47:55 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Poiesi", "Fabio", ""], ["Boscaini", "Davide", ""]]}, {"id": "2105.10403", "submitter": "Keivan Bahmani", "authors": "Keivan Bahmani, Richard Plesh, Peter Johnson, Stephanie Schuckers,\n  Timothy Swyka", "title": "High Fidelity Fingerprint Generation: Quality, Uniqueness, and Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we utilize progressive growth-based Generative Adversarial\nNetworks (GANs) to develop the Clarkson Fingerprint Generator (CFG). We\ndemonstrate that the CFG is capable of generating realistic, high fidelity,\n$512\\times512$ pixels, full, plain impression fingerprints. Our results suggest\nthat the fingerprints generated by the CFG are unique, diverse, and resemble\nthe training dataset in terms of minutiae configuration and quality, while not\nrevealing the underlying identities of the training data. We make the\npre-trained CFG model and the synthetically generated dataset publicly\navailable at https://github.com/keivanB/Clarkson_Finger_Gen\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:18:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bahmani", "Keivan", ""], ["Plesh", "Richard", ""], ["Johnson", "Peter", ""], ["Schuckers", "Stephanie", ""], ["Swyka", "Timothy", ""]]}, {"id": "2105.10414", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Brett Jefferson, Cliff Joslyn, Emilie Purvine", "title": "Sheaves as a Framework for Understanding and Interpreting Model Fit", "comments": "12 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.AT math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data grows in size and complexity, finding frameworks which aid in\ninterpretation and analysis has become critical. This is particularly true when\ndata comes from complex systems where extensive structure is available, but\nmust be drawn from peripheral sources. In this paper we argue that in such\nsituations, sheaves can provide a natural framework to analyze how well a\nstatistical model fits at the local level (that is, on subsets of related\ndatapoints) vs the global level (on all the data). The sheaf-based approach\nthat we propose is suitably general enough to be useful in a range of\napplications, from analyzing sensor networks to understanding the feature space\nof a deep learning model.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:34:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kvinge", "Henry", ""], ["Jefferson", "Brett", ""], ["Joslyn", "Cliff", ""], ["Purvine", "Emilie", ""]]}, {"id": "2105.10420", "submitter": "Julio Silva-Rodr\\'iguez", "authors": "Julio Silva-Rodr\\'iguez, Adri\\'an Colomer, Jose Dolz and Valery\n  Naranjo", "title": "Self-learning for weakly supervised Gleason grading of local patterns", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2021.3061457", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is one of the main diseases affecting men worldwide. The gold\nstandard for diagnosis and prognosis is the Gleason grading system. In this\nprocess, pathologists manually analyze prostate histology slides under\nmicroscope, in a high time-consuming and subjective task. In the last years,\ncomputer-aided-diagnosis (CAD) systems have emerged as a promising tool that\ncould support pathologists in the daily clinical practice. Nevertheless, these\nsystems are usually trained using tedious and prone-to-error pixel-level\nannotations of Gleason grades in the tissue. To alleviate the need of manual\npixel-wise labeling, just a handful of works have been presented in the\nliterature. Motivated by this, we propose a novel weakly-supervised\ndeep-learning model, based on self-learning CNNs, that leverages only the\nglobal Gleason score of gigapixel whole slide images during training to\naccurately perform both, grading of patch-level patterns and biopsy-level\nscoring. To evaluate the performance of the proposed method, we perform\nextensive experiments on three different external datasets for the patch-level\nGleason grading, and on two different test sets for global Grade Group\nprediction. We empirically demonstrate that our approach outperforms its\nsupervised counterpart on patch-level Gleason grading by a large margin, as\nwell as state-of-the-art methods on global biopsy-level scoring. Particularly,\nthe proposed model brings an average improvement on the Cohen's quadratic kappa\n(k) score of nearly 18% compared to full-supervision for the patch-level\nGleason grading task.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:39:50 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Silva-Rodr\u00edguez", "Julio", ""], ["Colomer", "Adri\u00e1n", ""], ["Dolz", "Jose", ""], ["Naranjo", "Valery", ""]]}, {"id": "2105.10422", "submitter": "Wenbo Li", "authors": "Wenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu, Jiaya Jia", "title": "LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single\n  Image Super-Resolution and Beyond", "comments": "NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) deals with a fundamental problem of\nupsampling a low-resolution (LR) image to its high-resolution (HR) version.\nLast few years have witnessed impressive progress propelled by deep learning\nmethods. However, one critical challenge faced by existing methods is to strike\na sweet spot of deep model complexity and resulting SISR quality. This paper\naddresses this pain point by proposing a linearly-assembled pixel-adaptive\nregression network (LAPAR), which casts the direct LR to HR mapping learning\ninto a linear coefficient regression task over a dictionary of multiple\npredefined filter bases. Such a parametric representation renders our model\nhighly lightweight and easy to optimize while achieving state-of-the-art\nresults on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended\nto tackle other restoration tasks, e.g., image denoising and JPEG image\ndeblocking, and again, yields strong performance. The code is available at\nhttps://github.com/dvlab-research/Simple-SR.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:47:18 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Wenbo", ""], ["Zhou", "Kun", ""], ["Qi", "Lu", ""], ["Jiang", "Nianjuan", ""], ["Lu", "Jiangbo", ""], ["Jia", "Jiaya", ""]]}, {"id": "2105.10436", "submitter": "Muhammad Tayyab", "authors": "Muhammad Tayyab, Fahad Ahmad Khan, Abhijit Mahalanobis", "title": "Compressing Deep CNNs using Basis Representation and Spectral\n  Fine-tuning", "comments": "arXiv admin note: text overlap with arXiv:1906.04509", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an efficient and straightforward method for compressing deep\nconvolutional neural networks (CNNs) that uses basis filters to represent the\nconvolutional layers, and optimizes the performance of the compressed network\ndirectly in the basis space. Specifically, any spatial convolution layer of the\nCNN can be replaced by two successive convolution layers: the first is a set of\nthree-dimensional orthonormal basis filters, followed by a layer of\none-dimensional filters that represents the original spatial filters in the\nbasis space. We jointly fine-tune both the basis and the filter representation\nto directly mitigate any performance loss due to the truncation. Generality of\nthe proposed approach is demonstrated by applying it to several well known deep\nCNN architectures and data sets for image classification and object detection.\nWe also present the execution time and power usage at different compression\nlevels on the Xavier Jetson AGX processor.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:14:26 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Tayyab", "Muhammad", ""], ["Khan", "Fahad Ahmad", ""], ["Mahalanobis", "Abhijit", ""]]}, {"id": "2105.10438", "submitter": "Dat Huynh", "authors": "Dat Huynh and Ehsan Elhamifar", "title": "Compositional Fine-Grained Low-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel compositional generative model for zero- and few-shot\nlearning to recognize fine-grained classes with a few or no training samples.\nOur key observation is that generating holistic features for fine-grained\nclasses fails to capture small attribute differences between classes.\nTherefore, we propose a feature composition framework that learns to extract\nattribute features from training samples and combines them to construct\nfine-grained features for rare and unseen classes. Feature composition allows\nus to not only selectively compose features of every class from only relevant\ntraining samples, but also obtain diversity among composed features via\nchanging samples used for the composition. In addition, instead of building\nholistic features for classes, we use our attribute features to form dense\nrepresentations capable of capturing fine-grained attribute details of classes.\nWe propose a training scheme that uses a discriminative model to construct\nfeatures that are subsequently used to train the model itself. Therefore, we\ndirectly train the discriminative model on the composed features without\nlearning a separate generative model. We conduct experiments on four popular\ndatasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:18:24 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Huynh", "Dat", ""], ["Elhamifar", "Ehsan", ""]]}, {"id": "2105.10441", "submitter": "Timur Bagautdinov", "authors": "Timur Bagautdinov, Chenglei Wu, Tomas Simon, Fabian Prada, Takaaki\n  Shiratori, Shih-En Wei, Weipeng Xu, Yaser Sheikh, Jason Saragih", "title": "Driving-Signal Aware Full-Body Avatars", "comments": null, "journal-ref": null, "doi": "10.1145/3450626.3459850", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:22:38 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:30:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bagautdinov", "Timur", ""], ["Wu", "Chenglei", ""], ["Simon", "Tomas", ""], ["Prada", "Fabian", ""], ["Shiratori", "Takaaki", ""], ["Wei", "Shih-En", ""], ["Xu", "Weipeng", ""], ["Sheikh", "Yaser", ""], ["Saragih", "Jason", ""]]}, {"id": "2105.10445", "submitter": "Julio Silva Rodr\\'iguez", "authors": "Julio Silva-Rodr\\'iguez, Adri\\'an Colomer, Valery Naranjo", "title": "WeGleNet: A Weakly-Supervised Convolutional Neural Network for the\n  Semantic Segmentation of Gleason Grades in Prostate Histology Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.compmedimag.2020.101846", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prostate cancer is one of the main diseases affecting men worldwide. The\nGleason scoring system is the primary diagnostic tool for prostate cancer. This\nis obtained via the visual analysis of cancerous patterns in prostate biopsies\nperformed by expert pathologists, and the aggregation of the main Gleason\ngrades in a combined score. Computer-aided diagnosis systems allow to reduce\nthe workload of pathologists and increase the objectivity. Recently, efforts\nhave been made in the literature to develop algorithms aiming the direct\nestimation of the global Gleason score at biopsy/core level with global labels.\nHowever, these algorithms do not cover the accurate localization of the Gleason\npatterns into the tissue. In this work, we propose a deep-learning-based system\nable to detect local cancerous patterns in the prostate tissue using only the\nglobal-level Gleason score during training. The methodological core of this\nwork is the proposed weakly-supervised-trained convolutional neural network,\nWeGleNet, based on a multi-class segmentation layer after the feature\nextraction module, a global-aggregation, and the slicing of the background\nclass for the model loss estimation during training. We obtained a Cohen's\nquadratic kappa (k) of 0.67 for the pixel-level prediction of cancerous\npatterns in the validation cohort. We compared the model performance for\nsemantic segmentation of Gleason grades with supervised state-of-the-art\narchitectures in the test cohort. We obtained a pixel-level k of 0.61 and a\nmacro-averaged f1-score of 0.58, at the same level as fully-supervised methods.\nRegarding the estimation of the core-level Gleason score, we obtained a k of\n0.76 and 0.67 between the model and two different pathologists. WeGleNet is\ncapable of performing the semantic segmentation of Gleason grades similarly to\nfully-supervised methods without requiring pixel-level annotations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:27:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Silva-Rodr\u00edguez", "Julio", ""], ["Colomer", "Adri\u00e1n", ""], ["Naranjo", "Valery", ""]]}, {"id": "2105.10446", "submitter": "Yaodong Yu", "authors": "Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, Yi\n  Ma", "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate\n  Reduction", "comments": "This paper integrates previous two manuscripts: arXiv:2006.08558 and\n  arXiv:2010.14765, with significantly improved organization, presentation, and\n  new results; V2 polishes writing and adds citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We argue that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, which shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, although they are amenable to\nfine-tuning via back propagation. All components of so-obtained ``white-box''\nnetwork have precise optimization, statistical, and geometric interpretation.\nMoreover, all linear operators of the so-derived network naturally become\nmulti-channel convolutions when we enforce classification to be rigorously\nshift-invariant. The derivation in the invariant setting suggests a trade-off\nbetween sparsity and invariance, and also indicates that such a deep\nconvolution network is significantly more efficient to construct and learn in\nthe spectral domain. Our preliminary simulations and experiments clearly verify\nthe effectiveness of both the rate reduction objective and the associated\nReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:29:57 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:09:24 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chan", "Kwan Ho Ryan", ""], ["Yu", "Yaodong", ""], ["You", "Chong", ""], ["Qi", "Haozhi", ""], ["Wright", "John", ""], ["Ma", "Yi", ""]]}, {"id": "2105.10448", "submitter": "Ricardo Real", "authors": "Ric Real, James Gopsill, David Jones, Chris Snider, Ben Hicks", "title": "Distinguishing artefacts: evaluating the saturation point of\n  convolutional neural networks", "comments": "6 Pages, 5 Figures, 2 Tables, Conference, Design Engineering, CNN,\n  Digital Twin", "journal-ref": "January 2021 Procedia CIRP 100:385-390", "doi": "10.1016/j.procir.2021.05.089", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prior work has shown Convolutional Neural Networks (CNNs) trained on\nsurrogate Computer Aided Design (CAD) models are able to detect and classify\nreal-world artefacts from photographs. The applications of which support\ntwinning of digital and physical assets in design, including rapid extraction\nof part geometry from model repositories, information search \\& retrieval and\nidentifying components in the field for maintenance, repair, and recording. The\nperformance of CNNs in classification tasks have been shown dependent on\ntraining data set size and number of classes. Where prior works have used\nrelatively small surrogate model data sets ($<100$ models), the question\nremains as to the ability of a CNN to differentiate between models in\nincreasingly large model repositories. This paper presents a method for\ngenerating synthetic image data sets from online CAD model repositories, and\nfurther investigates the capacity of an off-the-shelf CNN architecture trained\non synthetic data to classify models as class size increases. 1,000 CAD models\nwere curated and processed to generate large scale surrogate data sets,\nfeaturing model coverage at steps of 10$^{\\circ}$, 30$^{\\circ}$, 60$^{\\circ}$,\nand 120$^{\\circ}$ degrees. The findings demonstrate the capability of computer\nvision algorithms to classify artefacts in model repositories of up to 200,\nbeyond this point the CNN's performance is observed to deteriorate\nsignificantly, limiting its present ability for automated twinning of physical\nto digital artefacts. Although, a match is more often found in the top-5\nresults showing potential for information search and retrieval on large\nrepositories of surrogate models.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:33:20 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Real", "Ric", ""], ["Gopsill", "James", ""], ["Jones", "David", ""], ["Snider", "Chris", ""], ["Hicks", "Ben", ""]]}, {"id": "2105.10457", "submitter": "Aissatou Diallo", "authors": "A\\\"issatou Diallo and Johannes F\\\"urnkranz", "title": "Elliptical Ordinal Embedding", "comments": "14 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ordinal embedding aims at finding a low dimensional representation of objects\nfrom a set of constraints of the form \"item $j$ is closer to item $i$ than item\n$k$\". Typically, each object is mapped onto a point vector in a low dimensional\nmetric space. We argue that mapping to a density instead of a point vector\nprovides some interesting advantages, including an inherent reflection of the\nuncertainty about the representation itself and its relative location in the\nspace. Indeed, in this paper, we propose to embed each object as a Gaussian\ndistribution. We investigate the ability of these embeddings to capture the\nunderlying structure of the data while satisfying the constraints, and explore\nproperties of the representation. Experiments on synthetic and real-world\ndatasets showcase the advantages of our approach. In addition, we illustrate\nthe merit of modelling uncertainty, which enriches the visual perception of the\nmapped objects in the space.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:54:53 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 16:45:02 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Diallo", "A\u00efssatou", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "2105.10465", "submitter": "Boyan Xu", "authors": "Boyan Xu and Hujun Yin", "title": "Graph Convolutional Networks in Feature Space for Image Deblurring and\n  Super-resolution", "comments": "Accepted by IJCNN 2021 (Oral)", "journal-ref": "International Joint Conference on Neural Networks (IJCNN) 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) have achieved great success in dealing\nwith data of non-Euclidean structures. Their success directly attributes to\nfitting graph structures effectively to data such as in social media and\nknowledge databases. For image processing applications, the use of graph\nstructures and GCNs have not been fully explored. In this paper, we propose a\nnovel encoder-decoder network with added graph convolutions by converting\nfeature maps to vertexes of a pre-generated graph to synthetically construct\ngraph-structured data. By doing this, we inexplicitly apply graph Laplacian\nregularization to the feature maps, making them more structured. The\nexperiments show that it significantly boosts performance for image restoration\ntasks, including deblurring and super-resolution. We believe it opens up\nopportunities for GCN-based approaches in more applications.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:02:15 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Boyan", ""], ["Yin", "Hujun", ""]]}, {"id": "2105.10477", "submitter": "Roxana Daneshjou", "authors": "Roxana Daneshjou, Carrie Kovarik, and Justin M Ko", "title": "Towards Realization of Augmented Intelligence in Dermatology: Advances\n  and Future Directions", "comments": "5 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) algorithms using deep learning have advanced the\nclassification of skin disease images; however these algorithms have been\nmostly applied \"in silico\" and not validated clinically. Most dermatology AI\nalgorithms perform binary classification tasks (e.g. malignancy versus benign\nlesions), but this task is not representative of dermatologists' diagnostic\nrange. The American Academy of Dermatology Task Force on Augmented Intelligence\npublished a position statement emphasizing the importance of clinical\nvalidation to create human-computer synergy, termed augmented intelligence\n(AuI). Liu et al's recent paper, \"A deep learning system for differential\ndiagnosis of skin diseases\" represents a significant advancement of AI in\ndermatology, bringing it closer to clinical impact. However, significant issues\nmust be addressed before this algorithm can be integrated into clinical\nworkflow. These issues include accurate and equitable model development,\ndefining and assessing appropriate clinical outcomes, and real-world\nintegration.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:39:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Daneshjou", "Roxana", ""], ["Kovarik", "Carrie", ""], ["Ko", "Justin M", ""]]}, {"id": "2105.10490", "submitter": "Julio Silva-Rodr\\'iguez", "authors": "Julio Silva-Rodr\\'iguez, Adri\\'an Colomer, Mar\\'ia A. Sales, Rafael\n  Molina and Valery Naranjo", "title": "Going Deeper through the Gleason Scoring Scale: An Automatic end-to-end\n  System for Histology Prostate Grading and Cribriform Pattern Detection", "comments": null, "journal-ref": null, "doi": "10.1016/j.cmpb.2020.105637", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Gleason scoring system is the primary diagnostic and prognostic tool for\nprostate cancer. In recent years, with the development of digitisation devices,\nthe use of computer vision techniques for the analysis of biopsies has\nincreased. However, to the best of the authors' knowledge, the development of\nalgorithms to automatically detect individual cribriform patterns belonging to\nGleason grade 4 has not yet been studied in the literature. The objective of\nthe work presented in this paper is to develop a deep-learning-based system\nable to support pathologists in the daily analysis of prostate biopsies. The\nmethodological core of this work is a patch-wise predictive model based on\nconvolutional neural networks able to determine the presence of cancerous\npatterns. In particular, we train from scratch a simple self-design\narchitecture. The cribriform pattern is detected by retraining the set of\nfilters of the last convolutional layer in the network. From the reconstructed\nprediction map, we compute the percentage of each Gleason grade in the tissue\nto feed a multi-layer perceptron which provides a biopsy-level score.mIn our\nSICAPv2 database, composed of 182 annotated whole slide images, we obtained a\nCohen's quadratic kappa of 0.77 in the test set for the patch-level Gleason\ngrading with the proposed architecture trained from scratch. Our results\noutperform previous ones reported in the literature. Furthermore, this model\nreaches the level of fine-tuned state-of-the-art architectures in a\npatient-based four groups cross validation. In the cribriform pattern detection\ntask, we obtained an area under ROC curve of 0.82. Regarding the biopsy Gleason\nscoring, we achieved a quadratic Cohen's Kappa of 0.81 in the test subset.\nShallow CNN architectures trained from scratch outperform current\nstate-of-the-art methods for Gleason grades classification.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:51:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Silva-Rodr\u00edguez", "Julio", ""], ["Colomer", "Adri\u00e1n", ""], ["Sales", "Mar\u00eda A.", ""], ["Molina", "Rafael", ""], ["Naranjo", "Valery", ""]]}, {"id": "2105.10497", "submitter": "Salman Khan Dr.", "authors": "Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat,\n  Fahad Shahbaz Khan, Ming-Hsuan Yang", "title": "Intriguing Properties of Vision Transformers", "comments": "Code: https://git.io/Js15X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformers (ViT) have demonstrated impressive performance across\nvarious machine vision problems. These models are based on multi-head\nself-attention mechanisms that can flexibly attend to a sequence of image\npatches to encode contextual cues. An important question is how such\nflexibility in attending image-wide context conditioned on a given patch can\nfacilitate handling nuisances in natural images e.g., severe occlusions, domain\nshifts, spatial permutations, adversarial and natural perturbations. We\nsystematically study this question via an extensive set of experiments\nencompassing three ViT families and comparisons with a high-performing\nconvolutional neural network (CNN). We show and analyze the following\nintriguing properties of ViT: (a) Transformers are highly robust to severe\nocclusions, perturbations and domain shifts, e.g., retain as high as 60% top-1\naccuracy on ImageNet even after randomly occluding 80% of the image content.\n(b) The robust performance to occlusions is not due to a bias towards local\ntextures, and ViTs are significantly less biased towards textures compared to\nCNNs. When properly trained to encode shape-based features, ViTs demonstrate\nshape recognition capability comparable to that of human visual system,\npreviously unmatched in the literature. (c) Using ViTs to encode shape\nrepresentation leads to an interesting consequence of accurate semantic\nsegmentation without pixel-level supervision. (d) Off-the-shelf features from a\nsingle ViT model can be combined to create a feature ensemble, leading to high\naccuracy rates across a range of classification datasets in both traditional\nand few-shot learning paradigms. We show effective features of ViTs are due to\nflexible and dynamic receptive fields possible via the self-attention\nmechanism.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:59:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 13:21:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Naseer", "Muzammal", ""], ["Ranasinghe", "Kanchana", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2105.10528", "submitter": "Alexander Mattick", "authors": "Alexander Mattick, Martin Mayr, Mathias Seuret, Andreas Maier, Vincent\n  Christlein", "title": "SmartPatch: Improving Handwritten Word Imitation with Patch\n  Discriminators", "comments": "to be published the in 16th International Conference on Document\n  Analysis and Recognition 2021 ICDAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of recent generative adversarial networks have allowed for big leaps in\nthe realism of generated images in diverse domains, not the least of which\nbeing handwritten text generation. The generation of realistic-looking\nhand-written text is important because it can be used for data augmentation in\nhandwritten text recognition (HTR) systems or human-computer interaction. We\npropose SmartPatch, a new technique increasing the performance of current\nstate-of-the-art methods by augmenting the training feedback with a tailored\nsolution to mitigate pen-level artifacts. We combine the well-known patch loss\nwith information gathered from the parallel trained handwritten text\nrecognition system and the separate characters of the word. This leads to a\nmore enhanced local discriminator and results in more realistic and\nhigher-quality generated handwritten words.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 18:34:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mattick", "Alexander", ""], ["Mayr", "Martin", ""], ["Seuret", "Mathias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2105.10533", "submitter": "Shan You", "authors": "Xiu Su, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu", "title": "BCNet: Searching for Network Width with Bilaterally Coupled Network", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for a more compact network width recently serves as an effective\nway of channel pruning for the deployment of convolutional neural networks\n(CNNs) under hardware constraints. To fulfill the searching, a one-shot\nsupernet is usually leveraged to efficiently evaluate the performance\n\\wrt~different network widths. However, current methods mainly follow a\n\\textit{unilaterally augmented} (UA) principle for the evaluation of each\nwidth, which induces the training unfairness of channels in supernet. In this\npaper, we introduce a new supernet called Bilaterally Coupled Network (BCNet)\nto address this issue. In BCNet, each channel is fairly trained and responsible\nfor the same amount of network widths, thus each network width can be evaluated\nmore accurately. Besides, we leverage a stochastic complementary strategy for\ntraining the BCNet, and propose a prior initial population sampling method to\nboost the performance of the evolutionary search. Extensive experiments on\nbenchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve\nstate-of-the-art or competing performance over other baseline methods.\nMoreover, our method turns out to further boost the performance of NAS models\nby refining their network widths. For example, with the same FLOPs budget, our\nobtained EfficientNet-B0 achieves 77.36\\% Top-1 accuracy on ImageNet dataset,\nsurpassing the performance of original setting by 0.48\\%.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 18:54:03 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Su", "Xiu", ""], ["You", "Shan", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Xu", "Chang", ""]]}, {"id": "2105.10556", "submitter": "Julio Silva-Rodr\u00edguez", "authors": "Julio Silva-Rodr\\'iguez, Elena Pay\\'a-Bosch, Gabriel Garc\\'ia,\n  Adri\\'an Colomer and Valery Naranjo", "title": "Prostate Gland Segmentation in Histology Images via Residual and\n  Multi-Resolution U-Net", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-62362-3_1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prostate cancer is one of the most prevalent cancers worldwide. One of the\nkey factors in reducing its mortality is based on early detection. The\ncomputer-aided diagnosis systems for this task are based on the glandular\nstructural analysis in histology images. Hence, accurate gland detection and\nsegmentation is crucial for a successful prediction. The methodological basis\nof this work is a prostate gland segmentation based on U-Net convolutional\nneural network architectures modified with residual and multi-resolution\nblocks, trained using data augmentation techniques. The residual configuration\noutperforms in the test subset the previous state-of-the-art approaches in an\nimage-level comparison, reaching an average Dice Index of 0.77.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:11:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Silva-Rodr\u00edguez", "Julio", ""], ["Pay\u00e1-Bosch", "Elena", ""], ["Garc\u00eda", "Gabriel", ""], ["Colomer", "Adri\u00e1n", ""], ["Naranjo", "Valery", ""]]}, {"id": "2105.10559", "submitter": "Tianyu Ma", "authors": "Tianyu Ma, Adrian V. Dalca, Mert R. Sabuncu", "title": "Hyper-Convolution Networks for Biomedical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The convolution operation is a central building block of neural network\narchitectures widely used in computer vision. The size of the convolution\nkernels determines both the expressiveness of convolutional neural networks\n(CNN), as well as the number of learnable parameters. Increasing the network\ncapacity to capture rich pixel relationships requires increasing the number of\nlearnable parameters, often leading to overfitting and/or lack of robustness.\nIn this paper, we propose a powerful novel building block, the\nhyper-convolution, which implicitly represents the convolution kernel as a\nfunction of kernel coordinates. Hyper-convolutions enable decoupling the kernel\nsize, and hence its receptive field, from the number of learnable parameters.\nIn our experiments, focused on challenging biomedical image segmentation tasks,\nwe demonstrate that replacing regular convolutions with hyper-convolutions\nleads to more efficient architectures that achieve improved accuracy. Our\nanalysis also shows that learned hyper-convolutions are naturally regularized,\nwhich can offer better generalization performance. We believe that\nhyper-convolutions can be a powerful building block in future neural network\narchitectures solving computer vision tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:31:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ma", "Tianyu", ""], ["Dalca", "Adrian V.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2105.10563", "submitter": "Kanav Vats", "authors": "Kanav Vats, Mehrnaz Fani, David A. Clausi, John Zelek", "title": "Puck localization and multi-task event recognition in broadcast hockey\n  videos", "comments": "Accepted at CVSports 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Puck localization is an important problem in ice hockey video analytics\nuseful for analyzing the game, determining play location, and assessing puck\npossession. The problem is challenging due to the small size of the puck,\nexcessive motion blur due to high puck velocity and occlusions due to players\nand boards. In this paper, we introduce and implement a network for puck\nlocalization in broadcast hockey video. The network leverages expert NHL\nplay-by-play annotations and uses temporal context to locate the puck. Player\nlocations are incorporated into the network through an attention mechanism by\nencoding player positions with a Gaussian-based spatial heatmap drawn at player\npositions. Since event occurrence on the rink and puck location are related, we\nalso perform event recognition by augmenting the puck localization network with\nan event recognition head and training the network through multi-task learning.\nExperimental results demonstrate that the network is able to localize the puck\nwith an AUC of $73.1 \\%$ on the test set. The puck location can be inferred in\n720p broadcast videos at $5$ frames per second. It is also demonstrated that\nmulti-task learning with puck location improves event recognition accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:43:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Vats", "Kanav", ""], ["Fani", "Mehrnaz", ""], ["Clausi", "David A.", ""], ["Zelek", "John", ""]]}, {"id": "2105.10568", "submitter": "Girish Chowdhary", "authors": "Michael McGuire, Chinmay Soman, Brian Diers, and Girish Chowdhary", "title": "High Throughput Soybean Pod-Counting with In-Field Robotic Data\n  Collection and Machine-Vision Based Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We report promising results for high-throughput on-field soybean pod count\nwith small mobile robots and machine-vision algorithms. Our results show that\nthe machine-vision based soybean pod counts are strongly correlated with\nsoybean yield. While pod counts has a strong correlation with soybean yield,\npod counting is extremely labor intensive, and has been difficult to automate.\nOur results establish that an autonomous robot equipped with vision sensors can\nautonomously collect soybean data at maturity. Machine-vision algorithms can be\nused to estimate pod-counts across a large diversity panel planted across\nexperimental units (EUs, or plots) in a high-throughput, automated manner. We\nreport a correlation of 0.67 between our automated pod counts and soybean\nyield. The data was collected in an experiment consisting of 1463 single-row\nplots maintained by the University of Illinois soybean breeding program during\nthe 2020 growing season. We also report a correlation of 0.88 between automated\npod counts and manual pod counts over a smaller data set of 16 plots.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:52:18 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:20:51 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["McGuire", "Michael", ""], ["Soman", "Chinmay", ""], ["Diers", "Brian", ""], ["Chowdhary", "Girish", ""]]}, {"id": "2105.10598", "submitter": "Coen Needell", "authors": "Coen D. Needell and Wilma A. Bainbridge", "title": "Embracing New Techniques in Deep Learning for Estimating Image\n  Memorability", "comments": "27 pages, 15 figures, Presented at the Proceedings of the Vision\n  Sciences Society 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Various work has suggested that the memorability of an image is consistent\nacross people, and thus can be treated as an intrinsic property of an image.\nUsing computer vision models, we can make specific predictions about what\npeople will remember or forget. While older work has used now-outdated deep\nlearning architectures to predict image memorability, innovations in the field\nhave given us new techniques to apply to this problem. Here, we propose and\nevaluate five alternative deep learning models which exploit developments in\nthe field from the last five years, largely the introduction of residual neural\nnetworks, which are intended to allow the model to use semantic information in\nthe memorability estimation process. These new models were tested against the\nprior state of the art with a combined dataset built to optimize both\nwithin-category and across-category predictions. Our findings suggest that the\nkey prior memorability network had overstated its generalizability and was\noverfit on its training set. Our new models outperform this prior model,\nleading us to conclude that Residual Networks outperform simpler convolutional\nneural networks in memorability regression. We make our new state-of-the-art\nmodel readily available to the research community, allowing memory researchers\nto make predictions about memorability on a wider range of images.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:05:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Needell", "Coen D.", ""], ["Bainbridge", "Wilma A.", ""]]}, {"id": "2105.10603", "submitter": "Subhash Chandra Sadhu", "authors": "Subhash Chandra Sadhu, Abhishek Singh, Tomohiro Maeda, Tristan\n  Swedish, Ryan Kim, Lagnojita Sinha, and Ramesh Raskar", "title": "Automatic calibration of time of flight based non-line-of-sight\n  reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Time of flight based Non-line-of-sight (NLOS) imaging approaches require\nprecise calibration of illumination and detector positions on the visible scene\nto produce reasonable results. If this calibration error is sufficiently high,\nreconstruction can fail entirely without any indication to the user. In this\nwork, we highlight the necessity of building autocalibration into NLOS\nreconstruction in order to handle mis-calibration. We propose a forward model\nof NLOS measurements that is differentiable with respect to both, the hidden\nscene albedo, and virtual illumination and detector positions. With only a mean\nsquared error loss and no regularization, our model enables joint\nreconstruction and recovery of calibration parameters by minimizing the\nmeasurement residual using gradient descent. We demonstrate our method is able\nto produce robust reconstructions using simulated and real data where the\ncalibration error applied causes other state of the art algorithms to fail.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:16:32 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sadhu", "Subhash Chandra", ""], ["Singh", "Abhishek", ""], ["Maeda", "Tomohiro", ""], ["Swedish", "Tristan", ""], ["Kim", "Ryan", ""], ["Sinha", "Lagnojita", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2105.10620", "submitter": "Siming Yan", "authors": "Siming Yan, Zhenpei Yang, Chongyang Ma, Haibin Huang, Etienne Vouga,\n  Qixing Huang", "title": "HPNet: Deep Primitive Segmentation Using Hybrid Representations", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces HPNet, a novel deep-learning approach for segmenting a\n3D shape represented as a point cloud into primitive patches. The key to deep\nprimitive segmentation is learning a feature representation that can separate\npoints of different primitives. Unlike utilizing a single feature\nrepresentation, HPNet leverages hybrid representations that combine one learned\nsemantic descriptor, two spectral descriptors derived from predicted geometric\nparameters, as well as an adjacency matrix that encodes sharp edges. Moreover,\ninstead of merely concatenating the descriptors, HPNet optimally combines\nhybrid representations by learning combination weights. This weighting module\nbuilds on the entropy of input features. The output primitive segmentation is\nobtained from a mean-shift clustering module. Experimental results on benchmark\ndatasets ANSI and ABCParts show that HPNet leads to significant performance\ngains from baseline approaches.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:12:46 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 14:40:35 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 14:19:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Yan", "Siming", ""], ["Yang", "Zhenpei", ""], ["Ma", "Chongyang", ""], ["Huang", "Haibin", ""], ["Vouga", "Etienne", ""], ["Huang", "Qixing", ""]]}, {"id": "2105.10626", "submitter": "Yuhao Huang", "authors": "Xin Yang, Yuhao Huang, Ruobing Huang, Haoran Dou, Rui Li, Jikuan Qian,\n  Xiaoqiong Huang, Wenlong Shi, Chaoyu Chen, Yuanji Zhang, Haixia Wang, Yi\n  Xiong, Dong Ni", "title": "Searching Collaborative Agents for Multi-plane Localization in 3D\n  Ultrasound", "comments": "Accepted by Medical Image Analysis (10 figures, 8 tabels)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D ultrasound (US) has become prevalent due to its rich spatial and\ndiagnostic information not contained in 2D US. Moreover, 3D US can contain\nmultiple standard planes (SPs) in one shot. Thus, automatically localizing SPs\nin 3D US has the potential to improve user-independence and\nscanning-efficiency. However, manual SP localization in 3D US is challenging\nbecause of the low image quality, huge search space and large anatomical\nvariability. In this work, we propose a novel multi-agent reinforcement\nlearning (MARL) framework to simultaneously localize multiple SPs in 3D US. Our\ncontribution is four-fold. First, our proposed method is general and it can\naccurately localize multiple SPs in different challenging US datasets. Second,\nwe equip the MARL system with a recurrent neural network (RNN) based\ncollaborative module, which can strengthen the communication among agents and\nlearn the spatial relationship among planes effectively. Third, we explore to\nadopt the neural architecture search (NAS) to automatically design the network\narchitecture of both the agents and the collaborative module. Last, we believe\nwe are the first to realize automatic SP localization in pelvic US volumes, and\nnote that our approach can handle both normal and abnormal uterus cases.\nExtensively validated on two challenging datasets of the uterus and fetal\nbrain, our proposed method achieves the average localization accuracy of 7.03\ndegrees/1.59mm and 9.75 degrees/1.19mm. Experimental results show that our\nlight-weight MARL model has higher accuracy than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:48:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Xin", ""], ["Huang", "Yuhao", ""], ["Huang", "Ruobing", ""], ["Dou", "Haoran", ""], ["Li", "Rui", ""], ["Qian", "Jikuan", ""], ["Huang", "Xiaoqiong", ""], ["Shi", "Wenlong", ""], ["Chen", "Chaoyu", ""], ["Zhang", "Yuanji", ""], ["Wang", "Haixia", ""], ["Xiong", "Yi", ""], ["Ni", "Dong", ""]]}, {"id": "2105.10633", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi-Dehkordi", "title": "Revisiting Knowledge Distillation for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The existing solutions for object detection distillation rely on the\navailability of both a teacher model and ground-truth labels. We propose a new\nperspective to relax this constraint. In our framework, a student is first\ntrained with pseudo labels generated by the teacher, and then fine-tuned using\nlabeled data, if any available. Extensive experiments demonstrate improvements\nover existing object detection distillation algorithms. In addition, decoupling\nthe teacher and ground-truth distillation in this framework provides\ninteresting properties such: as 1) using unlabeled data to further improve the\nstudent's performance, 2) combining multiple teacher models of different\narchitectures, even with different object categories, and 3) reducing the need\nfor labeled data (with only 20% of COCO labels, this method achieves the same\nperformance as the model trained on the entire set of labels). Furthermore, a\nby-product of this approach is the potential usage for domain adaptation. We\nverify these properties through extensive experiments.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 03:46:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Banitalebi-Dehkordi", "Amin", ""]]}, {"id": "2105.10644", "submitter": "Yusuke Ohtsubo", "authors": "Yusuke Ohtsubo, Tetsu Matsukawa, Einoshin Suzuki", "title": "Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid\n  Models", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a deep invertible hybrid model which integrates\ndiscriminative and generative learning at a latent space level for\nsemi-supervised few-shot classification. Various tasks for classifying new\nspecies from image data can be modeled as a semi-supervised few-shot\nclassification, which assumes a labeled and unlabeled training examples and a\nsmall support set of the target classes. Predicting target classes with a few\nsupport examples per class makes the learning task difficult for existing\nsemi-supervised classification methods, including selftraining, which\niteratively estimates class labels of unlabeled training examples to learn a\nclassifier for the training classes. To exploit unlabeled training examples\neffectively, we adopt as the objective function the composite likelihood, which\nintegrates discriminative and generative learning and suits better with deep\nneural networks than the parameter coupling prior, the other popular integrated\nlearning approach. In our proposed model, the discriminative and generative\nmodels are respectively Prototypical Networks, which have shown excellent\nperformance in various kinds of few-shot learning, and Normalizing Flow a deep\ninvertible model which returns the exact marginal likelihood unlike the other\nthree major methods, i.e., VAE, GAN, and autoregressive model. Our main\noriginality lies in our integration of these components at a latent space\nlevel, which is effective in preventing overfitting. Experiments using\nmini-ImageNet and VGG-Face datasets show that our method outperforms\nselftraining based Prototypical Networks.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 05:55:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ohtsubo", "Yusuke", ""], ["Matsukawa", "Tetsu", ""], ["Suzuki", "Einoshin", ""]]}, {"id": "2105.10650", "submitter": "Hangjie Ji", "authors": "Hangjie Ji, Kyle Lafata, Yvonne Mowery, David Brizel, Andrea L.\n  Bertozzi, Fang-Fang Yin, Chunhao Wang", "title": "Post-Radiotherapy PET Image Outcome Prediction by Deep Learning under\n  Biological Model Guidance: A Feasibility Study of Oropharyngeal Cancer\n  Application", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops a method of biologically guided deep learning for\npost-radiation FDG-PET image outcome prediction based on pre-radiation images\nand radiotherapy dose information. Based on the classic reaction-diffusion\nmechanism, a novel biological model was proposed using a partial differential\nequation that incorporates spatial radiation dose distribution as a\npatient-specific treatment information variable. A 7-layer\nencoder-decoder-based convolutional neural network (CNN) was designed and\ntrained to learn the proposed biological model. As such, the model could\ngenerate post-radiation FDG-PET image outcome predictions with possible\ntime-series transition from pre-radiotherapy image states to post-radiotherapy\nstates. The proposed method was developed using 64 oropharyngeal patients with\npaired FDG-PET studies before and after 20Gy delivery (2Gy/daily fraction) by\nIMRT. In a two-branch deep learning execution, the proposed CNN learns specific\nterms in the biological model from paired FDG-PET images and spatial dose\ndistribution as in one branch, and the biological model generates post-20Gy\nFDG-PET image prediction in the other branch. The proposed method successfully\ngenerated post-20Gy FDG-PET image outcome prediction with breakdown\nillustrations of biological model components. Time-series FDG-PET image\npredictions were generated to demonstrate the feasibility of disease response\nrendering. The developed biologically guided deep learning method achieved\npost-20Gy FDG-PET image outcome predictions in good agreement with ground-truth\nresults. With break-down biological modeling components, the outcome image\npredictions could be used in adaptive radiotherapy decision-making to optimize\npersonalized plans for the best outcome in the future.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 06:32:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ji", "Hangjie", ""], ["Lafata", "Kyle", ""], ["Mowery", "Yvonne", ""], ["Brizel", "David", ""], ["Bertozzi", "Andrea L.", ""], ["Yin", "Fang-Fang", ""], ["Wang", "Chunhao", ""]]}, {"id": "2105.10678", "submitter": "Chih-Ting Liu", "authors": "Chih-Ting Liu, Jun-Cheng Chen, Chu-Song Chen, Shao-Yi Chien", "title": "Video-based Person Re-identification without Bells and Whistles", "comments": "This paper was accepted by CVPR 2021 Biometrics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) aims at matching the video\ntracklets with cropped video frames for identifying the pedestrians under\ndifferent cameras. However, there exists severe spatial and temporal\nmisalignment for those cropped tracklets due to the imperfect detection and\ntracking results generated with obsolete methods. To address this issue, we\npresent a simple re-Detect and Link (DL) module which can effectively reduce\nthose unexpected noise through applying the deep learning-based detection and\ntracking on the cropped tracklets. Furthermore, we introduce an improved model\ncalled Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical\nNon-local Network, we replace the non-local module with three 1-D\nposition-sensitive axial attentions, in addition to our proposed coarse-to-fine\nstructure. With the developed CF-AAN, compared to the original non-local\noperation, we can not only significantly reduce the computation cost but also\nobtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on\nthe large-scale MARS dataset. Meanwhile, by simply adopting our DL module for\ndata alignment, to our surprise, several baseline models can achieve better or\ncomparable results with the current state-of-the-arts. Besides, we discover the\nerrors not only for the identity labels of tracklets but also for the\nevaluation protocol for the test data of MARS. We hope that our work can help\nthe community for the further development of invariant representation without\nthe hassle of the spatial and temporal alignment and dataset noise. The code,\ncorrected labels, evaluation protocol, and the aligned data will be available\nat https://github.com/jackie840129/CF-AAN.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:17:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Chih-Ting", ""], ["Chen", "Jun-Cheng", ""], ["Chen", "Chu-Song", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2105.10686", "submitter": "Baudouin Denis de Senneville PhD", "authors": "Vincent Estrade, Michel Daudon, Emmanuel Richard, Jean-Christophe\n  Bernhard, Franck Bladou, Gregoire Robert, Baudouin Denis de Senneville", "title": "Towards Automatic Recognition of Pure & Mixed Stones using\n  Intraoperative Endoscopic Digital Images", "comments": "19 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To assess automatic computer-aided in-situ recognition of\nmorphological features of pure and mixed urinary stones using intraoperative\ndigital endoscopic images acquired in a clinical setting. Materials and\nmethods: In this single-centre study, an experienced urologist intraoperatively\nand prospectively examined the surface and section of all kidney stones\nencountered. Calcium oxalate monohydrate (COM/Ia), dihydrate (COD/IIb) and uric\nacid (UA/IIIb) morphological criteria were collected and classified to generate\nannotated datasets. A deep convolutional neural network (CNN) was trained to\npredict the composition of both pure and mixed stones. To explain the\npredictions of the deep neural network model, coarse localisation heat-maps\nwere plotted to pinpoint key areas identified by the network. Results: This\nstudy included 347 and 236 observations of stone surface and stone section,\nrespectively. A highest sensitivity of 98 % was obtained for the type \"pure\nIIIb/UA\" using surface images. The most frequently encountered morphology was\nthat of the type \"pure Ia/COM\"; it was correctly predicted in 91 % and 94 % of\ncases using surface and section images, respectively. Of the mixed type\n\"Ia/COM+IIb/COD\", Ia/COM was predicted in 84 % of cases using surface images,\nIIb/COD in 70 % of cases, and both in 65 % of cases. Concerning mixed\nIa/COM+IIIb/UA stones, Ia/COM was predicted in 91 % of cases using section\nimages, IIIb/UA in 69 % of cases, and both in 74 % of cases. Conclusions: This\npreliminary study demonstrates that deep convolutional neural networks are\npromising to identify kidney stone composition from endoscopic images acquired\nintraoperatively. Both pure and mixed stone composition could be discriminated.\nCollected in a clinical setting, surface and section images analysed by deep\nCNN provide valuable information about stone morphology for computer-aided\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:52:19 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Estrade", "Vincent", ""], ["Daudon", "Michel", ""], ["Richard", "Emmanuel", ""], ["Bernhard", "Jean-Christophe", ""], ["Bladou", "Franck", ""], ["Robert", "Gregoire", ""], ["de Senneville", "Baudouin Denis", ""]]}, {"id": "2105.10697", "submitter": "Zhen Liu", "authors": "Zhen Liu, Wenjie Lin, Xinpeng Li, Qing Rao, Ting Jiang, Mingyan Han,\n  Haoqiang Fan, Jian Sun, Shuaicheng Liu", "title": "ADNet: Attention-guided Deformable Convolutional Network for High\n  Dynamic Range Imaging", "comments": "Accepted by CVPRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an attention-guided deformable convolutional\nnetwork for hand-held multi-frame high dynamic range (HDR) imaging, namely\nADNet. This problem comprises two intractable challenges of how to handle\nsaturation and noise properly and how to tackle misalignments caused by object\nmotion or camera jittering. To address the former, we adopt a spatial attention\nmodule to adaptively select the most appropriate regions of various exposure\nlow dynamic range (LDR) images for fusion. For the latter one, we propose to\nalign the gamma-corrected images in the feature-level with a Pyramid, Cascading\nand Deformable (PCD) alignment module. The proposed ADNet shows\nstate-of-the-art performance compared with previous methods, achieving a\nPSNR-$l$ of 39.4471 and a PSNR-$\\mu$ of 37.6359 in NTIRE 2021 Multi-Frame HDR\nChallenge.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:37:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Zhen", ""], ["Lin", "Wenjie", ""], ["Li", "Xinpeng", ""], ["Rao", "Qing", ""], ["Jiang", "Ting", ""], ["Han", "Mingyan", ""], ["Fan", "Haoqiang", ""], ["Sun", "Jian", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2105.10699", "submitter": "Yulin Shao", "authors": "Yulin Shao and Soung Chang Liew and Deniz Gunduz", "title": "Denoising Noisy Neural Networks: A Bayesian Approach with Compensation", "comments": "Keywords: Noisy neural network, Bayesian estimation, analog device,\n  federated edge learning, over-the-air computation, analog storage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy neural networks (NoisyNNs) refer to the inference and training of NNs\nin the presence of noise. Noise is inherent in most communication and storage\nsystems; hence, NoisyNNs emerge in many new applications, including federated\nedge learning, where wireless devices collaboratively train a NN over a noisy\nwireless channel, or when NNs are implemented/stored in an analog storage\nmedium. This paper studies a fundamental problem of NoisyNNs: how to estimate\nthe uncontaminated NN weights from their noisy observations or manifestations.\nWhereas all prior works relied on the maximum likelihood (ML) estimation to\nmaximize the likelihood function of the estimated NN weights, this paper\ndemonstrates that the ML estimator is in general suboptimal. To overcome the\nsuboptimality of the conventional ML estimator, we put forth an\n$\\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)\nwith a population compensator and a bias compensator. Our approach works well\nfor NoisyNNs arising in both 1) noisy inference, where noise is introduced only\nin the inference phase on the already-trained NN weights; and 2) noisy\ntraining, where noise is introduced over the course of training. Extensive\nexperiments on the CIFAR-10 and SST-2 datasets with different NN architectures\nverify the significant performance gains of the $\\text{MMSE}_{pb}$ estimator\nover the ML estimator when used to denoise the NoisyNN. For noisy inference,\nthe average gains are up to $156\\%$ for a noisy ResNet34 model and $14.7\\%$ for\na noisy BERT model; for noisy training, the average gains are up to $18.1$ dB\nfor a noisy ResNet18 model.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:51:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shao", "Yulin", ""], ["Liew", "Soung Chang", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2105.10700", "submitter": "Jos\\'e Henrique Brito", "authors": "Eloi Martins, Jos\\'e Henrique Brito", "title": "Soccer Player Tracking in Low Quality Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we propose a system capable of tracking multiple soccer players\nin different types of video quality. The main goal, in contrast to most\nstate-of-art soccer player tracking systems, is the ability of execute\neffectively tracking in videos of low-quality. We adapted a state-of-art\nMultiple Object Tracking to the task. In order to do that adaptation, we\ncreated a Detection and a Tracking Dataset for 3 different qualities of video.\nThe results of our system are conclusive of its high performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:55:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Martins", "Eloi", ""], ["Brito", "Jos\u00e9 Henrique", ""]]}, {"id": "2105.10702", "submitter": "Aydan Gasimova", "authors": "Aydan Gasimova, Giovanni Montana, Daniel Rueckert", "title": "Automated Knee X-ray Report Generation", "comments": null, "journal-ref": "NeurIPS Machine Learning for Health Workshop 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gathering manually annotated images for the purpose of training a predictive\nmodel is far more challenging in the medical domain than for natural images as\nit requires the expertise of qualified radiologists. We therefore propose to\ntake advantage of past radiological exams (specifically, knee X-ray\nexaminations) and formulate a framework capable of learning the correspondence\nbetween the images and reports, and hence be capable of generating diagnostic\nreports for a given X-ray examination consisting of an arbitrary number of\nimage views. We demonstrate how aggregating the image features of individual\nexams and using them as conditional inputs when training a language generation\nmodel results in auto-generated exam reports that correlate well with\nradiologist-generated reports.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:59:42 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gasimova", "Aydan", ""], ["Montana", "Giovanni", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2105.10735", "submitter": "Mina Khan", "authors": "Mina Khan and Pattie Maes", "title": "PAL: Intelligence Augmentation using Egocentric Visual Context Detection", "comments": null, "journal-ref": "CVPR EPIC Workshop 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Egocentric visual context detection can support intelligence augmentation\napplications. We created a wearable system, called PAL, for wearable,\npersonalized, and privacy-preserving egocentric visual context detection. PAL\nhas a wearable device with a camera, heart-rate sensor, on-device deep\nlearning, and audio input/output. PAL also has a mobile/web application for\npersonalized context labeling. We used on-device deep learning models for\ngeneric object and face detection, low-shot custom face and context recognition\n(e.g., activities like brushing teeth), and custom context clustering (e.g.,\nindoor locations). The models had over 80\\% accuracy in in-the-wild contexts\n(~1000 images) and we tested PAL for intelligence augmentation applications\nlike behavior change. We have made PAL is open-source to further support\nintelligence augmentation using personalized and privacy-preserving egocentric\nvisual contexts.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 14:01:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Khan", "Mina", ""], ["Maes", "Pattie", ""]]}, {"id": "2105.10738", "submitter": "Jin Zhu", "authors": "Jin Zhu, Chuan Tan, Junwei Yang, Guang Yang and Pietro Lio'", "title": "MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Single image super-resolution (SISR) aims to obtain a high-resolution output\nfrom one low-resolution image. Currently, deep learning-based SISR approaches\nhave been widely discussed in medical image processing, because of their\npotential to achieve high-quality, high spatial resolution images without the\ncost of additional scans. However, most existing methods are designed for\nscale-specific SR tasks and are unable to generalise over magnification scales.\nIn this paper, we propose an approach for medical image arbitrary-scale\nsuper-resolution (MIASSR), in which we couple meta-learning with generative\nadversarial networks (GANs) to super-resolve medical images at any scale of\nmagnification in (1, 4]. Compared to state-of-the-art SISR algorithms on\nsingle-modal magnetic resonance (MR) brain images (OASIS-brains) and\nmulti-modal MR brain images (BraTS), MIASSR achieves comparable fidelity\nperformance and the best perceptual quality with the smallest model size. We\nalso employ transfer learning to enable MIASSR to tackle SR tasks of new\nmedical modalities, such as cardiac MR images (ACDC) and chest computed\ntomography images (COVID-CT). The source code of our work is also public. Thus,\nMIASSR has the potential to become a new foundational pre-/post-processing step\nin clinical image analysis tasks such as reconstruction, image quality\nenhancement, and segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 14:24:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhu", "Jin", ""], ["Tan", "Chuan", ""], ["Yang", "Junwei", ""], ["Yang", "Guang", ""], ["Lio'", "Pietro", ""]]}, {"id": "2105.10782", "submitter": "Kevin Duarte", "authors": "Kevin Duarte, Yogesh S. Rawat, Mubarak Shah", "title": "PLM: Partial Label Masking for Imbalanced Multi-label Classification", "comments": "Accepted to the CVPR 2021 Learning from Limited or Imperfect Data\n  (L2ID) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks trained on real-world datasets with long-tailed label\ndistributions are biased towards frequent classes and perform poorly on\ninfrequent classes. The imbalance in the ratio of positive and negative samples\nfor each class skews network output probabilities further from ground-truth\ndistributions. We propose a method, Partial Label Masking (PLM), which utilizes\nthis ratio during training. By stochastically masking labels during loss\ncomputation, the method balances this ratio for each class, leading to improved\nrecall on minority classes and improved precision on frequent classes. The\nratio is estimated adaptively based on the network's performance by minimizing\nthe KL divergence between predicted and ground-truth distributions. Whereas\nmost existing approaches addressing data imbalance are mainly focused on\nsingle-label classification and do not generalize well to the multi-label case,\nthis work proposes a general approach to solve the long-tail data imbalance\nissue for multi-label classification. PLM is versatile: it can be applied to\nmost objective functions and it can be used alongside other strategies for\nclass imbalance. Our method achieves strong performance when compared to\nexisting methods on both multi-label (MultiMNIST and MSCOCO) and single-label\n(imbalanced CIFAR-10 and CIFAR-100) image classification datasets.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 18:07:56 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Duarte", "Kevin", ""], ["Rawat", "Yogesh S.", ""], ["Shah", "Mubarak", ""]]}, {"id": "2105.10793", "submitter": "Henri Marcelo Tomas", "authors": "Henri Tomas, Marcus Reyes, Raimarc Dionido, Mark Ty, Jonric Mirando,\n  Joel Casimiro, Rowel Atienza, Richard Guinto", "title": "GOO: A Dataset for Gaze Object Prediction in Retail Environments", "comments": "CVPR 20201 Workshop on Gaze Estimation and Prediction in the Wild\n  (GAZE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the most fundamental and information-laden actions humans do is to\nlook at objects. However, a survey of current works reveals that existing\ngaze-related datasets annotate only the pixel being looked at, and not the\nboundaries of a specific object of interest. This lack of object annotation\npresents an opportunity for further advancing gaze estimation research. To this\nend, we present a challenging new task called gaze object prediction, where the\ngoal is to predict a bounding box for a person's gazed-at object. To train and\nevaluate gaze networks on this task, we present the Gaze On Objects (GOO)\ndataset. GOO is composed of a large set of synthetic images (GOO Synth)\nsupplemented by a smaller subset of real images (GOO-Real) of people looking at\nobjects in a retail environment. Our work establishes extensive baselines on\nGOO by re-implementing and evaluating selected state-of-the art models on the\ntask of gaze following and domain adaptation. Code is available on github.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 18:55:35 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 03:00:55 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Tomas", "Henri", ""], ["Reyes", "Marcus", ""], ["Dionido", "Raimarc", ""], ["Ty", "Mark", ""], ["Mirando", "Jonric", ""], ["Casimiro", "Joel", ""], ["Atienza", "Rowel", ""], ["Guinto", "Richard", ""]]}, {"id": "2105.10825", "submitter": "Matthew Hirn", "authors": "Jieqian He and Matthew Hirn", "title": "Texture synthesis via projection onto multiscale, multilayer statistics", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new model for texture synthesis based on a multiscale,\nmultilayer feature extractor. Within the model, textures are represented by a\nset of statistics computed from ReLU wavelet coefficients at different layers,\nscales and orientations. A new image is synthesized by matching the target\nstatistics via an iterative projection algorithm. We explain the necessity of\nthe different types of pre-defined wavelet filters used in our model and the\nadvantages of multilayer structures for image synthesis. We demonstrate the\npower of our model by generating samples of high quality textures and providing\ninsights into deep representations for texture images.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 23:32:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["He", "Jieqian", ""], ["Hirn", "Matthew", ""]]}, {"id": "2105.10827", "submitter": "Agostina Larrazabal", "authors": "Agostina J. Larrazabal, C\\'esar Mart\\'inez, Jose Dolz and Enzo\n  Ferrante", "title": "Orthogonal Ensemble Networks for Biomedical Image Segmentation", "comments": "Accepted for publication at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the astonishing performance of deep-learning based approaches for\nvisual tasks such as semantic segmentation, they are known to produce\nmiscalibrated predictions, which could be harmful for critical decision-making\nprocesses. Ensemble learning has shown to not only boost the performance of\nindividual models but also reduce their miscalibration by averaging independent\npredictions. In this scenario, model diversity has become a key factor, which\nfacilitates individual models converging to different functional solutions. In\nthis work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework\nto explicitly enforce model diversity by means of orthogonal constraints. The\nproposed method is based on the hypothesis that inducing orthogonality among\nthe constituents of the ensemble will increase the overall model diversity. We\nresort to a new pairwise orthogonality constraint which can be used to\nregularize a sequential ensemble training process, resulting on improved\npredictive performance and better calibrated model outputs. We benchmark the\nproposed framework in two challenging brain lesion segmentation tasks --brain\ntumor and white matter hyper-intensity segmentation in MR images. The\nexperimental results show that our approach produces more robust and\nwell-calibrated ensemble models and can deal with challenging tasks in the\ncontext of biomedical image segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 23:44:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Larrazabal", "Agostina J.", ""], ["Mart\u00ednez", "C\u00e9sar", ""], ["Dolz", "Jose", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2105.10831", "submitter": "Soumyabrata Dev", "authors": "Hewei Wang, Muhammad Salman Pathan, and Soumyabrata Dev", "title": "Stereo Matching Based on Visual Sensitive Information", "comments": "Published in 6th IEEE International Conference on Image, Vision and\n  Computing (ICIVC), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of computer vision is one of the most discussed topics amongst many\nscholars, and stereo matching is its most important sub fields. After the\nparallax map is transformed into a depth map, it can be applied to many\nintelligent fields. In this paper, a stereo matching algorithm based on visual\nsensitive information is proposed by using standard images from Middlebury\ndataset. Aiming at the limitation of traditional stereo matching algorithms\nregarding the cost window, a cost aggregation algorithm based on the dynamic\nwindow is proposed, and the disparity image is optimized by using left and\nright consistency detection to further reduce the error matching rate. The\nexperimental results show that the proposed algorithm can effectively enhance\nthe stereo matching effect of the image providing significant improvement in\naccuracy as compared with the classical census algorithm. The proposed model\ncode, dataset, and experimental results are available at\nhttps://github.com/WangHewei16/Stereo-Matching.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 00:20:32 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Hewei", ""], ["Pathan", "Muhammad Salman", ""], ["Dev", "Soumyabrata", ""]]}, {"id": "2105.10837", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu, Naveen Sehgal, Sarah Ostadabbas", "title": "Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D\n  Pose Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ultimate goal for an inference model is to be robust and functional in\nreal life applications. However, training vs. test data domain gaps often\nnegatively affect model performance. This issue is especially critical for the\nmonocular 3D human pose estimation problem, in which 3D human data is often\ncollected in a controlled lab setting. In this paper, we focus on alleviating\nthe negative effect of domain shift by presenting our adapted human pose (AHuP)\napproach that addresses adaptation problems in both appearance and pose spaces.\nAHuP is built around a practical assumption that in real applications, data\nfrom target domain could be inaccessible or only limited information can be\nacquired. We illustrate the 3D pose estimation performance of AHuP in two\nscenarios. First, when source and target data differ significantly in both\nappearance and pose spaces, in which we learn from synthetic 3D human data\n(with zero real 3D human data) and show comparable performance with the\nstate-of-the-art 3D pose estimation models that have full access to the real 3D\nhuman pose benchmarks for training. Second, when source and target datasets\ndiffer mainly in the pose space, in which AHuP approach can be applied to\nfurther improve the performance of the state-of-the-art models when tested on\nthe datasets different from their training dataset.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 01:20:40 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Shuangjun", ""], ["Sehgal", "Naveen", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "2105.10843", "submitter": "Jinyu Yang", "authors": "Jinyu Yang, Chunyuan Li, Weizhi An, Hehuan Ma, Yuzhi Guo, Yu Rong,\n  Peilin Zhao, Junzhou Huang", "title": "Exploring Robustness of Unsupervised Domain Adaptation in Semantic\n  Segmentation", "comments": "ICCV 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies imply that deep neural networks are vulnerable to adversarial\nexamples -- inputs with a slight but intentional perturbation are incorrectly\nclassified by the network. Such vulnerability makes it risky for some\nsecurity-related applications (e.g., semantic segmentation in autonomous cars)\nand triggers tremendous concerns on the model reliability. For the first time,\nwe comprehensively evaluate the robustness of existing UDA methods and propose\na robust UDA approach. It is rooted in two observations: (i) the robustness of\nUDA methods in semantic segmentation remains unexplored, which pose a security\nconcern in this field; and (ii) although commonly used self-supervision (e.g.,\nrotation and jigsaw) benefits image tasks such as classification and\nrecognition, they fail to provide the critical supervision signals that could\nlearn discriminative representation for segmentation tasks. These observations\nmotivate us to propose adversarial self-supervision UDA (or ASSUDA) that\nmaximizes the agreement between clean images and their adversarial examples by\na contrastive loss in the output space. Extensive empirical studies on commonly\nused benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 01:50:44 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 17:13:43 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yang", "Jinyu", ""], ["Li", "Chunyuan", ""], ["An", "Weizhi", ""], ["Ma", "Hehuan", ""], ["Guo", "Yuzhi", ""], ["Rong", "Yu", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""]]}, {"id": "2105.10859", "submitter": "Dipika Singhania", "authors": "Dipika Singhania, Rahul Rahaman, Angela Yao", "title": "Coarse to Fine Multi-Resolution Temporal Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal convolutional networks (TCNs) are a commonly used architecture for\ntemporal video segmentation. TCNs however, tend to suffer from\nover-segmentation errors and require additional refinement modules to ensure\nsmoothness and temporal coherency. In this work, we propose a novel temporal\nencoder-decoder to tackle the problem of sequence fragmentation. In particular,\nthe decoder follows a coarse-to-fine structure with an implicit ensemble of\nmultiple temporal resolutions. The ensembling produces smoother segmentations\nthat are more accurate and better-calibrated, bypassing the need for additional\nrefinement modules. In addition, we enhance our training with a\nmulti-resolution feature-augmentation strategy to promote robustness to varying\ntemporal resolutions. Finally, to support our architecture and encourage\nfurther sequence coherency, we propose an action loss that penalizes\nmisclassifications at the video level. Experiments show that our stand-alone\narchitecture, together with our novel feature-augmentation strategy and new\nloss, outperforms the state-of-the-art on three temporal video segmentation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 06:07:40 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Singhania", "Dipika", ""], ["Rahaman", "Rahul", ""], ["Yao", "Angela", ""]]}, {"id": "2105.10860", "submitter": "Pan Chen", "authors": "Pan Chen, Danfeng Hong, Zhengchao Chen, Xuan Yang, Baipeng Li, Bing\n  Zhang", "title": "FCCDN: Feature Constraint Network for VHR Image Change Detection", "comments": "33 pages, 12 figures. Submitted to ISPRS Journal of Photogrammetry\n  and Remote Sensing. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is the process of identifying pixel-wise differences of\nbi-temporal co-registered images. It is of great significance to Earth\nobservation. Recently, with the emerging of deep learning (DL), deep\nconvolutional neural networks (CNNs) based methods have shown their power and\nfeasibility in the field of change detection. However, there is still a lack of\neffective supervision for change feature learning. In this work, a feature\nconstraint change detection network (FCCDN) is proposed. We constrain features\nboth on bi-temporal feature extraction and feature fusion. More specifically,\nwe propose a dual encoder-decoder network backbone for the change detection\ntask. At the center of the backbone, we design a non-local feature pyramid\nnetwork to extract and fuse multi-scale features. To fuse bi-temporal features\nin a robust way, we build a dense connection-based feature fusion module.\nMoreover, a self-supervised learning-based strategy is proposed to constrain\nfeature learning. Based on FCCDN, we achieve state-of-the-art performance on\ntwo building change detection datasets (LEVIR-CD and WHU). On the LEVIR-CD\ndataset, we achieve IoU of 0.8569 and F1 score of 0.9229. On the WHU dataset,\nwe achieve IoU of 0.8820 and F1 score of 0.9373. Moreover, we, for the first\ntime, achieve the acquire of accurate bi-temporal semantic segmentation results\nwithout using semantic segmentation labels. It is vital for the application of\nchange detection because it saves the cost of labeling.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 06:13:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Pan", ""], ["Hong", "Danfeng", ""], ["Chen", "Zhengchao", ""], ["Yang", "Xuan", ""], ["Li", "Baipeng", ""], ["Zhang", "Bing", ""]]}, {"id": "2105.10872", "submitter": "Hao Huang", "authors": "Hao Huang, Yongtao Wang, Zhaoyu Chen, Yuheng Li, Zhi Tang, Wei Chu,\n  Jingdong Chen, Weisi Lin, Kai-Kuang Ma", "title": "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for\n  Combating Deepfakes", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Malicious application of deepfakes (i.e., technologies can generate target\nfaces or face attributes) has posed a huge threat to our society. The fake\nmultimedia content generated by deepfake models can harm the reputation and\neven threaten the property of the person who has been impersonated.\nFortunately, the adversarial watermark could be used for combating deepfake\nmodels, leading them to generate distorted images. The existing methods require\nan individual training process for every facial image, to generate the\nadversarial watermark against a specific deepfake model, which are extremely\ninefficient. To address this problem, we propose a universal adversarial attack\nmethod on deepfake models, to generate a Cross-Model Universal Adversarial\nWatermark (CMUA-Watermark) that can protect thousands of facial images from\nmultiple deepfake models. Specifically, we first propose a cross-model\nuniversal attack pipeline by attacking multiple deepfake models and combining\ngradients from these models iteratively. Then we introduce a batch-based method\nto alleviate the conflict of adversarial watermarks generated by different\nfacial images. Finally, we design a more reasonable and comprehensive\nevaluation method for evaluating the effectiveness of the adversarial\nwatermark. Experimental results demonstrate that the proposed CMUA-Watermark\ncan effectively distort the fake facial images generated by deepfake models and\nsuccessfully protect facial images from deepfakes in real scenes.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 07:28:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Huang", "Hao", ""], ["Wang", "Yongtao", ""], ["Chen", "Zhaoyu", ""], ["Li", "Yuheng", ""], ["Tang", "Zhi", ""], ["Chu", "Wei", ""], ["Chen", "Jingdong", ""], ["Lin", "Weisi", ""], ["Ma", "Kai-Kuang", ""]]}, {"id": "2105.10882", "submitter": "Guoliang Hua", "authors": "Guoliang Hua, Wenhao Li, Qian Zhang, Runwei Ding, Hong Liu", "title": "Weakly-supervised Cross-view 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although monocular 3D human pose estimation methods have made significant\nprogress, it's far from being solved due to the inherent depth ambiguity.\nInstead, exploiting multi-view information is a practical way to achieve\nabsolute 3D human pose estimation. In this paper, we propose a simple yet\neffective pipeline for weakly-supervised cross-view 3D human pose estimation.\nBy only using two camera views, our method can achieve state-of-the-art\nperformance in a weakly-supervised manner, requiring no 3D ground truth but\nonly 2D annotations. Specifically, our method contains two steps: triangulation\nand refinement. First, given the 2D keypoints that can be obtained through any\nclassic 2D detection methods, triangulation is performed across two views to\nlift the 2D keypoints into coarse 3D poses.Then, a novel cross-view U-shaped\ngraph convolutional network (CV-UGCN), which can explore the spatial\nconfigurations and cross-view correlations, is designed to refine the coarse 3D\nposes. In particular, the refinement progress is achieved through\nweakly-supervised learning, in which geometric and structure-aware consistency\nchecks are performed. We evaluate our method on the standard benchmark dataset,\nHuman3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4\nmm, which outperforms the state-of-the-arts remarkably (27.4 mm vs 30.2 mm).\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:16:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hua", "Guoliang", ""], ["Li", "Wenhao", ""], ["Zhang", "Qian", ""], ["Ding", "Runwei", ""], ["Liu", "Hong", ""]]}, {"id": "2105.10886", "submitter": "Zhaoyang Huang", "authors": "Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei\n  Zhou, Hujun Bao, Guofeng Zhang, Hongsheng Li", "title": "VS-Net: Voting with Segmentation for Visual Localization", "comments": "Project Page: https://drinkingcoder.github.io/publication/vs-net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual localization is of great importance in robotics and computer vision.\nRecently, scene coordinate regression based methods have shown good performance\nin visual localization in small static scenes. However, it still estimates\ncamera poses from many inferior scene coordinates. To address this problem, we\npropose a novel visual localization framework that establishes 2D-to-3D\ncorrespondences between the query image and the 3D map with a series of\nlearnable scene-specific landmarks. In the landmark generation stage, the 3D\nsurfaces of the target scene are over-segmented into mosaic patches whose\ncenters are regarded as the scene-specific landmarks. To robustly and\naccurately recover the scene-specific landmarks, we propose the Voting with\nSegmentation Network (VS-Net) to segment the pixels into different landmark\npatches with a segmentation branch and estimate the landmark locations within\neach patch with a landmark location voting branch. Since the number of\nlandmarks in a scene may reach up to 5000, training a segmentation network with\nsuch a large number of classes is both computation and memory costly for the\ncommonly used cross-entropy loss. We propose a novel prototype-based triplet\nloss with hard negative mining, which is able to train semantic segmentation\nnetworks with a large number of labels efficiently. Our proposed VS-Net is\nextensively tested on multiple public benchmarks and can outperform\nstate-of-the-art visual localization methods. Code and models are available at\n\\href{https://github.com/zju3dv/VS-Net}{https://github.com/zju3dv/VS-Net}.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:44:11 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Huang", "Zhaoyang", ""], ["Zhou", "Han", ""], ["Li", "Yijin", ""], ["Yang", "Bangbang", ""], ["Xu", "Yan", ""], ["Zhou", "Xiaowei", ""], ["Bao", "Hujun", ""], ["Zhang", "Guofeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2105.10902", "submitter": "Ikram Kourbane", "authors": "Ikram Kourbane, Yakup Genc", "title": "A hybrid classification-regression approach for 3D hand pose estimation\n  using graph convolutional networks", "comments": "18 pages, 8 figures, 6 tables, 4 Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hand pose estimation is a crucial part of a wide range of augmented reality\nand human-computer interaction applications. Predicting the 3D hand pose from a\nsingle RGB image is challenging due to occlusion and depth ambiguities.\nGCN-based (Graph Convolutional Networks) methods exploit the structural\nrelationship similarity between graphs and hand joints to model kinematic\ndependencies between joints. These techniques use predefined or globally\nlearned joint relationships, which may fail to capture pose-dependent\nconstraints. To address this problem, we propose a two-stage GCN-based\nframework that learns per-pose relationship constraints. Specifically, the\nfirst phase quantizes the 2D/3D space to classify the joints into 2D/3D blocks\nbased on their locality. This spatial dependency information guides this phase\nto estimate reliable 2D and 3D poses. The second stage further improves the 3D\nestimation through a GCN-based module that uses an adaptative nearest neighbor\nalgorithm to determine joint relationships. Extensive experiments show that our\nmulti-stage GCN approach yields an efficient model that produces accurate 2D/3D\nhand poses and outperforms the state-of-the-art on two public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 10:09:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kourbane", "Ikram", ""], ["Genc", "Yakup", ""]]}, {"id": "2105.10904", "submitter": "Ikram Kourbane", "authors": "Ikram Kourbane, Yakup Genc", "title": "Skeleton-aware multi-scale heatmap regression for 2D hand pose\n  estimation", "comments": "5 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing RGB-based 2D hand pose estimation methods learn the joint locations\nfrom a single resolution, which is not suitable for different hand sizes. To\ntackle this problem, we propose a new deep learning-based framework that\nconsists of two main modules. The former presents a segmentation-based approach\nto detect the hand skeleton and localize the hand bounding box. The second\nmodule regresses the 2D joint locations through a multi-scale heatmap\nregression approach that exploits the predicted hand skeleton as a constraint\nto guide the model. Furthermore, we construct a new dataset that is suitable\nfor both hand detection and pose estimation. We qualitatively and\nquantitatively validate our method on two datasets. Results demonstrate that\nthe proposed method outperforms state-of-the-art and can recover the pose even\nin cluttered images and complex poses.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 10:23:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kourbane", "Ikram", ""], ["Genc", "Yakup", ""]]}, {"id": "2105.10920", "submitter": "Lu He", "authors": "Lu He, Qianyu Zhou, Xiangtai Li, Li Niu, Guangliang Cheng, Xiao Li,\n  Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang", "title": "End-to-End Video Object Detection with Spatial-Temporal Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, DETR and Deformable DETR have been proposed to eliminate the need\nfor many hand-designed components in object detection while demonstrating good\nperformance as previous complex hand-crafted detectors. However, their\nperformance on Video Object Detection (VOD) has not been well explored. In this\npaper, we present TransVOD, an end-to-end video object detection model based on\na spatial-temporal Transformer architecture. The goal of this paper is to\nstreamline the pipeline of VOD, effectively removing the need for many\nhand-crafted components for feature aggregation, e.g., optical flow, recurrent\nneural networks, relation networks. Besides, benefited from the object query\ndesign in DETR, our method does not need complicated post-processing methods\nsuch as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and\nclean. In particular, we present temporal Transformer to aggregate both the\nspatial object queries and the feature memories of each frame. Our temporal\nTransformer consists of three components: Temporal Deformable Transformer\nEncoder (TDTE) to encode the multiple frame spatial details, Temporal Query\nEncoder (TQE) to fuse object queries, and Temporal Deformable Transformer\nDecoder to obtain current frame detection results. These designs boost the\nstrong baseline deformable DETR by a significant margin (3%-4% mAP) on the\nImageNet VID dataset. TransVOD yields comparable results performance on the\nbenchmark of ImageNet VID. We hope our TransVOD can provide a new perspective\nfor video object detection. Code will be made publicly available at\nhttps://github.com/SJTU-LuHe/TransVOD.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:44:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["He", "Lu", ""], ["Zhou", "Qianyu", ""], ["Li", "Xiangtai", ""], ["Niu", "Li", ""], ["Cheng", "Guangliang", ""], ["Li", "Xiao", ""], ["Liu", "Wenxuan", ""], ["Tong", "Yunhai", ""], ["Ma", "Lizhuang", ""], ["Zhang", "Liqing", ""]]}, {"id": "2105.10925", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen, Chaonan Lin, Shaohua Zheng", "title": "COTR: Convolution in Transformer Network for End to End Polyp Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Colorectal cancer (CRC) is the second most common cause of cancer\nmortality worldwide. Colonoscopy is a widely used technique for colon screening\nand polyp lesions diagnosis. Nevertheless, manual screening using colonoscopy\nsuffers from a substantial miss rate of polyps and is an overwhelming burden\nfor endoscopists. Computer-aided diagnosis (CAD) for polyp detection has the\npotential to reduce human error and human burden. However, current polyp\ndetection methods based on object detection framework need many handcrafted\npre-processing and post-processing operations or user guidance that require\ndomain-specific knowledge.\n  Methods: In this paper, we propose a convolution in transformer (COTR)\nnetwork for end-to-end polyp detection. Motivated by the detection transformer\n(DETR), COTR is constituted by a CNN for feature extraction, transformer\nencoder layers interleaved with convolutional layers for feature encoding and\nrecalibration, transformer decoder layers for object querying, and a\nfeed-forward network for detection prediction. Considering the slow convergence\nof DETR, COTR embeds convolution layers into transformer encoder for feature\nreconstruction and convergence acceleration.\n  Results: Experimental results on two public polyp datasets show that COTR\nachieved 91.49\\% precision, 82.69% sensitivity, and 86.87% F1-score on the\nETIS-LARIB, and 91.67% precision, 93.54% sensitivity, and 92.60% F1-score on\nthe CVC-ColonDB.\n  Conclusion: This study proposed an end to end detection method based on\ndetection transformer for colorectal polyp detection. Experimental results on\nETIS-LARIB and CVC-ColonDB dataset demonstrated that the proposed model\nachieved comparable performance against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 12:36:48 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Lin", "Chaonan", ""], ["Zheng", "Shaohua", ""]]}, {"id": "2105.10926", "submitter": "Guolei Sun", "authors": "Guolei Sun, Yun Liu, Thomas Probst, Danda Pani Paudel, Nikola Popovic,\n  Luc Van Gool", "title": "Boosting Crowd Counting with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress on the crowd counting problem has been achieved by\nintegrating larger context into convolutional neural networks (CNNs). This\nindicates that global scene context is essential, despite the seemingly\nbottom-up nature of the problem. This may be explained by the fact that context\nknowledge can adapt and improve local feature extraction to a given scene. In\nthis paper, we therefore investigate the role of global context for crowd\ncounting. Specifically, a pure transformer is used to extract features with\nglobal information from overlapping image patches. Inspired by classification,\nwe add a context token to the input sequence, to facilitate information\nexchange with tokens corresponding to image patches throughout transformer\nlayers. Due to the fact that transformers do not explicitly model the\ntried-and-true channel-wise interactions, we propose a token-attention module\n(TAM) to recalibrate encoded features through channel-wise attention informed\nby the context token. Beyond that, it is adopted to predict the total person\ncount of the image through regression-token module (RTM). Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance on various\ndatasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the\nlarge-scale JHU-CROWD++ dataset, our method improves over the previous best\nresults by 26.9% and 29.9% in terms of MAE and MSE, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 12:44:27 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sun", "Guolei", ""], ["Liu", "Yun", ""], ["Probst", "Thomas", ""], ["Paudel", "Danda Pani", ""], ["Popovic", "Nikola", ""], ["Van Gool", "Luc", ""]]}, {"id": "2105.10937", "submitter": "Marco Visca", "authors": "Marco Visca, Sampo Kuutti, Roger Powell, Yang Gao and Saber Fallah", "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured\n  Environments", "comments": "Accepted for inclusion in Towards Autonomous Robotic Systems\n  Conference (TAROS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework trained in an end-to-end fashion from\nelevation maps and trajectories to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 13:49:05 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 15:03:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Visca", "Marco", ""], ["Kuutti", "Sampo", ""], ["Powell", "Roger", ""], ["Gao", "Yang", ""], ["Fallah", "Saber", ""]]}, {"id": "2105.10949", "submitter": "Xiao Huang", "authors": "Zhiqiang Wang, Zhenfeng Shao, Xiao Huang, Jiaming Wang, Tao Lu, Sihang\n  Zhang", "title": "SSCAN: A Spatial-spectral Cross Attention Network for Hyperspectral\n  Image Denoising", "comments": "5 pages, 5 figures, submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral images (HSIs) have been widely used in a variety of\napplications thanks to the rich spectral information they are able to provide.\nAmong all HSI processing tasks, HSI denoising is a crucial step. Recently, deep\nlearning-based image denoising methods have made great progress and achieved\ngreat performance. However, existing methods tend to ignore the correlations\nbetween adjacent spectral bands, leading to problems such as spectral\ndistortion and blurred edges in denoised results. In this study, we propose a\nnovel HSI denoising network, termed SSCAN, that combines group convolutions and\nattention modules. Specifically, we use a group convolution with a spatial\nattention module to facilitate feature extraction by directing models'\nattention to band-wise important features. We propose a spectral-spatial\nattention block (SSAB) to exploit the spatial and spectral information in\nhyperspectral images in an effective manner. In addition, we adopt residual\nlearning operations with skip connections to ensure training stability. The\nexperimental results indicate that the proposed SSCAN outperforms several\nstate-of-the-art HSI denoising algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 14:36:17 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Zhiqiang", ""], ["Shao", "Zhenfeng", ""], ["Huang", "Xiao", ""], ["Wang", "Jiaming", ""], ["Lu", "Tao", ""], ["Zhang", "Sihang", ""]]}, {"id": "2105.10967", "submitter": "Jaeseok Byun", "authors": "Jaeseok Byun, Sungmin Cha, and Taesup Moon", "title": "FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise", "comments": "CVPR 2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the challenging blind denoising problem for Poisson-Gaussian\nnoise, in which no additional information about clean images or noise level\nparameters is available. Particularly, when only \"single\" noisy images are\navailable for training a denoiser, the denoising performance of existing\nmethods was not satisfactory. Recently, the blind pixelwise affine image\ndenoiser (BP-AIDE) was proposed and significantly improved the performance in\nthe above setting, to the extent that it is competitive with denoisers which\nutilized additional information. However, BP-AIDE seriously suffered from slow\ninference time due to the inefficiency of noise level estimation procedure and\nthat of the blind-spot network (BSN) architecture it used. To that end, we\npropose Fast Blind Image Denoiser (FBI-Denoiser) for Poisson-Gaussian noise,\nwhich consists of two neural network models; 1) PGE-Net that estimates\nPoisson-Gaussian noise parameters 2000 times faster than the conventional\nmethods and 2) FBI-Net that realizes a much more efficient BSN for pixelwise\naffine denoiser in terms of the number of parameters and inference speed.\nConsequently, we show that our FBI-Denoiser blindly trained solely based on\nsingle noisy images can achieve the state-of-the-art performance on several\nreal-world noisy image benchmark datasets with much faster inference time (x\n10), compared to BP-AIDE. The official code of our method is available at\nhttps://github.com/csm9493/FBI-Denoiser.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 16:20:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Byun", "Jaeseok", ""], ["Cha", "Sungmin", ""], ["Moon", "Taesup", ""]]}, {"id": "2105.10968", "submitter": "Thomas Gilles", "authors": "Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan\n  Stanciulescu, Fabien Moutarde", "title": "HOME: Heatmap Output for future Motion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose HOME, a framework tackling the motion forecasting\nproblem with an image output representing the probability distribution of the\nagent's future location. This method allows for a simple architecture with\nclassic convolution networks coupled with attention mechanism for agent\ninteractions, and outputs an unconstrained 2D top-view representation of the\nagent's possible future. Based on this output, we design two methods to sample\na finite set of agent's future locations. These methods allow us to control the\noptimization trade-off between miss rate and final displacement error for\nmultiple modalities without having to retrain any part of the model. We apply\nour method to the Argoverse Motion Forecasting Benchmark and achieve 1st place\non the online leaderboard.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 16:27:04 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 11:26:47 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Gilles", "Thomas", ""], ["Sabatini", "Stefano", ""], ["Tsishkou", "Dzmitry", ""], ["Stanciulescu", "Bogdan", ""], ["Moutarde", "Fabien", ""]]}, {"id": "2105.10983", "submitter": "Bulut Aygunes", "authors": "Bulut Aygunes, Ramazan Gokberk Cinbis, Selim Aksoy", "title": "Weakly Supervised Instance Attention for Multisource Fine-Grained Object\n  Recognition with an Application to Tree Species Classification", "comments": "Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.03.021", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multisource image analysis that leverages complementary spectral, spatial,\nand structural information benefits fine-grained object recognition that aims\nto classify an object into one of many similar subcategories. However, for\nmultisource tasks that involve relatively small objects, even the smallest\nregistration errors can introduce high uncertainty in the classification\nprocess. We approach this problem from a weakly supervised learning perspective\nin which the input images correspond to larger neighborhoods around the\nexpected object locations where an object with a given class label is present\nin the neighborhood without any knowledge of its exact location. The proposed\nmethod uses a single-source deep instance attention model with parallel\nbranches for joint localization and classification of objects, and extends this\nmodel into a multisource setting where a reference source that is assumed to\nhave no location uncertainty is used to aid the fusion of multiple sources in\nfour different levels: probability level, logit level, feature level, and pixel\nlevel. We show that all levels of fusion provide higher accuracies compared to\nthe state-of-the-art, with the best performing method of feature-level fusion\nresulting in 53% accuracy for the recognition of 40 different types of trees,\ncorresponding to an improvement of 5.7% over the best performing baseline when\nRGB, multispectral, and LiDAR data are used. We also provide an in-depth\ncomparison by evaluating each model at various parameter complexity settings,\nwhere the increased model capacity results in a further improvement of 6.3%\nover the default capacity setting.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 17:51:14 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 20:35:38 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Aygunes", "Bulut", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Aksoy", "Selim", ""]]}, {"id": "2105.10990", "submitter": "Julian Posada", "authors": "Milagros Miceli and Julian Posada", "title": "Wisdom for the Crowd: Discoursive Power in Annotation Instructions for\n  Computer Vision", "comments": null, "journal-ref": "CVPR 2021 Workshop: Beyond Fairness: Towards a Just, Equitable,\n  and Accountable Computer Vision", "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developers of computer vision algorithms outsource some of the labor involved\nin annotating training data through business process outsourcing companies and\ncrowdsourcing platforms. Many data annotators are situated in the Global South\nand are considered independent contractors. This paper focuses on the\nexperiences of Argentinian and Venezuelan annotation workers. Through\nqualitative methods, we explore the discourses encoded in the task instructions\nthat these workers follow to annotate computer vision datasets. Our preliminary\nfindings indicate that annotation instructions reflect worldviews imposed on\nworkers and, through their labor, on datasets. Moreover, we observe that\nfor-profit goals drive task instructions and that managers and algorithms make\nsure annotations are done according to requesters' commands. This configuration\npresents a form of commodified labor that perpetuates power asymmetries while\nreinforcing social inequalities and is compelled to reproduce them into\ndatasets and, subsequently, in computer vision systems.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 18:20:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Miceli", "Milagros", ""], ["Posada", "Julian", ""]]}, {"id": "2105.10996", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu, Xiaofei Huang, Nihang Fu, and Sarah Ostadabbas", "title": "Heuristic Weakly Supervised 3D Human Pose Estimation in Novel Contexts\n  without Any 3D Pose Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monocular 3D human pose estimation from a single RGB image has received a lot\nattentions in the past few year. Pose inference models with competitive\nperformance however require supervision with 3D pose ground truth data or at\nleast known pose priors in their target domain. Yet, these data requirements in\nmany real-world applications with data collection constraints may not be\nachievable. In this paper, we present a heuristic weakly supervised solution,\ncalled HW-HuP to estimate 3D human pose in contexts that no ground truth 3D\ndata is accessible, even for fine-tuning. HW-HuP learns partial pose priors\nfrom public 3D human pose datasets and uses easy-to-access observations from\nthe target domain to iteratively estimate 3D human pose and shape in an\noptimization and regression hybrid cycle. In our design, depth data as an\nauxiliary information is employed as weak supervision during training, yet it\nis not needed for the inference. We evaluate HW-HuP performance qualitatively\non datasets of both in-bed human and infant poses, where no ground truth 3D\npose is provided neither any target prior. We also test HW-HuP performance\nquantitatively on a publicly available motion capture dataset against the 3D\nground truth. HW-HuP is also able to be extended to other input modalities for\npose estimation tasks especially under adverse vision conditions, such as\nocclusion or full darkness. On the Human3.6M benchmark, HW-HuP shows 104.1mm in\nMPJPE and 50.4mm in PA MPJPE, comparable to the existing state-of-the-art\napproaches that benefit from full 3D pose supervision.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 18:40:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Shuangjun", ""], ["Huang", "Xiaofei", ""], ["Fu", "Nihang", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "2105.11010", "submitter": "Gil Shomron", "authors": "Gil Shomron, Freddy Gabbay, Samer Kurzum, Uri Weiser", "title": "Post-Training Sparsity-Aware Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,\n2x speedup over widely used hardware architectures, and a practical hardware\nimplementation. The code is available at https://github.com/gilshm/sparq.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:12:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shomron", "Gil", ""], ["Gabbay", "Freddy", ""], ["Kurzum", "Samer", ""], ["Weiser", "Uri", ""]]}, {"id": "2105.11016", "submitter": "Yecheng Lyu", "authors": "Yecheng Lyu, Xinming Huang, Ziming Zhang", "title": "Revisiting 2D Convolutional Neural Networks for Graph-based Applications", "comments": "Accepted by T-PAMI. arXiv admin note: substantial text overlap with\n  arXiv:1909.12383", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3083614", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph convolutional networks (GCNs) are widely used in graph-based\napplications such as graph classification and segmentation. However, current\nGCNs have limitations on implementation such as network architectures due to\ntheir irregular inputs. In contrast, convolutional neural networks (CNNs) are\ncapable of extracting rich features from large-scale input data, but they do\nnot support general graph inputs. To bridge the gap between GCNs and CNNs, in\nthis paper we study the problem of how to effectively and efficiently map\ngeneral graphs to 2D grids that CNNs can be directly applied to, while\npreserving graph topology as much as possible. We therefore propose two novel\ngraph-to-grid mapping schemes, namely, {\\em graph-preserving grid layout\n(GPGL)} and its extension {\\em Hierarchical GPGL (H-GPGL)} for computational\nefficiency. We formulate the GPGL problem as integer programming and further\npropose an approximate yet efficient solver based on a penalized Kamada-Kawai\nmethod, a well-known optimization algorithm in 2D graph drawing. We propose a\nnovel vertex separation penalty that encourages graph vertices to lay on the\ngrid without any overlap. Along with this image representation, even extra 2D\nmaxpooling layers contribute to the PointNet, a widely applied point-based\nneural network. We demonstrate the empirical success of GPGL on general graph\nclassification with small graphs and H-GPGL on 3D point cloud segmentation with\nlarge graphs, based on 2D CNNs including VGG16, ResNet50 and multi-scale maxout\n(MSM) CNN.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:34:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lyu", "Yecheng", ""], ["Huang", "Xinming", ""], ["Zhang", "Ziming", ""]]}, {"id": "2105.11021", "submitter": "Pasccal Fischer", "authors": "Pascal Fischer, Alen Smajic, Alexander Mehler, Giuseppe Abrami", "title": "Multi-Type-TD-TSR -- Extracting Tables from Document Images using a\n  Multi-stage Pipeline for Table Detection and Table Structure Recognition:\n  from OCR to Structured Table Representations", "comments": "8 pages, 8 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As global trends are shifting towards data-driven industries, the demand for\nautomated algorithms that can convert digital images of scanned documents into\nmachine readable information is rapidly growing. Besides the opportunity of\ndata digitization for the application of data analytic tools, there is also a\nmassive improvement towards automation of processes, which previously would\nrequire manual inspection of the documents. Although the introduction of\noptical character recognition technologies mostly solved the task of converting\nhuman-readable characters from images into machine-readable characters, the\ntask of extracting table semantics has been less focused on over the years. The\nrecognition of tables consists of two main tasks, namely table detection and\ntable structure recognition. Most prior work on this problem focuses on either\ntask without offering an end-to-end solution or paying attention to real\napplication conditions like rotated images or noise artefacts inside the\ndocument image. Recent work shows a clear trend towards deep learning\napproaches coupled with the use of transfer learning for the task of table\nstructure recognition due to the lack of sufficiently large datasets. In this\npaper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an\nend-to-end solution for the problem of table recognition. It utilizes\nstate-of-the-art deep learning models for table detection and differentiates\nbetween 3 different types of tables based on the tables' borders. For the table\nstructure recognition we use a deterministic non-data driven algorithm, which\nworks on all table types. We additionally present two algorithms. One for\nunbordered tables and one for bordered tables, which are the base of the used\ntable structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the\nICDAR 2019 table structure recognition dataset and achieve a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:17:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Fischer", "Pascal", ""], ["Smajic", "Alen", ""], ["Mehler", "Alexander", ""], ["Abrami", "Giuseppe", ""]]}, {"id": "2105.11058", "submitter": "Muhammad Zaigham Zaheer", "authors": "Jin-Ha Lee, Marcella Astrid, Muhammad Zaigham Zaheer, Seung-Ik Lee", "title": "Deep Visual Anomaly detection with Negative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increase in the learning capability of deep convolution-based\narchitectures, various applications of such models have been proposed over\ntime. In the field of anomaly detection, improvements in deep learning opened\nnew prospects of exploration for the researchers whom tried to automate the\nlabor-intensive features of data collection. First, in terms of data\ncollection, it is impossible to anticipate all the anomalies that might exist\nin a given environment. Second, assuming we limit the possibilities of\nanomalies, it will still be hard to record all these scenarios for the sake of\ntraining a model. Third, even if we manage to record a significant amount of\nabnormal data, it's laborious to annotate this data on pixel or even frame\nlevel. Various approaches address the problem by proposing one-class\nclassification using generative models trained on only normal data. In such\nmethods, only the normal data is used, which is abundantly available and\ndoesn't require significant human input. However, these are trained with only\nnormal data and at the test time, given abnormal data as input, may often\ngenerate normal-looking output. This happens due to the hallucination\ncharacteristic of generative models. Next, these systems are designed to not\nuse abnormal examples during the training. In this paper, we propose anomaly\ndetection with negative learning (ADNL), which employs the negative learning\nconcept for the enhancement of anomaly detection by utilizing a very small\nnumber of labeled anomaly data as compared with the normal data during\ntraining. The idea is to limit the reconstruction capability of a generative\nmodel using the given a small amount of anomaly examples. This way, the network\nnot only learns to reconstruct normal data but also encloses the normal\ndistribution far from the possible distribution of anomalies.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 01:48:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lee", "Jin-Ha", ""], ["Astrid", "Marcella", ""], ["Zaheer", "Muhammad Zaigham", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "2105.11060", "submitter": "Miguel Angel Saavedra Ruiz", "authors": "Gustavo A. Salazar-Gomez, Miguel A. Saavedra-Ruiz, Victor A.\n  Romero-Cano", "title": "High-level camera-LiDAR fusion for 3D object detection with machine\n  learning", "comments": "LatinX Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the 3D object detection problem, which is of vital\nimportance for applications such as autonomous driving. Our framework uses a\nMachine Learning (ML) pipeline on a combination of monocular camera and LiDAR\ndata to detect vehicles in the surrounding 3D space of a moving platform. It\nuses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object\ndetectors to segment LiDAR point clouds into point clusters which represent\npotentially individual objects. We evaluate the performance of classical ML\nalgorithms as part of an holistic pipeline for estimating the parameters of 3D\nbounding boxes which surround the vehicles around the moving platform. Our\nresults demonstrate an efficient and accurate inference on a validation set,\nachieving an overall accuracy of 87.1%.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 01:57:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Salazar-Gomez", "Gustavo A.", ""], ["Saavedra-Ruiz", "Miguel A.", ""], ["Romero-Cano", "Victor A.", ""]]}, {"id": "2105.11062", "submitter": "Ting Pan", "authors": "Ting Pan and Zhuqing Jiang and Jianan Han and Shiping Wen and Aidong\n  Men and Haiying Wang", "title": "Taylor saves for later: disentanglement for video prediction using\n  Taylor representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction is a challenging task with wide application prospects in\nmeteorology and robot systems. Existing works fail to trade off short-term and\nlong-term prediction performances and extract robust latent dynamics laws in\nvideo frames. We propose a two-branch seq-to-seq deep model to disentangle the\nTaylor feature and the residual feature in video frames by a novel recurrent\nprediction module (TaylorCell) and residual module. TaylorCell can expand the\nvideo frames' high-dimensional features into the finite Taylor series to\ndescribe the latent laws. In TaylorCell, we propose the Taylor prediction unit\n(TPU) and the memory correction unit (MCU). TPU employs the first input frame's\nderivative information to predict the future frames, avoiding error\naccumulation. MCU distills all past frames' information to correct the\npredicted Taylor feature from TPU. Correspondingly, the residual module\nextracts the residual feature complementary to the Taylor feature. On three\ngeneralist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or\nreaches state-of-the-art models, and ablation experiments demonstrate the\neffectiveness of our model in long-term prediction.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 01:59:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Pan", "Ting", ""], ["Jiang", "Zhuqing", ""], ["Han", "Jianan", ""], ["Wen", "Shiping", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""]]}, {"id": "2105.11087", "submitter": "Xi Li", "authors": "Jabeen Summaira, Xi Li, Amin Muhammad Shoib, Songyuan Li and Jabbar\n  Abdul", "title": "Recent Advances and Trends in Multimodal Deep Learning: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has implemented a wide range of applications and has become\nincreasingly popular in recent years. The goal of multimodal deep learning is\nto create models that can process and link information using various\nmodalities. Despite the extensive development made for unimodal learning, it\nstill cannot cover all the aspects of human learning. Multimodal learning helps\nto understand and analyze better when various senses are engaged in the\nprocessing of information. This paper focuses on multiple types of modalities,\ni.e., image, video, text, audio, body gestures, facial expressions, and\nphysiological signals. Detailed analysis of past and current baseline\napproaches and an in-depth study of recent advancements in multimodal deep\nlearning applications has been provided. A fine-grained taxonomy of various\nmultimodal deep learning applications is proposed, elaborating on different\napplications in more depth. Architectures and datasets used in these\napplications are also discussed, along with their evaluation metrics. Last,\nmain issues are highlighted separately for each domain along with their\npossible future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 04:20:45 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Summaira", "Jabeen", ""], ["Li", "Xi", ""], ["Shoib", "Amin Muhammad", ""], ["Li", "Songyuan", ""], ["Abdul", "Jabbar", ""]]}, {"id": "2105.11088", "submitter": "Wensheng Zhang", "authors": "Wensheng Zhang, Yan Zheng, Taiga Miyazono, Seiichi Uchida, Brian Kenji\n  Iwana", "title": "Towards Book Cover Design via Layout Graphs", "comments": "Accepted at ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Book covers are intentionally designed and provide an introduction to a book.\nHowever, they typically require professional skills to design and produce the\ncover images. Thus, we propose a generative neural network that can produce\nbook covers based on an easy-to-use layout graph. The layout graph contains\nobjects such as text, natural scene objects, and solid color spaces. This\nlayout graph is embedded using a graph convolutional neural network and then\nused with a mask proposal generator and a bounding-box generator and filled\nusing an object proposal generator. Next, the objects are compiled into a\nsingle image and the entire network is trained using a combination of\nadversarial training, perceptual training, and reconstruction. Finally, a Style\nRetention Network (SRNet) is used to transfer the learned font style onto the\ndesired text. Using the proposed method allows for easily controlled and unique\nbook covers.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 04:28:35 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 09:14:08 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Wensheng", ""], ["Zheng", "Yan", ""], ["Miyazono", "Taiga", ""], ["Uchida", "Seiichi", ""], ["Iwana", "Brian Kenji", ""]]}, {"id": "2105.11107", "submitter": "Yi Liu", "authors": "Yi Liu, Limin Wang, Xiao Ma, Yali Wang, Yu Qiao", "title": "FineAction: A Fined Video Dataset for Temporal Action Localization", "comments": "One track of DeeperAction Workshop@ICCV2021. HomePage:\n  https://deeperaction.github.io/fineaction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the existing benchmark datasets, THUMOS14 and ActivityNet, temporal action\nlocalization techniques have achieved great success. However, there are still\nexisting some problems, such as the source of the action is too single, there\nare only sports categories in THUMOS14, coarse instances with uncertain\nboundaries in ActivityNet and HACS Segments interfering with proposal\ngeneration and behavior prediction. To take temporal action localization to a\nnew level, we develop FineAction, a new large-scale fined video dataset\ncollected from existing video datasets and web videos. Overall, this dataset\ncontains 139K fined action instances densely annotated in almost 17K untrimmed\nvideos spanning 106 action categories. FineAction has a more fined definition\nof action categories and high-quality annotations to reduce the boundary\nuncertainty compared to the existing action localization datasets. We\nsystematically investigate representative methods of temporal action\nlocalization on our dataset and obtain some interesting findings with further\nanalysis. Experimental results reveal that our FineAction brings new challenges\nfor action localization on fined and multi-label instances with shorter\nduration. This dataset will be public in the future and we hope our FineAction\ncould advance research towards temporal action localization. Our dataset\nwebsite is at https://deeperaction.github.io/fineaction/.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:06:32 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Yi", ""], ["Wang", "Limin", ""], ["Ma", "Xiao", ""], ["Wang", "Yali", ""], ["Qiao", "Yu", ""]]}, {"id": "2105.11111", "submitter": "Wentong Li", "authors": "Wentong Li, Jianke Zhu", "title": "Oriented RepPoints for Aerial Object Detection", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In contrast to the oriented bounding boxes, point set representation has\ngreat potential to capture the detailed structure of instances with the\narbitrary orientations, large aspect ratios and dense distribution in aerial\nimages. However, the conventional point set-based approaches are handcrafted\nwith the fixed locations using points-to-points supervision, which hurts their\nflexibility on the fine-grained feature extraction. To address these\nlimitations, in this paper, we propose a novel approach to aerial object\ndetection, named Oriented RepPoints. Specifically, we suggest to employ a set\nof adaptive points to capture the geometric and spatial information of the\narbitrary-oriented objects, which is able to automatically arrange themselves\nover the object in a spatial and semantic scenario. To facilitate the\nsupervised learning, the oriented conversion function is proposed to explicitly\nmap the adaptive point set into an oriented bounding box. Moreover, we\nintroduce an effective quality assessment measure to select the point set\nsamples for training, which can choose the representative items with respect to\ntheir potentials on orientated object detection. Furthermore, we suggest a\nspatial constraint to penalize the outlier points outside the ground-truth\nbounding box. In addition to the traditional evaluation metric mAP focusing on\noverlap ratio, we propose a new metric mAOE to measure the orientation accuracy\nthat is usually neglected in the previous studies on oriented object detection.\nExperiments on three widely used datasets including DOTA, HRSC2016 and UCAS-AOD\ndemonstrate that our proposed approach is effective.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:18:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Wentong", ""], ["Zhu", "Jianke", ""]]}, {"id": "2105.11113", "submitter": "Bi Li", "authors": "Bi Li, Teng Xi, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu,\n  Errui Ding, Wenyu Liu", "title": "Dynamic Class Queue for Large Scale Face Recognition In the Wild", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Learning discriminative representation using large-scale face datasets in the\nwild is crucial for real-world applications, yet it remains challenging. The\ndifficulties lie in many aspects and this work focus on computing resource\nconstraint and long-tailed class distribution. Recently, classification-based\nrepresentation learning with deep neural networks and well-designed losses have\ndemonstrated good recognition performance. However, the computing and memory\ncost linearly scales up to the number of identities (classes) in the training\nset, and the learning process suffers from unbalanced classes. In this work, we\npropose a dynamic class queue (DCQ) to tackle these two problems. Specifically,\nfor each iteration during training, a subset of classes for recognition are\ndynamically selected and their class weights are dynamically generated\non-the-fly which are stored in a queue. Since only a subset of classes is\nselected for each iteration, the computing requirement is reduced. By using a\nsingle server without model parallel, we empirically verify in large-scale\ndatasets that 10% of classes are sufficient to achieve similar performance as\nusing all classes. Moreover, the class weights are dynamically generated in a\nfew-shot manner and therefore suitable for tail classes with only a few\ninstances. We show clear improvement over a strong baseline in the largest\npublic dataset Megaface Challenge2 (MF2) which has 672K identities and over 88%\nof them have less than 10 instances. Code is available at\nhttps://github.com/bilylee/DCQ\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:31:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Bi", ""], ["Xi", "Teng", ""], ["Zhang", "Gang", ""], ["Feng", "Haocheng", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""], ["Liu", "Wenyu", ""]]}, {"id": "2105.11120", "submitter": "Qinwei Xu", "authors": "Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "A Fourier-based Framework for Domain Generalization", "comments": "Accepted as CVPR 2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks suffer from performance degradation when\nevaluated on testing data under different distributions from training data.\nDomain generalization aims at tackling this problem by learning transferable\nknowledge from multiple source domains in order to generalize to unseen target\ndomains. This paper introduces a novel Fourier-based perspective for domain\ngeneralization. The main assumption is that the Fourier phase information\ncontains high-level semantics and is not easily affected by domain shifts. To\nforce the model to capture phase information, we develop a novel Fourier-based\ndata augmentation strategy called amplitude mix which linearly interpolates\nbetween the amplitude spectrums of two images. A dual-formed consistency loss\ncalled co-teacher regularization is further introduced between the predictions\ninduced from original and augmented images. Extensive experiments on three\nbenchmarks have demonstrated that the proposed method is able to achieve\nstate-of-the-arts performance for domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:50:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xu", "Qinwei", ""], ["Zhang", "Ruipeng", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "2105.11131", "submitter": "Guoqiang Liang", "authors": "Guoqiang Liang, Yanbing Lv, Shucheng Li, Shizhou Zhang, Yanning Zhang", "title": "Unsupervised Video Summarization with a Convolutional Attentive\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive growth of video data, video summarization, which attempts\nto seek the minimum subset of frames while still conveying the main story, has\nbecome one of the hottest topics. Nowadays, substantial achievements have been\nmade by supervised learning techniques, especially after the emergence of deep\nlearning. However, it is extremely expensive and difficult to collect human\nannotation for large-scale video datasets. To address this problem, we propose\na convolutional attentive adversarial network (CAAN), whose key idea is to\nbuild a deep summarizer in an unsupervised way. Upon the generative adversarial\nnetwork, our overall framework consists of a generator and a discriminator. The\nformer predicts importance scores for all frames of a video while the latter\ntries to distinguish the score-weighted frame features from original frame\nfeatures. Specifically, the generator employs a fully convolutional sequence\nnetwork to extract global representation of a video, and an attention-based\nnetwork to output normalized importance scores. To learn the parameters, our\nobjective function is composed of three loss functions, which can guide the\nframe-level importance score prediction collaboratively. To validate this\nproposed method, we have conducted extensive experiments on two public\nbenchmarks SumMe and TVSum. The results show the superiority of our proposed\nmethod against other state-of-the-art unsupervised approaches. Our method even\noutperforms some published supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:24:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liang", "Guoqiang", ""], ["Lv", "Yanbing", ""], ["Li", "Shucheng", ""], ["Zhang", "Shizhou", ""], ["Zhang", "Yanning", ""]]}, {"id": "2105.11137", "submitter": "Tianxiang Ma", "authors": "Tianxiang Ma, Dongze Li, Wei Wang, Jing Dong", "title": "Face Anonymization by Manipulating Decoupled Identity Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy protection on human biological information has drawn increasing\nattention in recent years, among which face anonymization plays an importance\nrole. We propose a novel approach which protects identity information of facial\nimages from leakage with slightest modification. Specifically, we disentangle\nidentity representation from other facial attributes leveraging the power of\ngenerative adversarial networks trained on a conditional multi-scale\nreconstruction (CMR) loss and an identity loss. We evaulate the disentangle\nability of our model, and propose an effective method for identity\nanonymization, namely Anonymous Identity Generation (AIG), to reach the goal of\nface anonymization meanwhile maintaining similarity to the original image as\nmuch as possible. Quantitative and qualitative results demonstrate our method's\nsuperiority compared with the SOTAs on both visual quality and anonymization\nsuccess rate.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:39:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ma", "Tianxiang", ""], ["Li", "Dongze", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""]]}, {"id": "2105.11160", "submitter": "Celia Cintas", "authors": "Hannah Kim, Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Kush\n  Varshney", "title": "Out-of-Distribution Detection in Dermatology using Input Perturbation\n  and Subset Scanning", "comments": "Under review for 6th Outlier Detection & Description Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning have led to breakthroughs in the development\nof automated skin disease classification. As we observe an increasing interest\nin these models in the dermatology space, it is crucial to address aspects such\nas the robustness towards input data distribution shifts. Current skin disease\nmodels could make incorrect inferences for test samples from different hardware\ndevices and clinical settings or unknown disease samples, which are\nout-of-distribution (OOD) from the training samples. To this end, we propose a\nsimple yet effective approach that detect these OOD samples prior to making any\ndecision. The detection is performed via scanning in the latent space\nrepresentation (e.g., activations of the inner layers of any pre-trained skin\ndisease classifier). The input samples could also perturbed to maximise\ndivergence of OOD samples. We validate our ODD detection approach in two use\ncases: 1) identify samples collected from different protocols, and 2) detect\nsamples from unknown disease classes. Additionally, we evaluate the performance\nof the proposed approach and compare it with other state-of-the-art methods.\nFurthermore, data-driven dermatology applications may deepen the disparity in\nclinical care across racial and ethnic groups since most datasets are reported\nto suffer from bias in skin tone distribution. Therefore, we also evaluate the\nfairness of these OOD detection methods across different skin tones. Our\nexperiments resulted in competitive performance across multiple datasets in\ndetecting OOD samples, which could be used (in the future) to design more\neffective transfer learning techniques prior to inferring on these samples.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:04:47 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 11:53:15 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 07:40:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kim", "Hannah", ""], ["Tadesse", "Girmaw Abebe", ""], ["Cintas", "Celia", ""], ["Speakman", "Skyler", ""], ["Varshney", "Kush", ""]]}, {"id": "2105.11166", "submitter": "Mikolaj Jankowski", "authors": "Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk", "title": "AirNet: Neural Network Transmission over the Air", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art performance for many emerging edge applications is achieved\nby deep neural networks (DNNs). Often, these DNNs are location and time\nsensitive, and the parameters of a specific DNN must be delivered from an edge\nserver to the edge device rapidly and efficiently to carry out time-sensitive\ninference tasks. We introduce AirNet, a novel training and analog transmission\nmethod that allows efficient wireless delivery of DNNs. We first train the DNN\nwith noise injection to counter the wireless channel noise. We also employ\npruning to reduce the channel bandwidth necessary for transmission, and perform\nknowledge distillation from a larger model to achieve satisfactory performance,\ndespite the channel perturbations. We show that AirNet achieves significantly\nhigher test accuracy compared to digital alternatives under the same bandwidth\nand power constraints. It also exhibits graceful degradation with channel\nquality, which reduces the requirement for accurate channel estimation.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:16:04 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:44:49 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jankowski", "Mikolaj", ""], ["Gunduz", "Deniz", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2105.11168", "submitter": "Zitian Wang", "authors": "Si Liu, Zitian Wang, Yulu Gao, Lejian Ren, Yue Liao, Guanghui Ren, Bo\n  Li, Shuicheng Yan", "title": "Human-centric Relation Segmentation: Dataset and Solution", "comments": "Accepted by TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and language understanding techniques have achieved remarkable\nprogress, but currently it is still difficult to well handle problems involving\nvery fine-grained details. For example, when the robot is told to \"bring me the\nbook in the girl's left hand\", most existing methods would fail if the girl\nholds one book respectively in her left and right hand. In this work, we\nintroduce a new task named human-centric relation segmentation (HRS), as a\nfine-grained case of HOI-det. HRS aims to predict the relations between the\nhuman and surrounding entities and identify the relation-correlated human\nparts, which are represented as pixel-level masks. For the above exemplar case,\nour HRS task produces results in the form of relation triplets <girl [left\nhand], hold, book> and exacts segmentation masks of the book, with which the\nrobot can easily accomplish the grabbing task. Correspondingly, we collect a\nnew Person In Context (PIC) dataset for this new task, which contains 17,122\nhigh-resolution images and densely annotated entity segmentation and relations,\nincluding 141 object categories, 23 relation categories and 25 semantic human\nparts. We also propose a Simultaneous Matching and Segmentation (SMS) framework\nas a solution to the HRS task. I Outputs of the three branches are fused to\nproduce the final HRS results. Extensive experiments on PIC and V-COCO datasets\nshow that the proposed SMS method outperforms baselines with the 36 FPS\ninference speed.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:20:37 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 12:53:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Liu", "Si", ""], ["Wang", "Zitian", ""], ["Gao", "Yulu", ""], ["Ren", "Lejian", ""], ["Liao", "Yue", ""], ["Ren", "Guanghui", ""], ["Li", "Bo", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2105.11179", "submitter": "Anastasiia Kornilova", "authors": "A. Kornilova, I. Kirilenko, D. Iarosh, V. Kutuev, M. Strutovsky", "title": "Smart mobile microscopy: towards fully-automated digitization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mobile microscopy is a newly formed field that emerged from a combination of\noptical microscopy capabilities and spread, functionality, and ever-increasing\ncomputing resources of mobile devices. Despite the idea of creating a system\nthat would successfully merge a microscope, numerous computer vision methods,\nand a mobile device is regularly examined, the resulting implementations still\nrequire the presence of a qualified operator to control specimen digitization.\nIn this paper, we address the task of surpassing this constraint and present a\n``smart'' mobile microscope concept aimed at automatic digitization of the most\nvaluable visual information about the specimen. We perform this through\ncombining automated microscope setup control and classic techniques such as\nauto-focusing, in-focus filtering, and focus-stacking -- adapted and optimized\nas parts of a mobile cross-platform library.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:55:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kornilova", "A.", ""], ["Kirilenko", "I.", ""], ["Iarosh", "D.", ""], ["Kutuev", "V.", ""], ["Strutovsky", "M.", ""]]}, {"id": "2105.11187", "submitter": "Chairi Kiourt", "authors": "Chairi Kiourt, Georgios Feretzakis, Konstantinos Dalamarinis, Dimitris\n  Kalles, Georgios Pantos, Ioannis Papadopoulos, Spyros Kouris, George\n  Ioannakis, Evangelos Loupelis, Petros Antonopoulos, Aikaterini Sakagianni", "title": "Pulmonary embolism identification in computerized tomography pulmonary\n  angiography scans with deep learning technologies in COVID-19 patients", "comments": "16 pages, 6 figures, 1 table, Submitted to the European Radiology\n  journal of Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main objective of this work is to utilize state-of-the-art deep learning\napproaches for the identification of pulmonary embolism in CTPA-Scans for\nCOVID-19 patients, provide an initial assessment of their performance and,\nultimately, provide a fast-track prototype solution (system). We adopted and\nassessed some of the most popular convolutional neural network architectures\nthrough transfer learning approaches, to strive to combine good model accuracy\nwith fast training. Additionally, we exploited one of the most popular\none-stage object detection models for the localization (through object\ndetection) of the pulmonary embolism regions-of-interests. The models of both\napproaches are trained on an original CTPA-Scan dataset, where we annotated of\n673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary\nembolism regions-of-interests. We provide a brief assessment of some\nstate-of-the-art image classification models by achieving validation accuracies\nof 91% in pulmonary embolism classification. Additionally, we achieved a\nprecision of about 68% on average in the object detection model for the\npulmonary embolism localization under 50% IoU threshold. For both approaches,\nwe provide the entire training pipelines for future studies (step by step\nprocesses through source code). In this study, we present some of the most\naccurate and fast deep learning models for pulmonary embolism identification in\nCTPA-Scans images, through classification and localization (object detection)\napproaches for patients infected by COVID-19. We provide a fast-track solution\n(system) for the research community of the area, which combines both\nclassification and object detection models for improving the precision of\nidentifying pulmonary embolisms.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 10:23:21 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 06:50:04 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 06:05:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kiourt", "Chairi", ""], ["Feretzakis", "Georgios", ""], ["Dalamarinis", "Konstantinos", ""], ["Kalles", "Dimitris", ""], ["Pantos", "Georgios", ""], ["Papadopoulos", "Ioannis", ""], ["Kouris", "Spyros", ""], ["Ioannakis", "George", ""], ["Loupelis", "Evangelos", ""], ["Antonopoulos", "Petros", ""], ["Sakagianni", "Aikaterini", ""]]}, {"id": "2105.11207", "submitter": "Andres C Rodriguez", "authors": "Andr\\'es C. Rodr\\'iguez, Stefano D'Aronco, Konrad Schindler, Jan\n  D.Wegner", "title": "Mapping oil palm density at country scale: An active learning approach", "comments": null, "journal-ref": "Remote Sensing of Environment Volume 261, August 2021, 112479", "doi": "10.1016/j.rse.2021.112479", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate mapping of oil palm is important for understanding its past and\nfuture impact on the environment. We propose to map and count oil palms by\nestimating tree densities per pixel for large-scale analysis. This allows for\nfine-grained analysis, for example regarding different planting patterns. To\nthat end, we propose a new, active deep learning method to estimate oil palm\ndensity at large scale from Sentinel-2 satellite images, and apply it to\ngenerate complete maps for Malaysia and Indonesia. What makes the regression of\noil palm density challenging is the need for representative reference data that\ncovers all relevant geographical conditions across a large territory.\nSpecifically for density estimation, generating reference data involves\ncounting individual trees. To keep the associated labelling effort low we\npropose an active learning (AL) approach that automatically chooses the most\nrelevant samples to be labelled. Our method relies on estimates of the\nepistemic model uncertainty and of the diversity among samples, making it\npossible to retrieve an entire batch of relevant samples in a single iteration.\nMoreover, our algorithm has linear computational complexity and is easily\nparallelisable to cover large areas. We use our method to compute the first oil\npalm density map with $10\\,$m Ground Sampling Distance (GSD) , for all of\nIndonesia and Malaysia and for two different years, 2017 and 2019. The maps\nhave a mean absolute error of $\\pm$7.3 trees/$ha$, estimated from an\nindependent validation set. We also analyse density variations between\ndifferent states within a country and compare them to official estimates.\nAccording to our estimates there are, in total, $>1.2$ billion oil palms in\nIndonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia\ncovering $>6$ million $ha$.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:23:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Rodr\u00edguez", "Andr\u00e9s C.", ""], ["D'Aronco", "Stefano", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan D.", ""]]}, {"id": "2105.11228", "submitter": "Yuchao Li", "authors": "Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei\n  Chao, Fan Yang, Jincheng Ma, Qi Tian, Rongrong Ji", "title": "Towards Compact CNNs via Collaborative Compression", "comments": "This paper is published in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Channel pruning and tensor decomposition have received extensive attention in\nconvolutional neural network compression. However, these two techniques are\ntraditionally deployed in an isolated manner, leading to significant accuracy\ndrop when pursuing high compression rates. In this paper, we propose a\nCollaborative Compression (CC) scheme, which joints channel pruning and tensor\ndecomposition to compress CNN models by simultaneously learning the model\nsparsity and low-rankness. Specifically, we first investigate the compression\nsensitivity of each layer in the network, and then propose a Global Compression\nRate Optimization that transforms the decision problem of compression rate into\nan optimization problem. After that, we propose multi-step heuristic\ncompression to remove redundant compression units step-by-step, which fully\nconsiders the effect of the remaining compression space (i.e., unremoved\ncompression units). Our method demonstrates superior performance gains over\nprevious ones on various datasets and backbone architectures. For example, we\nachieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with\nonly a Top-1 accuracy drop of 0.56% on ImageNet 2012.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:07:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Yuchao", ""], ["Lin", "Shaohui", ""], ["Liu", "Jianzhuang", ""], ["Ye", "Qixiang", ""], ["Wang", "Mengdi", ""], ["Chao", "Fei", ""], ["Yang", "Fan", ""], ["Ma", "Jincheng", ""], ["Tian", "Qi", ""], ["Ji", "Rongrong", ""]]}, {"id": "2105.11237", "submitter": "Jinlong Peng", "authors": "Jinlong Peng, Zhengkai Jiang, Yueyang Gu, Yang Wu, Yabiao Wang, Ying\n  Tai, Chengjie Wang, Weiyao Lin", "title": "SiamRCR: Reciprocal Classification and Regression for Visual Object\n  Tracking", "comments": "The 30th International Joint Conference on Artificial Intelligence\n  (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, most siamese network based trackers locate targets via object\nclassification and bounding-box regression. Generally, they select the\nbounding-box with maximum classification confidence as the final prediction.\nThis strategy may miss the right result due to the accuracy misalignment\nbetween classification and regression. In this paper, we propose a novel\nsiamese tracking algorithm called SiamRCR, addressing this problem with a\nsimple, light and effective solution. It builds reciprocal links between\nclassification and regression branches, which can dynamically re-weight their\nlosses for each positive sample. In addition, we add a localization branch to\npredict the localization accuracy, so that it can work as the replacement of\nthe regression assistance link during inference. This branch makes the training\nand inference more consistent. Extensive experimental results demonstrate the\neffectiveness of SiamRCR and its superiority over the state-of-the-art\ncompetitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.\nMoreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:21:25 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 02:47:07 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 06:32:27 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 11:31:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Peng", "Jinlong", ""], ["Jiang", "Zhengkai", ""], ["Gu", "Yueyang", ""], ["Wu", "Yang", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Lin", "Weiyao", ""]]}, {"id": "2105.11239", "submitter": "Fernando P\\'erez-Garc\\'ia", "authors": "Fernando P\\'erez-Garc\\'ia, Reuben Dorent, Michele Rizzi, Francesco\n  Cardinale, Valerio Frazzini, Vincent Navarro, Caroline Essert, Ir\\`ene\n  Ollivier, Tom Vercauteren, Rachel Sparks, John S. Duncan and S\\'ebastien\n  Ourselin", "title": "A self-supervised learning strategy for postoperative brain cavity\n  segmentation simulating resections", "comments": "To be published in the International Journal of Computer Assisted\n  Radiology and Surgery (IJCARS) - Special issue MICCAI 2020", "journal-ref": null, "doi": "10.1007/s11548-021-02420-2", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate segmentation of brain resection cavities (RCs) aids in postoperative\nanalysis and determining follow-up treatment. Convolutional neural networks\n(CNNs) are the state-of-the-art image segmentation technique, but require large\nannotated datasets for training. Annotation of 3D medical images is\ntime-consuming, requires highly-trained raters, and may suffer from high\ninter-rater variability. Self-supervised learning strategies can leverage\nunlabeled data for training.\n  We developed an algorithm to simulate resections from preoperative magnetic\nresonance images (MRIs). We performed self-supervised training of a 3D CNN for\nRC segmentation using our simulation method. We curated EPISURG, a dataset\ncomprising 430 postoperative and 268 preoperative MRIs from 430 refractory\nepilepsy patients who underwent resective neurosurgery. We fine-tuned our model\non three small annotated datasets from different institutions and on the\nannotated images in EPISURG, comprising 20, 33, 19 and 133 subjects.\n  The model trained on data with simulated resections obtained median\n(interquartile range) Dice score coefficients (DSCs) of 81.7 (16.4), 82.4\n(36.4), 74.9 (24.2) and 80.5 (18.7) for each of the four datasets. After\nfine-tuning, DSCs were 89.2 (13.3), 84.1 (19.8), 80.2 (20.1) and 85.2 (10.8).\nFor comparison, inter-rater agreement between human annotators from our\nprevious study was 84.0 (9.9).\n  We present a self-supervised learning strategy for 3D CNNs using simulated\nRCs to accurately segment real RCs on postoperative MRI. Our method generalizes\nwell to data from different institutions, pathologies and modalities. Source\ncode, segmentation models and the EPISURG dataset are available at\nhttps://github.com/fepegar/ressegijcars .\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:27:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["P\u00e9rez-Garc\u00eda", "Fernando", ""], ["Dorent", "Reuben", ""], ["Rizzi", "Michele", ""], ["Cardinale", "Francesco", ""], ["Frazzini", "Valerio", ""], ["Navarro", "Vincent", ""], ["Essert", "Caroline", ""], ["Ollivier", "Ir\u00e8ne", ""], ["Vercauteren", "Tom", ""], ["Sparks", "Rachel", ""], ["Duncan", "John S.", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2105.11241", "submitter": "Saurabh Mittal", "authors": "Prerak Mann, Sahaj Jain, Saurabh Mittal, Aruna Bhat", "title": "Generation of COVID-19 Chest CT Scan Images using Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious\ndisease that is infected by a novel coronavirus, and has been rapidly spreading\nacross the globe. It is very important to test and isolate people to reduce\nspread, and from here comes the need to do this quickly and efficiently.\nAccording to some studies, Chest-CT outperforms RT-PCR lab testing, which is\nthe current standard, when diagnosing COVID-19 patients. Due to this, computer\nvision researchers have developed various deep learning systems that can\npredict COVID-19 using a Chest-CT scan correctly to a certain degree. The\naccuracy of these systems is limited since deep learning neural networks such\nas CNNs (Convolutional Neural Networks) need a significantly large quantity of\ndata for training in order to produce good quality results. Since the disease\nis relatively recent and more focus has been on CXR (Chest XRay) images, the\navailable chest CT Scan image dataset is much less. We propose a method, by\nutilizing GANs, to generate synthetic chest CT images of both positive and\nnegative COVID-19 patients. Using a pre-built predictive model, we concluded\nthat around 40% of the generated images are correctly predicted as COVID-19\npositive. The dataset thus generated can be used to train a CNN-based\nclassifier which can help determine COVID-19 in a patient with greater\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:04:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mann", "Prerak", ""], ["Jain", "Sahaj", ""], ["Mittal", "Saurabh", ""], ["Bhat", "Aruna", ""]]}, {"id": "2105.11277", "submitter": "Andr\\'e Vict\\'oria Matias", "authors": "Andr\\'e Vict\\'oria Matias, Jo\\~ao Gustavo Atkinson Amorim, Luiz\n  Antonio Buschetto Macarini, Allan Cerentini, Alexandre Sherlley Casimiro\n  Onofre, Fabiana Botelho de Miranda Onofre, Felipe Perozzo Dalto\\'e, Marcelo\n  Ricardo Stemmer, Aldo von Wangenheim", "title": "What is the State of the Art of Computer Vision-Assisted Cytology? A\n  Systematic Literature Review", "comments": null, "journal-ref": null, "doi": "10.1016/j.compmedimag.2021.101934", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cytology is a low-cost and non-invasive diagnostic procedure employed to\nsupport the diagnosis of a broad range of pathologies. Computer Vision\ntechnologies, by automatically generating quantitative and objective\ndescriptions of examinations' contents, can help minimize the chances of\nmisdiagnoses and shorten the time required for analysis. To identify the\nstate-of-art of computer vision techniques currently applied to cytology, we\nconducted a Systematic Literature Review. We analyzed papers published in the\nlast 5 years. The initial search was executed in September 2020 and resulted in\n431 articles. After applying the inclusion/exclusion criteria, 157 papers\nremained, which we analyzed to build a picture of the tendencies and problems\npresent in this research area, highlighting the computer vision methods,\nstaining techniques, evaluation metrics, and the availability of the used\ndatasets and computer code. As a result, we identified that the most used\nmethods in the analyzed works are deep learning-based (70 papers), while fewer\nworks employ classic computer vision only (101 papers). The most recurrent\nmetric used for classification and object detection was the accuracy (33 papers\nand 5 papers), while for segmentation it was the Dice Similarity Coefficient\n(38 papers). Regarding staining techniques, Papanicolaou was the most employed\none (130 papers), followed by H&E (20 papers) and Feulgen (5 papers). Twelve of\nthe datasets used in the papers are publicly available, with the DTU/Herlev\ndataset being the most used one. We conclude that there still is a lack of\nhigh-quality datasets for many types of stains and most of the works are not\nmature enough to be applied in a daily clinical diagnostic routine. We also\nidentified a growing tendency towards adopting deep learning-based approaches\nas the methods of choice.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:50:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Matias", "Andr\u00e9 Vict\u00f3ria", ""], ["Amorim", "Jo\u00e3o Gustavo Atkinson", ""], ["Macarini", "Luiz Antonio Buschetto", ""], ["Cerentini", "Allan", ""], ["Onofre", "Alexandre Sherlley Casimiro", ""], ["Onofre", "Fabiana Botelho de Miranda", ""], ["Dalto\u00e9", "Felipe Perozzo", ""], ["Stemmer", "Marcelo Ricardo", ""], ["von Wangenheim", "Aldo", ""]]}, {"id": "2105.11283", "submitter": "Eugene Valassakis", "authors": "Eugene Valassakis, Norman Di Palo and Edward Johns", "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide\n  Task Spaces", "comments": "To be published at IROS 2021. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:12:38 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:42:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Valassakis", "Eugene", ""], ["Di Palo", "Norman", ""], ["Johns", "Edward", ""]]}, {"id": "2105.11293", "submitter": "Shijie Fang", "authors": "Shijie Fang, Yuhang Cao, Xinjiang Wang, Kai Chen, Dahua Lin, Wayne\n  Zhang", "title": "WSSOD: A New Pipeline for Weakly- and Semi-Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of object detection, to a great extent, depends on the\navailability of large annotated datasets. To alleviate the annotation cost, the\nresearch community has explored a number of ways to exploit unlabeled or weakly\nlabeled data. However, such efforts have met with limited success so far. In\nthis work, we revisit the problem with a pragmatic standpoint, trying to\nexplore a new balance between detection performance and annotation cost by\njointly exploiting fully and weakly annotated data. Specifically, we propose a\nweakly- and semi-supervised object detection framework (WSSOD), which involves\na two-stage learning procedure. An agent detector is first trained on a joint\ndataset and then used to predict pseudo bounding boxes on weakly-annotated\nimages. The underlying assumptions in the current as well as common\nsemi-supervised pipelines are also carefully examined under a unified EM\nformulation. On top of this framework, weakly-supervised loss (WSL), label\nattention and random pseudo-label sampling (RPS) strategies are introduced to\nrelax these assumptions, bringing additional improvement on the efficacy of the\ndetection pipeline. The proposed framework demonstrates remarkable performance\non PASCAL-VOC and MSCOCO benchmark, achieving a high performance comparable to\nthose obtained in fully-supervised settings, with only one third of the\nannotations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 11:58:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Fang", "Shijie", ""], ["Cao", "Yuhang", ""], ["Wang", "Xinjiang", ""], ["Chen", "Kai", ""], ["Lin", "Dahua", ""], ["Zhang", "Wayne", ""]]}, {"id": "2105.11307", "submitter": "Deng Li", "authors": "Deng Li, Yue Wu, and Yicong Zhou", "title": "LineCounter: Learning Handwritten Text Line Segmentation by Counting", "comments": "Submitted to 28th IEEE International Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten Text Line Segmentation (HTLS) is a low-level but important task\nfor many higher-level document processing tasks like handwritten text\nrecognition. It is often formulated in terms of semantic segmentation or object\ndetection in deep learning. However, both formulations have serious\nshortcomings. The former requires heavy post-processing of splitting/merging\nadjacent segments, while the latter may fail on dense or curved texts. In this\npaper, we propose a novel Line Counting formulation for HTLS -- that involves\ncounting the number of text lines from the top at every pixel location. This\nformulation helps learn an end-to-end HTLS solution that directly predicts\nper-pixel line number for a given document image. Furthermore, we propose a\ndeep neural network (DNN) model LineCounter to perform HTLS through the Line\nCounting formulation. Our extensive experiments on the three public datasets\n(ICDAR2013-HSC, HIT-MW, and VML-AHTE) demonstrate that LineCounter outperforms\nstate-of-the-art HTLS approaches. Source code is available at\nhttps://github.com/Leedeng/Line-Counter.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:42:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Deng", ""], ["Wu", "Yue", ""], ["Zhou", "Yicong", ""]]}, {"id": "2105.11312", "submitter": "Bin Sun", "authors": "Bin Sun, Dehui Kong, Shaofan Wang, Lichun Wang, Baocai Yin", "title": "Real-time Human Action Recognition Using Locally Aggregated\n  Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model", "comments": "CYB-R1,12 pages with 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition is referred to as the classification of action\nsequences which consist of 3D skeleton joints. While many research work are\ndevoted to 3D action recognition, it mainly suffers from three problems: highly\ncomplicated articulation, a great amount of noise, and a low implementation\nefficiency. To tackle all these problems, we propose a real-time 3D action\nrecognition framework by integrating the locally aggregated kinematic-guided\nskeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first\ndefine the skeletonlet as a few combinations of joint offsets grouped in terms\nof kinematic principle, and then represent an action sequence using LAKS, which\nconsists of a denoising phase and a locally aggregating phase. The denoising\nphase detects the noisy action data and adjust it by replacing all the features\nwithin it with the features of the corresponding previous frame, while the\nlocally aggregating phase sums the difference between an offset feature of the\nskeletonlet and its cluster center together over all the offset features of the\nsequence. Finally, the SHA model which combines sparse representation with a\nhashing model, aiming at promoting the recognition accuracy while maintaining a\nhigh efficiency. Experimental results on MSRAction3D, UTKinectAction3D and\nFlorence3DAction datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in both recognition accuracy and implementation\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:46:40 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sun", "Bin", ""], ["Kong", "Dehui", ""], ["Wang", "Shaofan", ""], ["Wang", "Lichun", ""], ["Yin", "Baocai", ""]]}, {"id": "2105.11333", "submitter": "Jong Hak Moon", "authors": "Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Edward Choi", "title": "Multi-modal Understanding and Generation for Medical Images and Text via\n  Vision-Language Pre-Training", "comments": "v1: Main paper + supplementary material (15 pages, 5 figures, 6\n  tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)\nwhich adopts a Transformer-based architecture combined with a novel multimodal\nattention masking scheme to maximize generalization performance for both\nvision-language understanding tasks (image-report retrieval, disease\nclassification, medical visual question answering) and vision-language\ngeneration task (report generation). By rigorously evaluating the proposed\nmodel on four downstream tasks with two chest X-ray image datasets (MIMIC-CXR\nand Open-I), we empirically demonstrate the superior downstream task\nperformance of MedViLL against various baselines including task-specific\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:14:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Moon", "Jong Hak", ""], ["Lee", "Hyungyung", ""], ["Shin", "Woncheol", ""], ["Choi", "Edward", ""]]}, {"id": "2105.11352", "submitter": "Petr Hruby", "authors": "Petr Hruby and Tomas Pajdla", "title": "Reconstructing Small 3D Objects in front of a Textured Background", "comments": "36 pages total: 11 pages paper and 25 pages supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a technique for a complete 3D reconstruction of small objects\nmoving in front of a textured background. It is a particular variation of\nmultibody structure from motion, which specializes to two objects only. The\nscene is captured in several static configurations between which the relative\npose of the two objects may change. We reconstruct every static configuration\nindividually and segment the points locally by finding multiple poses of\ncameras that capture the scene's other configurations. Then, the local\nsegmentation results are combined, and the reconstructions are merged into the\nresulting model of the scene. In experiments with real artifacts, we show that\nour approach has practical advantages when reconstructing 3D objects from all\nsides. In this setting, our method outperforms the state-of-the-art. We\nintegrate our method into the state of the art 3D reconstruction pipeline\nCOLMAP.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:36:33 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hruby", "Petr", ""], ["Pajdla", "Tomas", ""]]}, {"id": "2105.11356", "submitter": "Vaanathi Sundaresan PhD", "authors": "Vaanathi Sundaresan, Ludovica Griffanti, Mark Jenkinson", "title": "Brain tumour segmentation using a triplanar ensemble of U-Nets", "comments": null, "journal-ref": "In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\n  Brain Injuries. BrainLes 2020, MICCAI 2020. LNCS, vol 12658. Springer, Cham\n  2021", "doi": "10.1007/978-3-030-72084-1_31", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gliomas appear with wide variation in their characteristics both in terms of\ntheir appearance and location on brain MR images, which makes robust tumour\nsegmentation highly challenging, and leads to high inter-rater variability even\nin manual segmentations. In this work, we propose a triplanar ensemble network,\nwith an independent tumour core prediction module, for accurate segmentation of\nthese tumours and their sub-regions. On evaluating our method on the MICCAI\nBrain Tumor Segmentation (BraTS) challenge validation dataset, for tumour\nsub-regions, we achieved a Dice similarity coefficient of 0.77 for both\nenhancing tumour (ET) and tumour core (TC). In the case of the whole tumour\n(WT) region, we achieved a Dice value of 0.89, which is on par with the\ntop-ranking methods from BraTS'17-19. Our method achieved an evaluation score\nthat was the equal 5th highest value (with our method ranking in 10th place) in\nthe BraTS'20 challenge, with mean Dice values of 0.81, 0.89 and 0.84 on ET, WT\nand TC regions respectively on the BraTS'20 unseen test dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:39:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sundaresan", "Vaanathi", ""], ["Griffanti", "Ludovica", ""], ["Jenkinson", "Mark", ""]]}, {"id": "2105.11361", "submitter": "Ankita Joshi", "authors": "Ankita Joshi, Yi Hong", "title": "DDR-Net: Dividing and Downsampling Mixed Network for Diffeomorphic Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep diffeomorphic registration faces significant challenges for\nhigh-dimensional images, especially in terms of memory limits. Existing\napproaches either downsample original images, or approximate underlying\ntransformations, or reduce model size. The information loss during the\napproximation or insufficient model capacity is a hindrance to the registration\naccuracy for high-dimensional images, e.g., 3D medical volumes. In this paper,\nwe propose a Dividing and Downsampling mixed Registration network (DDR-Net), a\ngeneral architecture that preserves most of the image information at multiple\nscales. DDR-Net leverages the global context via downsampling the input and\nutilizes the local details from divided chunks of the input images. This design\nreduces the network input size and its memory cost; meanwhile, by fusing global\nand local information, DDR-Net obtains both coarse-level and fine-level\nalignments in the final deformation fields. We evaluate DDR-Net on three public\ndatasets, i.e., OASIS, IBSR18, and 3DIRCADB-01, and the experimental results\ndemonstrate our approach outperforms existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:45:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Joshi", "Ankita", ""], ["Hong", "Yi", ""]]}, {"id": "2105.11364", "submitter": "Nikhil Kasukurthi", "authors": "Shivam Shah, Nikhil Kasukurthi, Harshit Pande", "title": "Dynamic region proposal networks for semantic segmentation in automated\n  glaucoma screening", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI.2019.8759171", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Screening for the diagnosis of glaucoma through a fundus image can be\ndetermined by the optic cup to disc diameter ratio (CDR), which requires the\nsegmentation of the cup and disc regions. In this paper, we propose two novel\napproaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of\nInterest Model-based segmentation (WRoIM) to identify disc and cup boundaries.\nUnlike the previous approaches, the proposed methods are trained end-to-end\nthrough a single neural network architecture and use dynamic cropping instead\nof manual or traditional computer vision-based cropping. We are able to achieve\nsimilar performance as that of state-of-the-art approaches with less number of\nnetwork parameters. Our experiments include comparison with different best\nknown methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With\n$7.8 \\times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89\nfor disc/cup segmentation on Drishti-GS1 data whereas the existing\nstate-of-the-art approach uses $19.8\\times 10^6$ parameters to achieve a dice\nscore of 0.97/0.89.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:19:14 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shah", "Shivam", ""], ["Kasukurthi", "Nikhil", ""], ["Pande", "Harshit", ""]]}, {"id": "2105.11373", "submitter": "Filip Radenovi\\'c", "authors": "Filip Radenovic, Animesh Sinha, Albert Gordo, Tamara Berg, Dhruv\n  Mahajan", "title": "Large-Scale Attribute-Object Compositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning how to predict attribute-object compositions\nfrom images, and its generalization to unseen compositions missing from the\ntraining data. To the best of our knowledge, this is a first large-scale study\nof this problem, involving hundreds of thousands of compositions. We train our\nframework with images from Instagram using hashtags as noisy weak supervision.\nWe make careful design choices for data collection and modeling, in order to\nhandle noisy annotations and unseen compositions. Finally, extensive\nevaluations show that learning to compose classifiers outperforms late fusion\nof individual attribute and object predictions, especially in the case of\nunseen attribute-object pairs.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:05:41 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Radenovic", "Filip", ""], ["Sinha", "Animesh", ""], ["Gordo", "Albert", ""], ["Berg", "Tamara", ""], ["Mahajan", "Dhruv", ""]]}, {"id": "2105.11422", "submitter": "Ouyang Ou", "authors": "Mengxiao Tian, Hao Guo, Chengjiang Long", "title": "Multi-Level Attentive Convoluntional Neural Network for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently the crowd counting has received more and more attention. Especially\nthe technology of high-density environment has become an important research\ncontent, and the relevant methods for the existence of extremely dense crowd\nare not optimal. In this paper, we propose a multi-level attentive\nConvolutional Neural Network (MLAttnCNN) for crowd counting. We extract\nhigh-level contextual information with multiple different scales applied in\npooling, and use multi-level attention modules to enrich the characteristics at\ndifferent layers to achieve more efficient multi-scale feature fusion, which is\nable to be used to generate a more accurate density map with dilated\nconvolutions and a $1\\times 1$ convolution. The extensive experiments on three\navailable public datasets show that our proposed network achieves\noutperformance to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:29:00 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tian", "Mengxiao", ""], ["Guo", "Hao", ""], ["Long", "Chengjiang", ""]]}, {"id": "2105.11427", "submitter": "Yunke Zhang", "authors": "Yunke Zhang, Chi Wang, Miaomiao Cui, Peiran Ren, Xuansong Xie,\n  Xian-sheng Hua, Hujun Bao, Qixing Huang, Weiwei Xu", "title": "Attention-guided Temporally Coherent Video Object Matting", "comments": "10 pages, 6 figures, MM '21 camera-ready", "journal-ref": null, "doi": "10.1145/3474085.3475623", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep learning-based video object matting method\nthat can achieve temporally coherent matting results. Its key component is an\nattention-based temporal aggregation module that maximizes image matting\nnetworks' strength for video matting networks. This module computes temporal\ncorrelations for pixels adjacent to each other along the time axis in feature\nspace, which is robust against motion noises. We also design a novel loss term\nto train the attention weights, which drastically boosts the video matting\nperformance. Besides, we show how to effectively solve the trimap generation\nproblem by fine-tuning a state-of-the-art video object segmentation network\nwith a sparse set of user-annotated keyframes. To facilitate video matting and\ntrimap generation networks' training, we construct a large-scale video matting\ndataset with 80 training and 28 validation foreground video clips with\nground-truth alpha mattes. Experimental results show that our method can\ngenerate high-quality alpha mattes for various videos featuring appearance\nchange, occlusion, and fast motion. Our code and dataset can be found at:\nhttps://github.com/yunkezhang/TCVOM\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:34:57 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 10:17:08 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 16:12:46 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhang", "Yunke", ""], ["Wang", "Chi", ""], ["Cui", "Miaomiao", ""], ["Ren", "Peiran", ""], ["Xie", "Xuansong", ""], ["Hua", "Xian-sheng", ""], ["Bao", "Hujun", ""], ["Huang", "Qixing", ""], ["Xu", "Weiwei", ""]]}, {"id": "2105.11432", "submitter": "Dinesh Jackson Samuel", "authors": "Dinesh Jackson Samuel and Rajesh Kanna Baskaran", "title": "Design to automate the detection and counting of Tuberculosis(TB)\n  bacilli", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tuberculosis is a contagious disease which is one of the leading causes of\ndeath, globally. The general diagnosis methods for tuberculosis include\nmicroscopic examination, tuberculin skin test, culture method, enzyme linked\nimmunosorbent assay (ELISA) and electronic nose system. World Health\nOrganization (WHO) recommends standard microscopic examination for early\ndiagnosis of tuberculosis. In microscopy, the technician examines field of\nviews (FOVs) in sputum smear for presence of any TB bacilli and counts the\nnumber of TB bacilli per FOV to report the level of severity. This process is\ntime consuming with an increased concentration for an experienced staff to\nexamine a single sputum smear. The examination demands for skilled technicians\nin high-prevalence countries which may lead to overload, fatigue and diminishes\nthe quality of microscopy. Thus, a computer assisted system is proposed and\ndesigned for the detection of tuberculosis bacilli to assist pathologists with\nincreased sensitivity and specificity. The manual efforts in detecting and\ncounting the number of TB bacilli is greatly minimized. The system obtains\nZiehl-Neelsen stained microscopic images from conventional microscope at 100x\nmagnification and passes the data to the detection system. Initially the\nsegmentation of TB bacilli was done using RGB thresholding and Sauvola's\nadaptive thresholding algorithm. To eliminate the non-TB bacilli from coarse\nlevel segmentation, shape descriptors like area, perimeter, convex hull, major\naxis length and eccentricity are used to extract only the TB bacilli features.\nFinally, the TB bacilli are counted using the generated bounding boxes to\nreport the level of severity.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:41:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Samuel", "Dinesh Jackson", ""], ["Baskaran", "Rajesh Kanna", ""]]}, {"id": "2105.11443", "submitter": "Arren Glover", "authors": "Arren Glover, Aiko Dinale, Leandro De Souza Rosa, Simeon Bamford, and\n  Chiara Bartolozzi", "title": "luvHarris: A Practical Corner Detector for Event-cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been a number of corner detection methods proposed for event\ncameras in the last years, since event-driven computer vision has become more\naccessible. Current state-of-the-art have either unsatisfactory accuracy or\nreal-time performance when considered for practical use; random motion using a\nlive camera in an unconstrained environment. In this paper, we present yet\nanother method to perform corner detection, dubbed look-up event-Harris\n(luvHarris), that employs the Harris algorithm for high accuracy but manages an\nimproved event throughput. Our method has two major contributions, 1. a novel\n\"threshold ordinal event-surface\" that removes certain tuning parameters and is\nwell suited for Harris operations, and 2. an implementation of the Harris\nalgorithm such that the computational load per-event is minimised and\ncomputational heavy convolutions are performed only 'as-fast-as-possible', i.e.\nonly as computational resources are available. The result is a practical,\nreal-time, and robust corner detector that runs more than $2.6\\times$ the speed\nof current state-of-the-art; a necessity when using high-resolution\nevent-camera in real-time. We explain the considerations taken for the\napproach, compare the algorithm to current state-of-the-art in terms of\ncomputational performance and detection accuracy, and discuss the validity of\nthe proposed approach for event cameras.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:54:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Glover", "Arren", ""], ["Dinale", "Aiko", ""], ["Rosa", "Leandro De Souza", ""], ["Bamford", "Simeon", ""], ["Bartolozzi", "Chiara", ""]]}, {"id": "2105.11450", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Songyang Zhang, Liwei Wang, Jiebo Luo", "title": "SAT: 2D Semantics Assisted Training for 3D Visual Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D visual grounding aims at grounding a natural language description about a\n3D scene, usually represented in the form of 3D point clouds, to the targeted\nobject region. Point clouds are sparse, noisy, and contain limited semantic\ninformation compared with 2D images. These inherent limitations make the 3D\nvisual grounding problem more challenging. In this study, we propose 2D\nSemantics Assisted Training (SAT) that utilizes 2D image semantics in the\ntraining stage to ease point-cloud-language joint representation learning and\nassist 3D visual grounding. The main idea is to learn auxiliary alignments\nbetween rich, clean 2D object representations and the corresponding objects or\nmentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object\nlabel, image feature, and 2D geometric feature, as the extra input in training\nbut does not require such inputs during inference. By effectively utilizing 2D\nsemantics in training, our approach boosts the accuracy on the Nr3D dataset\nfrom 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with\nthe identical network architecture and inference input. Our approach\noutperforms the state of the art by large margins on multiple 3D visual\ngrounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and\n+5.6% on ScanRef.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:58:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Zhang", "Songyang", ""], ["Wang", "Liwei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2105.11486", "submitter": "Ashwin Prakash Nalwade", "authors": "Ashwin Nalwade, Jackie Kisa", "title": "Experimenting with Knowledge Distillation techniques for performing\n  Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-modal magnetic resonance imaging (MRI) is a crucial method for\nanalyzing the human brain. It is usually used for diagnosing diseases and for\nmaking valuable decisions regarding the treatments - for instance, checking for\ngliomas in the human brain. With varying degrees of severity and detection,\nproperly diagnosing gliomas is one of the most daunting and significant\nanalysis tasks in modern-day medicine. Our primary focus is on working with\ndifferent approaches to perform the segmentation of brain tumors in multimodal\nMRI scans. Now, the quantity, variability of the data used for training has\nalways been considered to be crucial for developing excellent models. Hence, we\nalso want to experiment with Knowledge Distillation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:17:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Nalwade", "Ashwin", ""], ["Kisa", "Jackie", ""]]}, {"id": "2105.11494", "submitter": "Matthieu Zins", "authors": "Matthieu Zins, Gilles Simon, Marie-Odile Berger", "title": "3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation", "comments": "Presented at 3DV 2020. Code and models released at\n  https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization", "journal-ref": "3DV 2020", "doi": "10.1109/3DV50981.2020.00038", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a method for coarse camera pose computation which\nis robust to viewing conditions and does not require a detailed model of the\nscene. This method meets the growing need of easy deployment of robotics or\naugmented reality applications in any environments, especially those for which\nno accurate 3D model nor huge amount of ground truth data are available. It\nexploits the ability of deep learning techniques to reliably detect objects\nregardless of viewing conditions. Previous works have also shown that\nabstracting the geometry of a scene of objects by an ellipsoid cloud allows to\ncompute the camera pose accurately enough for various application needs. Though\npromising, these approaches use the ellipses fitted to the detection bounding\nboxes as an approximation of the imaged objects. In this paper, we go one step\nfurther and propose a learning-based method which detects improved elliptic\napproximations of objects which are coherent with the 3D ellipsoid in terms of\nperspective projection. Experiments prove that the accuracy of the computed\npose significantly increases thanks to our method and is more robust to the\nvariability of the boundaries of the detection boxes. This is achieved with\nvery little effort in terms of training data acquisition -- a few hundred\ncalibrated images of which only three need manual object annotation. Code and\nmodels are released at\nhttps://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:40:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zins", "Matthieu", ""], ["Simon", "Gilles", ""], ["Berger", "Marie-Odile", ""]]}, {"id": "2105.11527", "submitter": "Qi Qian", "authors": "Qi Qian, Yuanhong Xu, Juhua Hu, Hao Li, Rong Jin", "title": "Unsupervised Visual Representation Learning by Online Constrained\n  K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster discrimination is an effective pretext task for unsupervised\nrepresentation learning, which often consists of two phases: clustering and\ndiscrimination. Clustering is to assign each instance a pseudo label that will\nbe used to learn representations in discrimination. The main challenge resides\nin clustering since many prevalent clustering methods (e.g., k-means) have to\nrun in a batch mode that goes multiple iterations over the whole data.\nRecently, a balanced online clustering method, i.e., SwAV, is proposed for\nrepresentation learning. However, the assignment is optimized within only a\nsmall subset of data, which can be suboptimal. To address these challenges, we\nfirst investigate the objective of clustering-based representation learning\nfrom the perspective of distance metric learning. Based on this, we propose a\nnovel clustering-based pretext task with online \\textbf{Co}nstrained\n\\textbf{K}-m\\textbf{e}ans (\\textbf{CoKe}) to learn representations and\nrelations between instances simultaneously. Compared with the balanced\nclustering that each cluster has exactly the same size, we only constrain the\nminimum size of clusters to flexibly capture the inherent data structure. More\nimportantly, our online assignment method has a theoretical guarantee to\napproach the global optimum. Finally, two variance reduction strategies are\nproposed to make the clustering robust for different augmentations. Without\nkeeping representations of instances, the data is accessed in an online mode in\nCoKe while a single view of instances at each iteration is sufficient to\ndemonstrate a better performance than contrastive learning methods relying on\ntwo views. Extensive experiments on ImageNet verify the efficacy of our\nproposal. Code will be released.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 20:38:32 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Qian", "Qi", ""], ["Xu", "Yuanhong", ""], ["Hu", "Juhua", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2105.11541", "submitter": "Qing Ping", "authors": "Tao Tu, Qing Ping, Govind Thattai, Gokhan Tur, Prem Natarajan", "title": "Learning Better Visual Dialog Agents with Pretrained Visual-Linguistic\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  GuessWhat?! is a two-player visual dialog guessing game where player A asks a\nsequence of yes/no questions (Questioner) and makes a final guess (Guesser)\nabout a target object in an image, based on answers from player B (Oracle).\nBased on this dialog history between the Questioner and the Oracle, a Guesser\nmakes a final guess of the target object. Previous baseline Oracle model\nencodes no visual information in the model, and it cannot fully understand\ncomplex questions about color, shape, relationships and so on. Most existing\nwork for Guesser encode the dialog history as a whole and train the Guesser\nmodels from scratch on the GuessWhat?! dataset. This is problematic since\nlanguage encoder tend to forget long-term history and the GuessWhat?! data is\nsparse in terms of learning visual grounding of objects. Previous work for\nQuestioner introduces state tracking mechanism into the model, but it is\nlearned as a soft intermediates without any prior vision-linguistic insights.\nTo bridge these gaps, in this paper we propose Vilbert-based Oracle, Guesser\nand Questioner, which are all built on top of pretrained vision-linguistic\nmodel, Vilbert. We introduce two-way background/target fusion mechanism into\nVilbert-Oracle to account for both intra and inter-object questions. We propose\na unified framework for Vilbert-Guesser and Vilbert-Questioner, where\nstate-estimator is introduced to best utilize Vilbert's power on single-turn\nreferring expression comprehension. Experimental results show that our proposed\nmodels outperform state-of-the-art models significantly by 7%, 10%, 12% for\nOracle, Guesser and End-to-End Questioner respectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 21:09:20 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Tu", "Tao", ""], ["Ping", "Qing", ""], ["Thattai", "Govind", ""], ["Tur", "Gokhan", ""], ["Natarajan", "Prem", ""]]}, {"id": "2105.11547", "submitter": "Yuexuan Wu", "authors": "Yuexuan Wu, Suprateek Kundu, Jennifer S. Stevens, Negar Fani, Anuj\n  Srivastava", "title": "Elastic Shape Analysis of Brain Structures for Predictive Modeling of\n  PTSD", "comments": "33 pages; Supplementary Materials and interactive visualizations are\n  available in https://www.dropbox.com/home/Paper%20Interactive%20Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is increasing evidence on the importance of brain morphology in\npredicting and classifying mental disorders. However, the vast majority of\ncurrent shape approaches rely heavily on vertex-wise analysis that may not\nsuccessfully capture complexities of subcortical structures. Additionally, the\npast works do not include interactions between these structures and exposure\nfactors. Predictive modeling with such interactions is of paramount interest in\nheterogeneous mental disorders such as PTSD, where trauma exposure interacts\nwith brain shape changes to influence behavior. We propose a comprehensive\nframework that overcomes these limitations by representing brain substructures\nas continuous parameterized surfaces and quantifying their shape differences\nusing elastic shape metrics. Using the elastic shape metric, we compute shape\nsummaries of subcortical data and represent individual shapes by their\nprincipal scores. These representations allow visualization tools that help\nlocalize changes when these PCs are varied. Subsequently, these PCs, the\nauxiliary exposure variables, and their interactions are used for regression\nmodeling. We apply our method to data from the Grady Trauma Project, where the\ngoal is to predict clinical measures of PTSD using shapes of brain\nsubstructures. Our analysis revealed considerably greater predictive power\nunder the elastic shape analysis than widely used approaches such as\nvertex-wise shape analysis and even volumetric analysis. It helped identify\nlocal deformations in brain shapes related to change in PTSD severity. To our\nknowledge, this is one of the first brain shape analysis approaches that can\nseamlessly integrate the pre-processing steps under one umbrella for improved\naccuracy and are naturally able to account for interactions between brain shape\nand additional covariates to yield superior predictive performance when\nmodeling clinical outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 21:33:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wu", "Yuexuan", ""], ["Kundu", "Suprateek", ""], ["Stevens", "Jennifer S.", ""], ["Fani", "Negar", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2105.11559", "submitter": "Taylor Archibald", "authors": "Taylor Archibald, Mason Poggemann, Aaron Chan, Tony Martinez", "title": "TRACE: A Differentiable Approach to Line-level Stroke Recovery for\n  Offline Handwritten Text", "comments": "Accepted as a conference paper at the 16th International Conference\n  on Document Analysis and Recognition (ICDAR), Lausanne, Switzerland, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke order and velocity are helpful features in the fields of signature\nverification, handwriting recognition, and handwriting synthesis. Recovering\nthese features from offline handwritten text is a challenging and well-studied\nproblem. We propose a new model called TRACE (Trajectory Recovery by an\nAdaptively-trained Convolutional Encoder). TRACE is a differentiable approach\nthat uses a convolutional recurrent neural network (CRNN) to infer temporal\nstroke information from long lines of offline handwritten text with many\ncharacters and dynamic time warping (DTW) to align predictions and ground truth\npoints. TRACE is perhaps the first system to be trained end-to-end on entire\nlines of text of arbitrary width and does not require the use of dynamic\nexemplars. Moreover, the system does not require images to undergo any\npre-processing, nor do the predictions require any post-processing.\nConsequently, the recovered trajectory is differentiable and can be used as a\nloss function for other tasks, including synthesizing offline handwritten text.\n  We demonstrate that temporal stroke information recovered by TRACE from\noffline data can be used for handwriting synthesis and establish the first\nbenchmarks for a stroke trajectory recovery system trained on the IAM online\nhandwriting dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 22:15:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Archibald", "Taylor", ""], ["Poggemann", "Mason", ""], ["Chan", "Aaron", ""], ["Martinez", "Tony", ""]]}, {"id": "2105.11576", "submitter": "Xiao Huang", "authors": "Jiaming Wang, Zhenfeng Shao, Xiao Huang, Tao Lu, Ruiqian Zhang, Jiayi\n  Ma", "title": "Pan-sharpening via High-pass Modification Convolutional Neural Network", "comments": "5 pages, 5 figures, accepted by the 28th IEEE International\n  Conference on Image Processing (ICIP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing deep learning-based pan-sharpening methods have several widely\nrecognized issues, such as spectral distortion and insufficient spatial texture\nenhancement, we propose a novel pan-sharpening convolutional neural network\nbased on a high-pass modification block. Different from existing methods, the\nproposed block is designed to learn the high-pass information, leading to\nenhance spatial information in each band of the multi-spectral-resolution\nimages. To facilitate the generation of visually appealing pan-sharpened\nimages, we propose a perceptual loss function and further optimize the model\nbased on high-level features in the near-infrared space. Experiments\ndemonstrate the superior performance of the proposed method compared to the\nstate-of-the-art pan-sharpening methods, both quantitatively and qualitatively.\nThe proposed model is open-sourced at https://github.com/jiaming-wang/HMB.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:39:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Jiaming", ""], ["Shao", "Zhenfeng", ""], ["Huang", "Xiao", ""], ["Lu", "Tao", ""], ["Zhang", "Ruiqian", ""], ["Ma", "Jiayi", ""]]}, {"id": "2105.11578", "submitter": "Yi Zhang", "authors": "Yi Zhang, Lu Zhang, Jing Zhang, Kang Wang, Wassim Hamidouche, Olivier\n  Deforges", "title": "SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg}\n  Videos", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset containing\nvarious real-life daily scenes borrowed from http://hidden.for.anonymity, with\nhierarchical annotations for 6,268 key frames uniformly sampled from 37,403\nomnidirectional video frames at 4K resolution. Since so far there is no method\nproposed for 360{\\deg} image/video SHD, we systematically benchmark 11\nrepresentative state-of-the-art salient object detection approaches on our\nSHD360. We hope our proposed dataset and benchmark could serve as a good\nstarting point for advancing human-centric researches towards 360{\\deg}\npanoramic data. Our dataset and benchmark will be publicly available at\nhttps://github.com/PanoAsh/SHD360.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:51:29 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 14:54:53 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhang", "Yi", ""], ["Zhang", "Lu", ""], ["Zhang", "Jing", ""], ["Wang", "Kang", ""], ["Hamidouche", "Wassim", ""], ["Deforges", "Olivier", ""]]}, {"id": "2105.11587", "submitter": "Yanyan Li", "authors": "Hongzhi Du, Yanyan Li, Yanbiao Sun, Jigui Zhu and Federico Tombari", "title": "SRH-Net: Stacked Recurrent Hourglass Network for Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cost aggregation strategy shows a crucial role in learning-based stereo\nmatching tasks, where 3D convolutional filters obtain state of the art but\nrequire intensive computation resources, while 2D operations need less GPU\nmemory but are sensitive to domain shift. In this paper, we decouple the 4D\ncubic cost volume used by 3D convolutional filters into sequential cost maps\nalong the direction of disparity instead of dealing with it at once by\nexploiting a recurrent cost aggregation strategy. Furthermore, a novel\nrecurrent module, Stacked Recurrent Hourglass (SRH), is proposed to process\neach cost map. Our hourglass network is constructed based on Gated Recurrent\nUnits (GRUs) and down/upsampling layers, which provides GRUs larger receptive\nfields. Then two hourglass networks are stacked together, while multi-scale\ninformation is processed by skip connections to enhance the performance of the\npipeline in textureless areas. The proposed architecture is implemented in an\nend-to-end pipeline and evaluated on public datasets, which reduces GPU memory\nconsumption by up to 56.1\\% compared with PSMNet using stacked hourglass 3D\nCNNs without the degradation of accuracy. Then, we further demonstrate the\nscalability of the proposed method on several high-resolution pairs, while\npreviously learned approaches often fail due to the memory constraint. The code\nis released at \\url{https://github.com/hongzhidu/SRHNet}.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:10:56 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Du", "Hongzhi", ""], ["Li", "Yanyan", ""], ["Sun", "Yanbiao", ""], ["Zhu", "Jigui", ""], ["Tombari", "Federico", ""]]}, {"id": "2105.11589", "submitter": "Karthik Gopalakrishnan", "authors": "Ayush Shrivastava, Karthik Gopalakrishnan, Yang Liu, Robinson\n  Piramuthu, Gokhan T\\\"ur, Devi Parikh, Dilek Hakkani-T\\\"ur", "title": "VISITRON: Visual Semantics-Aligned Interactively Trained\n  Object-Navigator", "comments": "Accepted at NAACL 2021, Visually Grounded Interaction and Language\n  (ViGIL) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive robots navigating photo-realistic environments face challenges\nunderlying vision-and-language navigation (VLN), but in addition, they need to\nbe trained to handle the dynamic nature of dialogue. However, research in\nCooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts\nwith a guide in natural language in order to reach a goal, treats the dialogue\nhistory as a VLN-style static instruction. In this paper, we present VISITRON,\na navigator better suited to the interactive regime inherent to CVDN by being\ntrained to: i) identify and associate object-level concepts and semantics\nbetween the environment and dialogue history, ii) identify when to interact vs.\nnavigate via imitation learning of a binary classification head. We perform\nextensive ablations with VISITRON to gain empirical insights and improve\nperformance on CVDN. VISITRON is competitive with models on the static CVDN\nleaderboard. We also propose a generalized interactive regime to fine-tune and\nevaluate VISITRON and future such models with pre-trained guides for\nadaptability.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:21:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shrivastava", "Ayush", ""], ["Gopalakrishnan", "Karthik", ""], ["Liu", "Yang", ""], ["Piramuthu", "Robinson", ""], ["T\u00fcr", "Gokhan", ""], ["Parikh", "Devi", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "2105.11595", "submitter": "Bing Shuai", "authors": "Bing Shuai, Andrew Berneshawi, Xinyu Li, Davide Modolo, Joseph Tighe", "title": "SiamMOT: Siamese Multi-Object Tracking", "comments": null, "journal-ref": "CVPR2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on improving online multi-object tracking (MOT). In\nparticular, we introduce a region-based Siamese Multi-Object Tracking network,\nwhich we name SiamMOT. SiamMOT includes a motion model that estimates the\ninstance's movement between two frames such that detected instances are\nassociated. To explore how the motion modelling affects its tracking\ncapability, we present two variants of Siamese tracker, one that implicitly\nmodels motion and one that models it explicitly. We carry out extensive\nquantitative experiments on three different MOT datasets: MOT17, TAO-person and\nCaltech Roadside Pedestrians, showing the importance of motion modelling for\nMOT and the ability of SiamMOT to substantially outperform the\nstate-of-the-art. Finally, SiamMOT also outperforms the winners of ACM MM'20\nHiEve Grand Challenge on HiEve dataset. Moreover, SiamMOT is efficient, and it\nruns at 17 FPS for 720P videos on a single modern GPU. Codes are available in\n\\url{https://github.com/amazon-research/siam-mot}.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:09:26 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shuai", "Bing", ""], ["Berneshawi", "Andrew", ""], ["Li", "Xinyu", ""], ["Modolo", "Davide", ""], ["Tighe", "Joseph", ""]]}, {"id": "2105.11599", "submitter": "Ziang Cheng", "authors": "Ziang Cheng, Hongdong Li, Yuta Asano, Yinqiang Zheng, Imari Sato", "title": "Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown\n  Generic Reflectance", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recovering the 3D geometry of a purely texture-less object with generally\nunknown surface reflectance (e.g. non-Lambertian) is regarded as a challenging\ntask in multi-view reconstruction. The major obstacle revolves around\nestablishing cross-view correspondences where photometric constancy is\nviolated. This paper proposes a simple and practical solution to overcome this\nchallenge based on a co-located camera-light scanner device. Unlike existing\nsolutions, we do not explicitly solve for correspondence. Instead, we argue the\nproblem is generally well-posed by multi-view geometrical and photometric\nconstraints, and can be solved from a small number of input views. We formulate\nthe reconstruction task as a joint energy minimization over the surface\ngeometry and reflectance. Despite this energy is highly non-convex, we develop\nan optimization algorithm that robustly recovers globally optimal shape and\nreflectance even from a random initialization. Extensive experiments on both\nsimulated and real data have validated our method, and possible future\nextensions are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:28:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cheng", "Ziang", ""], ["Li", "Hongdong", ""], ["Asano", "Yuta", ""], ["Zheng", "Yinqiang", ""], ["Sato", "Imari", ""]]}, {"id": "2105.11605", "submitter": "Tianxing Xu", "authors": "Tian-Xing Xu, Yuan-Chen Guo, Yu-Kun Lai, Song-Hai Zhang", "title": "TransLoc3D : Point Cloud based Large-scale Place Recognition using\n  Adaptive Receptive Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition plays an essential role in the field of autonomous driving\nand robot navigation. Although a number of point cloud based methods have been\nproposed and achieved promising results, few of them take the size difference\nof objects into consideration. For small objects like pedestrians and vehicles,\nlarge receptive fields will capture unrelated information, while small\nreceptive fields would fail to encode complete geometric information for large\nobjects such as buildings. We argue that fixed receptive fields are not well\nsuited for place recognition, and propose a novel Adaptive Receptive Field\nModule (ARFM), which can adaptively adjust the size of the receptive field\nbased on the input point cloud. We also present a novel network architecture,\nnamed TransLoc3D, to obtain discriminative global descriptors of point clouds\nfor the place recognition task. TransLoc3D consists of a 3D sparse\nconvolutional module, an ARFM module, an external transformer network which\naims to capture long range dependency and a NetVLAD layer. Experiments show\nthat our method outperforms prior state-of-the-art results, with an improvement\nof 1.1\\% on average recall@1 on the Oxford RobotCar dataset, and 0.8\\% on the\nB.D. dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:54:31 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:38:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Xu", "Tian-Xing", ""], ["Guo", "Yuan-Chen", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Song-Hai", ""]]}, {"id": "2105.11606", "submitter": "Seung-Hwan Baek", "authors": "Seung-Hwan Baek, Noah Walsh, Ilya Chugunov, Zheng Shi, Felix Heide", "title": "Centimeter-Wave Free-Space Time-of-Flight Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth cameras are emerging as a cornerstone modality with diverse\napplications that directly or indirectly rely on measured depth, including\npersonal devices, robotics, and self-driving vehicles. Although time-of-flight\n(ToF) methods have fueled these applications, the precision and robustness of\nToF methods is limited by relying on photon time-tagging or modulation after\nphoto-conversion. Successful optical modulation approaches have been restricted\nfiber-coupled modulation with large coupling losses or interferometric\nmodulation with sub-cm range, and the precision gap between interferometric\nmethods and ToF methods is more than three orders of magnitudes. In this work,\nwe close this gap and propose a computational imaging method for all-optical\nfree-space correlation before photo-conversion that achieves micron-scale depth\nresolution with robustness to surface reflectance and ambient light with\nconventional silicon intensity sensors. To this end, we solve two technical\nchallenges: modulating at GHz rates and computational phase unwrapping. We\npropose an imaging approach with resonant polarization modulators and devise a\nnovel optical dual-pass frequency-doubling which achieves high modulation\ncontrast at more than 10GHz. At the same time, centimeter-wave modulation\ntogether with a small modulation bandwidth render existing phase unwrapping\nmethods ineffective. We tackle this problem with a neural phase unwrapping\nmethod that exploits that adjacent wraps are often highly correlated. We\nvalidate the proposed method in simulation and experimentally, where it\nachieves micron-scale depth precision. We demonstrate precise depth sensing\nindependently of surface texture and ambient light and compare against existing\nanalog demodulation methods, which we outperform across all tested scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:57:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Baek", "Seung-Hwan", ""], ["Walsh", "Noah", ""], ["Chugunov", "Ilya", ""], ["Shi", "Zheng", ""], ["Heide", "Felix", ""]]}, {"id": "2105.11609", "submitter": "Seung-Hwan Baek", "authors": "Seung-Hwan Baek, Felix Heide", "title": "Polarimetric Spatio-Temporal Light Transport Probing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light can undergo complex interactions with multiple scene surfaces of\ndifferent material types before being reflected towards a detector. During this\ntransport, every surface reflection and propagation is encoded in the\nproperties of the photons that ultimately reach the detector, including travel\ntime, direction, intensity, wavelength and polarization. Conventional imaging\nsystems capture intensity by integrating over all other dimensions of the light\ninto a single quantity, hiding this rich scene information in the accumulated\nmeasurements. Existing methods can untangle these into their spatial and\ntemporal dimensions, fueling geometric scene understanding. However, examining\npolarimetric material properties jointly with geometric properties is an open\nchallenge that could enable unprecedented capabilities beyond geometric\nunderstanding, allowing to incorporate material-dependent semantics. In this\nwork, we propose a computational light-transport imaging method that captures\nthe spatially- and temporally-resolved complete polarimetric response of a\nscene. Our method hinges on a novel 7D tensor theory of light transport. We\ndiscover low-rank structures in the polarimetric tensor dimension and propose a\ndata-driven rotating ellipsometry method that learns to exploit redundancy of\nthe polarimetric structures. We instantiate our theory in two imaging\nprototypes: spatio-polarimetric imaging and coaxial temporal-polarimetric\nimaging. This allows us to decompose scene light transport into temporal,\nspatial, and complete polarimetric dimensions that unveil scene properties\nhidden to conventional methods. We validate the applicability of our method on\ndiverse tasks, including shape reconstruction with subsurface scattering,\nseeing through scattering medium, untangling multi-bounce light transport,\nbreaking metamerism with polarization, and spatio-polarimetric decomposition of\ncrystals.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:16:07 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Baek", "Seung-Hwan", ""], ["Heide", "Felix", ""]]}, {"id": "2105.11610", "submitter": "Jiawang Bian", "authors": "Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao Li, Le Zhang,\n  Chunhua Shen, Ming-Ming Cheng, Ian Reid", "title": "Unsupervised Scale-consistent Depth Learning from Video", "comments": "Accept to IJCV. The source code is available at\n  https://github.com/JiawangBian/SC-SfMLearner-Release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a monocular depth estimator SC-Depth, which requires only\nunlabelled videos for training and enables the scale-consistent prediction at\ninference time. Our contributions include: (i) we propose a geometry\nconsistency loss, which penalizes the inconsistency of predicted depths between\nadjacent views; (ii) we propose a self-discovered mask to automatically\nlocalize moving objects that violate the underlying static scene assumption and\ncause noisy signals during training; (iii) we demonstrate the efficacy of each\ncomponent with a detailed ablation study and show high-quality depth estimation\nresults in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of\nscale-consistent prediction, we show that our monocular-trained deep networks\nare readily integrated into the ORB-SLAM2 system for more robust and accurate\ntracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in\nKITTI, and it generalizes well to the KAIST dataset without additional\ntraining. Finally, we provide several demos for qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:17:56 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bian", "Jia-Wang", ""], ["Zhan", "Huangying", ""], ["Wang", "Naiyan", ""], ["Li", "Zhichao", ""], ["Zhang", "Le", ""], ["Shen", "Chunhua", ""], ["Cheng", "Ming-Ming", ""], ["Reid", "Ian", ""]]}, {"id": "2105.11625", "submitter": "Shuhao Shi", "authors": "S. Shi, Kai Qiao, Shuai Yang, L. Wang, J. Chen and Bin Yan", "title": "AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on\n  Imbalanced Node Classification", "comments": "17 pages, 5 figures, Submitted to MACHINE LEARNING", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Neural Network (GNN) has achieved remarkable success in graph data\nrepresentation. However, the previous work only considered the ideal balanced\ndataset, and the practical imbalanced dataset was rarely considered, which, on\nthe contrary, is of more significance for the application of GNN. Traditional\nmethods such as resampling, reweighting and synthetic samples that deal with\nimbalanced datasets are no longer applicable in GNN. Ensemble models can handle\nimbalanced datasets better compared with single estimator. Besides, ensemble\nlearning can achieve higher estimation accuracy and has better reliability\ncompared with the single estimator. In this paper, we propose an ensemble model\ncalled AdaGCN, which uses a Graph Convolutional Network (GCN) as the base\nestimator during adaptive boosting. In AdaGCN, a higher weight will be set for\nthe training samples that are not properly classified by the previous\nclassifier, and transfer learning is used to reduce computational cost and\nincrease fitting capability. Experiments show that the AdaGCN model we proposed\nachieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of\nadvanced reweighting and resampling methods on synthetic imbalanced datasets,\nwith an average improvement of 4.3%. Our model also improves state-of-the-art\nbaselines on all of the challenging node classification tasks we consider:\nCora, Citeseer, Pubmed, and NELL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:43:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shi", "S.", ""], ["Qiao", "Kai", ""], ["Yang", "Shuai", ""], ["Wang", "L.", ""], ["Chen", "J.", ""], ["Yan", "Bin", ""]]}, {"id": "2105.11628", "submitter": "Guoqing Zhang", "authors": "Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang, Yuhui Zheng,\n  Ruili Wang", "title": "TIPCB: A Simple but Effective Part-based Convolutional Baseline for\n  Text-based Person Search", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based person search is a sub-task in the field of image retrieval, which\naims to retrieve target person images according to a given textual description.\nThe significant feature gap between two modalities makes this task very\nchallenging. Many existing methods attempt to utilize local alignment to\naddress this problem in the fine-grained level. However, most relevant methods\nintroduce additional models or complicated training and evaluation strategies,\nwhich are hard to use in realistic scenarios. In order to facilitate the\npractical application, we propose a simple but effective end-to-end learning\nframework for text-based person search named TIPCB (i.e., Text-Image Part-based\nConvolutional Baseline). Firstly, a novel dual-path local alignment network\nstructure is proposed to extract visual and textual local representations, in\nwhich images are segmented horizontally and texts are aligned adaptively. Then,\nwe propose a multi-stage cross-modal matching strategy, which eliminates the\nmodality gap from three feature levels, including low level, local level and\nglobal level. Extensive experiments are conducted on the widely-used benchmark\ndataset (CUHK-PEDES) and verify that our method outperforms the\nstate-of-the-art methods by 3.69%, 2.95% and 2.31% in terms of Top-1, Top-5 and\nTop-10. Our code has been released in https://github.com/OrangeYHChen/TIPCB.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:00:21 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Chen", "Yuhao", ""], ["Zhang", "Guoqing", ""], ["Lu", "Yujiang", ""], ["Wang", "Zhenxing", ""], ["Zheng", "Yuhui", ""], ["Wang", "Ruili", ""]]}, {"id": "2105.11636", "submitter": "Bo Li", "authors": "Bo Li, Qili Wang, Gim Hee Lee", "title": "FILTRA: Rethinking Steerable CNN by Filter Transform", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steerable CNN imposes the prior knowledge of transformation invariance or\nequivariance in the network architecture to enhance the the network robustness\non geometry transformation of data and reduce overfitting. It has been an\nintuitive and widely used technique to construct a steerable filter by\naugmenting a filter with its transformed copies in the past decades, which is\nnamed as filter transform in this paper. Recently, the problem of steerable CNN\nhas been studied from aspect of group representation theory, which reveals the\nfunction space structure of a steerable kernel function. However, it is not yet\nclear on how this theory is related to the filter transform technique. In this\npaper, we show that kernel constructed by filter transform can also be\ninterpreted in the group representation theory. This interpretation help\ncomplete the puzzle of steerable CNN theory and provides a novel and simple\napproach to implement steerable convolution operators. Experiments are executed\non multiple datasets to verify the feasibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:32:34 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Bo", ""], ["Wang", "Qili", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2105.11645", "submitter": "Yaya Cheng", "authors": "Lianli Gao, Yaya Cheng, Qilong Zhang, Xing Xu and Jingkuan Song", "title": "Feature Space Targeted Attacks by Statistic Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By adding human-imperceptible perturbations to images, DNNs can be easily\nfooled. As one of the mainstream methods, feature space targeted attacks\nperturb images by modulating their intermediate feature maps, for the\ndiscrepancy between the intermediate source and target features is minimized.\nHowever, the current choice of pixel-wise Euclidean Distance to measure the\ndiscrepancy is questionable because it unreasonably imposes a\nspatial-consistency constraint on the source and target features. Intuitively,\nan image can be categorized as \"cat\" no matter the cat is on the left or right\nof the image. To address this issue, we propose to measure this discrepancy\nusing statistic alignment. Specifically, we design two novel approaches called\nPair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to\nmeasure similarities between feature maps by high-order statistics with\ntranslation invariance. Furthermore, we systematically analyze the layer-wise\ntransferability with varied difficulties to obtain highly reliable attacks.\nExtensive experiments verify the effectiveness of our proposed method, and it\noutperforms the state-of-the-art algorithms by a large margin. Our code is\npublicly available at https://github.com/yaya-cheng/PAA-GAA.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:46:39 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 02:27:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gao", "Lianli", ""], ["Cheng", "Yaya", ""], ["Zhang", "Qilong", ""], ["Xu", "Xing", ""], ["Song", "Jingkuan", ""]]}, {"id": "2105.11649", "submitter": "Bo Li", "authors": "Bo Li", "title": "On Enhancing Ground Surface Detection from Sparse Lidar Point Cloud", "comments": "IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ground surface detection in point cloud is widely used as a key module in\nautonomous driving systems. Different from previous approaches which are mostly\ndeveloped for lidars with high beam resolution, e.g. Velodyne HDL-64, this\npaper proposes ground detection techniques applicable to much sparser point\ncloud captured by lidars with low beam resolution, e.g. Velodyne VLP-16. The\napproach is based on the RANSAC scheme of plane fitting. Inlier verification\nfor plane hypotheses is enhanced by exploiting the point-wise tangent, which is\na local feature available to compute regardless of the density of lidar beams.\nGround surface which is not perfectly planar is fitted by multiple\n(specifically 4 in our implementation) disjoint plane regions. By assuming\nthese plane regions to be rectanglar and exploiting the integral image\ntechnique, our approach approximately finds the optimal region partition and\nplane hypotheses under the RANSAC scheme with real-time computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:58:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Bo", ""]]}, {"id": "2105.11651", "submitter": "Xiangtai Li", "authors": "Yanran Wu, Xiangtai Li, Chen Shi, Yunhai Tong, Yang Hua, Tao Song,\n  Ruhui Ma, Haibing Guan", "title": "Fast and Accurate Scene Parsing via Bi-direction Alignment Networks", "comments": "accepted by ICIP-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an effective method for fast and accurate scene\nparsing called Bidirectional Alignment Network (BiAlignNet). Previously, one\nrepresentative work BiSeNet~\\cite{bisenet} uses two different paths (Context\nPath and Spatial Path) to achieve balanced learning of semantics and details,\nrespectively. However, the relationship between the two paths is not well\nexplored. We argue that both paths can benefit each other in a complementary\nway. Motivated by this, we propose a novel network by aligning two-path\ninformation into each other through a learned flow field. To avoid the noise\nand semantic gaps, we introduce a Gated Flow Alignment Module to align both\nfeatures in a bidirectional way. Moreover, to make the Spatial Path learn more\ndetailed information, we present an edge-guided hard pixel mining loss to\nsupervise the aligned learning process. Our method achieves 80.1\\% and 78.5\\%\nmIoU in validation and test set of Cityscapes while running at 30 FPS with full\nresolution inputs. Code and models will be available at\n\\url{https://github.com/jojacola/BiAlignNet}.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:04:00 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wu", "Yanran", ""], ["Li", "Xiangtai", ""], ["Shi", "Chen", ""], ["Tong", "Yunhai", ""], ["Hua", "Yang", ""], ["Song", "Tao", ""], ["Ma", "Ruhui", ""], ["Guan", "Haibing", ""]]}, {"id": "2105.11654", "submitter": "Jianhao Ding", "authors": "Jianhao Ding, Zhaofei Yu, Yonghong Tian and Tiejun Huang", "title": "Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep\n  Spiking Neural Networks", "comments": "9 pages, 7 figures, 2 tables. To appear in the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural\nnetworks, have attracted great attentions from researchers and industry. The\nmost efficient way to train deep SNNs is through ANN-SNN conversion. However,\nthe conversion usually suffers from accuracy loss and long inference time,\nwhich impede the practical application of SNN. In this paper, we theoretically\nanalyze ANN-SNN conversion and derive sufficient conditions of the optimal\nconversion. To better correlate ANN-SNN and get greater accuracy, we propose\nRate Norm Layer to replace the ReLU activation function in source ANN training,\nenabling direct conversion from a trained ANN to an SNN. Moreover, we propose\nan optimal fit curve to quantify the fit between the activation value of source\nANN and the actual firing rate of target SNN. We show that the inference time\ncan be reduced by optimizing the upper bound of the fit curve in the revised\nANN to achieve fast inference. Our theory can explain the existing work on fast\nreasoning and get better results. The experimental results show that the\nproposed method achieves near loss less conversion with VGG-16,\nPreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster\nreasoning performance under 0.265x energy consumption of the typical method.\nThe code is available at\nhttps://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:15:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ding", "Jianhao", ""], ["Yu", "Zhaofei", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""]]}, {"id": "2105.11657", "submitter": "Xiangtai Li", "authors": "Chen Shi, Xiangtai Li, Yanran Wu, Yunhai Tong, Yi Xu", "title": "Dynamic Dual Sampling Module for Fine-Grained Semantic Segmentation", "comments": "accepted by ICIP-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representation of semantic context and local details is the essential issue\nfor building modern semantic segmentation models. However, the\ninterrelationship between semantic context and local details is not well\nexplored in previous works. In this paper, we propose a Dynamic Dual Sampling\nModule (DDSM) to conduct dynamic affinity modeling and propagate semantic\ncontext to local details, which yields a more discriminative representation.\nSpecifically, a dynamic sampling strategy is used to sparsely sample\nrepresentative pixels and channels in the higher layer, forming adaptive\ncompact support for each pixel and channel in the lower layer. The sampled\nfeatures with high semantics are aggregated according to the affinities and\nthen propagated to detailed lower-layer features, leading to a fine-grained\nsegmentation result with well-preserved boundaries. Experiment results on both\nCityscapes and Camvid datasets validate the effectiveness and efficiency of the\nproposed approach. Code and models will be available at\n\\url{x3https://github.com/Fantasticarl/DDSM}.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:25:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shi", "Chen", ""], ["Li", "Xiangtai", ""], ["Wu", "Yanran", ""], ["Tong", "Yunhai", ""], ["Xu", "Yi", ""]]}, {"id": "2105.11668", "submitter": "Xiangtai Li", "authors": "Hao He, Xiangtai Li, Kuiyuan Yang, Guangliang Cheng, Jianping Shi,\n  Yunhai Tong, Zhengjun Zha, Lubin Weng", "title": "BoundarySqueeze: Image Segmentation as Boundary Squeezing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel method for fine-grained high-quality image segmentation of\nboth objects and scenes. Inspired by dilation and erosion from morphological\nimage processing techniques, we treat the pixel level segmentation problems as\nsqueezing object boundary. From this perspective, we propose \\textbf{Boundary\nSqueeze} module: a novel and efficient module that squeezes the object boundary\nfrom both inner and outer directions which leads to precise mask\nrepresentation. To generate such squeezed representation, we propose a new\nbidirectionally flow-based warping process and design specific loss signals to\nsupervise the learning process. Boundary Squeeze Module can be easily applied\nto both instance and semantic segmentation tasks as a plug-and-play module by\nbuilding on top of existing models. We show that our simple yet effective\ndesign can lead to high qualitative results on several different datasets and\nwe also provide several different metrics on boundary to prove the\neffectiveness over previous work. Moreover, the proposed module is\nlight-weighted and thus has potential for practical usage. Our method yields\nlarge gains on COCO, Cityscapes, for both instance and semantic segmentation\nand outperforms previous state-of-the-art PointRend in both accuracy and speed\nunder the same setting. Code and model will be available.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:58:51 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["He", "Hao", ""], ["Li", "Xiangtai", ""], ["Yang", "Kuiyuan", ""], ["Cheng", "Guangliang", ""], ["Shi", "Jianping", ""], ["Tong", "Yunhai", ""], ["Zha", "Zhengjun", ""], ["Weng", "Lubin", ""]]}, {"id": "2105.11672", "submitter": "Weihong Lin", "authors": "Weihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong, Kai Hu, Qin Ren,\n  Qiang Huo", "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for\n  Key Information Extraction from Documents", "comments": "To be published at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:12:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Weihong", ""], ["Gao", "Qifang", ""], ["Sun", "Lei", ""], ["Zhong", "Zhuoyao", ""], ["Hu", "Kai", ""], ["Ren", "Qin", ""], ["Huo", "Qiang", ""]]}, {"id": "2105.11683", "submitter": "Yuan Xie", "authors": "Yanbo Wang, Shaohui Lin, Yanyun Qu, Haiyan Wu, Zhizhong Zhang, Yuan\n  Xie, Angela Yao", "title": "Towards Compact Single Image Super-Resolution via Contrastive\n  Self-distillation", "comments": "Accepted by IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) are highly successful for\nsuper-resolution (SR) but often require sophisticated architectures with heavy\nmemory cost and computational overhead, significantly restricts their practical\ndeployments on resource-limited devices. In this paper, we proposed a novel\ncontrastive self-distillation (CSD) framework to simultaneously compress and\naccelerate various off-the-shelf SR models. In particular, a channel-splitting\nsuper-resolution network can first be constructed from a target teacher network\nas a compact student network. Then, we propose a novel contrastive loss to\nimprove the quality of SR images and PSNR/SSIM via explicit knowledge transfer.\nExtensive experiments demonstrate that the proposed CSD scheme effectively\ncompresses and accelerates several standard SR models such as EDSR, RCAN and\nCARN. Code is available at https://github.com/Booooooooooo/CSD.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:44:11 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Yanbo", ""], ["Lin", "Shaohui", ""], ["Qu", "Yanyun", ""], ["Wu", "Haiyan", ""], ["Zhang", "Zhizhong", ""], ["Xie", "Yuan", ""], ["Yao", "Angela", ""]]}, {"id": "2105.11692", "submitter": "Liyue Shen", "authors": "Liyue Shen, Wei Zhao, Dante Capaldi, John Pauly, Lei Xing", "title": "A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D\n  Tomographic Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning affords enormous opportunities to augment the armamentarium of\nbiomedical imaging, albeit its design and implementation have potential flaws.\nFundamentally, most deep learning models are driven entirely by data without\nconsideration of any prior knowledge, which dramatically increases the\ncomplexity of neural networks and limits the application scope and model\ngeneralizability. Here we establish a geometry-informed deep learning framework\nfor ultra-sparse 3D tomographic image reconstruction. We introduce a novel\nmechanism for integrating geometric priors of the imaging system. We\ndemonstrate that the seamless inclusion of known priors is essential to enhance\nthe performance of 3D volumetric computed tomography imaging with ultra-sparse\nsampling. The study opens new avenues for data-driven biomedical imaging and\npromises to provide substantially improved imaging tools for various clinical\nimaging and image-guided interventions.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:20:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shen", "Liyue", ""], ["Zhao", "Wei", ""], ["Capaldi", "Dante", ""], ["Pauly", "John", ""], ["Xing", "Lei", ""]]}, {"id": "2105.11694", "submitter": "Jihao Liu", "authors": "Jihao Liu and Ming Zhang and Yangting Sun and Boxiao Liu and Guanglu\n  Song and Yu Liu and Hongsheng Li", "title": "FNAS: Uncertainty-Aware Fast Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL)-based neural architecture search (NAS) generally\nguarantees better convergence yet suffers from the requirement of huge\ncomputational resources compared with gradient-based approaches, due to the\nrollout bottleneck -- exhaustive training for each sampled generation on proxy\ntasks. In this paper, we propose a general pipeline to accelerate the\nconvergence of the rollout process as well as the RL process in NAS. It is\nmotivated by the interesting observation that both the architecture and the\nparameter knowledge can be transferred between different experiments and even\ndifferent tasks. We first introduce an uncertainty-aware critic (value\nfunction) in Proximal Policy Optimization (PPO) to utilize the architecture\nknowledge in previous experiments, which stabilizes the training process and\nreduces the searching time by 4 times. Further, an architecture knowledge pool\ntogether with a block similarity function is proposed to utilize parameter\nknowledge and reduces the searching time by 2 times. It is the first to\nintroduce block-level weight sharing in RLbased NAS. The block similarity\nfunction guarantees a 100% hitting ratio with strict fairness. Besides, we show\nthat a simply designed off-policy correction factor used in \"replay buffer\" in\nRL optimization can further reduce half of the searching time. Experiments on\nthe Mobile Neural Architecture Search (MNAS) search space show the proposed\nFast Neural Architecture Search (FNAS) accelerates standard RL-based NAS\nprocess by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x\nhour for MNAS), and guarantees better performance on various vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:32:52 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:36:28 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 07:53:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liu", "Jihao", ""], ["Zhang", "Ming", ""], ["Sun", "Yangting", ""], ["Liu", "Boxiao", ""], ["Song", "Guanglu", ""], ["Liu", "Yu", ""], ["Li", "Hongsheng", ""]]}, {"id": "2105.11705", "submitter": "Divam Gupta", "authors": "Divam Gupta, Wei Pu, Trenton Tabor, Jeff Schneider", "title": "SBEVNet: End-to-End Deep Stereo Layout Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate layout estimation is crucial for planning and navigation in robotics\napplications, such as self-driving. In this paper, we introduce the Stereo\nBird's Eye ViewNetwork (SBEVNet), a novel supervised end-to-end framework for\nestimation of bird's eye view layout from a pair of stereo images. Although our\nnetwork reuses some of the building blocks from the state-of-the-art deep\nlearning networks for disparity estimation, we show that explicit depth\nestimation is neither sufficient nor necessary. Instead, the learning of a good\ninternal bird's eye view feature representation is effective for layout\nestimation. Specifically, we first generate a disparity feature volume using\nthe features of the stereo images and then project it to the bird's eye view\ncoordinates. This gives us coarse-grained information about the scene\nstructure. We also apply inverse perspective mapping (IPM) to map the input\nimages and their features to the bird's eye view. This gives us fine-grained\ntexture information. Concatenating IPM features with the projected feature\nvolume creates a rich bird's eye view representation which is useful for\nspatial reasoning. We use this representation to estimate the BEV semantic map.\nAdditionally, we show that using the IPM features as a supervisory signal for\nstereo features can give an improvement in performance. We demonstrate our\napproach on two datasets:the KITTI dataset and a synthetically generated\ndataset from the CARLA simulator. For both of these datasets, we establish\nstate-of-the-art performance compared to baseline techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:10:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gupta", "Divam", ""], ["Pu", "Wei", ""], ["Tabor", "Trenton", ""], ["Schneider", "Jeff", ""]]}, {"id": "2105.11711", "submitter": "Hyungmin Roh", "authors": "Hyungmin Roh and Myungjoo Kang", "title": "High-Frequency aware Perceptual Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a novel deep neural network suitable for\nmulti-scale analysis and propose efficient model-agnostic methods that help the\nnetwork extract information from high-frequency domains to reconstruct clearer\nimages. Our model can be applied to multi-scale image enhancement problems\nincluding denoising, deblurring and single image super-resolution. Experiments\non SIDD, Flickr2K, DIV2K, and REDS datasets show that our method achieves\nstate-of-the-art performance on each task. Furthermore, we show that our model\ncan overcome the over-smoothing problem commonly observed in existing\nPSNR-oriented methods and generate more natural high-resolution images by\napplying adversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:33:14 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Roh", "Hyungmin", ""], ["Kang", "Myungjoo", ""]]}, {"id": "2105.11715", "submitter": "Inyong Koo", "authors": "Inyong Koo, Minki Jeong, Changick Kim", "title": "Improving Few-shot Learning with Weakly-supervised Object Localization", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning often involves metric learning-based classifiers, which\npredict the image label by comparing the distance between the extracted feature\nvector and class representations. However, applying global pooling in the\nbackend of the feature extractor may not produce an embedding that correctly\nfocuses on the class object. In this work, we propose a novel framework that\ngenerates class representations by extracting features from class-relevant\nregions of the images. Given only a few exemplary images with image-level\nlabels, our framework first localizes the class objects by spatially\ndecomposing the similarity between the images and their class prototypes. Then,\nenhanced class representations are achieved from the localization results. We\nalso propose a loss function to enhance distinctions of the refined features.\nOur method outperforms the baseline few-shot model in miniImageNet and\ntieredImageNet benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:39:32 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Koo", "Inyong", ""], ["Jeong", "Minki", ""], ["Kim", "Changick", ""]]}, {"id": "2105.11722", "submitter": "Guoqing Zhang", "authors": "Guoqing Zhang, Yu Ge, Zhicheng Dong, Hao Wang, Yuhui Zheng, Shengyong\n  Chen", "title": "Deep High-Resolution Representation Learning for Cross-Resolution Person\n  Re-identification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) tackles the problem of matching person\nimages with the same identity from different cameras. In practical\napplications, due to the differences in camera performance and distance between\ncameras and persons of interest, captured person images usually have various\nresolutions. We name this problem as Cross-Resolution Person Re-identification\nwhich brings a great challenge for matching correctly. In this paper, we\npropose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the\nabove problem. Specifically, in order to restore the resolution of\nlow-resolution images and make reasonable use of different channel information\nof feature maps, we introduce and innovate VDSR module with channel attention\n(CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel\nrepresentation head to extract discriminating features, named as HRNet-ReID. In\naddition, a pseudo-siamese framework is constructed to reduce the difference of\nfeature distributions between low-resolution images and high-resolution images.\nThe experimental results on five cross-resolution person datasets verify the\neffectiveness of our proposed approach. Compared with the state-of-the-art\nmethods, our proposed PS-HRNet improves 3.4\\%, 6.2\\%, 2.5\\%,1.1\\% and 4.2\\% at\nRank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR\ndatasets, respectively. Our code is available at\n\\url{https://github.com/zhguoqing}.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:45:38 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhang", "Guoqing", ""], ["Ge", "Yu", ""], ["Dong", "Zhicheng", ""], ["Wang", "Hao", ""], ["Zheng", "Yuhui", ""], ["Chen", "Shengyong", ""]]}, {"id": "2105.11731", "submitter": "Meng-Jiun Chiou", "authors": "Meng-Jiun Chiou, Chun-Yu Liao, Li-Wei Wang, Roger Zimmermann and\n  Jiashi Feng", "title": "ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction\n  Detection in Videos", "comments": "Accepted at ACM ICMR'21 Workshop on Intelligent Cross-Data Analysis\n  and Retrieval. The dataset and source code are available at\n  https://github.com/coldmanck/VidHOI", "journal-ref": null, "doi": "10.1145/3463944.3469097", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting human-object interactions (HOI) is an important step toward a\ncomprehensive visual understanding of machines. While detecting non-temporal\nHOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely\neven for humans to guess temporal-related HOIs (e.g., opening/closing a door)\nfrom a single video frame, where the neighboring frames play an essential role.\nHowever, conventional HOI methods operating on only static images have been\nused to predict temporal-related interactions, which is essentially guessing\nwithout temporal contexts and may lead to sub-optimal performance. In this\npaper, we bridge this gap by detecting video-based HOIs with explicit temporal\ninformation. We first show that a naive temporal-aware variant of a common\naction detection baseline does not work on video-based HOIs due to a\nfeature-inconsistency issue. We then propose a simple yet effective\narchitecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal\ninformation such as human and object trajectories, correctly-localized visual\nfeatures, and spatial-temporal masking pose features. We construct a new video\nHOI benchmark dubbed VidHOI where our proposed approach serves as a solid\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:54:35 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 09:10:17 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chiou", "Meng-Jiun", ""], ["Liao", "Chun-Yu", ""], ["Wang", "Li-Wei", ""], ["Zimmermann", "Roger", ""], ["Feng", "Jiashi", ""]]}, {"id": "2105.11748", "submitter": "Weiyi Xie", "authors": "Weiyi Xie, Colin Jacobs, Bram van Ginneken", "title": "Dense Regression Activation Maps For Lesion Segmentation in CT scans of\n  COVID-19 patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic lesion segmentation on thoracic CT enables rapid quantitative\nanalysis of lung involvement in COVID- 19 infections. Obtaining voxel-level\nannotations for training segmentation networks is prohibitively expensive.\nTherefore we propose a weakly-supervised segmentation method based on dense\nregression activation maps (dRAM). Most advanced weakly supervised segmentation\napproaches exploit class activation maps (CAMs) to localize objects generated\nfrom high-level semantic features at a coarse resolution. As a result, CAMs\nprovide coarse outlines that do not align precisely with the object\nsegmentations. Instead, we exploit dense features from a segmentation network\nto compute dense regression activation maps (dRAMs) for preserving local\ndetails. During training, dRAMs are pooled lobe-wise to regress the per-lobe\nlesion percentage. In such a way, the network achieves additional information\nregarding the lesion quantification in comparison with the classification\napproach. Furthermore, we refine dRAMs based on an attention module and dense\nconditional random field trained together with the main regression task. The\nrefined dRAMs are served as the pseudo labels for training a final segmentation\nnetwork. When evaluated on 69 CT scans, our method substantially improves the\nintersection over union from 0.335 in the CAM-based weakly supervised\nsegmentation method to 0.495.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:29:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Xie", "Weiyi", ""], ["Jacobs", "Colin", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2105.11765", "submitter": "Ann-Katrin Thebille", "authors": "Ann-Katrin Thebille and Esther Dietrich and Martin Klaus and Lukas\n  Gernhold and Maximilian Lennartz and Christoph Kuppe and Rafael Kramann and\n  Tobias B. Huber and Guido Sauter and Victor G. Puelles and Marina Zimmermann\n  and Stefan Bonn", "title": "Deep learning-based bias transfer for overcoming laboratory differences\n  of microscopic images", "comments": "Accepted as a regular conference paper at MIUA 2021", "journal-ref": null, "doi": "10.1007/978-3-030-80432-9_25", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automated analysis of medical images is currently limited by technical\nand biological noise and bias. The same source tissue can be represented by\nvastly different images if the image acquisition or processing protocols vary.\nFor an image analysis pipeline, it is crucial to compensate such biases to\navoid misinterpretations. Here, we evaluate, compare, and improve existing\ngenerative model architectures to overcome domain shifts for immunofluorescence\n(IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine\nthe performance of the generative models, the original and transformed images\nwere segmented or classified by deep neural networks that were trained only on\nimages of the target bias. In the scope of our analysis, U-Net cycleGANs\ntrained with an additional identity and an MS-SSIM-based loss and Fixed-Point\nGANs trained with an additional structure loss led to the best results for the\nIF and H&E stained samples, respectively. Adapting the bias of the samples\nsignificantly improved the pixel-level segmentation for human kidney glomeruli\nand podocytes and improved the classification accuracy for human prostate\nbiopsies by up to 14%.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:02:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Thebille", "Ann-Katrin", ""], ["Dietrich", "Esther", ""], ["Klaus", "Martin", ""], ["Gernhold", "Lukas", ""], ["Lennartz", "Maximilian", ""], ["Kuppe", "Christoph", ""], ["Kramann", "Rafael", ""], ["Huber", "Tobias B.", ""], ["Sauter", "Guido", ""], ["Puelles", "Victor G.", ""], ["Zimmermann", "Marina", ""], ["Bonn", "Stefan", ""]]}, {"id": "2105.11789", "submitter": "Bin Sun", "authors": "Bin Sun, Dehui Kong, Shaofan Wang, Jinghua Li, Baocai Yin, Xiaonan Luo", "title": "GAN for Vision, KG for Relation: a Two-stage Deep Network for Zero-shot\n  Action Recognition", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot action recognition can recognize samples of unseen classes that are\nunavailable in training by exploring common latent semantic representation in\nsamples. However, most methods neglected the connotative relation and\nextensional relation between the action classes, which leads to the poor\ngeneralization ability of the zero-shot learning. Furthermore, the learned\nclassifier incline to predict the samples of seen class, which leads to poor\nclassification performance. To solve the above problems, we propose a two-stage\ndeep neural network for zero-shot action recognition, which consists of a\nfeature generation sub-network serving as the sampling stage and a graph\nattention sub-network serving as the classification stage. In the sampling\nstage, we utilize a generative adversarial networks (GAN) trained by action\nfeatures and word vectors of seen classes to synthesize the action features of\nunseen classes, which can balance the training sample data of seen classes and\nunseen classes. In the classification stage, we construct a knowledge graph\n(KG) based on the relationship between word vectors of action classes and\nrelated objects, and propose a graph convolution network (GCN) based on\nattention mechanism, which dynamically updates the relationship between action\nclasses and objects, and enhances the generalization ability of zero-shot\nlearning. In both stages, we all use word vectors as bridges for feature\ngeneration and classifier generalization from seen classes to unseen classes.\nWe compare our method with state-of-the-art methods on UCF101 and HMDB51\ndatasets. Experimental results show that our proposed method improves the\nclassification performance of the trained classifier and achieves higher\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:34:42 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sun", "Bin", ""], ["Kong", "Dehui", ""], ["Wang", "Shaofan", ""], ["Li", "Jinghua", ""], ["Yin", "Baocai", ""], ["Luo", "Xiaonan", ""]]}, {"id": "2105.11804", "submitter": "Etienne Bennequin", "authors": "Etienne Bennequin, Victor Bouvier, Myriam Tami, Antoine Toubhans,\n  C\\'eline Hudelot", "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of\n  Support-Query Shift", "comments": "Preprint. Under review at ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:10:09 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bennequin", "Etienne", ""], ["Bouvier", "Victor", ""], ["Tami", "Myriam", ""], ["Toubhans", "Antoine", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "2105.11809", "submitter": "Mehmet Turkan", "authors": "Diclehan Karakaya, Oguzhan Ulucan, Mehmet Turkan", "title": "PAS-MEF: Multi-exposure image fusion based on principal component\n  analysis, adaptive well-exposedness and saliency map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High dynamic range (HDR) imaging enables to immortalize natural scenes\nsimilar to the way that they are perceived by human observers. With regular low\ndynamic range (LDR) capture/display devices, significant details may not be\npreserved in images due to the huge dynamic range of natural scenes. To\nminimize the information loss and produce high quality HDR-like images for LDR\nscreens, this study proposes an efficient multi-exposure fusion (MEF) approach\nwith a simple yet effective weight extraction method relying on principal\ncomponent analysis, adaptive well-exposedness and saliency maps. These weight\nmaps are later refined through a guided filter and the fusion is carried out by\nemploying a pyramidal decomposition. Experimental comparisons with existing\ntechniques demonstrate that the proposed method produces very strong\nstatistical and visual results.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:22:43 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Karakaya", "Diclehan", ""], ["Ulucan", "Oguzhan", ""], ["Turkan", "Mehmet", ""]]}, {"id": "2105.11828", "submitter": "Dominik Seu{\\ss}", "authors": "Dominik Seu{\\ss}", "title": "Bridging the Gap Between Explainable AI and Uncertainty Quantification\n  to Enhance Trustability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the tremendous advances of deep learning and other AI methods, more\nattention is flowing into other properties of modern approaches, such as\ninterpretability, fairness, etc. combined in frameworks like Responsible AI.\nTwo research directions, namely Explainable AI and Uncertainty Quantification\nare becoming more and more important, but have been so far never combined and\njointly explored. In this paper, I show how both research areas provide\npotential for combination, why more research should be done in this direction\nand how this would lead to an increase in trustability in AI systems.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:53:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Seu\u00df", "Dominik", ""]]}, {"id": "2105.11844", "submitter": "Francisco P\\'erez-Hern\\'andez", "authors": "P\\'erez-Hern\\'andez Francisco, Rodr\\'iguez-Ortega Jos\\'e, Benhammou\n  Yassir, Herrera Francisco, Tabik Siham", "title": "Small and large scale critical infrastructures detection based on deep\n  learning using high resolution orthogonal images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of critical infrastructures is of high importance in several\nfields such as security, anomaly detection, land use planning and land use\nchange detection. However, critical infrastructures detection in aerial and\nsatellite images is still a challenge as each one has completely different size\nand requires different spacial resolution to be identified correctly.\nHeretofore, there are no special datasets for training critical infrastructures\ndetectors. This paper presents a smart dataset as well as a\nresolution-independent critical infrastructure detection system. In particular,\nguided by the performance of the detection model, we built a dataset organized\ninto two scales, small and large scale, and designed a two-stage deep learning\ndetection of different scale critical infrastructures (DetDSCI) methodology in\northo-images. DetDSCI methodology first determines the input image zoom level\nusing a classification model, then analyses the input image with the\nappropriate scale detection model. Our experiments show that DetDSCI\nmethodology achieves up to 37,53% F1 improvement with respect to the baseline\ndetector.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:38:15 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Francisco", "P\u00e9rez-Hern\u00e1ndez", ""], ["Jos\u00e9", "Rodr\u00edguez-Ortega", ""], ["Yassir", "Benhammou", ""], ["Francisco", "Herrera", ""], ["Siham", "Tabik", ""]]}, {"id": "2105.11848", "submitter": "Tao Luo", "authors": "Tao Luo, Wai Teng Tang, Matthew Kay Fei Lee, Chuping Qu, Weng-Fai\n  Wong, Rick Goh", "title": "DTNN: Energy-efficient Inference with Dendrite Tree Inspired Neural\n  Networks for Edge Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have achieved remarkable success in computer\nvision (CV). However, training and inference of DNN models are both memory and\ncomputation intensive, incurring significant overhead in terms of energy\nconsumption and silicon area. In particular, inference is much more\ncost-sensitive than training because training can be done offline with powerful\nplatforms, while inference may have to be done on battery powered devices with\nconstrained form factors, especially for mobile or edge vision applications. In\norder to accelerate DNN inference, model quantization was proposed. However\nprevious works only focus on the quantization rate without considering the\nefficiency of operations. In this paper, we propose Dendrite-Tree based Neural\nNetwork (DTNN) for energy-efficient inference with table lookup operations\nenabled by activation quantization. In DTNN both costly weight access and\narithmetic computations are eliminated for inference. We conducted experiments\non various kinds of DNN models such as LeNet-5, MobileNet, VGG, and ResNet with\ndifferent datasets, including MNIST, Cifar10/Cifar100, SVHN, and ImageNet. DTNN\nachieved significant energy saving (19.4X and 64.9X improvement on ResNet-18\nand VGG-11 with ImageNet, respectively) with negligible loss of accuracy. To\nfurther validate the effectiveness of DTNN and compare with state-of-the-art\nlow energy implementation for edge vision, we design and implement DTNN based\nMLP image classifiers using off-the-shelf FPGAs. The results show that DTNN on\nthe FPGA, with higher accuracy, could achieve orders of magnitude better energy\nconsumption and latency compared with the state-of-the-art low energy\napproaches reported that use ASIC chips.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:44:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Luo", "Tao", ""], ["Tang", "Wai Teng", ""], ["Lee", "Matthew Kay Fei", ""], ["Qu", "Chuping", ""], ["Wong", "Weng-Fai", ""], ["Goh", "Rick", ""]]}, {"id": "2105.11852", "submitter": "Cheikh Brahim El Vaigh", "authors": "Cheikh Brahim El Vaigh, Noa Garcia, Benjamin Renoust, Chenhui Chu,\n  Yuta Nakashima and Hajime Nagahara", "title": "GCNBoost: Artwork Classification by Label Propagation through a\n  Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of digitization of cultural documents offers large-scale contents,\nopening the road for development of AI systems in order to preserve, search,\nand deliver cultural heritage. To organize such cultural content also means to\nclassify them, a task that is very familiar to modern computer science.\nContextual information is often the key to structure such real world data, and\nwe propose to use it in form of a knowledge graph. Such a knowledge graph,\ncombined with content analysis, enhances the notion of proximity between\nartworks so it improves the performances in classification tasks. In this\npaper, we propose a novel use of a knowledge graph, that is constructed on\nannotated data and pseudo-labeled data. With label propagation, we boost\nartwork classification by training a model using a graph convolutional network,\nrelying on the relationships between entities of the knowledge graph. Following\na transductive learning framework, our experiments show that relying on a\nknowledge graph modeling the relations between labeled data and unlabeled data\nallows to achieve state-of-the-art results on multiple classification tasks on\na dataset of paintings, and on a dataset of Buddha statues. Additionally, we\nshow state-of-the-art results for the difficult case of dealing with unbalanced\ndata, with the limitation of disregarding classes with extremely low degrees in\nthe knowledge graph.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:50:05 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Vaigh", "Cheikh Brahim El", ""], ["Garcia", "Noa", ""], ["Renoust", "Benjamin", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""]]}, {"id": "2105.11857", "submitter": "Kaaviya Velumani", "authors": "Kaaviya Velumani, Raul Lopez-Lozano, Simon Madec, Wei Guo, Joss\n  Gillet, Alexis Comar, Frederic Baret", "title": "Estimates of maize plant density from UAV RGB images using Faster-RCNN\n  detection model: impact of the spatial resolution", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Early-stage plant density is an essential trait that determines the fate of a\ngenotype under given environmental conditions and management practices. The use\nof RGB images taken from UAVs may replace traditional visual counting in fields\nwith improved throughput, accuracy and access to plant localization. However,\nhigh-resolution (HR) images are required to detect small plants present at\nearly stages. This study explores the impact of image ground sampling distance\n(GSD) on the performances of maize plant detection at 3-5 leaves stage using\nFaster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used\nfor model training. Two additional sites with images acquired both at high and\nlow (GSD=0.6cm) resolution were used for model evaluation. Results show that\nFaster-RCNN achieved very good plant detection and counting (rRMSE=0.08)\nperformances when native HR images are used both for training and validation.\nSimilarly, good performances were observed (rRMSE=0.11) when the model is\ntrained over synthetic low-resolution (LR) images obtained by down-sampling the\nnative training HR images, and applied to the synthetic LR validation images.\nConversely, poor performances are obtained when the model is trained on a given\nspatial resolution and applied to another spatial resolution. Training on a mix\nof HR and LR images allows to get very good performances on the native HR\n(rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low\nperformances are still observed over the native LR images (rRMSE=0.48), mainly\ndue to the poor quality of the native LR images. Finally, an advanced\nsuper-resolution method based on GAN (generative adversarial network) that\nintroduces additional textural information derived from the native HR images\nwas applied to the native LR validation images. Results show some significant\nimprovement (rRMSE=0.22) compared to bicubic up-sampling approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:54:51 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Velumani", "Kaaviya", ""], ["Lopez-Lozano", "Raul", ""], ["Madec", "Simon", ""], ["Guo", "Wei", ""], ["Gillet", "Joss", ""], ["Comar", "Alexis", ""], ["Baret", "Frederic", ""]]}, {"id": "2105.11863", "submitter": "Manvel Avetisian", "authors": "Manvel Avetisian, Ilya Burenko, Konstantin Egorov, Vladimir Kokh,\n  Aleksandr Nesterov, Aleksandr Nikolaev, Alexander Ponomarchuk, Elena\n  Sokolova, Alex Tuzhilin, Dmitry Umerenkov", "title": "CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19\n  Patients Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of chest CT scans can be used in detecting parts of lungs that are\naffected by infectious diseases such as COVID-19.Determining the volume of\nlungs affected by lesions is essential for formulating treatment\nrecommendations and prioritizingpatients by severity of the disease. In this\npaper we adopted an approach based on using an ensemble of deep\nconvolutionalneural networks for segmentation of slices of lung CT scans. Using\nour models we are able to segment the lesions, evaluatepatients dynamics,\nestimate relative volume of lungs affected by lesions and evaluate the lung\ndamage stage. Our modelswere trained on data from different medical centers. We\ncompared predictions of our models with those of six experiencedradiologists\nand our segmentation model outperformed most of them. On the task of\nclassification of disease severity, ourmodel outperformed all the radiologists.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:06:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Avetisian", "Manvel", ""], ["Burenko", "Ilya", ""], ["Egorov", "Konstantin", ""], ["Kokh", "Vladimir", ""], ["Nesterov", "Aleksandr", ""], ["Nikolaev", "Aleksandr", ""], ["Ponomarchuk", "Alexander", ""], ["Sokolova", "Elena", ""], ["Tuzhilin", "Alex", ""], ["Umerenkov", "Dmitry", ""]]}, {"id": "2105.11871", "submitter": "Yawen Duan", "authors": "Yawen Duan, Xin Chen, Hang Xu, Zewei Chen, Xiaodan Liang, Tong Zhang,\n  Zhenguo Li", "title": "TransNAS-Bench-101: Improving Transferability and Generalizability of\n  Cross-Task Neural Architecture Search", "comments": "Published at CVPR 2021. 8 pages main paper, 13 pages in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent breakthroughs of Neural Architecture Search (NAS) extend the field's\nresearch scope towards a broader range of vision tasks and more diversified\nsearch spaces. While existing NAS methods mostly design architectures on a\nsingle task, algorithms that look beyond single-task search are surging to\npursue a more efficient and universal solution across various tasks. Many of\nthem leverage transfer learning and seek to preserve, reuse, and refine network\ndesign knowledge to achieve higher efficiency in future tasks. However, the\nenormous computational cost and experiment complexity of cross-task NAS are\nimposing barriers for valuable research in this direction. Existing NAS\nbenchmarks all focus on one type of vision task, i.e., classification. In this\nwork, we propose TransNAS-Bench-101, a benchmark dataset containing network\nperformance across seven tasks, covering classification, regression,\npixel-level prediction, and self-supervised tasks. This diversity provides\nopportunities to transfer NAS methods among tasks and allows for more complex\ntransfer schemes to evolve. We explore two fundamentally different types of\nsearch space: cell-level search space and macro-level search space. With 7,352\nbackbones evaluated on seven tasks, 51,464 trained models with detailed\ntraining information are provided. With TransNAS-Bench-101, we hope to\nencourage the advent of exceptional NAS algorithms that raise cross-task search\nefficiency and generalizability to the next level. Our dataset file will be\navailable at Mindspore, VEGA.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:15:21 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Duan", "Yawen", ""], ["Chen", "Xin", ""], ["Xu", "Hang", ""], ["Chen", "Zewei", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Tong", ""], ["Li", "Zhenguo", ""]]}, {"id": "2105.11874", "submitter": "Wentao Chen", "authors": "Wentao Chen, Chenyang Si, Wei Wang, Liang Wang, Zilei Wang, Tieniu Tan", "title": "Few-Shot Learning with Part Discovery and Augmentation from Unlabeled\n  Images", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a challenging task since only few instances are given\nfor recognizing an unseen class. One way to alleviate this problem is to\nacquire a strong inductive bias via meta-learning on similar tasks. In this\npaper, we show that such inductive bias can be learned from a flat collection\nof unlabeled images, and instantiated as transferable representations among\nseen and unseen classes. Specifically, we propose a novel part-based\nself-supervised representation learning scheme to learn transferable\nrepresentations by maximizing the similarity of an image to its discriminative\npart. To mitigate the overfitting in few-shot classification caused by data\nscarcity, we further propose a part augmentation strategy by retrieving extra\nimages from a base dataset. We conduct systematic studies on miniImageNet and\ntieredImageNet benchmarks. Remarkably, our method yields impressive results,\noutperforming the previous best unsupervised methods by 7.74% and 9.24% under\n5-way 1-shot and 5-way 5-shot settings, which are comparable with\nstate-of-the-art supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:22:11 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Chen", "Wentao", ""], ["Si", "Chenyang", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Wang", "Zilei", ""], ["Tan", "Tieniu", ""]]}, {"id": "2105.11879", "submitter": "Marcin Namysl", "authors": "Marcin Namysl, Alexander M. Esser, Sven Behnke, Joachim K\\\"ohler", "title": "Tab.IAIS: Flexible Table Recognition and Semantic Interpretation System", "comments": "14 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table extraction is an important but still unsolved problem. In this paper,\nwe introduce a flexible end-to-end table extraction system. We develop two\nrule-based algorithms that perform the complete table recognition process and\nsupport the most frequent table formats found in the scientific literature.\nMoreover, to incorporate the extraction of semantic information into the table\nrecognition process, we develop a graph-based table interpretation method. We\nconduct extensive experiments on the challenging table recognition benchmarks\nICDAR 2013 and ICDAR 2019. Our table recognition approach achieves results\ncompetitive with state-of-the-art approaches. Moreover, our complete\ninformation extraction system exhibited a high F1 score of 0.7380 proving the\nutility of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:31:02 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Namysl", "Marcin", ""], ["Esser", "Alexander M.", ""], ["Behnke", "Sven", ""], ["K\u00f6hler", "Joachim", ""]]}, {"id": "2105.11925", "submitter": "Jos\\'e Mennesson", "authors": "Sami Barchid, Jos\\'e Mennesson, Chaabane Dj\\'eraba", "title": "Review on Indoor RGB-D Semantic Segmentation with Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research works focus on leveraging the complementary geometric\ninformation of indoor depth sensors in vision tasks performed by deep\nconvolutional neural networks, notably semantic segmentation. These works deal\nwith a specific vision task known as \"RGB-D Indoor Semantic Segmentation\". The\nchallenges and resulting solutions of this task differ from its standard RGB\ncounterpart. This results in a new active research topic. The objective of this\npaper is to introduce the field of Deep Convolutional Neural Networks for RGB-D\nIndoor Semantic Segmentation. This review presents the most popular public\ndatasets, proposes a categorization of the strategies employed by recent\ncontributions, evaluates the performance of the current state-of-the-art, and\ndiscusses the remaining challenges and promising directions for future works.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:30:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Barchid", "Sami", ""], ["Mennesson", "Jos\u00e9", ""], ["Dj\u00e9raba", "Chaabane", ""]]}, {"id": "2105.11927", "submitter": "Chong Peng", "authors": "Yang Liu, Qian Zhang, Yongyong Chen, Qiang Cheng and Chong Peng", "title": "Hyperspectral Image Denoising with Log-Based Robust PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a challenging task to remove heavy and mixed types of noise from\nHyperspectral images (HSIs). In this paper, we propose a novel nonconvex\napproach to RPCA for HSI denoising, which adopts the log-determinant rank\napproximation and a novel $\\ell_{2,\\log}$ norm, to restrict the low-rank or\ncolumn-wise sparse properties for the component matrices, respectively.For the\n$\\ell_{2,\\log}$-regularized shrinkage problem, we develop an efficient,\nclosed-form solution, which is named $\\ell_{2,\\log}$-shrinkage operator, which\ncan be generally used in other problems. Extensive experiments on both\nsimulated and real HSIs demonstrate the effectiveness of the proposed method in\ndenoising HSIs.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:32:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Liu", "Yang", ""], ["Zhang", "Qian", ""], ["Chen", "Yongyong", ""], ["Cheng", "Qiang", ""], ["Peng", "Chong", ""]]}, {"id": "2105.11941", "submitter": "Jingwen Fu", "authors": "Jingwen Fu, Xiaoyi Zhang, Yuwang Wang, Wenjun Zeng, Sam Yang and\n  Grayson Hilliard", "title": "Understanding Mobile GUI: from Pixel-Words to Screen-Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of mobile phones makes mobile GUI understanding an important\ntask. Most previous works in this domain require human-created metadata of\nscreens (e.g. View Hierarchy) during inference, which unfortunately is often\nnot available or reliable enough for GUI understanding. Inspired by the\nimpressive success of Transformers in NLP tasks, targeting for purely\nvision-based GUI understanding, we extend the concepts of Words/Sentence to\nPixel-Words/Screen-Sentence, and propose a mobile GUI understanding\narchitecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the\nindividual Words, we define the Pixel-Words as atomic visual components (text\nand graphic components), which are visually consistent and semantically clear\nacross screenshots of a large variety of design styles. The Pixel-Words\nextracted from a screenshot are aggregated into Screen-Sentence with a Screen\nTransformer proposed to model their relations. Since the Pixel-Words are\ndefined as atomic visual components, the ambiguity between their visual\nappearance and semantics is dramatically reduced. We are able to make use of\nmetadata available in training data to auto-generate high-quality annotations\nfor Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words\nannotations is built based on the public RICO dataset, which will be released\nto help to address the lack of high-quality training data in this area. We\ntrain a detector to extract Pixel-Words from screenshots on this dataset and\nachieve metadata-free GUI understanding during inference. We conduct\nexperiments and show that Pixel-Words can be well extracted on RICO-PW and well\ngeneralized to a new dataset, P2S-UI, collected by ourselves. The effectiveness\nof PW2SS is further verified in the GUI understanding tasks including relation\nprediction, clickability prediction, screen retrieval, and app type\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:45:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fu", "Jingwen", ""], ["Zhang", "Xiaoyi", ""], ["Wang", "Yuwang", ""], ["Zeng", "Wenjun", ""], ["Yang", "Sam", ""], ["Hilliard", "Grayson", ""]]}, {"id": "2105.11953", "submitter": "Peter Gloor", "authors": "Luis A. Corujo, Peter A. Gloor, Emily Kieson", "title": "Emotion Recognition in Horses with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating intelligent systems capable of recognizing emotions is a difficult\ntask, especially when looking at emotions in animals. This paper describes the\nprocess of designing a \"proof of concept\" system to recognize emotions in\nhorses. This system is formed by two elements, a detector and a model. The\ndetector is a faster region-based convolutional neural network that detects\nhorses in an image. The second one, the model, is a convolutional neural\nnetwork that predicts the emotion of those horses. These two models were\ntrained with multiple images of horses until they achieved high accuracy in\ntheir tasks, creating therefore the desired system. 400 images of horses were\nused to train both the detector and the model while 80 were used to validate\nthe system. Once the two components were validated they were combined into a\ntestable system that would detect equine emotions based on established\nbehavioral ethograms indicating emotional affect through head, neck, ear,\nmuzzle, and eye position. The system showed an accuracy of between 69% and 74%\non the validation set, demonstrating that it is possible to predict emotions in\nanimals using autonomous intelligent systems. It is a first \"proof of concept\"\napproach that can be enhanced in many ways. Such a system has multiple\napplications including further studies in the growing field of animal emotions\nas well as in the veterinary field to determine the physical welfare of horses\nor other livestock.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:04:43 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Corujo", "Luis A.", ""], ["Gloor", "Peter A.", ""], ["Kieson", "Emily", ""]]}, {"id": "2105.11956", "submitter": "Vinayak Killedar", "authors": "Vinayak Killedar, Praveen Kumar Pokala, Chandra Sekhar Seelamantula", "title": "Learning Generative Prior with Latent Space Sparsity Constraints", "comments": "Submitted to UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of compressed sensing using a deep generative prior\nmodel and consider both linear and learned nonlinear sensing mechanisms, where\nthe nonlinear one involves either a fully connected neural network or a\nconvolutional neural network. Recently, it has been argued that the\ndistribution of natural images do not lie in a single manifold but rather lie\nin a union of several submanifolds. We propose a sparsity-driven latent space\nsampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm\nto enforce sparsity in the latent space. SDLSS allows the range-space of the\ngenerator to be considered as a union-of-submanifolds. We also derive the\nsample complexity bounds within the SDLSS framework for the linear measurement\nmodel. The results demonstrate that for a higher degree of compression, the\nSDLSS method is more efficient than the state-of-the-art method. We first\nconsider a comparison between linear and nonlinear sensing mechanisms on\nFashion-MNIST dataset and show that the learned nonlinear version is superior\nto the linear one. Subsequent comparisons with the deep compressive sensing\n(DCS) framework proposed in the literature are reported. We also consider the\neffect of the dimension of the latent space and the sparsity factor in\nvalidating the SDLSS framework. Performance quantification is carried out by\nemploying three objective metrics: peak signal-to-noise ratio (PSNR),\nstructural similarity index metric (SSIM), and reconstruction error (RE).\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:12:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Killedar", "Vinayak", ""], ["Pokala", "Praveen Kumar", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "2105.12038", "submitter": "Aleksandr Safin", "authors": "Aleksandr Safin, Maxim Kan, Nikita Drobyshev, Oleg Voynov, Alexey\n  Artemov, Alexander Filippov, Denis Zorin, Evgeny Burnaev", "title": "Towards Unpaired Depth Enhancement and Super-Resolution in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth maps captured with commodity sensors are often of low quality and\nresolution; these maps need to be enhanced to be used in many applications.\nState-of-the-art data-driven methods of depth map super-resolution rely on\nregistered pairs of low- and high-resolution depth maps of the same scenes.\nAcquisition of real-world paired data requires specialized setups. Another\nalternative, generating low-resolution maps from high-resolution maps by\nsubsampling, adding noise and other artificial degradation methods, does not\nfully capture the characteristics of real-world low-resolution images. As a\nconsequence, supervised learning methods trained on such artificial paired data\nmay not perform well on real-world low-resolution inputs. We consider an\napproach to depth map enhancement based on learning from unpaired data. While\nmany techniques for unpaired image-to-image translation have been proposed,\nmost are not directly applicable to depth maps. We propose an unpaired learning\nmethod for simultaneous depth enhancement and super-resolution, which is based\non a learnable degradation model and surface normal estimates as features to\nproduce more accurate depth maps. We demonstrate that our method outperforms\nexisting unpaired methods and performs on par with paired methods on a new\nbenchmark for unpaired learning that we developed.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:19:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Safin", "Aleksandr", ""], ["Kan", "Maxim", ""], ["Drobyshev", "Nikita", ""], ["Voynov", "Oleg", ""], ["Artemov", "Alexey", ""], ["Filippov", "Alexander", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2105.12043", "submitter": "Wenhao Wu", "authors": "Lining Wang, Haosen Yang, Wenhao Wu, Hongxun Yao, Hujie Huang", "title": "Temporal Action Proposal Generation with Transformers", "comments": "The first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer networks are effective at modeling long-range contextual\ninformation and have recently demonstrated exemplary performance in the natural\nlanguage processing domain. Conventionally, the temporal action proposal\ngeneration (TAPG) task is divided into two main sub-tasks: boundary prediction\nand proposal confidence prediction, which rely on the frame-level dependencies\nand proposal-level relationships separately. To capture the dependencies at\ndifferent levels of granularity, this paper intuitively presents a unified\ntemporal action proposal generation framework with original Transformers,\ncalled TAPG Transformer, which consists of a Boundary Transformer and a\nProposal Transformer. Specifically, the Boundary Transformer captures long-term\ntemporal dependencies to predict precise boundary information and the Proposal\nTransformer learns the rich inter-proposal relationships for reliable\nconfidence evaluation. Extensive experiments are conducted on two popular\nbenchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG\nTransformer outperforms state-of-the-art methods. Equipped with the existing\naction classifier, our method achieves remarkable performance on the temporal\naction localization task. Codes and models will be available.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:22:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Lining", ""], ["Yang", "Haosen", ""], ["Wu", "Wenhao", ""], ["Yao", "Hongxun", ""], ["Huang", "Hujie", ""]]}, {"id": "2105.12053", "submitter": "Mehmet Kerim Yucel", "authors": "Mehmet Kerim Yucel, Valia Dimaridou, Anastasios Drosou, Albert\n  Sa\\`a-Garriga", "title": "Real-time Monocular Depth Estimation with Sparse Supervision on Mobile", "comments": "To appear at CVPR 2021 Mobile AI (MAI) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular (relative or metric) depth estimation is a critical task for\nvarious applications, such as autonomous vehicles, augmented reality and image\nediting. In recent years, with the increasing availability of mobile devices,\naccurate and mobile-friendly depth models have gained importance. Increasingly\naccurate models typically require more computational resources, which inhibits\nthe use of such models on mobile devices. The mobile use case is arguably the\nmost unrestricted one, which requires highly accurate yet mobile-friendly\narchitectures. Therefore, we try to answer the following question: How can we\nimprove a model without adding further complexity (i.e. parameters)? Towards\nthis end, we systematically explore the design space of a relative depth\nestimation model from various dimensions and we show, with key design choices\nand ablation studies, even an existing architecture can reach highly\ncompetitive performance to the state of the art, with a fraction of the\ncomplexity. Our study spans an in-depth backbone model selection process,\nknowledge distillation, intermediate predictions, model pruning and loss\nrebalancing. We show that our model, using only DIW as the supervisory dataset,\nachieves 0.1156 WHDR on DIW with 2.6M parameters and reaches 37 FPS on a mobile\nGPU, without pruning or hardware-specific optimization. A pruned version of our\nmodel achieves 0.1208 WHDR on DIW with 1M parameters and reaches 44 FPS on a\nmobile GPU.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:33:28 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yucel", "Mehmet Kerim", ""], ["Dimaridou", "Valia", ""], ["Drosou", "Anastasios", ""], ["Sa\u00e0-Garriga", "Albert", ""]]}, {"id": "2105.12056", "submitter": "Cassandra Burgess", "authors": "Cassandra Burgess, Cordelia Neisinger, Rafael Dinner", "title": "Matching Targets Across Domains with RADON, the Re-Identification Across\n  Domain Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel convolutional neural network that learns to match images\nof an object taken from different viewpoints or by different optical sensors.\nOur Re-Identification Across Domain Network (RADON) scores pairs of input\nimages from different domains on similarity. Our approach extends previous work\non Siamese networks and modifies them to more challenging use cases, including\nlow- and no-shot learning, in which few images of a specific target are\navailable for training. RADON shows strong performance on cross-view vehicle\nmatching and cross-domain person identification in a no-shot learning\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:36:38 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Burgess", "Cassandra", ""], ["Neisinger", "Cordelia", ""], ["Dinner", "Rafael", ""]]}, {"id": "2105.12085", "submitter": "Wenhao Wu", "authors": "Wenhao Wu, Yuxiang Zhao, Yanwu Xu, Xiao Tan, Dongliang He, Zhikang\n  Zou, Jin Ye, Yingying Li, Mingde Yao, Zichao Dong, Yifeng Shi", "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level\n  Representation Learning", "comments": "Accepted by ACMMM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:09:57 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 13:43:37 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wu", "Wenhao", ""], ["Zhao", "Yuxiang", ""], ["Xu", "Yanwu", ""], ["Tan", "Xiao", ""], ["He", "Dongliang", ""], ["Zou", "Zhikang", ""], ["Ye", "Jin", ""], ["Li", "Yingying", ""], ["Yao", "Mingde", ""], ["Dong", "Zichao", ""], ["Shi", "Yifeng", ""]]}, {"id": "2105.12097", "submitter": "Abdul Rehman Javed", "authors": "Waqas Ahmed, Amir Rasool, Neeraj Kumar, Abdul RehmanJaved, Thippa\n  Reddy Gadekallu, Zunera Jalil, Natalia Kryvinska", "title": "Security in Next Generation Mobile Payment Systems: A Comprehensive\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cash payment is still king in several markets, accounting for more than 90\\\nof the payments in almost all the developing countries. The usage of mobile\nphones is pretty ordinary in this present era. Mobile phones have become an\ninseparable friend for many users, serving much more than just communication\ntools. Every subsequent person is heavily relying on them due to multifaceted\nusage and affordability. Every person wants to manage his/her daily\ntransactions and related issues by using his/her mobile phone. With the rise\nand advancements of mobile-specific security, threats are evolving as well. In\nthis paper, we provide a survey of various security models for mobile phones.\nWe explore multiple proposed models of the mobile payment system (MPS), their\ntechnologies and comparisons, payment methods, different security mechanisms\ninvolved in MPS, and provide analysis of the encryption technologies,\nauthentication methods, and firewall in MPS. We also present current challenges\nand future directions of mobile phone security.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:34:22 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 17:43:12 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ahmed", "Waqas", ""], ["Rasool", "Amir", ""], ["Kumar", "Neeraj", ""], ["RehmanJaved", "Abdul", ""], ["Gadekallu", "Thippa Reddy", ""], ["Jalil", "Zunera", ""], ["Kryvinska", "Natalia", ""]]}, {"id": "2105.12106", "submitter": "Mst. Tasnim Pervin", "authors": "Mst. Tasnim Pervin, Linmi Tao, Aminul Huq, Zuoxiang He, Li Huo", "title": "Adversarial Attack Driven Data Augmentation for Accurate And Robust\n  Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation is considered to be a very crucial task in medical image\nanalysis. This task has been easier since deep learning models have taken over\nwith its high performing behavior. However, deep learning models dependency on\nlarge data proves it to be an obstacle in medical image analysis because of\ninsufficient data samples. Several data augmentation techniques have been used\nto mitigate this problem. We propose a new augmentation method by introducing\nadversarial learning attack techniques, specifically Fast Gradient Sign Method\n(FGSM). Furthermore, We have also introduced the concept of Inverse FGSM\n(InvFGSM), which works in the opposite manner of FGSM for the data\naugmentation. This two approaches worked together to improve the segmentation\naccuracy, as well as helped the model to gain robustness against adversarial\nattacks. The overall analysis of experiments indicates a novel use of\nadversarial machine learning along with robustness enhancement.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:44:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Pervin", "Mst. Tasnim", ""], ["Tao", "Linmi", ""], ["Huq", "Aminul", ""], ["He", "Zuoxiang", ""], ["Huo", "Li", ""]]}, {"id": "2105.12107", "submitter": "Akin Yilmaz", "authors": "M. Ak{\\i}n Y{\\i}lmaz, Onur Kele\\c{s}, Hilal G\\\"uven, A. Murat Tekalp,\n  Junaid Malik, Serkan K{\\i}ranyaz", "title": "Self-Organized Variational Autoencoders (Self-VAE) for Learned Image\n  Compression", "comments": "Accepted for publication in IEEE International Conference on Image\n  Processing (ICIP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In end-to-end optimized learned image compression, it is standard practice to\nuse a convolutional variational autoencoder with generalized divisive\nnormalization (GDN) to transform images into a latent space. Recently,\nOperational Neural Networks (ONNs) that learn the best non-linearity from a set\nof alternatives, and their self-organized variants, Self-ONNs, that approximate\nany non-linearity via Taylor series have been proposed to address the\nlimitations of convolutional layers and a fixed nonlinear activation. In this\npaper, we propose to replace the convolutional and GDN layers in the\nvariational autoencoder with self-organized operational layers, and propose a\nnovel self-organized variational autoencoder (Self-VAE) architecture that\nbenefits from stronger non-linearity. The experimental results demonstrate that\nthe proposed Self-VAE yields improvements in both rate-distortion performance\nand perceptual image quality.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:44:20 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 21:34:05 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 15:43:13 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Y\u0131lmaz", "M. Ak\u0131n", ""], ["Kele\u015f", "Onur", ""], ["G\u00fcven", "Hilal", ""], ["Tekalp", "A. Murat", ""], ["Malik", "Junaid", ""], ["K\u0131ranyaz", "Serkan", ""]]}, {"id": "2105.12115", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Ehsan Zabihi Naeini", "title": "Calibration and Uncertainty Quantification of Bayesian Convolutional\n  Neural Networks for Geophysical Applications", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks offer numerous potential applications across geoscience,\nfor example, one could argue that they are the state-of-the-art method for\npredicting faults in seismic datasets. In quantitative reservoir\ncharacterization workflows, it is common to incorporate the uncertainty of\npredictions thus such subsurface models should provide calibrated probabilities\nand the associated uncertainties in their predictions. It has been shown that\npopular Deep Learning-based models are often miscalibrated, and due to their\ndeterministic nature, provide no means to interpret the uncertainty of their\npredictions. We compare three different approaches to obtaining probabilistic\nmodels based on convolutional neural networks in a Bayesian formalism, namely\nDeep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian\n(SWAG). These methods are consistently applied to fault detection case studies\nwhere Deep Ensembles use independently trained models to provide fault\nprobabilities, Concrete Dropout represents an extension to the popular Dropout\ntechnique to approximate Bayesian neural networks, and finally, we apply SWAG,\na recent method that is based on the Bayesian inference equivalence of\nmini-batch Stochastic Gradient Descent. We provide quantitative results in\nterms of model calibration and uncertainty representation, as well as\nqualitative results on synthetic and real seismic datasets. Our results show\nthat the approximate Bayesian methods, Concrete Dropout and SWAG, both provide\nwell-calibrated predictions and uncertainty attributes at a lower computational\ncost when compared to the baseline Deep Ensemble approach. The resulting\nuncertainties also offer a possibility to further improve the model performance\nas well as enhancing the interpretability of the models.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:54:23 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mosser", "Lukas", ""], ["Naeini", "Ehsan Zabihi", ""]]}, {"id": "2105.12151", "submitter": "Baozhou Zhu", "authors": "Baozhou Zhu and Peter Hofstee and Johan Peltenburg and Jinho Lee and\n  Zaid Alars", "title": "AutoReCon: Neural Architecture Search-based Reconstruction for Data-free\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-free compression raises a new challenge because the original training\ndataset for a pre-trained model to be compressed is not available due to\nprivacy or transmission issues. Thus, a common approach is to compute a\nreconstructed training dataset before compression. The current reconstruction\nmethods compute the reconstructed training dataset with a generator by\nexploiting information from the pre-trained model. However, current\nreconstruction methods focus on extracting more information from the\npre-trained model but do not leverage network engineering. This work is the\nfirst to consider network engineering as an approach to design the\nreconstruction method. Specifically, we propose the AutoReCon method, which is\na neural architecture search-based reconstruction method. In the proposed\nAutoReCon method, the generator architecture is designed automatically given\nthe pre-trained model for reconstruction. Experimental results show that using\ngenerators discovered by the AutoRecon method always improve the performance of\ndata-free compression.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 18:03:25 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhu", "Baozhou", ""], ["Hofstee", "Peter", ""], ["Peltenburg", "Johan", ""], ["Lee", "Jinho", ""], ["Alars", "Zaid", ""]]}, {"id": "2105.12161", "submitter": "Srishti Yadav", "authors": "Srishti Yadav", "title": "Occlusion Aware Kernel Correlation Filter Tracker using RGB-D", "comments": "Thesis, Simon Fraser University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unlike deep learning which requires large training datasets, correlation\nfilter-based trackers like Kernelized Correlation Filter (KCF) uses implicit\nproperties of tracked images (circulant matrices) for training in real-time.\nDespite their practical application in tracking, a need for a better\nunderstanding of the fundamentals associated with KCF in terms of\ntheoretically, mathematically, and experimentally exists. This thesis first\ndetails the workings prototype of the tracker and investigates its\neffectiveness in real-time applications and supporting visualizations. We\nfurther address some of the drawbacks of the tracker in cases of occlusions,\nscale changes, object rotation, out-of-view and model drift with our novel\nRGB-D Kernel Correlation tracker. We also study the use of particle filters to\nimprove trackers' accuracy. Our results are experimentally evaluated using a)\nstandard dataset and b) real-time using the Microsoft Kinect V2 sensor. We\nbelieve this work will set the basis for a better understanding of the\neffectiveness of kernel-based correlation filter trackers and to further define\nsome of its possible advantages in tracking.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 18:37:39 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Yadav", "Srishti", ""]]}, {"id": "2105.12186", "submitter": "Chenran Zhang", "authors": "Zhongzhen Luo, Fengjia Zhang, Guoyi Fu, Jiajie Xu", "title": "Self-Guided Instance-Aware Network for Depth Completion and Enhancement", "comments": "Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth completion aims at inferring a dense depth image from sparse depth\nmeasurement since glossy, transparent or distant surface cannot be scanned\nproperly by the sensor. Most of existing methods directly interpolate the\nmissing depth measurements based on pixel-wise image content and the\ncorresponding neighboring depth values. Consequently, this leads to blurred\nboundaries or inaccurate structure of object. To address these problems, we\npropose a novel self-guided instance-aware network (SG-IANet) that: (1) utilize\nself-guided mechanism to extract instance-level features that is needed for\ndepth restoration, (2) exploit the geometric and context information into\nnetwork learning to conform to the underlying constraints for edge clarity and\nstructure consistency, (3) regularize the depth estimation and mitigate the\nimpact of noise by instance-aware learning, and (4) train with synthetic data\nonly by domain randomization to bridge the reality gap. Extensive experiments\non synthetic and real world dataset demonstrate that our proposed method\noutperforms previous works. Further ablation studies give more insights into\nthe proposed method and demonstrate the generalization capability of our model.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 19:41:38 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 19:25:16 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Luo", "Zhongzhen", ""], ["Zhang", "Fengjia", ""], ["Fu", "Guoyi", ""], ["Xu", "Jiajie", ""]]}, {"id": "2105.12210", "submitter": "George Philipp", "authors": "George Philipp", "title": "The Nonlinearity Coefficient - A Practical Guide to Neural Architecture\n  Design", "comments": "This work is based on the PhD thesis with the same name, author, year\n  and institution. Both works may be cited interchangeably", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In essence, a neural network is an arbitrary differentiable, parametrized\nfunction. Choosing a neural network architecture for any task is as complex as\nsearching the space of those functions. For the last few years, 'neural\narchitecture design' has been largely synonymous with 'neural architecture\nsearch' (NAS), i.e. brute-force, large-scale search. NAS has yielded\nsignificant gains on practical tasks. However, NAS methods end up searching for\na local optimum in architecture space in a small neighborhood around\narchitectures that often go back decades, based on CNN or LSTM.\n  In this work, we present a different and complementary approach to\narchitecture design, which we term 'zero-shot architecture design' (ZSAD). We\ndevelop methods that can predict, without any training, whether an architecture\nwill achieve a relatively high test or training error on a task after training.\nWe then go on to explain the error in terms of the architecture definition\nitself and develop tools for modifying the architecture based on this\nexplanation. This confers an unprecedented level of control on the deep\nlearning practitioner. They can make informed design decisions before the first\nline of code is written, even for tasks for which no prior art exists.\n  Our first major contribution is to show that the 'degree of nonlinearity' of\na neural architecture is a key causal driver behind its performance, and a\nprimary aspect of the architecture's model complexity. We introduce the\n'nonlinearity coefficient' (NLC), a scalar metric for measuring nonlinearity.\nVia extensive empirical study, we show that the value of the NLC in the\narchitecture's randomly initialized state before training is a powerful\npredictor of test error after training and that attaining a right-sized NLC is\nessential for attaining an optimal test error. The NLC is also conceptually\nsimple, well-defined for any feedforward network, easy and cheap to compute,\nhas extensive theoretical, empirical and conceptual grounding, follows\ninstructively from the architecture definition, and can be easily controlled\nvia our 'nonlinearity normalization' algorithm. We argue that the NLC is the\nmost powerful scalar statistic for architecture design specifically and neural\nnetwork analysis in general. Our analysis is fueled by mean field theory, which\nwe use to uncover the 'meta-distribution' of layers.\n  Beyond the NLC, we uncover and flesh out a range of metrics and properties\nthat have a significant explanatory influence on test and training error. We go\non to explain the majority of the error variation across a wide range of\nrandomly generated architectures with these metrics and properties. We compile\nour insights into a practical guide for architecture designers, which we argue\ncan significantly shorten the trial-and-error phase of deep learning\ndeployment.\n  Our results are grounded in an experimental protocol that exceeds that of the\nvast majority of other deep learning studies in terms of carefulness and rigor.\nWe study the impact of e.g. dataset, learning rate, floating-point precision,\nloss function, statistical estimation error and batch inter-dependency on\nperformance and other key properties. We promote research practices that we\nbelieve can significantly accelerate progress in architecture design research.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:47:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Philipp", "George", ""]]}, {"id": "2105.12227", "submitter": "Xi Jia", "authors": "Xi Jia, Alexander Thorley, Wei Chen, Huaqi Qiu, Linlin Shen, Iain B\n  Styles, Hyung Jin Chang, Ales Leonardis, Antonio de Marvao, Declan P.\n  O'Regan, Daniel Rueckert, Jinming Duan", "title": "Learning a Model-Driven Variational Network for Deformable Image\n  Registration", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven deep learning approaches to image registration can be less\naccurate than conventional iterative approaches, especially when training data\nis limited. To address this whilst retaining the fast inference speed of deep\nlearning, we propose VR-Net, a novel cascaded variational network for\nunsupervised deformable image registration. Using the variable splitting\noptimization scheme, we first convert the image registration problem,\nestablished in a generic variational framework, into two sub-problems, one with\na point-wise, closed-form solution while the other one is a denoising problem.\nWe then propose two neural layers (i.e. warping layer and intensity consistency\nlayer) to model the analytical solution and a residual U-Net to formulate the\ndenoising problem (i.e. generalized denoising layer). Finally, we cascade the\nwarping layer, intensity consistency layer, and generalized denoising layer to\nform the VR-Net. Extensive experiments on three (two 2D and one 3D) cardiac\nmagnetic resonance imaging datasets show that VR-Net outperforms\nstate-of-the-art deep learning methods on registration accuracy, while\nmaintains the fast inference speed of deep learning and the data-efficiency of\nvariational model.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 21:37:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jia", "Xi", ""], ["Thorley", "Alexander", ""], ["Chen", "Wei", ""], ["Qiu", "Huaqi", ""], ["Shen", "Linlin", ""], ["Styles", "Iain B", ""], ["Chang", "Hyung Jin", ""], ["Leonardis", "Ales", ""], ["de Marvao", "Antonio", ""], ["O'Regan", "Declan P.", ""], ["Rueckert", "Daniel", ""], ["Duan", "Jinming", ""]]}, {"id": "2105.12238", "submitter": "Benjamin Jones", "authors": "Benjamin Jones, Dalton Hildreth, Duowen Chen, Ilya Baran, Vova Kim,\n  Adriana Schulz", "title": "SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating\n  of CAD Assemblies", "comments": "16 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assembly modeling is a core task of computer aided design (CAD), comprising\naround one third of the work in a CAD workflow. Optimizing this process\ntherefore represents a huge opportunity in the design of a CAD system, but\ncurrent research of assembly based modeling is not directly applicable to\nmodern CAD systems because it eschews the dominant data structure of modern\nCAD: parametric boundary representations (BREPs). CAD assembly modeling defines\nassemblies as a system of pairwise constraints, called mates, between parts,\nwhich are defined relative to BREP topology rather than in world coordinates\ncommon to existing work. We propose SB-GCN, a representation learning scheme on\nBREPs that retains the topological structure of parts, and use these learned\nrepresentations to predict CAD type mates. To train our system, we compiled the\nfirst large scale dataset of BREP CAD assemblies, which we are releasing along\nwith benchmark mate prediction tasks. Finally, we demonstrate the compatibility\nof our model with an existing commercial CAD system by building a tool that\nassists users in mate creation by suggesting mate completions, with 72.2%\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:07:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jones", "Benjamin", ""], ["Hildreth", "Dalton", ""], ["Chen", "Duowen", ""], ["Baran", "Ilya", ""], ["Kim", "Vova", ""], ["Schulz", "Adriana", ""]]}, {"id": "2105.12247", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg", "comments": "Paper Accepted in the Weakly Supervised Representation Learning\n  Workshop, IJCAI 2021 (IJCAI2021-WSRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:34:19 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:51:26 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 03:18:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "2105.12256", "submitter": "Mathew Schwartz", "authors": "Mathew Schwartz, Tomer Weiss, Esra Ataer-Cansizoglu, Jae-Woo Choi", "title": "Style Similarity as Feedback for Product Design", "comments": "15 pages, 9 figures, interdisciplinary book chapter on using computer\n  vision and style similarity for industrial design", "journal-ref": "In: Lee JH. (eds) A New Perspective of Cultural DNA. KAIST\n  Research Series. Springer, Singapore (2021)", "doi": "10.1007/978-981-15-7707-9_3", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching and recommending products is beneficial for both customers and\ncompanies. With the rapid increase in home goods e-commerce, there is an\nincreasing demand for quantitative methods for providing such recommendations\nfor millions of products. This approach is facilitated largely by online stores\nsuch as Amazon and Wayfair, in which the goal is to maximize overall sales.\nInstead of focusing on overall sales, we take a product design perspective, by\nemploying big-data analysis for determining the design qualities of a highly\nrecommended product. Specifically, we focus on the visual style compatibility\nof such products. We build off previous work which implemented a style-based\nsimilarity metric for thousands of furniture products. Using analysis and\nvisualization, we extract attributes of furniture products that are highly\ncompatible style-wise. We propose a designer in-the-loop workflow that mirrors\nmethods of displaying similar products to consumers browsing e-commerce\nwebsites. Our findings are useful when designing new products, since they\nprovide insight regarding what furniture will be strongly compatible across\nmultiple styles, and hence, more likely to be recommended.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:30:29 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Schwartz", "Mathew", ""], ["Weiss", "Tomer", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Choi", "Jae-Woo", ""]]}, {"id": "2105.12281", "submitter": "Rafael Baldasso Audibert", "authors": "Rafael Baldasso Audibert and Vinicius Marinho Maschio", "title": "FINNger -- Applying artificial intelligence to ease math learning for\n  children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Kids have an amazing capacity to use modern electronic devices such as\ntablets, smartphones, etc. This has been incredibly boosted by the ease of\naccess of these devices given the expansion of such devices through the world,\nreaching even third world countries. Also, it is well known that children tend\nto have difficulty learning some subjects at pre-school. We as a society focus\nextensively on alphabetization, but in the end, children end up having\ndifferences in another essential area: Mathematics. With this work, we create\nthe basis for an intuitive application that could join the fact that children\nhave a lot of ease when using such technological applications, trying to shrink\nthe gap between a fun and enjoyable activity with something that will improve\nthe children knowledge and ability to understand concepts when in a low age, by\nusing a novel convolutional neural network to achieve so, named FINNger.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:02:31 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Audibert", "Rafael Baldasso", ""], ["Maschio", "Vinicius Marinho", ""]]}, {"id": "2105.12311", "submitter": "Ant\\'onio Ramires Fernandes", "authors": "Joel Tom\\'as Morais, Ant\\'onio Ramires Fernandes, Andr\\'e Leite\n  Ferreira, Bruno Faria", "title": "Performance Analysis of a Foreground Segmentation Neural Network Model", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent years the interest in segmentation has been growing, being used in\na wide range of applications such as fraud detection, anomaly detection in\npublic health and intrusion detection. We present an ablation study of\nFgSegNet_v2, analysing its three stages: (i) Encoder, (ii) Feature Pooling\nModule and (iii) Decoder. The result of this study is a proposal of a variation\nof the aforementioned method that surpasses state of the art results. Three\ndatasets are used for testing: CDNet2014, SBI2015 and CityScapes. In CDNet2014\nwe got an overall improvement compared to the state of the art, mainly in the\nLowFrameRate subset. The presented approach is promising as it produces\ncomparable results with the state of the art (SBI2015 and Cityscapes datasets)\nin very different conditions, such as different lighting conditions.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 03:07:07 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Morais", "Joel Tom\u00e1s", ""], ["Fernandes", "Ant\u00f3nio Ramires", ""], ["Ferreira", "Andr\u00e9 Leite", ""], ["Faria", "Bruno", ""]]}, {"id": "2105.12324", "submitter": "Wentao Jiang", "authors": "Si Liu, Wentao Jiang, Chen Gao, Ran He, Jiashi Feng, Bo Li, Shuicheng\n  Yan", "title": "PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal", "comments": "Accepted by TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the makeup transfer and removal tasks\nsimultaneously, which aim to transfer the makeup from a reference image to a\nsource image and remove the makeup from the with-makeup image respectively.\nExisting methods have achieved much advancement in constrained scenarios, but\nit is still very challenging for them to transfer makeup between images with\nlarge pose and expression differences, or handle makeup details like blush on\ncheeks or highlight on the nose. In addition, they are hardly able to control\nthe degree of makeup during transferring or to transfer a specified part in the\ninput face. In this work, we propose the PSGAN++, which is capable of\nperforming both detail-preserving makeup transfer and effective makeup removal.\nFor makeup transfer, PSGAN++ uses a Makeup Distill Network to extract makeup\ninformation, which is embedded into spatial-aware makeup matrices. We also\ndevise an Attentive Makeup Morphing module that specifies how the makeup in the\nsource image is morphed from the reference image, and a makeup detail loss to\nsupervise the model within the selected makeup detail area. On the other hand,\nfor makeup removal, PSGAN++ applies an Identity Distill Network to embed the\nidentity information from with-makeup images into identity matrices. Finally,\nthe obtained makeup/identity matrices are fed to a Style Transfer Network that\nis able to edit the feature maps to achieve makeup transfer or removal. To\nevaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the\nWild dataset that contains images with diverse poses and expressions and a\nMakeup Transfer High-Resolution dataset that contains high-resolution images.\nExperiments demonstrate that PSGAN++ not only achieves state-of-the-art results\nwith fine makeup details even in cases of large pose/expression differences but\nalso can perform partial or degree-controllable makeup transfer.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 04:37:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Liu", "Si", ""], ["Jiang", "Wentao", ""], ["Gao", "Chen", ""], ["He", "Ran", ""], ["Feng", "Jiashi", ""], ["Li", "Bo", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2105.12332", "submitter": "Peter Ondruska", "authors": "Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del\n  Pero, Blazej Osinski, Hugo Grimmett, Peter Ondruska", "title": "SimNet: Learning Reactive Self-driving Simulations from Real-world\n  Observations", "comments": "Published at 2021 International Conference on Robotics and Automation\n  (ICRA2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a simple end-to-end trainable machine learning\nsystem capable of realistically simulating driving experiences. This can be\nused for the verification of self-driving system performance without relying on\nexpensive and time-consuming road testing. In particular, we frame the\nsimulation problem as a Markov Process, leveraging deep neural networks to\nmodel both state distribution and transition function. These are trainable\ndirectly from the existing raw observations without the need for any\nhandcrafting in the form of plant or kinematic models. All that is needed is a\ndataset of historical traffic episodes. Our formulation allows the system to\nconstruct never seen scenes that unfold realistically reacting to the\nself-driving car's behaviour. We train our system directly from 1,000 hours of\ndriving logs and measure both realism, reactivity of the simulation as the two\nkey properties of the simulation. At the same time, we apply the method to\nevaluate the performance of a recently proposed state-of-the-art ML planning\nsystem trained from human driving logs. We discover this planning system is\nprone to previously unreported causal confusion issues that are difficult to\ntest by non-reactive simulation. To the best of our knowledge, this is the\nfirst work that directly merges highly realistic data-driven simulations with a\nclosed-loop evaluation for self-driving vehicles. We make the data, code, and\npre-trained models publicly available to further stimulate simulation\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 05:14:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bergamini", "Luca", ""], ["Ye", "Yawei", ""], ["Scheel", "Oliver", ""], ["Chen", "Long", ""], ["Hu", "Chih", ""], ["Del Pero", "Luca", ""], ["Osinski", "Blazej", ""], ["Grimmett", "Hugo", ""], ["Ondruska", "Peter", ""]]}, {"id": "2105.12337", "submitter": "Peter Ondruska", "authors": "Long Chen, Lukas Platinsky, Stefanie Speichert, Blazej Osinski, Oliver\n  Scheel, Yawei Ye, Hugo Grimmett, Luca del Pero, Peter Ondruska", "title": "What data do we need for training an AV motion planner?", "comments": "Published at 2021 International Conference on Robotics and Automation\n  (ICRA2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate what grade of sensor data is required for training an\nimitation-learning-based AV planner on human expert demonstration.\nMachine-learned planners are very hungry for training data, which is usually\ncollected using vehicles equipped with the same sensors used for autonomous\noperation. This is costly and non-scalable. If cheaper sensors could be used\nfor collection instead, data availability would go up, which is crucial in a\nfield where data volume requirements are large and availability is small. We\npresent experiments using up to 1000 hours worth of expert demonstration and\nfind that training with 10x lower-quality data outperforms 1x AV-grade data in\nterms of planner performance. The important implication of this is that cheaper\nsensors can indeed be used. This serves to improve data access and democratize\nthe field of imitation-based motion planning. Alongside this, we perform a\nsensitivity analysis of planner performance as a function of perception range,\nfield-of-view, accuracy, and data volume, and the reason why lower-quality data\nstill provide good planning results.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 05:37:12 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Long", ""], ["Platinsky", "Lukas", ""], ["Speichert", "Stefanie", ""], ["Osinski", "Blazej", ""], ["Scheel", "Oliver", ""], ["Ye", "Yawei", ""], ["Grimmett", "Hugo", ""], ["del Pero", "Luca", ""], ["Ondruska", "Peter", ""]]}, {"id": "2105.12355", "submitter": "Shijie Yu", "authors": "Shijie Yu, Feng Zhu, Dapeng Chen, Rui Zhao, Haobin Chen, Shixiang\n  Tang, Jinguo Zhu, Yu Qiao", "title": "Multiple Domain Experts Collaborative Learning: Multi-Source Domain\n  Generalization For Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed significant progress in person re-identification\n(ReID). However, current ReID approaches suffer from considerable performance\ndegradation when the test target domains exhibit different characteristics from\nthe training ones, known as the domain shift problem. To make ReID more\npractical and generalizable, we formulate person re-identification as a Domain\nGeneralization (DG) problem and propose a novel training framework, named\nMultiple Domain Experts Collaborative Learning (MD-ExCo). Specifically, the\nMD-ExCo consists of a universal expert and several domain experts. Each domain\nexpert focuses on learning from a specific domain, and periodically\ncommunicates with other domain experts to regulate its learning strategy in the\nmeta-learning manner to avoid overfitting. Besides, the universal expert\ngathers knowledge from the domain experts, and also provides supervision to\nthem as feedback. Extensive experiments on DG-ReID benchmarks show that our\nMD-ExCo outperforms the state-of-the-art methods by a large margin, showing its\nability to improve the generalization capability of the ReID models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 06:38:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Yu", "Shijie", ""], ["Zhu", "Feng", ""], ["Chen", "Dapeng", ""], ["Zhao", "Rui", ""], ["Chen", "Haobin", ""], ["Tang", "Shixiang", ""], ["Zhu", "Jinguo", ""], ["Qiao", "Yu", ""]]}, {"id": "2105.12357", "submitter": "Alfred Laugros", "authors": "Alfred Laugros and Alice Caplier and Matthieu Ospici", "title": "Using the Overlapping Score to Improve Corruption Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks are sensitive to various corruptions that usually occur in\nreal-world applications such as blurs, noises, low-lighting conditions, etc. To\nestimate the robustness of neural networks to these common corruptions, we\ngenerally use a group of modeled corruptions gathered into a benchmark.\nUnfortunately, no objective criterion exists to determine whether a benchmark\nis representative of a large diversity of independent corruptions. In this\npaper, we propose a metric called corruption overlapping score, which can be\nused to reveal flaws in corruption benchmarks. Two corruptions overlap when the\nrobustnesses of neural networks to these corruptions are correlated. We argue\nthat taking into account overlappings between corruptions can help to improve\nexisting benchmarks or build better ones.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 06:42:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Laugros", "Alfred", ""], ["Caplier", "Alice", ""], ["Ospici", "Matthieu", ""]]}, {"id": "2105.12362", "submitter": "Mathias Gehrig", "authors": "Manasi Muglikar and Mathias Gehrig and Daniel Gehrig and Davide\n  Scaramuzza", "title": "How to Calibrate Your Event Camera", "comments": "IEEE Conference on Computer Vision and Pattern Recognition Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a generic event camera calibration framework using image\nreconstruction. Instead of relying on blinking LED patterns or external\nscreens, we show that neural-network-based image reconstruction is well suited\nfor the task of intrinsic and extrinsic calibration of event cameras. The\nadvantage of our proposed approach is that we can use standard calibration\npatterns that do not rely on active illumination. Furthermore, our approach\nenables the possibility to perform extrinsic calibration between frame-based\nand event-based sensors without additional complexity. Both simulation and\nreal-world experiments indicate that calibration through image reconstruction\nis accurate under common distortion models and a wide variety of distortion\nparameters\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 07:06:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Muglikar", "Manasi", ""], ["Gehrig", "Mathias", ""], ["Gehrig", "Daniel", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2105.12385", "submitter": "Allan Gr{\\o}nlund", "authors": "Allan Gr{\\o}nlund and Jonas Tranberg", "title": "Learning to Detect Fortified Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution data models like grid terrain models made from LiDAR data are\na prerequisite for modern day Geographic Information Systems applications.\nBesides providing the foundation for the very accurate digital terrain models,\nLiDAR data is also extensively used to classify which parts of the considered\nsurface comprise relevant elements like water, buildings and vegetation. In\nthis paper we consider the problem of classifying which areas of a given\nsurface are fortified by for instance, roads, sidewalks, parking spaces, paved\ndriveways and terraces. We consider using LiDAR data and orthophotos, combined\nand alone, to show how well the modern machine learning algorithms Gradient\nBoosted Trees and Convolutional Neural Networks are able to detect fortified\nareas on large real world data. The LiDAR data features, in particular the\nintensity feature that measures the signal strength of the return, that we\nconsider in this project are heavily dependent on the actual LiDAR sensor that\nmade the measurement. This is highly problematic, in particular for the\ngeneralisation capability of pattern matching algorithms, as this means that\ndata features for test data may be very different from the data the model is\ntrained on. We propose an algorithmic solution to this problem by designing a\nneural net embedding architecture that transforms data from all the different\nsensor systems into a new common representation that works as well as if the\ntraining data and test data originated from the same sensor. The final\nalgorithm result has an accuracy above 96 percent, and an AUC score above 0.99.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:03:42 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Tranberg", "Jonas", ""]]}, {"id": "2105.12386", "submitter": "Jinyang Guo", "authors": "Jinyang Guo, Dong Xu, Guo Lu", "title": "CBANet: Towards Complexity and Bitrate Adaptive Deep Image Compression\n  using a Single Network", "comments": "Submitted to T-IP", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep image compression framework called\nComplexity and Bitrate Adaptive Network (CBANet), which aims to learn one\nsingle network to support variable bitrate coding under different computational\ncomplexity constraints. In contrast to the existing state-of-the-art learning\nbased image compression frameworks that only consider the rate-distortion\ntrade-off without introducing any constraint related to the computational\ncomplexity, our CBANet considers the trade-off between the rate and distortion\nunder dynamic computational complexity constraints. Specifically, to decode the\nimages with one single decoder under various computational complexity\nconstraints, we propose a new multi-branch complexity adaptive module, in which\neach branch only takes a small portion of the computational budget of the\ndecoder. The reconstructed images with different visual qualities can be\nreadily generated by using different numbers of branches. Furthermore, to\nachieve variable bitrate decoding with one single decoder, we propose a bitrate\nadaptive module to project the representation from a base bitrate to the\nexpected representation at a target bitrate for transmission. Then it will\nproject the transmitted representation at the target bitrate back to that at\nthe base bitrate for the decoding process. The proposed bit adaptive module can\nsignificantly reduce the storage requirement for deployment platforms. As a\nresult, our CBANet enables one single codec to support multiple bitrate\ndecoding under various computational complexity constraints. Comprehensive\nexperiments on two benchmark datasets demonstrate the effectiveness of our\nCBANet for deep image compression.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:13:56 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Guo", "Jinyang", ""], ["Xu", "Dong", ""], ["Lu", "Guo", ""]]}, {"id": "2105.12397", "submitter": "Hao Zhou", "authors": "Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, Houqiang Li", "title": "Improving Sign Language Translation with Monolingual Data by Sign\n  Back-Translation", "comments": "To appear in 2021 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite existing pioneering works on sign language translation (SLT), there\nis a non-trivial obstacle, i.e., the limited quantity of parallel sign-text\ndata. To tackle this parallel data bottleneck, we propose a sign\nback-translation (SignBT) approach, which incorporates massive spoken language\ntexts into SLT training. With a text-to-gloss translation model, we first\nback-translate the monolingual text to its gloss sequence. Then, the paired\nsign sequence is generated by splicing pieces from an estimated gloss-to-sign\nbank at the feature level. Finally, the synthetic parallel data serves as a\nstrong supplement for the end-to-end training of the encoder-decoder SLT\nframework.\n  To promote the SLT research, we further contribute CSL-Daily, a large-scale\ncontinuous SLT dataset. It provides both spoken language translations and\ngloss-level annotations. The topic revolves around people's daily lives (e.g.,\ntravel, shopping, medical care), the most likely SLT application scenario.\nExtensive experimental results and analysis of SLT methods are reported on\nCSL-Daily. With the proposed sign back-translation method, we obtain a\nsubstantial improvement over previous state-of-the-art SLT methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:49:30 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhou", "Hao", ""], ["Zhou", "Wengang", ""], ["Qi", "Weizhen", ""], ["Pu", "Junfu", ""], ["Li", "Houqiang", ""]]}, {"id": "2105.12405", "submitter": "Shilong Liu", "authors": "Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, Jun Zhu", "title": "Unsupervised Part Segmentation through Disentangling Appearance and\n  Shape", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of unsupervised discovery and segmentation of object\nparts, which, as an intermediate local representation, are capable of finding\nintrinsic object structure and providing more explainable recognition results.\nRecent unsupervised methods have greatly relaxed the dependency on annotated\ndata which are costly to obtain, but still rely on additional information such\nas object segmentation mask or saliency map. To remove such a dependency and\nfurther improve the part segmentation performance, we develop a novel approach\nby disentangling the appearance and shape representations of object parts\nfollowed with reconstruction losses without using additional object mask\ninformation. To avoid degenerated solutions, a bottleneck block is designed to\nsqueeze and expand the appearance representation, leading to a more effective\ndisentanglement between geometry and appearance. Combined with a\nself-supervised part classification loss and an improved geometry concentration\nconstraint, we can segment more consistent parts with semantic meanings.\nComprehensive experiments on a wide variety of objects such as face, bird, and\nPASCAL VOC objects demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:59:31 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Liu", "Shilong", ""], ["Zhang", "Lei", ""], ["Yang", "Xiao", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2105.12409", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Enrico Magli", "title": "Permutation invariance and uncertainty in multitemporal image\n  super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances have shown how deep neural networks can be extremely\neffective at super-resolving remote sensing imagery, starting from a\nmultitemporal collection of low-resolution images. However, existing models\nhave neglected the issue of temporal permutation, whereby the temporal ordering\nof the input images does not carry any relevant information for the\nsuper-resolution task and causes such models to be inefficient with the, often\nscarce, ground truth data that available for training. Thus, models ought not\nto learn feature extractors that rely on temporal ordering. In this paper, we\nshow how building a model that is fully invariant to temporal permutation\nsignificantly improves performance and data efficiency. Moreover, we study how\nto quantify the uncertainty of the super-resolved image so that the final user\nis informed on the local quality of the product. We show how uncertainty\ncorrelates with temporal variation in the series, and how quantifying it\nfurther improves model performance. Experiments on the Proba-V challenge\ndataset show significant improvements over the state of the art without the\nneed for self-ensembling, as well as improved data efficiency, reaching the\nperformance of the challenge winner with just 25% of the training data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:03:12 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Valsesia", "Diego", ""], ["Magli", "Enrico", ""]]}, {"id": "2105.12414", "submitter": "Basura Fernando", "authors": "Basura Fernando, Samitha Herath", "title": "Anticipating human actions by correlating past with the future with\n  Jaccard similarity measures", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework for early action recognition and anticipation by\ncorrelating past features with the future using three novel similarity measures\ncalled Jaccard vector similarity, Jaccard cross-correlation and Jaccard\nFrobenius inner product over covariances. Using these combinations of novel\nlosses and using our framework, we obtain state-of-the-art results for early\naction recognition in UCF101 and JHMDB datasets by obtaining 91.7 % and 83.5 %\naccuracy respectively for an observation percentage of 20. Similarly, we obtain\nstate-of-the-art results for Epic-Kitchen55 and Breakfast datasets for action\nanticipation by obtaining 20.35 and 41.8 top-1 accuracy respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:11:02 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Fernando", "Basura", ""], ["Herath", "Samitha", ""]]}, {"id": "2105.12430", "submitter": "Jiansheng Fang", "authors": "Jiansheng Fang, Yanwu Xu, Yitian Zhao, Yuguang Yan, Junling Liu and\n  Jiang Liu", "title": "Weighing Features of Lung and Heart Regions for Thoracic Disease\n  Classification", "comments": "17 pages, 4 figures, BMC Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays are the most commonly available and affordable radiological\nexamination for screening thoracic diseases. According to the domain knowledge\nof screening chest X-rays, the pathological information usually lay on the lung\nand heart regions. However, it is costly to acquire region-level annotation in\npractice, and model training mainly relies on image-level class labels in a\nweakly supervised manner, which is highly challenging for computer-aided chest\nX-ray screening. To address this issue, some methods have been proposed\nrecently to identify local regions containing pathological information, which\nis vital for thoracic disease classification. Inspired by this, we propose a\nnovel deep learning framework to explore discriminative information from lung\nand heart regions. We design a feature extractor equipped with a multi-scale\nattention module to learn global attention maps from global images. To exploit\ndisease-specific cues effectively, we locate lung and heart regions containing\npathological information by a well-trained pixel-wise segmentation model to\ngenerate binarization masks. By introducing element-wise logical AND operator\non the learned global attention maps and the binarization masks, we obtain\nlocal attention maps in which pixels are $1$ for lung and heart region and $0$\nfor other regions. By zeroing features of non-lung and heart regions in\nattention maps, we can effectively exploit their disease-specific cues in lung\nand heart regions. Compared to existing methods fusing global and local\nfeatures, we adopt feature weighting to avoid weakening visual cues unique to\nlung and heart regions. Evaluated by the benchmark split on the publicly\navailable chest X-ray14 dataset, the comprehensive experiments show that our\nmethod achieves superior performance compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:37:39 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Fang", "Jiansheng", ""], ["Xu", "Yanwu", ""], ["Zhao", "Yitian", ""], ["Yan", "Yuguang", ""], ["Liu", "Junling", ""], ["Liu", "Jiang", ""]]}, {"id": "2105.12434", "submitter": "Mohamad Wehbi", "authors": "Mohamad Wehbi, Tim Hamann, Jens Barth, Peter Kaempf, Dario Zanca, and\n  Bjoern Eskofier", "title": "Towards an IMU-based Pen Online Handwriting Recognizer", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most online handwriting recognition systems require the use of specific\nwriting surfaces to extract positional data. In this paper we present a online\nhandwriting recognition system for word recognition which is based on inertial\nmeasurement units (IMUs) for digitizing text written on paper. This is obtained\nby means of a sensor-equipped pen that provides acceleration, angular velocity,\nand magnetic forces streamed via Bluetooth. Our model combines convolutional\nand bidirectional LSTM networks, and is trained with the Connectionist Temporal\nClassification loss that allows the interpretation of raw sensor data into\nwords without the need of sequence segmentation. We use a dataset of words\ncollected using multiple sensor-enhanced pens and evaluate our model on\ndistinct test sets of seen and unseen words achieving a character error rate of\n17.97% and 17.08%, respectively, without the use of a dictionary or language\nmodel\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:47:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wehbi", "Mohamad", ""], ["Hamann", "Tim", ""], ["Barth", "Jens", ""], ["Kaempf", "Peter", ""], ["Zanca", "Dario", ""], ["Eskofier", "Bjoern", ""]]}, {"id": "2105.12436", "submitter": "Chi Zhang", "authors": "Chi Zhang (1), Christian Berger (1), Marco Dozza (2) ((1) Department\n  of Computer Science and Engineering, University of Gothenburg, Gothenburg,\n  Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers\n  University of Technology, Gothenburg, Sweden)", "title": "Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal\n  Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban\n  Traffic Scenarios", "comments": "8 pages, 4 figures. Accepted in IEEE Intelligent Vehicles Symposium\n  (IV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrian trajectory prediction in urban scenarios is essential for\nautomated driving. This task is challenging because the behavior of pedestrians\nis influenced by both their own history paths and the interactions with others.\nPrevious research modeled these interactions with pooling mechanisms or\naggregating with hand-crafted attention weights. In this paper, we present the\nSocial Interaction-Weighted Spatio-Temporal Convolutional Neural Network\n(Social-IWSTCNN), which includes both the spatial and the temporal features. We\npropose a novel design, namely the Social Interaction Extractor, to learn the\nspatial and social interaction features of pedestrians. Most previous works\nused ETH and UCY datasets which include five scenes but do not cover urban\ntraffic scenarios extensively for training and evaluation. In this paper, we\nuse the recently released large-scale Waymo Open Dataset in urban traffic\nscenarios, which includes 374 urban training scenes and 76 urban testing scenes\nto analyze the performance of our proposed algorithm in comparison to the\nstate-of-the-art (SOTA) models. The results show that our algorithm outperforms\nSOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both\nAverage Displacement Error (ADE) and Final Displacement Error (FDE).\nFurthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing\nspeed, and 4.7 times faster in total test speed than the current best SOTA\nalgorithm Social-STGCNN.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:53:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhang", "Chi", ""], ["Berger", "Christian", ""], ["Dozza", "Marco", ""]]}, {"id": "2105.12441", "submitter": "Akis Linardos", "authors": "Akis Linardos, Matthias K\\\"ummerer, Ori Press, Matthias Bethge", "title": "Calibrated prediction in and out-of-domain for state-of-the-art saliency\n  modeling", "comments": "Joint first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since 2014 transfer learning has become the key driver for the improvement of\nspatial saliency prediction; however, with stagnant progress in the last 3-5\nyears. We conduct a large-scale transfer learning study which tests different\nImageNet backbones, always using the same read out architecture and learning\nprotocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze\nII with ResNet50 features we improve the performance on saliency prediction\nfrom 78% to 85%. However, as we continue to test better ImageNet models as\nbackbones (such as EfficientNetB5) we observe no additional improvement on\nsaliency prediction. By analyzing the backbones further, we find that\ngeneralization to other datasets differs substantially, with models being\nconsistently overconfident in their fixation predictions. We show that by\ncombining multiple backbones in a principled manner a good confidence\ncalibration on unseen datasets can be achieved. This yields a significant leap\nin benchmark performance in and out-of-domain with a 15 percent point\nimprovement over DeepGaze II to 93% on MIT1003, marking a new state of the art\non the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%,\nsAUC: 79.4%, CC: 82.4%).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:59:56 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 15:21:50 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Linardos", "Akis", ""], ["K\u00fcmmerer", "Matthias", ""], ["Press", "Ori", ""], ["Bethge", "Matthias", ""]]}, {"id": "2105.12479", "submitter": "Celia Cintas", "authors": "Celia Cintas, Skyler Speakman, Girmaw Abebe Tadesse, Victor Akinwande,\n  Edward McFowland III, Komminist Weldemariam", "title": "Pattern Detection in the Activation Space for Identifying Synthesized\n  Content", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently achieved unprecedented\nsuccess in photo-realistic image synthesis from low-dimensional random noise.\nThe ability to synthesize high-quality content at a large scale brings\npotential risks as the generated samples may lead to misinformation that can\ncreate severe social, political, health, and business hazards. We propose\nSubsetGAN to identify generated content by detecting a subset of anomalous\nnode-activations in the inner layers of pre-trained neural networks. These\nnodes, as a group, maximize a non-parametric measure of divergence away from\nthe expected distribution of activations created from real data. This enable us\nto identify synthesised images without prior knowledge of their distribution.\nSubsetGAN efficiently scores subsets of nodes and returns the group of nodes\nwithin the pre-trained classifier that contributed to the maximum score. The\nclassifier can be a general fake classifier trained over samples from multiple\nsources or the discriminator network from different GANs. Our approach shows\nconsistently higher detection power than existing detection methods across\nseveral state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\nproportions of generated content.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:28:36 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 08:40:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cintas", "Celia", ""], ["Speakman", "Skyler", ""], ["Tadesse", "Girmaw Abebe", ""], ["Akinwande", "Victor", ""], ["McFowland", "Edward", "III"], ["Weldemariam", "Komminist", ""]]}, {"id": "2105.12508", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "Adversarial robustness against multiple $l_p$-threat models at the price\n  of one and how to quickly fine-tune robust models to another threat model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) in order to achieve adversarial robustness wrt\nsingle $l_p$-threat models has been discussed extensively. However, for\nsafety-critical systems adversarial robustness should be achieved wrt all\n$l_p$-threat models simultaneously. In this paper we develop a simple and\nefficient training scheme to achieve adversarial robustness against the union\nof $l_p$-threat models. Our novel $l_1+l_\\infty$-AT scheme is based on\ngeometric considerations of the different $l_p$-balls and costs as much as\nnormal adversarial training against a single $l_p$-threat model. Moreover, we\nshow that using our $l_1+l_\\infty$-AT scheme one can fine-tune with just 3\nepochs any $l_p$-robust model (for $p \\in \\{1,2,\\infty\\}$) and achieve multiple\nnorm adversarial robustness. In this way we boost the previous state-of-the-art\nreported for multiple-norm robustness by more than $6\\%$ on CIFAR-10 and report\nup to our knowledge the first ImageNet models with multiple norm robustness.\nMoreover, we study the general transfer of adversarial robustness between\ndifferent threat models and in this way boost the previous SOTA\n$l_1$-robustness on CIFAR-10 by almost $10\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:20:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "2105.12532", "submitter": "Sherzod Hakimov", "authors": "Hussain Kanafani, Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth", "title": "Unsupervised Video Summarization via Multi-source Features", "comments": "Accepted for publication at the ACM International Conference on\n  Multimedia Retrieval (ICMR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization aims at generating a compact yet representative visual\nsummary that conveys the essence of the original video. The advantage of\nunsupervised approaches is that they do not require human annotations to learn\nthe summarization capability and generalize to a wider range of domains.\nPrevious work relies on the same type of deep features, typically based on a\nmodel pre-trained on ImageNet data. Therefore, we propose the incorporation of\nmultiple feature sources with chunk and stride fusion to provide more\ninformation about the visual content. For a comprehensive evaluation on the two\nbenchmarks TVSum and SumMe, we compare our method with four state-of-the-art\napproaches. Two of these approaches were implemented by ourselves to reproduce\nthe reported results. Our evaluation shows that we obtain state-of-the-art\nresults on both datasets, while also highlighting the shortcomings of previous\nwork with regard to the evaluation methodology. Finally, we perform error\nanalysis on videos for the two benchmark datasets to summarize and spot the\nfactors that lead to misclassifications.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:12:46 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kanafani", "Hussain", ""], ["Ghauri", "Junaid Ahmed", ""], ["Hakimov", "Sherzod", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2105.12549", "submitter": "Artem Savkin", "authors": "Artem Savkin and Federico Tombari", "title": "KLIEP-based Density Ratio Estimation for Semantically Consistent\n  Synthetic to Real Images Adaptation in Urban Traffic Scenes", "comments": null, "journal-ref": "2020 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS) (2020) 5901-5908", "doi": "10.1109/IROS45743.2020.9341547", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data has been applied in many deep learning based computer vision\ntasks. Limited performance of algorithms trained solely on synthetic data has\nbeen approached with domain adaptation techniques such as the ones based on\ngenerative adversarial framework. We demonstrate how adversarial training alone\ncan introduce semantic inconsistencies in translated images. To tackle this\nissue we propose density prematching strategy using KLIEP-based density ratio\nestimation procedure. Finally, we show that aforementioned strategy improves\nquality of translated images of underlying method and their usability for the\nsemantic segmentation task in the context of autonomous driving.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:59:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Savkin", "Artem", ""], ["Tombari", "Federico", ""]]}, {"id": "2105.12555", "submitter": "Tao Zhou", "authors": "Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, Nian Liu", "title": "Context-aware Cross-level Fusion Network for Camouflaged Object\n  Detection", "comments": "7 pages, 4 figures. Accepted by IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camouflaged object detection (COD) is a challenging task due to the low\nboundary contrast between the object and its surroundings. In addition, the\nappearance of camouflaged objects varies significantly, e.g., object size and\nshape, aggravating the difficulties of accurate COD. In this paper, we propose\na novel Context-aware Cross-level Fusion Network (C2F-Net) to address the\nchallenging COD task. Specifically, we propose an Attention-induced Cross-level\nFusion Module (ACFM) to integrate the multi-level features with informative\nattention coefficients. The fused features are then fed to the proposed\nDual-branch Global Context Module (DGCM), which yields multi-scale feature\nrepresentations for exploiting rich global context information. In C2F-Net, the\ntwo modules are conducted on high-level features using a cascaded manner.\nExtensive experiments on three widely used benchmark datasets demonstrate that\nour C2F-Net is an effective COD model and outperforms state-of-the-art models\nremarkably. Our code is publicly available at:\nhttps://github.com/thograce/C2FNet.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:03:36 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Sun", "Yujia", ""], ["Chen", "Geng", ""], ["Zhou", "Tao", ""], ["Zhang", "Yi", ""], ["Liu", "Nian", ""]]}, {"id": "2105.12564", "submitter": "Rushabh Patel", "authors": "Rushabh Patel", "title": "Predicting invasive ductal carcinoma using a Reinforcement Sample\n  Learning Strategy using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive ductal carcinoma is a prevalent, potentially deadly disease\nassociated with a high rate of morbidity and mortality. Its malignancy is the\nsecond leading cause of death from cancer in women. The mammogram is an\nextremely useful resource for mass detection and invasive ductal carcinoma\ndiagnosis. We are proposing a method for Invasive ductal carcinoma that will\nuse convolutional neural networks (CNN) on mammograms to assist radiologists in\ndiagnosing the disease. Due to the varying image clarity and structure of\ncertain mammograms, it is difficult to observe major cancer characteristics\nsuch as microcalcification and mass, and it is often difficult to interpret and\ndiagnose these attributes. The aim of this study is to establish a novel method\nfor fully automated feature extraction and classification in invasive ductal\ncarcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor\nclassification algorithm that makes novel use of convolutional neural networks\non breast mammogram images to increase feature extraction and training speed.\nThe algorithm makes two contributions.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:14:45 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Patel", "Rushabh", ""]]}, {"id": "2105.12628", "submitter": "Yujia Bao", "authors": "Yujia Bao, Shiyu Chang, Regina Barzilay", "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:37:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bao", "Yujia", ""], ["Chang", "Shiyu", ""], ["Barzilay", "Regina", ""]]}, {"id": "2105.12633", "submitter": "Joshua Abraham", "authors": "Joshua Abraham, Calden Wloka", "title": "Edge Detection for Satellite Images without Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite imagery is widely used in many application sectors, including\nagriculture, navigation, and urban planning. Frequently, satellite imagery\ninvolves both large numbers of images as well as high pixel counts, making\nsatellite datasets computationally expensive to analyze. Recent approaches to\nsatellite image analysis have largely emphasized deep learning methods. Though\nextremely powerful, deep learning has some drawbacks, including the requirement\nof specialized computing hardware and a high reliance on training data. When\ndealing with large satellite datasets, the cost of both computational resources\nand training data annotation may be prohibitive.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:47:42 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Abraham", "Joshua", ""], ["Wloka", "Calden", ""]]}, {"id": "2105.12639", "submitter": "Namuk Park", "authors": "Namuk Park, Songkuk Kim", "title": "Blurs Make Results Clearer: Spatial Smoothings to Improve Accuracy,\n  Uncertainty, and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian neural networks (BNNs) have shown success in the areas of\nuncertainty estimation and robustness. However, a crucial challenge prohibits\ntheir use in practice: Bayesian NNs require a large number of predictions to\nproduce reliable results, leading to a significant increase in computational\ncost. To alleviate this issue, we propose spatial smoothing, a method that\nensembles neighboring feature map points of CNNs. By simply adding a few blur\nlayers to the models, we empirically show that the spatial smoothing improves\naccuracy, uncertainty estimation, and robustness of BNNs across a whole range\nof ensemble sizes. In particular, BNNs incorporating the spatial smoothing\nachieve high predictive performance merely with a handful of ensembles.\nMoreover, this method also can be applied to canonical deterministic neural\nnetworks to improve the performances. A number of evidences suggest that the\nimprovements can be attributed to the smoothing and flattening of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing to\nthem as special cases of the spatial smoothing. These not only enhance\naccuracy, but also improve uncertainty estimation and robustness by making the\nloss landscape smoother in the same manner as the spatial smoothing. The code\nis available at https://github.com/xxxnell/spatial-smoothing.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:58:11 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Park", "Namuk", ""], ["Kim", "Songkuk", ""]]}, {"id": "2105.12653", "submitter": "Wen Gao", "authors": "Wen Gao, Shan Liu, Xiaozhong Xu, Manouchehr Rafie, Yuan Zhang, Igor\n  Curcio", "title": "Recent Standard Development Activities on Video Coding for Machines", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, video data has dominated internet traffic and becomes one of\nthe major data formats. With the emerging 5G and internet of things (IoT)\ntechnologies, more and more videos are generated by edge devices, sent across\nnetworks, and consumed by machines. The volume of video consumed by machine is\nexceeding the volume of video consumed by humans. Machine vision tasks include\nobject detection, segmentation, tracking, and other machine-based applications,\nwhich are quite different from those for human consumption. On the other hand,\ndue to large volumes of video data, it is essential to compress video before\ntransmission. Thus, efficient video coding for machines (VCM) has become an\nimportant topic in academia and industry. In July 2019, the international\nstandardization organization, i.e., MPEG, created an Ad-Hoc group named VCM to\nstudy the requirements for potential standardization work. In this paper, we\nwill address the recent development activities in the MPEG VCM group.\nSpecifically, we will first provide an overview of the MPEG VCM group including\nuse cases, requirements, processing pipelines, plan for potential VCM\nstandards, followed by the evaluation framework including machine-vision tasks,\ndataset, evaluation metrics, and anchor generation. We then introduce\ntechnology solutions proposed so far and discuss the recent responses to the\nCall for Evidence issued by MPEG VCM group.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:11:11 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gao", "Wen", ""], ["Liu", "Shan", ""], ["Xu", "Xiaozhong", ""], ["Rafie", "Manouchehr", ""], ["Zhang", "Yuan", ""], ["Curcio", "Igor", ""]]}, {"id": "2105.12660", "submitter": "Yuxuan Han", "authors": "Yuxuan Han, Jiaolong Yang, and Ying Fu", "title": "Disentangled Face Attribute Editing via Instance-Aware Latent Space\n  Search", "comments": "Accepted by IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have shown that a rich set of semantic directions exist in the\nlatent space of Generative Adversarial Networks (GANs), which enables various\nfacial attribute editing applications. However, existing methods may suffer\npoor attribute variation disentanglement, leading to unwanted change of other\nattributes when altering the desired one. The semantic directions used by\nexisting methods are at attribute level, which are difficult to model complex\nattribute correlations, especially in the presence of attribute distribution\nbias in GAN's training set. In this paper, we propose a novel framework (IALS)\nthat performs Instance-Aware Latent-Space Search to find semantic directions\nfor disentangled attribute editing. The instance information is injected by\nleveraging the supervision from a set of attribute classifiers evaluated on the\ninput images. We further propose a Disentanglement-Transformation (DT) metric\nto quantify the attribute transformation and disentanglement efficacy and find\nthe optimal control factor between attribute-level and instance-specific\ndirections based on it. Experimental results on both GAN-generated and\nreal-world images collectively show that our method outperforms\nstate-of-the-art methods proposed recently by a wide margin. Code is available\nat https://github.com/yxuhan/IALS.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:19:08 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:24:35 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Han", "Yuxuan", ""], ["Yang", "Jiaolong", ""], ["Fu", "Ying", ""]]}, {"id": "2105.12661", "submitter": "Richard Wildes", "authors": "Soo Min Kang and Richard P. Wildes", "title": "Detecting Biological Locomotion in Video: A Computational Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animals locomote for various reasons: to search for food, find suitable\nhabitat, pursue prey, escape from predators, or seek a mate. The grand scale of\nbiodiversity contributes to the great locomotory design and mode diversity.\nVarious creatures make use of legs, wings, fins and other means to move through\nthe world. In this report, we refer to the locomotion of general biological\nspecies as biolocomotion. We present a computational approach to detect\nbiolocomotion in unprocessed video.\n  Significantly, the motion exhibited by the body parts of a biological entity\nto navigate through an environment can be modeled by a combination of an\noverall positional advance with an overlaid asymmetric oscillatory pattern, a\ndistinctive signature that tends to be absent in non-biological objects in\nlocomotion. We exploit this key trait of positional advance with asymmetric\noscillation along with differences in an object's common motion (extrinsic\nmotion) and localized motion of its parts (intrinsic motion) to detect\nbiolocomotion. An algorithm is developed to measure the presence of these\ntraits in tracked objects to determine if they correspond to a biological\nentity in locomotion. An alternative algorithm, based on generic features\ncombined with learning is assembled out of components from allied areas of\ninvestigation, also is presented as a basis of comparison.\n  A novel biolocomotion dataset encompassing a wide range of moving biological\nand non-biological objects in natural settings is provided. Also, biolocomotion\nannotations to an extant camouflage animals dataset are provided. Quantitative\nresults indicate that the proposed algorithm considerably outperforms the\nalternative approach, supporting the hypothesis that biolocomotion can be\ndetected reliably based on its distinct signature of positional advance with\nasymmetric oscillation and extrinsic/intrinsic motion dissimilarity.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:19:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kang", "Soo Min", ""], ["Wildes", "Richard P.", ""]]}, {"id": "2105.12684", "submitter": "Guoqing Zhang", "authors": "Guoqing Zhang, Yuhao Chen, Weisi Lin, Arun Chandran, Xuan Jing", "title": "Low Resolution Information Also Matters: Learning Multi-Resolution\n  Representations for Person Re-Identification", "comments": "accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a prevailing task in video surveillance and forensics field, person\nre-identification (re-ID) aims to match person images captured from\nnon-overlapped cameras. In unconstrained scenarios, person images often suffer\nfrom the resolution mismatch problem, i.e., \\emph{Cross-Resolution Person\nRe-ID}. To overcome this problem, most existing methods restore low resolution\n(LR) images to high resolution (HR) by super-resolution (SR). However, they\nonly focus on the HR feature extraction and ignore the valid information from\noriginal LR images. In this work, we explore the influence of resolutions on\nfeature extraction and develop a novel method for cross-resolution person re-ID\ncalled \\emph{\\textbf{M}ulti-Resolution \\textbf{R}epresentations \\textbf{J}oint\n\\textbf{L}earning} (\\textbf{MRJL}). Our method consists of a Resolution\nReconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN\nuses an input image to construct a HR version and a LR version with an encoder\nand two decoders, while the DFFN adopts a dual-branch structure to generate\nperson representations from multi-resolution images. Comprehensive experiments\non five benchmarks verify the superiority of the proposed MRJL over the\nrelevent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:54:56 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhang", "Guoqing", ""], ["Chen", "Yuhao", ""], ["Lin", "Weisi", ""], ["Chandran", "Arun", ""], ["Jing", "Xuan", ""]]}, {"id": "2105.12686", "submitter": "Lizeth Gonzalez Carabarin", "authors": "Lizeth Gonzalez-Carabarin, Iris A.M. Huijben, Bastiaan S. Veeling,\n  Alexandre Schmid, Ruud J.G. van Sloun", "title": "Dynamic Probabilistic Pruning: A general framework for\n  hardware-constrained pruning at different granularities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured neural network pruning algorithms have achieved impressive\ncompression rates. However, the resulting - typically irregular - sparse\nmatrices hamper efficient hardware implementations, leading to additional\nmemory usage and complex control logic that diminishes the benefits of\nunstructured pruning. This has spurred structured coarse-grained pruning\nsolutions that prune entire filters or even layers, enabling efficient\nimplementation at the expense of reduced flexibility. Here we propose a\nflexible new pruning mechanism that facilitates pruning at different\ngranularities (weights, kernels, filters/feature maps), while retaining\nefficient memory organization (e.g. pruning exactly k-out-of-n weights for\nevery output neuron, or pruning exactly k-out-of-n kernels for every feature\nmap). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP\nleverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling,\nfacilitating end-to-end optimization. We show that DPP achieves competitive\ncompression rates and classification accuracy when pruning common deep learning\nmodels trained on different benchmark datasets for image classification.\nRelevantly, the non-magnitude-based nature of DPP allows for joint optimization\nof pruning and weight quantization in order to even further compress the\nnetwork, which we show as well. Finally, we propose novel information theoretic\nmetrics that show the confidence and pruning diversity of pruning masks within\na layer.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:01:52 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gonzalez-Carabarin", "Lizeth", ""], ["Huijben", "Iris A. M.", ""], ["Veeling", "Bastiaan S.", ""], ["Schmid", "Alexandre", ""], ["van Sloun", "Ruud J. G.", ""]]}, {"id": "2105.12691", "submitter": "Rui Pimentel De Figueiredo", "authors": "Rui Pimentel de Figueiredo, Jakob Grimm Hansen, Jonas Le Fevre, Martim\n  Brand\\~ao, Erdal Kayacan", "title": "On the Advantages of Multiple Stereo Vision Camera Designs for\n  Autonomous Drone Navigation", "comments": null, "journal-ref": "in ICRA workshop on Resilient and Long-Term Autonomy for Aerial\n  Robotic Systems, 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we showcase the design and assessment of the performance of a\nmulti-camera UAV, when coupled with state-of-the-art planning and mapping\nalgorithms for autonomous navigation. The system leverages state-of-the-art\nreceding horizon exploration techniques for Next-Best-View (NBV) planning with\n3D and semantic information, provided by a reconfigurable multi stereo camera\nsystem. We employ our approaches in an autonomous drone-based inspection task\nand evaluate them in an autonomous exploration and mapping scenario. We discuss\nthe advantages and limitations of using multi stereo camera flying systems, and\nthe trade-off between number of cameras and mapping performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:10:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["de Figueiredo", "Rui Pimentel", ""], ["Hansen", "Jakob Grimm", ""], ["Fevre", "Jonas Le", ""], ["Brand\u00e3o", "Martim", ""], ["Kayacan", "Erdal", ""]]}, {"id": "2105.12694", "submitter": "Feifei Shao", "authors": "Feifei Shao, Long Chen, Jian Shao, Wei Ji, Shaoning Xiao, Lu Ye,\n  Yueting Zhuang, Jun Xiao", "title": "Deep Learning for Weakly-Supervised Object Detection and Object\n  Localization: A Survey", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e.,\ndetecting multiple and single instances with bounding boxes in an image using\nimage-level labels, are long-standing and challenging tasks in the CV\ncommunity. With the success of deep neural networks in object detection, both\nWSOD and WSOL have received unprecedented attention. Hundreds of WSOD and WSOL\nmethods and numerous techniques have been proposed in the deep learning era. To\nthis end, in this paper, we consider WSOL is a sub-task of WSOD and provide a\ncomprehensive survey of the recent achievements of WSOD. Specifically, we\nfirstly describe the formulation and setting of the WSOD, including the\nbackground, challenges, basic framework. Meanwhile, we summarize and analyze\nall advanced techniques and training tricks for improving detection\nperformance. Then, we introduce the widely-used datasets and evaluation metrics\nof WSOD. Lastly, we discuss the future directions of WSOD. We believe that\nthese summaries can help pave a way for future research on WSOD and WSOL.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:15:53 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Shao", "Feifei", ""], ["Chen", "Long", ""], ["Shao", "Jian", ""], ["Ji", "Wei", ""], ["Xiao", "Shaoning", ""], ["Ye", "Lu", ""], ["Zhuang", "Yueting", ""], ["Xiao", "Jun", ""]]}, {"id": "2105.12700", "submitter": "Luka Murn", "authors": "Luka Murn, Marc Gorriz Blanch, Maria Santamaria, Fiona Rivera, Marta\n  Mrak", "title": "Towards Transparent Application of Machine Learning in Video Processing", "comments": "International Broadcasting Convention, 11-14 Sep 2020, Amsterdam,\n  Netherlands (Technical Paper section, Virtual)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:24:23 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:35:54 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Murn", "Luka", ""], ["Blanch", "Marc Gorriz", ""], ["Santamaria", "Maria", ""], ["Rivera", "Fiona", ""], ["Mrak", "Marta", ""]]}, {"id": "2105.12710", "submitter": "Mohamed Ali Souibgui", "authors": "Sana Khamekhem Jemni and Mohamed Ali Souibgui and Yousri Kessentini\n  and Alicia Forn\\'es", "title": "Enhance to Read Better: An Improved Generative Adversarial Network for\n  Handwritten Document Image Enhancement", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten document images can be highly affected by degradation for\ndifferent reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.),\nbad scanning process and so on. These artifacts raise many readability issues\nfor current Handwritten Text Recognition (HTR) algorithms and severely devalue\ntheir efficiency. In this paper, we propose an end to end architecture based on\nGenerative Adversarial Networks (GANs) to recover the degraded documents into a\nclean and readable form. Unlike the most well-known document binarization\nmethods, which try to improve the visual quality of the degraded document, the\nproposed architecture integrates a handwritten text recognizer that promotes\nthe generated document image to be more readable. To the best of our knowledge,\nthis is the first work to use the text information while binarizing handwritten\ndocuments. Extensive experiments conducted on degraded Arabic and Latin\nhandwritten documents demonstrate the usefulness of integrating the recognizer\nwithin the GAN architecture, which improves both the visual quality and the\nreadability of the degraded document images. Moreover, we outperform the state\nof the art in H-DIBCO 2018 challenge, after fine tuning our pre-trained model\nwith synthetically degraded Latin handwritten images, on this task.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:44:45 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jemni", "Sana Khamekhem", ""], ["Souibgui", "Mohamed Ali", ""], ["Kessentini", "Yousri", ""], ["Forn\u00e9s", "Alicia", ""]]}, {"id": "2105.12713", "submitter": "Senthil Yogamani", "authors": "Kinjal Dasgupta, Arindam Das, Sudip Das, Ujjwal Bhattacharya and\n  Senthil Yogamani", "title": "Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian Detection is the most critical module of an Autonomous Driving\nsystem. Although a camera is commonly used for this purpose, its quality\ndegrades severely in low-light night time driving scenarios. On the other hand,\nthe quality of a thermal camera image remains unaffected in similar conditions.\nThis paper proposes an end-to-end multimodal fusion model for pedestrian\ndetection using RGB and thermal images. Its novel spatio-contextual deep\nnetwork architecture is capable of exploiting the multimodal input efficiently.\nIt consists of two distinct deformable ResNeXt-50 encoders for feature\nextraction from the two modalities. Fusion of these two encoded features takes\nplace inside a multimodal feature embedding module (MuFEm) consisting of\nseveral groups of a pair of Graph Attention Network and a feature fusion unit.\nThe output of the last feature fusion unit of MuFEm is subsequently passed to\ntwo CRFs for their spatial refinement. Further enhancement of the features is\nachieved by applying channel-wise attention and extraction of contextual\ninformation with the help of four RNNs traversing in four different directions.\nFinally, these feature maps are used by a single-stage decoder to generate the\nbounding box of each pedestrian and the score map. We have performed extensive\nexperiments of the proposed framework on three publicly available multimodal\npedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The\nresults on each of them improved the respective state-of-the-art performance. A\nshort video giving an overview of this work along with its qualitative results\ncan be seen at https://youtu.be/FDJdSifuuCs.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:50:36 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Dasgupta", "Kinjal", ""], ["Das", "Arindam", ""], ["Das", "Sudip", ""], ["Bhattacharya", "Ujjwal", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2105.12722", "submitter": "Pak Hei Yeung", "authors": "Pak-Hei Yeung, Ana I.L. Namburete, Weidi Xie", "title": "Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised\n  Learning", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is to segment any arbitrary structures of interest\n(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D\nsegmentation). We show that high accuracy can be achieved by simply propagating\nthe 2D slice segmentation with an affinity matrix between consecutive slices,\nwhich can be learnt in a self-supervised manner, namely slice reconstruction.\nSpecifically, we compare the proposed framework, termed as Sli2Vol, with\nsupervised approaches and two other unsupervised/ self-supervised slice\nregistration approaches, on 8 public datasets (both CT and MRI scans), spanning\n9 different SOIs. Without any parameter-tuning, the same model achieves\nsuperior performance with Dice scores (0-100 scale) of over 80 for most of the\nbenchmarks, including the ones that are unseen during training. Our results\nshow generalizability of the proposed approach across data from different\nmachines and with different SOIs: a major use case of semi-automatic\nsegmentation methods where fully supervised approaches would normally struggle.\nThe source code will be made publicly available at\nhttps://github.com/pakheiyeung/Sli2Vol.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:56:39 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 18:28:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Yeung", "Pak-Hei", ""], ["Namburete", "Ana I. L.", ""], ["Xie", "Weidi", ""]]}, {"id": "2105.12723", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Tomas Pfister", "title": "Aggregating Nested Transformers", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although hierarchical structures are popular in recent vision transformers,\nthey require sophisticated designs and massive datasets to work well. In this\nwork, we explore the idea of nesting basic local transformers on\nnon-overlapping image blocks and aggregating them in a hierarchical manner. We\nfind that the block aggregation function plays a critical role in enabling\ncross-block non-local information communication. This observation leads us to\ndesign a simplified architecture with minor code changes upon the original\nvision transformer and obtains improved performance compared to existing\nmethods. Our empirical results show that the proposed method NesT converges\nfaster and requires much less training data to achieve good generalization. For\nexample, a NesT with 68M parameters trained on ImageNet for 100/300 epochs\nachieves $82.3\\%/83.8\\%$ accuracy evaluated on $224\\times 224$ image size,\noutperforming previous methods with up to $57\\%$ parameter reduction. Training\na NesT with 6M parameters from scratch on CIFAR10 achieves $96\\%$ accuracy\nusing a single GPU, setting a new state of the art for vision transformers.\nBeyond image classification, we extend the key idea to image generation and\nshow NesT leads to a strong decoder that is 8$\\times$ faster than previous\ntransformer based generators. Furthermore, we also propose a novel method for\nvisually interpreting the learned model. Source code is available\nhttps://github.com/google-research/nested-transformer.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:56:48 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 02:36:02 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Zizhao", ""], ["Zhang", "Han", ""], ["Zhao", "Long", ""], ["Chen", "Ting", ""], ["Pfister", "Tomas", ""]]}, {"id": "2105.12724", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yuhang Hu, Lianfeng Li, Sara Cummings, Hod Lipson", "title": "Smile Like You Mean It: Driving Animatronic Robotic Face with Learned\n  Models", "comments": "ICRA 2021. Website:http://www.cs.columbia.edu/~bchen/aiface/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ability to generate intelligent and generalizable facial expressions is\nessential for building human-like social robots. At present, progress in this\nfield is hindered by the fact that each facial expression needs to be\nprogrammed by humans. In order to adapt robot behavior in real time to\ndifferent situations that arise when interacting with human subjects, robots\nneed to be able to train themselves without requiring human labels, as well as\nmake fast action decisions and generalize the acquired knowledge to diverse and\nnew contexts. We addressed this challenge by designing a physical animatronic\nrobotic face with soft skin and by developing a vision-based self-supervised\nlearning framework for facial mimicry. Our algorithm does not require any\nknowledge of the robot's kinematic model, camera calibration or predefined\nexpression set. By decomposing the learning process into a generative model and\nan inverse model, our framework can be trained using a single motor babbling\ndataset. Comprehensive evaluations show that our method enables accurate and\ndiverse face mimicry across diverse human subjects. The project website is at\nhttp://www.cs.columbia.edu/~bchen/aiface/\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:57:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Boyuan", ""], ["Hu", "Yuhang", ""], ["Li", "Lianfeng", ""], ["Cummings", "Sara", ""], ["Lipson", "Hod", ""]]}, {"id": "2105.12754", "submitter": "Margot Hanley", "authors": "Margot Hanley, Solon Barocas, Karen Levy, Shiri Azenkot, Helen\n  Nissenbaum", "title": "Computer Vision and Conflicting Values: Describing People with Automated\n  Alt Text", "comments": null, "journal-ref": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES '21)", "doi": "10.1145/3461702.3462620", "report-no": null, "categories": "cs.CY cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scholars have recently drawn attention to a range of controversial issues\nposed by the use of computer vision for automatically generating descriptions\nof people in images. Despite these concerns, automated image description has\nbecome an important tool to ensure equitable access to information for blind\nand low vision people. In this paper, we investigate the ethical dilemmas faced\nby companies that have adopted the use of computer vision for producing alt\ntext: textual descriptions of images for blind and low vision people, We use\nFacebook's automatic alt text tool as our primary case study. First, we analyze\nthe policies that Facebook has adopted with respect to identity categories,\nsuch as race, gender, age, etc., and the company's decisions about whether to\npresent these terms in alt text. We then describe an alternative -- and manual\n-- approach practiced in the museum community, focusing on how museums\ndetermine what to include in alt text descriptions of cultural artifacts. We\ncompare these policies, using notable points of contrast to develop an analytic\nframework that characterizes the particular apprehensions behind these policy\nchoices. We conclude by considering two strategies that seem to sidestep some\nof these concerns, finding that there are no easy ways to avoid the normative\ndilemmas posed by the use of computer vision to automate alt text.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:01:16 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hanley", "Margot", ""], ["Barocas", "Solon", ""], ["Levy", "Karen", ""], ["Azenkot", "Shiri", ""], ["Nissenbaum", "Helen", ""]]}, {"id": "2105.12763", "submitter": "Senthil Yogamani", "authors": "Ashok Dahal, Varun Ravi Kumar, Senthil Yogamani and Ciaran Eising", "title": "An Online Learning System for Wireless Charging Alignment using\n  Surround-view Fisheye Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric Vehicles are increasingly common, with inductive chargepads being\nconsidered a convenient and efficient means of charging electric vehicles.\nHowever, drivers are typically poor at aligning the vehicle to the necessary\naccuracy for efficient inductive charging, making the automated alignment of\nthe two charging plates desirable. In parallel to the electrification of the\nvehicular fleet, automated parking systems that make use of surround-view\ncamera systems are becoming increasingly popular. In this work, we propose a\nsystem based on the surround-view camera architecture to detect, localize and\nautomatically align the vehicle with the inductive chargepad. The visual design\nof the chargepads is not standardized and not necessarily known beforehand.\nTherefore a system that relies on offline training will fail in some\nsituations. Thus we propose an online learning method that leverages the\ndriver's actions when manually aligning the vehicle with the chargepad and\ncombine it with weak supervision from semantic segmentation and depth to learn\na classifier to auto-annotate the chargepad in the video for further training.\nIn this way, when faced with a previously unseen chargepad, the driver needs\nonly manually align the vehicle a single time. As the chargepad is flat on the\nground, it is not easy to detect it from a distance. Thus, we propose using a\nVisual SLAM pipeline to learn landmarks relative to the chargepad to enable\nalignment from a greater range. We demonstrate the working system on an\nautomated vehicle as illustrated in the video https://youtu.be/_cLCmkW4UYo. To\nencourage further research, we will share a chargepad dataset used in this\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:02:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dahal", "Ashok", ""], ["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""], ["Eising", "Ciaran", ""]]}, {"id": "2105.12774", "submitter": "Sabyasachi Sahoo", "authors": "Prashant Kumar, Sabyasachi Sahoo, Vanshil Shah, Vineetha Kondameedi,\n  Abhinav Jain, Akshaj Verma, Chiranjib Bhattacharyya, Vinay Viswanathan", "title": "DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially\n  Trained Autoencoder", "comments": "17 pages, 15 figures, Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate reconstruction of static environments from LiDAR scans of scenes\ncontaining dynamic objects, which we refer to as Dynamic to Static Translation\n(DST), is an important area of research in Autonomous Navigation. This problem\nhas been recently explored for visual SLAM, but to the best of our knowledge no\nwork has been attempted to address DST for LiDAR scans. The problem is of\ncritical importance due to wide-spread adoption of LiDAR in Autonomous\nVehicles. We show that state-of the art methods developed for the visual domain\nwhen adapted for LiDAR scans perform poorly.\n  We develop DSLR, a deep generative model which learns a mapping between\ndynamic scan to its static counterpart through an adversarially trained\nautoencoder. Our model yields the first solution for DST on LiDAR that\ngenerates static scans without using explicit segmentation labels. DSLR cannot\nalways be applied to real world data due to lack of paired dynamic-static\nscans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer\nto real world data and experimentally show that this performs well in real\nworld settings. Additionally, if segmentation information is available, we\nextend DSLR to DSLR-Seg to further improve the reconstruction quality.\n  DSLR gives the state of the art performance on simulated and real-world\ndatasets and also shows at least 4x improvement. We show that DSLR, unlike the\nexisting baselines, is a practically viable model with its reconstruction\nquality within the tolerable limits for tasks pertaining to autonomous\nnavigation like SLAM in dynamic environments.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:19:21 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kumar", "Prashant", ""], ["Sahoo", "Sabyasachi", ""], ["Shah", "Vanshil", ""], ["Kondameedi", "Vineetha", ""], ["Jain", "Abhinav", ""], ["Verma", "Akshaj", ""], ["Bhattacharyya", "Chiranjib", ""], ["Viswanathan", "Vinay", ""]]}, {"id": "2105.12786", "submitter": "Amit Amram", "authors": "Eran Dahan, Tzvi Diskin, Amit Amram, Amit Moryossef, Omer Koren", "title": "cofga: A Dataset for Fine Grained Classification of Objects from Aerial\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and classification of objects in overhead images are two important\nand challenging problems in computer vision. Among various research areas in\nthis domain, the task of fine-grained classification of objects in overhead\nimages has become ubiquitous in diverse real-world applications, due to recent\nadvances in high-resolution satellite and airborne imaging systems. The small\ninter-class variations and the large intra class variations caused by the fine\ngrained nature make it a challenging task, especially in low-resource cases. In\nthis paper, we introduce COFGA a new open dataset for the advancement of\nfine-grained classification research. The 2,104 images in the dataset are\ncollected from an airborne imaging system at 5 15 cm ground sampling distance,\nproviding higher spatial resolution than most public overhead imagery datasets.\nThe 14,256 annotated objects in the dataset were classified into 2 classes, 15\nsubclasses, 14 unique features, and 8 perceived colors a total of 37 distinct\nlabels making it suitable to the task of fine-grained classification more than\nany other publicly available overhead imagery dataset. We compare COFGA to\nother overhead imagery datasets and then describe some distinguished fine-grain\nclassification approaches that were explored during an open data-science\ncompetition we have conducted for this task.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:39:47 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dahan", "Eran", ""], ["Diskin", "Tzvi", ""], ["Amram", "Amit", ""], ["Moryossef", "Amit", ""], ["Koren", "Omer", ""]]}, {"id": "2105.12789", "submitter": "Jiachen Li", "authors": "Jiachen Li, Yuan Lin, Rongrong Liu, Chiu Man Ho and Humphrey Shi", "title": "RSCA: Real-time Segmentation-based Context-Aware Scene Text Detection", "comments": "CVPR 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation-based scene text detection methods have been widely adopted for\narbitrary-shaped text detection recently, since they make accurate pixel-level\npredictions on curved text instances and can facilitate real-time inference\nwithout time-consuming processing on anchors. However, current\nsegmentation-based models are unable to learn the shapes of curved texts and\noften require complex label assignments or repeated feature aggregations for\nmore accurate detection. In this paper, we propose RSCA: a Real-time\nSegmentation-based Context-Aware model for arbitrary-shaped scene text\ndetection, which sets a strong baseline for scene text detection with two\nsimple yet effective strategies: Local Context-Aware Upsampling and Dynamic\nText-Spine Labeling, which model local spatial transformation and simplify\nlabel assignments separately. Based on these strategies, RSCA achieves\nstate-of-the-art performance in both speed and accuracy, without complex label\nassignments or repeated feature aggregations. We conduct extensive experiments\non multiple benchmarks to validate the effectiveness of our method. RSCA-640\nreaches 83.9% F-measure at 48.3 FPS on CTW1500 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:43:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Jiachen", ""], ["Lin", "Yuan", ""], ["Liu", "Rongrong", ""], ["Ho", "Chiu Man", ""], ["Shi", "Humphrey", ""]]}, {"id": "2105.12794", "submitter": "Akin Yilmaz", "authors": "M. Ak{\\i}n Y{\\i}lmaz, A. Murat Tekalp", "title": "DFPN: Deformable Frame Prediction Network", "comments": "Accepted for publication in IEEE International Conference on Image\n  Processing (ICIP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned frame prediction is a current problem of interest in computer vision\nand video compression. Although several deep network architectures have been\nproposed for learned frame prediction, to the best of our knowledge, there is\nno work based on using deformable convolutions for frame prediction. To this\neffect, we propose a deformable frame prediction network (DFPN) for task\noriented implicit motion modeling and next frame prediction. Experimental\nresults demonstrate that the proposed DFPN model achieves state of the art\nresults in next frame prediction. Our models and results are available at\nhttps://github.com/makinyilmaz/DFPN.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 19:00:19 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Y\u0131lmaz", "M. Ak\u0131n", ""], ["Tekalp", "A. Murat", ""]]}, {"id": "2105.12810", "submitter": "Md Hasib Zunair", "authors": "Hasib Zunair, Aimon Rahman, and Nabeel Mohammed", "title": "ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis\n  type classification from chest CT scans", "comments": "Under review at CLEF 2021. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretraining has sparked groundswell of interest in deep learning workflows to\nlearn from limited data and improve generalization. While this is common for 2D\nimage classification tasks, its application to 3D medical imaging tasks like\nchest CT interpretation is limited. We explore the idea of whether pretraining\na model on realistic videos could improve performance rather than training the\nmodel from scratch, intended for tuberculosis type classification from chest CT\nscans. To incorporate both spatial and temporal features, we develop a hybrid\nconvolutional neural network (CNN) and recurrent neural network (RNN) model,\nwhere the features are extracted from each axial slice of the CT scan by a CNN,\nthese sequence of image features are input to a RNN for classification of the\nCT scan. Our model termed as ViPTT-Net, was trained on over 1300 video clips\nwith labels of human activities, and then fine-tuned on chest CT scans with\nlabels of tuberculosis type. We find that pretraining the model on videos lead\nto better representations and significantly improved model validation\nperformance from a kappa score of 0.17 to 0.35, especially for\nunder-represented class samples. Our best method achieved 2nd place in the\nImageCLEF 2021 Tuberculosis - TBT classification task with a kappa score of\n0.20 on the final test set with only image information (without using clinical\nmeta-data). All codes and models are made available.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:00:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zunair", "Hasib", ""], ["Rahman", "Aimon", ""], ["Mohammed", "Nabeel", ""]]}, {"id": "2105.12822", "submitter": "Spencer Ploeger", "authors": "Spencer Ploeger and Lucas Dasovic", "title": "Issues in Object Detection in Videos using Common Single-Image CNNs", "comments": "5 pages, 6 figures, supplementary material at:\n  https://drive.google.com/drive/folders/1qXZd8nObw84jSYklAFjNnH6djVymr65N?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A growing branch of computer vision is object detection. Object detection is\nused in many applications such as industrial process, medical imaging analysis,\nand autonomous vehicles. The ability to detect objects in videos is crucial.\nObject detection systems are trained on large image datasets. For applications\nsuch as autonomous vehicles, it is crucial that the object detection system can\nidentify objects through multiple frames in video. There are many problems with\napplying these systems to video. Shadows or changes in brightness that can\ncause the system to incorrectly identify objects frame to frame and cause an\nunintended system response. There are many neural networks that have been used\nfor object detection and if there was a way of connecting objects between\nframes then these problems could be eliminated. For these neural networks to\nget better at identifying objects in video, they need to be re-trained. A\ndataset must be created with images that represent consecutive video frames and\nhave matching ground-truth layers. A method is proposed that can generate these\ndatasets. The ground-truth layer contains only moving objects. To generate this\nlayer, FlowNet2-Pytorch was used to create the flow mask using the novel\nMagnitude Method. As well, a segmentation mask will be generated using networks\nsuch as Mask R-CNN or Refinenet. These segmentation masks will contain all\nobjects detected in a frame. By comparing this segmentation mask to the flow\nmask ground-truth layer, a loss function is generated. This loss function can\nbe used to train a neural network to be better at making consistent predictions\non video. The system was tested on multiple video samples and a loss was\ngenerated for each frame, proving the Magnitude Method's ability to be used to\ntrain object detection neural networks in future work.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:33:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ploeger", "Spencer", ""], ["Dasovic", "Lucas", ""]]}, {"id": "2105.12855", "submitter": "Scott McCrae", "authors": "Scott McCrae, Kehan Wang, Avideh Zakhor", "title": "Multi-Modal Semantic Inconsistency Detection in Social Media News Posts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As computer-generated content and deepfakes make steady improvements,\nsemantic approaches to multimedia forensics will become more important. In this\npaper, we introduce a novel classification architecture for identifying\nsemantic inconsistencies between video appearance and text caption in social\nmedia news posts. We develop a multi-modal fusion framework to identify\nmismatches between videos and captions in social media posts by leveraging an\nensemble method based on textual analysis of the caption, automatic audio\ntranscription, semantic video analysis, object detection, named entity\nconsistency, and facial verification. To train and test our approach, we curate\na new video-based dataset of 4,000 real-world Facebook news posts for analysis.\nOur multi-modal approach achieves 60.5% classification accuracy on random\nmismatches between caption and appearance, compared to accuracy below 50% for\nuni-modal models. Further ablation studies confirm the necessity of fusion\nacross modalities for correctly identifying semantic inconsistencies.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:25:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["McCrae", "Scott", ""], ["Wang", "Kehan", ""], ["Zakhor", "Avideh", ""]]}, {"id": "2105.12872", "submitter": "Jo\\~ao Phillipe Cardenuto", "authors": "Jo\\~ao P. Cardenuto, Anderson Rocha", "title": "Benchmarking Scientific Image Forgery Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific image integrity area presents a challenging research\nbottleneck, the lack of available datasets to design and evaluate forensic\ntechniques. Its data sensitivity creates a legal hurdle that prevents one to\nrely on real tampered cases to build any sort of accessible forensic benchmark.\nTo mitigate this bottleneck, we present an extendable open-source library that\nreproduces the most common image forgery operations reported by the research\nintegrity community: duplication, retouching, and cleaning. Using this library\nand realistic scientific images, we create a large scientific forgery image\nbenchmark (39,423 images) with an enriched ground-truth. In addition, concerned\nabout the high number of retracted papers due to image duplication, this work\nevaluates the state-of-the-art copy-move detection methods in the proposed\ndataset, using a new metric that asserts consistent match detection between the\nsource and the copied region. The dataset and source-code will be freely\navailable upon acceptance of the paper.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 22:58:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cardenuto", "Jo\u00e3o P.", ""], ["Rocha", "Anderson", ""]]}, {"id": "2105.12883", "submitter": "Peng Yin", "authors": "Peng Yin, Lingyun Xu, Ji Zhang, Howie Choset, Sebastian Scherer", "title": "i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent\n  Environmental Conditions", "comments": "8 Pages, 8 Figures, Accepted Robotics: Science and Systems 2021 paper", "journal-ref": "Robotics: Science and Systems 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a method for localizing a single camera with respect to a point\ncloud map in indoor and outdoor scenes. The problem is challenging because\ncorrespondences of local invariant features are inconsistent across the domains\nbetween image and 3D. The problem is even more challenging as the method must\nhandle various environmental conditions such as illumination, weather, and\nseasonal changes. Our method can match equirectangular images to the 3D range\nprojections by extracting cross-domain symmetric place descriptors. Our key\ninsight is to retain condition-invariant 3D geometry features from limited data\nsamples while eliminating the condition-related features by a designed\nGenerative Adversarial Network. Based on such features, we further design a\nspherical convolution network to learn viewpoint-invariant symmetric place\ndescriptors. We evaluate our method on extensive self-collected datasets, which\ninvolve \\textit{Long-term} (variant appearance conditions),\n\\textit{Large-scale} (up to $2km$ structure/unstructured environment), and\n\\textit{Multistory} (four-floor confined space). Our method surpasses other\ncurrent state-of-the-arts by achieving around $3$ times higher place retrievals\nto inconsistent environments, and above $3$ times accuracy on online\nlocalization. To highlight our method's generalization capabilities, we also\nevaluate the recognition across different datasets. With a single trained\nmodel, i3dLoc can demonstrate reliable visual localization in random\nconditions.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 00:13:11 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 14:40:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yin", "Peng", ""], ["Xu", "Lingyun", ""], ["Zhang", "Ji", ""], ["Choset", "Howie", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2105.12885", "submitter": "Peng Yin", "authors": "Peng Yin, Lingyun Xu, Jianmin Ji, Sebastian Scherer and Howie Choset", "title": "3D Segmentation Learning from Sparse Annotations and Hierarchical\n  Descriptors", "comments": "8 pages, 7 figures, Accepted in IEEE Robotics and Automation Letters,\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One of the main obstacles to 3D semantic segmentation is the significant\namount of endeavor required to generate expensive point-wise annotations for\nfully supervised training. To alleviate manual efforts, we propose GIDSeg, a\nnovel approach that can simultaneously learn segmentation from sparse\nannotations via reasoning global-regional structures and individual-vicinal\nproperties. GIDSeg depicts global- and individual- relation via a dynamic edge\nconvolution network coupled with a kernelized identity descriptor. The ensemble\neffects are obtained by endowing a fine-grained receptive field to a\nlow-resolution voxelized map. In our GIDSeg, an adversarial learning module is\nalso designed to further enhance the conditional constraint of identity\ndescriptors within the joint feature distribution. Despite the apparent\nsimplicity, our proposed approach achieves superior performance over\nstate-of-the-art for inferencing 3D dense segmentation with only sparse\nannotations. Particularly, with $5\\%$ annotations of raw data, GIDSeg\noutperforms other 3D segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 00:31:37 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 14:50:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yin", "Peng", ""], ["Xu", "Lingyun", ""], ["Ji", "Jianmin", ""], ["Scherer", "Sebastian", ""], ["Choset", "Howie", ""]]}, {"id": "2105.12923", "submitter": "Tianqi Wang", "authors": "Tianqi Wang, Dong Eui Chang", "title": "Robust Navigation for Racing Drones based on Imitation Learning and\n  Modularization", "comments": "Published at the 2021 International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a vision-based modularized drone racing navigation system\nthat uses a customized convolutional neural network (CNN) for the perception\nmodule to produce high-level navigation commands and then leverages a\nstate-of-the-art planner and controller to generate low-level control commands,\nthus exploiting the advantages of both data-based and model-based approaches.\nUnlike the state-of-the-art method which only takes the current camera image as\nthe CNN input, we further add the latest three drone states as part of the\ninputs. Our method outperforms the state-of-the-art method in various track\nlayouts and offers two switchable navigation behaviors with a single trained\nnetwork. The CNN-based perception module is trained to imitate an expert policy\nthat automatically generates ground truth navigation commands based on the\npre-computed global trajectories. Owing to the extensive randomization and our\nmodified dataset aggregation (DAgger) policy during data collection, our\nnavigation system, which is purely trained in simulation with synthetic\ntextures, successfully operates in environments with randomly-chosen\nphotorealistic textures without further fine-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:26:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wang", "Tianqi", ""], ["Chang", "Dong Eui", ""]]}, {"id": "2105.12924", "submitter": "Jinxi Xiang", "authors": "Jinxi Xiang, Zhuowei Li, Wenji Wang, Qing Xia and Shaoting Zhang", "title": "Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has demonstrated significant improvements in medical image\nsegmentation using a sufficiently large amount of training data with manual\nlabels. Acquiring well-representative labels requires expert knowledge and\nexhaustive labors. In this paper, we aim to boost the performance of\nsemi-supervised learning for medical image segmentation with limited labels\nusing a self-ensembling contrastive learning technique. To this end, we propose\nto train an encoder-decoder network at image-level with small amounts of\nlabeled images, and more importantly, we learn latent representations directly\nat feature-level by imposing contrastive loss on unlabeled images. This method\nstrengthens intra-class compactness and inter-class separability, so as to get\na better pixel classifier. Moreover, we devise a student encoder for online\nlearning and an exponential moving average version of it, called teacher\nencoder, to improve the performance iteratively in a self-ensembling manner. To\nconstruct contrastive samples with unlabeled images, two sampling strategies\nthat exploit structure similarity across medical images and utilize\npseudo-labels for construction, termed region-aware and anatomical-aware\ncontrastive sampling, are investigated. We conduct extensive experiments on an\nMRI and a CT segmentation dataset and demonstrate that in a limited label\nsetting, the proposed method achieves state-of-the-art performance. Moreover,\nthe anatomical-aware strategy that prepares contrastive samples on-the-fly\nusing pseudo-labels realizes better contrastive regularization on feature\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:27:58 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 10:47:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xiang", "Jinxi", ""], ["Li", "Zhuowei", ""], ["Wang", "Wenji", ""], ["Xia", "Qing", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2105.12926", "submitter": "Changye Yang", "authors": "Changye Yang, Sriram Baireddy, Enyu Cai, Valerian Meline, Denise\n  Caldwell, Anjali S. Iyer-Pascuzzi, Edward J. Delp", "title": "Image-Based Plant Wilting Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many plants become limp or droop through heat, loss of water, or disease.\nThis is also known as wilting. In this paper, we examine plant wilting caused\nby bacterial infection. In particular, we want to design a metric for wilting\nbased on images acquired of the plant. A quantifiable wilting metric will be\nuseful in studying bacterial wilt and identifying resistance genes. Since there\nis no standard way to estimate wilting, it is common to use ad hoc visual\nscores. This is very subjective and requires expert knowledge of the plants and\nthe disease mechanism. Our solution consists of using various wilting metrics\nacquired from RGB images of the plants. We also designed several experiments to\ndemonstrate that our metrics are effective at estimating wilting in plants.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:29:21 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Changye", ""], ["Baireddy", "Sriram", ""], ["Cai", "Enyu", ""], ["Meline", "Valerian", ""], ["Caldwell", "Denise", ""], ["Iyer-Pascuzzi", "Anjali S.", ""], ["Delp", "Edward J.", ""]]}, {"id": "2105.12931", "submitter": "Weijun Tan", "authors": "Delong Qi, Weijun Tan, Qi Yao, Jingfeng Liu", "title": "YOLO5Face: Why Reinventing a Face Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tremendous progress has been made on face detection in recent years using\nconvolutional neural networks. While many face detectors use designs designated\nfor the detection of face, we treat face detection as a general object\ndetection task. We implement a face detector based on YOLOv5 object detector\nand call it YOLO5Face. We add a five-point landmark regression head into it and\nuse the Wing loss function. We design detectors with different model sizes,\nfrom a large model to achieve the best performance, to a super small model for\nreal-time detection on an embedded or mobile device. Experiment results on the\nWiderFace dataset show that our face detectors can achieve state-of-the-art\nperformance in almost all the Easy, Medium, and Hard subsets, exceeding the\nmore complex designated face detectors. The code is available at\n\\url{https://www.github.com/deepcam-cn/yolov5-face}.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:54:38 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Qi", "Delong", ""], ["Tan", "Weijun", ""], ["Yao", "Qi", ""], ["Liu", "Jingfeng", ""]]}, {"id": "2105.12939", "submitter": "Guanyu Cai", "authors": "Guanyu Cai, Lianghua He", "title": "Unsupervised Adaptive Semantic Segmentation with Local Lipschitz\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in unsupervised domain adaptation have seen considerable\nprogress in semantic segmentation. Existing methods either align different\ndomains with adversarial training or involve the self-learning that utilizes\npseudo labels to conduct supervised training. The former always suffers from\nthe unstable training caused by adversarial training and only focuses on the\ninter-domain gap that ignores intra-domain knowledge. The latter tends to put\noverconfident label prediction on wrong categories, which propagates errors to\nmore samples. To solve these problems, we propose a two-stage adaptive semantic\nsegmentation method based on the local Lipschitz constraint that satisfies both\ndomain alignment and domain-specific exploration under a unified principle. In\nthe first stage, we propose the local Lipschitzness regularization as the\nobjective function to align different domains by exploiting intra-domain\nknowledge, which explores a promising direction for non-adversarial adaptive\nsemantic segmentation. In the second stage, we use the local Lipschitzness\nregularization to estimate the probability of satisfying Lipschitzness for each\npixel, and then dynamically sets the threshold of pseudo labels to conduct\nself-learning. Such dynamical self-learning effectively avoids the error\npropagation caused by noisy labels. Optimization in both stages is based on the\nsame principle, i.e., the local Lipschitz constraint, so that the knowledge\nlearned in the first stage can be maintained in the second stage. Further, due\nto the model-agnostic property, our method can easily adapt to any CNN-based\nsemantic segmentation networks. Experimental results demonstrate the excellent\nperformance of our method on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 04:28:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cai", "Guanyu", ""], ["He", "Lianghua", ""]]}, {"id": "2105.12964", "submitter": "Tan Sixiang", "authors": "Tan Sixiang", "title": "Feature Reuse and Fusion for Real-time Semantic segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real-time semantic segmentation, how to increase the speed while\nmaintaining high resolution is a problem that has been discussed and solved.\nBackbone design and fusion design have always been two essential parts of\nreal-time semantic segmentation. We hope to design a light-weight network based\non previous design experience and reach the level of state-of-the-art real-time\nsemantic segmentation without any pre-training. To achieve this goal, a\nencoder-decoder architectures are proposed to solve this problem by applying a\ndecoder network onto a backbone model designed for real-time segmentation tasks\nand designed three different ways to fuse semantics and detailed information in\nthe aggregation phase. We have conducted extensive experiments on two semantic\nsegmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show\nthat the proposed FRFNet strikes a balance between speed calculation and\naccuracy. It achieves 72% Mean Intersection over Union (mIoU%) on the\nCityscapes test dataset with the speed of 144 on a single RTX 1080Ti card. The\nCode is available at https://github.com/favoMJ/FRFNet.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 06:47:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:56:13 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 07:17:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sixiang", "Tan", ""]]}, {"id": "2105.12971", "submitter": "Renjie Pi Mr", "authors": "Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li, Tong Zhang", "title": "Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic\n  Distillation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Joint-DetNAS, a unified NAS framework for object detection, which\nintegrates 3 key components: Neural Architecture Search, pruning, and Knowledge\nDistillation. Instead of naively pipelining these techniques, our Joint-DetNAS\noptimizes them jointly. The algorithm consists of two core processes: student\nmorphism optimizes the student's architecture and removes the redundant\nparameters, while dynamic distillation aims to find the optimal matching\nteacher. For student morphism, weight inheritance strategy is adopted, allowing\nthe student to flexibly update its architecture while fully utilize the\npredecessor's weights, which considerably accelerates the search; To facilitate\ndynamic distillation, an elastic teacher pool is trained via integrated\nprogressive shrinking strategy, from which teacher detectors can be sampled\nwithout additional cost in subsequent searches. Given a base detector as the\ninput, our algorithm directly outputs the derived student detector with high\nperformance without additional training. Experiments demonstrate that our\nJoint-DetNAS outperforms the naive pipelining approach by a great margin. Given\na classic R101-FPN as the base detector, Joint-DetNAS is able to boost its mAP\nfrom 41.4 to 43.9 on MS COCO and reduce the latency by 47%, which is on par\nwith the SOTA EfficientDet while requiring less search cost. We hope our\nproposed method can provide the community with a new way of jointly optimizing\nNAS, KD and pruning.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 07:25:43 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yao", "Lewei", ""], ["Pi", "Renjie", ""], ["Xu", "Hang", ""], ["Zhang", "Wei", ""], ["Li", "Zhenguo", ""], ["Zhang", "Tong", ""]]}, {"id": "2105.12990", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang, Jie Lin, Peng Hu, Bin Zhao, Mohamed M. Sabry Aly", "title": "PSRR-MaxpoolNMS: Pyramid Shifted MaxpoolNMS with Relationship Recovery", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-maximum Suppression (NMS) is an essential postprocessing step in modern\nconvolutional neural networks for object detection. Unlike convolutions which\nare inherently parallel, the de-facto standard for NMS, namely GreedyNMS,\ncannot be easily parallelized and thus could be the performance bottleneck in\nconvolutional object detection pipelines. MaxpoolNMS is introduced as a\nparallelizable alternative to GreedyNMS, which in turn enables faster speed\nthan GreedyNMS at comparable accuracy. However, MaxpoolNMS is only capable of\nreplacing the GreedyNMS at the first stage of two-stage detectors like\nFaster-RCNN. There is a significant drop in accuracy when applying MaxpoolNMS\nat the final detection stage, due to the fact that MaxpoolNMS fails to\napproximate GreedyNMS precisely in terms of bounding box selection. In this\npaper, we propose a general, parallelizable and configurable approach\nPSRR-MaxpoolNMS, to completely replace GreedyNMS at all stages in all\ndetectors. By introducing a simple Relationship Recovery module and a Pyramid\nShifted MaxpoolNMS module, our PSRR-MaxpoolNMS is able to approximate GreedyNMS\nmore precisely than MaxpoolNMS. Comprehensive experiments show that our\napproach outperforms MaxpoolNMS by a large margin, and it is proven faster than\nGreedyNMS with comparable accuracy. For the first time, PSRR-MaxpoolNMS\nprovides a fully parallelizable solution for customized hardware design, which\ncan be reused for accelerating NMS everywhere.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:24:21 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Tianyi", ""], ["Lin", "Jie", ""], ["Hu", "Peng", ""], ["Zhao", "Bin", ""], ["Aly", "Mohamed M. Sabry", ""]]}, {"id": "2105.13012", "submitter": "Laurent Belcour", "authors": "Thomas Chambon, Eric Heitz, and Laurent Belcour", "title": "Passing Multi-Channel Material Textures to a 3-Channel Loss", "comments": "2 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3450623.3464685", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our objective is to compute a textural loss that can be used to train texture\ngenerators with multiple material channels typically used for physically based\nrendering such as albedo, normal, roughness, metalness, ambient occlusion, etc.\nNeural textural losses often build on top of the feature spaces of pretrained\nconvolutional neural networks. Unfortunately, these pretrained models are only\navailable for 3-channel RGB data and hence limit neural textural losses to this\nformat. To overcome this limitation, we show that passing random triplets to a\n3-channel loss provides a multi-channel loss that can be used to generate\nhigh-quality material textures.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:58:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Chambon", "Thomas", ""], ["Heitz", "Eric", ""], ["Belcour", "Laurent", ""]]}, {"id": "2105.13016", "submitter": "Pei-Ze Chiang", "authors": "Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen\n  Chiu", "title": "Stylizing 3D Scene via Implicit Representation and HyperNetwork", "comments": "Project page: https://ztex08010518.github.io/3dstyletransfer/; typos\n  corrected, Figure11, 12 revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to address the 3D scene stylization problem - generating\nstylized images of the scene at arbitrary novel view angles. A straightforward\nsolution is to combine existing novel view synthesis and image/video style\ntransfer approaches, which often leads to blurry results or inconsistent\nappearance. Inspired by the high quality results of the neural radiance fields\n(NeRF) method, we propose a joint framework to directly render novel views with\nthe desired style. Our framework consists of two components: an implicit\nrepresentation of the 3D scene with the neural radiance field model, and a\nhypernetwork to transfer the style information into the scene representation.\nIn particular, our implicit representation model disentangles the scene into\nthe geometry and appearance branches, and the hypernetwork learns to predict\nthe parameters of the appearance branch from the reference style image. To\nalleviate the training difficulties and memory burden, we propose a two-stage\ntraining procedure and a patch sub-sampling approach to optimize the style and\ncontent losses with the neural radiance field model. After optimization, our\nmodel is able to render consistent novel views at arbitrary view angles with\narbitrary style. Both quantitative evaluation and human subject study have\ndemonstrated that the proposed method generates faithful stylization results\nwith consistent appearance across different views.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:11:30 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 13:46:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chiang", "Pei-Ze", ""], ["Tsai", "Meng-Shiun", ""], ["Tseng", "Hung-Yu", ""], ["Lai", "Wei-sheng", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "2105.13033", "submitter": "Xun Guo", "authors": "Xudong Guo, Xun Guo, Yan Lu", "title": "SSAN: Separable Self-Attention Network for Video Representation Learning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-attention has been successfully applied to video representation learning\ndue to the effectiveness of modeling long range dependencies. Existing\napproaches build the dependencies merely by computing the pairwise correlations\nalong spatial and temporal dimensions simultaneously. However, spatial\ncorrelations and temporal correlations represent different contextual\ninformation of scenes and temporal reasoning. Intuitively, learning spatial\ncontextual information first will benefit temporal modeling. In this paper, we\npropose a separable self-attention (SSA) module, which models spatial and\ntemporal correlations sequentially, so that spatial contexts can be efficiently\nused in temporal modeling. By adding SSA module into 2D CNN, we build a SSA\nnetwork (SSAN) for video representation learning. On the task of video action\nrecognition, our approach outperforms state-of-the-art methods on\nSomething-Something and Kinetics-400 datasets. Our models often outperform\ncounterparts with shallower network and fewer modalities. We further verify the\nsemantic learning ability of our method in visual-language task of video\nretrieval, which showcases the homogeneity of video representations and text\nembeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by\nSSA significantly improve the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:02:04 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Guo", "Xudong", ""], ["Guo", "Xun", ""], ["Lu", "Yan", ""]]}, {"id": "2105.13061", "submitter": "Junxiao Shen Mr", "authors": "Junxiao Shen and John Dudley and Per Ola Kristensson", "title": "The Imaginative Generative Adversarial Network: Automatic Data\n  Augmentation for Dynamic Skeleton-Based Hand Gesture and Human Action\n  Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches deliver state-of-the-art performance in recognition\nof spatiotemporal human motion data. However, one of the main challenges in\nthese recognition tasks is limited available training data. Insufficient\ntraining data results in over-fitting and data augmentation is one approach to\naddress this challenge. Existing data augmentation strategies, such as\ntransformations including scaling, shifting and interpolating, require\nhyperparameter optimization that can easily cost hundreds of GPU hours. In this\npaper, we present a novel automatic data augmentation model, the Imaginative\nGenerative Adversarial Network (GAN) that approximates the distribution of the\ninput data and samples new data from this distribution. It is automatic in that\nit requires no data inspection and little hyperparameter tuning and therefore\nit is a low-cost and low-effort approach to generate synthetic data. The\nproposed data augmentation strategy is fast to train and the synthetic data\nleads to higher recognition accuracy than using data augmented with a classical\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:07:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shen", "Junxiao", ""], ["Dudley", "John", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "2105.13067", "submitter": "Laxman Kumarapu", "authors": "Kumarapu Laxman, Shiv Ram Dubey, Baddam Kalyan, and Satya Raj Vineel\n  Kojjarapu", "title": "Efficient High-Resolution Image-to-Image Translation using Multi-Scale\n  Gradient U-Net", "comments": "12 pages, 6 figurea", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Conditional Generative Adversarial Network (Conditional GAN) have\nshown very promising performance in several image-to-image translation\napplications. However, the uses of these conditional GANs are quite limited to\nlow-resolution images, such as 256X256.The Pix2Pix-HD is a recent attempt to\nutilize the conditional GAN for high-resolution image synthesis. In this paper,\nwe propose a Multi-Scale Gradient based U-Net (MSG U-Net) model for\nhigh-resolution image-to-image translation up to 2048X1024 resolution. The\nproposed model is trained by allowing the flow of gradients from\nmultiple-discriminators to a single generator at multiple scales. The proposed\nMSG U-Net architecture leads to photo-realistic high-resolution image-to-image\ntranslation. Moreover, the proposed model is computationally efficient as\ncom-pared to the Pix2Pix-HD with an improvement in the inference time nearly by\n2.5 times. We provide the code of MSG U-Net model at\nhttps://github.com/laxmaniron/MSG-U-Net.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:32:35 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Laxman", "Kumarapu", ""], ["Dubey", "Shiv Ram", ""], ["Kalyan", "Baddam", ""], ["Kojjarapu", "Satya Raj Vineel", ""]]}, {"id": "2105.13077", "submitter": "Kaihao Zhang", "authors": "Wenjia Niu, Kaihao Zhang, Wenhan Luo, Yiran Zhong, Xin Yu, Hongdong Li", "title": "Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal\n  Learning Meets Static Image Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution (SR) and multi-frame SR are two ways to super\nresolve low-resolution images. Single-Image SR generally handles each image\nindependently, but ignores the temporal information implied in continuing\nframes. Multi-frame SR is able to model the temporal dependency via capturing\nmotion information. However, it relies on neighbouring frames which are not\nalways available in the real world. Meanwhile, slight camera shake easily\ncauses heavy motion blur on long-distance-shot low-resolution images. To\naddress these problems, a Blind Motion Deblurring Super-Reslution Networks,\nBMDSRNet, is proposed to learn dynamic spatio-temporal information from single\nstatic motion-blurred images. Motion-blurred images are the accumulation over\ntime during the exposure of cameras, while the proposed BMDSRNet learns the\nreverse process and uses three-streams to learn Bidirectional spatio-temporal\ninformation based on well designed reconstruction loss functions to recover\nclean high-resolution images. Extensive experiments demonstrate that the\nproposed BMDSRNet outperforms recent state-of-the-art methods, and has the\nability to simultaneously deal with image deblurring and SR.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:52:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Niu", "Wenjia", ""], ["Zhang", "Kaihao", ""], ["Luo", "Wenhan", ""], ["Zhong", "Yiran", ""], ["Yu", "Xin", ""], ["Li", "Hongdong", ""]]}, {"id": "2105.13084", "submitter": "Xiangyu Chen", "authors": "Xiangyu Chen, Yihao Liu, Zhengwen Zhang, Yu Qiao and Chao Dong", "title": "HDRUNet: Single Image HDR Reconstruction with Denoising and\n  Dequantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most consumer-grade digital cameras can only capture a limited range of\nluminance in real-world scenes due to sensor constraints. Besides, noise and\nquantization errors are often introduced in the imaging process. In order to\nobtain high dynamic range (HDR) images with excellent visual quality, the most\ncommon solution is to combine multiple images with different exposures.\nHowever, it is not always feasible to obtain multiple images of the same scene\nand most HDR reconstruction methods ignore the noise and quantization loss. In\nthis work, we propose a novel learning-based approach using a spatially dynamic\nencoder-decoder network, HDRUNet, to learn an end-to-end mapping for single\nimage HDR reconstruction with denoising and dequantization. The network\nconsists of a UNet-style base network to make full use of the hierarchical\nmulti-scale information, a condition network to perform pattern-specific\nmodulation and a weighting network for selectively retaining information.\nMoreover, we propose a Tanh_L1 loss function to balance the impact of\nover-exposed values and well-exposed values on the network learning. Our method\nachieves the state-of-the-art performance in quantitative comparisons and\nvisual quality. The proposed HDRUNet model won the second place in the single\nframe track of NITRE2021 High Dynamic Range Challenge.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:12:34 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 09:47:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Xiangyu", ""], ["Liu", "Yihao", ""], ["Zhang", "Zhengwen", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2105.13127", "submitter": "Lorenzo Pellegrini", "authors": "Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Graffieti, Davide\n  Maltoni", "title": "Continual Learning at the Edge: Real-Time Training on Smartphone Devices", "comments": "6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device training for personalized learning is a challenging research\nproblem. Being able to quickly adapt deep prediction models at the edge is\nnecessary to better suit personal user needs. However, adaptation on the edge\nposes some questions on both the efficiency and sustainability of the learning\nprocess and on the ability to work under shifting data distributions. Indeed,\nnaively fine-tuning a prediction model only on the newly available data results\nin catastrophic forgetting, a sudden erasure of previously acquired knowledge.\nIn this paper, we detail the implementation and deployment of a hybrid\ncontinual learning strategy (AR1*) on a native Android application for\nreal-time on-device personalization without forgetting. Our benchmark, based on\nan extension of the CORe50 dataset, shows the efficiency and effectiveness of\nour solution.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:00:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Pellegrini", "Lorenzo", ""], ["Lomonaco", "Vincenzo", ""], ["Graffieti", "Gabriele", ""], ["Maltoni", "Davide", ""]]}, {"id": "2105.13137", "submitter": "David Ahmedt-Aristizabal", "authors": "David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton\n  Fookes, Lars Petersson", "title": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past,\n  Present and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances of data-driven machine learning research, a wide variety of\nprediction problems have been tackled. It has become critical to explore how\nmachine learning and specifically deep learning methods can be exploited to\nanalyse healthcare data. A major limitation of existing methods has been the\nfocus on grid-like data; however, the structure of physiological recordings are\noften irregular and unordered which makes it difficult to conceptualise them as\na matrix. As such, graph neural networks have attracted significant attention\nby exploiting implicit information that resides in a biological system, with\ninteractive nodes connected by edges whose weights can be either temporal\nassociations or anatomical junctions. In this survey, we thoroughly review the\ndifferent types of graph architectures and their applications in healthcare. We\nprovide an overview of these methods in a systematic manner, organized by their\ndomain of application including functional connectivity, anatomical structure\nand electrical-based analysis. We also outline the limitations of existing\ntechniques and discuss potential directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:32:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ahmedt-Aristizabal", "David", ""], ["Armin", "Mohammad Ali", ""], ["Denman", "Simon", ""], ["Fookes", "Clinton", ""], ["Petersson", "Lars", ""]]}, {"id": "2105.13150", "submitter": "Haibo Jin", "authors": "Haibo Jin, Jinpeng Li, Shengcai Liao, Ling Shao", "title": "When Liebig's Barrel Meets Facial Landmark Detection: A Practical Model", "comments": "Fix minor errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, significant progress has been made in the research of facial\nlandmark detection. However, few prior works have thoroughly discussed about\nmodels for practical applications. Instead, they often focus on improving a\ncouple of issues at a time while ignoring the others. To bridge this gap, we\naim to explore a practical model that is accurate, robust, efficient,\ngeneralizable, and end-to-end trainable at the same time. To this end, we first\npropose a baseline model equipped with one transformer decoder as detection\nhead. In order to achieve a better accuracy, we further propose two lightweight\nmodules, namely dynamic query initialization (DQInit) and query-aware memory\n(QAMem). Specifically, DQInit dynamically initializes the queries of decoder\nfrom the inputs, enabling the model to achieve as good accuracy as the ones\nwith multiple decoder layers. QAMem is designed to enhance the discriminative\nability of queries on low-resolution feature maps by assigning separate memory\nvalues to each query rather than a shared one. With the help of QAMem, our\nmodel removes the dependence on high-resolution feature maps and is still able\nto obtain superior accuracy. Extensive experiments and analysis on three\npopular benchmarks show the effectiveness and practical advantages of the\nproposed model. Notably, our model achieves new state of the art on WFLW as\nwell as competitive results on 300W and COFW, while still running at 50+ FPS.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:51:42 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 10:07:37 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jin", "Haibo", ""], ["Li", "Jinpeng", ""], ["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2105.13153", "submitter": "Sanguk Park", "authors": "Sanguk Park and Minyoung Chung", "title": "Cardiac Segmentation on CT Images through Shape-Aware Contour Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac segmentation of atriums, ventricles, and myocardium in computed\ntomography (CT) images is an important first-line task for presymptomatic\ncardiovascular disease diagnosis. In several recent studies, deep learning\nmodels have shown significant breakthroughs in medical image segmentation\ntasks. Unlike other organs such as the lungs and liver, the cardiac organ\nconsists of multiple substructures, i.e., ventricles, atriums, aortas,\narteries, veins, and myocardium. These cardiac substructures are proximate to\neach other and have indiscernible boundaries (i.e., homogeneous intensity\nvalues), making it difficult for the segmentation network focus on the\nboundaries between the substructures. In this paper, to improve the\nsegmentation accuracy between proximate organs, we introduce a novel model to\nexploit shape and boundary-aware features. We primarily propose a shape-aware\nattention module, that exploits distance regression, which can guide the model\nto focus on the edges between substructures so that it can outperform the\nconventional contour-based attention method. In the experiments, we used the\nMulti-Modality Whole Heart Segmentation dataset that has 20 CT cardiac images\nfor training and validation, and 40 CT cardiac images for testing. The\nexperimental results show that the proposed network produces more accurate\nresults than state-of-the-art networks by improving the Dice similarity\ncoefficient score by 4.97%. Our proposed shape-aware contour attention\nmechanism demonstrates that distance transformation and boundary features\nimprove the actual attention map to strengthen the responses in the boundary\narea. Moreover, our proposed method significantly reduces the false-positive\nresponses of the final output, resulting in accurate segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:54:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Park", "Sanguk", ""], ["Chung", "Minyoung", ""]]}, {"id": "2105.13183", "submitter": "Xixi Tao", "authors": "Shanchen Pang, Xixi Tao, Neal N. Xiong, Yukun Dong", "title": "An Efficient Style Virtual Try on Network for Clothing Business Industry", "comments": "10 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing development of garment manufacturing industry, the method\nof combining neural network with industry to reduce product redundancy has been\npaid more and more attention.In order to reduce garment redundancy and achieve\npersonalized customization, more researchers have appeared in the field of\nvirtual trying on.They try to transfer the target clothing to the reference\nfigure, and then stylize the clothes to meet user's requirements for\nfashion.But the biggest problem of virtual try on is that the shape and motion\nblocking distort the clothes, causing the patterns and texture on the clothes\nto be impossible to restore. This paper proposed a new stylized virtual try on\nnetwork, which can not only retain the authenticity of clothing texture and\npattern, but also obtain the undifferentiated stylized try on. The network is\ndivided into three sub-networks, the first is the user image, the front of the\ntarget clothing image, the semantic segmentation image and the posture heat map\nto generate a more detailed human parsing map. Second, UV position map and\ndense correspondence are used to map patterns and textures to the deformed\nsilhouettes in real time, so that they can be retained in real time, and the\nrationality of spatial structure can be guaranteed on the basis of improving\nthe authenticity of images. Third,Stylize and adjust the generated virtual try\non image. Through the most subtle changes, users can choose the texture, color\nand style of clothing to improve the user's experience.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:37:08 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:34:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pang", "Shanchen", ""], ["Tao", "Xixi", ""], ["Xiong", "Neal N.", ""], ["Dong", "Yukun", ""]]}, {"id": "2105.13204", "submitter": "Constantin Seibold", "authors": "Zdravko Marinov, Stanka Vasileva, Qing Wang, Constantin Seibold,\n  Jiaming Zhang and Rainer Stiefelhagen", "title": "Pose2Drone: A Skeleton-Pose-based Framework for Human-Drone Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drones have become a common tool, which is utilized in many tasks such as\naerial photography, surveillance, and delivery. However, operating a drone\nrequires more and more interaction with the user. A natural and safe method for\nHuman-Drone Interaction (HDI) is using gestures. In this paper, we introduce an\nHDI framework building upon skeleton-based pose estimation. Our framework\nprovides the functionality to control the movement of the drone with simple arm\ngestures and to follow the user while keeping a safe distance. We also propose\na monocular distance estimation method, which is entirely based on image\nfeatures and does not require any additional depth sensors. To perform\ncomprehensive experiments and quantitative analysis, we create a customized\ntesting dataset. The experiments indicate that our HDI framework can achieve an\naverage of 93.5\\% accuracy in the recognition of 11 common gestures. The code\nis available at: https://github.com/Zrrr1997/Pose2Drone\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:50:57 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 11:15:20 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Marinov", "Zdravko", ""], ["Vasileva", "Stanka", ""], ["Wang", "Qing", ""], ["Seibold", "Constantin", ""], ["Zhang", "Jiaming", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2105.13236", "submitter": "Sascha Saralajew", "authors": "Sascha Saralajew and Lars Ohnemus and Lukas Ewecker and Ebubekir Asan\n  and Simon Isele and Stefan Roos", "title": "A Dataset for Provident Vehicle Detection at Night", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current object detection, algorithms require the object to be directly\nvisible in order to be detected. As humans, however, we intuitively use visual\ncues caused by the respective object to already make assumptions about its\nappearance. In the context of driving, such cues can be shadows during the day\nand often light reflections at night. In this paper, we study the problem of\nhow to map this intuitive human behavior to computer vision algorithms to\ndetect oncoming vehicles at night just from the light reflections they cause by\ntheir headlights. For that, we present an extensive open-source dataset\ncontaining 59746 annotated grayscale images out of 346 different scenes in a\nrural environment at night. In these images, all oncoming vehicles, their\ncorresponding light objects (e.g., headlamps), and their respective light\nreflections (e.g., light reflections on guardrails) are labeled. In this\ncontext, we discuss the characteristics of the dataset and the challenges in\nobjectively describing visual cues such as light reflections. We provide\ndifferent metrics for different ways to approach the task and report the\nresults we achieved using state-of-the-art and custom object detection models\nas a first benchmark. With that, we want to bring attention to a new and so far\nneglected field in computer vision research, encourage more researchers to\ntackle the problem, and thereby further close the gap between human performance\nand computer vision systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:31:33 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Saralajew", "Sascha", ""], ["Ohnemus", "Lars", ""], ["Ewecker", "Lukas", ""], ["Asan", "Ebubekir", ""], ["Isele", "Simon", ""], ["Roos", "Stefan", ""]]}, {"id": "2105.13244", "submitter": "Roderick Karlemstrand", "authors": "Alessio Galatolo, Alfred Nilsson, Roderick Karlemstrand, Yineng Wang", "title": "Using Early-Learning Regularization to Classify Real-World Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memorization problem is well-known in the field of computer vision. Liu\net al. propose a technique called Early-Learning Regularization, which improves\naccuracy on the CIFAR datasets when label noise is present. This project\nreplicates their experiments and investigates the performance on a real-world\ndataset with intrinsic noise. Results show that their experimental results are\nconsistent. We also explore Sharpness-Aware Minimization in addition to SGD and\nobserved a further 14.6 percentage points improvement. Future work includes\nusing all 6 million images and manually clean a fraction of the images to\nfine-tune a transfer learning model. Last but not the least, having access to\nclean data for testing would also improve the measurement of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:41:45 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:57:46 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Galatolo", "Alessio", ""], ["Nilsson", "Alfred", ""], ["Karlemstrand", "Roderick", ""], ["Wang", "Yineng", ""]]}, {"id": "2105.13264", "submitter": "Iana Sereda", "authors": "Iana Sereda and Grigory Osipov", "title": "How saccadic vision might help with theinterpretability of deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe how some problems (interpretability,lack of object-orientedness)\nof modern deep networks potentiallycould be solved by adapting a biologically\nplausible saccadicmechanism of perception. A sketch of such a saccadic\nvisionmodel is proposed. Proof of concept experimental results areprovided to\nsupport the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:02:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sereda", "Iana", ""], ["Osipov", "Grigory", ""]]}, {"id": "2105.13265", "submitter": "Joseph Chazalon", "authors": "Joseph Chazalon (1), Edwin Carlinet (1), Yizi Chen (1 and 2), Julien\n  Perret (2 and 3), Bertrand Dum\\'enieu (3), Cl\\'ement Mallet (2), Thierry\n  G\\'eraud (1), Vincent Nguyen (4 and 5), Nam Nguyen (4), Josef Baloun (6 and\n  7), Ladislav Lenc (6 and 7), Pavel Kr\\'al (6 and 7) ((1) EPITA Research and\n  Development Lab. (LRDE), EPITA, France, (2) Univ. Gustave Eiffel, IGN-ENSG,\n  LaSTIG, France, (3) LaD\\'eHiS, CRH, EHESS, France, (4) L3i, University of La\n  Rochelle, France, (5) Liris, INSA-Lyon, France, (6) Department of Computer\n  Science and Engineering, University of West Bohemia, Univerzitn\\'i, Pilsen,\n  Czech Republic, (7) NTIS - New Technologies for the Information Society,\n  University of West Bohemia, Univerzitn\\'i, Pilsen, Czech Republic)", "title": "ICDAR 2021 Competition on Historical Map Segmentation", "comments": "Selected as one of the official competitions for the 16th\n  International Conference on Document Analysis and Recognition (ICDAR 2021),\n  September 5-10, 2021, Lausanne, Switzerland (https://icdar2021.org/). Extra\n  material available at https://icdar21-mapseg.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents the final results of the ICDAR 2021 Competition on\nHistorical Map Segmentation (MapSeg), encouraging research on a series of\nhistorical atlases of Paris, France, drawn at 1/5000 scale between 1894 and\n1937. The competition featured three tasks, awarded separately. Task~1 consists\nin detecting building blocks and was won by the L3IRIS team using a\nDenseNet-121 network trained in a weakly supervised fashion. This task is\nevaluated on 3 large images containing hundreds of shapes to detect. Task~2\nconsists in segmenting map content from the larger map sheet, and was won by\nthe UWB team using a U-Net-like FCN combined with a binarization method to\nincrease detection edge accuracy. Task~3 consists in locating intersection\npoints of geo-referencing lines, and was also won by the UWB team who used a\ndedicated pipeline combining binarization, line detection with Hough transform,\ncandidate filtering, and template matching for intersection refinement. Tasks~2\nand~3 are evaluated on 95 map sheets with complex content. Dataset, evaluation\ntools and results are available under permissive licensing at\n\\url{https://icdar21-mapseg.github.io/}.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:06:50 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Chazalon", "Joseph", "", "1 and 2"], ["Carlinet", "Edwin", "", "1 and 2"], ["Chen", "Yizi", "", "1 and 2"], ["Perret", "Julien", "", "2 and 3"], ["Dum\u00e9nieu", "Bertrand", "", "4 and 5"], ["Mallet", "Cl\u00e9ment", "", "4 and 5"], ["G\u00e9raud", "Thierry", "", "4 and 5"], ["Nguyen", "Vincent", "", "4 and 5"], ["Nguyen", "Nam", "", "6 and\n  7"], ["Baloun", "Josef", "", "6 and\n  7"], ["Lenc", "Ladislav", "", "6 and 7"], ["Kr\u00e1l", "Pavel", "", "6 and 7"]]}, {"id": "2105.13279", "submitter": "Gianluca Palermo", "authors": "Emanuele Vitali and Anton Lokhmotov and Gianluca Palermo", "title": "Dynamic Network selection for the Object Detection task: why it matters\n  and what we (didn't) achieve", "comments": "Paper accepted at SAMOS21 - International Conference on Embedded\n  Computer Systems: Architectures, Modeling and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we want to show the potential benefit of a dynamic auto-tuning\napproach for the inference process in the Deep Neural Network (DNN) context,\ntackling the object detection challenge. We benchmarked different neural\nnetworks to find the optimal detector for the well-known COCO 17 database, and\nwe demonstrate that even if we only consider the quality of the prediction\nthere is not a single optimal network. This is even more evident if we also\nconsider the time to solution as a metric to evaluate, and then select, the\nmost suitable network. This opens to the possibility for an adaptive\nmethodology to switch among different object detection networks according to\nrun-time requirements (e.g. maximum quality subject to a time-to-solution\nconstraint).\n  Moreover, we demonstrated by developing an ad hoc oracle, that an additional\nproactive methodology could provide even greater benefits, allowing us to\nselect the best network among the available ones given some characteristics of\nthe processed image. To exploit this method, we need to identify some image\nfeatures that can be used to steer the decision on the most promising network.\nDespite the optimization opportunity that has been identified, we were not able\nto identify a predictor function that validates this attempt neither adopting\nclassical image features nor by using a DNN classifier.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:25:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Vitali", "Emanuele", ""], ["Lokhmotov", "Anton", ""], ["Palermo", "Gianluca", ""]]}, {"id": "2105.13290", "submitter": "Ming Ding", "authors": "Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin,\n  Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang", "title": "CogView: Mastering Text-to-Image Generation via Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-Image generation in the general domain has long been an open problem,\nwhich requires both a powerful generative model and cross-modal understanding.\nWe propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to\nadvance this problem. We also demonstrate the finetuning strategies for various\ndownstream tasks, e.g. style learning, super-resolution, text-image ranking and\nfashion design, and methods to stabilize pretraining, e.g. eliminating NaN\nlosses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS\nCOCO, outperforms previous GAN-based models and a recent similar work DALL-E.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:52:53 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 18:05:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ding", "Ming", ""], ["Yang", "Zhuoyi", ""], ["Hong", "Wenyi", ""], ["Zheng", "Wendi", ""], ["Zhou", "Chang", ""], ["Yin", "Da", ""], ["Lin", "Junyang", ""], ["Zou", "Xu", ""], ["Shao", "Zhou", ""], ["Yang", "Hongxia", ""], ["Tang", "Jie", ""]]}, {"id": "2105.13343", "submitter": "Stanislav Fort", "authors": "Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, Samuel L.\n  Smith", "title": "Drawing Multiple Augmentation Samples Per Image During Training\n  Efficiently Decreases Test Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, it is standard practice to draw a single sample from the\ndata augmentation procedure for each unique image in the mini-batch, however it\nis not clear whether this choice is optimal for generalization. In this work,\nwe provide a detailed empirical evaluation of how the number of augmentation\nsamples per unique image influences performance on held out data. Remarkably,\nwe find that drawing multiple samples per image consistently enhances the test\naccuracy achieved for both small and large batch training, despite reducing the\nnumber of unique training examples in each mini-batch. This benefit arises even\nwhen different augmentation multiplicities perform the same number of parameter\nupdates and gradient evaluations. Our results suggest that, although the\nvariance in the gradient estimate arising from subsampling the dataset has an\nimplicit regularization benefit, the variance which arises from the data\naugmentation process harms test accuracy. By applying augmentation multiplicity\nto the recently proposed NFNet model family, we achieve a new ImageNet state of\nthe art of 86.8$\\%$ top-1 w/o extra data.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:51:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Fort", "Stanislav", ""], ["Brock", "Andrew", ""], ["Pascanu", "Razvan", ""], ["De", "Soham", ""], ["Smith", "Samuel L.", ""]]}, {"id": "2105.13351", "submitter": "Drew Linsley", "authors": "Drew Linsley, Girik Malik, Junkyung Kim, Lakshmi N Govindarajan, Ennio\n  Mingolla, and Thomas Serre", "title": "Tracking Without Re-recognition in Humans and Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imagine trying to track one particular fruitfly in a swarm of hundreds.\nHigher biological visual systems have evolved to track moving objects by\nrelying on both appearance and motion features. We investigate if\nstate-of-the-art deep neural networks for visual tracking are capable of the\nsame. For this, we introduce PathTracker, a synthetic visual challenge that\nasks human observers and machines to track a target object in the midst of\nidentical-looking \"distractor\" objects. While humans effortlessly learn\nPathTracker and generalize to systematic variations in task design,\nstate-of-the-art deep networks struggle. To address this limitation, we\nidentify and model circuit mechanisms in biological brains that are implicated\nin tracking objects based on motion cues. When instantiated as a recurrent\nnetwork, our circuit model learns to solve PathTracker with a robust visual\nstrategy that rivals human performance and explains a significant proportion of\ntheir decision-making on the challenge. We also show that the success of this\ncircuit model extends to object tracking in natural videos. Adding it to a\ntransformer-based architecture for object tracking builds tolerance to visual\nnuisances that affect object appearance, resulting in a new state-of-the-art\nperformance on the large-scale TrackingNet object tracking challenge. Our work\nhighlights the importance of building artificial vision models that can help us\nbetter understand human vision and improve computer vision.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:56:37 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 00:26:33 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Linsley", "Drew", ""], ["Malik", "Girik", ""], ["Kim", "Junkyung", ""], ["Govindarajan", "Lakshmi N", ""], ["Mingolla", "Ennio", ""], ["Serre", "Thomas", ""]]}, {"id": "2105.13353", "submitter": "Quoc-Huy Tran", "authors": "Sateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey Konin, M. Zeeshan\n  Zia, Quoc-Huy Tran", "title": "Unsupervised Activity Segmentation by Joint Representation Learning and\n  Online Clustering", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for unsupervised activity segmentation, which\nuses video frame clustering as a pretext task and simultaneously performs\nrepresentation learning and online clustering. This is in contrast with prior\nworks where representation learning and clustering are often performed\nsequentially. We leverage temporal information in videos by employing temporal\noptimal transport and temporal coherence loss. In particular, we incorporate a\ntemporal regularization term into the standard optimal transport module, which\npreserves the temporal order of the activity, yielding the temporal optimal\ntransport module for computing pseudo-label cluster assignments. Next, the\ntemporal coherence loss encourages neighboring video frames to be mapped to\nnearby points while distant video frames are mapped to farther away points in\nthe embedding space. The combination of these two components results in\neffective representations for unsupervised activity segmentation. Furthermore,\nprevious methods require storing learned features for the entire dataset before\nclustering them in an offline manner, whereas our approach processes one\nmini-batch at a time in an online manner. Extensive evaluations on three public\ndatasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,\ni.e., Desktop Assembly, show that our approach performs on par or better than\nprevious methods for unsupervised activity segmentation, despite having\nsignificantly less memory constraints.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:57:37 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:06:22 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 06:05:28 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kumar", "Sateesh", ""], ["Haresh", "Sanjay", ""], ["Ahmed", "Awais", ""], ["Konin", "Andrey", ""], ["Zia", "M. Zeeshan", ""], ["Tran", "Quoc-Huy", ""]]}, {"id": "2105.13381", "submitter": "Xuxin Chen", "authors": "Xuxin Chen, Ximin Wang, Ke Zhang, Roy Zhang, Kar-Ming Fung, Theresa C.\n  Thai, Kathleen Moore, Robert S. Mannel, Hong Liu, Bin Zheng, Yuchen Qiu", "title": "Recent advances and clinical applications of deep learning in medical\n  image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the mainstream technology in computer vision, and it\nhas received extensive research interest in developing new medical image\nprocessing algorithms to support disease detection and diagnosis. As compared\nto conventional machine learning technologies, the major advantage of deep\nlearning is that models can automatically identify and recognize representative\nfeatures through the hierarchal model architecture, while avoiding the\nlaborious development of hand-crafted features. In this paper, we reviewed and\nsummarized more than 200 recently published papers to provide a comprehensive\noverview of applying deep learning methods in various medical image analysis\ntasks. Especially, we emphasize the latest progress and contributions of\nstate-of-the-art unsupervised and semi-supervised deep learning in medical\nimages, which are summarized based on different application scenarios,\nincluding lesion classification, segmentation, detection, and image\nregistration. Additionally, we also discussed the major technical challenges\nand suggested the possible solutions in future research efforts.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:05:12 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Xuxin", ""], ["Wang", "Ximin", ""], ["Zhang", "Ke", ""], ["Zhang", "Roy", ""], ["Fung", "Kar-Ming", ""], ["Thai", "Theresa C.", ""], ["Moore", "Kathleen", ""], ["Mannel", "Robert S.", ""], ["Liu", "Hong", ""], ["Zheng", "Bin", ""], ["Qiu", "Yuchen", ""]]}, {"id": "2105.13387", "submitter": "Jeremiah Scully Mr", "authors": "Jeremiah Scully, Ronan Flynn, Eoin Carley, Peter Gallagher and Mark\n  Daly", "title": "Type III solar radio burst detection and classification: A deep learning\n  approach", "comments": "6 pages, 6 figures, Irish Signals & Systems Conference 2021\n  (pre-print)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solar Radio Bursts (SRBs) are generally observed in dynamic spectra and have\nfive major spectral classes, labelled Type I to Type V depending on their shape\nand extent in frequency and time. Due to their complex characterisation, a\nchallenge in solar radio physics is the automatic detection and classification\nof such radio bursts. Classification of SRBs has become fundamental in recent\nyears due to large data rates generated by advanced radio telescopes such as\nthe LOw-Frequency ARray, (LOFAR). Current state-of-the-art algorithms implement\nthe Hough or Radon transform as a means of detecting predefined parametric\nshapes in images. These algorithms achieve up to 84% accuracy, depending on the\nType of radio burst being classified. Other techniques include procedures that\nrely on Constant-FalseAlarm-Rate detection, which is essentially detection of\nradio bursts using a de-noising and adaptive threshold in dynamic spectra. It\nworks well for a variety of different Types of radio bursts and achieves an\naccuracy of up to 70%. In this research, we are introducing a methodology named\nYou Only Look Once v2 (YOLOv2) for solar radio burst classification. By using\nType III simulation methods we can train the algorithm to classify real Type\nIII solar radio bursts in real-time at an accu\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:30:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Scully", "Jeremiah", ""], ["Flynn", "Ronan", ""], ["Carley", "Eoin", ""], ["Gallagher", "Peter", ""], ["Daly", "Mark", ""]]}, {"id": "2105.13393", "submitter": "Sebastian Dorn", "authors": "Philipp Joppich, Sebastian Dorn, Oliver De Candido, Wolfgang Utschick,\n  Jakob Knollm\\\"uller", "title": "Classification and Uncertainty Quantification of Corrupted Data using\n  Semi-Supervised Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric and non-parametric classifiers often have to deal with real-world\ndata, where corruptions like noise, occlusions, and blur are unavoidable -\nposing significant challenges. We present a probabilistic approach to classify\nstrongly corrupted data and quantify uncertainty, despite the model only having\nbeen trained with uncorrupted data. A semi-supervised autoencoder trained on\nuncorrupted data is the underlying architecture. We use the decoding part as a\ngenerative model for realistic data and extend it by convolutions, masking, and\nadditive Gaussian noise to describe imperfections. This constitutes a\nstatistical inference task in terms of the optimal latent space activations of\nthe underlying uncorrupted datum. We solve this problem approximately with\nMetric Gaussian Variational Inference (MGVI). The supervision of the\nautoencoder's latent space allows us to classify corrupted data directly under\nuncertainty with the statistically inferred latent space activations.\nFurthermore, we demonstrate that the model uncertainty strongly depends on\nwhether the classification is correct or wrong, setting a basis for a\nstatistical \"lie detector\" of the classification. Independent of that, we show\nthat the generative model can optimally restore the uncorrupted datum by\ndecoding the inferred latent space activations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:47:55 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Joppich", "Philipp", ""], ["Dorn", "Sebastian", ""], ["De Candido", "Oliver", ""], ["Utschick", "Wolfgang", ""], ["Knollm\u00fcller", "Jakob", ""]]}, {"id": "2105.13426", "submitter": "Wadii Boulila Prof.", "authors": "Wadii Boulila, Anmar Abuhamdah, Maha Driss, Slim Kammoun, Jawad Ahmad", "title": "GuideMe: A Mobile Application based on Global Positioning System and\n  Object Recognition Towards a Smart Tourist Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding information about tourist places to visit is a challenging problem\nthat people face while visiting different countries. This problem is\naccentuated when people are coming from different countries, speak different\nlanguages, and are from all segments of society. In this context, visitors and\npilgrims face important problems to find the appropriate doaas when visiting\nholy places. In this paper, we propose a mobile application that helps the user\nfind the appropriate doaas for a given holy place in an easy and intuitive\nmanner. Three different options are developed to achieve this goal: 1) manual\nsearch, 2) GPS location to identify the holy places and therefore their\ncorresponding doaas, and 3) deep learning (DL) based method to determine the\nholy place by analyzing an image taken by the visitor. Experiments show good\nperformance of the proposed mobile application in providing the appropriate\ndoaas for visited holy places.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:58:25 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Boulila", "Wadii", ""], ["Abuhamdah", "Anmar", ""], ["Driss", "Maha", ""], ["Kammoun", "Slim", ""], ["Ahmad", "Jawad", ""]]}, {"id": "2105.13464", "submitter": "Shreyas Saxena", "authors": "Shreyas Saxena, Nidhi Vyas, Dennis DeCoste", "title": "Training With Data Dependent Dynamic Learning Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently many first and second order variants of SGD have been proposed to\nfacilitate training of Deep Neural Networks (DNNs). A common limitation of\nthese works stem from the fact that they use the same learning rate across all\ninstances present in the dataset. This setting is widely adopted under the\nassumption that loss functions for each instance are similar in nature, and\nhence, a common learning rate can be used. In this work, we relax this\nassumption and propose an optimization framework which accounts for difference\nin loss function characteristics across instances. More specifically, our\noptimizer learns a dynamic learning rate for each instance present in the\ndataset. Learning a dynamic learning rate for each instance allows our\noptimization framework to focus on different modes of training data during\noptimization. When applied to an image classification task, across different\nCNN architectures, learning dynamic learning rates leads to consistent gains\nover standard optimizers. When applied to a dataset containing corrupt\ninstances, our framework reduces the learning rates on noisy instances, and\nimproves over the state-of-the-art. Finally, we show that our optimization\nframework can be used for personalization of a machine learning model towards a\nknown targeted data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:52:29 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Saxena", "Shreyas", ""], ["Vyas", "Nidhi", ""], ["DeCoste", "Dennis", ""]]}, {"id": "2105.13482", "submitter": "Grzegorz Sarwas", "authors": "Malwina Kubas and Grzegorz Sarwas", "title": "FastRIFE: Optimization of Real-Time Intermediate Flow Estimation for\n  Video Frame Interpolation", "comments": "WSCG 2021 29. International Conference in Central Europe on Computer\n  Graphics, Visualization and Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of video inter-frame interpolation is an essential task in the\nfield of image processing. Correctly increasing the number of frames in the\nrecording while maintaining smooth movement allows to improve the quality of\nplayed video sequence, enables more effective compression and creating a\nslow-motion recording. This paper proposes the FastRIFE algorithm, which is\nsome speed improvement of the RIFE (Real-Time Intermediate Flow Estimation)\nmodel. The novel method was examined and compared with other recently published\nalgorithms. All source codes are available at\nhttps://gitlab.com/malwinq/interpolation-of-images-for-slow-motion-videos\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:31:40 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:45:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kubas", "Malwina", ""], ["Sarwas", "Grzegorz", ""]]}, {"id": "2105.13495", "submitter": "Byung-Hoon Kim M.D. Ph.D.", "authors": "Byung-Hoon Kim, Jong Chul Ye, Jae-Jin Kim", "title": "Learning Dynamic Graph Representation of Brain Connectome with\n  Spatio-Temporal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity (FC) between regions of the brain can be assessed by\nthe degree of temporal correlation measured with functional neuroimaging\nmodalities. Based on the fact that these connectivities build a network,\ngraph-based approaches for analyzing the brain connectome have provided\ninsights into the functions of the human brain. The development of graph neural\nnetworks (GNNs) capable of learning representation from graph structured data\nhas led to increased interest in learning the graph representation of the brain\nconnectome. Although recent attempts to apply GNN to the FC network have shown\npromising results, there is still a common limitation that they usually do not\nincorporate the dynamic characteristics of the FC network which fluctuates over\ntime. In addition, a few studies that have attempted to use dynamic FC as an\ninput for the GNN reported a reduction in performance compared to static FC\nmethods, and did not provide temporal explainability. Here, we propose STAGIN,\na method for learning dynamic graph representation of the brain connectome with\nspatio-temporal attention. Specifically, a temporal sequence of brain graphs is\ninput to the STAGIN to obtain the dynamic graph representation, while novel\nREADOUT functions and the Transformer encoder provide spatial and temporal\nexplainability with attention, respectively. Experiments on the HCP-Rest and\nthe HCP-Task datasets demonstrate exceptional performance of our proposed\nmethod. Analysis of the spatio-temporal attention also provide concurrent\ninterpretation with the neuroscientific knowledge, which further validates our\nmethod. Code is available at https://github.com/egyptdj/stagin\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:06:50 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kim", "Byung-Hoon", ""], ["Ye", "Jong Chul", ""], ["Kim", "Jae-Jin", ""]]}, {"id": "2105.13502", "submitter": "Poojan Oza", "authors": "Poojan Oza, Vishwanath A. Sindagi, Vibashan VS, Vishal M. Patel", "title": "Unsupervised Domain Adaptation of Object Detectors: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have led to the development of accurate and\nefficient models for various computer vision applications such as\nclassification, segmentation, and detection. However, learning highly accurate\nmodels relies on the availability of large-scale annotated datasets. Due to\nthis, model performance drops drastically when evaluated on label-scarce\ndatasets having visually distinct images, termed as domain adaptation problem.\nThere is a plethora of works to adapt classification and segmentation models to\nlabel-scarce target datasets through unsupervised domain adaptation.\nConsidering that detection is a fundamental task in computer vision, many\nrecent works have focused on developing novel domain adaptive detection\ntechniques. Here, we describe in detail the domain adaptation problem for\ndetection and present an extensive survey of the various methods. Furthermore,\nwe highlight strategies proposed and the associated shortcomings. Subsequently,\nwe identify multiple aspects of the problem that are most promising for future\nresearch. We believe that this survey shall be valuable to the pattern\nrecognition experts working in the fields of computer vision, biometrics,\nmedical imaging, and autonomous navigation by introducing them to the problem,\nand familiarizing them with the current status of the progress while providing\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:34:06 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 04:56:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Oza", "Poojan", ""], ["Sindagi", "Vishwanath A.", ""], ["VS", "Vibashan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2105.13509", "submitter": "Hsin-Ping Huang", "authors": "Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh,\n  Ming-Hsuan Yang", "title": "Learning to Stylize Novel Views", "comments": "Project page: https://hhsinping.github.io/3d_scene_stylization/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle a 3D scene stylization problem - generating stylized images of a\nscene from arbitrary novel views given a set of images of the same scene and a\nreference image of the desired style as inputs. Direct solution of combining\nnovel view synthesis and stylization approaches lead to results that are blurry\nor not consistent across different views. We propose a point cloud-based method\nfor consistent 3D scene stylization. First, we construct the point cloud by\nback-projecting the image features to the 3D space. Second, we develop point\ncloud aggregation modules to gather the style information of the 3D scene, and\nthen modulate the features in the point cloud with a linear transformation\nmatrix. Finally, we project the transformed features to 2D space to obtain the\nnovel views. Experimental results on two diverse datasets of real-world scenes\nvalidate that our method generates consistent stylized novel view synthesis\nresults against other alternative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:58:18 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Huang", "Hsin-Ping", ""], ["Tseng", "Hung-Yu", ""], ["Saini", "Saurabh", ""], ["Singh", "Maneesh", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2105.13531", "submitter": "Ad\\'in Ram\\'irez Rivera", "authors": "Darwin Saire and Ad\\'in Ram\\'irez Rivera", "title": "Empirical Study of Multi-Task Hourglass Model for Semantic Segmentation\n  Task", "comments": "To appear in IEEE Access. Code available at\n  https://gitlab.com/mipl/mtl-ss", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3085218", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The semantic segmentation (SS) task aims to create a dense classification by\nlabeling at the pixel level each object present on images. Convolutional neural\nnetwork (CNN) approaches have been widely used, and exhibited the best results\nin this task. However, the loss of spatial precision on the results is a main\ndrawback that has not been solved. In this work, we propose to use a multi-task\napproach by complementing the semantic segmentation task with edge detection,\nsemantic contour, and distance transform tasks. We propose that by sharing a\ncommon latent space, the complementary tasks can produce more robust\nrepresentations that can enhance the semantic labels. We explore the influence\nof contour-based tasks on latent space, as well as their impact on the final\nresults of SS. We demonstrate the effectiveness of learning in a multi-task\nsetting for hourglass models in the Cityscapes, CamVid, and Freiburg Forest\ndatasets by improving the state-of-the-art without any refinement\npost-processing.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:08:10 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Saire", "Darwin", ""], ["Rivera", "Ad\u00edn Ram\u00edrez", ""]]}, {"id": "2105.13533", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Naimul Khan", "title": "Inertial Sensor Data To Image Encoding For Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are successful deep learning models in\nthe field of computer vision. To get the maximum advantage of CNN model for\nHuman Action Recognition (HAR) using inertial sensor data, in this paper, we\nuse 4 types of spatial domain methods for transforming inertial sensor data to\nactivity images, which are then utilized in a novel fusion framework. These\nfour types of activity images are Signal Images (SI), Gramian Angular Field\n(GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP)\nImages. Furthermore, for creating a multimodal fusion framework and to exploit\nactivity image, we made each type of activity images multimodal by convolving\nwith two spatial domain filters : Prewitt filter and High-boost filter.\nResnet-18, a CNN model, is used to learn deep features from multi-modalities.\nLearned features are extracted from the last pooling layer of each ReNet and\nthen fused by canonical correlation based fusion (CCF) for improving the\naccuracy of human action recognition. These highly informative features are\nserved as input to a multiclass Support Vector Machine (SVM). Experimental\nresults on three publicly available inertial datasets show the superiority of\nthe proposed method over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:22:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "2105.13536", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Anika Tabassum, Naimul Khan, Ling Guan", "title": "ECG Heart-beat Classification Using Multimodal Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel Image Fusion Model (IFM) for ECG heart-beat\nclassification to overcome the weaknesses of existing machine learning\ntechniques that rely either on manual feature extraction or direct utilization\nof 1D raw ECG signal. At the input of IFM, we first convert the heart beats of\nECG into three different images using Gramian Angular Field (GAF), Recurrence\nPlot (RP) and Markov Transition Field (MTF) and then fuse these images to\ncreate a single imaging modality. We use AlexNet for feature extraction and\nclassification and thus employ end to end deep learning. We perform experiments\non PhysioNet MIT-BIH dataset for five different arrhythmias in accordance with\nthe AAMI EC57 standard and on PTB diagnostics dataset for myocardial infarction\n(MI) classification. We achieved an state of an art results in terms of\nprediction accuracy, precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:31:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Tabassum", "Anika", ""], ["Khan", "Naimul", ""], ["Guan", "Ling", ""]]}, {"id": "2105.13557", "submitter": "Jingyun Jia", "authors": "Jingyun Jia, Philip K. Chan", "title": "Self-supervised Detransformation Autoencoder for Representation Learning\n  in Open Set Recognition", "comments": "arXiv admin note: text overlap with arXiv:2006.15117", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Open set recognition (OSR) is to learn a classifier that can\nreject the unknown samples while classifying the known classes accurately. In\nthis paper, we propose a self-supervision method, Detransformation Autoencoder\n(DTAE), for the OSR problem. This proposed method engages in learning\nrepresentations that are invariant to the transformations of the input data.\nExperiments on several standard image datasets indicate that the pre-training\nprocess significantly improves the model performance in the OSR tasks.\nMeanwhile, our proposed self-supervision method achieves significant gains in\ndetecting the unknown class and classifying the known classes. Moreover, our\nanalysis indicates that DTAE can yield representations that contain more target\nclass information and less transformation information than RotNet.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 02:45:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Jia", "Jingyun", ""], ["Chan", "Philip K.", ""]]}, {"id": "2105.13559", "submitter": "Hao Su", "authors": "Hao Su", "title": "One-shot Learning with Absolute Generalization", "comments": "8 pages, 41 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One-shot learning is proposed to make a pretrained classifier workable on a\nnew dataset based on one labeled samples from each pattern. However, few of\nresearchers consider whether the dataset itself supports one-shot learning. In\nthis paper, we propose a set of definitions to explain what kind of datasets\ncan support one-shot learning and propose the concept \"absolute\ngeneralization\". Based on these definitions, we proposed a method to build an\nabsolutely generalizable classifier. The proposed method concatenates two\nsamples as a new single sample, and converts a classification problem to an\nidentity identification problem or a similarity metric problem. Experiments\ndemonstrate that the proposed method is superior to baseline on one-shot\nlearning datasets and artificial datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 02:52:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Su", "Hao", ""]]}, {"id": "2105.13575", "submitter": "Lin Xu", "authors": "Yichen Cao, Yufei Wei, Shichao Liu, Lin Xu", "title": "2nd Place Solution for IJCAI-PRICAI 2020 3D AI Challenge: 3D Object\n  Reconstruction from A Single Image", "comments": "5 pages, 2 figures, 5 tables", "journal-ref": "IJCAI 2020 workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our solution for the {\\it IJCAI--PRICAI--20 3D AI\nChallenge: 3D Object Reconstruction from A Single Image}. We develop a variant\nof AtlasNet that consumes single 2D images and generates 3D point clouds\nthrough 2D to 3D mapping. To push the performance to the limit and present\nguidance on crucial implementation choices, we conduct extensive experiments to\nanalyze the influence of decoder design and different settings on the\nnormalization, projection, and sampling methods. Our method achieves 2nd place\nin the final track with a score of $70.88$, a chamfer distance of $36.87$, and\na mean f-score of $59.18$. The source code of our method will be available at\nhttps://github.com/em-data/Enhanced_AtlasNet_3DReconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:54:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Cao", "Yichen", ""], ["Wei", "Yufei", ""], ["Liu", "Shichao", ""], ["Xu", "Lin", ""]]}, {"id": "2105.13580", "submitter": "Marcus Hebel", "authors": "Bj\\\"orn Borgmann (1 and 2), Volker Schatz (1), Marcus Hammer (1),\n  Marcus Hebel (1), Michael Arens (1), Uwe Stilla (2) ((1) Fraunhofer IOSB,\n  Ettlingen, Germany, (2) Technical University of Munich (TUM), Munich,\n  Germany)", "title": "MODISSA: a multipurpose platform for the prototypical realization of\n  vehicle-related applications using optical sensors", "comments": "Authors' version of an article accepted for publication in Applied\n  Optics, 9 May 2021", "journal-ref": "Applied Optic 60(22), pp. F50-F65, 2021", "doi": "10.1364/AO.423599", "report-no": null, "categories": "cs.CV cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the current state of development of the sensor-equipped car\nMODISSA, with which Fraunhofer IOSB realizes a configurable experimental\nplatform for hardware evaluation and software development in the context of\nmobile mapping and vehicle-related safety and protection. MODISSA is based on a\nvan that has successively been equipped with a variety of optical sensors over\nthe past few years, and contains hardware for complete raw data acquisition,\ngeoreferencing, real-time data analysis, and immediate visualization on in-car\ndisplays. We demonstrate the capabilities of MODISSA by giving a deeper insight\ninto experiments with its specific configuration in the scope of three\ndifferent applications. Other research groups can benefit from these\nexperiences when setting up their own mobile sensor system, especially\nregarding the selection of hardware and software, the knowledge of possible\nsources of error, and the handling of the acquired sensor data.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 04:21:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Borgmann", "Bj\u00f6rn", "", "1 and 2"], ["Schatz", "Volker", ""], ["Hammer", "Marcus", ""], ["Hebel", "Marcus", ""], ["Arens", "Michael", ""], ["Stilla", "Uwe", ""]]}, {"id": "2105.13593", "submitter": "Runnan Chen Mr.", "authors": "Runnan Chen, Yuexin Ma, Lingjie Liu, Nenglun Chen, Zhiming Cui,\n  Guodong Wei, Wenping Wang", "title": "Semi-supervised Anatomical Landmark Detection via Shape-regulated\n  Self-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Well-annotated medical images are costly and sometimes even impossible to\nacquire, hindering landmark detection accuracy to some extent. Semi-supervised\nlearning alleviates the reliance on large-scale annotated data by exploiting\nthe unlabeled data to understand the population structure of anatomical\nlandmarks. The global shape constraint is the inherent property of anatomical\nlandmarks that provides valuable guidance for more consistent pseudo labelling\nof the unlabeled data, which is ignored in the previously semi-supervised\nmethods. In this paper, we propose a model-agnostic shape-regulated\nself-training framework for semi-supervised landmark detection by fully\nconsidering the global shape constraint. Specifically, to ensure pseudo labels\nare reliable and consistent, a PCA-based shape model adjusts pseudo labels and\neliminate abnormal ones. A novel Region Attention loss to make the network\nautomatically focus on the structure consistent regions around pseudo labels.\nExtensive experiments show that our approach outperforms other semi-supervised\nmethods and achieves notable improvement on three medical image datasets.\nMoreover, our framework is flexible and can be used as a plug-and-play module\nintegrated into most supervised methods to improve performance further.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 05:23:07 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Runnan", ""], ["Ma", "Yuexin", ""], ["Liu", "Lingjie", ""], ["Chen", "Nenglun", ""], ["Cui", "Zhiming", ""], ["Wei", "Guodong", ""], ["Wang", "Wenping", ""]]}, {"id": "2105.13617", "submitter": "Shahroz Tariq", "authors": "Minha Kim and Shahroz Tariq and Simon S. Woo", "title": "FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and\n  Representation Learning", "comments": "12 pages, 2 figures, 5 tables, accepted for publication at the\n  Workshop on Media Forensics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As GAN-based video and image manipulation technologies become more\nsophisticated and easily accessible, there is an urgent need for effective\ndeepfake detection technologies. Moreover, various deepfake generation\ntechniques have emerged over the past few years. While many deepfake detection\nmethods have been proposed, their performance suffers from new types of\ndeepfake methods on which they are not sufficiently trained. To detect new\ntypes of deepfakes, the model should learn from additional data without losing\nits prior knowledge about deepfakes (catastrophic forgetting), especially when\nnew deepfakes are significantly different. In this work, we employ the\nRepresentation Learning (ReL) and Knowledge Distillation (KD) paradigms to\nintroduce a transfer learning-based Feature Representation Transfer Adaptation\nLearning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on\nnew deepfake datasets while minimizing catastrophic forgetting. Our student\nmodel can quickly adapt to new types of deepfake by distilling knowledge from a\npre-trained teacher model and applying transfer learning without using source\ndomain data during domain adaptation. Through experiments on FaceForensics++\ndatasets, we demonstrate that FReTAL outperforms all baselines on the domain\nadaptation task with up to 86.97% accuracy on low-quality deepfakes.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:54:10 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kim", "Minha", ""], ["Tariq", "Shahroz", ""], ["Woo", "Simon S.", ""]]}, {"id": "2105.13659", "submitter": "Usama Bajwa Dr.", "authors": "Hammad Ud Din Ahmed, Usama Ijaz Bajwa, Fan Zhang, Muhammad Waqas Anwar", "title": "Deception Detection in Videos using the Facial Action Coding System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facts are important in decision making in every situation, which is why it is\nimportant to catch deceptive information before they are accepted as facts.\nDeception detection in videos has gained traction in recent times for its\nvarious real-life application. In our approach, we extract facial action units\nusing the facial action coding system which we use as parameters for training a\ndeep learning model. We specifically use long short-term memory (LSTM) which we\ntrained using the real-life trial dataset and it provided one of the best\nfacial only approaches to deception detection. We also tested cross-dataset\nvalidation using the Real-life trial dataset, the Silesian Deception Dataset,\nand the Bag-of-lies Deception Dataset which has not yet been attempted by\nanyone else for a deception detection system. We tested and compared all\ndatasets amongst each other individually and collectively using the same deep\nlearning training model. The results show that adding different datasets for\ntraining worsen the accuracy of the model. One of the primary reasons is that\nthe nature of these datasets vastly differs from one another.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:10:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahmed", "Hammad Ud Din", ""], ["Bajwa", "Usama Ijaz", ""], ["Zhang", "Fan", ""], ["Anwar", "Muhammad Waqas", ""]]}, {"id": "2105.13677", "submitter": "Qing-Long Zhang", "authors": "Qinglong Zhang and Yubin Yang", "title": "ResT: An Efficient Transformer for Visual Recognition", "comments": "ResT is an efficient multi-scale vision Transformer that can tackle\n  input images with arbitrary size. arXiv admin note: text overlap with\n  arXiv:2103.14030 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an efficient multi-scale vision Transformer, called ResT,\nthat capably served as a general-purpose backbone for image recognition. Unlike\nexisting Transformer methods, which employ standard Transformer blocks to\ntackle raw images with a fixed resolution, our ResT have several advantages:\n(1) A memory-efficient multi-head self-attention is built, which compresses the\nmemory by a simple depth-wise convolution, and projects the interaction across\nthe attention-heads dimension while keeping the diversity ability of\nmulti-heads; (2) Position encoding is constructed as spatial attention, which\nis more flexible and can tackle with input images of arbitrary size without\ninterpolation or fine-tune; (3) Instead of the straightforward tokenization at\nthe beginning of each stage, we design the patch embedding as a stack of\noverlapping convolution operation with stride on the 2D-reshaped token map. We\ncomprehensively validate ResT on image classification and downstream tasks.\nExperimental results show that the proposed ResT can outperform the recently\nstate-of-the-art backbones by a large margin, demonstrating the potential of\nResT as strong backbones. The code and models will be made publicly available\nat https://github.com/wofmanaf/ResT.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:53:54 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:16:31 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 09:42:56 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 08:12:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Qinglong", ""], ["Yang", "Yubin", ""]]}, {"id": "2105.13680", "submitter": "Huan Jin", "authors": "Zhan Qu, Huan Jin, Yang Zhou, Zhen Yang, Wei Zhang", "title": "Focus on Local: Detecting Lane Marker from Bottom Up via Key Point", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream lane marker detection methods are implemented by predicting the\noverall structure and deriving parametric curves through post-processing.\nComplex lane line shapes require high-dimensional output of CNNs to model\nglobal structures, which further increases the demand for model capacity and\ntraining data. In contrast, the locality of a lane marker has finite geometric\nvariations and spatial coverage. We propose a novel lane marker detection\nsolution, FOLOLane, that focuses on modeling local patterns and achieving\nprediction of global structures in a bottom-up manner. Specifically, the CNN\nmodels lowcomplexity local patterns with two separate heads, the first one\npredicts the existence of key points, and the second refines the location of\nkey points in the local range and correlates key points of the same lane line.\nThe locality of the task is consistent with the limited FOV of the feature in\nCNN, which in turn leads to more stable training and better generalization. In\naddition, an efficiency-oriented decoding algorithm was proposed as well as a\ngreedy one, which achieving 36% runtime gains at the cost of negligible\nperformance degradation. Both of the two decoders integrated local information\ninto the global geometry of lane markers. In the absence of a complex network\narchitecture design, the proposed method greatly outperforms all existing\nmethods on public datasets while achieving the best state-of-the-art results\nand real-time processing simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:59:14 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Qu", "Zhan", ""], ["Jin", "Huan", ""], ["Zhou", "Yang", ""], ["Yang", "Zhen", ""], ["Zhang", "Wei", ""]]}, {"id": "2105.13688", "submitter": "Victor Besnier", "authors": "Victor Besnier, David Picard, Alexandre Briot", "title": "Learning Uncertainty For Safety-Oriented Semantic Segmentation In\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we show how uncertainty estimation can be leveraged to enable\nsafety critical image segmentation in autonomous driving, by triggering a\nfallback behavior if a target accuracy cannot be guaranteed. We introduce a new\nuncertainty measure based on disagreeing predictions as measured by a\ndissimilarity function. We propose to estimate this dissimilarity by training a\ndeep neural architecture in parallel to the task-specific network. It allows\nthis observer to be dedicated to the uncertainty estimation, and let the\ntask-specific network make predictions. We propose to use self-supervision to\ntrain the observer, which implies that our method does not require additional\ntraining data. We show experimentally that our proposed approach is much less\ncomputationally intensive at inference time than competing methods (e.g.\nMCDropout), while delivering better results on safety-oriented evaluation\nmetrics on the CamVid dataset, especially in the case of glare artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 09:23:05 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Besnier", "Victor", ""], ["Picard", "David", ""], ["Briot", "Alexandre", ""]]}, {"id": "2105.13695", "submitter": "Ming Sun", "authors": "Ming Sun, Haoxuan Dou, Baopu Li, Lei Cui, Junjie Yan, Wanli Ouyang", "title": "AutoSampling: Search for Effective Data Sampling Schedules", "comments": "Automl for sampling firstly without any assumpation", "journal-ref": "ICML 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sampling acts as a pivotal role in training deep learning models.\nHowever, an effective sampling schedule is difficult to learn due to the\ninherently high dimension of parameters in learning the sampling schedule. In\nthis paper, we propose an AutoSampling method to automatically learn sampling\nschedules for model training, which consists of the multi-exploitation step\naiming for optimal local sampling schedules and the exploration step for the\nideal sampling distribution. More specifically, we achieve sampling schedule\nsearch with shortened exploitation cycle to provide enough supervision. In\naddition, we periodically estimate the sampling distribution from the learned\nsampling schedules and perturb it to search in the distribution space. The\ncombination of two searches allows us to learn a robust sampling schedule. We\napply our AutoSampling method to a variety of image classification tasks\nillustrating the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 09:39:41 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Ming", ""], ["Dou", "Haoxuan", ""], ["Li", "Baopu", ""], ["Cui", "Lei", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2105.13731", "submitter": "Yongtao Hu", "authors": "Zhuming Zhang, Yongtao Hu, Guoxing Yu, Jingwen Dai", "title": "DeepTag: A General Framework for Fiducial Marker Design and Detection", "comments": "preprint in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fiducial marker system usually consists of markers, a detection algorithm,\nand a coding system. The appearance of markers and the detection robustness are\ngenerally limited by the existing detection algorithms, which are hand-crafted\nwith traditional low-level image processing techniques. Furthermore, a\nsophisticatedly designed coding system is required to overcome the shortcomings\nof both markers and detection algorithms. To improve the flexibility and\nrobustness in various applications, we propose a general deep learning based\nframework, DeepTag, for fiducial marker design and detection. DeepTag not only\nsupports detection of a wide variety of existing marker families, but also\nmakes it possible to design new marker families with customized local patterns.\nMoreover, we propose an effective procedure to synthesize training data on the\nfly without manual annotations. Thus, DeepTag can easily adapt to existing and\nnewly-designed marker families. To validate DeepTag and existing methods,\nbeside existing datasets, we further collect a new large and challenging\ndataset where markers are placed in different view distances and angles.\nExperiments show that DeepTag well supports different marker families and\ngreatly outperforms the existing methods in terms of both detection robustness\nand pose accuracy. Both code and dataset are available at\n\\url{https://herohuyongtao.github.io/research/publications/deep-tag/}.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:54:59 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Zhuming", ""], ["Hu", "Yongtao", ""], ["Yu", "Guoxing", ""], ["Dai", "Jingwen", ""]]}, {"id": "2105.13753", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son, Pung-Hwi Ye", "title": "New Image Captioning Encoder via Semantic Visual Feature Matching for\n  Heavy Rain Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning generates text that describes scenes from input images. It\nhas been developed for high quality images taken in clear weather. However, in\nbad weather conditions, such as heavy rain, snow, and dense fog, the poor\nvisibility owing to rain streaks, rain accumulation, and snowflakes causes a\nserious degradation of image quality. This hinders the extraction of useful\nvisual features and results in deteriorated image captioning performance. To\naddress practical issues, this study introduces a new encoder for captioning\nheavy rain images. The central idea is to transform output features extracted\nfrom heavy rain input images into semantic visual features associated with\nwords and sentence context. To achieve this, a target encoder is initially\ntrained in an encoder-decoder framework to associate visual features with\nsemantic words. Subsequently, the objects in a heavy rain image are rendered\nvisible by using an initial reconstruction subnetwork (IRS) based on a heavy\nrain model. The IRS is then combined with another semantic visual feature\nmatching subnetwork (SVFMS) to match the output features of the IRS with the\nsemantic visual features of the pretrained target encoder. The proposed encoder\nis based on the joint learning of the IRS and SVFMS. It is is trained in an\nend-to-end manner, and then connected to the pretrained decoder for image\ncaptioning. It is experimentally demonstrated that the proposed encoder can\ngenerate semantic visual features associated with words even from heavy rain\nimages, thereby increasing the accuracy of the generated captions.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:40:40 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:03:21 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Son", "Chang-Hwan", ""], ["Ye", "Pung-Hwi", ""]]}, {"id": "2105.13754", "submitter": "Bogdan Trasnea", "authors": "Sorin Grigorescu, Mihai Zaha, Bogdan Trasnea and Cosmin Ginerica", "title": "Embedded Vision for Self-Driving on Forest Roads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forest roads in Romania are unique natural wildlife sites used for recreation\nby countless tourists. In order to protect and maintain these roads, we propose\nRovisLab AMTU (Autonomous Mobile Test Unit), which is a robotic system designed\nto autonomously navigate off-road terrain and inspect if any deforestation or\ndamage occurred along tracked route. AMTU's core component is its embedded\nvision module, optimized for real-time environment perception. For achieving a\nhigh computation speed, we use a learning system to train a multi-task Deep\nNeural Network (DNN) for scene and instance segmentation of objects, while the\nkeypoints required for simultaneous localization and mapping are calculated\nusing a handcrafted FAST feature detector and the Lucas-Kanade tracking\nalgorithm. Both the DNN and the handcrafted backbone are run in parallel on the\nGPU of an NVIDIA AGX Xavier board. We show experimental results on the test\ntrack of our research facility.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:05:08 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Grigorescu", "Sorin", ""], ["Zaha", "Mihai", ""], ["Trasnea", "Bogdan", ""], ["Ginerica", "Cosmin", ""]]}, {"id": "2105.13771", "submitter": "Tuomo Sipola", "authors": "Janne Alatalo, Joni Korpihalkola, Tuomo Sipola, Tero Kokkonen", "title": "Chromatic and spatial analysis of one-pixel attacks against an image\n  classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One-pixel attack is a curious way of deceiving neural network classifier by\nchanging only one pixel in the input image. The full potential and boundaries\nof this attack method are not yet fully understood. In this research, the\nsuccessful and unsuccessful attacks are studied in more detail to illustrate\nthe working mechanisms of a one-pixel attack created using differential\nevolution. The data comes from our earlier studies where we applied the attack\nagainst medical imaging. We used a real breast cancer tissue dataset and a real\nclassifier as the attack target. This research presents ways to analyze\nchromatic and spatial distributions of one-pixel attacks. In addition, we\npresent one-pixel attack confidence maps to illustrate the behavior of the\ntarget classifier. We show that the more effective attacks change the color of\nthe pixel more, and that the successful attacks are situated at the center of\nthe images. This kind of analysis is not only useful for understanding the\nbehavior of the attack but also the qualities of the classifying neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:21:58 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 12:46:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Alatalo", "Janne", ""], ["Korpihalkola", "Joni", ""], ["Sipola", "Tuomo", ""], ["Kokkonen", "Tero", ""]]}, {"id": "2105.13789", "submitter": "Maxwell Hogan Mr", "authors": "Maxwell Hogan, Duarte Rondao, Nabil Aouf, and Olivier Dubois-Matra", "title": "Using Convolutional Neural Networks for Relative Pose Estimation of a\n  Non-Cooperative Spacecraft with Thermal Infrared Imagery", "comments": "14 pages; 11 figures; European Space Agency Guidance, Navigation and\n  Control Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent interest in on-orbit servicing and Active Debris Removal (ADR)\nmissions have driven the need for technologies to enable non-cooperative\nrendezvous manoeuvres. Such manoeuvres put heavy burden on the perception\ncapabilities of a chaser spacecraft. This paper demonstrates Convolutional\nNeural Networks (CNNs) capable of providing an initial coarse pose estimation\nof a target from a passive thermal infrared camera feed. Thermal cameras offer\na promising alternative to visible cameras, which struggle in low light\nconditions and are susceptible to overexposure. Often, thermal information on\nthe target is not available a priori; this paper therefore proposes using\nvisible images to train networks. The robustness of the models is demonstrated\non two different targets, first on synthetic data, and then in a laboratory\nenvironment for a realistic scenario that might be faced during an ADR mission.\nGiven that there is much concern over the use of CNN in critical applications\ndue to their black box nature, we use innovative techniques to explain what is\nimportant to our network and fault conditions.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:51:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hogan", "Maxwell", ""], ["Rondao", "Duarte", ""], ["Aouf", "Nabil", ""], ["Dubois-Matra", "Olivier", ""]]}, {"id": "2105.13793", "submitter": "Atilla Ozgur", "authors": "Burcu Oltu and B\\\"u\\c{s}ra K\\\"ubra Karaca and Hamit Erdem and Atilla\n  \\\"Ozg\\\"ur", "title": "A systematic review of transfer learning based approaches for diabetic\n  retinopathy detection", "comments": "25 pages 9 figures 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cases of diabetes and related diabetic retinopathy (DR) have been increasing\nat an alarming rate in modern times. Early detection of DR is an important\nproblem since it may cause permanent blindness in the late stages. In the last\ntwo decades, many different approaches have been applied in DR detection.\nReviewing academic literature shows that deep neural networks (DNNs) have\nbecome the most preferred approach for DR detection. Among these DNN\napproaches, Convolutional Neural Network (CNN) models are the most used ones in\nthe field of medical image classification. Designing a new CNN architecture is\na tedious and time-consuming approach. Additionally, training an enormous\nnumber of parameters is also a difficult task. Due to this reason, instead of\ntraining CNNs from scratch, using pre-trained models has been suggested in\nrecent years as transfer learning approach. Accordingly, the present study as a\nreview focuses on DNN and Transfer Learning based applications of DR detection\nconsidering 38 publications between 2015 and 2020. The published papers are\nsummarized using 9 figures and 10 tables, giving information about 22\npre-trained CNN models, 12 DR data sets and standard performance metrics.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:58:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Oltu", "Burcu", ""], ["Karaca", "B\u00fc\u015fra K\u00fcbra", ""], ["Erdem", "Hamit", ""], ["\u00d6zg\u00fcr", "Atilla", ""]]}, {"id": "2105.13794", "submitter": "Richard Klein", "authors": "Richard Klein and Turgay Celik", "title": "The Wits Intelligent Teaching System: Detecting Student Engagement\n  During Lectures Using Convolutional Neural Networks", "comments": null, "journal-ref": "2017 IEEE International Conference on Image Processing (ICIP),\n  2017, pp. 2856-2860", "doi": "10.1109/ICIP.2017.8296804", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform contingent teaching and be responsive to students' needs during\nclass, lecturers must be able to quickly assess the state of their audience.\nWhile effective teachers are able to gauge easily the affective state of the\nstudents, as class sizes grow this becomes increasingly difficult and less\nprecise. The Wits Intelligent Teaching System (WITS) aims to assist lecturers\nwith real-time feedback regarding student affect. The focus is primarily on\nrecognising engagement or lack thereof. Student engagement is labelled based on\nbehaviour and postures that are common to classroom settings. These proxies are\nthen used in an observational checklist to construct a dataset of engagement\nupon which a CNN based on AlexNet is successfully trained and which\nsignificantly outperforms a Support Vector Machine approach. The deep learning\napproach provides satisfactory results on a challenging, real-world dataset\nwith significant occlusion, lighting and resolution constraints.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:59:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Klein", "Richard", ""], ["Celik", "Turgay", ""]]}, {"id": "2105.13808", "submitter": "Riccardo de Lutio", "authors": "Riccardo de Lutio, Damon Little, Barbara Ambrose, Serge Belongie", "title": "The Herbarium 2021 Half-Earth Challenge Dataset", "comments": "FGVC8 Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herbarium sheets present a unique view of the world's botanical history,\nevolution, and diversity. This makes them an all-important data source for\nbotanical research. With the increased digitisation of herbaria worldwide and\nthe advances in the fine-grained classification domain that can facilitate\nautomatic identification of herbarium specimens, there are a lot of\nopportunities for supporting research in this field. However, existing datasets\nare either too small, or not diverse enough, in terms of represented taxa,\ngeographic distribution or host institutions. Furthermore, aggregating multiple\ndatasets is difficult as taxa exist under a multitude of different names and\nthe taxonomy requires alignment to a common reference. We present the Herbarium\nHalf-Earth dataset, the largest and most diverse dataset of herbarium specimens\nto date for automatic taxon recognition.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:24:12 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["de Lutio", "Riccardo", ""], ["Little", "Damon", ""], ["Ambrose", "Barbara", ""], ["Belongie", "Serge", ""]]}, {"id": "2105.13825", "submitter": "Zhenghao Chen Mr", "authors": "Zhenghao Chen and Shuhang Gu and Feng Zhu and Jing Xu and Rui Zhao", "title": "Improving Facial Attribute Recognition by Group and Graph Learning", "comments": "ICME2021(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the relationships between attributes is a key challenge for\nimproving multiple facial attribute recognition. In this work, we are concerned\nwith two types of correlations that are spatial and non-spatial relationships.\nFor the spatial correlation, we aggregate attributes with spatial similarity\ninto a part-based group and then introduce a Group Attention Learning to\ngenerate the group attention and the part-based group feature. On the other\nhand, to discover the non-spatial relationship, we model a group-based Graph\nCorrelation Learning to explore affinities of predefined part-based groups. We\nutilize such affinity information to control the communication between all\ngroups and then refine the learned group features. Overall, we propose a\nunified network called Multi-scale Group and Graph Network. It incorporates\nthese two newly proposed learning strategies and produces coarse-to-fine\ngraph-based group features for improving facial attribute recognition.\nComprehensive experiments demonstrate that our approach outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:36:28 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Zhenghao", ""], ["Gu", "Shuhang", ""], ["Zhu", "Feng", ""], ["Xu", "Jing", ""], ["Zhao", "Rui", ""]]}, {"id": "2105.13865", "submitter": "Yi Ke Yun", "authors": "Yi Ke Yun, Chun Wei Tan, Takahiro Tsubono", "title": "Recursive Contour Saliency Blending Network for Accurate Salient Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Contour information plays a vital role in salient object detection. However,\nexcessive false positives remain in predictions from existing contour-based\nmodels due to insufficient contour-saliency fusion. In this work, we designed a\nnetwork for better edge quality in salient object detection. We proposed a\ncontour-saliency blending module to exchange information between contour and\nsaliency. We adopted recursive CNN to increase contour-saliency fusion while\nkeeping the total trainable parameters the same. Furthermore, we designed a\nstage-wise feature extraction module to help the model pick up the most helpful\nfeatures from previous intermediate saliency predictions. Besides, we proposed\ntwo new loss functions, namely Dual Confinement Loss and Confidence Loss, for\nour model to generate better boundary predictions. Evaluation results on five\ncommon benchmark datasets reveal that our model achieves competitive\nstate-of-the-art performance. Last but not least, our model is lightweight and\nfast, with only 27.9 million parameters and real-time inferencing at 31 FPS.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:19:54 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 02:43:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yun", "Yi Ke", ""], ["Tan", "Chun Wei", ""], ["Tsubono", "Takahiro", ""]]}, {"id": "2105.13868", "submitter": "Shuhuai Ren", "authors": "Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren\n  Zhou, Xu Sun, Hongxia Yang", "title": "Learning Relation Alignment for Calibrated Cross-modal Retrieval", "comments": "Accepted by ACL-IJCNLP 2021 main conference (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the achievements of large-scale multimodal pre-training approaches,\ncross-modal retrieval, e.g., image-text retrieval, remains a challenging task.\nTo bridge the semantic gap between the two modalities, previous studies mainly\nfocus on word-region alignment at the object level, lacking the matching\nbetween the linguistic relation among the words and the visual relation among\nthe regions. The neglect of such relation consistency impairs the\ncontextualized representation of image-text pairs and hinders the model\nperformance and the interpretability. In this paper, we first propose a novel\nmetric, Intra-modal Self-attention Distance (ISD), to quantify the relation\nconsistency by measuring the semantic distance between linguistic and visual\nrelations. In response, we present Inter-modal Alignment on Intra-modal\nSelf-attentions (IAIS), a regularized training method to optimize the ISD and\ncalibrate intra-modal self-attentions from the two modalities mutually via\ninter-modal alignment. The IAIS regularizer boosts the performance of\nprevailing models on Flickr30k and MS COCO datasets by a considerable margin,\nwhich demonstrates the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:25:49 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 05:16:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ren", "Shuhuai", ""], ["Lin", "Junyang", ""], ["Zhao", "Guangxiang", ""], ["Men", "Rui", ""], ["Yang", "An", ""], ["Zhou", "Jingren", ""], ["Sun", "Xu", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.13902", "submitter": "Gaston Lenczner", "authors": "Adrien Chan-Hon-Tong and Gaston Lenczner and Aurelien Plyer", "title": "Demotivate adversarial defense in remote sensing", "comments": "4 pages, 1 figure, 2 tables. Conference IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks are currently the state-of-the-art algorithms\nfor many remote sensing applications such as semantic segmentation or object\ndetection. However, these algorithms are extremely sensitive to over-fitting,\ndomain change and adversarial examples specifically designed to fool them.\nWhile adversarial attacks are not a threat in most remote sensing applications,\none could wonder if strengthening networks to adversarial attacks could also\nincrease their resilience to over-fitting and their ability to deal with the\ninherent variety of worldwide data. In this work, we study both adversarial\nretraining and adversarial regularization as adversarial defenses to this\npurpose. However, we show through several experiments on public remote sensing\ndatasets that adversarial robustness seems uncorrelated to geographic and\nover-fitting robustness.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:04:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chan-Hon-Tong", "Adrien", ""], ["Lenczner", "Gaston", ""], ["Plyer", "Aurelien", ""]]}, {"id": "2105.13906", "submitter": "Saif Ur Rehaman", "authors": "Saif Ur Rehman, Muhammad Rashid Razzaq, Muhammad Hadi Hussian", "title": "Training of SSD(Single Shot Detector) for Facial Detection using Nvidia\n  Jetson Nano", "comments": "7 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this project, we have used the computer vision algorithm SSD (Single Shot\ndetector) computer vision algorithm and trained this algorithm from the dataset\nwhich consists of 139 Pictures. Images were labeled using Intel CVAT (Computer\nVision Annotation Tool)\n  We trained this model for facial detection. We have deployed our trained\nmodel and software in the Nvidia Jetson Nano Developer kit. Model code is\nwritten in Pytorch's deep learning framework. The programming language used is\nPython.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:16:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Rehman", "Saif Ur", ""], ["Razzaq", "Muhammad Rashid", ""], ["Hussian", "Muhammad Hadi", ""]]}, {"id": "2105.13926", "submitter": "Oscar Carlsson", "authors": "Jan E. Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander,\n  Fredrik Ohlsson, Christoffer Petersson, Daniel Persson", "title": "Geometric Deep Learning and Equivariant Neural Networks", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the mathematical foundations of geometric deep learning, focusing\non group equivariant and gauge equivariant neural networks. We develop gauge\nequivariant convolutional neural networks on arbitrary manifolds $\\mathcal{M}$\nusing principal bundles with structure group $K$ and equivariant maps between\nsections of associated vector bundles. We also discuss group equivariant neural\nnetworks for homogeneous spaces $\\mathcal{M}=G/K$, which are instead\nequivariant with respect to the global symmetry $G$ on $\\mathcal{M}$. Group\nequivariant layers can be interpreted as intertwiners between induced\nrepresentations of $G$, and we show their relation to gauge equivariant\nconvolutional layers. We analyze several applications of this formalism,\nincluding semantic segmentation and object detection networks. We also discuss\nthe case of spherical networks in great detail, corresponding to the case\n$\\mathcal{M}=S^2=\\mathrm{SO}(3)/\\mathrm{SO}(2)$. Here we emphasize the use of\nFourier analysis involving Wigner matrices, spherical harmonics and\nClebsch-Gordan coefficients for $G=\\mathrm{SO}(3)$, illustrating the power of\nrepresentation theory for deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:41:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Gerken", "Jan E.", ""], ["Aronsson", "Jimmy", ""], ["Carlsson", "Oscar", ""], ["Linander", "Hampus", ""], ["Ohlsson", "Fredrik", ""], ["Petersson", "Christoffer", ""], ["Persson", "Daniel", ""]]}, {"id": "2105.13962", "submitter": "Stan Birchfield", "authors": "Nathan Morrical, Jonathan Tremblay, Yunzhi Lin, Stephen Tyree, Stan\n  Birchfield, Valerio Pascucci, Ingo Wald", "title": "NViSII: A Scriptable Tool for Photorealistic Image Generation", "comments": "SDG Workshop at ICLR 2021. Project page is at\n  https://github.com/owl-project/NVISII", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine\nand the OptiX AI denoiser, designed to generate high-quality synthetic images\nfor research in computer vision and deep learning. Our tool enables the\ndescription and manipulation of complex dynamic 3D scenes containing object\nmeshes, materials, textures, lighting, volumetric data (e.g., smoke), and\nbackgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth\nmaps, normal maps, material properties, and optical flow vectors, can also be\ngenerated. In this work, we discuss design goals, architecture, and\nperformance. We demonstrate the use of data generated by path tracing for\ntraining an object detector and pose estimator, showing improved performance in\nsim-to-real transfer in situations that are difficult for traditional\nraster-based renderers. We offer this tool as an easy-to-use, performant,\nhigh-quality renderer for advancing research in synthetic data generation and\ndeep learning.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:35:32 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Morrical", "Nathan", ""], ["Tremblay", "Jonathan", ""], ["Lin", "Yunzhi", ""], ["Tyree", "Stephen", ""], ["Birchfield", "Stan", ""], ["Pascucci", "Valerio", ""], ["Wald", "Ingo", ""]]}, {"id": "2105.13965", "submitter": "Todd Murphey", "authors": "Taosha Fan, Kalyan Vasudev Alwala, Donglai Xiang, Weipeng Xu, Todd\n  Murphey, Mustafa Mukadam", "title": "Revitalizing Optimization for 3D Human Pose and Shape Estimation: A\n  Sparse Constrained Formulation", "comments": "20 pages, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sparse constrained formulation and from it derive a\nreal-time optimization method for 3D human pose and shape estimation. Our\noptimization method is orders of magnitude faster (avg. 4 ms convergence) than\nexisting optimization methods, while being mathematically equivalent to their\ndense unconstrained formulation. We achieve this by exploiting the underlying\nsparsity and constraints of our formulation to efficiently compute the\nGauss-Newton direction. We show that this computation scales linearly with the\nnumber of joints of a complex 3D human model, in contrast to prior work where\nit scales cubically due to their dense unconstrained formulation. Based on our\noptimization method, we present a real-time motion capture framework that\nestimates 3D human poses and shapes from a single image at over 30 FPS. In\nbenchmarks against state-of-the-art methods on multiple public datasets, our\nframe-work outperforms other optimization methods and achieves competitive\naccuracy against regression methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:44:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Fan", "Taosha", ""], ["Alwala", "Kalyan Vasudev", ""], ["Xiang", "Donglai", ""], ["Xu", "Weipeng", ""], ["Murphey", "Todd", ""], ["Mukadam", "Mustafa", ""]]}, {"id": "2105.13978", "submitter": "Lingxi Xie", "authors": "Lingxi Xie, Xiaopeng Zhang, Longhui Wei, Jianlong Chang, Qi Tian", "title": "What Is Considered Complete for Visual Recognition?", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is an opinion paper. We hope to deliver a key message that current\nvisual recognition systems are far from complete, i.e., recognizing everything\nthat human can recognize, yet it is very unlikely that the gap can be bridged\nby continuously increasing human annotations. Based on the observation, we\nadvocate for a new type of pre-training task named learning-by-compression. The\ncomputational models (e.g., a deep network) are optimized to represent the\nvisual data using compact features, and the features preserve the ability to\nrecover the original data. Semantic annotations, when available, play the role\nof weak supervision. An important yet challenging issue is the evaluation of\nimage recovery, where we suggest some design principles and future research\ndirections. We hope our proposal can inspire the community to pursue the\ncompression-recovery tradeoff rather than the accuracy-complexity tradeoff.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:59:14 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Xie", "Lingxi", ""], ["Zhang", "Xiaopeng", ""], ["Wei", "Longhui", ""], ["Chang", "Jianlong", ""], ["Tian", "Qi", ""]]}, {"id": "2105.13979", "submitter": "Ekaterina Nepovinnykh Mrs", "authors": "Ilja Chelak, Ekaterina Nepovinnykh, Tuomas Eerola, Heikki Kalviainen,\n  Igor Belykh", "title": "EDEN: Deep Feature Distribution Pooling for Saimaa Ringed Seals Pattern\n  Matching", "comments": "10 pages, 4 figures,submitted to 2nd International Conference on\n  Cyber-Physical Systems & Control (CPS&C'2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, pelage pattern matching is considered to solve the individual\nre-identification of the Saimaa ringed seals. Animal re-identification together\nwith the access to large amount of image material through camera traps and\ncrowd-sourcing provide novel possibilities for animal monitoring and\nconservation. We propose a novel feature pooling approach that allow\naggregating the local pattern features to get a fixed size embedding vector\nthat incorporate global features by taking into account the spatial\ndistribution of features. This is obtained by eigen decomposition of\ncovariances computed for probability mass functions representing feature maps.\nEmbedding vectors can then be used to find the best match in the database of\nknown individuals allowing animal re-identification. The results show that the\nproposed pooling method outperforms the existing methods on the challenging\nSaimaa ringed seal image data.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:59:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chelak", "Ilja", ""], ["Nepovinnykh", "Ekaterina", ""], ["Eerola", "Tuomas", ""], ["Kalviainen", "Heikki", ""], ["Belykh", "Igor", ""]]}, {"id": "2105.13993", "submitter": "Xuzhe Zhang", "authors": "Xuzhe Zhang, Xinzi He, Jia Guo, Nabil Ettehadi, Natalie Aw, David\n  Semanek, Jonathan Posner, Andrew Laine, Yun Wang", "title": "PTNet: A High-Resolution Infant MRI Synthesizer Based on Transformer", "comments": "arXiv Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) noninvasively provides critical information\nabout how human brain structures develop across stages of life. Developmental\nscientists are particularly interested in the first few years of\nneurodevelopment. Despite the success of MRI collection and analysis for\nadults, it is a challenge for researchers to collect high-quality multimodal\nMRIs from developing infants mainly because of their irregular sleep pattern,\nlimited attention, inability to follow instructions to stay still, and a lack\nof analysis approaches. These challenges often lead to a significant reduction\nof usable data. To address this issue, researchers have explored various\nsolutions to replace corrupted scans through synthesizing realistic MRIs. Among\nthem, the convolution neural network (CNN) based generative adversarial network\nhas demonstrated promising results and achieves state-of-the-art performance.\nHowever, adversarial training is unstable and may need careful tuning of\nregularization terms to stabilize the training. In this study, we introduced a\nnovel MRI synthesis framework - Pyramid Transformer Net (PTNet). PTNet consists\nof transformer layers, skip-connections, and multi-scale pyramid\nrepresentation. Compared with the most widely used CNN-based conditional GAN\nmodels (namely pix2pix and pix2pixHD), our model PTNet shows superior\nperformance in terms of synthesis accuracy and model size. Notably, PTNet does\nnot require any type of adversarial training and can be easily trained using\nthe simple mean squared error loss.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:20:19 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Xuzhe", ""], ["He", "Xinzi", ""], ["Guo", "Jia", ""], ["Ettehadi", "Nabil", ""], ["Aw", "Natalie", ""], ["Semanek", "David", ""], ["Posner", "Jonathan", ""], ["Laine", "Andrew", ""], ["Wang", "Yun", ""]]}, {"id": "2105.13994", "submitter": "Keren Ye", "authors": "Keren Ye and Adriana Kovashka", "title": "Linguistic Structures as Weak Supervision for Visual Scene Graph\n  Generation", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work in scene graph generation requires categorical supervision at the\nlevel of triplets - subjects and objects, and predicates that relate them,\neither with or without bounding box information. However, scene graph\ngeneration is a holistic task: thus holistic, contextual supervision should\nintuitively improve performance. In this work, we explore how linguistic\nstructures in captions can benefit scene graph generation. Our method captures\nthe information provided in captions about relations between individual\ntriplets, and context for subjects and objects (e.g. visual properties are\nmentioned). Captions are a weaker type of supervision than triplets since the\nalignment between the exhaustive list of human-annotated subjects and objects\nin triplets, and the nouns in captions, is weak. However, given the large and\ndiverse sources of multimodal data on the web (e.g. blog posts with images and\ncaptions), linguistic supervision is more scalable than crowdsourced triplets.\nWe show extensive experimental comparisons against prior methods which leverage\ninstance- and image-level supervision, and ablate our method to show the impact\nof leveraging phrasal and sequential context, and techniques to improve\nlocalization of subjects and objects.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:20:27 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ye", "Keren", ""], ["Kovashka", "Adriana", ""]]}, {"id": "2105.13997", "submitter": "Tingwei Meng", "authors": "J\\'er\\^ome Darbon, Tingwei Meng, Elena Resmerita", "title": "On Hamilton-Jacobi PDEs and image denoising models with certain\n  non-additive noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider image denoising problems formulated as variational problems. It\nis known that Hamilton-Jacobi PDEs govern the solution of such optimization\nproblems when the noise model is additive. In this work, we address certain\nnon-additive noise models and show that they are also related to\nHamilton-Jacobi PDEs. These findings allow us to establish new connections\nbetween additive and non-additive noise imaging models. With these connections,\nsome non-convex models for non-additive noise can be solved by applying convex\noptimization algorithms to the equivalent convex models for additive noise.\nSeveral numerical results are provided for denoising problems with Poisson\nnoise or multiplicative noise.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:21:25 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Darbon", "J\u00e9r\u00f4me", ""], ["Meng", "Tingwei", ""], ["Resmerita", "Elena", ""]]}, {"id": "2105.14009", "submitter": "Juan Tapia Dr.", "authors": "Juan Tapia, Sebastian Gonzalez, Christoph Busch", "title": "Iris Liveness Detection using a Cascade of Dedicated Deep Learning\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Iris pattern recognition has significantly improved the biometric\nauthentication field due to its high stability and uniqueness. Such physical\ncharacteristics have played an essential role in security and other related\nareas. However, presentation attacks, also known as spoofing techniques, can\nbypass biometric authentication systems using artefacts such as printed images,\nartificial eyes, textured contact lenses, etc. Many liveness detection methods\nthat improve the security of these systems have been proposed. The first\nInternational Iris Liveness Detection competition, where the effectiveness of\nliveness detection methods is evaluated, was first launched in 2013, and its\nlatest iteration was held in 2020. This paper proposes a serial architecture\nbased on a MobileNetV2 modification, trained from scratch to classify bona fide\niris images versus presentation attack images. The bona fide class consists of\nlive iris images, whereas the attack presentation instrument classes are\ncomprised of cadaver, printed, and contact lenses images, for a total of four\nscenarios. All the images were pre-processed and weighted per class to present\na fair evaluation. This proposal won the LivDet-Iris 2020 competition using\ntwo-class scenarios. Additionally, we present new three-class and four-class\nscenarios that further improve the competition results. This approach is\nprimarily focused in detecting the bona fide class over improving the detection\nof presentation attack instruments. For the two, three, and four classes\nscenarios, an Equal Error Rate (EER) of 4.04\\%, 0.33\\%, and 4,53\\% was obtained\nrespectively. Overall, the best serial model proposed, using three scenarios,\nreached an ERR of 0.33\\% with an Attack Presentation Classification Error Rate\n(APCER) of 0.0100 and a Bona Fide Classification Error Rate (BPCER) of 0.000.\nThis work outperforms the LivDet-Iris 2020 competition results.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:37:11 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tapia", "Juan", ""], ["Gonzalez", "Sebastian", ""], ["Busch", "Christoph", ""]]}, {"id": "2105.14021", "submitter": "SeyedMahdi HosseiniMiangoleh", "authors": "S. Mahdi H. Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris,\n  Ya\\u{g}{\\i}z Aksoy", "title": "Boosting Monocular Depth Estimation Models to High-Resolution via\n  Content-Adaptive Multi-Resolution Merging", "comments": "For more details visit http://yaksoy.github.io/highresdepth/", "journal-ref": "Proc. CVPR (2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks have shown great abilities in estimating depth from a single\nimage. However, the inferred depth maps are well below one-megapixel resolution\nand often lack fine-grained details, which limits their practicality. Our\nmethod builds on our analysis on how the input resolution and the scene\nstructure affects depth estimation performance. We demonstrate that there is a\ntrade-off between a consistent scene structure and the high-frequency details,\nand merge low- and high-resolution estimations to take advantage of this\nduality using a simple depth merging network. We present a double estimation\nmethod that improves the whole-image depth estimation and a patch selection\nmethod that adds local details to the final result. We demonstrate that by\nmerging estimations at different resolutions with changing context, we can\ngenerate multi-megapixel depth maps with a high level of detail using a\npre-trained model.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:55:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Miangoleh", "S. Mahdi H.", ""], ["Dille", "Sebastian", ""], ["Mai", "Long", ""], ["Paris", "Sylvain", ""], ["Aksoy", "Ya\u011f\u0131z", ""]]}, {"id": "2105.14033", "submitter": "Heng Yang", "authors": "Heng Yang, Ling Liang, Kim-Chuan Toh, Luca Carlone", "title": "STRIDE along Spectrahedral Vertices for Solving Large-Scale Rank-One\n  Semidefinite Relaxations", "comments": "9 pages main context, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider solving high-order semidefinite programming (SDP) relaxations of\nnonconvex polynomial optimization problems (POPs) that admit rank-one optimal\nsolutions. Existing approaches, which solve the SDP independently from the POP,\neither cannot scale to large problems or suffer from slow convergence due to\nthe typical degeneracy of such SDPs. We propose a new algorithmic framework,\ncalled SpecTrahedral pRoximal gradIent Descent along vErtices (STRIDE), that\nblends fast local search on the nonconvex POP with global descent on the convex\nSDP. Specifically, STRIDE follows a globally convergent trajectory driven by a\nproximal gradient method (PGM) for solving the SDP, while simultaneously\nprobing long, but safeguarded, rank-one \"strides\", generated by fast nonlinear\nprogramming algorithms on the POP, to seek rapid descent. We prove STRIDE has\nglobal convergence. To solve the subproblem of projecting a given point onto\nthe feasible set of the SDP, we reformulate the projection step as a\ncontinuously differentiable unconstrained optimization and apply a\nlimited-memory BFGS method to achieve both scalability and accuracy. We conduct\nnumerical experiments on solving second-order SDP relaxations arising from two\nimportant applications in machine learning and computer vision. STRIDE\ndominates a diverse set of five existing SDP solvers and is the only solver\nthat can solve degenerate rank-one SDPs to high accuracy (e.g., KKT residuals\nbelow 1e-9), even in the presence of millions of equality constraints.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:07:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Heng", ""], ["Liang", "Ling", ""], ["Toh", "Kim-Chuan", ""], ["Carlone", "Luca", ""]]}, {"id": "2105.14065", "submitter": "Xinyi Li", "authors": "Xinyi Li, Haibin Ling", "title": "TransCamP: Graph Transformer for 6-DoF Camera Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camera pose estimation or camera relocalization is the centerpiece in\nnumerous computer vision tasks such as visual odometry, structure from motion\n(SfM) and SLAM. In this paper we propose a neural network approach with a graph\ntransformer backbone, namely TransCamP, to address the camera relocalization\nproblem. In contrast with prior work where the pose regression is mainly guided\nby photometric consistency, TransCamP effectively fuses the image features,\ncamera pose information and inter-frame relative camera motions into encoded\ngraph attributes and is trained towards the graph consistency and accuracy\ninstead, yielding significantly higher computational efficiency. By leveraging\ngraph transformer layers with edge features and enabling tensorized adjacency\nmatrix, TransCamP dynamically captures the global attention and thus endows the\npose graph with evolving structures to achieve improved robustness and\naccuracy. In addition, optional temporal transformer layers actively enhance\nthe spatiotemporal inter-frame relation for sequential inputs. Evaluation of\nthe proposed network on various public benchmarks demonstrates that TransCamP\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:08:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Xinyi", ""], ["Ling", "Haibin", ""]]}, {"id": "2105.14071", "submitter": "Soumick Chatterjee", "authors": "Soumick Chatterjee, Faraz Ahmed Nizamani, Andreas N\\\"urnberger and\n  Oliver Speck", "title": "Classification of Brain Tumours in MR Images using Deep Spatiospatial\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain tumour is a mass or cluster of abnormal cells in the brain, which has\nthe possibility of becoming life-threatening because of its ability to invade\nneighbouring tissues and also form metastases. An accurate diagnosis is\nessential for successful treatment planning and magnetic resonance imaging is\nthe principal imaging modality for diagnostic of brain tumours and their\nextent. Deep Learning methods in computer vision applications have shown\nsignificant improvement in recent years, most of which can be credited to the\nfact that a sizeable amount of data is available to train models on, and the\nimprovements in the model architectures yielding better approximations in a\nsupervised setting. Classifying tumours using such deep learning methods has\nmade significant progress with the availability of open datasets with reliable\nannotations. Typically those methods are either 3D models, which use 3D\nvolumetric MRIs or even 2D models considering each slice separately. However,\nby treating the slice spatial dimension separately, spatiotemporal models can\nbe employed as spatiospatial models for this task. These models have the\ncapabilities of learning specific spatial and temporal relationship, while\nreducing computational costs. This paper uses two spatiotemporal models, ResNet\n(2+1)D and ResNet Mixed Convolution, to classify different types of brain\ntumours. It was observed that both these models performed superior to the pure\n3D convolutional model, ResNet18. Furthermore, it was also observed that\npre-training the models on a different, even unrelated dataset before training\nthem for the task of tumour classification improves the performance. Finally,\nPre-trained ResNet Mixed Convolution was observed to be the best model in these\nexperiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\\%,\nwhile at the same time being the model with the least computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:27:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chatterjee", "Soumick", ""], ["Nizamani", "Faraz Ahmed", ""], ["N\u00fcrnberger", "Andreas", ""], ["Speck", "Oliver", ""]]}, {"id": "2105.14077", "submitter": "George Cazenavette V", "authors": "George Cazenavette, Simon Lucey", "title": "On the Bias Against Inductive Biases", "comments": "Under Review at NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Borrowing from the transformer models that revolutionized the field of\nnatural language processing, self-supervised feature learning for visual tasks\nhas also seen state-of-the-art success using these extremely deep, isotropic\nnetworks. However, the typical AI researcher does not have the resources to\nevaluate, let alone train, a model with several billion parameters and\nquadratic self-attention activations. To facilitate further research, it is\nnecessary to understand the features of these huge transformer models that can\nbe adequately studied by the typical researcher. One interesting characteristic\nof these transformer models is that they remove most of the inductive biases\npresent in classical convolutional networks. In this work, we analyze the\neffect of these and more inductive biases on small to moderately-sized\nisotropic networks used for unsupervised visual feature learning and show that\ntheir removal is not always ideal.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:41:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cazenavette", "George", ""], ["Lucey", "Simon", ""]]}, {"id": "2105.14080", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau, Ke Li, R\\'emi Pich\\'e-Taillefer, Tal\n  Kachman, Ioannis Mitliagkas", "title": "Gotta Go Fast When Generating Data with Score-Based Models", "comments": "Code is available on\n  https://github.com/AlexiaJM/score_sde_fast_sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based (denoising diffusion) generative models have recently gained a\nlot of success in generating realistic and diverse data. These approaches\ndefine a forward diffusion process for transforming data to noise and generate\ndata by reversing it (thereby going from noise to data). Unfortunately, current\nscore-based models generate data very slowly due to the sheer number of score\nnetwork evaluations required by numerical SDE solvers.\n  In this work, we aim to accelerate this process by devising a more efficient\nSDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which\nuses a fixed step size. We found that naively replacing it with other SDE\nsolvers fares poorly - they either result in low-quality samples or become\nslower than EM. To get around this issue, we carefully devise an SDE solver\nwith adaptive step sizes tailored to score-based generative models piece by\npiece. Our solver requires only two score function evaluations, rarely rejects\nsamples, and leads to high-quality samples. Our approach generates data 2 to 10\ntimes faster than EM while achieving better or equal sample quality. For\nhigh-resolution images, our method leads to significantly higher quality\nsamples than all other methods tested. Our SDE solver has the benefit of\nrequiring no step size tuning.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:48:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", ""], ["Li", "Ke", ""], ["Pich\u00e9-Taillefer", "R\u00e9mi", ""], ["Kachman", "Tal", ""], ["Mitliagkas", "Ioannis", ""]]}, {"id": "2105.14086", "submitter": "Xiaopei Wan", "authors": "Xiaopei Wan, Shengjie Chen, Yujiu Yang, Zhenhua Guo, Fangbo Tao", "title": "Augmenting Anchors by the Detector Itself", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is difficult to determine the scale and aspect ratio of anchors for\nanchor-based object detection methods. Current state-of-the-art object\ndetectors either determine anchor parameters according to objects' shape and\nscale in a dataset, or avoid this problem by utilizing anchor-free method. In\nthis paper, we propose a gradient-free anchor augmentation method named AADI,\nwhich means Augmenting Anchors by the Detector Itself. AADI is not an\nanchor-free method, but it converts the scale and aspect ratio of anchors from\na continuous space to a discrete space, which greatly alleviates the problem of\nanchors' designation. Furthermore, AADI does not add any parameters or\nhyper-parameters, which is beneficial for future research and downstream tasks.\nExtensive experiments on COCO dataset show that AADI has obvious advantages for\nboth two-stage and single-stage methods, specifically, AADI achieves at least\n2.1 AP improvements on Faster R-CNN and 1.6 AP improvements on RetinaNet, using\nResNet-50 model. We hope that this simple and cost-efficient method can be\nwidely used in object detection.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:11:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wan", "Xiaopei", ""], ["Chen", "Shengjie", ""], ["Yang", "Yujiu", ""], ["Guo", "Zhenhua", ""], ["Tao", "Fangbo", ""]]}, {"id": "2105.14103", "submitter": "Hanlin Goh", "authors": "Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin\n  Goh, Ruixiang Zhang, Josh Susskind", "title": "An Attention Free Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:45:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Talbott", "Walter", ""], ["Srivastava", "Nitish", ""], ["Huang", "Chen", ""], ["Goh", "Hanlin", ""], ["Zhang", "Ruixiang", ""], ["Susskind", "Josh", ""]]}, {"id": "2105.14106", "submitter": "Francesco Pelosin", "authors": "Francesco Pelosin and Andrea Torsello", "title": "More Is Better: An Analysis of Instance Quantity/Quality Trade-off in\n  Rehearsal-based Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design of machines and algorithms capable of learning in a dynamically\nchanging environment has become an increasingly topical problem with the\nincrease of the size and heterogeneity of data available to learning systems.\nAs a consequence, the key issue of Continual Learning has become that of\naddressing the stability-plasticity dilemma of connectionist systems, as they\nneed to adapt their model without forgetting previously acquired knowledge.\nWithin this context, rehearsal-based methods i.e., solutions in where the\nlearner exploits memory to revisit past data, has proven to be very effective,\nleading to performance at the state-of-the-art. In our study, we propose an\nanalysis of the memory quantity/quality trade-off adopting various data\nreduction approaches to increase the number of instances storable in memory. In\nparticular, we investigate complex instance compression techniques such as deep\nencoders, but also trivial approaches such as image resizing and linear\ndimensionality reduction. Our findings suggest that the optimal trade-off is\nseverely skewed toward instance quantity, where rehearsal approaches with\nseveral heavily compressed instances easily outperform state-of-the-art\napproaches with the same amount of memory at their disposal. Further, in high\nmemory configurations, deep approaches extracting spatial structure combined\nwith extreme resizing (of the order of $8\\times8$ images) yield the best\nresults, while in memory-constrained configurations where deep approaches\ncannot be used due to their memory requirement in training, Extreme Learning\nMachines (ELM) offer a clear advantage.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:05:51 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 13:17:17 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Pelosin", "Francesco", ""], ["Torsello", "Andrea", ""]]}, {"id": "2105.14110", "submitter": "George Cazenavette V", "authors": "George Cazenavette, Manuel Ladron De Guevara", "title": "MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image\n  Translation", "comments": "Under Review for NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While attention-based transformer networks achieve unparalleled success in\nnearly all language tasks, the large number of tokens coupled with the\nquadratic activation memory usage makes them prohibitive for visual tasks. As\nsuch, while language-to-language translation has been revolutionized by the\ntransformer model, convolutional networks remain the de facto solution for\nimage-to-image translation. The recently proposed MLP-Mixer architecture\nalleviates some of the speed and memory issues associated with attention-based\nnetworks while still retaining the long-range connections that make transformer\nmodels desirable. Leveraging this efficient alternative to self-attention, we\npropose a new unpaired image-to-image translation model called MixerGAN: a\nsimpler MLP-based architecture that considers long-distance relationships\nbetween pixels without the need for expensive attention mechanisms.\nQuantitative and qualitative analysis shows that MixerGAN achieves competitive\nresults when compared to prior convolutional-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:12:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cazenavette", "George", ""], ["De Guevara", "Manuel Ladron", ""]]}, {"id": "2105.14117", "submitter": "Dmitrii Shubin", "authors": "Dmitrii Shubin, Danny Eytan, Sebastian D. Goodfellow", "title": "About Explicit Variance Minimization: Training Neural Networks for\n  Medical Imaging With Limited Data Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning methods for computer vision have demonstrated the\neffectiveness of pre-training feature representations, resulting in\nwell-generalizing Deep Neural Networks, even if the annotated data are limited.\nHowever, representation learning techniques require a significant amount of\ntime for model training, with most of it time spent on precise hyper-parameter\noptimization and selection of augmentation techniques. We hypothesized that if\nthe annotated dataset has enough morphological diversity to capture the general\npopulation's as is common in medical imaging, for example, due to conserved\nsimilarities of tissue mythologies, the variance error of the trained model is\nthe prevalent component of the Bias-Variance Trade-off. We propose the Variance\nAware Training (VAT) method that exploits this property by introducing the\nvariance error into the model loss function, i.e., enabling minimizing the\nvariance explicitly. Additionally, we provide the theoretical formulation and\nproof of the proposed method to aid in interpreting the approach. Our method\nrequires selecting only one hyper-parameter and was able to match or improve\nthe state-of-the-art performance of self-supervised methods while achieving an\norder of magnitude reduction in the GPU training time. We validated VAT on\nthree medical imaging datasets from diverse domains and various learning\nobjectives. These included a Magnetic Resonance Imaging (MRI) dataset for the\nheart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography\ndataset for ordinary regression of diabetic retinopathy progression (Kaggle\n2019 APTOS Blindness Detection challenge), and classification of\nhistopathologic scans of lymph node sections (PatchCamelyon dataset).\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:34:04 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:26:20 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 17:43:48 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shubin", "Dmitrii", ""], ["Eytan", "Danny", ""], ["Goodfellow", "Sebastian D.", ""]]}, {"id": "2105.14130", "submitter": "Doga Gunduzalp", "authors": "Doga Gunduzalp, Batuhan Cengiz, Mehmet Ozan Unal, Isa Yildirim", "title": "3D U-NetR: Low Dose Computed Tomography Reconstruction via Deep Learning\n  and 3 Dimensional Convolutions", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduced a novel deep learning based reconstruction\ntechnique using the correlations of all 3 dimensions with each other by taking\ninto account the correlation between 2-dimensional low-dose CT images. Sparse\nor noisy sinograms are back projected to the image domain with FBP operation,\nthen denoising process is applied with a U-Net like 3 dimensional network\ncalled 3D U-NetR. Proposed network is trained with synthetic and real chest CT\nimages, and 2D U-Net is also trained with the same dataset to prove the\nimportance of the 3rd dimension. Proposed network shows better quantitative\nperformance on SSIM and PSNR. More importantly, 3D U-NetR captures medically\ncritical visual details that cannot be visualized by 2D network.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 22:37:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gunduzalp", "Doga", ""], ["Cengiz", "Batuhan", ""], ["Unal", "Mehmet Ozan", ""], ["Yildirim", "Isa", ""]]}, {"id": "2105.14138", "submitter": "Hao Tang", "authors": "Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu\n  Sebe, Elisa Ricci", "title": "Transformer-Based Source-Free Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of source-free domain adaptation (SFDA),\nwhere the source data are not available during target adaptation. Previous\nworks on SFDA mainly focus on aligning the cross-domain distributions. However,\nthey ignore the generalization ability of the pretrained source model, which\nlargely influences the initial target outputs that are vital to the target\nadaptation stage. To address this, we make the interesting observation that the\nmodel accuracy is highly correlated with whether or not attention is focused on\nthe objects in an image. To this end, we propose a generic and effective\nframework based on Transformer, named TransDA, for learning a generalized model\nfor SFDA. Specifically, we apply the Transformer as the attention module and\ninject it into a convolutional network. By doing so, the model is encouraged to\nturn attention towards the object regions, which can effectively improve the\nmodel's generalization ability on the target domains. Moreover, a novel\nself-supervised knowledge distillation approach is proposed to adapt the\nTransformer with target pseudo-labels, thus further encouraging the network to\nfocus on the object regions. Experiments on three domain adaptation tasks,\nincluding closed-set, partial-set, and open-set adaption, demonstrate that\nTransDA can greatly improve the adaptation accuracy and produce\nstate-of-the-art results. The source code and trained models are available at\nhttps://github.com/ygjwd12345/TransDA.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:06:26 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Guanglei", ""], ["Tang", "Hao", ""], ["Zhong", "Zhun", ""], ["Ding", "Mingli", ""], ["Shao", "Ling", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "2105.14148", "submitter": "Kuniaki Saito", "authors": "Kuniaki Saito, Donghyun Kim, Kate Saenko", "title": "OpenMatch: Open-set Consistency Regularization for Semi-supervised\n  Learning with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) is an effective means to leverage unlabeled\ndata to improve a model's performance. Typical SSL methods like FixMatch assume\nthat labeled and unlabeled data share the same label space. However, in\npractice, unlabeled data can contain categories unseen in the labeled set,\ni.e., outliers, which can significantly harm the performance of SSL algorithms.\nTo address this problem, we propose a novel Open-set Semi-Supervised Learning\n(OSSL) approach called OpenMatch. Learning representations of inliers while\nrejecting outliers is essential for the success of OSSL. To this end, OpenMatch\nunifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers.\nThe OVA-classifier outputs the confidence score of a sample being an inlier,\nproviding a threshold to detect outliers. Another key contribution is an\nopen-set soft-consistency regularization loss, which enhances the smoothness of\nthe OVA-classifier with respect to input transformations and greatly improves\noutlier detection. OpenMatch achieves state-of-the-art performance on three\ndatasets, and even outperforms a fully supervised model in detecting outliers\nunseen in unlabeled data on CIFAR10.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:57:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Saito", "Kuniaki", ""], ["Kim", "Donghyun", ""], ["Saenko", "Kate", ""]]}, {"id": "2105.14158", "submitter": "Zhe Wang", "authors": "Zhe Wang, Hao Chen, Xinyu Li, Chunhui Liu, Yuanjun Xiong, Joseph\n  Tighe, Charless Fowlkes", "title": "Unsupervised Action Segmentation with Self-supervised Feature Learning\n  and Co-occurrence Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action segmentation is a task to classify each frame in the video\nwith an action label. However, it is quite expensive to annotate every frame in\na large corpus of videos to construct a comprehensive supervised training\ndataset. Thus in this work we explore a self-supervised method that operates on\na corpus of unlabeled videos and predicts a likely set of temporal segments\nacross the videos. To do this we leverage self-supervised video classification\napproaches to perform unsupervised feature extraction. On top of these features\nwe develop CAP, a novel co-occurrence action parsing algorithm that can not\nonly capture the correlation among sub-actions underlying the structure of\nactivities, but also estimate the temporal trajectory of the sub-actions in an\naccurate and general way. We evaluate on both classic datasets (Breakfast,\n50Salads) and emerging fine-grained action datasets (FineGym) with more complex\nactivity structures and similar sub-actions. Results show that our method\nachieves state-of-the-art performance on all three datasets with up to 22\\%\nimprovement, and can even outperform some weakly-supervised approaches,\ndemonstrating its effectiveness and generalizability.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:29:40 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:14:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Zhe", ""], ["Chen", "Hao", ""], ["Li", "Xinyu", ""], ["Liu", "Chunhui", ""], ["Xiong", "Yuanjun", ""], ["Tighe", "Joseph", ""], ["Fowlkes", "Charless", ""]]}, {"id": "2105.14159", "submitter": "Ben Chugg", "authors": "Ben Chugg, Brandon Anderson, Seiji Eicher, Sandy Lee, Daniel E. Ho", "title": "Enhancing Environmental Enforcement with Near Real-Time Monitoring:\n  Likelihood-Based Detection of Structural Expansion of Intensive Livestock\n  Farms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Environmental enforcement has historically relied on physical,\nresource-intensive, and infrequent inspections. Advances in remote sensing and\ncomputer vision have the potential to augment compliance monitoring, by\nproviding early warning signals of permit violations. We demonstrate a process\nfor rapid identification of significant structural expansion using satellite\nimagery and focusing on Concentrated Animal Feeding Operations (CAFOs) as a\ntest case. Unpermitted expansion has been a particular challenge with CAFOs,\nwhich pose significant health and environmental risks. Using a new hand-labeled\ndataset of 175,736 images of 1,513 CAFOs, we combine state-of-the-art building\nsegmentation with a likelihood-based change-point detection model to provide a\nrobust signal of building expansion (AUC = 0.80). A major advantage of this\napproach is that it is able to work with high-cadence (daily to weekly), but\nlower resolution (3m/pixel), satellite imagery. It is also highly generalizable\nand thus provides a near real-time monitoring tool to prioritize enforcement\nresources to other settings where unpermitted construction poses environmental\nrisk, e.g. zoning, habitat modification, or wetland protection.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:33:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chugg", "Ben", ""], ["Anderson", "Brandon", ""], ["Eicher", "Seiji", ""], ["Lee", "Sandy", ""], ["Ho", "Daniel E.", ""]]}, {"id": "2105.14162", "submitter": "Zhibo Zhang", "authors": "Ruiwen Li (co-first author), Zhibo Zhang (co-first author), Jiani Li,\n  Scott Sanner, Jongseong Jang, Yeonjeong Jeong, Dongsub Shim", "title": "EDDA: Explanation-driven Data Augmentation to Improve Model and\n  Explanation Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent years have seen the introduction of a range of methods for post-hoc\nexplainability of image classifier predictions. However, these post-hoc\nexplanations may not always align perfectly with classifier predictions, which\nposes a significant challenge when attempting to debug models based on such\nexplanations. To this end, we seek a methodology that can improve alignment\nbetween model predictions and explanation method that is both agnostic to the\nmodel and explanation classes and which does not require ground truth\nexplanations. We achieve this through a novel explanation-driven data\naugmentation (EDDA) method that augments the training data with occlusions of\nexisting data stemming from model-explanations; this is based on the simple\nmotivating principle that occluding salient regions for the model prediction\nshould decrease the model confidence in the prediction, while occluding\nnon-salient regions should not change the prediction -- if the model and\nexplainer are aligned. To verify that this augmentation method improves model\nand explainer alignment, we evaluate the methodology on a variety of datasets,\nimage classification models, and explanation methods. We verify in all cases\nthat our explanation-driven data augmentation method improves alignment of the\nmodel and explanation in comparison to no data augmentation and non-explanation\ndriven data augmentation methods. In conclusion, this approach provides a novel\nmodel- and explainer-agnostic methodology for improving alignment between model\npredictions and explanations, which we see as a critical step forward for\npractical deployment and debugging of image classification models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:42:42 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 00:01:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Ruiwen", "", "co-first author"], ["Zhang", "Zhibo", "", "co-first author"], ["Li", "Jiani", ""], ["Sanner", "Scott", ""], ["Jang", "Jongseong", ""], ["Jeong", "Yeonjeong", ""], ["Shim", "Dongsub", ""]]}, {"id": "2105.14173", "submitter": "Aditya Jonnalagadda", "authors": "Aditya Jonnalagadda, William Wang, Miguel P. Eckstein", "title": "FoveaTer: Foveated Transformer for Image Classification", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many animals and humans process the visual field with a varying spatial\nresolution (foveated vision) and use peripheral processing to make eye\nmovements and point the fovea to acquire high-resolution information about\nobjects of interest. This architecture results in computationally efficient\nrapid scene exploration. Recent progress in vision Transformers has brought\nabout new alternatives to the traditionally convolution-reliant computer vision\nsystems. However, these models do not explicitly model the foveated properties\nof the visual system nor the interaction between eye movements and the\nclassification task. We propose foveated Transformer (FoveaTer) model, which\nuses pooling regions and saccadic movements to perform object classification\ntasks using a vision Transformer architecture. Our proposed model pools the\nimage features using squared pooling regions, an approximation to the\nbiologically-inspired foveated architecture, and uses the pooled features as an\ninput to a Transformer Network. It decides on the following fixation location\nbased on the attention assigned by the Transformer to various locations from\nprevious and present fixations. The model uses a confidence threshold to stop\nscene exploration, allowing to dynamically allocate more fixation/computational\nresources to more challenging images. We construct an ensemble model using our\nproposed model and unfoveated model, achieving an accuracy 1.36% below the\nunfoveated model with 22% computational savings. Finally, we demonstrate our\nmodel's robustness against adversarial attacks, where it outperforms the\nunfoveated model.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:54:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jonnalagadda", "Aditya", ""], ["Wang", "William", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "2105.14184", "submitter": "John Brennan Peace", "authors": "J. Brennan Peace, Eric Psota, Yanfeng Liu, Lance C. P\\'erez", "title": "E2ETag: An End-to-End Trainable Method for Generating and Detecting\n  Fiducial Markers", "comments": "Accepted for publication at BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fiducial markers solutions are designed for efficient detection and\ndecoding, however, their ability to stand out in natural environments is\ndifficult to infer from relatively limited analysis. Furthermore, worsening\nperformance in challenging image capture scenarios - such as poor exposure,\nmotion blur, and off-axis viewing - sheds light on their limitations. E2ETag\nintroduces an end-to-end trainable method for designing fiducial markers and a\ncomplimentary detector. By introducing back-propagatable marker augmentation\nand superimposition into training, the method learns to generate markers that\ncan be detected and classified in challenging real-world environments using a\nfully convolutional detector network. Results demonstrate that E2ETag\noutperforms existing methods in ideal conditions and performs much better in\nthe presence of motion blur, contrast fluctuations, noise, and off-axis viewing\nangles. Source code and trained models are available at\nhttps://github.com/jbpeace/E2ETag.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 03:13:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Peace", "J. Brennan", ""], ["Psota", "Eric", ""], ["Liu", "Yanfeng", ""], ["P\u00e9rez", "Lance C.", ""]]}, {"id": "2105.14185", "submitter": "Chunhua Shen", "authors": "Weian Mao and Zhi Tian and Xinlong Wang and Chunhua Shen", "title": "FCPose: Fully Convolutional Multi-Person Pose Estimation with Dynamic\n  Instance-Aware Convolutions", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR) 2021. Code is at https://git.io/AdelaiDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a fully convolutional multi-person pose estimation framework using\ndynamic instance-aware convolutions, termed FCPose. Different from existing\nmethods, which often require ROI (Region of Interest) operations and/or\ngrouping post-processing, FCPose eliminates the ROIs and grouping\npost-processing with dynamic instance-aware keypoint estimation heads. The\ndynamic keypoint heads are conditioned on each instance (person), and can\nencode the instance concept in the dynamically-generated weights of their\nfilters. Moreover, with the strong representation capacity of dynamic\nconvolutions, the keypoint heads in FCPose are designed to be very compact,\nresulting in fast inference and making FCPose have almost constant inference\ntime regardless of the number of persons in the image. For example, on the COCO\ndataset, a real-time version of FCPose using the DLA-34 backbone infers about\n4.5x faster than Mask R-CNN (ResNet-101) (41.67 FPS vs. 9.26FPS) while\nachieving improved performance. FCPose also offers better speed/accuracy\ntrade-off than other state-of-the-art methods. Our experiment results show that\nFCPose is a simple yet effective multi-person pose estimation framework. Code\nis available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 03:24:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mao", "Weian", ""], ["Tian", "Zhi", ""], ["Wang", "Xinlong", ""], ["Shen", "Chunhua", ""]]}, {"id": "2105.14190", "submitter": "Ildar Rakhmatulin", "authors": "R. Ildar", "title": "RaspberryPI for mosquito neutralization by power laser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article for the first time, comprehensive studies of mosquito\nneutralization using machine vision and a 1 W power laser are considered.\nDeveloped laser installation with Raspberry Pi that changing the direction of\nthe laser with a galvanometer. We developed a program for mosquito tracking in\nreal. The possibility of using deep neural networks, Haar cascades, machine\nlearning for mosquito recognition was considered. We considered in detail the\nclassification problems of mosquitoes in images. A recommendation is given for\nthe implementation of this device based on a microcontroller for subsequent use\nas part of an unmanned aerial vehicle. Any harmful insects in the fields can be\nused as objects for control.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 01:38:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ildar", "R.", ""]]}, {"id": "2105.14191", "submitter": "Thomas Haugland Johansen", "authors": "Thomas Haugland Johansen, Steffen Aagaard S{\\o}rensen, Kajsa\n  M{\\o}llersen, Fred Godtliebsen", "title": "Instance Segmentation of Microscopic Foraminifera", "comments": "18 pages, 14 figures. Submitted to Applied Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Foraminifera are single-celled marine organisms that construct shells that\nremain as fossils in the marine sediments. Classifying and counting these\nfossils are important in e.g. paleo-oceanographic and -climatological research.\nHowever, the identification and counting process has been performed manually\nsince the 1800s and is laborious and time-consuming. In this work, we present a\ndeep learning-based instance segmentation model for classifying, detecting, and\nsegmenting microscopic foraminifera. Our model is based on the Mask R-CNN\narchitecture, using model weight parameters that have learned on the COCO\ndetection dataset. We use a fine-tuning approach to adapt the parameters on a\nnovel object detection dataset of more than 7000 microscopic foraminifera and\nsediment grains. The model achieves a (COCO-style) average precision of $0.78\n\\pm 0.00$ on the classification and detection task, and $0.80 \\pm 0.00$ on the\nsegmentation task. When the model is evaluated without challenging sediment\ngrain images, the average precision for both tasks increases to $0.84 \\pm 0.00$\nand $0.86 \\pm 0.00$, respectively. Prediction results are analyzed both\nquantitatively and qualitatively and discussed. Based on our findings we\npropose several directions for future work, and conclude that our proposed\nmodel is an important step towards automating the identification and counting\nof microscopic foraminifera.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:46:22 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Johansen", "Thomas Haugland", ""], ["S\u00f8rensen", "Steffen Aagaard", ""], ["M\u00f8llersen", "Kajsa", ""], ["Godtliebsen", "Fred", ""]]}, {"id": "2105.14192", "submitter": "Tarik A. Rashid", "authors": "Wu Chao, Mohammad Khishe, Mokhtar Mohammadi, Sarkhel H. Taher Karim,\n  Tarik A. Rashid", "title": "Evolving Deep Convolutional Neural Network by Hybrid Sine-Cosine and\n  Extreme Learning Machine for Real-time COVID19 Diagnosis from X-Ray Images", "comments": "28 pages, Soft Computing, 2021", "journal-ref": null, "doi": "10.1007/s00500-021-05839-6", "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID19 pandemic globally and significantly has affected the life and\nhealth of many communities. The early detection of infected patients is\neffective in fighting COVID19. Using radiology (X-Ray) images is perhaps the\nfastest way to diagnose the patients. Thereby, deep Convolutional Neural\nNetworks (CNNs) can be considered as applicable tools to diagnose COVID19\npositive cases. Due to the complicated architecture of a deep CNN, its\nreal-time training and testing become a challenging problem. This paper\nproposes using the Extreme Learning Machine (ELM) instead of the last fully\nconnected layer to address this deficiency. However, the parameters' stochastic\ntuning of ELM's supervised section causes the final model unreliability.\nTherefore, to cope with this problem and maintain network reliability, the\nsine-cosine algorithm was utilized to tune the ELM's parameters. The designed\nnetwork is then benchmarked on the COVID-Xray-5k dataset, and the results are\nverified by a comparative study with canonical deep CNN, ELM optimized by\ncuckoo search, ELM optimized by genetic algorithm, and ELM optimized by whale\noptimization algorithm. The proposed approach outperforms comparative\nbenchmarks with a final accuracy of 98.83% on the COVID-Xray-5k dataset,\nleading to a relative error reduction of 2.33% compared to a canonical deep\nCNN. Even more critical, the designed network's training time is only 0.9421\nmilliseconds and the overall detection test time for 3100 images is 2.721\nseconds.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 19:40:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chao", "Wu", ""], ["Khishe", "Mohammad", ""], ["Mohammadi", "Mokhtar", ""], ["Karim", "Sarkhel H. Taher", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "2105.14195", "submitter": "Omid Jafari", "authors": "Nada Ibrahim, Preeti Maurya, Omid Jafari, Parth Nagarkar", "title": "A Survey of Performance Optimization in Neural Network-Based Video\n  Analytics Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video analytics systems perform automatic events, movements, and actions\nrecognition in a video and make it possible to execute queries on the video. As\na result of a large number of video data that need to be processed, optimizing\nthe performance of video analytics systems has become an important research\ntopic. Neural networks are the state-of-the-art for performing video analytics\ntasks such as video annotation and object detection. Prior survey papers\nconsider application-specific video analytics techniques that improve accuracy\nof the results; however, in this survey paper, we provide a review of the\ntechniques that focus on optimizing the performance of Neural Network-Based\nVideo Analytics Systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:06:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ibrahim", "Nada", ""], ["Maurya", "Preeti", ""], ["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""]]}, {"id": "2105.14196", "submitter": "Qi Zheng", "authors": "Qi Zheng", "title": "Classifying States of Cooking Objects Using Convolutional Neural Network", "comments": "6 pages,9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated cooking machine is a goal for the future. The main aim is to make\nthe cooking process easier, safer, and create human welfare. To allow robots to\naccurately perform the cooking activities, it is important for them to\nunderstand the cooking environment and recognize the objects, especially\ncorrectly identifying the state of the cooking objects. This will significantly\nimprove the correctness of the following cooking recipes. In this project,\nseveral parts of the experiment were conducted to design a robust deep\nconvolutional neural network for classifying the state of the cooking objects\nfrom scratch. The model is evaluated by using various techniques, such as\nadjusting architecture layers, tuning key hyperparameters, and using different\noptimization techniques to maximize the accuracy of state classification.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:26:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zheng", "Qi", ""]]}, {"id": "2105.14202", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Chang Xu, Chao Xu, Chunjing Xu, Tong Zhang", "title": "Universal Adder Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.13200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. We first develop a theoretical foundation for\nAdderNets, by showing that both the single hidden layer AdderNet and the\nwidth-bounded deep AdderNet with ReLU activation functions are universal\nfunction approximators. An approximation bound for AdderNets with a single\nhidden layer is also presented. We further analyze the influence of this new\nsimilarity measure on the optimization of neural network and develop a special\ntraining scheme for AdderNets. Based on the gradient magnitude, an adaptive\nlearning rate strategy is proposed to enhance the training procedure of\nAdderNets. AdderNets can achieve a 75.7% Top-1 accuracy and a 92.3% Top-5\naccuracy using ResNet-50 on the ImageNet dataset without any multiplication in\nthe convolutional layer.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:02:51 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:16:59 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 02:00:56 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 10:39:42 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 09:52:33 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Xu", "Chao", ""], ["Xu", "Chunjing", ""], ["Zhang", "Tong", ""]]}, {"id": "2105.14211", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie\n  Tang, Jingren Zhou, and Hongxia Yang", "title": "UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image synthesis aims to create an image according to some\nmulti-modal guidance in the forms of textual descriptions, reference images,\nand image blocks to preserve, as well as their combinations. In this paper,\ninstead of investigating these control signals separately, we propose a new\ntwo-stage architecture, UFC-BERT, to unify any number of multi-modal controls.\nIn UFC-BERT, both the diverse control signals and the synthesized image are\nuniformly represented as a sequence of discrete tokens to be processed by\nTransformer. Different from existing two-stage autoregressive approaches such\nas DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the\nsecond stage to enhance the holistic consistency of the synthesized image, to\nsupport preserving specified image blocks, and to improve the synthesis speed.\nFurther, we design a progressive algorithm that iteratively improves the\nnon-autoregressively generated image, with the help of two estimators developed\nfor evaluating the compliance with the controls and evaluating the fidelity of\nthe synthesized image, respectively. Extensive experiments on a newly collected\nlarge-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal\nCelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply\nwith flexible multi-modal controls.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:42:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Zhu", ""], ["Ma", "Jianxin", ""], ["Zhou", "Chang", ""], ["Men", "Rui", ""], ["Li", "Zhikang", ""], ["Ding", "Ming", ""], ["Tang", "Jie", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.14217", "submitter": "Bohan Zhuang", "authors": "Zizheng Pan, Bohan Zhuang, Haoyu He, Jing Liu, Jianfei Cai", "title": "Less is More: Pay Less Attention in Vision Transformers", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers have become one of the dominant architectures in deep learning,\nparticularly as a powerful alternative to convolutional neural networks (CNNs)\nin computer vision. However, Transformer training and inference in previous\nworks can be prohibitively expensive due to the quadratic complexity of\nself-attention over a long sequence of representations, especially for\nhigh-resolution dense prediction tasks. To this end, we present a novel Less\nattention vIsion Transformer (LIT), building upon the fact that convolutions,\nfully-connected (FC) layers, and self-attentions have almost equivalent\nmathematical expressions for processing image patch sequences. Specifically, we\npropose a hierarchical Transformer where we use pure multi-layer perceptrons\n(MLPs) to encode rich local patterns in the early stages while applying\nself-attention modules to capture longer dependencies in deeper layers.\nMoreover, we further propose a learned deformable token merging module to\nadaptively fuse informative patches in a non-uniform manner. The proposed LIT\nachieves promising performance on image recognition tasks, including image\nclassification, object detection and instance segmentation, serving as a strong\nbackbone for many vision tasks. Code is available at:\nhttps://github.com/MonashAI/LIT\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:26:07 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:23:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Pan", "Zizheng", ""], ["Zhuang", "Bohan", ""], ["He", "Haoyu", ""], ["Liu", "Jing", ""], ["Cai", "Jianfei", ""]]}, {"id": "2105.14230", "submitter": "Heyi Li", "authors": "Heyi Li, Jinlong Liu, Yunzhi Bai, Huayan Wang, Klaus Mueller", "title": "Transforming the Latent Space of StyleGAN for Real Face Editing", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in semantic manipulation using StyleGAN, semantic\nediting of real faces remains challenging. The gap between the $W$ space and\nthe $W$+ space demands an undesirable trade-off between reconstruction quality\nand editing quality. To solve this problem, we propose to expand the latent\nspace by replacing fully-connected layers in the StyleGAN's mapping network\nwith attention-based transformers. This simple and effective technique\nintegrates the aforementioned two spaces and transforms them into one new\nlatent space called $W$++. Our modified StyleGAN maintains the state-of-the-art\ngeneration quality of the original StyleGAN with moderately better diversity.\nBut more importantly, the proposed $W$++ space achieves superior performance in\nboth reconstruction quality and editing quality. Despite these significant\nadvantages, our $W$++ space supports existing inversion algorithms and editing\nmethods with only negligible modifications thanks to its structural similarity\nwith the $W/W$+ space. Extensive experiments on the FFHQ dataset prove that our\nproposed $W$++ space is evidently more preferable than the previous $W/W$+\nspace for real face editing. The code is publicly available for research\npurposes at https://github.com/AnonSubm2021/TransStyleGAN.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 06:42:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Heyi", ""], ["Liu", "Jinlong", ""], ["Bai", "Yunzhi", ""], ["Wang", "Huayan", ""], ["Mueller", "Klaus", ""]]}, {"id": "2105.14240", "submitter": "Qi Tian", "authors": "Qi Tian, Kun Kuang, Kelu Jiang, Fei Wu, Yisen Wang", "title": "Analysis and Applications of Class-wise Robustness in Adversarial\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial training is one of the most effective approaches to improve model\nrobustness against adversarial examples. However, previous works mainly focus\non the overall robustness of the model, and the in-depth analysis on the role\nof each class involved in adversarial training is still missing. In this paper,\nwe propose to analyze the class-wise robustness in adversarial training. First,\nwe provide a detailed diagnosis of adversarial training on six benchmark\ndatasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.\nSurprisingly, we find that there are remarkable robustness discrepancies among\nclasses, leading to unbalance/unfair class-wise robustness in the robust\nmodels. Furthermore, we keep investigating the relations between classes and\nfind that the unbalanced class-wise robustness is pretty consistent among\ndifferent attack and defense methods. Moreover, we observe that the stronger\nattack methods in adversarial learning achieve performance improvement mainly\nfrom a more successful attack on the vulnerable classes (i.e., classes with\nless robustness). Inspired by these interesting findings, we design a simple\nbut effective attack method based on the traditional PGD attack, named\nTemperature-PGD attack, which proposes to enlarge the robustness disparity\namong classes with a temperature factor on the confidence distribution of each\nimage. Experiments demonstrate our method can achieve a higher attack rate than\nthe PGD attack. Furthermore, from the defense perspective, we also make some\nmodifications in the training and inference phase to improve the robustness of\nthe most vulnerable class, so as to mitigate the large difference in class-wise\nrobustness. We believe our work can contribute to a more comprehensive\nunderstanding of adversarial training as well as rethinking the class-wise\nproperties in robust models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 07:28:35 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 18:00:46 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 07:53:39 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 05:00:03 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 07:00:25 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Tian", "Qi", ""], ["Kuang", "Kun", ""], ["Jiang", "Kelu", ""], ["Wu", "Fei", ""], ["Wang", "Yisen", ""]]}, {"id": "2105.14246", "submitter": "Ashwin Balakrishna", "authors": "Shivin Devgon, Jeffrey Ichnowski, Ashwin Balakrishna, Harry Zhang, Ken\n  Goldberg", "title": "Orienting Novel 3D Objects Using Self-Supervised Learning of Rotation\n  Transforms", "comments": null, "journal-ref": "Conference on Automation Science and Engineering (CASE) 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Orienting objects is a critical component in the automation of many packing\nand assembly tasks. We present an algorithm to orient novel objects given a\ndepth image of the object in its current and desired orientation. We formulate\na self-supervised objective for this problem and train a deep neural network to\nestimate the 3D rotation as parameterized by a quaternion, between these\ncurrent and desired depth images. We then use the trained network in a\nproportional controller to re-orient objects based on the estimated rotation\nbetween the two depth images. Results suggest that in simulation we can rotate\nunseen objects with unknown geometries by up to 30{\\deg} with a median angle\nerror of 1.47{\\deg} over 100 random initial/desired orientations each for 22\nnovel objects. Experiments on physical objects suggest that the controller can\nachieve a median angle error of 4.2{\\deg} over 10 random initial/desired\norientations each for 5 objects.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 08:22:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Devgon", "Shivin", ""], ["Ichnowski", "Jeffrey", ""], ["Balakrishna", "Ashwin", ""], ["Zhang", "Harry", ""], ["Goldberg", "Ken", ""]]}, {"id": "2105.14250", "submitter": "Mikhail Usvyatsov", "authors": "Mikhail Usvyatsov, Anastasia Makarova, Rafael Ballester-Ripoll, Maxim\n  Rakhuba, Andreas Krause, Konrad Schindler", "title": "Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data\n  via Differentiable Cross-Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an end-to-end trainable framework that processes large-scale\nvisual data tensors by looking \\emph{at a fraction of their entries only}. Our\nmethod combines a neural network encoder with a \\emph{tensor train\ndecomposition} to learn a low-rank latent encoding, coupled with\ncross-approximation (CA) to learn the representation through a subset of the\noriginal samples. CA is an adaptive sampling algorithm that is native to tensor\ndecompositions and avoids working with the full high-resolution data\nexplicitly. Instead, it actively selects local representative samples that we\nfetch out-of-core and on-demand. The required number of samples grows only\nlogarithmically with the size of the input. Our implicit representation of the\ntensor in the network enables processing large grids that could not be\notherwise tractable in their uncompressed form. The proposed approach is\nparticularly useful for large-scale multidimensional grid data (e.g., 3D\ntomography), and for tasks that require context over a large receptive field\n(e.g., predicting the medical condition of entire organs). The code will be\navailable at https://github.com/aelphy/c-pic\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 08:39:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Usvyatsov", "Mikhail", ""], ["Makarova", "Anastasia", ""], ["Ballester-Ripoll", "Rafael", ""], ["Rakhuba", "Maxim", ""], ["Krause", "Andreas", ""], ["Schindler", "Konrad", ""]]}, {"id": "2105.14255", "submitter": "Hengrong Lan", "authors": "Hengrong Lan, Juze Zhang, Changchun Yang, and Fei Gao", "title": "Compressed Sensing for Photoacoustic Computed Tomography Using an\n  Untrained Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic (PA) computed tomography (PACT) shows great potentials in\nvarious preclinical and clinical applications. A great number of measurements\nare the premise that obtains a high-quality image, which implies a low imaging\nrate or a high system cost. The artifacts or sidelobes could pollute the image\nif we decrease the number of measured channels or limit the detected view. In\nthis paper, a novel compressed sensing method for PACT using an untrained\nneural network is proposed, which decreases half number of the measured\nchannels and recoveries enough details. This method uses a neural network to\nreconstruct without the requirement for any additional learning based on the\ndeep image prior. The model can reconstruct the image only using a few\ndetections with gradient descent. Our method can cooperate with other existing\nregularization, and further improve the quality. In addition, we introduce a\nshape prior to easily converge the model to the image. We verify the\nfeasibility of untrained network based compressed sensing in PA image\nreconstruction, and compare this method with a conventional method using total\nvariation minimization. The experimental results show that our proposed method\noutperforms 32.72% (SSIM) with the traditional compressed sensing method in the\nsame regularization. It could dramatically reduce the requirement for the\nnumber of transducers, by sparsely sampling the raw PA data, and improve the\nquality of PA image significantly.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 09:01:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lan", "Hengrong", ""], ["Zhang", "Juze", ""], ["Yang", "Changchun", ""], ["Gao", "Fei", ""]]}, {"id": "2105.14257", "submitter": "Korbinian Abstreiter", "authors": "Korbinian Abstreiter, Stefan Bauer, Arash Mehrjou", "title": "Representation Learning in Continuous-Time Score-Based Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Score-based methods represented as stochastic differential equations on a\ncontinuous time domain have recently proven successful as a non-adversarial\ngenerative model. Training such models relies on denoising score matching,\nwhich can be seen as multi-scale denoising autoencoders. Here, we augment the\ndenoising score-matching framework to enable representation learning without\nany supervised signal. GANs and VAEs learn representations by directly\ntransforming latent codes to data samples. In contrast, score-based\nrepresentation learning relies on a new formulation of the denoising\nscore-matching objective and thus encodes information needed for denoising. We\nshow how this difference allows for manual control of the level of detail\nencoded in the representation.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 09:26:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Abstreiter", "Korbinian", ""], ["Bauer", "Stefan", ""], ["Mehrjou", "Arash", ""]]}, {"id": "2105.14259", "submitter": "Mingfu Xue", "authors": "Mingfu Xue, Yinghao Wu, Zhiyu Wu, Yushu Zhang, Jian Wang, Weiqiang Liu", "title": "Detecting Backdoor in Deep Neural Networks via Intentional Adversarial\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches show that deep learning model is susceptible to backdoor\nattacks. Many defenses against backdoor attacks have been proposed. However,\nexisting defense works require high computational overhead or backdoor attack\ninformation such as the trigger size, which is difficult to satisfy in\nrealistic scenarios. In this paper, a novel backdoor detection method based on\nadversarial examples is proposed. The proposed method leverages intentional\nadversarial perturbations to detect whether an image contains a trigger, which\ncan be applied in both the training stage and the inference stage (sanitize the\ntraining set in training stage and detect the backdoor instances in inference\nstage). Specifically, given an untrusted image, the adversarial perturbation is\nadded to the image intentionally. If the prediction of the model on the\nperturbed image is consistent with that on the unperturbed image, the input\nimage will be considered as a backdoor instance. Compared with most existing\ndefense works, the proposed adversarial perturbation based method requires low\ncomputational resources and maintains the visual quality of the images.\nExperimental results show that, the backdoor detection rate of the proposed\ndefense method is 99.63%, 99.76% and 99.91% on Fashion-MNIST, CIFAR-10 and\nGTSRB datasets, respectively. Besides, the proposed method maintains the visual\nquality of the image as the l2 norm of the added perturbation are as low as\n2.8715, 3.0513 and 2.4362 on Fashion-MNIST, CIFAR-10 and GTSRB datasets,\nrespectively. In addition, it is also demonstrated that the proposed method can\nachieve high defense performance against backdoor attacks under different\nattack settings (trigger transparency, trigger size and trigger pattern).\nCompared with the existing defense work (STRIP), the proposed method has better\ndetection performance on all the three datasets, and is more efficient than\nSTRIP.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 09:33:05 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 12:30:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Xue", "Mingfu", ""], ["Wu", "Yinghao", ""], ["Wu", "Zhiyu", ""], ["Zhang", "Yushu", ""], ["Wang", "Jian", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2105.14275", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin and Matthew B. Blaschko", "title": "Greedy Bayesian Posterior Approximation with Deep Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensembles of independently trained neural networks are a state-of-the-art\napproach to estimate predictive uncertainty in Deep Learning, and can be\ninterpreted as an approximation of the posterior distribution via a mixture of\ndelta functions. The training of ensembles relies on non-convexity of the loss\nlandscape and random initialization of their individual members, making the\nresulting posterior approximation uncontrolled. This paper proposes a novel and\nprincipled method to tackle this limitation, minimizing an $f$-divergence\nbetween the true posterior and a kernel density estimator in a function space.\nWe analyze this objective from a combinatorial point of view, and show that it\nis submodular with respect to mixture components for any $f$. Subsequently, we\nconsider the problem of greedy ensemble construction, and from the marginal\ngain of the total objective, we derive a novel diversity term for ensemble\nmethods. The performance of our approach is demonstrated on computer vision\nout-of-distribution benchmarks in a range of architectures trained on multiple\ndatasets. The source code of our method is publicly available at\nhttps://github.com/MIPT-Oulu/greedy_ensembles_training.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 11:35:27 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 07:29:42 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "2105.14278", "submitter": "Navid Ghassemi", "authors": "Afshin Shoeibi, Navid Ghassemi, Marjane Khodatars, Mahboobeh Jafari,\n  Parisa Moridian, Roohallah Alizadehsani, Ali Khadem, Yinan Kong, Assef Zare,\n  Juan Manuel Gorriz, Javier Ram\\'irez, Maryam Panahiazar, Abbas Khosravi,\n  Saeid Nahavandi", "title": "Applications of Epileptic Seizures Detection in Neuroimaging Modalities\n  Using Deep Learning Techniques: Methods, Challenges, and Future Works", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Epileptic seizures are a type of neurological disorder that affect many\npeople worldwide. Specialist physicians and neurologists take advantage of\nstructural and functional neuroimaging modalities to diagnose various types of\nepileptic seizures. Neuroimaging modalities assist specialist physicians\nconsiderably in analyzing brain tissue and the changes made in it. One method\nto accelerate the accurate and fast diagnosis of epileptic seizures is to\nemploy computer aided diagnosis systems (CADS) based on artificial intelligence\n(AI) and functional and structural neuroimaging modalities. AI encompasses a\nvariety of areas, and one of its branches is deep learning (DL). Not long ago,\nand before the rise of DL algorithms, feature extraction was an essential part\nof every conventional machine learning method, yet handcrafting features limit\nthese models' performances to the knowledge of system designers. DL methods\nresolved this issue entirely by automating the feature extraction and\nclassification process; applications of these methods in many fields of\nmedicine, such as the diagnosis of epileptic seizures, have made notable\nimprovements. In this paper, a comprehensive overview of the types of DL\nmethods exploited to diagnose epileptic seizures from various neuroimaging\nmodalities has been studied. Additionally, rehabilitation systems and cloud\ncomputing in epileptic seizures diagnosis applications have been exactly\ninvestigated using various modalities.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 12:00:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shoeibi", "Afshin", ""], ["Ghassemi", "Navid", ""], ["Khodatars", "Marjane", ""], ["Jafari", "Mahboobeh", ""], ["Moridian", "Parisa", ""], ["Alizadehsani", "Roohallah", ""], ["Khadem", "Ali", ""], ["Kong", "Yinan", ""], ["Zare", "Assef", ""], ["Gorriz", "Juan Manuel", ""], ["Ram\u00edrez", "Javier", ""], ["Panahiazar", "Maryam", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2105.14291", "submitter": "Zhaoxin Fan", "authors": "Zhaoxin Fan, Yazhi Zhu, Yulin He, Qi Sun, Hongyan Liu and Jun He", "title": "Deep Learning on Monocular Object Pose Detection and Tracking: A\n  Comprehensive Overview", "comments": "24 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose detection and tracking has recently attracted increasing\nattention due to its wide applications in many areas, such as autonomous\ndriving, robotics, and augmented reality. Among methods for object pose\ndetection and tracking, deep learning is the most promising one that has shown\nbetter performance than others. However, there is lack of survey study about\nlatest development of deep learning based methods. Therefore, this paper\npresents a comprehensive review of recent progress in object pose detection and\ntracking that belongs to the deep learning technical route. To achieve a more\nthorough introduction, the scope of this paper is limited to methods taking\nmonocular RGB/RGBD data as input, covering three kinds of major tasks:\ninstance-level monocular object pose detection, category-level monocular object\npose detection, and monocular object pose tracking. In our work, metrics,\ndatasets, and methods about both detection and tracking are presented in\ndetail. Comparative results of current state-of-the-art methods on several\npublicly available datasets are also presented, together with insightful\nobservations and inspiring future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 12:59:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fan", "Zhaoxin", ""], ["Zhu", "Yazhi", ""], ["He", "Yulin", ""], ["Sun", "Qi", ""], ["Liu", "Hongyan", ""], ["He", "Jun", ""]]}, {"id": "2105.14300", "submitter": "Zujie Liang", "authors": "Zujie Liang, Haifeng Hu and Jiaying Zhu", "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual\n  Question Answering", "comments": "Accepted by ACM SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462981", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 13:48:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:46:24 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liang", "Zujie", ""], ["Hu", "Haifeng", ""], ["Zhu", "Jiaying", ""]]}, {"id": "2105.14314", "submitter": "Yuanpeng Liu", "authors": "Yuanpeng Liu, Qinglei Hui, Zhiyi Peng, Shaolin Gong and Dexing Kong", "title": "Automatic CT Segmentation from Bounding Box Annotations using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate segmentation for medical images is important for clinical diagnosis.\nExisting automatic segmentation methods are mainly based on fully supervised\nlearning and have an extremely high demand for precise annotations, which are\nvery costly and time-consuming to obtain. To address this problem, we proposed\nan automatic CT segmentation method based on weakly supervised learning, by\nwhich one could train an accurate segmentation model only with weak annotations\nin the form of bounding boxes. The proposed method is composed of two steps: 1)\ngenerating pseudo masks with bounding box annotations by k-means clustering,\nand 2) iteratively training a 3D U-Net convolutional neural network as a\nsegmentation model. Some data pre-processing methods are used to improve\nperformance. The method was validated on four datasets containing three types\nof organs with a total of 627 CT volumes. For liver, spleen and kidney\nsegmentation, it achieved an accuracy of 95.19%, 92.11%, and 91.45%,\nrespectively. Experimental results demonstrate that our method is accurate,\nefficient, and suitable for clinical use.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:48:16 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 16:06:47 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 13:20:08 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liu", "Yuanpeng", ""], ["Hui", "Qinglei", ""], ["Peng", "Zhiyi", ""], ["Gong", "Shaolin", ""], ["Kong", "Dexing", ""]]}, {"id": "2105.14320", "submitter": "Xi-Le Zhao", "authors": "Yi-Si Luo, Xi-Le Zhao, Tai-Xiang Jiang, Yi Chang, Michael K. Ng, and\n  Chao Li", "title": "Self-Supervised Nonlinear Transform-Based Tensor Nuclear Norm for\n  Multi-Dimensional Image Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study multi-dimensional image recovery. Recently,\ntransform-based tensor nuclear norm minimization methods are considered to\ncapture low-rank tensor structures to recover third-order tensors in\nmulti-dimensional image processing applications. The main characteristic of\nsuch methods is to perform the linear transform along the third mode of\nthird-order tensors, and then compute tensor nuclear norm minimization on the\ntransformed tensor so that the underlying low-rank tensors can be recovered.\nThe main aim of this paper is to propose a nonlinear multilayer neural network\nto learn a nonlinear transform via the observed tensor data under\nself-supervision. The proposed network makes use of low-rank representation of\ntransformed tensors and data-fitting between the observed tensor and the\nreconstructed tensor to construct the nonlinear transformation. Extensive\nexperimental results on tensor completion, background subtraction, robust\ntensor completion, and snapshot compressive imaging are presented to\ndemonstrate that the performance of the proposed method is better than that of\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:56:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Luo", "Yi-Si", ""], ["Zhao", "Xi-Le", ""], ["Jiang", "Tai-Xiang", ""], ["Chang", "Yi", ""], ["Ng", "Michael K.", ""], ["Li", "Chao", ""]]}, {"id": "2105.14322", "submitter": "Wei Jan Ko", "authors": "Wei-Jan Ko, Hui-Yu Huang, Yu-Liang Kuo, Chen-Yi Chiu, Li-Heng Wang,\n  Wei-Chen Chiu", "title": "RPG: Learning Recursive Point Cloud Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a novel point cloud generator that is able to\nreconstruct and generate 3D point clouds composed of semantic parts. Given a\nlatent representation of the target 3D model, the generation starts from a\nsingle point and gets expanded recursively to produce the high-resolution point\ncloud via a sequence of point expansion stages. During the recursive procedure\nof generation, we not only obtain the coarse-to-fine point clouds for the\ntarget 3D model from every expansion stage, but also unsupervisedly discover\nthe semantic segmentation of the target model according to the\nhierarchical/parent-child relation between the points across expansion stages.\nMoreover, the expansion modules and other elements used in our recursive\ngenerator are mostly sharing weights thus making the overall framework light\nand efficient. Extensive experiments are conducted to demonstrate that our\nproposed point cloud generator has comparable or even superior performance on\nboth generation and reconstruction tasks in comparison to various baselines, as\nwell as provides the consistent co-segmentation among 3D instances of the same\nobject class.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:01:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ko", "Wei-Jan", ""], ["Huang", "Hui-Yu", ""], ["Kuo", "Yu-Liang", ""], ["Chiu", "Chen-Yi", ""], ["Wang", "Li-Heng", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "2105.14326", "submitter": "Shriya Gupta", "authors": "Shriya T.P. Gupta, Basabdatta Sen Bhattacharya", "title": "Implementing a foveal-pit inspired filter in a Spiking Convolutional\n  Neural Network: a preliminary study", "comments": "8 pages, 8 figures, 4 tables. 2020 International Joint Conference on\n  Neural Networks (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207612", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have presented a Spiking Convolutional Neural Network (SCNN) that\nincorporates retinal foveal-pit inspired Difference of Gaussian filters and\nrank-order encoding. The model is trained using a variant of the\nbackpropagation algorithm adapted to work with spiking neurons, as implemented\nin the Nengo library. We have evaluated the performance of our model on two\npublicly available datasets - one for digit recognition task, and the other for\nvehicle recognition task. The network has achieved up to 90% accuracy, where\nloss is calculated using the cross-entropy function. This is an improvement\nover around 57% accuracy obtained with the alternate approach of performing the\nclassification without any kind of neural filtering. Overall, our\nproof-of-concept study indicates that introducing biologically plausible\nfiltering in existing SCNN architecture will work well with noisy input images\nsuch as those in our vehicle recognition task. Based on our results, we plan to\nenhance our SCNN by integrating lateral inhibition-based redundancy reduction\nprior to rank-ordering, which will further improve the classification accuracy\nby the network.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:28:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gupta", "Shriya T. P.", ""], ["Bhattacharya", "Basabdatta Sen", ""]]}, {"id": "2105.14327", "submitter": "Weihuan Deng", "authors": "Qiqi Zhu, Weihuan Deng, Zhuo Zheng, Yanfei Zhong, Qingfeng Guan,\n  Weihua Lin, Liangpei Zhang, and Deren Li", "title": "A Spectral-Spatial-Dependent Global Learning Framework for Insufficient\n  and Imbalanced Hyperspectral Image Classification", "comments": "14 pages,14 figures", "journal-ref": null, "doi": "10.1109/TCYB.2021.3070577", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been widely applied to hyperspectral image\n(HSI) classification and have achieved great success. However, the deep neural\nnetwork model has a large parameter space and requires a large number of\nlabeled data. Deep learning methods for HSI classification usually follow a\npatchwise learning framework. Recently, a fast patch-free global learning\n(FPGA) architecture was proposed for HSI classification according to global\nspatial context information. However, FPGA has difficulty extracting the most\ndiscriminative features when the sample data is imbalanced. In this paper, a\nspectral-spatial dependent global learning (SSDGL) framework based on global\nconvolutional long short-term memory (GCL) and global joint attention mechanism\n(GJAM) is proposed for insufficient and imbalanced HSI classification. In\nSSDGL, the hierarchically balanced (H-B) sampling strategy and the weighted\nsoftmax loss are proposed to address the imbalanced sample problem. To\neffectively distinguish similar spectral characteristics of land cover types,\nthe GCL module is introduced to extract the long short-term dependency of\nspectral features. To learn the most discriminative feature representations,\nthe GJAM module is proposed to extract attention areas. The experimental\nresults obtained with three public HSI datasets show that the SSDGL has\npowerful performance in insufficient and imbalanced sample problems and is\nsuperior to other state-of-the-art methods. Code can be obtained at:\nhttps://github.com/dengweihuan/SSDGL.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:39:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhu", "Qiqi", ""], ["Deng", "Weihuan", ""], ["Zheng", "Zhuo", ""], ["Zhong", "Yanfei", ""], ["Guan", "Qingfeng", ""], ["Lin", "Weihua", ""], ["Zhang", "Liangpei", ""], ["Li", "Deren", ""]]}, {"id": "2105.14331", "submitter": "Shriya Gupta", "authors": "Shriya T.P. Gupta, Pablo Linares-Serrano, Basabdatta Sen Bhattacharya,\n  Teresa Serrano-Gotarredona", "title": "Foveal-pit inspired filtering of DVS spike response", "comments": "6 pages, 4 figures, 2 tables. 2021 55th Annual Conference on\n  Information Sciences and Systems (CISS), 2021", "journal-ref": null, "doi": "10.1109/CISS50987.2021.9400245", "report-no": null, "categories": "cs.CV cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present results of processing Dynamic Vision Sensor (DVS)\nrecordings of visual patterns with a retinal model based on foveal-pit inspired\nDifference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying\nnumber of vertical white and black bars of different spatial frequencies moving\nhorizontally at a constant velocity. The output spikes generated by the DVS\nsensor were applied as input to a set of DoG filters inspired by the receptive\nfield structure of the primate visual pathway. In particular, these filters\nmimic the receptive fields of the midget and parasol ganglion cells (spiking\nneurons of the retina) that sub-serve the photo-receptors of the foveal-pit.\nThe features extracted with the foveal-pit model are used for further\nclassification using a spiking convolutional neural network trained with a\nbackpropagation variant adapted for spiking neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 16:01:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gupta", "Shriya T. P.", ""], ["Linares-Serrano", "Pablo", ""], ["Bhattacharya", "Basabdatta Sen", ""], ["Serrano-Gotarredona", "Teresa", ""]]}, {"id": "2105.14333", "submitter": "Dinesh J", "authors": "Dinesh J and Mohammed Rhithick A", "title": "Covid-19 diagnosis from x-ray using neural networks", "comments": "3 Graphs, 1 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Corona virus or COVID-19 is a pandemic illness, which has influenced more\nthan million of causalities worldwide and infected a few large number of\nindividuals .Innovative instrument empowering quick screening of the COVID-19\ncontamination with high precision can be critically useful to the medical care\nexperts. The primary clinical device presently being used for the analysis of\nCOVID-19 is the Reverse record polymerase chain response as known as RT-PCR,\nwhich is costly, less-delicate and requires specific clinical work force. X-Ray\nimaging is an effectively available apparatus that can be a great option in the\nCOVID-19 conclusion. This exploration was taken to examine the utility of\ncomputerized reasoning in the quick and exact recognition of COVID-19 from\nchest X-Ray pictures. The point of this paper is to propose a procedure for\nprogrammed recognition of COVID-19 from advanced chest X-Ray images applying\npre-prepared profound learning calculations while boosting the discovery\nexactness. The point is to give over-focused on clinical experts a second pair\nof eyes through a learning picture characterization models. We distinguish an\nappropriate Convolutional Neural Network-CNN model through beginning similar\ninvestigation of a few mainstream CNN models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 16:12:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["J", "Dinesh", ""], ["A", "Mohammed Rhithick", ""]]}, {"id": "2105.14338", "submitter": "Gianluca Gerard", "authors": "Gianluca Gerard, Marco Piastra", "title": "Conditional Deep Convolutional Neural Networks for Improving the\n  Automated Screening of Histopathological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Semantic segmentation of breast cancer metastases in histopathological slides\nis a challenging task. In fact, significant variation in data characteristics\nof histopathology images (domain shift) make generalization of deep learning to\nunseen data difficult. Our goal is to address this challenge by using a\nconditional Fully Convolutional Network (co-FCN) whose output can be\nconditioned at run time, and which can improve its performance when a properly\nselected set of reference slides are used to condition the output. We adapted\nto our task a co-FCN originally applied to organs segmentation in volumetric\nmedical images and we trained it on the Whole Slide Images (WSIs) from three\nout of five medical centers present in the CAMELYON17 dataset. We tested the\nperformance of the network on the WSIs of the remaining centers. We also\ndeveloped an automated selection strategy for selecting the conditioning\nsubset, based on an unsupervised clustering process applied to a\ntarget-specific set of reference patches, followed by a selection policy that\nrelies on the cluster similarities with the input patch. We benchmarked our\nproposed method against a U-Net trained on the same dataset with no\nconditioning. The conditioned network shows better performance that the U-Net\non the WSIs with Isolated Tumor Cells and micro-metastases from the medical\ncenters used as test. Our contributions are an architecture which can be\napplied to the histopathology domain and an automated procedure for the\nselection of conditioning data.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 16:42:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gerard", "Gianluca", ""], ["Piastra", "Marco", ""]]}, {"id": "2105.14355", "submitter": "Jhacson Meza", "authors": "Jhacson Meza, Sonia H. Contreras-Ortiz, Lenny A. Romero, Andres G.\n  Marrugo", "title": "Three-dimensional multimodal medical imaging system based on free-hand\n  ultrasound and structured light", "comments": null, "journal-ref": "Optical Engineering 60(5), 054106 (2021)", "doi": "10.1117/1.OE.60.5.054106", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a three-dimensional (3D) multimodal medical imaging system that\ncombines freehand ultrasound and structured light 3D reconstruction in a single\ncoordinate system without requiring registration. To the best of our knowledge,\nthese techniques have not been combined before as a multimodal imaging\ntechnique. The system complements the internal 3D information acquired with\nultrasound, with the external surface measured with the structure light\ntechnique. Moreover, the ultrasound probe's optical tracking for pose\nestimation was implemented based on a convolutional neural network.\nExperimental results show the system's high accuracy and reproducibility, as\nwell as its potential for preoperative and intraoperative applications. The\nexperimental multimodal error, or the distance from two surfaces obtained with\ndifferent modalities, was 0.12 mm. The code is available as a Github\nrepository.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 18:50:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Meza", "Jhacson", ""], ["Contreras-Ortiz", "Sonia H.", ""], ["Romero", "Lenny A.", ""], ["Marrugo", "Andres G.", ""]]}, {"id": "2105.14370", "submitter": "Ma Bing", "authors": "Deng Yongqiang, Wang Dengjiang, Cao Gang, Ma Bing, Guan Xijia, Wang\n  Yajun, Liu Jianchao, Fang Yanming, Li Juanjuan", "title": "BAAI-VANJEE Roadside Dataset: Towards the Connected Automated Vehicle\n  Highway technologies in Challenging Environments of China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the roadside perception plays an increasingly significant role in the\nConnected Automated Vehicle Highway(CAVH) technologies, there are immediate\nneeds of challenging real-world roadside datasets for bench marking and\ntraining various computer vision tasks such as 2D/3D object detection and\nmulti-sensor fusion. In this paper, we firstly introduce a challenging\nBAAI-VANJEE roadside dataset which consist of LiDAR data and RGB images\ncollected by VANJEE smart base station placed on the roadside about 4.5m high.\nThis dataset contains 2500 frames of LiDAR data, 5000 frames of RGB images,\nincluding 20% collected at the same time. It also contains 12 classes of\nobjects, 74K 3D object annotations and 105K 2D object annotations. By providing\na real complex urban intersections and highway scenes, we expect the\nBAAI-VANJEE roadside dataset will actively assist the academic and industrial\ncircles to accelerate the innovation research and achievement transformation in\nthe field of intelligent transportation in big data era.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 20:40:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yongqiang", "Deng", ""], ["Dengjiang", "Wang", ""], ["Gang", "Cao", ""], ["Bing", "Ma", ""], ["Xijia", "Guan", ""], ["Yajun", "Wang", ""], ["Jianchao", "Liu", ""], ["Yanming", "Fang", ""], ["Juanjuan", "Li", ""]]}, {"id": "2105.14376", "submitter": "Yang He", "authors": "Yang He and Ning Yu and Margret Keuper and Mario Fritz", "title": "Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis", "comments": "To appear in IJCAI2021. Source code at\n  https://github.com/SSAW14/BeyondtheSpectrum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advances in deep generative models over the past years have led to\nhighly {realistic media, known as deepfakes,} that are commonly\nindistinguishable from real to human eyes. These advances make assessing the\nauthenticity of visual data increasingly difficult and pose a misinformation\nthreat to the trustworthiness of visual content in general. Although recent\nwork has shown strong detection accuracy of such deepfakes, the success largely\nrelies on identifying frequency artifacts in the generated images, which will\nnot yield a sustainable detection approach as generative models continue\nevolving and closing the gap to real images. In order to overcome this issue,\nwe propose a novel fake detection that is designed to re-synthesize testing\nimages and extract visual cues for detection. The re-synthesis procedure is\nflexible, allowing us to incorporate a series of visual tasks - we adopt\nsuper-resolution, denoising and colorization as the re-synthesis. We\ndemonstrate the improved effectiveness, cross-GAN generalization, and\nrobustness against perturbations of our approach in a variety of detection\nscenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN\ndatasets. Source code is available at\nhttps://github.com/SSAW14/BeyondtheSpectrum.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 21:22:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["He", "Yang", ""], ["Yu", "Ning", ""], ["Keuper", "Margret", ""], ["Fritz", "Mario", ""]]}, {"id": "2105.14391", "submitter": "Bowen Wen", "authors": "Bowen Wen, Chaitanya Mitash and Kostas Bekris", "title": "Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic\n  Domains", "comments": "CVPR 2021 Workshop on 3D Vision and Robotics. arXiv admin note:\n  substantial text overlap with arXiv:2007.13866", "journal-ref": "CVPR 2021 Workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking the 6D pose of objects in video sequences is important for robot\nmanipulation. This work presents se(3)-TrackNet, a data-driven optimization\napproach for long term, 6D pose tracking. It aims to identify the optimal\nrelative pose given the current RGB-D observation and a synthetic image\nconditioned on the previous best estimate and the object's model. The key\ncontribution in this context is a novel neural network architecture, which\nappropriately disentangles the feature encoding to help reduce domain shift,\nand an effective 3D orientation representation via Lie Algebra. Consequently,\neven when the network is trained solely with synthetic data can work\neffectively over real images. Comprehensive experiments over multiple\nbenchmarks show se(3)-TrackNet achieves consistently robust estimates and\noutperforms alternatives, even though they have been trained with real images.\nThe approach runs in real time at 90.9Hz. Code, data and supplementary video\nfor this project are available at\nhttps://github.com/wenbowen123/iros20-6d-pose-tracking\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 23:56:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wen", "Bowen", ""], ["Mitash", "Chaitanya", ""], ["Bekris", "Kostas", ""]]}, {"id": "2105.14399", "submitter": "David Mac\\^edo", "authors": "David Mac\\^edo, Teresa Ludermir", "title": "Improving Entropic Out-of-Distribution Detection using Isometric\n  Distances and the Minimum Distance Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:55:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:18:07 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 04:02:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mac\u00eado", "David", ""], ["Ludermir", "Teresa", ""]]}, {"id": "2105.14412", "submitter": "Andrei Velichko", "authors": "Hanif Heidari and Andrei Velichko", "title": "An improved LogNNet classifier for IoT application", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet of things devices suffer of low memory while good accuracy is\nneeded. Designing suitable algorithms is vital in this subject. This paper\nproposes a feed forward LogNNet neural network which uses a semi-linear Henon\ntype discrete chaotic map to classify MNIST-10 dataset. The model is composed\nof reservoir part and trainable classifier. The aim of reservoir part is\ntransforming the inputs to maximize the classification accuracy using a special\nmatrix filing method and a time series generated by the chaotic map. The\nparameters of the chaotic map are optimized using particle swarm optimization\nwith random immigrants. The results show that the proposed LogNNet/Henon\nclassifier has higher accuracy and same RAM saving comparable to the original\nversion of LogNNet and has broad prospects for implementation in IoT devices.\nIn addition, the relation between the entropy and accuracy of the\nclassification is investigated. It is shown that there exists a direct relation\nbetween the value of entropy and accuracy of the classification.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:12:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Heidari", "Hanif", ""], ["Velichko", "Andrei", ""]]}, {"id": "2105.14421", "submitter": "Xi Li", "authors": "Pengyi Zhang, Huanzhang Dou, Wenhu Zhang, Yuhan Zhao, Songyuan Li,\n  Zequn Qin, Xi Li", "title": "VersatileGait: A Large-Scale Synthetic Gait Dataset Towards in-the-Wild\n  Simulation", "comments": "We should have updated 2101.01394 but we did a new submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition has a rapid development in recent years. However, gait\nrecognition in the wild is not well explored yet. An obvious reason could be\nascribed to the lack of diverse training data from the perspective of intrinsic\nand extrinsic factors. To remedy this problem, we propose to construct a\nlarge-scale gait dataset with the help of controllable computer simulation. In\ndetail, to diversify the intrinsic factors of gait, we generate numerous\ncharacters with diverse attributes and empower them with various types of\nwalking styles. To diversify the extrinsic factors of gait, we build a\ncomplicated scene with a dense camera layout. Finally, we design an automated\ngeneration toolkit under Unity3D for simulating the walking scenario and\ncapturing the gait data automatically. As a result, we obtain an in-the-wild\ngait dataset, called VersatileGait, which has more than one million silhouette\nsequences of 10,000 subjects with diverse scenarios. VersatileGait possesses\nseveral nice properties, including huge dataset size, diverse pedestrian\nattributes, complicated camera layout, high-quality annotations, small domain\ngap with the real one, good scalability for new demands, and no privacy issues.\nBased on VersatileGait, we propose series of experiments and applications for\nboth research exploration of gait in the wild and practical applications. Our\ndataset and its corresponding generation toolkit will be publicly available for\nfurther studies.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 03:39:53 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 02:53:54 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhang", "Pengyi", ""], ["Dou", "Huanzhang", ""], ["Zhang", "Wenhu", ""], ["Zhao", "Yuhan", ""], ["Li", "Songyuan", ""], ["Qin", "Zequn", ""], ["Li", "Xi", ""]]}, {"id": "2105.14424", "submitter": "Yihua Cheng", "authors": "Yihua Cheng and Feng Lu", "title": "Gaze Estimation using Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has proven the effectiveness of transformers in many computer\nvision tasks. However, the performance of transformers in gaze estimation is\nstill unexplored. In this paper, we employ transformers and assess their\neffectiveness for gaze estimation. We consider two forms of vision transformer\nwhich are pure transformers and hybrid transformers. We first follow the\npopular ViT and employ a pure transformer to estimate gaze from images. On the\nother hand, we preserve the convolutional layers and integrate CNNs as well as\ntransformers. The transformer serves as a component to complement CNNs. We\ncompare the performance of the two transformers in gaze estimation. The Hybrid\ntransformer significantly outperforms the pure transformer in all evaluation\ndatasets with less parameters. We further conduct experiments to assess the\neffectiveness of the hybrid transformer and explore the advantage of\nself-attention mechanism. Experiments show the hybrid transformer can achieve\nstate-of-the-art performance in all benchmarks with pre-training.To facilitate\nfurther research, we release codes and models in\nhttps://github.com/yihuacheng/GazeTR.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 04:06:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cheng", "Yihua", ""], ["Lu", "Feng", ""]]}, {"id": "2105.14426", "submitter": "Pratik Kayal", "authors": "Pratik Kayal, Mrinal Anand, Harsh Desai, Mayank Singh", "title": "ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX", "comments": "This submission has been removed by arXiv administrators because the\n  submitter did not have the right to grant the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tables present important information concisely in many scientific documents.\nVisual features like mathematical symbols, equations, and spanning cells make\nstructure and content extraction from tables embedded in research documents\ndifficult. This paper discusses the dataset, tasks, participants' methods, and\nresults of the ICDAR 2021 Competition on Scientific Table Image Recognition to\nLaTeX. Specifically, the task of the competition is to convert a tabular image\nto its corresponding LaTeX source code. We proposed two subtasks. In Subtask 1,\nwe ask the participants to reconstruct the LaTeX structure code from an image.\nIn Subtask 2, we ask the participants to reconstruct the LaTeX content code\nfrom an image. This report describes the datasets and ground truth\nspecification, details the performance evaluation metrics used, presents the\nfinal results, and summarizes the participating methods. Submission by team\nVCGroup got the highest Exact Match accuracy score of 74% for Subtask 1 and 55%\nfor Subtask 2, beating previous baselines by 5% and 12%, respectively. Although\nimprovements can still be made to the recognition capabilities of models, this\ncompetition contributes to the development of fully automated table recognition\nsystems by challenging practitioners to solve problems under specific\nconstraints and sharing their approaches; the platform will remain available\nfor post-challenge submissions at\nhttps://competitions.codalab.org/competitions/26979 .\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 04:17:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kayal", "Pratik", ""], ["Anand", "Mrinal", ""], ["Desai", "Harsh", ""], ["Singh", "Mayank", ""]]}, {"id": "2105.14430", "submitter": "Jianning Wu", "authors": "Jianning Wu, Zhuqing Jiang, Shiping Wen, Aidong Men, Haiying Wang", "title": "Rethinking the constraints of multimodal fusion: case study in\n  Weakly-Supervised Audio-Visual Video Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multimodal tasks, a good feature extraction network should extract\ninformation as much as possible and ensure that the extracted feature embedding\nand other modal feature embedding have an excellent mutual understanding. The\nlatter is often more critical in feature fusion than the former. Therefore,\nselecting the optimal feature extraction network collocation is a very\nimportant subproblem in multimodal tasks. Most of the existing studies ignore\nthis problem or adopt an ergodic approach. This problem is modeled as an\noptimization problem in this paper. A novel method is proposed to convert the\noptimization problem into an issue of comparative upper bounds by referring to\nthe general practice of extreme value conversion in mathematics. Compared with\nthe traditional method, it reduces the time cost.\n  Meanwhile, aiming at the common problem that the feature similarity and the\nfeature semantic similarity are not aligned in the multimodal time-series\nproblem, we refer to the idea of contrast learning and propose a multimodal\ntime-series contrastive loss(MTSC).\n  Based on the above issues, We demonstrated the feasibility of our approach in\nthe audio-visual video parsing task. Substantial analyses verify that our\nmethods promote the fusion of different modal features.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 05:13:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Jianning", ""], ["Jiang", "Zhuqing", ""], ["Wen", "Shiping", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""]]}, {"id": "2105.14432", "submitter": "Shengcai Liao", "authors": "Shengcai Liao and Ling Shao", "title": "Transformer-Based Deep Image Matching for Generalizable Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have recently gained increasing attention in computer vision.\nHowever, existing studies mostly use Transformers for feature representation\nlearning, e.g. for image classification and dense predictions. In this work, we\nfurther investigate the possibility of applying Transformers for image matching\nand metric learning given pairs of images. We find that the Vision Transformer\n(ViT) and the vanilla Transformer with decoders are not adequate for image\nmatching due to their lack of image-to-image attention. Thus, we further design\ntwo naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery\ncross-attention in the vanilla Transformer. The latter improves the\nperformance, but it is still limited. This implies that the attention mechanism\nin Transformers is primarily designed for global feature aggregation, which is\nnot naturally suitable for image matching. Accordingly, we propose a new\nsimplified decoder, which drops the full attention implementation with the\nsoftmax weighting, keeping only the query-key similarity computation.\nAdditionally, global max pooling and a multilayer perceptron (MLP) head are\napplied to decode the matching result. This way, the simplified decoder is\ncomputationally more efficient, while at the same time more effective for image\nmatching. The proposed method, called TransMatcher, achieves state-of-the-art\nperformance in generalizable person re-identification, with up to 6.1% and 5.7%\nperformance gains in Rank-1 and mAP, respectively, on several popular datasets.\nThe source code of this study will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 05:38:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2105.14447", "submitter": "Hu Zhang", "authors": "Hu Zhang and Keke Zu and Jian Lu and Yuru Zou and Deyu Meng", "title": "EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been demonstrated that the performance of a deep\nconvolutional neural network can be effectively improved by embedding an\nattention module into it. In this work, a novel lightweight and effective\nattention method named Pyramid Squeeze Attention (PSA) module is proposed. By\nreplacing the 3x3 convolution with the PSA module in the bottleneck blocks of\nthe ResNet, a novel representational block named Efficient Pyramid Squeeze\nAttention (EPSA) is obtained. The EPSA block can be easily added as a\nplug-and-play component into a well-established backbone network, and\nsignificant improvements on model performance can be achieved. Hence, a simple\nand efficient backbone architecture named EPSANet is developed in this work by\nstacking these ResNet-style EPSA blocks. Correspondingly, a stronger\nmulti-scale representation ability can be offered by the proposed EPSANet for\nvarious computer vision tasks including but not limited to, image\nclassification, object detection, instance segmentation, etc. Without bells and\nwhistles, the performance of the proposed EPSANet outperforms most of the\nstate-of-the-art channel attention methods. As compared to the SENet-50, the\nTop-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of\n+2.7 box AP for object detection and an improvement of +1.7 mask AP for\ninstance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained.\nOur source code is available at:https://github.com/murufeng/EPSANet.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:26:41 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 13:08:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhang", "Hu", ""], ["Zu", "Keke", ""], ["Lu", "Jian", ""], ["Zou", "Yuru", ""], ["Meng", "Deyu", ""]]}, {"id": "2105.14457", "submitter": "David Chuan-En Lin", "authors": "David Chuan-En Lin, Nikolas Martelaro", "title": "Learning Personal Style from Few Examples", "comments": null, "journal-ref": null, "doi": "10.1145/3461778.3462115", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key task in design work is grasping the client's implicit tastes. Designers\noften do this based on a set of examples from the client. However, recognizing\na common pattern among many intertwining variables such as color, texture, and\nlayout and synthesizing them into a composite preference can be challenging. In\nthis paper, we leverage the pattern recognition capability of computational\nmodels to aid in this task. We offer a set of principles for computationally\nlearning personal style. The principles are manifested in PseudoClient, a deep\nlearning framework that learns a computational model for personal graphic\ndesign style from only a handful of examples. In several experiments, we found\nthat PseudoClient achieves a 79.40% accuracy with only five positive and\nnegative examples, outperforming several alternative methods. Finally, we\ndiscuss how PseudoClient can be utilized as a building block to support the\ndevelopment of future design applications.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 08:04:42 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:30:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lin", "David Chuan-En", ""], ["Martelaro", "Nikolas", ""]]}, {"id": "2105.14477", "submitter": "Yuqing Song", "authors": "Yuqing Song, Shizhe Chen, Qin Jin", "title": "Towards Diverse Paragraph Captioning for Untrimmed Videos", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video paragraph captioning aims to describe multiple events in untrimmed\nvideos with descriptive paragraphs. Existing approaches mainly solve the\nproblem in two steps: event detection and then event captioning. Such two-step\nmanner makes the quality of generated paragraphs highly dependent on the\naccuracy of event proposal detection which is already a challenging task. In\nthis paper, we propose a paragraph captioning model which eschews the\nproblematic event detection stage and directly generates paragraphs for\nuntrimmed videos. To describe coherent and diverse events, we propose to\nenhance the conventional temporal attention with dynamic video memories, which\nprogressively exposes new video features and suppresses over-accessed video\ncontents to control visual focuses of the model. In addition, a\ndiversity-driven training strategy is proposed to improve diversity of\nparagraph on the language perspective. Considering that untrimmed videos\ngenerally contain massive but redundant frames, we further augment the video\nencoder with keyframe awareness to improve efficiency. Experimental results on\nthe ActivityNet and Charades datasets show that our proposed model\nsignificantly outperforms the state-of-the-art performance on both accuracy and\ndiversity metrics without using any event boundary annotations. Code will be\nreleased at https://github.com/syuqings/video-paragraph.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:28:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Song", "Yuqing", ""], ["Chen", "Shizhe", ""], ["Jin", "Qin", ""]]}, {"id": "2105.14513", "submitter": "Qi Lu", "authors": "Qi Lu and Chuyang Ye", "title": "Knowledge Transfer for Few-shot Segmentation of Novel White Matter\n  Tracts", "comments": "accepted by IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved stateof-the-art\nperformance for white matter (WM) tract segmentation based on diffusion\nmagnetic resonance imaging (dMRI). These CNNs require a large number of manual\ndelineations of the WM tracts of interest for training, which are generally\nlabor-intensive and costly. The expensive manual delineation can be a\nparticular disadvantage when novel WM tracts, i.e., tracts that have not been\nincluded in existing manual delineations, are to be analyzed. To accurately\nsegment novel WM tracts, it is desirable to transfer the knowledge learned\nabout existing WM tracts, so that even with only a few delineations of the\nnovel WM tracts, CNNs can learn adequately for the segmentation. In this paper,\nwe explore the transfer of such knowledge to the segmentation of novel WM\ntracts in the few-shot setting. Although a classic fine-tuning strategy can be\nused for the purpose, the information in the last task-specific layer for\nsegmenting existing WM tracts is completely discarded. We hypothesize that the\nweights of this last layer can bear valuable information for segmenting the\nnovel WM tracts and thus completely discarding the information is not optimal.\nIn particular, we assume that the novel WM tracts can correlate with existing\nWM tracts and the segmentation of novel WM tracts can be predicted with the\nlogits of existing WM tracts. In this way, better initialization of the last\nlayer than random initialization can be achieved for fine-tuning. Further, we\nshow that a more adaptive use of the knowledge in the last layer for segmenting\nexisting WM tracts can be conveniently achieved by simply inserting a warmup\nstage before classic fine-tuning. The proposed method was evaluated on a\npublicly available dMRI dataset, where we demonstrate the benefit of our method\nfor few-shot segmentation of novel WM tracts.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:57:41 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 04:54:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lu", "Qi", ""], ["Ye", "Chuyang", ""]]}, {"id": "2105.14520", "submitter": "Jianfeng Li", "authors": "Jianfeng Li, Junqiao Zhao, Shuangfu Song, Tiantian Feng", "title": "Unsupervised Joint Learning of Depth, Optical Flow, Ego-motion from\n  Video", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Estimating geometric elements such as depth, camera motion, and optical flow\nfrom images is an important part of the robot's visual perception. We use a\njoint self-supervised method to estimate the three geometric elements. Depth\nnetwork, optical flow network and camera motion network are independent of each\nother but are jointly optimized during training phase. Compared with\nindependent training, joint training can make full use of the geometric\nrelationship between geometric elements and provide dynamic and static\ninformation of the scene. In this paper, we improve the joint self-supervision\nmethod from three aspects: network structure, dynamic object segmentation, and\ngeometric constraints. In terms of network structure, we apply the attention\nmechanism to the camera motion network, which helps to take advantage of the\nsimilarity of camera movement between frames. And according to attention\nmechanism in Transformer, we propose a plug-and-play convolutional attention\nmodule. In terms of dynamic object, according to the different influences of\ndynamic objects in the optical flow self-supervised framework and the\ndepth-pose self-supervised framework, we propose a threshold algorithm to\ndetect dynamic regions, and mask that in the loss function respectively. In\nterms of geometric constraints, we use traditional methods to estimate the\nfundamental matrix from the corresponding points to constrain the camera motion\nnetwork. We demonstrate the effectiveness of our method on the KITTI dataset.\nCompared with other joint self-supervised methods, our method achieves\nstate-of-the-art performance in the estimation of pose and optical flow, and\nthe depth estimation has also achieved competitive results. Code will be\navailable https://github.com/jianfenglihg/Unsupervised_geometry.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 12:39:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Jianfeng", ""], ["Zhao", "Junqiao", ""], ["Song", "Shuangfu", ""], ["Feng", "Tiantian", ""]]}, {"id": "2105.14538", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, Marcel Worring", "title": "Longer Version for \"Deep Context-Encoding Network for Retinal Image\n  Captioning\"", "comments": "This paper is a longer version of \"Deep Context-Encoding Network for\n  Retinal Image Captioning\" which is accepted by IEEE International Conference\n  on Image Processing (ICIP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating medical reports for retinal images is one of the\npromising ways to help ophthalmologists reduce their workload and improve work\nefficiency. In this work, we propose a new context-driven encoding network to\nautomatically generate medical reports for retinal images. The proposed model\nis mainly composed of a multi-modal input encoder and a fused-feature decoder.\nOur experimental results show that our proposed method is capable of\neffectively leveraging the interactive information between the input image and\ncontext, i.e., keywords in our case. The proposed method creates more accurate\nand meaningful reports for retinal images than baseline models and achieves\nstate-of-the-art performance. This performance is shown in several commonly\nused metrics for the medical report generation task: BLEU-avg (+16%), CIDEr\n(+10.2%), and ROUGE (+8.6%).\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:37:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Yang", "Chao-Han Huck", ""], ["Worring", "Marcel", ""]]}, {"id": "2105.14540", "submitter": "Tashnim Chowdhury", "authors": "Tashnim Chowdhury, Maryam Rahnemoonfar", "title": "Attention Based Semantic Segmentation on UAV Dataset for Natural\n  Disaster Damage Assessment", "comments": "arXiv admin note: text overlap with arXiv:2009.01193", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The detrimental impacts of climate change include stronger and more\ndestructive hurricanes happening all over the world. Identifying different\ndamaged structures of an area including buildings and roads are vital since it\nhelps the rescue team to plan their efforts to minimize the damage caused by a\nnatural disaster. Semantic segmentation helps to identify different parts of an\nimage. We implement a novel self-attention based semantic segmentation model on\na high resolution UAV dataset and attain Mean IoU score of around 88% on the\ntest set. The result inspires to use self-attention schemes in natural disaster\ndamage assessment which will save human lives and reduce economic losses.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:39:03 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 18:11:43 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chowdhury", "Tashnim", ""], ["Rahnemoonfar", "Maryam", ""]]}, {"id": "2105.14548", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Raja Giryes, Niloy J. Mitra, Daniel Cohen-Or", "title": "Z2P: Instant Rendering of Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for rendering point clouds using a neural network.\nExisting point rendering techniques either use splatting, or first reconstruct\na surface mesh that can then be rendered. Both of these techniques require\nsolving for global point normal orientation, which is a challenging problem on\nits own. Furthermore, splatting techniques result in holes and overlaps,\nwhereas mesh reconstruction is particularly challenging, especially in the\ncases of thin surfaces and sheets.\n  We cast the rendering problem as a conditional image-to-image translation\nproblem. In our formulation, Z2P, i.e., depth-augmented point features as\nviewed from target camera view, are directly translated by a neural network to\nrendered images, conditioned on control variables (e.g., color, light). We\navoid inevitable issues with splatting (i.e., holes and overlaps), and bypass\nsolving the notoriously challenging surface reconstruction problem or\nestimating oriented normals. Yet, our approach results in a rendered image as\nif a surface mesh was reconstructed. We demonstrate that our framework produces\na plausible image, and can effectively handle noise, non-uniform sampling, thin\nsurfaces / sheets, and is fast.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:58:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Mitra", "Niloy J.", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.14572", "submitter": "Azim Ahmadzadeh", "authors": "Azim Ahmadzadeh, Dustin J. Kempton, Yang Chen, Rafal A. Angryk", "title": "Multiscale IoU: A Metric for Evaluation of Salient Object Detection with\n  Fine Structures", "comments": "5 pages, 3 figures, ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  General-purpose object-detection algorithms often dismiss the fine structure\nof detected objects. This can be traced back to how their proposed regions are\nevaluated. Our goal is to renegotiate the trade-off between the generality of\nthese algorithms and their coarse detections. In this work, we present a new\nmetric that is a marriage of a popular evaluation metric, namely Intersection\nover Union (IoU), and a geometrical concept, called fractal dimension. We\npropose Multiscale IoU (MIoU) which allows comparison between the detected and\nground-truth regions at multiple resolution levels. Through several\nreproducible examples, we show that MIoU is indeed sensitive to the fine\nboundary structures which are completely overlooked by IoU and f1-score. We\nfurther examine the overall reliability of MIoU by comparing its distribution\nwith that of IoU on synthetic and real-world datasets of objects. We intend\nthis work to re-initiate exploration of new evaluation methods for\nobject-detection algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 15:31:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ahmadzadeh", "Azim", ""], ["Kempton", "Dustin J.", ""], ["Chen", "Yang", ""], ["Angryk", "Rafal A.", ""]]}, {"id": "2105.14576", "submitter": "Yingying Deng", "authors": "Yingying Deng and Fan Tang and Xingjia Pan and Weiming Dong and\n  Chongyang Ma and Changsheng Xu", "title": "StyTr^2: Unbiased Image Style Transfer with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of image style transfer is to render an image with artistic features\nguided by a style reference while maintaining the original content. Due to the\nlocality and spatial invariance in CNNs, it is difficult to extract and\nmaintain the global information of input images. Therefore, traditional neural\nstyle transfer methods are usually biased and content leak can be observed by\nrunning several times of the style transfer process with the same reference\nstyle image. To address this critical issue, we take long-range dependencies of\ninput images into account for unbiased style transfer by proposing a\ntransformer-based approach, namely StyTr^2. In contrast with visual\ntransformers for other vision tasks, our StyTr^2 contains two different\ntransformer encoders to generate domain-specific sequences for content and\nstyle, respectively. Following the encoders, a multi-layer transformer decoder\nis adopted to stylize the content sequence according to the style sequence. In\naddition, we analyze the deficiency of existing positional encoding methods and\npropose the content-aware positional encoding (CAPE) which is scale-invariant\nand more suitable for image style transfer task. Qualitative and quantitative\nexperiments demonstrate the effectiveness of the proposed StyTr^2 compared to\nstate-of-the-art CNN-based and flow-based approaches.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 15:57:09 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 02:40:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Deng", "Yingying", ""], ["Tang", "Fan", ""], ["Pan", "Xingjia", ""], ["Dong", "Weiming", ""], ["Ma", "Chongyang", ""], ["Xu", "Changsheng", ""]]}, {"id": "2105.14584", "submitter": "Gunhee Nam", "authors": "Gunhee Nam, Miran Heo, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim", "title": "Polygonal Point Set Tracking", "comments": "14 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel learning-based polygonal point set tracking\nmethod. Compared to existing video object segmentation~(VOS) methods that\npropagate pixel-wise object mask information, we propagate a polygonal point\nset over frames.\n  Specifically, the set is defined as a subset of points in the target contour,\nand our goal is to track corresponding points on the target contour. Those\noutputs enable us to apply various visual effects such as motion tracking, part\ndeformation, and texture mapping. To this end, we propose a new method to track\nthe corresponding points between frames by the global-local alignment with\ndelicately designed losses and regularization terms. We also introduce a novel\nlearning strategy using synthetic and VOS datasets that makes it possible to\ntackle the problem without developing the point correspondence dataset. Since\nthe existing datasets are not suitable to validate our method, we build a new\npolygonal point set tracking dataset and demonstrate the superior performance\nof our method over the baselines and existing contour-based VOS methods. In\naddition, we present visual-effects applications of our method on part\ndistortion and text mapping.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:12:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Nam", "Gunhee", ""], ["Heo", "Miran", ""], ["Oh", "Seoung Wug", ""], ["Lee", "Joon-Young", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2105.14609", "submitter": "Sagie Benaim", "authors": "Noam Gat, Sagie Benaim, Lior Wolf", "title": "Identity and Attribute Preserving Thumbnail Upscaling", "comments": "ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the task of upscaling a low resolution thumbnail image of a\nperson, to a higher resolution image, which preserves the person's identity and\nother attributes. Since the thumbnail image is of low resolution, many higher\nresolution versions exist. Previous approaches produce solutions where the\nperson's identity is not preserved, or biased solutions, such as predominantly\nCaucasian faces. We address the existing ambiguity by first augmenting the\nfeature extractor to better capture facial identity, facial attributes (such as\nsmiling or not) and race, and second, use this feature extractor to generate\nhigh-resolution images which are identity preserving as well as conditioned on\nrace and facial attributes. Our results indicate an improvement in face\nsimilarity recognition and lookalike generation as well as in the ability to\ngenerate higher resolution images which preserve an input thumbnail identity\nand whose race and attributes are maintained.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 19:32:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gat", "Noam", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "2105.14620", "submitter": "Yicong He", "authors": "Yicong He, George K. Atia", "title": "Non-local Patch-based Low-rank Tensor Ring Completion for Visual Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor completion is the problem of estimating the missing entries of a\npartially observed tensor with a certain low-rank structure. It improves on\nmatrix completion for image and video data by capturing additional structural\ninformation intrinsic to such data. % With more inherent information involving\nin tensor structure than matrix, tensor completion has shown better performance\ncompared with matrix completion especially in image and video data. Traditional\ncompletion algorithms treat the entire visual data as a tensor, which may not\nalways work well especially when camera or object motion exists. In this paper,\nwe develop a novel non-local patch-based tensor ring completion algorithm. In\nthe proposed approach, similar patches are extracted for each reference patch\nalong both the spatial and temporal domains of the visual data. The collected\npatches are then formed into a high-order tensor and a tensor ring completion\nalgorithm is proposed to recover the completed tensor. A novel interval\nsampling-based block matching (ISBM) strategy and a hybrid completion strategy\nare also proposed to improve efficiency and accuracy. Further, we develop an\nonline patch-based completion algorithm to deal with streaming video data. An\nefficient online tensor ring completion algorithm is proposed to reduce the\ntime cost. Extensive experimental results demonstrate the superior performance\nof the proposed algorithms compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:33:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["He", "Yicong", ""], ["Atia", "George K.", ""]]}, {"id": "2105.14638", "submitter": "Johannes Otterbach", "authors": "Samuel von Bau{\\ss}nern, Johannes Otterbach, Adrian Loy, Mathieu\n  Salzmann, Thomas Wollmann", "title": "DAAIN: Detection of Anomalous and Adversarial Input using Normalizing\n  Flows", "comments": "14 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite much recent work, detecting out-of-distribution (OOD) inputs and\nadversarial attacks (AA) for computer vision models remains a challenge. In\nthis work, we introduce a novel technique, DAAIN, to detect OOD inputs and AA\nfor image segmentation in a unified setting. Our approach monitors the inner\nworkings of a neural network and learns a density estimator of the activation\ndistribution. We equip the density estimator with a classification head to\ndiscriminate between regular and anomalous inputs. To deal with the\nhigh-dimensional activation-space of typical segmentation networks, we\nsubsample them to obtain a homogeneous spatial and layer-wise coverage. The\nsubsampling pattern is chosen once per monitored model and kept fixed for all\ninputs. Since the attacker has access to neither the detection model nor the\nsampling key, it becomes harder for them to attack the segmentation network, as\nthe attack cannot be backpropagated through the detector. We demonstrate the\neffectiveness of our approach using an ESPNet trained on the Cityscapes dataset\nas segmentation model, an affine Normalizing Flow as density estimator and use\nblue noise to ensure homogeneous sampling. Our model can be trained on a single\nGPU making it compute efficient and deployable without requiring specialized\naccelerators.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:07:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["von Bau\u00dfnern", "Samuel", ""], ["Otterbach", "Johannes", ""], ["Loy", "Adrian", ""], ["Salzmann", "Mathieu", ""], ["Wollmann", "Thomas", ""]]}, {"id": "2105.14656", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Moezedin Javad Rafiee, Farnoosh Naderkhani, Shahin\n  Heidarian, Nastaran Enshaei, Anastasia Oikonomou, Faranak Babaki Fard, Reut\n  Anconina, Keyvan Farahani, Konstantinos N. Plataniotis, and Arash Mohammadi", "title": "Human-level COVID-19 Diagnosis from Low-dose CT Scans Using a Two-stage\n  Time-distributed Capsule Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reverse transcription-polymerase chain reaction (RT-PCR) is currently the\ngold standard in COVID-19 diagnosis. It can, however, take days to provide the\ndiagnosis, and false negative rate is relatively high. Imaging, in particular\nchest computed tomography (CT), can assist with diagnosis and assessment of\nthis disease. Nevertheless, it is shown that standard dose CT scan gives\nsignificant radiation burden to patients, especially those in need of multiple\nscans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT)\nscan protocols that reduce the radiation exposure close to that of a single\nX-Ray, while maintaining an acceptable resolution for diagnosis purposes. Since\nthoracic radiology expertise may not be widely available during the pandemic,\nwe develop an Artificial Intelligence (AI)-based framework using a collected\ndataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can\nprovide human-level performance. The AI model uses a two stage capsule network\narchitecture and can rapidly classify COVID-19, community acquired pneumonia\n(CAP), and normal cases, using LDCT/ULDCT scans. The AI model achieves COVID-19\nsensitivity of 89.5% +\\- 0.11, CAP sensitivity of 95% +\\- 0.11, normal cases\nsensitivity (specificity) of 85.7% +\\- 0.16, and accuracy of 90% +\\- 0.06. By\nincorporating clinical data (demographic and symptoms), the performance further\nimproves to COVID-19 sensitivity of 94.3% +\\- pm 0.05, CAP sensitivity of 96.7%\n+\\- 0.07, normal cases sensitivity (specificity) of 91% +\\- 0.09 , and accuracy\nof 94.1% +\\- 0.03. The proposed AI model achieves human-level diagnosis based\non the LDCT/ULDCT scans with reduced radiation exposure. We believe that the\nproposed AI model has the potential to assist the radiologists to accurately\nand promptly diagnose COVID-19 infection and help control the transmission\nchain during the pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 00:49:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Afshar", "Parnian", ""], ["Rafiee", "Moezedin Javad", ""], ["Naderkhani", "Farnoosh", ""], ["Heidarian", "Shahin", ""], ["Enshaei", "Nastaran", ""], ["Oikonomou", "Anastasia", ""], ["Fard", "Faranak Babaki", ""], ["Anconina", "Reut", ""], ["Farahani", "Keyvan", ""], ["Plataniotis", "Konstantinos N.", ""], ["Mohammadi", "Arash", ""]]}, {"id": "2105.14678", "submitter": "Xiaoguang Tu", "authors": "Xiaoguang Tu, Yingtian Zou, Jian Zhao, Wenjie Ai, Jian Dong, Yuan Yao,\n  Zhikang Wang, Guodong Guo, Zhifeng Li, Wei Liu, and Jiashi Feng", "title": "Image-to-Video Generation via 3D Facial Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a versatile model, FaceAnime, for various video generation tasks\nfrom still images. Video generation from a single face image is an interesting\nproblem and usually tackled by utilizing Generative Adversarial Networks (GANs)\nto integrate information from the input face image and a sequence of sparse\nfacial landmarks. However, the generated face images usually suffer from\nquality loss, image distortion, identity change, and expression mismatching due\nto the weak representation capacity of the facial landmarks. In this paper, we\npropose to \"imagine\" a face video from a single face image according to the\nreconstructed 3D face dynamics, aiming to generate a realistic and\nidentity-preserving face video, with precisely predicted pose and facial\nexpression. The 3D dynamics reveal changes of the facial expression and motion,\nand can serve as a strong prior knowledge for guiding highly realistic face\nvideo generation. In particular, we explore face video prediction and exploit a\nwell-designed 3D dynamic prediction network to predict a 3D dynamic sequence\nfor a single face image. The 3D dynamics are then further rendered by the\nsparse texture mapping algorithm to recover structural details and sparse\ntextures for generating face frames. Our model is versatile for various AR/VR\nand entertainment applications, such as face video retargeting and face video\nprediction. Superior experimental results have well demonstrated its\neffectiveness in generating high-fidelity, identity-preserving, and visually\npleasant face video clips from a single source face image.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:30:11 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tu", "Xiaoguang", ""], ["Zou", "Yingtian", ""], ["Zhao", "Jian", ""], ["Ai", "Wenjie", ""], ["Dong", "Jian", ""], ["Yao", "Yuan", ""], ["Wang", "Zhikang", ""], ["Guo", "Guodong", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""], ["Feng", "Jiashi", ""]]}, {"id": "2105.14683", "submitter": "Wentao Yu", "authors": "Yuhang He, Wentao Yu, Jie Han, Xing Wei, Xiaopeng Hong, Yihong Gong", "title": "Know Your Surroundings: Panoramic Multi-Object Tracking by Multimodality\n  Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the multi-object tracking (MOT) problem of\nautomatic driving and robot navigation. Most existing MOT methods track\nmultiple objects using a singular RGB camera, which are prone to camera\nfield-of-view and suffer tracking failures in complex scenarios due to\nbackground clutters and poor light conditions. To meet these challenges, we\npropose a MultiModality PAnoramic multi-object Tracking framework (MMPAT),\nwhich takes both 2D panorama images and 3D point clouds as input and then\ninfers target trajectories using the multimodality data. The proposed method\ncontains four major modules, a panorama image detection module, a multimodality\ndata fusion module, a data association module and a trajectory inference model.\nWe evaluate the proposed method on the JRDB dataset, where the MMPAT achieves\nthe top performance in both the detection and tracking tasks and significantly\noutperforms state-of-the-art methods by a large margin (15.7 and 8.5\nimprovement in terms of AP and MOTA, respectively).\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:16:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["He", "Yuhang", ""], ["Yu", "Wentao", ""], ["Han", "Jie", ""], ["Wei", "Xing", ""], ["Hong", "Xiaopeng", ""], ["Gong", "Yihong", ""]]}, {"id": "2105.14685", "submitter": "Peng Xu", "authors": "Peng Xu and Xiatian Zhu", "title": "Long-term Person Re-identification: A Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (Re-ID) works mostly consider a short-term\nsearch problem assuming unchanged clothes and personal appearance. However, in\nrealworld we often dress ourselves differently across locations, time, dates,\nseasons, weather, and events. As a result, the existing methods are unsuitable\nfor long-term person Re-ID with clothes change involved. Whilst there are\nseveral recent longterm Re-ID attempts, a large realistic dataset with clothes\nchange is lacking and indispensable for enabling extensive study as already\nexperienced in short-term Re-ID setting. In this work, we contribute timely a\nlarge, realistic long-term person re-identification benchmark. It consists of\n171K bounding boxes from 1.1K person identities, collected and constructed over\na course of 12 months. Unique characteristics of this dataset include: (1)\nNatural/native personal appearance (e.g., clothes and hair style) variations:\nThe degrees of clothes-change and dressing styles all are highly diverse, with\nthe reappearing gap in time ranging from minutes, hours, and days to weeks,\nmonths, seasons, and years. (2) Diverse walks of life: Persons across a wide\nrange of ages and professions appear in different weather conditions (e.g.,\nsunny, cloudy, windy, rainy, snowy, extremely cold) and events (e.g., working,\nleisure, daily activities). (3) Rich camera setups: The raw videos were\nrecorded by 17 outdoor security cameras with various resolutions operating in a\nreal-world surveillance system for a wide and dense block. (4) Largest scale:\nIt covers the largest number of (17) cameras, (1082) identities, and (171K)\nbounding boxes, as compared to alternative datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:35:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Peng", ""], ["Zhu", "Xiatian", ""]]}, {"id": "2105.14693", "submitter": "Junghoon Seo", "authors": "Chaehyeon Lee, Junghoon Seo, Heechul Jung", "title": "Training Domain-invariant Object Detector Faster with Feature Replay and\n  Slow Learner", "comments": "2021 CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning-based object detection on remote sensing domain, nuisance\nfactors, which affect observed variables while not affecting predictor\nvariables, often matters because they cause domain changes. Previously,\nnuisance disentangled feature transformation (NDFT) was proposed to build\ndomain-invariant feature extractor with with knowledge of nuisance factors.\nHowever, NDFT requires enormous time in a training phase, so it has been\nimpractical. In this paper, we introduce our proposed method, A-NDFT, which is\nan improvement to NDFT. A-NDFT utilizes two acceleration techniques, feature\nreplay and slow learner. Consequently, on a large-scale UAVDT benchmark, it is\nshown that our framework can reduce the training time of NDFT from 31 hours to\n3 hours while still maintaining the performance. The code will be made publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:09:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Chaehyeon", ""], ["Seo", "Junghoon", ""], ["Jung", "Heechul", ""]]}, {"id": "2105.14711", "submitter": "Ce Wang", "authors": "Yang Deng, Ce Wang, Yuan Hui, Qian Li, Jun Li, Shiwei Luo, Mengke Sun,\n  Quan Quan, Shuxin Yang, You Hao, Pengbo Liu, Honghu Xiao, Chunpeng Zhao,\n  Xinbao Wu, S. Kevin Zhou", "title": "CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in\n  Computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spine-related diseases have high morbidity and cause a huge burden of social\ncost. Spine imaging is an essential tool for noninvasively visualizing and\nassessing spinal pathology. Segmenting vertebrae in computed tomography (CT)\nimages is the basis of quantitative medical image analysis for clinical\ndiagnosis and surgery planning of spine diseases. Current publicly available\nannotated datasets on spinal vertebrae are small in size. Due to the lack of a\nlarge-scale annotated spine image dataset, the mainstream deep learning-based\nsegmentation methods, which are data-driven, are heavily restricted. In this\npaper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated\nfrom multiple sources for vertebra segmentation, which contains 1,005 CT\nvolumes with over 11,100 labeled vertebrae belonging to different spinal\nconditions. Based on this dataset, we conduct several spinal vertebrae\nsegmentation experiments to set the first benchmark. We believe that this\nlarge-scale dataset will facilitate further research in many spine-related\nimage analysis tasks, including but not limited to vertebrae segmentation,\nlabeling, 3D spine reconstruction from biplanar radiographs, image\nsuper-resolution, and enhancement.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 05:34:27 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 05:32:38 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 06:41:49 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Deng", "Yang", ""], ["Wang", "Ce", ""], ["Hui", "Yuan", ""], ["Li", "Qian", ""], ["Li", "Jun", ""], ["Luo", "Shiwei", ""], ["Sun", "Mengke", ""], ["Quan", "Quan", ""], ["Yang", "Shuxin", ""], ["Hao", "You", ""], ["Liu", "Pengbo", ""], ["Xiao", "Honghu", ""], ["Zhao", "Chunpeng", ""], ["Wu", "Xinbao", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2105.14713", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Yuchao Li, Yuxin Zhang, Bohong Chen, Fei Chao, Mengdi\n  Wang, Shen Li, Jun Yang, Rongrong Ji", "title": "1$\\times$N Block Pattern for Network Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though network sparsity emerges as a promising direction to overcome the\ndrastically increasing size of neural networks, it remains an open problem to\nconcurrently maintain model accuracy as well as achieve significant speedups on\ngeneral CPUs. In this paper, we propose one novel concept of $1\\times N$ block\nsparsity pattern (block pruning) to break this limitation. In particular,\nconsecutive $N$ output kernels with the same input channel index are grouped\ninto one block, which serves as a basic pruning granularity of our pruning\npattern. Our $1 \\times N$ sparsity pattern prunes these blocks considered\nunimportant. We also provide a workflow of filter rearrangement that first\nrearranges the weight matrix in the output channel dimension to derive more\ninfluential blocks for accuracy improvements, and then applies similar\nrearrangement to the next-layer weights in the input channel dimension to\nensure correct convolutional operations. Moreover, the output computation after\nour $1 \\times N$ block sparsity can be realized via a parallelized block-wise\nvectorized operation, leading to significant speedups on general CPUs-based\nplatforms. The efficacy of our pruning pattern is proved with experiments on\nILSVRC-2012. For example, in the case of 50% sparsity and $N=4$, our pattern\nobtains about 3.0% improvements over filter pruning in the top-1 accuracy of\nMobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU\nover weight pruning. Code is available at https://github.com/lmbxmu/1xN.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 05:50:33 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 11:59:07 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 08:09:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lin", "Mingbao", ""], ["Li", "Yuchao", ""], ["Zhang", "Yuxin", ""], ["Chen", "Bohong", ""], ["Chao", "Fei", ""], ["Wang", "Mengdi", ""], ["Li", "Shen", ""], ["Yang", "Jun", ""], ["Ji", "Rongrong", ""]]}, {"id": "2105.14727", "submitter": "Ziwen He", "authors": "Ziwen He, Wei Wang, Jing Dong, Tieniu Tan", "title": "Transferable Sparse Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown their vulnerability to adversarial attacks.\nIn this paper, we focus on sparse adversarial attack based on the $\\ell_0$ norm\nconstraint, which can succeed by only modifying a few pixels of an image.\nDespite a high attack success rate, prior sparse attack methods achieve a low\ntransferability under the black-box protocol due to overfitting the target\nmodel. Therefore, we introduce a generator architecture to alleviate the\noverfitting issue and thus efficiently craft transferable sparse adversarial\nexamples. Specifically, the generator decouples the sparse perturbation into\namplitude and position components. We carefully design a random quantization\noperator to optimize these two components jointly in an end-to-end way. The\nexperiment shows that our method has improved the transferability by a large\nmargin under a similar sparsity setting compared with state-of-the-art methods.\nMoreover, our method achieves superior inference speed, 700$\\times$ faster than\nother optimization-based methods. The code is available at\nhttps://github.com/shaguopohuaizhe/TSAA.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 06:44:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["He", "Ziwen", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Tan", "Tieniu", ""]]}, {"id": "2105.14732", "submitter": "Chenxin Li", "authors": "Chenxin Li, Wenao Ma, Liyan Sun, Xinghao Ding, Yue Huang, Guisheng\n  Wang, Yizhou Yu", "title": "Hierarchical Deep Network with Uncertainty-aware Semi-supervised\n  Learning for Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of organ vessels is essential for computer-aided diagnosis and\nsurgical planning. But it is not a easy task since the fine-detailed connected\nregions of organ vessel bring a lot of ambiguity in vessel segmentation and\nsub-type recognition, especially for the low-contrast capillary regions.\nFurthermore, recent two-staged approaches would accumulate and even amplify\nthese inaccuracies from the first-stage whole vessel segmentation into the\nsecond-stage sub-type vessel pixel-wise classification. Moreover, the scarcity\nof manual annotation in organ vessels poses another challenge. In this paper,\nto address the above issues, we propose a hierarchical deep network where an\nattention mechanism localizes the low-contrast capillary regions guided by the\nwhole vessels, and enhance the spatial activation in those areas for the\nsub-type vessels. In addition, we propose an uncertainty-aware semi-supervised\ntraining framework to alleviate the annotation-hungry limitation of deep\nmodels. The proposed method achieves the state-of-the-art performance in the\nbenchmarks of both retinal artery/vein segmentation in fundus images and liver\nportal/hepatic vessel segmentation in CT images.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 06:55:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Chenxin", ""], ["Ma", "Wenao", ""], ["Sun", "Liyan", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Wang", "Guisheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "2105.14734", "submitter": "Mingyuan Mao", "authors": "Mingyuan Mao, Renrui Zhang, Honghui Zheng, Peng Gao, Teli Ma, Yan\n  Peng, Errui Ding, Baochang Zhang, Shumin Han", "title": "Dual-stream Network for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 06:56:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:19:39 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 07:39:10 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Mao", "Mingyuan", ""], ["Zhang", "Renrui", ""], ["Zheng", "Honghui", ""], ["Gao", "Peng", ""], ["Ma", "Teli", ""], ["Peng", "Yan", ""], ["Ding", "Errui", ""], ["Zhang", "Baochang", ""], ["Han", "Shumin", ""]]}, {"id": "2105.14737", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Do-Hyeong Kim, Saehoon Yi, Taehoon Lee", "title": "Semi-orthogonal Embedding for Efficient Unsupervised Anomaly\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the efficiency of semi-orthogonal embedding for unsupervised\nanomaly segmentation. The multi-scale features from pre-trained CNNs are\nrecently used for the localized Mahalanobis distances with significant\nperformance. However, the increased feature size is problematic to scale up to\nthe bigger CNNs, since it requires the batch-inverse of multi-dimensional\ncovariance tensor. Here, we generalize an ad-hoc method, random feature\nselection, into semi-orthogonal embedding for robust approximation, cubically\nreducing the computational cost for the inverse of multi-dimensional covariance\ntensor. With the scrutiny of ablation studies, the proposed method achieves a\nnew state-of-the-art with significant margins for the MVTec AD, KolektorSDD,\nKolektorSDD2, and mSTC datasets. The theoretical and empirical analyses offer\ninsights and verification of our straightforward yet cost-effective approach.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:02:20 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Kim", "Do-Hyeong", ""], ["Yi", "Saehoon", ""], ["Lee", "Taehoon", ""]]}, {"id": "2105.14739", "submitter": "Jichao Zhang", "authors": "Jichao Zhang, Aliaksandr Siarohin, Hao Tang, Jingjing Chen, Enver\n  Sangineto, Wei Wang, Nicu Sebe", "title": "Controllable Person Image Synthesis with Spatially-Adaptive Warped\n  Normalization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controllable person image generation aims to produce realistic human images\nwith desirable attributes (e.g., the given pose, cloth textures or hair style).\nHowever, the large spatial misalignment between the source and target images\nmakes the standard architectures for image-to-image translation not suitable\nfor this task. Most of the state-of-the-art architectures avoid the alignment\nstep during the generation, which causes many artifacts, especially for person\nimages with complex textures. To solve this problem, we introduce a novel\nSpatially-Adaptive Warped Normalization (SAWN), which integrates a learned\nflow-field to warp modulation parameters. This allows us to align person\nspatial-adaptive styles with pose features efficiently. Moreover, we propose a\nnovel self-training part replacement strategy to refine the pretrained model\nfor the texture-transfer task, significantly improving the quality of the\ngenerated cloth and the preservation ability of irrelevant regions. Our\nexperimental results on the widely used DeepFashion dataset demonstrate a\nsignificant improvement of the proposed method over the state-of-the-art\nmethods on both pose-transfer and texture-transfer tasks. The source code is\navailable at https://github.com/zhangqianhui/Sawn.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:07:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:24:00 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Jichao", ""], ["Siarohin", "Aliaksandr", ""], ["Tang", "Hao", ""], ["Chen", "Jingjing", ""], ["Sangineto", "Enver", ""], ["Wang", "Wei", ""], ["Sebe", "Nicu", ""]]}, {"id": "2105.14740", "submitter": "Ioan Marius Bilasco PhD", "authors": "El-Assal Mireille and Tirilly Pierre and Bilasco Ioan Marius", "title": "A Study On the Effects of Pre-processing On Spatio-temporal Action\n  Recognition Using Spiking Neural Networks Trained with STDP", "comments": null, "journal-ref": null, "doi": "10.1109/CBMI50038.2021.9461922", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing interest in spiking neural networks in recent\nyears. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in\npattern recognition, such as energy efficiency. But current methods such as\nANN-to-SNN conversion and back-propagation do not take full advantage of these\nnetworks, and unsupervised methods have not yet reached a success comparable to\nadvanced artificial neural networks. It is important to study the behavior of\nSNNs trained with unsupervised learning methods such as spike-timing dependent\nplasticity (STDP) on video classification tasks, including mechanisms to model\nmotion information using spikes, as this information is critical for video\nunderstanding. This paper presents multiple methods of transposing temporal\ninformation into a static format, and then transforming the visual information\ninto spikes using latency coding. These methods are paired with two types of\ntemporal fusion known as early and late fusion, and are used to help the\nspiking neural network in capturing the spatio-temporal features from videos.\nIn this paper, we rely on the network architecture of a convolutional spiking\nneural network trained with STDP, and we test the performance of this network\nwhen challenged with action recognition tasks. Understanding how a spiking\nneural network responds to different methods of movement extraction and\nrepresentation can help reduce the performance gap between SNNs and ANNs. In\nthis paper we show the effect of the similarity in the shape and speed of\ncertain actions on action recognition with spiking neural networks, we also\nhighlight the effectiveness of some methods compared to others.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:07:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mireille", "El-Assal", ""], ["Pierre", "Tirilly", ""], ["Marius", "Bilasco Ioan", ""]]}, {"id": "2105.14753", "submitter": "Am\\'elie Gruel", "authors": "Am\\'elie Gruel and Jean Martinet", "title": "Bio-inspired visual attention for silicon retinas based on spiking\n  neural networks applied to pattern classification", "comments": "6 pages, 3 figures. To be published in Content-Based Multimedia\n  Indexing (CBMI) 2021, Lille, France. This work was supported by the European\n  Union's ERA-NET CHIST-ERA 2018 research and innovation programme under grant\n  agreement ANR-19-CHR3-0008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual attention can be defined as the behavioral and cognitive process of\nselectively focusing on a discrete aspect of sensory cues while disregarding\nother perceivable information. This biological mechanism, more specifically\nsaliency detection, has long been used in multimedia indexing to drive the\nanalysis only on relevant parts of images or videos for further processing.\n  The recent advent of silicon retinas (or event cameras -- sensors that\nmeasure pixel-wise changes in brightness and output asynchronous events\naccordingly) raises the question of how to adapt attention and saliency to the\nunconventional type of such sensors' output. Silicon retina aims to reproduce\nthe biological retina behaviour. In that respect, they produce punctual events\nin time that can be construed as neural spikes and interpreted as such by a\nneural network.\n  In particular, Spiking Neural Networks (SNNs) represent an asynchronous type\nof artificial neural network closer to biology than traditional artificial\nnetworks, mainly because they seek to mimic the dynamics of neural membrane and\naction potentials over time. SNNs receive and process information in the form\nof spike trains. Therefore, they make for a suitable candidate for the\nefficient processing and classification of incoming event patterns measured by\nsilicon retinas. In this paper, we review the biological background behind the\nattentional mechanism, and introduce a case study of event videos\nclassification with SNNs, using a biology-grounded low-level computational\nattention mechanism, with interesting preliminary results.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:34:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gruel", "Am\u00e9lie", ""], ["Martinet", "Jean", ""]]}, {"id": "2105.14756", "submitter": "AprilPyone MaungMaung", "authors": "AprilPyone MaungMaung and Hitoshi Kiya", "title": "A Protection Method of Trained CNN Model with Secret Key from\n  Unauthorized Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for protecting convolutional neural\nnetwork (CNN) models with a secret key set so that unauthorized users without\nthe correct key set cannot access trained models. The method enables us to\nprotect not only from copyright infringement but also the functionality of a\nmodel from unauthorized access without any noticeable overhead. We introduce\nthree block-wise transformations with a secret key set to generate learnable\ntransformed images: pixel shuffling, negative/positive transformation, and FFX\nencryption. Protected models are trained by using transformed images. The\nresults of experiments with the CIFAR and ImageNet datasets show that the\nperformance of a protected model was close to that of non-protected models when\nthe key set was correct, while the accuracy severely dropped when an incorrect\nkey set was given. The protected model was also demonstrated to be robust\nagainst various attacks. Compared with the state-of-the-art model protection\nwith passports, the proposed method does not have any additional layers in the\nnetwork, and therefore, there is no overhead during training and inference\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:37:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["MaungMaung", "AprilPyone", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2105.14758", "submitter": "Lu Xu", "authors": "Lu Xu, Yuwei Zhang, Ying Liu, Daoye Wang, Mu Zhou, Jimmy Ren, Jingwei\n  Wei, Zhaoxiang Ye", "title": "Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction\n  Network", "comments": "ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low-dose CT has been a key diagnostic imaging modality to reduce the\npotential risk of radiation overdose to patient health. Despite recent\nadvances, CNN-based approaches typically apply filters in a spatially invariant\nway and adopt similar pixel-level losses, which treat all regions of the CT\nimage equally and can be inefficient when fine-grained structures coexist with\nnon-uniformly distributed noises. To address this issue, we propose a\nStructure-preserving Kernel Prediction Network (StructKPN) that combines the\nkernel prediction network with a structure-aware loss function that utilizes\nthe pixel gradient statistics and guides the model towards spatially-variant\nfilters that enhance noise removal, prevent over-smoothing and preserve\ndetailed structures for different regions in CT imaging. Extensive experiments\ndemonstrated that our approach achieved superior performance on both synthetic\nand non-synthetic datasets, and better preserves structures that are highly\ndesired in clinical screening and low-dose protocol optimization.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:42:21 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 01:39:23 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 01:12:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Lu", ""], ["Zhang", "Yuwei", ""], ["Liu", "Ying", ""], ["Wang", "Daoye", ""], ["Zhou", "Mu", ""], ["Ren", "Jimmy", ""], ["Wei", "Jingwei", ""], ["Ye", "Zhaoxiang", ""]]}, {"id": "2105.14766", "submitter": "Junjun Jiang", "authors": "Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma", "title": "BaMBNet: A Blur-aware Multi-branch Network for Defocus Deblurring", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The defocus deblurring raised from the finite aperture size and exposure time\nis an essential problem in the computational photography. It is very\nchallenging because the blur kernel is spatially varying and difficult to\nestimate by traditional methods. Due to its great breakthrough in low-level\ntasks, convolutional neural networks (CNNs) have been introduced to the defocus\ndeblurring problem and achieved significant progress. However, they apply the\nsame kernel for different regions of the defocus blurred images, thus it is\ndifficult to handle these nonuniform blurred images. To this end, this study\ndesigns a novel blur-aware multi-branch network (BaMBNet), in which different\nregions (with different blur amounts) should be treated differentially. In\nparticular, we estimate the blur amounts of different regions by the internal\ngeometric constraint of the DP data, which measures the defocus disparity\nbetween the left and right views. Based on the assumption that different image\nregions with different blur amounts have different deblurring difficulties, we\nleverage different networks with different capacities (\\emph{i.e.} parameters)\nto process different image regions. Moreover, we introduce a meta-learning\ndefocus mask generation algorithm to assign each pixel to a proper branch. In\nthis way, we can expect to well maintain the information of the clear regions\nwhile recovering the missing details of the blurred regions. Both quantitative\nand qualitative experiments demonstrate that our BaMBNet outperforms the\nstate-of-the-art methods. Source code will be available at\nhttps://github.com/junjun-jiang/BaMBNet.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:55:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liang", "Pengwei", ""], ["Jiang", "Junjun", ""], ["Liu", "Xianming", ""], ["Ma", "Jiayi", ""]]}, {"id": "2105.14773", "submitter": "Yan Wang", "authors": "Yan Wang, Peng Tang, Yuyin Zhou, Wei Shen, Elliot K. Fishman, and Alan\n  L. Yuille", "title": "Learning Inductive Attention Guidance for Partially Supervised\n  Pancreatic Ductal Adenocarcinoma Prediction", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2021.3060066", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic ductal adenocarcinoma (PDAC) is the third most common cause of\ncancer death in the United States. Predicting tumors like PDACs (including both\nclassification and segmentation) from medical images by deep learning is\nbecoming a growing trend, but usually a large number of annotated data are\nrequired for training, which is very labor-intensive and time-consuming. In\nthis paper, we consider a partially supervised setting, where cheap image-level\nannotations are provided for all the training data, and the costly per-voxel\nannotations are only available for a subset of them. We propose an Inductive\nAttention Guidance Network (IAG-Net) to jointly learn a global image-level\nclassifier for normal/PDAC classification and a local voxel-level classifier\nfor semi-supervised PDAC segmentation. We instantiate both the global and the\nlocal classifiers by multiple instance learning (MIL), where the attention\nguidance, indicating roughly where the PDAC regions are, is the key to bridging\nthem: For global MIL based normal/PDAC classification, attention serves as a\nweight for each instance (voxel) during MIL pooling, which eliminates the\ndistraction from the background; For local MIL based semi-supervised PDAC\nsegmentation, the attention guidance is inductive, which not only provides\nbag-level pseudo-labels to training data without per-voxel annotations for MIL\ntraining, but also acts as a proxy of an instance-level classifier.\nExperimental results show that our IAG-Net boosts PDAC segmentation accuracy by\nmore than 5% compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:16:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Yan", ""], ["Tang", "Peng", ""], ["Zhou", "Yuyin", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2105.14784", "submitter": "Shen Cai", "authors": "Siyu Zhang, Hui Cao, Yuqi Liu, Shen Cai, Yanting Zhang, Yuanzhan Li,\n  Xiaoyu Chi", "title": "SN-Graph: a Minimalist 3D Object Representation for Classification", "comments": "ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep learning techniques to process 3D objects has achieved many\nsuccesses. However, few methods focus on the representation of 3D objects,\nwhich could be more effective for specific tasks than traditional\nrepresentations, such as point clouds, voxels, and multi-view images. In this\npaper, we propose a Sphere Node Graph (SN-Graph) to represent 3D objects.\nSpecifically, we extract a certain number of internal spheres (as nodes) from\nthe signed distance field (SDF), and then establish connections (as edges)\namong the sphere nodes to construct a graph, which is seamlessly suitable for\n3D analysis using graph neural network (GNN). Experiments conducted on the\nModelNet40 dataset show that when there are fewer nodes in the graph or the\ntested objects are rotated arbitrarily, the classification accuracy of SN-Graph\nis significantly higher than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:24:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Siyu", ""], ["Cao", "Hui", ""], ["Liu", "Yuqi", ""], ["Cai", "Shen", ""], ["Zhang", "Yanting", ""], ["Li", "Yuanzhan", ""], ["Chi", "Xiaoyu", ""]]}, {"id": "2105.14785", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen,\n  Jun Zhu, Tie-Yan Liu", "title": "Adversarial Training with Rectified Rejection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is one of the most effective strategies for\npromoting model robustness, whereas even the state-of-the-art adversarially\ntrained models struggle to exceed 60% robust test accuracy on CIFAR-10 without\nadditional data, which is far from practical. A natural way to break this\naccuracy bottleneck is to introduce a rejection option, where confidence is a\ncommonly used certainty proxy. However, the vanilla confidence can overestimate\nthe model certainty if the input is wrongly classified. To this end, we propose\nto use true confidence (T-Con) (i.e., predicted probability of the true class)\nas a certainty oracle, and learn to predict T-Con by rectifying confidence. We\nprove that under mild conditions, a rectified confidence (R-Con) rejector and a\nconfidence rejector can be coupled to distinguish any wrongly classified input\nfrom correctly classified ones, even under adaptive attacks. We also quantify\nthat training R-Con to be aligned with T-Con could be an easier task than\nlearning robust classifiers. In our experiments, we evaluate our rectified\nrejection (RR) module on CIFAR-10, CIFAR-10-C, and CIFAR-100 under several\nattacks, and demonstrate that the RR module is well compatible with different\nAT frameworks on improving robustness, with little extra computation.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:24:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pang", "Tianyu", ""], ["Zhang", "Huishuai", ""], ["He", "Di", ""], ["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Chen", "Wei", ""], ["Zhu", "Jun", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.14790", "submitter": "Simindokht Jahangard", "authors": "Mahdi Bonyani, Mina Rahmanian, Simindokht Jahangard", "title": "Predicting Driver Intention Using Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve driving safety and avoid car accidents, Advanced Driver Assistance\nSystems (ADAS) are given significant attention. Recent studies have focused on\npredicting driver intention as a key part of these systems. In this study, we\nproposed new framework in which 4 inputs are employed to anticipate diver\nmaneuver using Brain4Cars dataset and the maneuver prediction is achieved from\n5, 4, 3, 2, 1 seconds before the actual action occurs. We evaluated our\nframework in three scenarios: using only 1) inside view 2) outside view and 3)\nboth inside and outside view. We divided the dataset into training, validation\nand test sets, also K-fold cross validation is utilized. Compared with\nstate-of-the-art studies, our architecture is faster and achieved higher\nperformance in second and third scenario. Accuracy, precision, recall and\nf1-score as evaluation metrics were utilized and the result of 82.41%, 82.28%,\n82,42% and 82.24% for outside view and 98.90%, 98.96%, 98.90% and 98.88% for\nboth inside and outside view were gained, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:34:57 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 12:48:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bonyani", "Mahdi", ""], ["Rahmanian", "Mina", ""], ["Jahangard", "Simindokht", ""]]}, {"id": "2105.14797", "submitter": "Edouard Yvinec", "authors": "Edouard Yvinec, Arnaud Dapogny, Matthieu Cord and Kevin Bailly", "title": "RED : Looking for Redundancies for Data-Free Structured Compression of\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are ubiquitous in today's computer vision\nland-scape, despite involving considerable computational costs. The mainstream\napproaches for runtime acceleration consist in pruning connections\n(unstructured pruning) or, better, filters (structured pruning), both often\nrequiring data to re-train the model. In this paper, we present RED, a\ndata-free structured, unified approach to tackle structured pruning. First, we\npropose a novel adaptive hashing of the scalar DNN weight distribution\ndensities to increase the number of identical neurons represented by their\nweight vectors. Second, we prune the network by merging redundant neurons based\non their relative similarities, as defined by their distance. Third, we propose\na novel uneven depthwise separation technique to further prune convolutional\nlayers. We demonstrate through a large variety of benchmarks that RED largely\noutperforms other data-free pruning methods, often reaching performance similar\nto unconstrained, data-driven methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:44:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yvinec", "Edouard", ""], ["Dapogny", "Arnaud", ""], ["Cord", "Matthieu", ""], ["Bailly", "Kevin", ""]]}, {"id": "2105.14804", "submitter": "Jingbo Wang", "authors": "Jingbo Wang, Sijie Yan, Bo Dai, Dahua LIn", "title": "Scene-aware Generative Network for Human Motion Synthesis", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit human motion synthesis, a task useful in various real world\napplications, in this paper. Whereas a number of methods have been developed\npreviously for this task, they are often limited in two aspects: focusing on\nthe poses while leaving the location movement behind, and ignoring the impact\nof the environment on the human motion. In this paper, we propose a new\nframework, with the interaction between the scene and the human motion taken\ninto account. Considering the uncertainty of human motion, we formulate this\ntask as a generative task, whose objective is to generate plausible human\nmotion conditioned on both the scene and the human initial position. This\nframework factorizes the distribution of human motions into a distribution of\nmovement trajectories conditioned on scenes and that of body pose dynamics\nconditioned on both scenes and trajectories. We further derive a GAN based\nlearning approach, with discriminators to enforce the compatibility between the\nhuman motion and the contextual scene as well as the 3D to 2D projection\nconstraints. We assess the effectiveness of the proposed method on two\nchallenging datasets, which cover both synthetic and real world environments.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:05:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Jingbo", ""], ["Yan", "Sijie", ""], ["Dai", "Bo", ""], ["LIn", "Dahua", ""]]}, {"id": "2105.14824", "submitter": "Thomas Baumhauer", "authors": "Thomas Baumhauer and Djordje Slijepcevic and Matthias Zeppelzauer", "title": "Bounded logit attention: Learning to explain image classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable artificial intelligence is the attempt to elucidate the workings\nof systems too complex to be directly accessible to human cognition through\nsuitable side-information referred to as \"explanations\". We present a trainable\nexplanation module for convolutional image classifiers we call bounded logit\nattention (BLA). The BLA module learns to select a subset of the convolutional\nfeature map for each input instance, which then serves as an explanation for\nthe classifier's prediction. BLA overcomes several limitations of the\ninstancewise feature selection method \"learning to explain\" (L2X) introduced by\nChen et al. (2018): 1) BLA scales to real-world sized image classification\nproblems, and 2) BLA offers a canonical way to learn explanations of variable\nsize. Due to its modularity BLA lends itself to transfer learning setups and\ncan also be employed as a post-hoc add-on to trained classifiers. Beyond\nexplainability, BLA may serve as a general purpose method for differentiable\napproximation of subset selection. In a user study we find that BLA\nexplanations are preferred over explanations generated by the popular\n(Grad-)CAM method.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:36:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Baumhauer", "Thomas", ""], ["Slijepcevic", "Djordje", ""], ["Zeppelzauer", "Matthias", ""]]}, {"id": "2105.14829", "submitter": "Stephen James", "authors": "Stephen James and Andrew J. Davison", "title": "Q-attention: Enabling Efficient Learning for Vision-based Robotic\n  Manipulation", "comments": "Videos and code found at: https://sites.google.com/view/q-attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of reinforcement learning methods, they have yet to have\ntheir breakthrough moment when applied to a broad range of robotic manipulation\ntasks. This is partly due to the fact that reinforcement learning algorithms\nare notoriously difficult and time consuming to train, which is exacerbated\nwhen training from images rather than full-state inputs. As humans perform\nmanipulation tasks, our eyes closely monitor every step of the process with our\ngaze focusing sequentially on the objects being manipulated. With this in mind,\nwe present our Attention-driven Robotic Manipulation (ARM) algorithm, which is\na general manipulation algorithm that can be applied to a range of\nsparse-rewarded tasks, given only a small number of demonstrations. ARM splits\nthe complex task of manipulation into a 3 stage pipeline: (1) a Q-attention\nagent extracts interesting pixel locations from RGB and point cloud inputs, (2)\na next-best pose agent that accepts crops from the Q-attention agent and\noutputs poses, and (3) a control agent that takes the goal pose and outputs\njoint actions. We show that current learning algorithms fail on a range of\nRLBench tasks, whilst ARM is successful.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:44:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["James", "Stephen", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2105.14844", "submitter": "Christian Rathgeb", "authors": "Christian Rathgeb and Pawel Drozdowski and Naser Damer and Dinusha C.\n  Frings and Christoph Busch", "title": "Demographic Fairness in Biometric Systems: What do the Experts say?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic decision systems have frequently been labelled as \"biased\",\n\"racist\", \"sexist\", or \"unfair\" by numerous media outlets, organisations, and\nresearchers. There is an ongoing debate about whether such assessments are\njustified and whether citizens and policymakers should be concerned. These and\nother related matters have recently become a hot topic in the context of\nbiometric technologies, which are ubiquitous in personal, commercial, and\ngovernmental applications. Biometrics represent an essential component of many\nsurveillance, access control, and operational identity management systems, thus\ndirectly or indirectly affecting billions of people all around the world.\n  Recently, the European Association for Biometrics organised an event series\nwith \"demographic fairness in biometric systems\" as an overarching theme. The\nevents featured presentations by international experts from academic, industry,\nand governmental organisations and facilitated interactions and discussions\nbetween the experts and the audience. Further consultation of experts was\nundertaken by means of a questionnaire. This work summarises opinions of\nexperts and findings of said events on the topic of demographic fairness in\nbiometric systems including several important aspects such as the developments\nof evaluation metrics and standards as well as related issues, e.g. the need\nfor transparency and explainability in biometric systems or legal and ethical\nissues.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:58:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rathgeb", "Christian", ""], ["Drozdowski", "Pawel", ""], ["Damer", "Naser", ""], ["Frings", "Dinusha C.", ""], ["Busch", "Christoph", ""]]}, {"id": "2105.14848", "submitter": "Huy Trinh Quoc", "authors": "Quoc-Huy Trinh, Minh-Van Nguyen, Thiet-Gia Huynh, Minh-Triet Tran", "title": "Refined Deep Neural Network and U-Net for Polyps Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Medico: Multimedia Task 2020 focuses on developing an efficient and\naccurate computer-aided diagnosis system for automatic segmentation [3]. We\nparticipate in task 1, Polyps segmentation task, which is to develop algorithms\nfor segmenting polyps on a comprehensive dataset. In this task, we propose\nmethods combining Residual module, Inception module, Adaptive Convolutional\nneural network with U-Net model, and PraNet for semantic segmentation of\nvarious types of polyps in endoscopic images. We select 5 runs with different\narchitecture and parameters in our methods. Our methods show potential results\nin accuracy and efficiency through multiple experiments, and our team is in the\nTop 3 best results with a Jaccard index of 0.765.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:02:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Trinh", "Quoc-Huy", ""], ["Nguyen", "Minh-Van", ""], ["Huynh", "Thiet-Gia", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "2105.14857", "submitter": "Harim Jung", "authors": "Harim Jung, Myeong-Seok Oh, Seong-Whan Lee", "title": "Learning Free-Form Deformation for 3D Face Reconstruction from\n  In-The-Wild Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The 3D Morphable Model (3DMM), which is a Principal Component Analysis (PCA)\nbased statistical model that represents a 3D face using linear basis functions,\nhas shown promising results for reconstructing 3D faces from single-view\nin-the-wild images. However, 3DMM has restricted representation power due to\nthe limited number of 3D scans and the global linear basis. To address the\nlimitations of 3DMM, we propose a straightforward learning-based method that\nreconstructs a 3D face mesh through Free-Form Deformation (FFD) for the first\ntime. FFD is a geometric modeling method that embeds a reference mesh within a\nparallelepiped grid and deforms the mesh by moving the sparse control points of\nthe grid. As FFD is based on mathematically defined basis functions, it has no\nlimitation in representation power. Thus, we can recover accurate 3D face\nmeshes by estimating appropriate deviation of control points as deformation\nparameters. Although both 3DMM and FFD are parametric models, it is difficult\nto predict the effect of the 3DMM parameters on the face shape, while the\ndeformation parameters of FFD are interpretable in terms of their effect on the\nfinal shape of the mesh. This practical advantage of FFD allows the resulting\nmesh and control points to serve as a good starting point for 3D face modeling,\nin that ordinary users can fine-tune the mesh by using widely available 3D\nsoftware tools. Experiments on multiple datasets demonstrate how our method\nsuccessfully estimates the 3D face geometry and facial expressions from 2D face\nimages, achieving comparable performance to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:19:20 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jung", "Harim", ""], ["Oh", "Myeong-Seok", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2105.14859", "submitter": "Samarth Sinha", "authors": "Samarth Sinha, Adji B. Dieng", "title": "Consistency Regularization for Variational Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Variational auto-encoders (VAEs) are a powerful approach to unsupervised\nlearning. They enable scalable approximate posterior inference in\nlatent-variable models using variational inference (VI). A VAE posits a\nvariational family parameterized by a deep neural network called an encoder\nthat takes data as input. This encoder is shared across all the observations,\nwhich amortizes the cost of inference. However the encoder of a VAE has the\nundesirable property that it maps a given observation and a\nsemantics-preserving transformation of it to different latent representations.\nThis \"inconsistency\" of the encoder lowers the quality of the learned\nrepresentations, especially for downstream tasks, and also negatively affects\ngeneralization. In this paper, we propose a regularization method to enforce\nconsistency in VAEs. The idea is to minimize the Kullback-Leibler (KL)\ndivergence between the variational distribution when conditioning on the\nobservation and the variational distribution when conditioning on a random\nsemantic-preserving transformation of this observation. This regularization is\napplicable to any VAE. In our experiments we apply it to four different VAE\nvariants on several benchmark datasets and found it always improves the quality\nof the learned representations but also leads to better generalization. In\nparticular, when applied to the Nouveau Variational Auto-Encoder (NVAE), our\nregularization method yields state-of-the-art performance on MNIST and\nCIFAR-10. We also applied our method to 3D data and found it learns\nrepresentations of superior quality as measured by accuracy on a downstream\nclassification task.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:26:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sinha", "Samarth", ""], ["Dieng", "Adji B.", ""]]}, {"id": "2105.14891", "submitter": "Kyungseo Min", "authors": "Kyungseo Min, Gun-Hee Lee, Seong-Whan Lee", "title": "ACNet: Mask-Aware Attention with Dynamic Context Enhancement for Robust\n  Acne Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided diagnosis has recently received attention for its advantage of\nlow cost and time efficiency. Although deep learning played a major role in the\nrecent success of acne detection, there are still several challenges such as\ncolor shift by inconsistent illumination, variation in scales, and high density\ndistribution. To address these problems, we propose an acne detection network\nwhich consists of three components, specifically: Composite Feature Refinement,\nDynamic Context Enhancement, and Mask-Aware Multi-Attention. First, Composite\nFeature Refinement integrates semantic information and fine details to enrich\nfeature representation, which mitigates the adverse impact of imbalanced\nillumination. Then, Dynamic Context Enhancement controls different receptive\nfields of multi-scale features for context enhancement to handle scale\nvariation. Finally, Mask-Aware Multi-Attention detects densely arranged and\nsmall acne by suppressing uninformative regions and highlighting probable acne\nregions. Experiments are performed on acne image dataset ACNE04 and natural\nimage dataset PASCAL VOC 2007. We demonstrate how our method achieves the\nstate-of-the-art result on ACNE04 and competitive performance with previous\nstate-of-the-art methods on the PASCAL VOC 2007.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:31:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Min", "Kyungseo", ""], ["Lee", "Gun-Hee", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2105.14897", "submitter": "Shuai Bai", "authors": "Shuai Bai, Zhedong Zheng, Xiaohan Wang, Junyang Lin, Zhu Zhang, Chang\n  Zhou, Yi Yang, Hongxia Yang", "title": "Connecting Language and Vision for Natural Language-Based Vehicle\n  Retrieval", "comments": "CVPR 2021 AI CITY CHALLENGE Natural Language-Based Vehicle Retrieval\n  Top 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle search is one basic task for the efficient traffic management in\nterms of the AI City. Most existing practices focus on the image-based vehicle\nmatching, including vehicle re-identification and vehicle tracking. In this\npaper, we apply one new modality, i.e., the language description, to search the\nvehicle of interest and explore the potential of this task in the real-world\nscenario. The natural language-based vehicle search poses one new challenge of\nfine-grained understanding of both vision and language modalities. To connect\nlanguage and vision, we propose to jointly train the state-of-the-art vision\nmodels with the transformer-based language model in an end-to-end manner.\nExcept for the network structure design and the training strategy, several\noptimization objectives are also re-visited in this work. The qualitative and\nquantitative experiments verify the effectiveness of the proposed method. Our\nproposed method has achieved the 1st place on the 5th AI City Challenge,\nyielding competitive performance 18.69% MRR accuracy on the private test set.\nWe hope this work can pave the way for the future study on using language\ndescription effectively and efficiently for real-world vehicle retrieval\nsystems. The code will be available at\nhttps://github.com/ShuaiBai623/AIC2021-T5-CLV.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:42:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bai", "Shuai", ""], ["Zheng", "Zhedong", ""], ["Wang", "Xiaohan", ""], ["Lin", "Junyang", ""], ["Zhang", "Zhu", ""], ["Zhou", "Chang", ""], ["Yang", "Yi", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.14931", "submitter": "Jian Chen", "authors": "Meng Ling and Jian Chen and Torsten M\\\"oller and Petra Isenberg and\n  Tobias Isenberg and Michael Sedlmair and Robert S. Laramee and Han-Wei Shen\n  and Jian Wu and C. Lee Giles", "title": "Document Domain Randomization for Deep Learning Document Layout\n  Extraction", "comments": "Main paper to appear in ICDAR 2021 (16th International Conference on\n  Document Analysis and Recognition). This version contains additional\n  materials. The associated test data is hosted on IEEE Data Port:\n  http://doi.org/10.21227/326q-bf39", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present document domain randomization (DDR), the first successful transfer\nof convolutional neural networks (CNNs) trained only on graphically rendered\npseudo-paper pages to real-world document segmentation. DDR renders\npseudo-document pages by modeling randomized textual and non-textual contents\nof interest, with user-defined layout and font styles to support joint learning\nof fine-grained classes. We demonstrate competitive results using our DDR\napproach to extract nine document classes from the benchmark CS-150 and papers\npublished in two domains, namely annual meetings of Association for\nComputational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to\nconditions of style mismatch, fewer or more noisy samples that are more easily\nobtained in the real world. We show that high-fidelity semantic information is\nnot necessary to label semantic classes but style mismatch between train and\ntest can lower model accuracy. Using smaller training samples had a slightly\ndetrimental effect. Finally, network models still achieved high test accuracy\nwhen correct labels are diluted towards confusing labels; this behavior hold\nacross several classes.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:16:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ling", "Meng", ""], ["Chen", "Jian", ""], ["M\u00f6ller", "Torsten", ""], ["Isenberg", "Petra", ""], ["Isenberg", "Tobias", ""], ["Sedlmair", "Michael", ""], ["Laramee", "Robert S.", ""], ["Shen", "Han-Wei", ""], ["Wu", "Jian", ""], ["Giles", "C. Lee", ""]]}, {"id": "2105.14944", "submitter": "Anh Nguyen", "authors": "Giang Nguyen, Daeyoung Kim, Anh Nguyen", "title": "The effectiveness of feature attribution methods and its correlation\n  with automatic evaluation scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics. In this paper, we conduct the first, large-scale\nuser study on 320 lay and 11 expert users to shed light on the effectiveness of\nstate-of-the-art attribution methods in assisting humans in ImageNet\nclassification, Stanford Dogs fine-grained classification, and these two tasks\nbut when the input image contains adversarial perturbations. We found that, in\noverall, feature attribution is surprisingly not more effective than showing\nhumans nearest training-set examples. On a hard task of fine-grained dog\ncategorization, presenting attribution maps to humans does not help, but\ninstead hurts the performance of human-AI teams compared to AI alone.\nImportantly, we found automatic attribution-map evaluation measures to\ncorrelate poorly with the actual human-AI team performance. Our findings\nencourage the community to rigorously test their methods on the downstream\nhuman-in-the-loop applications and to rethink the existing evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:23:50 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 22:00:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Nguyen", "Giang", ""], ["Kim", "Daeyoung", ""], ["Nguyen", "Anh", ""]]}, {"id": "2105.14951", "submitter": "Bahjat Kawar", "authors": "Bahjat Kawar, Gregory Vaksman, Michael Elad", "title": "SNIPS: Solving Noisy Inverse Problems Stochastically", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a novel stochastic algorithm dubbed SNIPS, which\ndraws samples from the posterior distribution of any linear inverse problem,\nwhere the observation is assumed to be contaminated by additive white Gaussian\nnoise. Our solution incorporates ideas from Langevin dynamics and Newton's\nmethod, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian\ndenoiser. The proposed approach relies on an intricate derivation of the\nposterior score function that includes a singular value decomposition (SVD) of\nthe degradation operator, in order to obtain a tractable iterative algorithm\nfor the desired sampling. Due to its stochasticity, the algorithm can produce\nmultiple high perceptual quality samples for the same noisy observation. We\ndemonstrate the abilities of the proposed paradigm for image deblurring,\nsuper-resolution, and compressive sensing. We show that the samples produced\nare sharp, detailed and consistent with the given measurements, and their\ndiversity exposes the inherent uncertainty in the inverse problem being solved.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:33:21 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kawar", "Bahjat", ""], ["Vaksman", "Gregory", ""], ["Elad", "Michael", ""]]}, {"id": "2105.14954", "submitter": "Yuan Gan", "authors": "Yuan Gan, Yawei Luo, Xin Yu, Bang Zhang, Yi Yang", "title": "VidFace: A Full-Transformer Solver for Video FaceHallucination with\n  Unaligned Tiny Snapshots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the task of hallucinating an authentic\nhigh-resolution (HR) human face from multiple low-resolution (LR) video\nsnapshots. We propose a pure transformer-based model, dubbed VidFace, to fully\nexploit the full-range spatio-temporal information and facial structure cues\namong multiple thumbnails. Specifically, VidFace handles multiple snapshots all\nat once and harnesses the spatial and temporal information integrally to\nexplore face alignments across all the frames, thus avoiding accumulating\nalignment errors. Moreover, we design a recurrent position embedding module to\nequip our transformer with facial priors, which not only effectively\nregularises the alignment mechanism but also supplants notorious pre-training.\nFinally, we curate a new large-scale video face hallucination dataset from the\npublic Voxceleb2 benchmark, which challenges prior arts on tackling unaligned\nand tiny face snapshots. To the best of our knowledge, we are the first attempt\nto develop a unified transformer-based solver tailored for video-based face\nhallucination. Extensive experiments on public video face benchmarks show that\nthe proposed method significantly outperforms the state of the arts.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:40:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gan", "Yuan", ""], ["Luo", "Yawei", ""], ["Yu", "Xin", ""], ["Zhang", "Bang", ""], ["Yang", "Yi", ""]]}, {"id": "2105.14962", "submitter": "Yi Xu", "authors": "Yi Xu, Minyi Zhao, Jing Liu, Xinjian Zhang, Longwen Gao, Shuigeng\n  Zhou, Huyang Sun", "title": "Boosting the Performance of Video Compression Artifact Reduction with\n  Reference Frame Proposals and Frequency Domain Information", "comments": "CPVR Workshop, NTIRE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many deep learning based video compression artifact removal algorithms have\nbeen proposed to recover high-quality videos from low-quality compressed\nvideos. Recently, methods were proposed to mine spatiotemporal information via\nutilizing multiple neighboring frames as reference frames. However, these\npost-processing methods take advantage of adjacent frames directly, but neglect\nthe information of the video itself, which can be exploited. In this paper, we\npropose an effective reference frame proposal strategy to boost the performance\nof the existing multi-frame approaches. Besides, we introduce a loss based on\nfast Fourier transformation~(FFT) to further improve the effectiveness of\nrestoration. Experimental results show that our method achieves better fidelity\nand perceptual performance on MFQE 2.0 dataset than the state-of-the-art\nmethods. And our method won Track 1 and Track 2, and was ranked the 2nd in\nTrack 3 of NTIRE 2021 Quality enhancement of heavily compressed videos\nChallenge.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:46:11 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Yi", ""], ["Zhao", "Minyi", ""], ["Liu", "Jing", ""], ["Zhang", "Xinjian", ""], ["Gao", "Longwen", ""], ["Zhou", "Shuigeng", ""], ["Sun", "Huyang", ""]]}, {"id": "2105.14974", "submitter": "Yingqian Wang", "authors": "Ting Liu, Jungang Yang, Boyang Li, Chao Xiao, Yang Sun, Yingqian Wang,\n  Wei An", "title": "Non-Convex Tensor Low-Rank Approximation for Infrared Small Target\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared small target detection plays an important role in many infrared\nsystems. Recently, many infrared small target detection methods have been\nproposed, in which the lowrank model has been used as a powerful tool. However,\nmost low-rank-based methods assign the same weights for different singular\nvalues, which will lead to inaccurate background estimation. Considering that\ndifferent singular values have different importance and should be treated\ndiscriminatively, in this paper, we propose a non-convex tensor low-rank\napproximation (NTLA) method for infrared small target detection. In our method,\nNTLA adaptively assigns different weights to different singular values for\naccurate background estimation. Based on the proposed NTLA, we use the\nasymmetric spatial-temporal total variation (ASTTV) to thoroughly describe\nbackground feature, which can achieve good background estimation and detection\nin complex scenes. Compared with the traditional total variation approach,\nASTTV exploits different smoothness strength for spatial and temporal\nregularization. We develop an efficient algorithm to find the optimal solution\nof the proposed model. Compared with some state-of-the-art methods, the\nproposed method achieve an improvement in different evaluation metrics.\nExtensive experiments on both synthetic and real data demonstrate the proposed\nmethod provide a more robust detection in complex situations with low false\nrates.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:04:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Ting", ""], ["Yang", "Jungang", ""], ["Li", "Boyang", ""], ["Xiao", "Chao", ""], ["Sun", "Yang", ""], ["Wang", "Yingqian", ""], ["An", "Wei", ""]]}, {"id": "2105.14986", "submitter": "Mohammad Eslami", "authors": "Mohammad Eslami, Solale Tabarestani, Malek Adjouadi", "title": "Feasibility Assessment of Multitasking in MRI Neuroimaging Analysis:\n  Tissue Segmentation, Cross-Modality Conversion and Bias correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging is essential in brain studies for the diagnosis and\nidentification of disease, structure, and function of the brain in its healthy\nand disease states. Literature shows that there are advantages of multitasking\nwith some deep learning (DL) schemes in challenging neuroimaging applications.\nThis study examines the feasibility of using multitasking in three different\napplications, including tissue segmentation, cross-modality conversion, and\nbias-field correction. These applications reflect five different scenarios in\nwhich multitasking is explored and 280 training and testing sessions conducted\nfor empirical evaluations. Two well-known networks, U-Net as a well-known\nconvolutional neural network architecture, and a closed architecture based on\nthe conditional generative adversarial network are implemented. Different\nmetrics such as the normalized cross-correlation coefficient and Dice scores\nare used for comparison of methods and results of the different experiments.\nStatistical analysis is also provided by paired t-test. The present study\nexplores the pros and cons of these methods and their practical impacts on\nmultitasking in different implementation scenarios. This investigation shows\nthat bias correction and cross-modality conversion applications are\nsignificantly easier than the segmentation application, and having multitasking\nwith segmentation is not reasonable if one of them is identified as the main\ntarget application. However, when the main application is the segmentation of\ntissues, multitasking with cross-modality conversion is beneficial, especially\nfor the U-net architecture.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:16:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Eslami", "Mohammad", ""], ["Tabarestani", "Solale", ""], ["Adjouadi", "Malek", ""]]}, {"id": "2105.14993", "submitter": "Henry Bradler", "authors": "Henry Bradler, Adrian Kretz and Rudolf Mester", "title": "Urban Traffic Surveillance (UTS): A fully probabilistic 3D tracking\n  approach based on 2D detections", "comments": "Accepted at the 2021 IEEE Intelligent Vehicles Symposium (IV),\n  Nagoya, Japan, July 11-17, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban Traffic Surveillance (UTS) is a surveillance system based on a\nmonocular and calibrated video camera that detects vehicles in an urban traffic\nscenario with dense traffic on multiple lanes and vehicles performing sharp\nturning maneuvers. UTS then tracks the vehicles using a 3D bounding box\nrepresentation and a physically reasonable 3D motion model relying on an\nunscented Kalman filter based approach. Since UTS recovers positions, shape and\nmotion information in a three-dimensional world coordinate system, it can be\nemployed to recognize diverse traffic violations or to supply intelligent\nvehicles with valuable traffic information. We build on YOLOv3 as a detector\nyielding 2D bounding boxes and class labels for each vehicle. A 2D detector\nrenders our system much more independent to different camera perspectives as a\nvariety of labeled training data is available. This allows for a good\ngeneralization while also being more hardware efficient. The task of 3D\ntracking based on 2D detections is supported by integrating class specific\nprior knowledge about the vehicle shape. We quantitatively evaluate UTS using\nself generated synthetic data and ground truth from the CARLA simulator, due to\nthe non-existence of datasets with an urban vehicle surveillance setting and\nlabeled 3D bounding boxes. Additionally, we give a qualitative impression of\nhow UTS performs on real-world data. Our implementation is capable of operating\nin real time on a reasonably modern workstation. To the best of our knowledge,\nUTS is to date the only 3D vehicle tracking system in a surveillance scenario\n(static camera observing moving targets).\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:29:02 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 12:59:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bradler", "Henry", ""], ["Kretz", "Adrian", ""], ["Mester", "Rudolf", ""]]}, {"id": "2105.14994", "submitter": "Kirill Muravyev", "authors": "Andrey Bokovoy, Kirill Muravyev and Konstantin Yakovlev (Federal\n  Research Center for Computer Science and Control of Russian Academy of\n  Sciences)", "title": "MAOMaps: A Photo-Realistic Benchmark For vSLAM and Map Merging Quality\n  Assessment", "comments": "submitted to ECMR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running numerous experiments in simulation is a necessary step before\ndeploying a control system on a real robot. In this paper we introduce a novel\nbenchmark that is aimed at quantitatively evaluating the quality of\nvision-based simultaneous localization and mapping (vSLAM) and map merging\nalgorithms. The benchmark consists of both a dataset and a set of tools for\nautomatic evaluation. The dataset is photo-realistic and provides both the\nlocalization and the map ground truth data. This makes it possible to evaluate\nnot only the localization part of the SLAM pipeline but the mapping part as\nwell. To compare the vSLAM-built maps and the ground-truth ones we introduce a\nnovel way to find correspondences between them that takes the SLAM context into\naccount (as opposed to other approaches like nearest neighbors). The benchmark\nis ROS-compatable and is open-sourced to the community.\n  The data and the code are available at: \\texttt{github.com/CnnDepth/MAOMaps}.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:30:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bokovoy", "Andrey", "", "Federal\n  Research Center for Computer Science and Control of Russian Academy of\n  Sciences"], ["Muravyev", "Kirill", "", "Federal\n  Research Center for Computer Science and Control of Russian Academy of\n  Sciences"], ["Yakovlev", "Konstantin", "", "Federal\n  Research Center for Computer Science and Control of Russian Academy of\n  Sciences"]]}, {"id": "2105.15028", "submitter": "Gennaro Vessio Dr.", "authors": "Giovanna Castellano, Giovanni Sansaro, Gennaro Vessio", "title": "ArtGraph: Towards an Artistic Knowledge Graph", "comments": "Submitted to DS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our ongoing work towards ArtGraph: an artistic knowledge\ngraph based on WikiArt and DBpedia. Automatic art analysis has seen an\never-increasing interest from the pattern recognition and computer vision\ncommunity. However, most of the current work is mainly based solely on\ndigitized artwork images, sometimes supplemented with some metadata and textual\ncomments. A knowledge graph that integrates a rich body of information about\nartworks, artists, painting schools, etc., in a unified structured framework\ncan provide a valuable resource for more powerful information retrieval and\nknowledge discovery tools in the artistic domain.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:09:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Castellano", "Giovanna", ""], ["Sansaro", "Giovanni", ""], ["Vessio", "Gennaro", ""]]}, {"id": "2105.15034", "submitter": "Fei Tang", "authors": "Fei Tang, Michael Kopp", "title": "A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]", "comments": "1 page, 8 formulae", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In their recent paper titled \"Large Associative Memory Problem in\nNeurobiology and Machine Learning\" [arXiv:2008.06996] the authors gave a\nbiologically plausible microscopic theory from which one can recover many dense\nassociative memory models discussed in the literature. We show that the layers\nof the recent \"MLP-mixer\" [arXiv:2105.01601] as well as the essentially\nequivalent model in [arXiv:2105.02723] are amongst them.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:13:00 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:14:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tang", "Fei", ""], ["Kopp", "Michael", ""]]}, {"id": "2105.15041", "submitter": "Francisco Luis Giambelluca", "authors": "Francisco Luis Giambelluca, Marcelo A. Cappelletti, Jorge Osio, Luis\n  A. Giambelluca", "title": "Scorpion detection and classification systems based on computer vision\n  and deep learning for health security purposes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two novel automatic and real-time systems for the detection\nand classification of two genera of scorpions found in La Plata city\n(Argentina) were developed using computer vision and deep learning techniques.\nThe object detection technique was implemented with two different methods, YOLO\n(You Only Look Once) and MobileNet, based on the shape features of the\nscorpions. High accuracy values of 88% and 91%, and high recall values of 90%\nand 97%, have been achieved for both models, respectively, which guarantees\nthat they can successfully detect scorpions. In addition, the MobileNet method\nhas been shown to have excellent performance to detect scorpions within an\nuncontrolled environment and to perform multiple detections. The MobileNet\nmodel was also used for image classification in order to successfully\ndistinguish between dangerous scorpion (Tityus) and non-dangerous scorpion\n(Bothriurus) with the purpose of providing a health security tool. Applications\nfor smartphones were developed, with the advantage of the portability of the\nsystems, which can be used as a help tool for emergency services, or for\nbiological research purposes. The developed systems can be easily scalable to\nother genera and species of scorpions to extend the region where these\napplications can be used.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:26:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Giambelluca", "Francisco Luis", ""], ["Cappelletti", "Marcelo A.", ""], ["Osio", "Jorge", ""], ["Giambelluca", "Luis A.", ""]]}, {"id": "2105.15057", "submitter": "Zhixing Ye", "authors": "Zhixing Ye, Shaofei Qin, Sizhe Chen, Xiaolin Huang", "title": "Dominant Patterns: Critical Features Hidden in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we find the existence of critical features hidden in Deep\nNeuralNetworks (DNNs), which are imperceptible but can actually dominate the\noutputof DNNs. We call these features dominant patterns. As the name suggests,\nfor a natural image, if we add the dominant pattern of a DNN to it, the output\nof this DNN is determined by the dominant pattern instead of the original\nimage, i.e., DNN's prediction is the same with the dominant pattern's. We\ndesign an algorithm to find such patterns by pursuing the insensitivity in the\nfeature space. A direct application of the dominant patterns is the Universal\nAdversarial Perturbations(UAPs). Numerical experiments show that the found\ndominant patterns defeat state-of-the-art UAP methods, especially in label-free\nsettings. In addition, dominant patterns are proved to have the potential to\nattack downstream tasks in which DNNs share the same backbone. We claim that\nDNN-specific dominant patterns reveal some essential properties of a DNN and\nare of great importance for its feature analysis and robustness enhancement.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:43:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ye", "Zhixing", ""], ["Qin", "Shaofei", ""], ["Chen", "Sizhe", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2105.15075", "submitter": "Yulin Wang", "authors": "Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, Gao Huang", "title": "Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with\n  Adaptive Sequence Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision Transformers (ViT) have achieved remarkable success in large-scale\nimage recognition. They split every 2D image into a fixed number of patches,\neach of which is treated as a token. Generally, representing an image with more\ntokens would lead to higher prediction accuracy, while it also results in\ndrastically increased computational cost. To achieve a decent trade-off between\naccuracy and speed, the number of tokens is empirically set to 16x16. In this\npaper, we argue that every image has its own characteristics, and ideally the\ntoken number should be conditioned on each individual input. In fact, we have\nobserved that there exist a considerable number of \"easy\" images which can be\naccurately predicted with a mere number of 4x4 tokens, while only a small\nfraction of \"hard\" ones need a finer representation. Inspired by this\nphenomenon, we propose a Dynamic Transformer to automatically configure a\nproper number of tokens for each input image. This is achieved by cascading\nmultiple Transformers with increasing numbers of tokens, which are sequentially\nactivated in an adaptive fashion at test time, i.e., the inference is\nterminated once a sufficiently confident prediction is produced. We further\ndesign efficient feature reuse and relationship reuse mechanisms across\ndifferent components of the Dynamic Transformer to reduce redundant\ncomputations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100\ndemonstrate that our method significantly outperforms the competitive baselines\nin terms of both theoretical computational efficiency and practical inference\nspeed.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:04:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Yulin", ""], ["Huang", "Rui", ""], ["Song", "Shiji", ""], ["Huang", "Zeyi", ""], ["Huang", "Gao", ""]]}, {"id": "2105.15076", "submitter": "Xiujun Shu", "authors": "Xiujun Shu, Xiao Wang, Xianghao Zang, Shiliang Zhang, Yuanqi Chen, Ge\n  Li, Qi Tian", "title": "Large-Scale Spatio-Temporal Person Re-identification: Algorithm and\n  Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) in the scenario with large spatial and\ntemporal spans has not been fully explored. This is partially because that,\nexisting benchmark datasets were mainly collected with limited spatial and\ntemporal ranges, e.g., using videos recorded in a few days by cameras in a\nspecific region of the campus. Such limited spatial and temporal ranges make it\nhard to simulate the difficulties of person re-ID in real scenarios. In this\nwork, we contribute a novel Large-scale Spatio-Temporal LaST person re-ID\ndataset, including 10,862 identities with more than 228k images. Compared with\nexisting datasets, LaST presents more challenging and high-diversity re-ID\nsettings, and significantly larger spatial and temporal ranges. For instance,\neach person can appear in different cities or countries, and in various time\nslots from daytime to night, and in different seasons from spring to winter. To\nour best knowledge, LaST is a novel person re-ID dataset with the largest\nspatio-temporal ranges. Based on LaST, we verified its challenge by conducting\na comprehensive performance evaluation of 14 re-ID algorithms. We further\npropose an easy-to-implement baseline that works well on such challenging re-ID\nsetting. We also verified that models pre-trained on LaST can generalize well\non existing datasets with short-term and cloth-changing scenarios. We expect\nLaST to inspire future works toward more realistic and challenging re-ID tasks.\nMore information about the dataset is available at\nhttps://github.com/shuxjweb/last.git.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:05:51 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 02:29:58 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 12:40:26 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 02:51:12 GMT"}, {"version": "v5", "created": "Thu, 24 Jun 2021 14:09:30 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Shu", "Xiujun", ""], ["Wang", "Xiao", ""], ["Zang", "Xianghao", ""], ["Zhang", "Shiliang", ""], ["Chen", "Yuanqi", ""], ["Li", "Ge", ""], ["Tian", "Qi", ""]]}, {"id": "2105.15077", "submitter": "Fuxiang Tan", "authors": "Fuxiang Tan, YuTing Kong, Yingying Fan, Feng Liu, Daxin Zhou, Hao\n  zhang, Long Chen, Liang Gao and Yurong Qian", "title": "SDNet: mutil-branch for single image deraining using swin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks degrade the image quality and seriously affect the performance\nof subsequent computer vision tasks, such as autonomous driving, social\nsecurity, etc. Therefore, removing rain streaks from a given rainy images is of\ngreat significance. Convolutional neural networks(CNN) have been widely used in\nimage deraining tasks, however, the local computational characteristics of\nconvolutional operations limit the development of image deraining tasks.\nRecently, the popular transformer has global computational features that can\nfurther facilitate the development of image deraining tasks. In this paper, we\nintroduce Swin-transformer into the field of image deraining for the first time\nto study the performance and potential of Swin-transformer in the field of\nimage deraining. Specifically, we improve the basic module of Swin-transformer\nand design a three-branch model to implement single-image rain removal. The\nformer implements the basic rain pattern feature extraction, while the latter\nfuses different features to further extract and process the image features. In\naddition, we employ a jump connection to fuse deep features and shallow\nfeatures. In terms of experiments, the existing public dataset suffers from\nimage duplication and relatively homogeneous background. So we propose a new\ndataset Rain3000 to validate our model. Therefore, we propose a new dataset\nRain3000 for validating our model. Experimental results on the publicly\navailable datasets Rain100L, Rain100H and our dataset Rain3000 show that our\nproposed method has performance and inference speed advantages over the current\nmainstream single-image rain streaks removal models.The source code will be\navailable at https://github.com/H-tfx/SDNet.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:06:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tan", "Fuxiang", ""], ["Kong", "YuTing", ""], ["Fan", "Yingying", ""], ["Liu", "Feng", ""], ["Zhou", "Daxin", ""], ["zhang", "Hao", ""], ["Chen", "Long", ""], ["Gao", "Liang", ""], ["Qian", "Yurong", ""]]}, {"id": "2105.15078", "submitter": "Meng-Hao Guo", "authors": "Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, Dun Liang, Ralph R. Martin\n  and Shi-Min Hu", "title": "Can Attention Enable MLPs To Catch Up With CNNs?", "comments": "Computational Visual Media, 2021, accepted. 4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first week of May, 2021, researchers from four different institutions:\nGoogle, Tsinghua University, Oxford University and Facebook, shared their\nlatest work [16, 7, 12, 17] on arXiv.org almost at the same time, each\nproposing new learning architectures, consisting mainly of linear layers,\nclaiming them to be comparable, or even superior to convolutional-based models.\nThis sparked immediate discussion and debate in both academic and industrial\ncommunities as to whether MLPs are sufficient, many thinking that learning\narchitectures are returning to MLPs. Is this true? In this perspective, we give\na brief history of learning architectures, including multilayer perceptrons\n(MLPs), convolutional neural networks (CNNs) and transformers. We then examine\nwhat the four newly proposed architectures have in common. Finally, we give our\nviews on challenges and directions for new learning architectures, hoping to\ninspire future research.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:08:46 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Guo", "Meng-Hao", ""], ["Liu", "Zheng-Ning", ""], ["Mu", "Tai-Jiang", ""], ["Liang", "Dun", ""], ["Martin", "Ralph R.", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2105.15089", "submitter": "Jiangning Zhang", "authors": "Jiangning Zhang, Chao Xu, Jian Li, Wenzhou Chen, Yabiao Wang, Ying\n  Tai, Shuo Chen, Chengjie Wang, Feiyue Huang, Yong Liu", "title": "Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by biological evolution, we explain the rationality of Vision\nTransformer by analogy with the proven practical Evolutionary Algorithm (EA)\nand derive that both of them have consistent mathematical representation.\nAnalogous to the dynamic local population in EA, we improve the existing\ntransformer structure and propose a more efficient EAT model, and design\ntask-related heads to deal with different tasks more flexibly. Moreover, we\nintroduce the spatial-filling curve into the current vision transformer to\nsequence image data into a uniform sequential format. Thus we can design a\nunified EAT framework to address multi-modal tasks, separating the network\narchitecture from the data format adaptation. Our approach achieves\nstate-of-the-art results on the ImageNet classification task compared with\nrecent vision transformer works while having smaller parameters and greater\nthroughput. We further conduct multi-model tasks to demonstrate the superiority\nof the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves\nthe rank-1 by +3.7 points over the baseline on the CSS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:20:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Jiangning", ""], ["Xu", "Chao", ""], ["Li", "Jian", ""], ["Chen", "Wenzhou", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Chen", "Shuo", ""], ["Wang", "Chengjie", ""], ["Huang", "Feiyue", ""], ["Liu", "Yong", ""]]}, {"id": "2105.15093", "submitter": "Anuj Rai", "authors": "Anuj Rai, Narayanan C. Krishnan, and Sukalpa Chanda", "title": "Pho(SC)Net: An Approach Towards Zero-shot Word Image Recognition in\n  Historical Documents", "comments": "Published at 16th International Conference on Document Analysis and\n  Recognition (ICDAR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating words in a historical document image archive for word image\nrecognition purpose demands time and skilled human resource (like historians,\npaleographers). In a real-life scenario, obtaining sample images for all\npossible words is also not feasible. However, Zero-shot learning methods could\naptly be used to recognize unseen/out-of-lexicon words in such historical\ndocument images. Based on previous state-of-the-art methods for word spotting\nand recognition, we propose a hybrid representation that considers the\ncharacter's shape appearance to differentiate between two different words and\nhas shown to be more effective in recognizing unseen words. This representation\nhas been termed as Pyramidal Histogram of Shapes (PHOS), derived from PHOC,\nwhich embeds information about the occurrence and position of characters in the\nword. Later, the two representations are combined and experiments were\nconducted to examine the effectiveness of an embedding that has properties of\nboth PHOS and PHOC. Encouraging results were obtained on two publicly available\nhistorical document datasets and one synthetic handwritten dataset, which\njustifies the efficacy of \"Phos\" and the combined \"Pho(SC)\" representation.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:22:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rai", "Anuj", ""], ["Krishnan", "Narayanan C.", ""], ["Chanda", "Sukalpa", ""]]}, {"id": "2105.15094", "submitter": "Michael Horry Mr", "authors": "Michael J. Horry, Subrata Chakraborty, Biswajeet Pradhan, Maryam\n  Fallahpoor, Chegeni Hossein, Manoranjan Paul", "title": "Systematic investigation into generalization of COVID-19 CT deep\n  learning models with Gabor ensemble for lung involvement scoring", "comments": "39 Pages, 8 figures, 14 tables comparing the generalization of\n  COVID-19 CT Deep Learning Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has inspired unprecedented data collection and computer\nvision modelling efforts worldwide, focusing on diagnosis and stratification of\nCOVID-19 from medical images. Despite this large-scale research effort, these\nmodels have found limited practical application due in part to unproven\ngeneralization of these models beyond their source study. This study\ninvestigates the generalizability of key published models using the publicly\navailable COVID-19 Computed Tomography data through cross dataset validation.\nWe then assess the predictive ability of these models for COVID-19 severity\nusing an independent new dataset that is stratified for COVID-19 lung\ninvolvement. Each inter-dataset study is performed using histogram\nequalization, and contrast limited adaptive histogram equalization with and\nwithout a learning Gabor filter. The study shows high variability in the\ngeneralization of models trained on these datasets due to varied sample image\nprovenances and acquisition processes amongst other factors. We show that under\ncertain conditions, an internally consistent dataset can generalize well to an\nexternal dataset despite structural differences between these datasets with f1\nscores up to 86%. Our best performing model shows high predictive accuracy for\nlung involvement score for an independent dataset for which expertly labelled\nlung involvement stratification is available. Creating an ensemble of our best\nmodel for disease positive prediction with our best model for disease negative\nprediction using a min-max function resulted in a superior model for lung\ninvolvement prediction with average predictive accuracy of 75% for zero lung\ninvolvement and 96% for 75-100% lung involvement with almost linear\nrelationship between these stratifications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:49:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Horry", "Michael J.", ""], ["Chakraborty", "Subrata", ""], ["Pradhan", "Biswajeet", ""], ["Fallahpoor", "Maryam", ""], ["Hossein", "Chegeni", ""], ["Paul", "Manoranjan", ""]]}, {"id": "2105.15134", "submitter": "Zixin Wen", "authors": "Zixin Wen, Yuanzhi Li", "title": "Toward Understanding the Feature Learning Process of Self-supervised\n  Contrastive Learning", "comments": "V3 corrected related works. Accepted to ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n  In this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:42:09 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 10:05:33 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 17:48:01 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wen", "Zixin", ""], ["Li", "Yuanzhi", ""]]}, {"id": "2105.15157", "submitter": "Kai Zhao", "authors": "Tao Wang and Ruixin Zhang and Xingyu Chen and Kai Zhao and Xiaolin\n  Huang and Yuge Huang and Shaoxin Li and Jilin Li and Feiyue Huang", "title": "Adaptive Feature Alignment for Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies reveal that Convolutional Neural Networks (CNNs) are typically\nvulnerable to adversarial attacks, which pose a threat to security-sensitive\napplications. Many adversarial defense methods improve robustness at the cost\nof accuracy, raising the contradiction between standard and adversarial\naccuracies. In this paper, we observe an interesting phenomenon that feature\nstatistics change monotonically and smoothly w.r.t the rising of attacking\nstrength. Based on this observation, we propose the adaptive feature alignment\n(AFA) to generate features of arbitrary attacking strengths. Our method is\ntrained to automatically align features of arbitrary attacking strength. This\nis done by predicting a fusing weight in a dual-BN architecture. Unlike\nprevious works that need to either retrain the model or manually tune a\nhyper-parameters for different attacking strengths, our method can deal with\narbitrary attacking strengths with a single model without introducing any\nhyper-parameter. Importantly, our method improves the model robustness against\nadversarial samples without incurring much loss in standard accuracy.\nExperiments on CIFAR-10, SVHN, and tiny-ImageNet datasets demonstrate that our\nmethod outperforms the state-of-the-art under a wide range of attacking\nstrengths.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:01:05 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 08:45:33 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Tao", ""], ["Zhang", "Ruixin", ""], ["Chen", "Xingyu", ""], ["Zhao", "Kai", ""], ["Huang", "Xiaolin", ""], ["Huang", "Yuge", ""], ["Li", "Shaoxin", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2105.15168", "submitter": "Jiemin Fang", "authors": "Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, Qi\n  Tian", "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating\n  Messenger Tokens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have offered a new methodology of designing neural networks for\nvisual recognition. Compared to convolutional networks, Transformers enjoy the\nability of referring to global features at each stage, yet the attention module\nbrings higher computational overhead that obstructs the application of\nTransformers to process high-resolution visual data. This paper aims to\nalleviate the conflict between efficiency and flexibility, for which we propose\na specialized token for each region that serves as a messenger (MSG). Hence, by\nmanipulating these MSG tokens, one can flexibly exchange visual information\nacross regions and the computational complexity is reduced. We then integrate\nthe MSG token into a multi-scale architecture named MSG-Transformer. In\nstandard image classification and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU and CPU is accelerated.\nThe code will be available at https://github.com/hustvl/MSG-Transformer.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:16:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fang", "Jiemin", ""], ["Xie", "Lingxi", ""], ["Wang", "Xinggang", ""], ["Zhang", "Xiaopeng", ""], ["Liu", "Wenyu", ""], ["Tian", "Qi", ""]]}, {"id": "2105.15203", "submitter": "Enze Xie", "authors": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez,\n  Ping Luo", "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with\n  Transformers", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perception\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a\nnovel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the\ninterpolation of positional codes which leads to decreased performance when the\ntesting resolution differs from training. 2) SegFormer avoids complex decoders.\nThe proposed MLP decoder aggregates information from different layers, and thus\ncombining both local attention and global attention to render powerful\nrepresentations. We show that this simple and lightweight design is the key to\nefficient segmentation on Transformers. We scale our approach up to obtain a\nseries of models from SegFormer-B0 to SegFormer-B5, reaching significantly\nbetter performance and efficiency than previous counterparts. For example,\nSegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x\nsmaller and 2.2% better than the previous best method. Our best model,\nSegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows\nexcellent zero-shot robustness on Cityscapes-C. Code will be released at:\ngithub.com/NVlabs/SegFormer.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:59:51 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 22:51:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Enze", ""], ["Wang", "Wenhai", ""], ["Yu", "Zhiding", ""], ["Anandkumar", "Anima", ""], ["Alvarez", "Jose M.", ""], ["Luo", "Ping", ""]]}]