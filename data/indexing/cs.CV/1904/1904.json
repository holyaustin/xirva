[{"id": "1904.00048", "submitter": "Sanping Zhou", "authors": "Sanping Zhou, Jimuyang Zhang, Jinjun Wang, Fei Wang, Dong Huang", "title": "SE2Net: Siamese Edge-Enhancement Network for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network significantly boosted the capability of\nsalient object detection in handling large variations of scenes and object\nappearances. However, convolution operations seek to generate strong responses\non individual pixels, while lack the ability to maintain the spatial structure\nof objects. Moreover, the down-sampling operations, such as pooling and\nstriding, lose spatial details of the salient objects. In this paper, we\npropose a simple yet effective Siamese Edge-Enhancement Network (SE2Net) to\npreserve the edge structure for salient object detection. Specifically, a novel\nmulti-stage siamese network is built to aggregate the low-level and high-level\nfeatures, and parallelly estimate the salient maps of edges and regions. As a\nresult, the predicted regions become more accurate by enhancing the responses\nat edges, and the predicted edges become more semantic by suppressing the false\npositives in background. After the refined salient maps of edges and regions\nare produced by the SE2Net, an edge-guided inference algorithm is designed to\nfurther improve the resulting salient masks along the predicted edges.\nExtensive experiments on several benchmark datasets have been conducted, which\nshow that our method is superior than the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 18:50:10 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 21:19:38 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhou", "Sanping", ""], ["Zhang", "Jimuyang", ""], ["Wang", "Jinjun", ""], ["Wang", "Fei", ""], ["Huang", "Dong", ""]]}, {"id": "1904.00068", "submitter": "Fakrul Islam Tushar", "authors": "Fakrul Islam Tushar, Basel Alyafi, Md. Kamrul Hasan, Lavsen Dahal", "title": "Brain Tissue Segmentation Using NeuroNet With Different Pre-processing\n  Techniques", "comments": "3rd International Conference on Imaging, Vision & Pattern Recognition\n  (IVPR)2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of brain Magnetic Resonance Imaging (MRI) images is\none of the vital steps for quantitative analysis of brain for further\ninspection. In this paper, NeuroNet has been adopted to segment the brain\ntissues (white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF))\nwhich uses Residual Network (ResNet) in encoder and Fully Convolution Network\n(FCN) in the decoder. To achieve the best performance, various hyper-parameters\nhave been tuned, while, network parameters (kernel and bias) were initialized\nusing the NeuroNet pre-trained model. Different pre-processing pipelines have\nalso been introduced to get a robust trained model. The model has been trained\nand tested on IBSR18 data-set. To validate the research outcome, performance\nwas measured quantitatively using Dice Similarity Coefficient (DSC) and is\nreported on average as 0.84 for CSF, 0.94 for GM, and 0.94 for WM. The outcome\nof the research indicates that for the IBSR18 data-set, pre-processing and\nproper tuning of hyper-parameters for NeuroNet model have improvement in DSC\nfor the brain tissue segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 19:42:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Tushar", "Fakrul Islam", ""], ["Alyafi", "Basel", ""], ["Hasan", "Md. Kamrul", ""], ["Dahal", "Lavsen", ""]]}, {"id": "1904.00069", "submitter": "Xuelin Chen", "authors": "Xuelin Chen, Baoquan Chen and Niloy J. Mitra", "title": "Unpaired Point Cloud Completion on Real Scans using Adversarial Training", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D scanning solutions become increasingly popular, several deep learning\nsetups have been developed geared towards that task of scan completion, i.e.,\nplausibly filling in regions there were missed in the raw scans. These methods,\nhowever, largely rely on supervision in the form of paired training data, i.e.,\npartial scans with corresponding desired completed scans. While these methods\nhave been successfully demonstrated on synthetic data, the approaches cannot be\ndirectly used on real scans in absence of suitable paired training data. We\ndevelop a first approach that works directly on input point clouds, does not\nrequire paired training data, and hence can directly be applied to real scans\nfor scan completion. We evaluate the approach qualitatively on several\nreal-world datasets (ScanNet, Matterport, KITTI), quantitatively on 3D-EPN\nshape completion benchmark dataset, and demonstrate realistic completions under\nvarying levels of incompleteness.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 19:45:21 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 08:59:00 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 11:38:31 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chen", "Xuelin", ""], ["Chen", "Baoquan", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1904.00073", "submitter": "Elena Balashova", "authors": "Elena Balashova, Jiangping Wang, Vivek Singh, Bogdan Georgescu, Brian\n  Teixeira, Ankur Kapoor", "title": "3D Organ Shape Reconstruction from Topogram Images", "comments": "12 pages, accepted to International Conference on Information\n  Processing in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic delineation and measurement of main organs such as liver is one of\nthe critical steps for assessment of hepatic diseases, planning and\npostoperative or treatment follow-up. However, addressing this problem\ntypically requires performing computed tomography (CT) scanning and complicated\npostprocessing of the resulting scans using slice-by-slice techniques. In this\npaper, we show that 3D organ shape can be automatically predicted directly from\ntopogram images, which are easier to acquire and have limited exposure to\nradiation during acquisition, compared to CT scans. We evaluate our approach on\nthe challenging task of predicting liver shape using a generative model. We\nalso demonstrate that our method can be combined with user annotations, such as\na 2D mask, for improved prediction accuracy. We show compelling results on 3D\nliver shape reconstruction and volume estimation on 2129 CT scans.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 19:51:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Balashova", "Elena", ""], ["Wang", "Jiangping", ""], ["Singh", "Vivek", ""], ["Georgescu", "Bogdan", ""], ["Teixeira", "Brian", ""], ["Kapoor", "Ankur", ""]]}, {"id": "1904.00129", "submitter": "Yipin Zhou", "authors": "Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara L. Berg", "title": "Dance Dance Generation: Motion Transfer for Internet Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents computational methods for transferring body movements from\none person to another with videos collected in the wild. Specifically, we train\na personalized model on a single video from the Internet which can generate\nvideos of this target person driven by the motions of other people. Our model\nis built on two generative networks: a human (foreground) synthesis net which\ngenerates photo-realistic imagery of the target person in a novel pose, and a\nfusion net which combines the generated foreground with the scene (background),\nadding shadows or reflections as needed to enhance realism. We validate the the\nefficacy of our proposed models over baselines with qualitative and\nquantitative evaluations as well as a subjective test.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 01:01:12 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhou", "Yipin", ""], ["Wang", "Zhaowen", ""], ["Fang", "Chen", ""], ["Bui", "Trung", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1904.00152", "submitter": "Chieh-Hsin Lai", "authors": "Chieh-Hsin Lai, Dongmian Zou, and Gilad Lerman", "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "comments": "This work is on the ICLR 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 05:30:54 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 22:44:25 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lai", "Chieh-Hsin", ""], ["Zou", "Dongmian", ""], ["Lerman", "Gilad", ""]]}, {"id": "1904.00158", "submitter": "Peipei Li", "authors": "Peipei Li, Huaibo Huang, Yibo Hu, Xiang Wu, Ran He and Zhenan Sun", "title": "UVA: A Universal Variational Framework for Continuous Age Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods for facial age analysis tend to utilize accurate age\nlabels in a supervised way. However, existing age datasets lies in a limited\nrange of ages, leading to a long-tailed distribution. To alleviate the problem,\nthis paper proposes a Universal Variational Aging (UVA) framework to formulate\nfacial age priors in a disentangling manner. Benefiting from the variational\nevidence lower bound, the facial images are encoded and disentangled into an\nage-irrelevant distribution and an age-related distribution in the latent\nspace. A conditional introspective adversarial learning mechanism is introduced\nto boost the image quality. In this way, when manipulating the age-related\ndistribution, UVA can achieve age translation with arbitrary ages. Further, by\nsampling noise from the age-irrelevant distribution, we can generate\nphotorealistic facial images with a specific age. Moreover, given an input face\nimage, the mean value of age-related distribution can be treated as an age\nestimator. These indicate that UVA can efficiently and accurately estimate the\nage-related distribution by a disentangling manner, even if the training\ndataset performs a long-tailed age distribution. UVA is the first attempt to\nachieve facial age analysis tasks, including age translation, age generation\nand age estimation, in a universal framework. The qualitative and quantitative\nexperiments demonstrate the superiority of UVA on five popular datasets,\nincluding CACD2000, Morph, UTKFace, MegaAge-Asian and FG-NET.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 07:07:06 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Peipei", ""], ["Huang", "Huaibo", ""], ["Hu", "Yibo", ""], ["Wu", "Xiang", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1904.00167", "submitter": "Yuezun Li", "authors": "Xin Yang and Yuezun Li and Honggang Qi and Siwei Lyu", "title": "Exposing GAN-synthesized Faces Using Landmark Locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversary networks (GANs) have recently led to highly realistic\nimage synthesis results. In this work, we describe a new method to expose\nGAN-synthesized images using the locations of the facial landmark points. Our\nmethod is based on the observations that the facial parts configuration\ngenerated by GAN models are different from those of the real faces, due to the\nlack of global constraints. We perform experiments demonstrating this\nphenomenon, and show that an SVM classifier trained using the locations of\nfacial landmark points is sufficient to achieve good classification performance\nfor GAN-synthesized faces.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:27:46 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yang", "Xin", ""], ["Li", "Yuezun", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1904.00168", "submitter": "Peipei Li", "authors": "Peipei Li, Xiang Wu, Yibo Hu, Ran He and Zhenan Sun", "title": "M2FPA: A Multi-Yaw Multi-Pitch High-Quality Database and Benchmark for\n  Facial Pose Analysis", "comments": "Accepted for publication at ICCV2019; The M2FPA dataset is available\n  at https://pp2li.github.io/M2FPA-dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial images in surveillance or mobile scenarios often have large view-point\nvariations in terms of pitch and yaw angles. These jointly occurred angle\nvariations make face recognition challenging. Current public face databases\nmainly consider the case of yaw variations. In this paper, a new large-scale\nMulti-yaw Multi-pitch high-quality database is proposed for Facial Pose\nAnalysis (M2FPA), including face frontalization, face rotation, facial pose\nestimation and pose-invariant face recognition. It contains 397,544 images of\n229 subjects with yaw, pitch, attribute, illumination and accessory. M2FPA is\nthe most comprehensive multi-view face database for facial pose analysis.\nFurther, we provide an effective benchmark for face frontalization and\npose-invariant face recognition on M2FPA with several state-of-the-art methods,\nincluding DR-GAN, TP-GAN and CAPG-GAN. We believe that the new database and\nbenchmark can significantly push forward the advance of facial pose analysis in\nreal-world applications. Moreover, a simple yet effective parsing guided\ndiscriminator is introduced to capture the local consistency during GAN\noptimization. Extensive quantitative and qualitative results on M2FPA and\nMulti-PIE demonstrate the superiority of our face frontalization method.\nBaseline results for both face synthesis and face recognition from\nstate-of-theart methods demonstrate the challenge offered by this new database.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:35:36 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 07:34:22 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Peipei", ""], ["Wu", "Xiang", ""], ["Hu", "Yibo", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1904.00170", "submitter": "Jingcai Guo", "authors": "Jingcai Guo, Song Guo", "title": "Adaptive Adjustment with Semantic Feature Space for Zero-Shot\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most recent years, zero-shot recognition (ZSR) has gained increasing\nattention in machine learning and image processing fields. It aims at\nrecognizing unseen class instances with knowledge transferred from seen\nclasses. This is typically achieved by exploiting a pre-defined semantic\nfeature space (FS), i.e., semantic attributes or word vectors, as a bridge to\ntransfer knowledge between seen and unseen classes. However, due to the absence\nof unseen classes during training, the conventional ZSR easily suffers from\ndomain shift and hubness problems. In this paper, we propose a novel ZSR\nlearning framework that can handle these two issues well by adaptively\nadjusting semantic FS. To the best of our knowledge, our work is the first to\nconsider the adaptive adjustment of semantic FS in ZSR. Moreover, our solution\ncan be formulated to a more efficient framework that significantly boosts the\ntraining. Extensive experiments show the remarkable performance improvement of\nour model compared with other existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:39:03 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Guo", "Jingcai", ""], ["Guo", "Song", ""]]}, {"id": "1904.00187", "submitter": "Se-In Jang", "authors": "Zainab Alhakeem and Se-In Jang", "title": "An LBP-HOG Descriptor Based on Matrix Projection For Mammogram\n  Classification", "comments": "5 pages, submitted to ICIP 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In image based feature descriptor design, local information from image\npatches are extracted using iterative scanning operations which cause high\ncomputational costs. In order to avoid such scanning operations, we present\nmatrix multiplication based local feature descriptors, namely a Matrix\nprojection based Local Binary Pattern (M-LBP) descriptor and a Matrix\nprojection based Histogram of Oriented Gradients (M-HOG) descriptor.\nAdditionally, an integrated formulation of M-LBP and M-HOG (M-LBP-HOG) is also\nproposed to perform the two descriptors together in a single step. The proposed\ndescriptors are evaluated using a publicly available mammogram database. The\nresults show promising performances in terms of classification accuracy and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 09:51:40 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 02:13:45 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 07:04:26 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2021 11:49:46 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Alhakeem", "Zainab", ""], ["Jang", "Se-In", ""]]}, {"id": "1904.00197", "submitter": "Suraj Tripathi", "authors": "Abhay Kumar, Nishant Jain, Chirag Singh, Suraj Tripathi", "title": "Exploiting SIFT Descriptor for Rotation Invariant Convolutional Neural\n  Network", "comments": "Accepted in IEEE INDICON 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to exploit the distinctive invariant\nfeatures in convolutional neural network. The proposed CNN model uses Scale\nInvariant Feature Transform (SIFT) descriptor instead of the max-pooling layer.\nMax-pooling layer discards the pose, i.e., translational and rotational\nrelationship between the low-level features, and hence unable to capture the\nspatial hierarchies between low and high level features. The SIFT descriptor\nlayer captures the orientation and the spatial relationship of the features\nextracted by convolutional layer. The proposed SIFT Descriptor CNN therefore\ncombines the feature extraction capabilities of CNN model and rotation\ninvariance of SIFT descriptor. Experimental results on the MNIST and\nfashionMNIST datasets indicates reasonable improvements over conventional\nmethods available in literature.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 11:00:21 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kumar", "Abhay", ""], ["Jain", "Nishant", ""], ["Singh", "Chirag", ""], ["Tripathi", "Suraj", ""]]}, {"id": "1904.00198", "submitter": "Haoyu Ma", "authors": "Haoyu Ma, Juncheng Zhang, Shaojun Liu, Qingmin Liao", "title": "Boundary Aware Multi-Focus Image Fusion Using Deep Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/ICME.2019.00201", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since it is usually difficult to capture an all-in-focus image of a 3D scene\ndirectly, various multi-focus image fusion methods are employed to generate it\nfrom several images focusing at different depths. However, the performance of\nexisting methods is barely satisfactory and often degrades for areas near the\nfocused/defocused boundary (FDB). In this paper, a boundary aware method using\ndeep neural network is proposed to overcome this problem. (1) Aiming to acquire\nimproved fusion images, a 2-channel deep network is proposed to better extract\nthe relative defocus information of the two source images. (2) After analyzing\nthe different situations for patches far away from and near the FDB, we use two\nnetworks to handle them respectively. (3) To simulate the reality more\nprecisely, a new approach of dataset generation is designed. Experiments\ndemonstrate that the proposed method outperforms the state-of-the-art methods,\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 11:01:16 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ma", "Haoyu", ""], ["Zhang", "Juncheng", ""], ["Liu", "Shaojun", ""], ["Liao", "Qingmin", ""]]}, {"id": "1904.00205", "submitter": "Taimoor Tariq Mr.", "authors": "Taimoor Tariq, Juan Luis Gonzalez, Munchurl Kim", "title": "A HVS-inspired Attention to Improve Loss Metrics for CNN-based\n  Perception-Oriented Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Network (CNN) features have been demonstrated to be\neffective perceptual quality features. The perceptual loss, based on feature\nmaps of pre-trained CNN's has proven to be remarkably effective for CNN based\nperceptual image restoration problems. In this work, taking inspiration from\nthe the Human Visual System (HVS) and visual perception, we propose a spatial\nattention mechanism based on the dependency human contrast sensitivity on\nspatial frequency. We identify regions in input images, based on the underlying\nspatial frequency, which are not generally well reconstructed during\nSuper-Resolution but are most important in terms of visual sensitivity. Based\non this prior, we design a spatial attention map that is applied to feature\nmaps in the perceptual loss and its variants, helping them to identify regions\nthat are of more perceptual importance. The results demonstrate the our\ntechnique improves the ability of the perceptual loss and contextual loss to\ndeliver more natural images in CNN based super-resolution.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 12:14:50 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 15:19:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Tariq", "Taimoor", ""], ["Gonzalez", "Juan Luis", ""], ["Kim", "Munchurl", ""]]}, {"id": "1904.00227", "submitter": "Humam Alwassel", "authors": "Alejandro Pardo, Humam Alwassel, Fabian Caba Heilbron, Ali Thabet,\n  Bernard Ghanem", "title": "RefineLoc: Iterative Refinement for Weakly-Supervised Action\n  Localization", "comments": "Accepted to WACV 2021. Project website:\n  http://humamalwassel.com/publication/refineloc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action detectors are usually trained using datasets with\nfully-supervised temporal annotations. Building such datasets is an expensive\ntask. To alleviate this problem, recent methods have tried to leverage weak\nlabeling, where videos are untrimmed and only a video-level label is available.\nIn this paper, we propose RefineLoc, a novel weakly-supervised temporal action\nlocalization method. RefineLoc uses an iterative refinement approach by\nestimating and training on snippet-level pseudo ground truth at every\niteration. We show the benefit of this iterative approach and present an\nextensive analysis of five different pseudo ground truth generators. We show\nthe effectiveness of our model on two standard action datasets, ActivityNet\nv1.2 and THUMOS14. RefineLoc shows competitive results with the\nstate-of-the-art in weakly-supervised temporal localization. Additionally, our\niterative refinement process is able to significantly improve the performance\nof two state-of-the-art methods, setting a new state-of-the-art on THUMOS14.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 14:51:44 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 13:32:28 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 22:22:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Pardo", "Alejandro", ""], ["Alwassel", "Humam", ""], ["Heilbron", "Fabian Caba", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.00229", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Gim Hee Lee", "title": "USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the USIP detector: an Unsupervised Stable Interest\nPoint detector that can detect highly repeatable and accurately localized\nkeypoints from 3D point clouds under arbitrary transformations without the need\nfor any ground truth training data. Our USIP detector consists of a feature\nproposal network that learns stable keypoints from input 3D point clouds and\ntheir respective transformed pairs from randomly generated transformations. We\nprovide degeneracy analysis of our USIP detector and suggest solutions to\nprevent it. We encourage high repeatability and accurate localization of the\nkeypoints with a probabilistic chamfer loss that minimizes the distances\nbetween the detected keypoints from the training point cloud pairs. Extensive\nexperimental results of repeatability tests on several simulated and real-world\n3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP\ndetector significantly outperforms existing hand-crafted and deep\nlearning-based 3D keypoint detectors. Our code is available at the project\nwebsite. https://github.com/lijx10/USIP\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 15:11:20 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Jiaxin", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1904.00230", "submitter": "Humam Alwassel", "authors": "Ali Thabet, Humam Alwassel, Bernard Ghanem", "title": "MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised task on point clouds, in order to learn\nmeaningful point-wise features that encode local structure around each point.\nOur self-supervised network, named MortonNet, operates directly on\nunstructured/unordered point clouds. Using a multi-layer RNN, MortonNet\npredicts the next point in a point sequence created by a popular and fast Space\nFilling Curve, the Morton-order curve. The final RNN state (coined Morton\nfeature) is versatile and can be used in generic 3D tasks on point clouds. In\nfact, we show how Morton features can be used to significantly improve\nperformance (+3% for 2 popular semantic segmentation algorithms) in the task of\nsemantic segmentation of point clouds on the challenging and large-scale S3DIS\ndataset. We also show how MortonNet trained on S3DIS transfers well to another\nlarge-scale dataset, vKITTI, leading to an improvement over state-of-the-art of\n3.8%. Finally, we use Morton features to train a much simpler and more stable\nmodel for part segmentation in ShapeNet. Our results show how our\nself-supervised task results in features that are useful for 3D segmentation\ntasks, and generalize well to other datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 15:12:49 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Thabet", "Ali", ""], ["Alwassel", "Humam", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.00240", "submitter": "Prerana Mukherjee", "authors": "Chandra Sekhar, Prerana Mukherjee, Devanur S Guru and Viswanath\n  Pulabaigari", "title": "OSVNet: Convolutional Siamese Network for Writer Independent Online\n  Signature Verification", "comments": "accepted in International Conference on Document Analysis and\n  Recognition (ICDAR 2019), University of Technology Sydney (UTS), Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online signature verification (OSV) is one of the most challenging tasks in\nwriter identification and digital forensics. Owing to the large\nintra-individual variability, there is a critical requirement to accurately\nlearn the intra-personal variations of the signature to achieve higher\nclassification accuracy. To achieve this, in this paper, we propose an OSV\nframework based on deep convolutional Siamese network (DCSN). DCSN\nautomatically extracts robust feature descriptions based on metric-based loss\nfunction which decreases intra-writer variability (Genuine-Genuine) and\nincreases inter-individual variability (Genuine-Forgery) and directs the DCSN\nfor effective discriminative representation learning for online signatures and\nextend it for one shot learning framework. Comprehensive experimentation\nconducted on three widely accepted benchmark datasets MCYT-100 (DB1), MCYT-330\n(DB2) and SVC-2004-Task2 demonstrate the capability of our framework to\ndistinguish the genuine and forgery samples. Experimental results confirm the\nefficiency of deep convolutional Siamese network based OSV by achieving a lower\nerror rate as compared to many recent and state-of-the art OSV techniques.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 16:07:59 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 11:42:14 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Sekhar", "Chandra", ""], ["Mukherjee", "Prerana", ""], ["Guru", "Devanur S", ""], ["Pulabaigari", "Viswanath", ""]]}, {"id": "1904.00244", "submitter": "Sara Iodice", "authors": "Sara Iodice and Krystian Mikolajczyk", "title": "Person Re-identification with Bias-controlled Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the effectiveness of adversarial training in the area of\nGenerative Adversarial Networks we present a new approach for learning feature\nrepresentations in person re-identification. We investigate different types of\nbias that typically occur in re-ID scenarios, i.e., pose, body part and camera\nview, and propose a general approach to address them. We introduce an\nadversarial strategy for controlling bias, named Bias-controlled Adversarial\nframework (BCA), with two complementary branches to reduce or to enhance\nbias-related features. The results and comparison to the state of the art on\ndifferent benchmarks show that our framework is an effective strategy for\nperson re-identification. The performance improvements are in both full and\npartial views of persons.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 16:25:23 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Iodice", "Sara", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1904.00247", "submitter": "Adriano Belletti Felicio", "authors": "Adriano Belletti Felicio, Andr\\'e Luiz Cunha", "title": "Classification of Motorcycles using Extracted Images of Traffic\n  Monitoring Videos", "comments": "12 pages, 6 figures, 4 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the great growth of motorcycles in the urban fleet and the growth of\nthe study on its behavior and of how this vehicle affects the flow of traffic\nbecomes necessary the development of tools and techniques different from the\nconventional ones to identify its presence in the traffic flow and be able to\nextract your information. The article in question attempts to contribute to the\nstudy on this type of vehicle by generating a motorcycle image bank and\ndeveloping and calibrating a motorcycle classifier by combining the LBP\ntechniques to create the characteristic vectors and the classification\ntechnique LinearSVC to perform the predictions. In this way the classifier of\nvehicles of the type motorcycle developed in this research can classify the\nimages of vehicles extracted of videos of monitoring between two classes\nmotorcycles and non-motorcycles with a precision and an accuracy superior to\n0,9.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 16:40:15 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 15:32:26 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Felicio", "Adriano Belletti", ""], ["Cunha", "Andr\u00e9 Luiz", ""]]}, {"id": "1904.00276", "submitter": "Fei Wang", "authors": "Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han and Dong Huang", "title": "Person-in-WiFi: Fine-grained Person Perception using WiFi", "comments": "10 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained person perception such as body segmentation and pose estimation\nhas been achieved with many 2D and 3D sensors such as RGB/depth cameras, radars\n(e.g., RF-Pose) and LiDARs. These sensors capture 2D pixels or 3D point clouds\nof person bodies with high spatial resolution, such that the existing\nConvolutional Neural Networks can be directly applied for perception. In this\npaper, we take one step forward to show that fine-grained person perception is\npossible even with 1D sensors: WiFi antennas. To our knowledge, this is the\nfirst work to perceive persons with pervasive WiFi devices, which is cheaper\nand power efficient than radars and LiDARs, invariant to illumination, and has\nlittle privacy concern comparing to cameras. We used two sets of off-the-shelf\nWiFi antennas to acquire signals, i.e., one transmitter set and one receiver\nset. Each set contains three antennas lined-up as a regular household WiFi\nrouter. The WiFi signal generated by a transmitter antenna, penetrates through\nand reflects on human bodies, furniture and walls, and then superposes at a\nreceiver antenna as a 1D signal sample (instead of 2D pixels or 3D point\nclouds). We developed a deep learning approach that uses annotations on 2D\nimages, takes the received 1D WiFi signals as inputs, and performs body\nsegmentation and pose estimation in an end-to-end manner. Experimental results\non over 100000 frames under 16 indoor scenes demonstrate that Person-in-WiFi\nachieved person perception comparable to approaches using 2D images.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 19:48:59 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wang", "Fei", ""], ["Zhou", "Sanping", ""], ["Panev", "Stanislav", ""], ["Han", "Jinsong", ""], ["Huang", "Dong", ""]]}, {"id": "1904.00277", "submitter": "Fei Wang", "authors": "Fei Wang, Stanislav Panev, Ziyi Dai, Jinsong Han and Dong Huang", "title": "Can WiFi Estimate Person Pose?", "comments": "11 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WiFi human sensing has achieved great progress in indoor localization,\nactivity classification, etc. Retracing the development of these work, we have\na natural question: can WiFi devices work like cameras for vision applications?\nIn this paper We try to answer this question by exploring the ability of WiFi\non estimating single person pose. We use a 3-antenna WiFi sender and a\n3-antenna receiver to generate WiFi data. Meanwhile, we use a synchronized\ncamera to capture person videos for corresponding keypoint annotations. We\nfurther propose a fully convolutional network (FCN), termed WiSPPN, to estimate\nsingle person pose from the collected data and annotations. Evaluation on over\n80k images (16 sites and 8 persons) replies aforesaid question with a positive\nanswer. Codes have been made publicly available at\nhttps://github.com/geekfeiw/WiSPPN.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 19:50:52 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 04:46:21 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Fei", ""], ["Panev", "Stanislav", ""], ["Dai", "Ziyi", ""], ["Han", "Jinsong", ""], ["Huang", "Dong", ""]]}, {"id": "1904.00284", "submitter": "Chieh Hubert Lin", "authors": "Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei\n  Wei, Hwann-Tzong Chen", "title": "COCO-GAN: Generation by Parts via Conditional Coordinating", "comments": "Accepted to ICCV'19 (oral). All images are compressed due to size\n  limit, please access the full-resolution version via Google Drive:\n  http://bit.ly/COCO-GAN-full", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can only interact with part of the surrounding environment due to\nbiological restrictions. Therefore, we learn to reason the spatial\nrelationships across a series of observations to piece together the surrounding\nenvironment. Inspired by such behavior and the fact that machines also have\ncomputational constraints, we propose \\underline{CO}nditional\n\\underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images\nby parts based on their spatial coordinates as the condition. On the other\nhand, the discriminator learns to justify realism across multiple assembled\npatches by global coherence, local appearance, and edge-crossing continuity.\nDespite the full images are never generated during training, we show that\nCOCO-GAN can produce \\textbf{state-of-the-art-quality} full images during\ninference. We further demonstrate a variety of novel applications enabled by\nteaching the network to be aware of coordinates. First, we perform\nextrapolation to the learned coordinate manifold and generate off-the-boundary\npatches. Combining with the originally generated full image, COCO-GAN can\nproduce images that are larger than training samples, which we called\n\"beyond-boundary generation\". We then showcase panorama generation within a\ncylindrical coordinate system that inherently preserves horizontally cyclic\ntopology. On the computation side, COCO-GAN has a built-in divide-and-conquer\nparadigm that reduces memory requisition during training and inference,\nprovides high-parallelism, and can generate parts of images on-demand.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 20:37:24 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 15:55:44 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 05:58:45 GMT"}, {"version": "v4", "created": "Sun, 5 Jan 2020 06:28:59 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Lin", "Chieh Hubert", ""], ["Chang", "Chia-Che", ""], ["Chen", "Yu-Sheng", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1904.00285", "submitter": "Gregor Ehrensperger", "authors": "Gregor Ehrensperger, Sebastian Stabinger, Antonio Rodr\\'iguez\n  S\\'anchez", "title": "Evaluating CNNs on the Gestalt Principle of Closure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are widely known for their\noutstanding performance in classification and regression tasks over\nhigh-dimensional data. This made them a popular and powerful tool for a large\nvariety of applications in industry and academia. Recent publications show that\nseemingly easy classifaction tasks (for humans) can be very challenging for\nstate of the art CNNs. An attempt to describe how humans perceive visual\nelements is given by the Gestalt principles. In this paper we evaluate AlexNet\nand GoogLeNet regarding their performance on classifying the correctness of the\nwell known Kanizsa triangles, which heavily rely on the Gestalt principle of\nclosure. Therefore we created various datasets containing valid as well as\ninvalid variants of the Kanizsa triangle. Our findings suggest that perceiving\nobjects by utilizing the principle of closure is very challenging for the\napplied network architectures but they appear to adapt to the effect of\nclosure.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 20:54:50 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ehrensperger", "Gregor", ""], ["Stabinger", "Sebastian", ""], ["S\u00e1nchez", "Antonio Rodr\u00edguez", ""]]}, {"id": "1904.00291", "submitter": "Zhuoran Dang", "authors": "Zhuoran Dang, Mamoru Ishii", "title": "Two-phase flow regime prediction using LSTM based deep recurrent neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.app-ph physics.data-an physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory (LSTM) and recurrent neural network (RNN) has achieved\ngreat successes on time-series prediction. In this paper, a methodology of\nusing LSTM-based deep-RNN for two-phase flow regime prediction is proposed,\nmotivated by previous research on constructing deep RNN. The method is featured\nwith fast response and accuracy. The built RNN networks are trained and tested\nwith time-series void fraction data collected using impedance void meter. The\nresult shows that the prediction accuracy depends on the depth of network and\nthe number of layer cells. However, deeper and larger network consumes more\ntime in predicting.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 21:23:26 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Dang", "Zhuoran", ""], ["Ishii", "Mamoru", ""]]}, {"id": "1904.00310", "submitter": "Xilai Li", "authors": "Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, Caiming Xiong", "title": "Learn to Grow: A Continual Structure Learning Framework for Overcoming\n  Catastrophic Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing catastrophic forgetting is one of the key challenges in continual\nlearning where machine learning systems are trained with sequential or\nstreaming tasks. Despite recent remarkable progress in state-of-the-art deep\nlearning, deep neural networks (DNNs) are still plagued with the catastrophic\nforgetting problem. This paper presents a conceptually simple yet general and\neffective framework for handling catastrophic forgetting in continual learning\nwith DNNs. The proposed method consists of two components: a neural structure\noptimization component and a parameter learning and/or fine-tuning component.\nBy separating the explicit neural structure learning and the parameter\nestimation, not only is the proposed method capable of evolving neural\nstructures in an intuitively meaningful way, but also shows strong capabilities\nof alleviating catastrophic forgetting in experiments. Furthermore, the\nproposed method outperforms all other baselines on the permuted MNIST dataset,\nthe split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual\nlearning setting.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 00:35:36 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 03:35:25 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 16:36:25 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Li", "Xilai", ""], ["Zhou", "Yingbo", ""], ["Wu", "Tianfu", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "1904.00319", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Yingcai Bi, Gim Hee Lee", "title": "Discrete Rotation Equivariance for Point Cloud Recognition", "comments": "The 2019 International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent active research on processing point clouds with deep\nnetworks, few attention has been on the sensitivity of the networks to\nrotations. In this paper, we propose a deep learning architecture that achieves\ndiscrete $\\mathbf{SO}(2)$/$\\mathbf{SO}(3)$ rotation equivariance for point\ncloud recognition. Specifically, the rotation of an input point cloud with\nelements of a rotation group is similar to shuffling the feature vectors\ngenerated by our approach. The equivariance is easily reduced to invariance by\neliminating the permutation with operations such as maximum or average. Our\nmethod can be directly applied to any existing point cloud based networks,\nresulting in significant improvements in their performance for rotated inputs.\nWe show state-of-the-art results in the classification tasks with various\ndatasets under both $\\mathbf{SO}(2)$ and $\\mathbf{SO}(3)$ rotations. In\naddition, we further analyze the necessary conditions of applying our approach\nto PointNet based networks. Source codes at\nhttps://github.com/lijx10/rot-equ-net\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 01:50:36 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Jiaxin", ""], ["Bi", "Yingcai", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1904.00320", "submitter": "Chen Zhao", "authors": "Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang", "title": "NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR\n  2019) (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature correspondence selection is pivotal to many feature-matching based\ntasks in computer vision. Searching for spatially k-nearest neighbors is a\ncommon strategy for extracting local information in many previous works.\nHowever, there is no guarantee that the spatially k-nearest neighbors of\ncorrespondences are consistent because the spatial distribution of false\ncorrespondences is often irregular. To address this issue, we present a\ncompatibility-specific mining method to search for consistent neighbors.\nMoreover, in order to extract and aggregate more reliable features from\nneighbors, we propose a hierarchical network named NM-Net with a series of\nconvolution layers taking the generated graph as input, which is insensitive to\nthe order of correspondences. Our experimental results have shown the proposed\nmethod achieves the state-of-the-art performance on four datasets with various\ninlier ratios and varying numbers of feature consistencies.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 01:50:37 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhao", "Chen", ""], ["Cao", "Zhiguo", ""], ["Li", "Chi", ""], ["Li", "Xin", ""], ["Yang", "Jiaqi", ""]]}, {"id": "1904.00325", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Liang Yao, Yuan Luo", "title": "ImageGCN: Multi-Relational Image Graph Convolutional Networks for\n  Disease Identification with Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representation is a fundamental task in computer vision. However, most\nof the existing approaches for image representation ignore the relations\nbetween images and consider each input image independently. Intuitively,\nrelations between images can help to understand the images and maintain model\nconsistency over related images. In this paper, we consider modeling the\nimage-level relations to generate more informative image representations, and\npropose ImageGCN, an end-to-end graph convolutional network framework for\nmulti-relational image modeling. We also apply ImageGCN to chest X-ray (CXR)\nimages where rich relational information is available for disease\nidentification. Unlike previous image representation models, ImageGCN learns\nthe representation of an image using both its original pixel features and the\nfeatures of related images. Besides learning informative representations for\nimages, ImageGCN can also be used for object detection in a weakly supervised\nmanner. The Experimental results on ChestX-ray14 dataset demonstrate that\nImageGCN can outperform respective baselines in both disease identification and\nlocalization tasks and can achieve comparable and often better results than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 02:42:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mao", "Chengsheng", ""], ["Yao", "Liang", ""], ["Luo", "Yuan", ""]]}, {"id": "1904.00328", "submitter": "Lin Zhang", "authors": "Lin Zhang", "title": "An Efficient Approach for Cell Segmentation in Phase Contrast Microscopy\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new model to segment cells in phase contrast\nmicroscopy images. Cell images collected from the similar scenario share a\nsimilar background. Inspired by this, we separate cells from the background in\nimages by formulating the problem as a low-rank and structured sparse matrix\ndecomposition problem. Then, we propose the inverse diffraction pattern\nfiltering method to further segment individual cells in the images. This is a\ndeconvolution process that has a much lower computational complexity when\ncompared to the other restoration methods. Experiments demonstrate the\neffectiveness of the proposed model when it is compared with recent works.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 03:01:10 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhang", "Lin", ""]]}, {"id": "1904.00346", "submitter": "Xijun Wang", "authors": "Xijun Wang, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Fully Learnable Group Convolution for Acceleration of Deep Neural\n  Networks", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefitted from its great success on many tasks, deep learning is\nincreasingly used on low-computational-cost devices, e.g. smartphone, embedded\ndevices, etc. To reduce the high computational and memory cost, in this work,\nwe propose a fully learnable group convolution module (FLGC for short) which is\nquite efficient and can be embedded into any deep neural networks for\nacceleration. Specifically, our proposed method automatically learns the group\nstructure in the training stage in a fully end-to-end manner, leading to a\nbetter structure than the existing pre-defined, two-steps, or iterative\nstrategies. Moreover, our method can be further combined with depthwise\nseparable convolution, resulting in 5 times acceleration than the vanilla\nResnet50 on single CPU. An additional advantage is that in our FLGC the number\nof groups can be set as any value, but not necessarily 2^k as in most existing\nmethods, meaning better tradeoff between accuracy and speed. As evaluated in\nour experiments, our method achieves better performance than existing learnable\ngroup convolution and standard group convolution when using the same number of\ngroups.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 06:24:07 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wang", "Xijun", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1904.00352", "submitter": "Jonathan Samuel Lumentut", "authors": "Jonathan Samuel Lumentut and Tae Hyun Kim and Ravi Ramamoorthi and In\n  Kyu Park", "title": "Fast and Full-Resolution Light Field Deblurring using a Deep Neural\n  Network", "comments": "9 pages, 8 figures", "journal-ref": "IEEE Signal Processing Letters, vol. 26, no. 12, pp. 1788-1792,\n  December 2019", "doi": "10.1109/LSP.2019.2947379", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restoring a sharp light field image from its blurry input has become\nessential due to the increasing popularity of parallax-based image processing.\nState-of-the-art blind light field deblurring methods suffer from several\nissues such as slow processing, reduced spatial size, and a limited motion blur\nmodel. In this work, we address these challenging problems by generating a\ncomplex blurry light field dataset and proposing a learning-based deblurring\napproach. In particular, we model the full 6-degree of freedom (6-DOF) light\nfield camera motion, which is used to create the blurry dataset using a\ncombination of real light fields captured with a Lytro Illum camera, and\nsynthetic light field renderings of 3D scenes. Furthermore, we propose a light\nfield deblurring network that is built with the capability of large receptive\nfields. We also introduce a simple strategy of angular sampling to train on the\nlarge-scale blurry light field effectively. We evaluate our method through both\nquantitative and qualitative measurements and demonstrate superior performance\ncompared to the state-of-the-art method with a massive speedup in execution\ntime. Our method is about 16K times faster than Srinivasan et. al. [22] and can\ndeblur a full-resolution light field in less than 2 seconds.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 07:36:27 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lumentut", "Jonathan Samuel", ""], ["Kim", "Tae Hyun", ""], ["Ramamoorthi", "Ravi", ""], ["Park", "In Kyu", ""]]}, {"id": "1904.00355", "submitter": "Zitong Yu", "authors": "Hui Li, Meng Yang, Zhihui Lai, Weishi Zheng, Zitong Yu", "title": "Pedestrian re-identification based on Tree branch network with local and\n  global learning", "comments": "accepted by ICME2019(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep part-based methods in recent literature have revealed the great\npotential of learning local part-level representation for pedestrian image in\nthe task of person re-identification. However, global features that capture\ndiscriminative holistic information of human body are usually ignored or not\nwell exploited. This motivates us to investigate joint learning global and\nlocal features from pedestrian images. Specifically, in this work, we propose a\nnovel framework termed tree branch network (TBN) for person re-identification.\nGiven a pedestrain image, the feature maps generated by the backbone CNN, are\npartitioned recursively into several pieces, each of which is followed by a\nbottleneck structure that learns finer-grained features for each level in the\nhierarchical tree-like framework. In this way, representations are learned in a\ncoarse-to-fine manner and finally assembled to produce more discriminative\nimage descriptions. Experimental results demonstrate the effectiveness of the\nglobal and local feature learning method in the proposed TBN framework. We also\nshow significant improvement in performance over state-of-the-art methods on\nthree public benchmarks: Market-1501, CUHK-03 and DukeMTMC.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 07:51:08 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Hui", ""], ["Yang", "Meng", ""], ["Lai", "Zhihui", ""], ["Zheng", "Weishi", ""], ["Yu", "Zitong", ""]]}, {"id": "1904.00370", "submitter": "Sayna Ebrahimi", "authors": "Samarth Sinha, Sayna Ebrahimi, Trevor Darrell", "title": "Variational Adversarial Active Learning", "comments": "First two authors contributed equally, listed alphabetically.\n  Accepted as Oral at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to develop label-efficient algorithms by sampling the\nmost representative queries to be labeled by an oracle. We describe a\npool-based semi-supervised active learning algorithm that implicitly learns\nthis sampling mechanism in an adversarial manner. Unlike conventional active\nlearning algorithms, our approach is task agnostic, i.e., it does not depend on\nthe performance of the task for which we are trying to acquire labeled data.\nOur method learns a latent space using a variational autoencoder (VAE) and an\nadversarial network trained to discriminate between unlabeled and labeled data.\nThe mini-max game between the VAE and the adversarial network is played such\nthat while the VAE tries to trick the adversarial network into predicting that\nall data points are from the labeled pool, the adversarial network learns how\nto discriminate between dissimilarities in the latent space. We extensively\nevaluate our method on various image classification and semantic segmentation\nbenchmark datasets and establish a new state of the art on\n$\\text{CIFAR10/100}$, $\\text{Caltech-256}$, $\\text{ImageNet}$,\n$\\text{Cityscapes}$, and $\\text{BDD100K}$. Our results demonstrate that our\nadversarial approach learns an effective low dimensional latent space in\nlarge-scale settings and provides for a computationally efficient sampling\nmethod. Our code is available at https://github.com/sinhasam/vaal.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 09:54:17 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 18:48:22 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 18:03:08 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Sinha", "Samarth", ""], ["Ebrahimi", "Sayna", ""], ["Darrell", "Trevor", ""]]}, {"id": "1904.00386", "submitter": "Zhihang Li", "authors": "Zhihang Li, Xu Tang, Junyu Han, Jingtuo Liu, Ran He", "title": "PyramidBox++: High Performance Detector for Finding Tiny Face", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of deep convolutional neural network, face\ndetection has made great progress in recent years. WIDER FACE dataset, as a\nmain benchmark, contributes greatly to this area. A large amount of methods\nhave been put forward where PyramidBox designs an effective data augmentation\nstrategy (Data-anchor-sampling) and context-based module for face detector. In\nthis report, we improve each part to further boost the performance, including\nBalanced-data-anchor-sampling, Dual-PyramidAnchors and Dense Context Module.\nSpecifically, Balanced-data-anchor-sampling obtains more uniform sampling of\nfaces with different sizes. Dual-PyramidAnchors facilitate feature learning by\nintroducing progressive anchor loss. Dense Context Module with dense connection\nnot only enlarges receptive filed, but also passes information efficiently.\nIntegrating these techniques, PyramidBox++ is constructed and achieves\nstate-of-the-art performance in hard set.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 11:44:31 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Zhihang", ""], ["Tang", "Xu", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["He", "Ran", ""]]}, {"id": "1904.00388", "submitter": "Xiaoye Sun", "authors": "Xiaoye Sun, Liyan Ma and Gongyan Li", "title": "Multi-vision Attention Networks for On-line Red Jujube Grading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the red jujube classification problem, this paper designs a\nconvolutional neural network model with low computational cost and high\nclassification accuracy. The architecture of the model is inspired by the\nmulti-visual mechanism of the organism and DenseNet. To further improve our\nmodel, we add the attention mechanism of SE-Net. We also construct a dataset\nwhich contains 23,735 red jujube images captured by a jujube grading system.\nAccording to the appearance of the jujube and the characteristics of the\ngrading system, the dataset is divided into four classes: invalid, rotten,\nwizened and normal. The numerical experiments show that the classification\naccuracy of our model reaches to 91.89%, which is comparable to DenseNet-121,\nInceptionV3, InceptionV4, and Inception-ResNet v2. However, our model has\nreal-time performance.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 11:57:41 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sun", "Xiaoye", ""], ["Ma", "Liyan", ""], ["Li", "Gongyan", ""]]}, {"id": "1904.00415", "submitter": "Shaul Oron", "authors": "Liat Sless, Gilad Cohen, Bat El Shlomo, Shaul Oron", "title": "Road Scene Understanding by Occupancy Grid Learning from Sparse Radar\n  Clusters using Semantic Segmentation", "comments": null, "journal-ref": "ICCV 2019 CVRSUAD", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Occupancy grid mapping is an important component in road scene understanding\nfor autonomous driving. It encapsulates information of the drivable area, road\nobstacles and enables safe autonomous driving. Radars are an emerging sensor in\nautonomous vehicle vision, becoming more widely used due to their long range\nsensing, low cost, and robustness to severe weather conditions. Despite recent\nadvances in deep learning technology, occupancy grid mapping from radar data is\nstill mostly done using classical filtering approaches.In this work, we propose\nlearning the inverse sensor model used for occupancy grid mapping from\nclustered radar data. This is done in a data driven approach that leverages\ncomputer vision techniques. This task is very challenging due to data sparsity\nand noise characteristics of the radar sensor. The problem is formulated as a\nsemantic segmentation task and we show how it can be learned using lidar data\nfor generating ground truth. We show both qualitatively and quantitatively that\nour learned occupancy net outperforms classic methods by a large margin using\nthe recently released NuScenes real-world driving data.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 14:04:16 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 18:35:09 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sless", "Liat", ""], ["Cohen", "Gilad", ""], ["Shlomo", "Bat El", ""], ["Oron", "Shaul", ""]]}, {"id": "1904.00420", "submitter": "Zichao Guo", "authors": "Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen\n  Wei, Jian Sun", "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze\nits advantages over existing NAS approaches. Existing one-shot method, however,\nis hard to train and not yet effective on large scale datasets like ImageNet.\nThis work propose a Single Path One-Shot model to address the challenge in the\ntraining. Our central idea is to construct a simplified supernet, where all\narchitectures are single paths so that weight co-adaption problem is\nalleviated. Training is performed by uniform path sampling. All architectures\n(and their weights) are trained fully and equally.\n  Comprehensive experiments verify that our approach is flexible and effective.\nIt is easy to train and fast to search. It effortlessly supports complex search\nspaces (e.g., building blocks, channel, mixed-precision quantization) and\ndifferent search constraints (e.g., FLOPs, latency). It is thus convenient to\nuse for various needs. It achieves start-of-the-art performance on the large\ndataset ImageNet.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 14:34:43 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 06:52:29 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 17:08:55 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 10:55:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Guo", "Zichao", ""], ["Zhang", "Xiangyu", ""], ["Mu", "Haoyuan", ""], ["Heng", "Wen", ""], ["Liu", "Zechun", ""], ["Wei", "Yichen", ""], ["Sun", "Jian", ""]]}, {"id": "1904.00523", "submitter": "Jianrui Cai", "authors": "Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, Lei Zhang", "title": "Toward Real-World Single Image Super-Resolution: A New Benchmark and A\n  New Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing learning-based single image superresolution (SISR)\nmethods are trained and evaluated on simulated datasets, where the\nlow-resolution (LR) images are generated by applying a simple and uniform\ndegradation (i.e., bicubic downsampling) to their high-resolution (HR)\ncounterparts. However, the degradations in real-world LR images are far more\ncomplicated. As a consequence, the SISR models trained on simulated data become\nless effective when applied to practical scenarios. In this paper, we build a\nreal-world super-resolution (RealSR) dataset where paired LR-HR images on the\nsame scene are captured by adjusting the focal length of a digital camera. An\nimage registration algorithm is developed to progressively align the image\npairs at different resolutions. Considering that the degradation kernels are\nnaturally non-uniform in our dataset, we present a Laplacian pyramid based\nkernel prediction network (LP-KPN), which efficiently learns per-pixel kernels\nto recover the HR image. Our extensive experiments demonstrate that SISR models\ntrained on our RealSR dataset deliver better visual quality with sharper edges\nand finer textures on real-world scenes than those trained on simulated\ndatasets. Though our RealSR dataset is built by using only two cameras (Canon\n5D3 and Nikon D810), the trained model generalizes well to other camera devices\nsuch as Sony a7II and mobile phones.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 01:14:23 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Cai", "Jianrui", ""], ["Zeng", "Hui", ""], ["Yong", "Hongwei", ""], ["Cao", "Zisheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1904.00537", "submitter": "Sun Yifan", "authors": "Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, Jian\n  Sun", "title": "Perceive Where to Focus: Learning Visibility-aware Part-level Features\n  for Partial Person Re-identification", "comments": "8 pages, 5 figures, accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a realistic problem in person re-identification (re-ID)\ntask, i.e., partial re-ID. Under partial re-ID scenario, the images may contain\na partial observation of a pedestrian. If we directly compare a partial\npedestrian image with a holistic one, the extreme spatial misalignment\nsignificantly compromises the discriminative ability of the learned\nrepresentation. We propose a Visibility-aware Part Model (VPM), which learns to\nperceive the visibility of regions through self-supervision. The visibility\nawareness allows VPM to extract region-level features and compare two images\nwith focus on their shared regions (which are visible on both images). VPM\ngains two-fold benefit toward higher accuracy for partial re-ID. On the one\nhand, compared with learning a global feature, VPM learns region-level features\nand benefits from fine-grained information. On the other hand, with visibility\nawareness, VPM is capable to estimate the shared regions between two images and\nthus suppresses the spatial misalignment. Experimental results confirm that our\nmethod significantly improves the learned representation and the achieved\naccuracy is on par with the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 02:14:25 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sun", "Yifan", ""], ["Xu", "Qin", ""], ["Li", "Yali", ""], ["Zhang", "Chi", ""], ["Li", "Yikang", ""], ["Wang", "Shengjin", ""], ["Sun", "Jian", ""]]}, {"id": "1904.00540", "submitter": "Mehrdad Shoeiby", "authors": "Mehrdad Shoeiby, Antonio Robles-Kelly, Ran Wei, Radu Timofte", "title": "PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a newly collected and novel dataset (StereoMSI) for\nexample-based single and colour-guided spectral image super-resolution. The\ndataset was first released and promoted during the PIRM2018 spectral image\nsuper-resolution challenge. To the best of our knowledge, the dataset is the\nfirst of its kind, comprising 350 registered colour-spectral image pairs. The\ndataset has been used for the two tracks of the challenge and, for each of\nthese, we have provided a split into training, validation and testing. This\narrangement is a result of the challenge structure and phases, with the first\ntrack focusing on example-based spectral image super-resolution and the second\none aiming at exploiting the registered stereo colour imagery to improve the\nresolution of the spectral images. Each of the tracks and splits has been\nselected to be consistent across a number of image quality metrics. The dataset\nis quite general in nature and can be used for a wide variety of applications\nin addition to the development of spectral image super-resolution methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 02:41:08 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 07:12:43 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Shoeiby", "Mehrdad", ""], ["Robles-Kelly", "Antonio", ""], ["Wei", "Ran", ""], ["Timofte", "Radu", ""]]}, {"id": "1904.00551", "submitter": "Xiaoyan Li", "authors": "Xiaoyan Li, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Weakly Supervised Object Detection with Segmentation Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection aims at learning precise object detectors,\ngiven image category labels. In recent prevailing works, this problem is\ngenerally formulated as a multiple instance learning module guided by an image\nclassification loss. The object bounding box is assumed to be the one\ncontributing most to the classification among all proposals. However, the\nregion contributing most is also likely to be a crucial part or the supporting\ncontext of an object. To obtain a more accurate detector, in this work we\npropose a novel end-to-end weakly supervised detection approach, where a newly\nintroduced generative adversarial segmentation module interacts with the\nconventional detection module in a collaborative loop. The collaboration\nmechanism takes full advantages of the complementary interpretations of the\nweakly supervised localization task, namely detection and segmentation tasks,\nforming a more comprehensive solution. Consequently, our method obtains more\nprecise object bounding boxes, rather than parts or irrelevant surroundings.\nExpectedly, the proposed method achieves an accuracy of 51.0% on the PASCAL VOC\n2007 dataset, outperforming the state-of-the-arts and demonstrating its\nsuperiority for weakly supervised object detection.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 03:53:49 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Xiaoyan", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1904.00558", "submitter": "Yuki Fujimura", "authors": "Yuki Fujimura and Motoharu Sonogashira and Masaaki Iiyama", "title": "Defogging Kinect: Simultaneous Estimation of Object Region and Depth in\n  Foggy Scenes", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) reconstruction and scene depth estimation from\n2-dimensional (2D) images are major tasks in computer vision. However, using\nconventional 3D reconstruction techniques gets challenging in participating\nmedia such as murky water, fog, or smoke. We have developed a method that uses\na time-of-flight (ToF) camera to estimate an object region and depth in\nparticipating media simultaneously. The scattering component is saturated, so\nit does not depend on the scene depth, and received signals bouncing off\ndistant points are negligible due to light attenuation in the participating\nmedia, so the observation of such a point contains only a scattering component.\nThese phenomena enable us to estimate the scattering component in an object\nregion from a background that only contains the scattering component. The\nproblem is formulated as robust estimation where the object region is regarded\nas outliers, and it enables the simultaneous estimation of an object region and\ndepth on the basis of an iteratively reweighted least squares (IRLS)\noptimization scheme. We demonstrate the effectiveness of the proposed method\nusing captured images from a Kinect v2 in real foggy scenes and evaluate the\napplicability with synthesized data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:35:01 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Fujimura", "Yuki", ""], ["Sonogashira", "Motoharu", ""], ["Iiyama", "Masaaki", ""]]}, {"id": "1904.00560", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu and Handong Zhao and Zhe Lin and Sheng Li and Jianfei Cai\n  and Mingyang Ling", "title": "Scene Graph Generation with External Knowledge and Image Reconstruction", "comments": "10 pages, 5 figures, Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation has received growing attention with the advancements\nin image understanding tasks such as object detection, attributes and\nrelationship prediction,~\\etc. However, existing datasets are biased in terms\nof object and relationship labels, or often come with noisy and missing\nannotations, which makes the development of a reliable scene graph prediction\nmodel very challenging. In this paper, we propose a novel scene graph\ngeneration algorithm with external knowledge and image reconstruction loss to\novercome these dataset issues. In particular, we extract commonsense knowledge\nfrom the external knowledge base to refine object and phrase features for\nimproving generalizability in scene graph generation. To address the bias of\nnoisy object annotations, we introduce an auxiliary image reconstruction path\nto regularize the scene graph generation network. Extensive experiments show\nthat our framework can generate better scene graphs, achieving the\nstate-of-the-art performance on two benchmark datasets: Visual Relationship\nDetection and Visual Genome datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:37:35 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Zhao", "Handong", ""], ["Lin", "Zhe", ""], ["Li", "Sheng", ""], ["Cai", "Jianfei", ""], ["Ling", "Mingyang", ""]]}, {"id": "1904.00566", "submitter": "Yu Zeng", "authors": "Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang, Mingyang Qian, Yizhou\n  Yu", "title": "Multi-source weak supervision for saliency detection", "comments": "cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high cost of pixel-level annotations makes it appealing to train saliency\ndetection models with weak supervision. However, a single weak supervision\nsource usually does not contain enough information to train a well-performing\nmodel. To this end, we propose a unified framework to train saliency detection\nmodels with diverse weak supervision sources. In this paper, we use category\nlabels, captions, and unlabelled data for training, yet other supervision\nsources can also be plugged into this flexible framework. We design a\nclassification network (CNet) and a caption generation network (PNet), which\nlearn to predict object categories and generate captions, respectively,\nmeanwhile highlight the most important regions for corresponding tasks. An\nattention transfer loss is designed to transmit supervision signal between\nnetworks, such that the network designed to be trained with one supervision\nsource can benefit from another. An attention coherence loss is defined on\nunlabelled data to encourage the networks to detect generally salient regions\ninstead of task-specific regions. We use CNet and PNet to generate pixel-level\npseudo labels to train a saliency prediction network (SNet). During the testing\nphases, we only need SNet to predict saliency maps. Experiments demonstrate the\nperformance of our method compares favourably against unsupervised and weakly\nsupervised methods and even some supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 05:19:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zeng", "Yu", ""], ["Zhuge", "Yunzhi", ""], ["Lu", "Huchuan", ""], ["Zhang", "Lihe", ""], ["Qian", "Mingyang", ""], ["Yu", "Yizhou", ""]]}, {"id": "1904.00579", "submitter": "Hossein Soleimani", "authors": "Mohsen Ahmadi and Hossein Soleimani", "title": "Palmprint image registration using convolutional neural networks and\n  Hough transform", "comments": "6 figures, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minutia-based palmprint recognition systems has got lots of interest in last\ntwo decades. Due to the large number of minutiae in a palmprint, approximately\n1000 minutiae, the matching process is time consuming which makes it\nunpractical for real time applications. One way to address this issue is\naligning all palmprint images to a reference image and bringing them to a same\ncoordinate system. Bringing all palmprint images to a same coordinate system,\nresults in fewer computations during minutia matching. In this paper, using\nconvolutional neural network (CNN) and generalized Hough transform (GHT), we\npropose a new method to register palmprint images accurately. This method,\nfinds the corresponding rotation and displacement (in both x and y direction)\nbetween the palmprint and a reference image. Exact palmprint registration can\nenhance the speed and the accuracy of matching process. Proposed method is\ncapable of distinguishing between left and right palmprint automatically which\nhelps to speed up the matching process. Furthermore, designed structure of CNN\nin registration stage, gives us the segmented palmprint image from background\nwhich is a pre-processing step for minutia extraction. The proposed\nregistration method followed by minutia-cylinder code (MCC) matching algorithm\nhas been evaluated on the THUPALMLAB database, and the results show the\nsuperiority of our algorithm over most of the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 06:27:10 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 03:20:47 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ahmadi", "Mohsen", ""], ["Soleimani", "Hossein", ""]]}, {"id": "1904.00592", "submitter": "Foivos Diakogiannis", "authors": "Foivos I. Diakogiannis, Fran\\c{c}ois Waldner, Peter Caccetta, Chen Wu", "title": "ResUNet-a: a deep learning framework for semantic segmentation of\n  remotely sensed data", "comments": "Accepted for publication to the ISPRS Journal of Photogrammetry and\n  Remote Sensing", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2020.01.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding of high resolution aerial images is of great importance\nfor the task of automated monitoring in various remote sensing applications.\nDue to the large within-class and small between-class variance in pixel values\nof objects of interest, this remains a challenging task. In recent years, deep\nconvolutional neural networks have started being used in remote sensing\napplications and demonstrate state of the art performance for pixel level\nclassification of objects. \\textcolor{black}{Here we propose a reliable\nframework for performant results for the task of semantic segmentation of\nmonotemporal very high resolution aerial images. Our framework consists of a\nnovel deep learning architecture, ResUNet-a, and a novel loss function based on\nthe Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination\nwith residual connections, atrous convolutions, pyramid scene parsing pooling\nand multi-tasking inference. ResUNet-a infers sequentially the boundary of the\nobjects, the distance transform of the segmentation mask, the segmentation mask\nand a colored reconstruction of the input. Each of the tasks is conditioned on\nthe inference of the previous ones, thus establishing a conditioned\nrelationship between the various tasks, as this is described through the\narchitecture's computation graph. We analyse the performance of several\nflavours of the Generalized Dice loss for semantic segmentation, and we\nintroduce a novel variant loss function for semantic segmentation of objects\nthat has excellent convergence properties and behaves well even under the\npresence of highly imbalanced classes.} The performance of our modeling\nframework is evaluated on the ISPRS 2D Potsdam dataset. Results show\nstate-of-the-art performance with an average F1 score of 92.9\\% over all\nclasses for our best model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 06:54:29 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 10:00:22 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 09:20:24 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Diakogiannis", "Foivos I.", ""], ["Waldner", "Fran\u00e7ois", ""], ["Caccetta", "Peter", ""], ["Wu", "Chen", ""]]}, {"id": "1904.00597", "submitter": "Runzhong Wang", "authors": "Runzhong Wang, Junchi Yan, Xiaokang Yang", "title": "Learning Combinatorial Embedding Networks for Deep Graph Matching", "comments": "ICCV2019 oral. Code available at\n  https://github.com/Thinklab-SJTU/PCA-GM", "journal-ref": null, "doi": "10.1109/ICCV.2019.00315", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching refers to finding node correspondence between graphs, such\nthat the corresponding node and edge's affinity can be maximized. In addition\nwith its NP-completeness nature, another important challenge is effective\nmodeling of the node-wise and structure-wise affinity across graphs and the\nresulting objective, to guide the matching procedure effectively finding the\ntrue matching against noises. To this end, this paper devises an end-to-end\ndifferentiable deep network pipeline to learn the affinity for graph matching.\nIt involves a supervised permutation loss regarding with node correspondence to\ncapture the combinatorial nature for graph matching. Meanwhile deep graph\nembedding models are adopted to parameterize both intra-graph and cross-graph\naffinity functions, instead of the traditional shallow and simple parametric\nforms e.g. a Gaussian kernel. The embedding can also effectively capture the\nhigher-order structure beyond second-order edges. The permutation loss model is\nagnostic to the number of nodes, and the embedding model is shared among nodes\nsuch that the network allows for varying numbers of nodes in graphs for\ntraining and inference. Moreover, our network is class-agnostic with some\ngeneralization capability across different categories. All these features are\nwelcomed for real-world applications. Experiments show its superiority against\nstate-of-the-art graph matching learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:01:15 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:10:21 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 14:35:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Runzhong", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1904.00605", "submitter": "Woojeoung Nam", "authors": "Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf and Seong-Whan Lee", "title": "Relative Attributing Propagation: Interpreting the Comparative\n  Contributions of Individual Units in Deep Neural Networks", "comments": "8 pages, 7 figures, Accepted paper in AAAI Conference on Artificial\n  Intelligence (AAAI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Deep Neural Networks (DNNs) have demonstrated superhuman performance in a\nvariety of fields, there is an increasing interest in understanding the complex\ninternal mechanisms of DNNs. In this paper, we propose Relative Attributing\nPropagation (RAP), which decomposes the output predictions of DNNs with a new\nperspective of separating the relevant (positive) and irrelevant (negative)\nattributions according to the relative influence between the layers. The\nrelevance of each neuron is identified with respect to its degree of\ncontribution, separated into positive and negative, while preserving the\nconservation rule. Considering the relevance assigned to neurons in terms of\nrelative priority, RAP allows each neuron to be assigned with a bi-polar\nimportance score concerning the output: from highly relevant to highly\nirrelevant. Therefore, our method makes it possible to interpret DNNs with much\nclearer and attentive visualizations of the separated attributions than the\nconventional explaining methods. To verify that the attributions propagated by\nRAP correctly account for each meaning, we utilize the evaluation metrics: (i)\nOutside-inside relevance ratio, (ii) Segmentation mIOU and (iii) Region\nperturbation. In all experiments and metrics, we present a sizable gap in\ncomparison to the existing literature. Our source code is available in\n\\url{https://github.com/wjNam/Relative_Attributing_Propagation}.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:24:35 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 17:40:00 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 14:28:54 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 07:27:10 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Nam", "Woo-Jeoung", ""], ["Gur", "Shir", ""], ["Choi", "Jaesik", ""], ["Wolf", "Lior", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "1904.00607", "submitter": "Seoung Wug Oh", "authors": "Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim", "title": "Video Object Segmentation using Space-Time Memory Networks", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel solution for semi-supervised video object segmentation. By\nthe nature of the problem, available cues (e.g. video frame(s) with object\nmasks) become richer with the intermediate predictions. However, the existing\nmethods are unable to fully exploit this rich source of information. We resolve\nthe issue by leveraging memory networks and learn to read relevant information\nfrom all available sources. In our framework, the past frames with object masks\nform an external memory, and the current frame as the query is segmented using\nthe mask information in the memory. Specifically, the query and the memory are\ndensely matched in the feature space, covering all the space-time pixel\nlocations in a feed-forward fashion. Contrast to the previous approaches, the\nabundant use of the guidance information allows us to better handle the\nchallenges such as appearance changes and occlussions. We validate our method\non the latest benchmark sets and achieved the state-of-the-art performance\n(overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS\n2016/2017 val set respectively) while having a fast runtime (0.16 second/frame\non DAVIS 2016 val set).\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:27:24 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 07:19:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Oh", "Seoung Wug", ""], ["Lee", "Joon-Young", ""], ["Xu", "Ning", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1904.00623", "submitter": "Yu-Jung Heo", "authors": "Yu-Jung Heo, Kyoung-Woon On, Seongho Choi, Jaeseo Lim, Jinah Kim,\n  Jeh-Kwang Ryu, Byung-Chull Bae and Byoung-Tak Zhang", "title": "Constructing Hierarchical Q&A Datasets for Video Story Understanding", "comments": "Accepted to AAAI 2019 Spring Symposium Series : Story-Enabled\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is emerging as a new paradigm for studying human-like AI.\nQuestion-and-Answering (Q&A) is used as a general benchmark to measure the\nlevel of intelligence for video understanding. While several previous studies\nhave suggested datasets for video Q&A tasks, they did not really incorporate\nstory-level understanding, resulting in highly-biased and lack of variance in\ndegree of question difficulty. In this paper, we propose a hierarchical method\nfor building Q&A datasets, i.e. hierarchical difficulty levels. We introduce\nthree criteria for video story understanding, i.e. memory capacity, logical\ncomplexity, and DIKW (Data-Information-Knowledge-Wisdom) pyramid. We discuss\nhow three-dimensional map constructed from these criteria can be used as a\nmetric for evaluating the levels of intelligence relating to video story\nunderstanding.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:05:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Heo", "Yu-Jung", ""], ["On", "Kyoung-Woon", ""], ["Choi", "Seongho", ""], ["Lim", "Jaeseo", ""], ["Kim", "Jinah", ""], ["Ryu", "Jeh-Kwang", ""], ["Bae", "Byung-Chull", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1904.00625", "submitter": "Sihong Chen", "authors": "Sihong Chen, Kai Ma, Yefeng Zheng", "title": "Med3D: Transfer Learning for 3D Medical Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance on deep learning is significantly affected by volume of\ntraining data. Models pre-trained from massive dataset such as ImageNet become\na powerful weapon for speeding up training convergence and improving accuracy.\nSimilarly, models based on large dataset are important for the development of\ndeep learning in 3D medical images. However, it is extremely challenging to\nbuild a sufficiently large dataset due to difficulty of data acquisition and\nannotation in 3D medical imaging. We aggregate the dataset from several medical\nchallenges to build 3DSeg-8 dataset with diverse modalities, target organs, and\npathologies. To extract general medical three-dimension (3D) features, we\ndesign a heterogeneous 3D network called Med3D to co-train multi-domain 3DSeg-8\nso as to make a series of pre-trained models. We transfer Med3D pre-trained\nmodels to lung segmentation in LIDC dataset, pulmonary nodule classification in\nLIDC dataset and liver segmentation on LiTS challenge. Experiments show that\nthe Med3D can accelerate the training convergence speed of target 3D medical\ntasks 2 times compared with model pre-trained on Kinetics dataset, and 10 times\ncompared with training from scratch as well as improve accuracy ranging from 3%\nto 20%. Transferring our Med3D model on state-the-of-art DenseASPP segmentation\nnetwork, in case of single model, we achieve 94.6\\% Dice coefficient which\napproaches the result of top-ranged algorithms on the LiTS challenge.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:14:29 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 01:57:57 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 06:41:44 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 10:19:12 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Chen", "Sihong", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1904.00631", "submitter": "Sihong Chen", "authors": "Sihong Chen, Kai Ma, Yefeng Zheng", "title": "TAN: Temporal Affine Network for Real-Time Left Ventricle Anatomical\n  Structure Analysis Based on 2D Ultrasound Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With superiorities on low cost, portability, and free of radiation,\nechocardiogram is a widely used imaging modality for left ventricle (LV)\nfunction quantification. However, automatic LV segmentation and motion tracking\nis still a challenging task. In addition to fuzzy border definition, low\ncontrast, and abounding artifacts on typical ultrasound images, the shape and\nsize of the LV change significantly in a cardiac cycle. In this work, we\npropose a temporal affine network (TAN) to perform image analysis in a warped\nimage space, where the shape and size variations due to the cardiac motion as\nwell as other artifacts are largely compensated. Furthermore, we perform three\nfrequent echocardiogram interpretation tasks simultaneously: standard cardiac\nplane recognition, LV landmark detection, and LV segmentation. Instead of using\nthree networks with one dedicating to each task, we use a multi-task network to\nperform three tasks simultaneously. Since three tasks share the same encoder,\nthe compact network improves the segmentation accuracy with more supervision.\nThe network is further finetuned with optical flow adjusted annotations to\nenhance motion coherence in the segmentation result. Experiments on 1,714 2D\nechocardiographic sequences demonstrate that the proposed method achieves\nstate-of-the-art segmentation accuracy with real-time efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:23:32 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Sihong", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1904.00634", "submitter": "Wei Wang", "authors": "Wei Wang, Ruiming Guo, Yapeng Tian, Wenming Yang", "title": "CFSNet: Toward a Controllable Feature Space for Image Restoration", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have witnessed the great progress in image restoration\nwith specific metrics (e.g., PSNR, SSIM). However, the perceptual quality of\nthe restored image is relatively subjective, and it is necessary for users to\ncontrol the reconstruction result according to personal preferences or image\ncharacteristics, which cannot be done using existing deterministic networks.\nThis motivates us to exquisitely design a unified interactive framework for\ngeneral image restoration tasks. Under this framework, users can control\ncontinuous transition of different objectives, e.g., the perception-distortion\ntrade-off of image super-resolution, the trade-off between noise reduction and\ndetail preservation. We achieve this goal by controlling the latent features of\nthe designed network. To be specific, our proposed framework, named\nControllable Feature Space Network (CFSNet), is entangled by two branches based\non different objectives. Our framework can adaptively learn the coupling\ncoefficients of different layers and channels, which provides finer control of\nthe restored image quality. Experiments on several typical image restoration\ntasks fully validate the effective benefits of the proposed method. Code is\navailable at https://github.com/qibao77/CFSNet.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:27:05 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 02:34:43 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Wang", "Wei", ""], ["Guo", "Ruiming", ""], ["Tian", "Yapeng", ""], ["Yang", "Wenming", ""]]}, {"id": "1904.00637", "submitter": "Kaixuan Wei", "authors": "Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, Hua Huang", "title": "Single Image Reflection Removal Exploiting Misaligned Training Data and\n  Network Enhancements", "comments": "Accepted to CVPR2019; code is available at\n  https://github.com/Vandermode/ERRNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing undesirable reflections from a single image captured through a glass\nwindow is of practical importance to visual computing systems. Although\nstate-of-the-art methods can obtain decent results in certain situations,\nperformance declines significantly when tackling more general real-world cases.\nThese failures stem from the intrinsic difficulty of single image reflection\nremoval -- the fundamental ill-posedness of the problem, and the insufficiency\nof densely-labeled training data needed for resolving this ambiguity within\nlearning-based neural network pipelines. In this paper, we address these issues\nby exploiting targeted network enhancements and the novel use of misaligned\ndata. For the former, we augment a baseline network architecture by embedding\ncontext encoding modules that are capable of leveraging high-level contextual\nclues to reduce indeterminacy within areas containing strong reflections. For\nthe latter, we introduce an alignment-invariant loss function that facilitates\nexploiting misaligned real-world training data that is much easier to collect.\nExperimental results collectively show that our method outperforms the\nstate-of-the-art with aligned data, and that significant improvements are\npossible when using additional misaligned data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:38:37 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wei", "Kaixuan", ""], ["Yang", "Jiaolong", ""], ["Fu", "Ying", ""], ["Wipf", "David", ""], ["Huang", "Hua", ""]]}, {"id": "1904.00641", "submitter": "Kan Wu", "authors": "Kan Wu, Guanbin Li, Haofeng Li, Jianjun Zhang, Yizhou Yu", "title": "Harvesting Visual Objects from Internet Images via Deep Learning Based\n  Objectness Assessment", "comments": "Accepted by ACM Transactions on Multimedia Computing, Communications\n  and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection of internet images has been growing in an astonishing speed.\nIt is undoubted that these images contain rich visual information that can be\nuseful in many applications, such as visual media creation and data-driven\nimage synthesis. In this paper, we focus on the methodologies for building a\nvisual object database from a collection of internet images. Such database is\nbuilt to contain a large number of high-quality visual objects that can help\nwith various data-driven image applications. Our method is based on dense\nproposal generation and objectness-based re-ranking. A novel deep convolutional\nneural network is designed for the inference of proposal objectness, the\nprobability of a proposal containing optimally-located foreground object. In\nour work, the objectness is quantitatively measured in regard of completeness\nand fullness, reflecting two complementary features of an optimal proposal: a\ncomplete foreground and relatively small background. Our experiments indicate\nthat object proposals re-ranked according to the output of our network\ngenerally achieve higher performance than those produced by other\nstate-of-the-art methods. As a concrete example, a database of over 1.2 million\nvisual objects has been built using the proposed method, and has been\nsuccessfully used in various data-driven image applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:56:00 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wu", "Kan", ""], ["Li", "Guanbin", ""], ["Li", "Haofeng", ""], ["Zhang", "Jianjun", ""], ["Yu", "Yizhou", ""]]}, {"id": "1904.00649", "submitter": "Domen Tabernik", "authors": "Domen Tabernik and Danijel Sko\\v{c}aj", "title": "Deep Learning for Large-Scale Traffic-Sign Detection and Recognition", "comments": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection and recognition of traffic signs plays a crucial role in\nmanagement of the traffic-sign inventory. It provides accurate and timely way\nto manage traffic-sign inventory with a minimal human effort. In the computer\nvision community the recognition and detection of traffic signs is a\nwell-researched problem. A vast majority of existing approaches perform well on\ntraffic signs needed for advanced drivers-assistance and autonomous systems.\nHowever, this represents a relatively small number of all traffic signs (around\n50 categories out of several hundred) and performance on the remaining set of\ntraffic signs, which are required to eliminate the manual labor in traffic-sign\ninventory management, remains an open question. In this paper, we address the\nissue of detecting and recognizing a large number of traffic-sign categories\nsuitable for automating traffic-sign inventory management. We adopt a\nconvolutional neural network (CNN) approach, the Mask R-CNN, to address the\nfull pipeline of detection and recognition with automatic end-to-end learning.\nWe propose several improvements that are evaluated on the detection of traffic\nsigns and result in an improved overall performance. This approach is applied\nto detection of 200 traffic-sign categories represented in our novel dataset.\nResults are reported on highly challenging traffic-sign categories that have\nnot yet been considered in previous works. We provide comprehensive analysis of\nthe deep learning method for the detection of traffic signs with large\nintra-category appearance variation and show below 3% error rates with the\nproposed approach, which is sufficient for deployment in practical applications\nof traffic-sign inventory management.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:10:16 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Tabernik", "Domen", ""], ["Sko\u010daj", "Danijel", ""]]}, {"id": "1904.00664", "submitter": "Mu Li", "authors": "Mu Li, Wangmeng Zuo, Shuhang Gu, Jane You, David Zhang", "title": "Learning Content-Weighted Deep Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based lossy image compression usually involves the joint\noptimization of rate-distortion performance. Most existing methods adopt\nspatially invariant bit length allocation and incorporate discrete entropy\napproximation to constrain compression rate. Nonetheless, the information\ncontent is spatially variant, where the regions with complex and salient\nstructures generally are more essential to image compression. Taking the\nspatial variation of image content into account, this paper presents a\ncontent-weighted encoder-decoder model, which involves an importance map subnet\nto produce the importance mask for locally adaptive bit rate allocation.\nConsequently, the summation of importance mask can thus be utilized as an\nalternative of entropy estimation for compression rate control. Furthermore,\nthe quantized representations of the learned code and importance map are still\nspatially dependent, which can be losslessly compressed using arithmetic\ncoding. To compress the codes effectively and efficiently, we propose a trimmed\nconvolutional network to predict the conditional probability of quantized\ncodes. Experiments show that the proposed method can produce visually much\nbetter results, and performs favorably in comparison with deep and traditional\nlossy image compression approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:40:37 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Mu", ""], ["Zuo", "Wangmeng", ""], ["Gu", "Shuhang", ""], ["You", "Jane", ""], ["Zhang", "David", ""]]}, {"id": "1904.00674", "submitter": "Waqas Sultani", "authors": "Anza Shakeel, Waqas Sultani, Mohsen Ali", "title": "Deep Built-Structure Counting in Satellite Imagery Using Attention Based\n  Re-Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to address the challenging problem of counting\nbuilt-structures in the satellite imagery. Building density is a more accurate\nestimate of the population density, urban area expansion and its impact on the\nenvironment, than the built-up area segmentation. However, building shape\nvariances, overlapping boundaries, and variant densities make this a complex\ntask. To tackle this difficult problem, we propose a deep learning based\nregression technique for counting built-structures in satellite imagery. Our\nproposed framework intelligently combines features from different regions of\nsatellite image using attention based re-weighting techniques. Multiple\nparallel convolutional networks are designed to capture information at\ndifferent granulates. These features are combined into the FusionNet which is\ntrained to weigh features from different granularity differently, allowing us\nto predict a precise building count. To train and evaluate the proposed method,\nwe put forward a new large-scale and challenging built-structure-count dataset.\nOur dataset is constructed by collecting satellite imagery from diverse\ngeographical areas (planes, urban centers, deserts, etc.,) across the globe\n(Asia, Europe, North America, and Africa) and captures the wide density of\nbuilt structures. Detailed experimental results and analysis validate the\nproposed technique. FusionNet has Mean Absolute Error of 3.65 and R-squared\nmeasure of 88% over the testing data. Finally, we perform the test on the 274:3\n? 103 m2 of the unseen region, with the error of 19 buildings off the 656\nbuildings in that area.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:57:08 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Shakeel", "Anza", ""], ["Sultani", "Waqas", ""], ["Ali", "Mohsen", ""]]}, {"id": "1904.00680", "submitter": "Seonghyeon Nam", "authors": "Seonghyeon Nam, Chongyang Ma, Menglei Chai, William Brendel, Ning Xu,\n  Seon Joo Kim", "title": "End-to-End Time-Lapse Video Synthesis from a Single Outdoor Image", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-lapse videos usually contain visually appealing content but are often\ndifficult and costly to create. In this paper, we present an end-to-end\nsolution to synthesize a time-lapse video from a single outdoor image using\ndeep neural networks. Our key idea is to train a conditional generative\nadversarial network based on existing datasets of time-lapse videos and image\nsequences. We propose a multi-frame joint conditional generation framework to\neffectively learn the correlation between the illumination change of an outdoor\nscene and the time of the day. We further present a multi-domain training\nscheme for robust training of our generative models from two datasets with\ndifferent distributions and missing timestamp labels. Compared to alternative\ntime-lapse video synthesis algorithms, our method uses the timestamp as the\ncontrol variable and does not require a reference video to guide the synthesis\nof the final output. We conduct ablation studies to validate our algorithm and\ncompare with state-of-the-art techniques both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:13:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Nam", "Seonghyeon", ""], ["Ma", "Chongyang", ""], ["Chai", "Menglei", ""], ["Brendel", "William", ""], ["Xu", "Ning", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1904.00682", "submitter": "Hugo Kuijf", "authors": "Hugo J. Kuijf, J. Matthijs Biesbroek, Jeroen de Bresser, Rutger\n  Heinen, Simon Andermatt, Mariana Bento, Matt Berseth, Mikhail Belyaev, M.\n  Jorge Cardoso, Adri\\`a Casamitjana, D. Louis Collins, Mahsa Dadar, Achilleas\n  Georgiou, Mohsen Ghafoorian, Dakai Jin, April Khademi, Jesse Knight, Hongwei\n  Li, Xavier Llad\\'o, Miguel Luna, Qaiser Mahmood, Richard McKinley, Alireza\n  Mehrtash, S\\'ebastien Ourselin, Bo-yong Park, Hyunjin Park, Sang Hyun Park,\n  Simon Pezold, Elodie Puybareau, Leticia Rittner, Carole H. Sudre, Sergi\n  Valverde, Ver\\'onica Vilaplana, Roland Wiest, Yongchao Xu, Ziyue Xu, Guodong\n  Zeng, Jianguo Zhang, Guoyan Zheng, Christopher Chen, Wiesje van der Flier,\n  Frederik Barkhof, Max A. Viergever, Geert Jan Biessels", "title": "Standardized Assessment of Automatic Segmentation of White Matter\n  Hyperintensities and Results of the WMH Segmentation Challenge", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2905770", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of cerebral white matter hyperintensities (WMH) of presumed\nvascular origin is of key importance in many neurological research studies.\nCurrently, measurements are often still obtained from manual segmentations on\nbrain MR images, which is a laborious procedure. Automatic WMH segmentation\nmethods exist, but a standardized comparison of the performance of such methods\nis lacking. We organized a scientific challenge, in which developers could\nevaluate their method on a standardized multi-center/-scanner image dataset,\ngiving an objective comparison: the WMH Segmentation Challenge\n(https://wmh.isi.uu.nl/).\n  Sixty T1+FLAIR images from three MR scanners were released with manual WMH\nsegmentations for training. A test set of 110 images from five MR scanners was\nused for evaluation. Segmentation methods had to be containerized and submitted\nto the challenge organizers. Five evaluation metrics were used to rank the\nmethods: (1) Dice similarity coefficient, (2) modified Hausdorff distance (95th\npercentile), (3) absolute log-transformed volume difference, (4) sensitivity\nfor detecting individual lesions, and (5) F1-score for individual lesions.\nAdditionally, methods were ranked on their inter-scanner robustness.\n  Twenty participants submitted their method for evaluation. This paper\nprovides a detailed analysis of the results. In brief, there is a cluster of\nfour methods that rank significantly better than the other methods, with one\nclear winner. The inter-scanner robustness ranking shows that not all methods\ngeneralize to unseen scanners.\n  The challenge remains open for future submissions and provides a public\nplatform for method evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:16:02 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kuijf", "Hugo J.", ""], ["Biesbroek", "J. Matthijs", ""], ["de Bresser", "Jeroen", ""], ["Heinen", "Rutger", ""], ["Andermatt", "Simon", ""], ["Bento", "Mariana", ""], ["Berseth", "Matt", ""], ["Belyaev", "Mikhail", ""], ["Cardoso", "M. Jorge", ""], ["Casamitjana", "Adri\u00e0", ""], ["Collins", "D. Louis", ""], ["Dadar", "Mahsa", ""], ["Georgiou", "Achilleas", ""], ["Ghafoorian", "Mohsen", ""], ["Jin", "Dakai", ""], ["Khademi", "April", ""], ["Knight", "Jesse", ""], ["Li", "Hongwei", ""], ["Llad\u00f3", "Xavier", ""], ["Luna", "Miguel", ""], ["Mahmood", "Qaiser", ""], ["McKinley", "Richard", ""], ["Mehrtash", "Alireza", ""], ["Ourselin", "S\u00e9bastien", ""], ["Park", "Bo-yong", ""], ["Park", "Hyunjin", ""], ["Park", "Sang Hyun", ""], ["Pezold", "Simon", ""], ["Puybareau", "Elodie", ""], ["Rittner", "Leticia", ""], ["Sudre", "Carole H.", ""], ["Valverde", "Sergi", ""], ["Vilaplana", "Ver\u00f3nica", ""], ["Wiest", "Roland", ""], ["Xu", "Yongchao", ""], ["Xu", "Ziyue", ""], ["Zeng", "Guodong", ""], ["Zhang", "Jianguo", ""], ["Zheng", "Guoyan", ""], ["Chen", "Christopher", ""], ["van der Flier", "Wiesje", ""], ["Barkhof", "Frederik", ""], ["Viergever", "Max A.", ""], ["Biessels", "Geert Jan", ""]]}, {"id": "1904.00696", "submitter": "Jiaojiao Zhao", "authors": "Jiaojiao Zhao, Cees G.M. Snoek", "title": "Dance with Flow: Two-in-One Stream Action Detection", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to detect the spatio-temporal extent of an action.\nThe two-stream detection network based on RGB and flow provides\nstate-of-the-art accuracy at the expense of a large model-size and heavy\ncomputation. We propose to embed RGB and optical-flow into a single two-in-one\nstream network with new layers. A motion condition layer extracts motion\ninformation from flow images, which is leveraged by the motion modulation layer\nto generate transformation parameters for modulating the low-level RGB\nfeatures. The method is easily embedded in existing appearance- or two-stream\naction detection networks, and trained end-to-end. Experiments demonstrate that\nleveraging the motion condition to modulate RGB features improves detection\naccuracy. With only half the computation and parameters of the state-of-the-art\ntwo-stream methods, our two-in-one stream still achieves impressive results on\nUCF101-24, UCFSports and J-HMDB.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 11:09:03 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 18:05:37 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 11:29:06 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zhao", "Jiaojiao", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1904.00699", "submitter": "Quang-Hieu Pham", "authors": "Quang-Hieu Pham, Duc Thanh Nguyen, Binh-Son Hua, Gemma Roig, Sai-Kit\n  Yeung", "title": "JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with\n  Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields", "comments": "CVPR 2019 (Oral). More information at\n  https://pqhieu.github.io/cvpr19.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning techniques have become the to-go models for most vision-related\ntasks on 2D images. However, their power has not been fully realised on several\ntasks in 3D space, e.g., 3D scene understanding. In this work, we jointly\naddress the problems of semantic and instance segmentation of 3D point clouds.\nSpecifically, we develop a multi-task pointwise network that simultaneously\nperforms two tasks: predicting the semantic classes of 3D points and embedding\nthe points into high-dimensional vectors so that points of the same object\ninstance are represented by similar embeddings. We then propose a multi-value\nconditional random field model to incorporate the semantic and instance labels\nand formulate the problem of semantic and instance segmentation as jointly\noptimising labels in the field model. The proposed method is thoroughly\nevaluated and compared with existing methods on different indoor scene datasets\nincluding S3DIS and SceneNN. Experimental results showed the robustness of the\nproposed joint semantic-instance segmentation scheme over its single\ncomponents. Our method also achieved state-of-the-art performance on semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 11:21:33 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 10:51:55 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Pham", "Quang-Hieu", ""], ["Nguyen", "Duc Thanh", ""], ["Hua", "Binh-Son", ""], ["Roig", "Gemma", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1904.00708", "submitter": "Kolja Thormann", "authors": "Kolja Thormann and Marcus Baum", "title": "Optimal Fusion of Elliptic Extended Target Estimates based on the\n  Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the fusion of multiple estimates of a spatially extended\nobject, where the object extent is modeled as an ellipse parameterized by the\norientation and semiaxes lengths. For this purpose, we propose a novel\nsystematic approach that employs a distance measure for ellipses, i.e., the\nGaussian Wasserstein distance, as a cost function. We derive an explicit\napproximate expression for the Minimum Mean Gaussian Wasserstein distance\n(MMGW) estimate. Based on the concept of a MMGW estimator, we develop efficient\nmethods for the fusion of extended target estimates. The proposed fusion\nmethods are evaluated in a simulated experiment and the benefits of the novel\nmethods are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 11:52:27 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 13:13:22 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 13:03:32 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Thormann", "Kolja", ""], ["Baum", "Marcus", ""]]}, {"id": "1904.00724", "submitter": "Joseph Suarez", "authors": "Joseph Suarez", "title": "GAN You Do the GAN GAN?", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become a dominant class of\ngenerative models. In recent years, GAN variants have yielded especially\nimpressive results in the synthesis of a variety of forms of data. Examples\ninclude compelling natural and artistic images, textures, musical sequences,\nand 3D object files. However, one obvious synthesis candidate is missing. In\nthis work, we answer one of deep learning's most pressing questions: GAN you do\nthe GAN GAN? That is, is it possible to train a GAN to model a distribution of\nGANs? We release the full source code for this project under the MIT license.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:19:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Suarez", "Joseph", ""]]}, {"id": "1904.00726", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu", "title": "Unsupervised Multi-modal Hashing for Cross-modal retrieval", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advantage of low storage cost and high efficiency, hashing learning\nhas received much attention in the domain of Big Data. In this paper, we\npropose a novel unsupervised hashing learning method to cope with this open\nproblem to directly preserve the manifold structure by hashing. To address this\nproblem, both the semantic correlation in textual space and the locally\ngeometric structure in the visual space are explored simultaneously in our\nframework. Besides, the `2;1-norm constraint is imposed on the projection\nmatrices to learn the discriminative hash function for each modality. Extensive\nexperiments are performed to evaluate the proposed method on the three publicly\navailable datasets and the experimental results show that our method can\nachieve superior performance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 07:47:13 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 08:05:43 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 09:41:25 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2020 10:51:56 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1904.00738", "submitter": "Xiaolong Wu", "authors": "Xiaolong Wu, Assia Benbihi, Antoine Richard, and Cedric Pradalier", "title": "Semantic Nearest Neighbor Fields Monocular Edge Visual-Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning for edge detection and segmentation opens up\na new path for semantic-edge-based ego-motion estimation. In this work, we\npropose a robust monocular visual odometry (VO) framework using category-aware\nsemantic edges. It can reconstruct large-scale semantic maps in challenging\noutdoor environments. The core of our approach is a semantic nearest neighbor\nfield that facilitates a robust data association of edges across frames using\nsemantics. This significantly enlarges the convergence radius during tracking\nphases. The proposed edge registration method can be easily integrated into\ndirect VO frameworks to estimate photometrically, geometrically, and\nsemantically consistent camera motions. Different types of edges are evaluated\nand extensive experiments demonstrate that our proposed system outperforms\nstate-of-art indirect, direct, and semantic monocular VO systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:25:50 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wu", "Xiaolong", ""], ["Benbihi", "Assia", ""], ["Richard", "Antoine", ""], ["Pradalier", "Cedric", ""]]}, {"id": "1904.00739", "submitter": "Kevin Meng", "authors": "Kevin Meng, Yu Meng", "title": "Through-Wall Pose Imaging in Real-Time with a Many-to-Many\n  Encoder/Decoder Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overcoming the visual barrier and developing \"see-through vision\" has been\none of mankind's long-standing dreams. Unlike visible light, Radio Frequency\n(RF) signals penetrate opaque obstructions and reflect highly off humans. This\npaper establishes a deep-learning model that can be trained to reconstruct\ncontinuous video of a 15-point human skeleton even through visual occlusion.\nThe training process adopts a student/teacher learning procedure inspired by\nthe Feynman learning technique, in which video frames and RF data are first\ncollected simultaneously using a co-located setup containing an optical camera\nand an RF antenna array transceiver. Next, the video frames are processed with\na computer-vision-based gait analysis \"teacher\" module to generate ground-truth\nhuman skeletons for each frame. Then, the same type of skeleton is predicted\nfrom corresponding RF data using a \"student\" deep-learning model consisting of\na Residual Convolutional Neural Network (CNN), Region Proposal Network (RPN),\nand Recurrent Neural Network with Long-Short Term Memory (LSTM) that 1)\nextracts spatial features from RF images, 2) detects all people present in a\nscene, and 3) aggregates information over many time-steps, respectively. The\nmodel is shown to both accurately and completely predict the pose of humans\nbehind visual obstruction solely using RF signals. Primary academic\ncontributions include the novel many-to-many imaging methodology, unique\nintegration of RPN and LSTM networks, and original training pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 19:05:05 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 05:52:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Meng", "Kevin", ""], ["Meng", "Yu", ""]]}, {"id": "1904.00740", "submitter": "Hamid Tizhoosh", "authors": "Aditya Sriram, Shivam Kalra, H.R. Tizhoosh", "title": "Projectron -- A Shallow and Interpretable Network for Classifying\n  Medical Images", "comments": "Accepted for publication in the 2019 International Joint Conference\n  on Neural Networks (IJCNN), Budapest, Hungary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the `Projectron' as a new neural network architecture\nthat uses Radon projections to both classify and represent medical images. The\nmotivation is to build shallow networks which are more interpretable in the\nmedical imaging domain. Radon transform is an established technique that can\nreconstruct images from parallel projections. The Projectron first applies\nglobal Radon transform to each image using equidistant angles and then feeds\nthese transformations for encoding to a single layer of neurons followed by a\nlayer of suitable kernels to facilitate a linear separation of projections.\nFinally, the Projectron provides the output of the encoding as an input to two\nmore layers for final classification. We validate the Projectron on five\npublicly available datasets, a general dataset (namely MNIST) and four medical\ndatasets (namely Emphysema, IDC, IRMA, and Pneumonia). The results are\nencouraging as we compared the Projectron's performance against MLPs with raw\nimages and Radon projections as inputs, respectively. Experiments clearly\ndemonstrate the potential of the proposed Projectron for\nrepresenting/classifying medical images.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 12:13:23 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sriram", "Aditya", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1904.00741", "submitter": "Benjamin Chamberlain", "authors": "Elaine M. Bettaney, Stephen R. Hardwick, Odysseas Zisimopoulos,\n  Benjamin Paul Chamberlain", "title": "Fashion Outfit Generation for E-commerce", "comments": "9 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining items of clothing into an outfit is a major task in fashion retail.\nRecommending sets of items that are compatible with a particular seed item is\nuseful for providing users with guidance and inspiration, but is currently a\nmanual process that requires expert stylists and is therefore not scalable or\neasy to personalise. We use a multilayer neural network fed by visual and\ntextual features to learn embeddings of items in a latent style space such that\ncompatible items of different types are embedded close to one another. We train\nour model using the ASOS outfits dataset, which consists of a large number of\noutfits created by professional stylists and which we release to the research\ncommunity. Our model shows strong performance in an offline outfit\ncompatibility prediction task. We use our model to generate outfits and for the\nfirst time in this field perform an AB test, comparing our generated outfits to\nthose produced by a baseline model which matches appropriate product types but\nuses no information on style. Users approved of outfits generated by our model\n21% and 34% more frequently than those generated by the baseline model for\nwomenswear and menswear respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 11:19:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bettaney", "Elaine M.", ""], ["Hardwick", "Stephen R.", ""], ["Zisimopoulos", "Odysseas", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1904.00742", "submitter": "Renato Krohling", "authors": "Giuliano L. Manso, Helder Knidel, Renato A. Krohling, Jose A. Ventura", "title": "A smartphone application to detection and classification of coffee leaf\n  miner and coffee leaf rust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, the identification and classification of plant diseases and/or\npests are performed by an expert . One of the problems facing coffee farmers in\nBrazil is crop infestation, particularly by leaf rust Hemileia vastatrix and\nleaf miner Leucoptera coffeella. The progression of the diseases and or pests\noccurs spatially and temporarily. So, it is very important to automatically\nidentify the degree of severity. The main goal of this article consists on the\ndevelopment of a method and its i implementation as an App that allow the\ndetection of the foliar damages from images of coffee leaf that are captured\nusing a smartphone, and identify whether it is rust or leaf miner, and in turn\nthe calculation of its severity degree. The method consists of identifying a\nleaf from the image and separates it from the background with the use of a\nsegmentation algorithm. In the segmentation process, various types of\nbackgrounds for the image using the HSV and YCbCr color spaces are tested. In\nthe segmentation of foliar damages, the Otsu algorithm and the iterative\nthreshold algorithm, in the YCgCr color space, have been used and compared to\nk-means. Next, features of the segmented foliar damages are calculated. For the\nclassification, artificial neural network trained with extreme learning machine\nhave been used. The results obtained shows the feasibility and effectiveness of\nthe approach to identify and classify foliar damages, and the automatic\ncalculation of the severity. The results obtained are very promising according\nto experts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 21:45:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Manso", "Giuliano L.", ""], ["Knidel", "Helder", ""], ["Krohling", "Renato A.", ""], ["Ventura", "Jose A.", ""]]}, {"id": "1904.00744", "submitter": "Xiushan Nie", "authors": "Xingbo Liu, Xiushan Nie, Yilong Yin", "title": "Mutual Linear Regression-based Discrete Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label information is widely used in hashing methods because of its\neffectiveness of improving the precision. The existing hashing methods always\nuse two different projections to represent the mutual regression between hash\ncodes and class labels. In contrast to the existing methods, we propose a novel\nlearning-based hashing method termed stable supervised discrete hashing with\nmutual linear regression (S2DHMLR) in this study, where only one stable\nprojection is used to describe the linear correlation between hash codes and\ncorresponding labels. To the best of our knowledge, this strategy has not been\nused for hashing previously. In addition, we further use a boosting strategy to\nimprove the final performance of the proposed method without adding extra\nconstraints and with little extra expenditure in terms of time and space.\nExtensive experiments conducted on three image benchmarks demonstrate the\nsuperior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 01:13:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Liu", "Xingbo", ""], ["Nie", "Xiushan", ""], ["Yin", "Yilong", ""]]}, {"id": "1904.00747", "submitter": "Amir Rastar", "authors": "Amir Rastar", "title": "A Novel Pixel-Averaging Technique for Extracting Training Data from a\n  Single Image, Used in ML-Based Image Enlargement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Size of the training dataset is an important factor in the performance of a\nmachine learning algorithms and tools used in medical image processing are not\nexceptions. Machine learning tools normally require a decent amount of training\ndata before they could efficiently predict a target. For image processing and\ncomputer vision, the number of images determines the validity and reliability\nof the training set. Medical images in some cases, suffer from poor quality and\ninadequate quantity required for a suitable training set. The proposed\nalgorithm in this research obviates the need for large or even small image\ndatasets used in machine learning based image enlargement techniques by\nextracting the required data from a single image. The extracted data was then\nintroduced to a decision tree regressor for upscaling greyscale medical images\nat different zoom levels. Results from the algorithm are relatively acceptable\ncompared to third-party applications and promising for future research. This\ntechnique could be tailored to the requirements of other machine learning tools\nand the results may be improved by further tweaking of the tools\nhyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:48:36 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Rastar", "Amir", ""]]}, {"id": "1904.00758", "submitter": "Lex Fridman", "authors": "Li Ding, Jack Terwilliger, Rini Sherony, Bryan Reimer, Lex Fridman", "title": "Value of Temporal Dynamics Information in Driving Scene Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene segmentation has primarily been addressed by forming\nrepresentations of single images both with supervised and unsupervised methods.\nThe problem of semantic segmentation in dynamic scenes has begun to recently\nreceive attention with video object segmentation approaches. What is not known\nis how much extra information the temporal dynamics of the visual scene carries\nthat is complimentary to the information available in the individual frames of\nthe video. There is evidence that the human visual system can effectively\nperceive the scene from temporal dynamics information of the scene's changing\nvisual characteristics without relying on the visual characteristics of\nindividual snapshots themselves. Our work takes steps to explore whether\nmachine perception can exhibit similar properties by combining appearance-based\nrepresentations and temporal dynamics representations in a joint-learning\nproblem that reveals the contribution of each toward successful dynamic scene\nsegmentation. Additionally, we provide the MIT Driving Scene Segmentation\ndataset, which is a large-scale full driving scene segmentation dataset,\ndensely annotated for every pixel and every one of 5,000 video frames. This\ndataset is intended to help further the exploration of the value of temporal\ndynamics information for semantic segmentation in video.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 03:56:30 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ding", "Li", ""], ["Terwilliger", "Jack", ""], ["Sherony", "Rini", ""], ["Reimer", "Bryan", ""], ["Fridman", "Lex", ""]]}, {"id": "1904.00759", "submitter": "Juncheng Li", "authors": "Juncheng Li, Frank R. Schmidt, J. Zico Kolter", "title": "Adversarial camera stickers: A physical camera-based attack on deep\n  learning systems", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:3896-3904, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has documented the susceptibility of deep learning systems to\nadversarial examples, but most such attacks directly manipulate the digital\ninput to a classifier. Although a smaller line of work considers physical\nadversarial attacks, in all cases these involve manipulating the object of\ninterest, e.g., putting a physical sticker on an object to misclassify it, or\nmanufacturing an object specifically intended to be misclassified. In this\nwork, we consider an alternative question: is it possible to fool deep\nclassifiers, over all perceived objects of a certain type, by physically\nmanipulating the camera itself? We show that by placing a carefully crafted and\nmainly-translucent sticker over the lens of a camera, one can create universal\nperturbations of the observed images that are inconspicuous, yet misclassify\ntarget objects as a different (targeted) class. To accomplish this, we propose\nan iterative procedure for both updating the attack perturbation (to make it\nadversarial for a given classifier), and the threat model itself (to ensure it\nis physically realizable). For example, we show that we can achieve\nphysically-realizable attacks that fool ImageNet classifiers in a targeted\nfashion 49.6% of the time. This presents a new class of physically-realizable\nthreat models to consider in the context of adversarially robust machine\nlearning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 23:33:12 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 01:46:23 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 17:31:40 GMT"}, {"version": "v4", "created": "Sat, 8 Jun 2019 19:23:56 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Juncheng", ""], ["Schmidt", "Frank R.", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1904.00760", "submitter": "Wieland Brendel", "authors": "Wieland Brendel and Matthias Bethge", "title": "Approximating CNNs with Bag-of-local-Features models works surprisingly\n  well on ImageNet", "comments": "Published as a conference paper at the Seventh International\n  Conference on Learning Representations (ICLR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has\nproven notoriously difficult to understand how they reach their decisions. We\nhere introduce a high-performance DNN architecture on ImageNet whose decisions\nare considerably easier to explain. Our model, a simple variant of the\nResNet-50 architecture called BagNet, classifies an image based on the\noccurrences of small local image features without taking into account their\nspatial ordering. This strategy is closely related to the bag-of-feature (BoF)\nmodels popular before the onset of deep learning and reaches a surprisingly\nhigh accuracy on ImageNet (87.6% top-5 for 33 x 33 px features and Alexnet\nperformance for 17 x 17 px features). The constraint on local features makes it\nstraight-forward to analyse how exactly each part of the image influences the\nclassification. Furthermore, the BagNets behave similar to state-of-the art\ndeep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of\nfeature sensitivity, error distribution and interactions between image parts.\nThis suggests that the improvements of DNNs over previous bag-of-feature\nclassifiers in the last few years is mostly achieved by better fine-tuning\nrather than by qualitatively different decision strategies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:37:17 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1904.00763", "submitter": "Samy Blusseau", "authors": "Bastien Ponchon (CMM, LTCI), Santiago Velasco-Forero (CMM), Samy\n  Blusseau (CMM), Jesus Angulo (CMM), Isabelle Bloch (LTCI)", "title": "Part-based approximations for morphological operators using asymmetric\n  auto-encoders", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology, Jul 2019,\n  Saarbr{\\\"u}cken, Germany", "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of building a part-based representation of a\ndataset of images. More precisely, we look for a non-negative, sparse\ndecomposition of the images on a reduced set of atoms, in order to unveil a\nmorphological and interpretable structure of the data. Additionally, we want\nthis decomposition to be computed online for any new sample that is not part of\nthe initial dataset. Therefore, our solution relies on a sparse, non-negative\nauto-encoder where the encoder is deep (for accuracy) and the decoder shallow\n(for interpretability). This method compares favorably to the state-of-the-art\nonline methods on two datasets (MNIST and Fashion MNIST), according to\nclassical metrics and to a new one we introduce, based on the invariance of the\nrepresentation to morphological dilation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 08:16:48 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:03:34 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ponchon", "Bastien", "", "CMM, LTCI"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Blusseau", "Samy", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Bloch", "Isabelle", "", "LTCI"]]}, {"id": "1904.00764", "submitter": "Hazrat Ali", "authors": "Mohammad Farhad Bulbul, Saiful Islam, Hazrat Ali", "title": "3D human action analysis and recognition through GLAC descriptor on 2D\n  motion and static posture images", "comments": "Multimed Tools Appl (2019)", "journal-ref": null, "doi": "10.1007/s11042-019-7365-2", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present an approach for identification of actions within\ndepth action videos. First, we process the video to get motion history images\n(MHIs) and static history images (SHIs) corresponding to an action video based\non the use of 3D Motion Trail Model (3DMTM). We then characterize the action\nvideo by extracting the Gradient Local Auto-Correlations (GLAC) features from\nthe SHIs and the MHIs. The two sets of features i.e., GLAC features from MHIs\nand GLAC features from SHIs are concatenated to obtain a representation vector\nfor action. Finally, we perform the classification on all the action samples by\nusing the l2-regularized Collaborative Representation Classifier (l2-CRC) to\nrecognize different human actions in an effective way. We perform evaluation of\nthe proposed method on three action datasets, MSR-Action3D, DHA and UTD-MHAD.\nThrough experimental results, we observe that the proposed method performs\nsuperior to other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 17:52:16 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bulbul", "Mohammad Farhad", ""], ["Islam", "Saiful", ""], ["Ali", "Hazrat", ""]]}, {"id": "1904.00765", "submitter": "Haohao Li", "authors": "Haohao Li, Shengfa Wang, Nannan Li, Zhixun Su, Ximin Liu", "title": "Non-rigid 3D shape retrieval based on multi-view metric learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a novel multi-view metric learning algorithm, which aims\nto improve 3D non-rigid shape retrieval. With the development of non-rigid 3D\nshape analysis, there exist many shape descriptors. The intrinsic descriptors\ncan be explored to construct various intrinsic representations for non-rigid 3D\nshape retrieval task. The different intrinsic representations (features) focus\non different geometric properties to describe the same 3D shape, which makes\nthe representations are related. Therefore, it is possible and necessary to\nlearn multiple metrics for different representations jointly. We propose an\neffective multi-view metric learning algorithm by extending the Marginal Fisher\nAnalysis (MFA) into the multi-view domain, and exploring Hilbert-Schmidt\nIndependence Criteria (HSCI) as a diversity term to jointly learning the new\nmetrics. The different classes can be separated by MFA in our method.\nMeanwhile, HSCI is exploited to make the multiple representations to be\nconsensus. The learned metrics can reduce the redundancy between the multiple\nrepresentations, and improve the accuracy of the retrieval results. Experiments\nare performed on SHREC'10 benchmarks, and the results show that the proposed\nmethod outperforms the state-of-the-art non-rigid 3D shape retrieval methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 02:03:09 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Haohao", ""], ["Wang", "Shengfa", ""], ["Li", "Nannan", ""], ["Su", "Zhixun", ""], ["Liu", "Ximin", ""]]}, {"id": "1904.00766", "submitter": "Hassan Maleki Galandouz", "authors": "Hassan Maleki Galandouz, Mohsen Ebrahimi Moghaddam, Mehrnoush\n  Shamsfard", "title": "A Weighted Multi-Criteria Decision Making Approach for Image Captioning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning aims at automatically generating descriptions of an image in\nnatural language. This is a challenging problem in the field of artificial\nintelligence that has recently received significant attention in the computer\nvision and natural language processing. Among the existing approaches, visual\nretrieval based methods have been proven to be highly effective. These\napproaches search for similar images, then build a caption for the query image\nbased on the captions of the retrieved images. In this study, we present a\nmethod for visual retrieval based image captioning, in which we use a multi\ncriteria decision making algorithm to effectively combine several criteria with\nproportional impact weights to retrieve the most relevant caption for the query\nimage. The main idea of the proposed approach is to design a mechanism to\nretrieve more semantically relevant captions with the query image and then\nselecting the most appropriate caption by imitation of the human act based on a\nweighted multi-criteria decision making algorithm. Experiments conducted on MS\nCOCO benchmark dataset have shown that proposed method provides much more\neffective results in compare to the state-of-the-art models by using criteria\nwith proportional impact weights .\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 13:20:01 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Galandouz", "Hassan Maleki", ""], ["Moghaddam", "Mohsen Ebrahimi", ""], ["Shamsfard", "Mehrnoush", ""]]}, {"id": "1904.00767", "submitter": "Shi  Chen", "authors": "Shi Chen and Qi Zhao", "title": "Boosted Attention: Leveraging Human Attention for Image Captioning", "comments": "Published in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention has shown usefulness in image captioning, with the goal of\nenabling a caption model to selectively focus on regions of interest. Existing\nmodels typically rely on top-down language information and learn attention\nimplicitly by optimizing the captioning objectives. While somewhat effective,\nthe learned top-down attention can fail to focus on correct regions of interest\nwithout direct supervision of attention. Inspired by the human visual system\nwhich is driven by not only the task-specific top-down signals but also the\nvisual stimuli, we in this work propose to use both types of attention for\nimage captioning. In particular, we highlight the complementary nature of the\ntwo types of attention and develop a model (Boosted Attention) to integrate\nthem for image captioning. We validate the proposed approach with\nstate-of-the-art performance across various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:59:44 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Shi", ""], ["Zhao", "Qi", ""]]}, {"id": "1904.00768", "submitter": "Pranav Shenoy K P", "authors": "Yongqing Sun, Pranav Shenoy K P, Jun Shimamura, Atsushi Sagata", "title": "Concatenated Feature Pyramid Network for Instance Segmentation", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low level features like edges and textures play an important role in\naccurately localizing instances in neural networks. In this paper, we propose\nan architecture which improves feature pyramid networks commonly used instance\nsegmentation networks by incorporating low level features in all layers of the\npyramid in an optimal and efficient way. Specifically, we introduce a new layer\nwhich learns new correlations from feature maps of multiple feature pyramid\nlevels holistically and enhances the semantic information of the feature\npyramid to improve accuracy. Our architecture is simple to implement in\ninstance segmentation or object detection frameworks to boost accuracy. Using\nthis method in Mask RCNN, our model achieves consistent improvement in\nprecision on COCO Dataset with the computational overhead compared to the\noriginal feature pyramid network.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 07:44:10 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sun", "Yongqing", ""], ["P", "Pranav Shenoy K", ""], ["Shimamura", "Jun", ""], ["Sagata", "Atsushi", ""]]}, {"id": "1904.00770", "submitter": "Daniel Salles Civitarese", "authors": "Reinaldo Mozart Silva, Lais Baroni, Rodrigo S. Ferreira, Daniel\n  Civitarese, Daniela Szwarcman, Emilio Vital Brazil", "title": "Netherlands Dataset: A New Public Dataset for Machine Learning in\n  Seismic Interpretation", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and, more specifically, deep learning algorithms have seen\nremarkable growth in their popularity and usefulness in the last years. This is\narguably due to three main factors: powerful computers, new techniques to train\ndeeper networks and larger datasets. Although the first two are readily\navailable in modern computers and ML libraries, the last one remains a\nchallenge for many domains. It is a fact that big data is a reality in almost\nall fields nowadays, and geosciences are not an exception. However, to achieve\nthe success of general-purpose applications such as ImageNet - for which there\nare +14 million labeled images for 1000 target classes - we not only need more\ndata, we need more high-quality labeled data. When it comes to the Oil&Gas\nindustry, confidentiality issues hamper even more the sharing of datasets. In\nthis work, we present the Netherlands interpretation dataset, a contribution to\nthe development of machine learning in seismic interpretation. The Netherlands\nF3 dataset acquisition was carried out in the North Sea, Netherlands offshore.\nThe data is publicly available and contains pos-stack data, 8 horizons and well\nlogs of 4 wells. For the purposes of our machine learning tasks, the original\ndataset was reinterpreted, generating 9 horizons separating different seismic\nfacies intervals. The interpreted horizons were used to generate approximatelly\n190,000 labeled images for inlines and crosslines. Finally, we present two deep\nlearning applications in which the proposed dataset was employed and produced\ncompelling results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 13:12:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Silva", "Reinaldo Mozart", ""], ["Baroni", "Lais", ""], ["Ferreira", "Rodrigo S.", ""], ["Civitarese", "Daniel", ""], ["Szwarcman", "Daniela", ""], ["Brazil", "Emilio Vital", ""]]}, {"id": "1904.00775", "submitter": "Ramchalam Kinattinkara Ramakrishnan Mr", "authors": "Ramchalam Kinattinkara Ramakrishnan, Shangling Jui and Vahid Patrovi\n  Nia", "title": "Deep Demosaicing for Edge Implementation", "comments": "Accepted in the 16th International Conference of Image Analysis and\n  Recognition (ICIAR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most digital cameras use sensors coated with a Color Filter Array (CFA) to\ncapture channel components at every pixel location, resulting in a mosaic image\nthat does not contain pixel values in all channels. Current research on\nreconstructing these missing channels, also known as demosaicing, introduces\nmany artifacts, such as zipper effect and false color. Many deep learning\ndemosaicing techniques outperform other classical techniques in reducing the\nimpact of artifacts. However, most of these models tend to be\nover-parametrized. Consequently, edge implementation of the state-of-the-art\ndeep learning-based demosaicing algorithms on low-end edge devices is a major\nchallenge. We provide an exhaustive search of deep neural network architectures\nand obtain a pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the\nperformance criterion versus the number of parameters as the model complexity\nthat beats the state-of-the-art. Architectures on the pareto front can then be\nused to choose the best architecture for a variety of resource constraints.\nSimple architecture search methods such as exhaustive search and grid search\nrequire some conditions of the loss function to converge to the optimum. We\nclarify these conditions in a brief theoretical study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:04:17 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 19:30:39 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 15:20:54 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Ramakrishnan", "Ramchalam Kinattinkara", ""], ["Jui", "Shangling", ""], ["Nia", "Vahid Patrovi", ""]]}, {"id": "1904.00776", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu", "title": "Cross-modal Subspace Learning via Kernel Correlation Maximization and\n  Discriminative Structure Preserving", "comments": "The paper is under consideration at Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measure between heterogeneous data is still an open problem. Many\nresearch works have been developed to learn a common subspace where the\nsimilarity between different modalities can be calculated directly. However,\nmost of existing works focus on learning a latent subspace but the semantically\nstructural information is not well preserved. Thus, these approaches cannot get\ndesired results. In this paper, we propose a novel framework, termed\nCross-modal subspace learning via Kernel correlation maximization and\nDiscriminative structure-preserving (CKD), to solve this problem in two\naspects. Firstly, we construct a shared semantic graph to make each modality\ndata preserve the neighbor relationship semantically. Secondly, we introduce\nthe Hilbert-Schmidt Independence Criteria (HSIC) to ensure the consistency\nbetween feature-similarity and semantic-similarity of samples. Our model not\nonly considers the inter-modality correlation by maximizing the kernel\ncorrelation but also preserves the semantically structural information within\neach modality. The extensive experiments are performed to evaluate the proposed\nframework on the three public datasets. The experimental results demonstrated\nthat the proposed CKD is competitive compared with the classic subspace\nlearning methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 11:29:47 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:09:43 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 03:25:14 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1904.00781", "submitter": "Dawei Li", "authors": "Dawei Li, Serafettin Tasci, Shalini Ghosh, Jingwen Zhu, Junting Zhang,\n  Larry Heck", "title": "RILOD: Near Real-Time Incremental Learning for Object Detection at the\n  Edge", "comments": "Camera-ready for ACM/IEEE SEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection models shipped with camera-equipped edge devices cannot\ncover the objects of interest for every user. Therefore, the incremental\nlearning capability is a critical feature for a robust and personalized object\ndetection system that many applications would rely on. In this paper, we\npresent an efficient yet practical system, RILOD, to incrementally train an\nexisting object detection model such that it can detect new object classes\nwithout losing its capability to detect old classes. The key component of RILOD\nis a novel incremental learning algorithm that trains end-to-end for one-stage\ndeep object detection models only using training data of new object classes.\nSpecifically to avoid catastrophic forgetting, the algorithm distills three\ntypes of knowledge from the old model to mimic the old model's behavior on\nobject classification, bounding box regression and feature extraction. In\naddition, since the training data for the new classes may not be available, a\nreal-time dataset construction pipeline is designed to collect training images\non-the-fly and automatically label the images with both category and bounding\nbox annotations. We have implemented RILOD under both edge-cloud and edge-only\nsetups. Experiment results show that the proposed system can learn to detect a\nnew object class in just a few minutes, including both dataset construction and\nmodel training. In comparison, traditional fine-tuning based method may take a\nfew hours for training, and in most cases would also need a tedious and costly\nmanual dataset labeling step.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:22:01 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:37:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Li", "Dawei", ""], ["Tasci", "Serafettin", ""], ["Ghosh", "Shalini", ""], ["Zhu", "Jingwen", ""], ["Zhang", "Junting", ""], ["Heck", "Larry", ""]]}, {"id": "1904.00783", "submitter": "Md. Abu Bakr Siddique", "authors": "Shadman Sakib, Zahidun Ashrafi, Md. Abu Bakr Siddique", "title": "Implementation of Fruits Recognition Classifier using Convolutional\n  Neural Network Algorithm for Observation of Accuracies for Various Hidden\n  Layers", "comments": "4 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fruit recognition using Deep Convolutional Neural Network (CNN) is one of the\nmost promising applications in computer vision. In recent times, deep learning\nbased classifications are making it possible to recognize fruits from images.\nHowever, fruit recognition is still a problem for the stacked fruits on\nweighing scale because of the complexity and similarity. In this paper, a fruit\nrecognition system using CNN is proposed. The proposed method uses deep\nlearning techniques for the classification. We have used Fruits-360 dataset for\nthe evaluation purpose. From the dataset, we have established a dataset which\ncontains 17,823 images from 25 different categories. The images are divided\ninto training and test dataset. Moreover, for the classification accuracies, we\nhave used various combinations of hidden layer and epochs for different cases\nand made a comparison between them. The overall performance losses of the\nnetwork for different cases also observed. Finally, we have achieved the best\ntest accuracy of 100% and a training accuracy of 99.79%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:03:33 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 03:20:33 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 10:55:38 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 06:40:49 GMT"}, {"version": "v5", "created": "Thu, 16 Jan 2020 02:35:33 GMT"}, {"version": "v6", "created": "Sat, 25 Jan 2020 12:10:41 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Sakib", "Shadman", ""], ["Ashrafi", "Zahidun", ""], ["Siddique", "Md. Abu Bakr", ""]]}, {"id": "1904.00786", "submitter": "Amjad Rehman Dr", "authors": "Azhar Ahmad Jaini, Ghazali Sulong and Amjad Rehman", "title": "Improved Dynamic Time Warping (DTW) Approach for Online Signature\n  Verification", "comments": "This paper is first author thesis paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online signature verification is the process of verifying time series\nsignature data which is generally obtained from the tablet-based device. Unlike\noffline signature images, the online signature image data consists of points\nthat are arranged in a sequence of time. The aim of this research is to develop\nan improved approach to map the strokes in both test and reference signatures.\nCurrent methods make use of the Dynamic Time Warping (DTW) algorithm and its\nvariant to segment them before comparing each of its data dimension. This paper\npresents a modified DTW algorithm with the proposed Lost Box Recovery Algorithm\naims to improve the mapping performance for online signature verification\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 10:05:22 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Jaini", "Azhar Ahmad", ""], ["Sulong", "Ghazali", ""], ["Rehman", "Amjad", ""]]}, {"id": "1904.00787", "submitter": "Svitlana Alkhimova", "authors": "Svitlana Alkhimova", "title": "CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with\n  Abnormal Brain Anatomy", "comments": null, "journal-ref": "Proceedings of the 2019 3rd International Conference on Frontiers\n  of Image Processing (ICFIP 2019). - Florence, Italy, March 16-18, 2019. -\n  P008", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for relatively accurate brain region of\ninterest (ROI) detection from dynamic susceptibility contrast (DSC) perfusion\nmagnetic resonance (MR) images of a human head with abnormal brain anatomy.\nSuch images produce problems for automatic brain segmentation algorithms, and\nas a result, poor perfusion ROI detection affects both quantitative\nmeasurements and visual assessment of perfusion data. In the proposed approach\nimage segmentation is based on CUSUM filter usage that was adapted to be\napplicable to process DSC perfusion MR images. The result of segmentation is a\nbinary mask of brain ROI that is generated via usage of brain boundary\nlocation. Each point of the boundary between the brain and surrounding tissues\nis detected as a change-point by CUSUM filter. Proposed adopted CUSUM filter\noperates by accumulating the deviations between the observed and expected\nintensities of image points at the time of moving on a trajectory. Motion\ntrajectory is created by the iterative change of movement direction inside the\nbackground region in order to reach brain region, and vice versa after boundary\ncrossing. Proposed segmentation approach was evaluated with Dice index\ncomparing obtained results to the reference standard. Manually marked brain\nregion pixels (reference standard), as well as visual inspection of detected\nwith CUSUM filter usage brain ROI, were provided by experienced radiologists.\nThe results showed that proposed approach is suitable to be used for brain ROI\ndetection from DSC perfusion MR images of a human head with abnormal brain\nanatomy and can, therefore, be applied in the DSC perfusion data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 08:56:07 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Alkhimova", "Svitlana", ""]]}, {"id": "1904.00790", "submitter": "Max-Heinrich Laves", "authors": "Max-Heinrich Laves, Sontje Ihler, L\\\"uder A. Kahrs, Tobias Ortmaier", "title": "Retinal OCT disease classification with variational autoencoder\n  regularization", "comments": "Accepted for publication at 33rd international conference on Computer\n  Assisted Radiology and Surgery (CARS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, 285 million people worldwide live\nwith visual impairment. The most commonly used imaging technique for diagnosis\nin ophthalmology is optical coherence tomography (OCT). However, analysis of\nretinal OCT requires trained ophthalmologists and time, making a comprehensive\nearly diagnosis unlikely. A recent study established a diagnostic tool based on\nconvolutional neural networks (CNN), which was trained on a large database of\nretinal OCT images. The performance of the tool in classifying retinal\nconditions was on par to that of trained medical experts. However, the training\nof these networks is based on an enormous amount of labeled data, which is\nexpensive and difficult to obtain. Therefore, this paper describes a method\nbased on variational autoencoder regularization that improves classification\nperformance when using a limited amount of labeled data. This work uses a\ntwo-path CNN model combining a classification network with an autoencoder (AE)\nfor regularization. The key idea behind this is to prevent overfitting when\nusing a limited training dataset size with small number of patients. Results\nshow superior classification performance compared to a pre-trained and fully\nfine-tuned baseline ResNet-34. Clustering of the latent space in relation to\nthe disease class is distinct. Neural networks for disease classification on\nOCTs can benefit from regularization using variational autoencoders when\ntrained with limited amount of patient data. Especially in the medical imaging\ndomain, data annotated by experts is expensive to obtain.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:20:51 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Ihler", "Sontje", ""], ["Kahrs", "L\u00fcder A.", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "1904.00792", "submitter": "Amjad Rehman Dr", "authors": "Amjad Rehman", "title": "Cursive Overlapped Character Segmentation: An Enhanced Approach", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of highly slanted and horizontally overlapped characters is a\nchallenging research area that is still fresh. Several techniques are reported\nin the state of art, but produce low accuracy for the highly slanted characters\nsegmentation and cause overall low handwriting recognition precision.\nAccordingly, this paper presents a simple yet effective approach for character\nsegmentation of such difficult slanted cursive words without using any slant\ncorrection technique. Rather a new concept of core-zone is introduced for\nsegmenting such difficult slanted handwritten words. However, due to the\ninherent nature of cursive words, few characters are over-segmented and\ntherefore, a threshold is selected heuristically to overcome this problem. For\nfair comparison, difficult words are extracted from the IAM benchmark database.\nExperiments thus performed exhibit promising result and high speed.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 09:59:03 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Rehman", "Amjad", ""]]}, {"id": "1904.00813", "submitter": "Yuliang Liu", "authors": "Yuliang Liu, Lianwen Jin, Zecheng Xie, Canjie Luo, Shuaitao Zhang,\n  Lele Xie", "title": "Tightness-aware Evaluation Protocol for Scene Text Detection", "comments": "Accepted to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation protocols play key role in the developmental progress of text\ndetection methods. There are strict requirements to ensure that the evaluation\nmethods are fair, objective and reasonable. However, existing metrics exhibit\nsome obvious drawbacks: 1) They are not goal-oriented; 2) they cannot recognize\nthe tightness of detection methods; 3) existing one-to-many and many-to-one\nsolutions involve inherent loopholes and deficiencies. Therefore, this paper\nproposes a novel evaluation protocol called Tightness-aware\nIntersect-over-Union (TIoU) metric that could quantify completeness of ground\ntruth, compactness of detection, and tightness of matching degree.\nSpecifically, instead of merely using the IoU value, two common detection\nbehaviors are properly considered; meanwhile, directly using the score of TIoU\nto recognize the tightness. In addition, we further propose a straightforward\nmethod to address the annotation granularity issue, which can fairly evaluate\nword and text-line detections simultaneously. By adopting the detection results\nfrom published methods and general object detection frameworks, comprehensive\nexperiments on ICDAR 2013 and ICDAR 2015 datasets are conducted to compare\nrecent metrics and the proposed TIoU metric. The comparison demonstrated some\npromising new prospects, e.g., determining the methods and frameworks for which\nthe detection is tighter and more beneficial to recognize. Our method is\nextremely simple; however, the novelty is none other than the proposed metric\ncan utilize simplest but reasonable improvements to lead to many interesting\nand insightful prospects and solving most the issues of the previous metrics.\nThe code is publicly available at https://github.com/Yuliang-Liu/TIoU-metric .\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 11:06:17 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Liu", "Yuliang", ""], ["Jin", "Lianwen", ""], ["Xie", "Zecheng", ""], ["Luo", "Canjie", ""], ["Zhang", "Shuaitao", ""], ["Xie", "Lele", ""]]}, {"id": "1904.00815", "submitter": "Chollette Olisah Dr", "authors": "Chollette C. Olisah, Lyndon Smith", "title": "Understanding Unconventional Preprocessors in Deep Convolutional Neural\n  Networks for Face Identification", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have achieved huge successes in application domains like object\nand face recognition. The performance gain is attributed to different facets of\nthe network architecture such as: depth of the convolutional layers, activation\nfunction, pooling, batch normalization, forward and back propagation and many\nmore. However, very little emphasis is made on the preprocessors. Therefore, in\nthis paper, the network's preprocessing module is varied across different\npreprocessing approaches while keeping constant other facets of the network\narchitecture, to investigate the contribution preprocessing makes to the\nnetwork. Commonly used preprocessors are the data augmentation and\nnormalization and are termed conventional preprocessors. Others are termed the\nunconventional preprocessors, they are: color space converters; HSV, CIE L*a*b*\nand YCBCR, grey-level resolution preprocessors; full-based and plane-based\nimage quantization, illumination normalization and insensitive feature\npreprocessing using: histogram equalization (HE), local contrast normalization\n(LN) and complete face structural pattern (CFSP). To achieve fixed network\nparameters, CNNs with transfer learning is employed. Knowledge from the\nhigh-level feature vectors of the Inception-V3 network is transferred to\noffline preprocessed LFW target data; and features trained using the SoftMax\nclassifier for face identification. The experiments show that the\ndiscriminative capability of the deep networks can be improved by preprocessing\nRGB data with HE, full-based and plane-based quantization, rgbGELog, and YCBCR,\npreprocessors before feeding it to CNNs. However, for best performance, the\nright setup of preprocessed data with augmentation and/or normalization is\nrequired. The plane-based image quantization is found to increase the\nhomogeneity of neighborhood pixels and utilizes reduced bit depth for better\nstorage efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:05:55 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 10:54:14 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Olisah", "Chollette C.", ""], ["Smith", "Lyndon", ""]]}, {"id": "1904.00816", "submitter": "Kuo Teng Ding", "authors": "Yi-Lun Pan, Min-Jhih Huang, Kuo-Teng Ding, Ja-Ling Wu, Jyh-Shing Jang", "title": "k-Same-Siamese-GAN: k-Same Algorithm with Generative Adversarial Network\n  for Facial Image De-identification with Hyperparameter Tuning and Mixed\n  Precision Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a data holder, such as a hospital or a government entity, who has a\nprivately held collection of personal data, in which the revealing and/or\nprocessing of the personal identifiable data is restricted and prohibited by\nlaw. Then, \"how can we ensure the data holder does conceal the identity of each\nindividual in the imagery of personal data while still preserving certain\nuseful aspects of the data after de-identification?\" becomes a challenge issue.\nIn this work, we propose an approach towards high-resolution facial image\nde-identification, called k-Same-Siamese-GAN, which leverages the\nk-Same-Anonymity mechanism, the Generative Adversarial Network, and the\nhyperparameter tuning methods. Moreover, to speed up model training and reduce\nmemory consumption, the mixed precision training technique is also applied to\nmake kSS-GAN provide guarantees regarding privacy protection on close-form\nidentities and be trained much more efficiently as well. Finally, to validate\nits applicability, the proposed work has been applied to actual datasets - RafD\nand CelebA for performance testing. Besides protecting privacy of\nhigh-resolution facial images, the proposed system is also justified for its\nability in automating parameter tuning and breaking through the limitation of\nthe number of adjustable parameters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:27:07 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 05:24:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Pan", "Yi-Lun", ""], ["Huang", "Min-Jhih", ""], ["Ding", "Kuo-Teng", ""], ["Wu", "Ja-Ling", ""], ["Jang", "Jyh-Shing", ""]]}, {"id": "1904.00817", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Brejesh Lall", "title": "DeepPoint3D: Learning Discriminative Local Descriptors using Deep Metric\n  Learning on 3D Point Clouds", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2019.02.027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning local descriptors is an important problem in computer vision. While\nthere are many techniques for learning local patch descriptors for 2D images,\nrecently efforts have been made for learning local descriptors for 3D points.\nThe recent progress towards solving this problem in 3D leverages the strong\nfeature representation capability of image based convolutional neural networks\nby utilizing RGB-D or multi-view representations. However, in this paper, we\npropose to learn 3D local descriptors by directly processing unstructured 3D\npoint clouds without needing any intermediate representation. The method\nconstitutes a deep network for learning permutation invariant representation of\n3D points. To learn the local descriptors, we use a multi-margin contrastive\nloss which discriminates between similar and dissimilar points on a surface\nwhile also leveraging the extent of dissimilarity among the negative samples at\nthe time of training. With comprehensive evaluation against strong baselines,\nwe show that the proposed method outperforms state-of-the-art methods for\nmatching points in 3D point clouds. Further, we demonstrate the effectiveness\nof the proposed method on various applications achieving state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 15:47:07 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Lall", "Brejesh", ""]]}, {"id": "1904.00818", "submitter": "Simone Bonechi", "authors": "Simone Bonechi, Paolo Andreini, Monica Bianchini and Franco Scarselli", "title": "COCO_TS Dataset: Pixel-level Annotations Based on Weak Supervision for\n  Scene Text Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30490-4_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The absence of large scale datasets with pixel-level supervisions is a\nsignificant obstacle for the training of deep convolutional networks for scene\ntext segmentation. For this reason, synthetic data generation is normally\nemployed to enlarge the training dataset. Nonetheless, synthetic data cannot\nreproduce the complexity and variability of natural images. In this paper, a\nweakly supervised learning approach is used to reduce the shift between\ntraining on real and synthetic data. Pixel-level supervisions for a text\ndetection dataset (i.e. where only bounding-box annotations are available) are\ngenerated. In particular, the COCO-Text-Segmentation (COCO_TS) dataset, which\nprovides pixel-level supervisions for the COCO-Text dataset, is created and\nreleased. The generated annotations are used to train a deep convolutional\nneural network for semantic segmentation. Experiments show that the proposed\ndataset can be used instead of synthetic data, allowing us to use only a\nfraction of the training samples and significantly improving the performances.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:03:31 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 07:58:00 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 10:32:58 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 08:37:26 GMT"}, {"version": "v5", "created": "Sat, 29 Jun 2019 15:00:31 GMT"}, {"version": "v6", "created": "Tue, 24 Sep 2019 10:22:35 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Bonechi", "Simone", ""], ["Andreini", "Paolo", ""], ["Bianchini", "Monica", ""], ["Scarselli", "Franco", ""]]}, {"id": "1904.00824", "submitter": "Sebastian Hartwig", "authors": "Sebastian Hartwig, Timo Ropinski", "title": "Training Object Detectors on Synthetic Images Containing Reflecting\n  Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the grand challenges of deep learning is the requirement to obtain\nlarge labeled training data sets. While synthesized data sets can be used to\novercome this challenge, it is important that these data sets close the reality\ngap, i.e., a model trained on synthetic image data is able to generalize to\nreal images. Whereas, the reality gap can be considered bridged in several\napplication scenarios, training on synthesized images containing reflecting\nmaterials requires further research. Since the appearance of objects with\nreflecting materials is dominated by the surrounding environment, this\ninteraction needs to be considered during training data generation. Therefore,\nwithin this paper we examine the effect of reflecting materials in the context\nof synthetic image generation for training object detectors. We investigate the\ninfluence of rendering approach used for image synthesis, the effect of domain\nrandomization, as well as the amount of used training data. To be able to\ncompare our results to the state-of-the-art, we focus on indoor scenes as they\nhave been investigated extensively. Within this scenario, bathroom furniture is\na natural choice for objects with reflecting materials, for which we report our\nfindings on real and synthetic testing data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 13:27:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Hartwig", "Sebastian", ""], ["Ropinski", "Timo", ""]]}, {"id": "1904.00830", "submitter": "Wenbo Bao", "authors": "Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao,\n  Ming-Hsuan Yang", "title": "Depth-Aware Video Frame Interpolation", "comments": "This work is accepted in CVPR 2019. The source code and pre-trained\n  model are available on https://github.com/baowenbo/DAIN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation aims to synthesize nonexistent frames in-between\nthe original frames. While significant advances have been made from the recent\ndeep convolutional neural networks, the quality of interpolation is often\nreduced due to large object motion or occlusion. In this work, we propose a\nvideo frame interpolation method which explicitly detects the occlusion by\nexploring the depth information. Specifically, we develop a depth-aware flow\nprojection layer to synthesize intermediate flows that preferably sample closer\nobjects than farther ones. In addition, we learn hierarchical features to\ngather contextual information from neighboring pixels. The proposed model then\nwarps the input frames, depth maps, and contextual features based on the\noptical flow and local interpolation kernels for synthesizing the output frame.\nOur model is compact, efficient, and fully differentiable. Quantitative and\nqualitative results demonstrate that the proposed model performs favorably\nagainst state-of-the-art frame interpolation methods on a wide variety of\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:19:59 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bao", "Wenbo", ""], ["Lai", "Wei-Sheng", ""], ["Ma", "Chao", ""], ["Zhang", "Xiaoyun", ""], ["Gao", "Zhiyong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1904.00838", "submitter": "Changhee Han", "authors": "Changhee Han, Kohei Murao, Shin'ichi Satoh, Hideki Nakayama", "title": "Learning More with Less: GAN-based Medical Image Augmentation", "comments": "6 pages, 2 figures, to appear in MEDICAL IMAGING TECHNOLOGY Special\n  Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN)-based accurate prediction typically\nrequires large-scale annotated training data. In Medical Imaging, however, both\nobtaining medical data and annotating them by expert physicians are\nchallenging; to overcome this lack of data, Data Augmentation (DA) using\nGenerative Adversarial Networks (GANs) is essential, since they can synthesize\nadditional annotated training data to handle small and fragmented medical\nimages from various scanners--those generated images, realistic but completely\nnovel, can further fill the real image distribution uncovered by the original\ndataset. As a tutorial, this paper introduces GAN-based Medical Image\nAugmentation, along with tricks to boost classification/object\ndetection/segmentation performance using them, based on our experience and\nrelated work. Moreover, we show our first GAN-based DA work using automatic\nbounding box annotation, for robust CNN-based brain metastases detection on 256\nx 256 MR images; GAN-based DA can boost 10% sensitivity in diagnosis with a\nclinically acceptable number of additional False Positives, even with\nhighly-rough and inconsistent bounding boxes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:41:28 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 15:21:40 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 13:08:32 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Han", "Changhee", ""], ["Murao", "Kohei", ""], ["Satoh", "Shin'ichi", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1904.00839", "submitter": "David Dov", "authors": "David Dov, Shahar Kovalsky, Jonathan Cohen, Danielle Range, Ricardo\n  Henao, and Lawrence Carin", "title": "Thyroid Cancer Malignancy Prediction From Whole Slide Cytopathology\n  Images", "comments": null, "journal-ref": "Proceedings of Machine Learning Research, 2019, Vol. 106", "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider preoperative prediction of thyroid cancer based on\nultra-high-resolution whole-slide cytopathology images. Inspired by how human\nexperts perform diagnosis, our approach first identifies and classifies\ndiagnostic image regions containing informative thyroid cells, which only\ncomprise a tiny fraction of the entire image. These local estimates are then\naggregated into a single prediction of thyroid malignancy. Several unique\ncharacteristics of thyroid cytopathology guide our deep-learning-based\napproach. While our method is closely related to multiple-instance learning, it\ndeviates from these methods by using a supervised procedure to extract\ndiagnostically relevant regions. Moreover, we propose to simultaneously predict\nthyroid malignancy, as well as a diagnostic score assigned by a human expert,\nwhich further allows us to devise an improved training strategy. Experimental\nresults show that the proposed algorithm achieves performance comparable to\nhuman experts, and demonstrate the potential of using the algorithm for\nscreening and as an assistive tool for the improved diagnosis of indeterminate\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 17:47:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Dov", "David", ""], ["Kovalsky", "Shahar", ""], ["Cohen", "Jonathan", ""], ["Range", "Danielle", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1904.00842", "submitter": "Daniel Bauer", "authors": "Daniel Bauer, Lars Kuhnert, Lutz Eckstein", "title": "Deep, spatially coherent Inverse Sensor Models with Uncertainty\n  Incorporation using the evidential Framework", "comments": "Submitted for Intelligent Vehicle Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform high speed tasks, sensors of autonomous cars have to provide as\nmuch information in as few time steps as possible. However, radars, one of the\nsensor modalities autonomous cars heavily rely on, often only provide sparse,\nnoisy detections. These have to be accumulated over time to reach a high enough\nconfidence about the static parts of the environment. For radars, the state is\ntypically estimated by accumulating inverse detection models (IDMs). We employ\nthe recently proposed evidential convolutional neural networks which, in\ncontrast to IDMs, compute dense, spatially coherent inference of the\nenvironment state. Moreover, these networks are able to incorporate sensor\nnoise in a principled way which we further extend to also incorporate model\nuncertainty. We present experimental results that show This makes it possible\nto obtain a denser environment perception in fewer time steps.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 11:50:13 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bauer", "Daniel", ""], ["Kuhnert", "Lars", ""], ["Eckstein", "Lutz", ""]]}, {"id": "1904.00853", "submitter": "Eran Goldman", "authors": "Eran Goldman, Roei Herzig, Aviv Eisenschtat, Oria Ratzon, Itsik Levi,\n  Jacob Goldberger, Tal Hassner", "title": "Precise Detection in Densely Packed Scenes", "comments": "CVPR 2019", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Man-made scenes can be densely packed, containing numerous objects, often\nidentical, positioned in close proximity. We show that precise object detection\nin such scenes remains a challenging frontier even for state-of-the-art object\ndetectors. We propose a novel, deep-learning based method for precise object\ndetection, designed for such challenging settings. Our contributions include:\n(1) A layer for estimating the Jaccard index as a detection quality score; (2)\na novel EM merging unit, which uses our quality scores to resolve detection\noverlap ambiguities; finally, (3) an extensive, annotated data set, SKU-110K,\nrepresenting packed retail environments, released for training and testing\nunder such extreme settings. Detection tests on SKU-110K and counting tests on\nthe CARPK and PUCPR+ show our method to outperform existing state-of-the-art\nwith substantial margins. The code and data will be made available on\n\\url{www.github.com/eg4000/SKU110K_CVPR19}.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:53:05 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 21:22:19 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 17:46:20 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Goldman", "Eran", ""], ["Herzig", "Roei", ""], ["Eisenschtat", "Aviv", ""], ["Ratzon", "Oria", ""], ["Levi", "Itsik", ""], ["Goldberger", "Jacob", ""], ["Hassner", "Tal", ""]]}, {"id": "1904.00863", "submitter": "Nantheera Anantrasirichai", "authors": "N. Anantrasirichai and David Bull", "title": "DefectNET: multi-class fault detection on highly-imbalanced datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a data-driven method, the performance of deep convolutional neural\nnetworks (CNN) relies heavily on training data. The prediction results of\ntraditional networks give a bias toward larger classes, which tend to be the\nbackground in the semantic segmentation task. This becomes a major problem for\nfault detection, where the targets appear very small on the images and vary in\nboth types and sizes. In this paper we propose a new network architecture,\nDefectNet, that offers multi-class (including but not limited to) defect\ndetection on highly-imbalanced datasets. DefectNet consists of two parallel\npaths, which are a fully convolutional network and a dilated convolutional\nnetwork to detect large and small objects respectively. We propose a hybrid\nloss maximising the usefulness of a dice loss and a cross entropy loss, and we\nalso employ the leaky rectified linear unit (ReLU) to deal with rare occurrence\nof some targets in training batches. The prediction results show that our\nDefectNet outperforms state-of-the-art networks for detecting multi-class\ndefects with the average accuracy improvement of approximately 10% on a wind\nturbine.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:05:29 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 09:23:43 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Anantrasirichai", "N.", ""], ["Bull", "David", ""]]}, {"id": "1904.00865", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Juliette Rengot", "title": "Non-linear aggregation of filters to improve image denoising", "comments": "To appear at Computing Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a novel aggregation method to efficiently perform image\ndenoising. Preliminary filters are aggregated in a non-linear fashion, using a\nnew metric of pixel proximity based on how the pool of filters reaches a\nconsensus. We provide a theoretical bound to support our aggregation scheme,\nits numerical performance is illustrated and we show that the aggregate\nsignificantly outperforms each of the preliminary filters.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:10:21 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 18:54:55 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 15:43:09 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Guedj", "Benjamin", ""], ["Rengot", "Juliette", ""]]}, {"id": "1904.00876", "submitter": "Yawei Luo", "authors": "Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang", "title": "Significance-aware Information Bottleneck for Domain Adaptive Semantic\n  Segmentation", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For unsupervised domain adaptation problems, the strategy of aligning the two\ndomains in latent feature space through adversarial learning has achieved much\nprogress in image classification, but usually fails in semantic segmentation\ntasks in which the latent representations are overcomplex. In this work, we\nequip the adversarial network with a \"significance-aware information bottleneck\n(SIB)\", to address the above problem. The new network structure, called SIBAN,\nenables a significance-aware feature purification before the adversarial\nadaptation, which eases the feature alignment and stabilizes the adversarial\ntraining course. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and\nSYNTHIA -> Cityscapes, we validate that the proposed method can yield leading\nresults compared with other feature-space alternatives. Moreover, SIBAN can\neven match the state-of-the-art output-space methods in segmentation accuracy,\nwhile the latter are often considered to be better choices for domain adaptive\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:19:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Luo", "Yawei", ""], ["Liu", "Ping", ""], ["Guan", "Tao", ""], ["Yu", "Junqing", ""], ["Yang", "Yi", ""]]}, {"id": "1904.00887", "submitter": "Aamir Mustafa", "authors": "Aamir Mustafa, Salman Khan, Munawar Hayat, Roland Goecke, Jianbing\n  Shen, Ling Shao", "title": "Adversarial Defense by Restricting the Hidden Space of Deep Neural\n  Networks", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks, which can fool\nthem by adding minuscule perturbations to the input images. The robustness of\nexisting defenses suffers greatly under white-box attack settings, where an\nadversary has full knowledge about the network and can iterate several times to\nfind strong perturbations. We observe that the main reason for the existence of\nsuch perturbations is the close proximity of different class samples in the\nlearned feature space. This allows model decisions to be totally changed by\nadding an imperceptible perturbation in the inputs. To counter this, we propose\nto class-wise disentangle the intermediate feature representations of deep\nnetworks. Specifically, we force the features for each class to lie inside a\nconvex polytope that is maximally separated from the polytopes of other\nclasses. In this manner, the network is forced to learn distinct and distant\ndecision regions for each class. We observe that this simple constraint on the\nfeatures greatly enhances the robustness of learned models, even against the\nstrongest white-box attacks, without degrading the classification performance\non clean images. We report extensive evaluations in both black-box and\nwhite-box attack scenarios and show significant gains in comparison to\nstate-of-the art defenses.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:42:38 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 13:19:07 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 06:10:01 GMT"}, {"version": "v4", "created": "Sun, 28 Jul 2019 08:53:05 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Mustafa", "Aamir", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Goecke", "Roland", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "1904.00889", "submitter": "Axel Barroso Laguna", "authors": "Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, Krystian Mikolajczyk", "title": "Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters", "comments": null, "journal-ref": "International Conference on Computer Vision (ICCV) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for keypoint detection task that combines\nhandcrafted and learned CNN filters within a shallow multi-scale architecture.\nHandcrafted filters provide anchor structures for learned filters, which\nlocalize, score and rank repeatable features. Scale-space representation is\nused within the network to extract keypoints at different levels. We design a\nloss function to detect robust features that exist across a range of scales and\nto maximize the repeatability score. Our Key.Net model is trained on data\nsynthetically created from ImageNet and evaluated on HPatches benchmark.\nResults show that our approach outperforms state-of-the-art detectors in terms\nof repeatability, matching performance and complexity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:47:24 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 14:05:11 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2019 15:13:02 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Barroso-Laguna", "Axel", ""], ["Riba", "Edgar", ""], ["Ponsa", "Daniel", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1904.00906", "submitter": "Dongming Wei", "authors": "Fenqiang Zhao, Shunren Xia, Zhengwang Wu, Dingna Duan, Li Wang, Weili\n  Lin, John H Gilmore, Dinggang Shen, Gang Li", "title": "Spherical U-Net on Cortical Surfaces: Methods and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been providing the state-of-the-art\nperformance for learning-related problems involving 2D/3D images in Euclidean\nspace. However, unlike in the Euclidean space, the shapes of many structures in\nmedical imaging have a spherical topology in a manifold space, e.g., brain\ncortical or subcortical surfaces represented by triangular meshes, with large\ninter-subject and intrasubject variations in vertex number and local\nconnectivity. Hence, there is no consistent neighborhood definition and thus no\nstraightforward convolution/transposed convolution operations for\ncortical/subcortical surface data. In this paper, by leveraging the regular and\nconsistent geometric structure of the resampled cortical surface mapped onto\nthe spherical space, we propose a novel convolution filter analogous to the\nstandard convolution on the image grid. Accordingly, we develop corresponding\noperations for convolution, pooling, and transposed convolution for spherical\nsurface data and thus construct spherical CNNs. Specifically, we propose the\nSpherical U-Net architecture by replacing all operations in the standard U-Net\nwith their spherical operation counterparts. We then apply the Spherical U-Net\nto two challenging and neuroscientifically important tasks in infant brains:\ncortical surface parcellation and cortical attribute map development\nprediction. Both applications demonstrate the competitive performance in the\naccuracy, computational efficiency, and effectiveness of our proposed Spherical\nU-Net, in comparison with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 15:18:53 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhao", "Fenqiang", ""], ["Xia", "Shunren", ""], ["Wu", "Zhengwang", ""], ["Duan", "Dingna", ""], ["Wang", "Li", ""], ["Lin", "Weili", ""], ["Gilmore", "John H", ""], ["Shen", "Dinggang", ""], ["Li", "Gang", ""]]}, {"id": "1904.00912", "submitter": "Fabio Cermelli", "authors": "Fabio Cermelli, Massimiliano Mancini, Elisa Ricci, Barbara Caputo", "title": "The RGB-D Triathlon: Towards Agile Visual Toolboxes for Robots", "comments": "This work has been submitted to IROS/RAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have brought significant advances in robot perception, enabling\nto improve the capabilities of robots in several visual tasks, ranging from\nobject detection and recognition to pose estimation, semantic scene\nsegmentation and many others. Still, most approaches typically address visual\ntasks in isolation, resulting in overspecialized models which achieve strong\nperformances in specific applications but work poorly in other (often related)\ntasks. This is clearly sub-optimal for a robot which is often required to\nperform simultaneously multiple visual recognition tasks in order to properly\nact and interact with the environment. This problem is exacerbated by the\nlimited computational and memory resources typically available onboard to a\nrobotic platform. The problem of learning flexible models which can handle\nmultiple tasks in a lightweight manner has recently gained attention in the\ncomputer vision community and benchmarks supporting this research have been\nproposed. In this work we study this problem in the robot vision context,\nproposing a new benchmark, the RGB-D Triathlon, and evaluating state of the art\nalgorithms in this novel challenging scenario. We also define a new evaluation\nprotocol, better suited to the robot vision setting. Results shed light on the\nstrengths and weaknesses of existing approaches and on open issues, suggesting\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 15:33:02 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 11:59:33 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Cermelli", "Fabio", ""], ["Mancini", "Massimiliano", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""]]}, {"id": "1904.00923", "submitter": "Matthew Wicker", "authors": "Matthew Wicker, Marta Kwiatkowska", "title": "Robustness of 3D Deep Learning in an Adversarial Setting", "comments": "10 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spatial arrangement and nature of real-world objects is of\nparamount importance to many complex engineering tasks, including autonomous\nnavigation. Deep learning has revolutionized state-of-the-art performance for\ntasks in 3D environments; however, relatively little is known about the\nrobustness of these approaches in an adversarial setting. The lack of\ncomprehensive analysis makes it difficult to justify deployment of 3D deep\nlearning models in real-world, safety-critical applications. In this work, we\ndevelop an algorithm for analysis of pointwise robustness of neural networks\nthat operate on 3D data. We show that current approaches presented for\nunderstanding the resilience of state-of-the-art models vastly overestimate\ntheir robustness. We then use our algorithm to evaluate an array of\nstate-of-the-art models in order to demonstrate their vulnerability to\nocclusion attacks. We show that, in the worst case, these networks can be\nreduced to 0% classification accuracy after the occlusion of at most 6.5% of\nthe occupied input space.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 15:51:12 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wicker", "Matthew", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "1904.00937", "submitter": "Can Jozef Saul", "authors": "Can Jozef Saul, Deniz Yagmur Urey, Can Doruk Taktakoglu", "title": "Early Diagnosis of Pneumonia with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumonia has been one of the fatal diseases and has the potential to result\nin severe consequences within a short period of time, due to the flow of fluid\nin lungs, which leads to drowning. If not acted upon by drugs at the right\ntime, pneumonia may result in death of individuals. Therefore, the early\ndiagnosis is a key factor along the progress of the disease. This paper focuses\non the biological progress of pneumonia and its detection by x-ray imaging,\noverviews the studies conducted on enhancing the level of diagnosis, and\npresents the methodology and results of an automation of xray images based on\nvarious parameters in order to detect the disease at very early stages. In this\nstudy we propose our deep learning architecture for the classification task,\nwhich is trained with modified images, through multiple steps of preprocessing.\nOur classification method uses convolutional neural networks and residual\nnetwork architecture for classifying the images. Our findings yield an accuracy\nof 78.73%, surpassing the previously top scoring accuracy of 76.8%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:18:35 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Saul", "Can Jozef", ""], ["Urey", "Deniz Yagmur", ""], ["Taktakoglu", "Can Doruk", ""]]}, {"id": "1904.00979", "submitter": "Yingwei Li", "authors": "Yingwei Li, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen and Alan\n  L. Yuille", "title": "Regional Homogeneity: Towards Learning Transferable Universal\n  Adversarial Perturbations Against Defenses", "comments": "ECCV 2020. Project page:\n  https://github.com/LiYingwei/Regional-Homogeneity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on learning transferable adversarial examples specifically\nagainst defense models (models to defense adversarial attacks). In particular,\nwe show that a simple universal perturbation can fool a series of\nstate-of-the-art defenses.\n  Adversarial examples generated by existing attacks are generally hard to\ntransfer to defense models. We observe the property of regional homogeneity in\nadversarial perturbations and suggest that the defenses are less robust to\nregionally homogeneous perturbations. Therefore, we propose an effective\ntransforming paradigm and a customized gradient transformer module to transform\nexisting perturbations into regionally homogeneous ones. Without explicitly\nforcing the perturbations to be universal, we observe that a well-trained\ngradient transformer module tends to output input-independent gradients (hence\nuniversal) benefiting from the under-fitting phenomenon. Thorough experiments\ndemonstrate that our work significantly outperforms the prior art attacking\nalgorithms (either image-dependent or universal ones) by an average improvement\nof 14.0% when attacking 9 defenses in the transfer-based attack setting. In\naddition to the cross-model transferability, we also verify that regionally\nhomogeneous perturbations can well transfer across different vision tasks\n(attacking with the semantic segmentation task and testing on the object\ndetection task). The code is available here:\nhttps://github.com/LiYingwei/Regional-Homogeneity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:31:02 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 01:42:37 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Li", "Yingwei", ""], ["Bai", "Song", ""], ["Xie", "Cihang", ""], ["Liao", "Zhenyu", ""], ["Shen", "Xiaohui", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1904.00982", "submitter": "Marek Wodzinski", "authors": "Marek Wodzinski and Andrzej Skalski", "title": "Automatic Nonrigid Histological Image Registration with Adaptive\n  Multistep Algorithm", "comments": "Submission to ANHIR challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a short description of the method proposed to ANHIR\nchallenge organized jointly with the IEEE ISBI 2019 conference. We propose a\nmethod consisting of preprocessing, initial alignment, nonrigid registration\nalgorithms and a method to automatically choose the best result. The method\nturned out to be robust (99.792% robustness) and accurate (0.38% average median\nrTRE). The main drawback of the proposed method is relatively high computation\ntime. However, this aspect can be easily improved by cleaning the code and\nproposing a GPU implementation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:38:05 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wodzinski", "Marek", ""], ["Skalski", "Andrzej", ""]]}, {"id": "1904.00993", "submitter": "Carlos Esteves", "authors": "Carlos Esteves, Yinshuang Xu, Christine Allen-Blanchette, Kostas\n  Daniilidis", "title": "Equivariant Multi-View Networks", "comments": "Camera-ready. Accepted to ICCV'19 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several popular approaches to 3D vision tasks process multiple views of the\ninput independently with deep neural networks pre-trained on natural images,\nachieving view permutation invariance through a single round of pooling over\nall views. We argue that this operation discards important information and\nleads to subpar global descriptors. In this paper, we propose a group\nconvolutional approach to multiple view aggregation where convolutions are\nperformed over a discrete subgroup of the rotation group, enabling, thus, joint\nreasoning over all views in an equivariant (instead of invariant) fashion, up\nto the very last layer. We further develop this idea to operate on smaller\ndiscrete homogeneous spaces of the rotation group, where a polar view\nrepresentation is used to maintain equivariance with only a fraction of the\nnumber of input views. We set the new state of the art in several large scale\n3D shape retrieval tasks, and show additional applications to panoramic scene\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:58:17 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 14:48:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Esteves", "Carlos", ""], ["Xu", "Yinshuang", ""], ["Allen-Blanchette", "Christine", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1904.01080", "submitter": "Lee Clement", "authors": "Lee Clement, Mona Gridseth, Justin Tomasi and Jonathan Kelly", "title": "Learning Matchable Image Transformations for Long-term Metric Visual\n  Localization", "comments": "In IEEE Robotics and Automation Letters (RA-L) and presented at the\n  IEEE International Conference on Robotics and Automation (ICRA'20), Paris,\n  France, May 31-June 4, 2020", "journal-ref": null, "doi": "10.1109/LRA.2020.2967659", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term metric self-localization is an essential capability of autonomous\nmobile robots, but remains challenging for vision-based systems due to\nappearance changes caused by lighting, weather, or seasonal variations. While\nexperience-based mapping has proven to be an effective technique for bridging\nthe `appearance gap,' the number of experiences required for reliable metric\nlocalization over days or months can be very large, and methods for reducing\nthe necessary number of experiences are needed for this approach to scale.\nTaking inspiration from color constancy theory, we learn a nonlinear\nRGB-to-grayscale mapping that explicitly maximizes the number of inlier feature\nmatches for images captured under different lighting and weather conditions,\nand use it as a pre-processing step in a conventional single-experience\nlocalization pipeline to improve its robustness to appearance change. We train\nthis mapping by approximating the target non-differentiable localization\npipeline with a deep neural network, and find that incorporating a learned\nlow-dimensional context feature can further improve cross-appearance feature\nmatching. Using synthetic and real-world datasets, we demonstrate substantial\nimprovements in localization performance across day-night cycles, enabling\ncontinuous metric localization over a 30-hour period using a single mapping\nexperience, and allowing experience-based localization to scale to long\ndeployments with dramatically reduced data requirements.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 19:38:56 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 17:06:19 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 03:53:09 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 20:23:40 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Clement", "Lee", ""], ["Gridseth", "Mona", ""], ["Tomasi", "Justin", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1904.01091", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Debayan Deb, Anil K. Jain, Prem S. Sudhish, Anjoo\n  Bhatnager", "title": "Infant-Prints: Fingerprints for Reducing Infant Mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In developing countries around the world, a multitude of infants continue to\nsuffer and die from vaccine-preventable diseases, and malnutrition. Lamentably,\nthe lack of any official identification documentation makes it exceedingly\ndifficult to prevent these infant deaths. To solve this global crisis, we\npropose Infant-Prints which is comprised of (i) a custom, compact, low-cost (85\nUSD), high-resolution (1,900 ppi) fingerprint reader, (ii) a high-resolution\nfingerprint matcher, and (iii) a mobile application for search and verification\nfor the infant fingerprint. Using Infant-Prints, we have collected a\nlongitudinal database of infant fingerprints and demonstrate its ability to\nperform accurate and reliable recognition of infants enrolled at the ages 0-3\nmonths, in time for effective delivery of critical vaccinations and nutritional\nsupplements (TAR=90% @ FAR = 0.1% for infants older than 8 weeks).\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 20:03:40 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Deb", "Debayan", ""], ["Jain", "Anil K.", ""], ["Sudhish", "Prem S.", ""], ["Bhatnager", "Anjoo", ""]]}, {"id": "1904.01099", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Kai Cao, Anil K. Jain", "title": "Fingerprints: Fixed Length Representation via Deep Networks and Domain\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn a discriminative fixed length feature representation of fingerprints\nwhich stands in contrast to commonly used unordered, variable length sets of\nminutiae points. To arrive at this fixed length representation, we embed\nfingerprint domain knowledge into a multitask deep convolutional neural network\narchitecture. Empirical results, on two public-domain fingerprint databases\n(NIST SD4 and FVC 2004 DB1) show that compared to minutiae representations,\nextracted by two state-of-the-art commercial matchers (Verifinger v6.3 and\nInnovatrics v2.0.3), our fixed-length representations provide (i) higher search\naccuracy: Rank-1 accuracy of 97.9% vs. 97.3% on NIST SD4 against a gallery size\nof 2000 and (ii) significantly faster, large scale search: 682,594 matches per\nsecond vs. 22 matches per second for commercial matchers on an i5 3.3 GHz\nprocessor with 8 GB of RAM.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 20:41:43 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1904.01109", "submitter": "Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny and Mohamed Elfeki", "title": "Creativity Inspired Zero-Shot Learning", "comments": "This paper was published at the International Conference on Computer\n  Vision 2019, Seoul, South Korea,\n  http://openaccess.thecvf.com/content_ICCV_2019/papers/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.pdf", "journal-ref": "International Conference on Computer Vision-2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims at understanding unseen categories with no\ntraining examples from class-level descriptions. To improve the discriminative\npower of zero-shot learning, we model the visual learning process of unseen\ncategories with inspiration from the psychology of human creativity for\nproducing novel art. We relate ZSL to human creativity by observing that\nzero-shot learning is about recognizing the unseen and creativity is about\ncreating a likable unseen. We introduce a learning signal inspired by\ncreativity literature that explores the unseen space with hallucinated\nclass-descriptions and encourages careful deviation of their visual feature\ngenerations from seen classes while allowing knowledge transfer from seen to\nunseen classes. Empirically, we show consistent improvement over the state of\nthe art of several percents on the largest available benchmarks on the\nchallenging task or generalized ZSL from a noisy text that we focus on, using\nthe CUB and NABirds datasets. We also show the advantage of our approach on\nAttribute-based ZSL on three additional datasets (AwA2, aPY, and SUN). Code is\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:05:23 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 17:18:51 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 21:43:56 GMT"}, {"version": "v4", "created": "Wed, 17 Apr 2019 02:57:29 GMT"}, {"version": "v5", "created": "Sun, 1 Dec 2019 19:27:06 GMT"}, {"version": "v6", "created": "Tue, 3 Dec 2019 02:42:29 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elfeki", "Mohamed", ""]]}, {"id": "1904.01112", "submitter": "Florian Knoll", "authors": "Florian Knoll, Kerstin Hammernik, Chi Zhang, Steen Moeller, Thomas\n  Pock, Daniel K. Sodickson, Mehmet Akcakaya", "title": "Deep Learning Methods for Parallel Magnetic Resonance Image\n  Reconstruction", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the success of deep learning in a wide range of applications,\nneural network-based machine learning techniques have received interest as a\nmeans of accelerating magnetic resonance imaging (MRI). A number of ideas\ninspired by deep learning techniques from computer vision and image processing\nhave been successfully applied to non-linear image reconstruction in the spirit\nof compressed sensing for both low dose computed tomography and accelerated\nMRI. The additional integration of multi-coil information to recover missing\nk-space lines in the MRI reconstruction process, is still studied less\nfrequently, even though it is the de-facto standard for currently used\naccelerated MR acquisitions. This manuscript provides an overview of the recent\nmachine learning approaches that have been proposed specifically for improving\nparallel imaging. A general background introduction to parallel MRI is given\nthat is structured around the classical view of image space and k-space based\nmethods. Both linear and non-linear methods are covered, followed by a\ndiscussion of recent efforts to further improve parallel imaging using machine\nlearning, and specifically using artificial neural networks. Image-domain based\ntechniques that introduce improved regularizers are covered as well as k-space\nbased methods, where the focus is on better interpolation strategies using\nneural networks. Issues and open problems are discussed as well as recent\nefforts for producing open datasets and benchmarks for the community.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:23:23 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Knoll", "Florian", ""], ["Hammernik", "Kerstin", ""], ["Zhang", "Chi", ""], ["Moeller", "Steen", ""], ["Pock", "Thomas", ""], ["Sodickson", "Daniel K.", ""], ["Akcakaya", "Mehmet", ""]]}, {"id": "1904.01114", "submitter": "Samuel Albanie", "authors": "Samuel Albanie, James Thewlis, Sebastien Ehrhardt, Joao Henriques", "title": "Deep Industrial Espionage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of deep learning is now considered largely solved, and is well\nunderstood by researchers and influencers alike. To maintain our relevance, we\ntherefore seek to apply our skills to under-explored, lucrative applications of\nthis technology. To this end, we propose and Deep Industrial Espionage, an\nefficient end-to-end framework for industrial information propagation and\nproductisation. Specifically, given a single image of a product or service, we\naim to reverse-engineer, rebrand and distribute a copycat of the product at a\nprofitable price-point to consumers in an emerging market---all within in a\nsingle forward pass of a Neural Network. Differently from prior work in machine\nperception which has been restricted to classifying, detecting and reasoning\nabout object instances, our method offers tangible business value in a wide\nrange of corporate settings. Our approach draws heavily on a promising recent\narxiv paper until its original authors' names can no longer be read (we use\nfelt tip pen). We then rephrase the anonymised paper, add the word \"novel\" to\nthe title, and submit it a prestigious, closed-access espionage journal who\nassure us that someday, we will be entitled to some fraction of their\nextortionate readership fees.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:27:52 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Albanie", "Samuel", ""], ["Thewlis", "James", ""], ["Ehrhardt", "Sebastien", ""], ["Henriques", "Joao", ""]]}, {"id": "1904.01121", "submitter": "Sharon Zhou", "authors": "Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Li\n  Fei-Fei, Michael S. Bernstein", "title": "HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative\n  Models", "comments": "https://hype.stanford.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models often use human evaluations to measure the perceived\nquality of their outputs. Automated metrics are noisy indirect proxies, because\nthey rely on heuristics or pretrained embeddings. However, up until now, direct\nhuman evaluation strategies have been ad-hoc, neither standardized nor\nvalidated. Our work establishes a gold standard human benchmark for generative\nrealism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark\nthat is (1) grounded in psychophysics research in perception, (2) reliable\nacross different sets of randomly sampled outputs from a model, (3) able to\nproduce separable model performances, and (4) efficient in cost and time. We\nintroduce two variants: one that measures visual perception under adaptive time\nconstraints to determine the threshold at which a model's outputs appear real\n(e.g. 250ms), and the other a less expensive variant that measures human error\nrate on fake and real images sans time constraints. We test HYPE across six\nstate-of-the-art generative adversarial networks and two sampling techniques on\nconditional and unconditional image generation using four datasets: CelebA,\nFFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements\nacross training epochs, and we confirm via bootstrap sampling that HYPE\nrankings are consistent and replicable.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:48:41 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 03:58:24 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 05:35:31 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 23:43:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Zhou", "Sharon", ""], ["Gordon", "Mitchell L.", ""], ["Krishna", "Ranjay", ""], ["Narcomey", "Austin", ""], ["Fei-Fei", "Li", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1904.01133", "submitter": "Amro Alasta", "authors": "Amro F. Alasta, Abdrazag Algamudi, Fatma Almesrati, Mustapha Meftah\n  and Rami Qahwaji", "title": "Filling Factors of Sunspots in SODISM Images", "comments": "11 pages, 7 figures, 2 tables This article is an extension of our\n  previous studies investigating the detection of sunspots using SODISM images.\n  The paper presented in August 2018 at the IEEE International Conference on\n  Computing, Electronics and Communications Engineering.\n  http://aetic.theiaer.org/archive/v3/v3n2/p1.html", "journal-ref": "Published by International Association of Educators and\n  Researchers (IAER ) Annals of Emerging Technologies in Computing (AETiC) Vol.\n  3, No. 2, 2019", "doi": "10.33166/AETiC.2019.02.001", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Received: 1st December 2018; Accepted: 18th February 2019; Published: 1st\nApril 2019 Abstract: The calculated filling factors (FFs) for a feature reflect\nthe fraction of the solar disc covered by that feature, and the assignment of\nreference synthetic spectra. In this paper, the FFs, specified as a function of\nradial position on the solar disc, are computed for each image in a tabular\nform. The filling factor (FF) is an important parameter and is defined as the\nfraction of area in a pixel covered with the magnetic field, whereas the rest\nof the area in the pixel is field-free. However, this does not provide\nextensive information about the experiments conducted on tens or hundreds of\nsuch images. This is the first time that filling factors for SODISM images have\nbeen catalogued in tabular formation. This paper presents a new method that\nprovides the means to detect sunspots on full-disk solar images recorded by the\nSolar Diameter Imager and Surface Mapper (SODISM) on the PICARD satellite. The\nmethod is a totally automated detection process that achieves a sunspot\nrecognition rate of 97.6%. The number of sunspots detected by this method\nstrongly agrees with the NOAA catalogue. The sunspot areas calculated by this\nmethod have a 99% correlation with SOHO over the same period, and thus help to\ncalculate the filling factor for wavelength (W.L.) 607nm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 22:41:53 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Alasta", "Amro F.", ""], ["Algamudi", "Abdrazag", ""], ["Almesrati", "Fatma", ""], ["Meftah", "Mustapha", ""], ["Qahwaji", "Rami", ""]]}, {"id": "1904.01143", "submitter": "Duygu Sarikaya", "authors": "Duygu Sarikaya, Pierre Jannin", "title": "Surgical Gesture Recognition with Optical Flow only", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the open research problem of surgical gesture\nrecognition using motion cues from video data only. We adapt Optical flow\nConvNets initially proposed by Simonyan et al.. While Simonyan uses both RGB\nframes and dense optical flow, we use only dense optical flow representations\nas input to emphasize the role of motion in surgical gesture recognition, and\npresent it as a robust alternative to kinematic data. We also overcome one of\nthe limitations of Optical flow ConvNets by initializing our model with cross\nmodality pre-training. A large number of promising studies that address\nsurgical gesture recognition highly rely on kinematic data which requires\nadditional recording devices. To our knowledge, this is the first paper that\naddresses surgical gesture recognition using dense optical flow information\nonly. We achieve competitive results on JIGSAWS dataset, moreover, our model\nachieves more robust results with less standard deviation, which suggests\noptical flow information can be used as an alternative to kinematic data for\nthe recognition of surgical gestures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 23:40:21 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 22:10:21 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sarikaya", "Duygu", ""], ["Jannin", "Pierre", ""]]}, {"id": "1904.01150", "submitter": "Qihang Yu", "authors": "Qihang Yu, Yingda Xia, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille", "title": "Thickened 2D Networks for Efficient 3D Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a debate in 3D medical image segmentation on whether to use 2D\nor 3D networks, where both pipelines have advantages and disadvantages. 2D\nmethods enjoy a low inference time and greater transfer-ability while 3D\nmethods are superior in performance for hard targets requiring contextual\ninformation. This paper investigates efficient 3D segmentation from another\nperspective, which uses 2D networks to mimic 3D segmentation. To compensate the\nlack of contextual information in 2D manner, we propose to thicken the 2D\nnetwork inputs by feeding multiple slices as multiple channels into 2D networks\nand thus 3D contextual information is incorporated. We also put forward to use\nearly-stage multiplexing and slice sensitive attention to solve the confusion\nproblem of information loss which occurs when 2D networks face thickened\ninputs. With this design, we achieve a higher performance while maintaining a\nlower inference latency on a few abdominal organs from CT scans, in particular\nwhen the organ has a peculiar 3D shape and thus strongly requires contextual\ninformation, demonstrating our method's effectiveness and ability in capturing\n3D information. We also point out that \"thickened\" 2D inputs pave a new method\nof 3D segmentation, and look forward to more efforts in this direction.\nExperiments on segmenting a few abdominal targets in particular blood vessels\nwhich require strong 3D contexts demonstrate the advantages of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 00:08:51 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 01:29:28 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Yu", "Qihang", ""], ["Xia", "Yingda", ""], ["Xie", "Lingxi", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1904.01160", "submitter": "Yucheng Shi", "authors": "Yucheng Shi, Siyu Wang, Yahong Han", "title": "Curls & Whey: Boosting Black-Box Adversarial Attacks", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classifiers based on deep neural networks suffer from harassment caused\nby adversarial examples. Two defects exist in black-box iterative attacks that\ngenerate adversarial examples by incrementally adjusting the noise-adding\ndirection for each step. On the one hand, existing iterative attacks add noises\nmonotonically along the direction of gradient ascent, resulting in a lack of\ndiversity and adaptability of the generated iterative trajectories. On the\nother hand, it is trivial to perform adversarial attack by adding excessive\nnoises, but currently there is no refinement mechanism to squeeze redundant\nnoises. In this work, we propose Curls & Whey black-box attack to fix the above\ntwo defects. During Curls iteration, by combining gradient ascent and descent,\nwe `curl' up iterative trajectories to integrate more diversity and\ntransferability into adversarial examples. Curls iteration also alleviates the\ndiminishing marginal effect in existing iterative attacks. The Whey\noptimization further squeezes the `whey' of noises by exploiting the robustness\nof adversarial perturbation. Extensive experiments on Imagenet and\nTiny-Imagenet demonstrate that our approach achieves impressive decrease on\nnoise magnitude in l2 norm. Curls & Whey attack also shows promising\ntransferability against ensemble models as well as adversarially trained\nmodels. In addition, we extend our attack to the targeted misclassification,\neffectively reducing the difficulty of targeted attacks under black-box\ncondition.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 01:16:01 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Shi", "Yucheng", ""], ["Wang", "Siyu", ""], ["Han", "Yahong", ""]]}, {"id": "1904.01169", "submitter": "Ming-Ming Cheng Prof.", "authors": "Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan\n  Yang, Philip Torr", "title": "Res2Net: A New Multi-scale Backbone Architecture", "comments": "11 pages, 7 figures", "journal-ref": "IEEE TPAMI 2021", "doi": "10.1109/TPAMI.2019.2938758", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing features at multiple scales is of great importance for numerous\nvision tasks. Recent advances in backbone convolutional neural networks (CNNs)\ncontinually demonstrate stronger multi-scale representation ability, leading to\nconsistent performance gains on a wide range of applications. However, most\nexisting methods represent the multi-scale features in a layer-wise manner. In\nthis paper, we propose a novel building block for CNNs, namely Res2Net, by\nconstructing hierarchical residual-like connections within one single residual\nblock. The Res2Net represents multi-scale features at a granular level and\nincreases the range of receptive fields for each network layer. The proposed\nRes2Net block can be plugged into the state-of-the-art backbone CNN models,\ne.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these\nmodels and demonstrate consistent performance gains over baseline models on\nwidely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies\nand experimental results on representative computer vision tasks, i.e., object\ndetection, class activation mapping, and salient object detection, further\nverify the superiority of the Res2Net over the state-of-the-art baseline\nmethods. The source code and trained models are available on\nhttps://mmcheng.net/res2net/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 01:56:34 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 01:55:50 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 09:55:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Gao", "Shang-Hua", ""], ["Cheng", "Ming-Ming", ""], ["Zhao", "Kai", ""], ["Zhang", "Xin-Yu", ""], ["Yang", "Ming-Hsuan", ""], ["Torr", "Philip", ""]]}, {"id": "1904.01175", "submitter": "Chloe LeGendre", "authors": "Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent\n  Charbonnel, Jay Busch, Paul Debevec", "title": "DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method to infer plausible high dynamic range\n(HDR), omnidirectional illumination given an unconstrained, low dynamic range\n(LDR) image from a mobile phone camera with a limited field of view (FOV). For\ntraining data, we collect videos of various reflective spheres placed within\nthe camera's FOV, leaving most of the background unoccluded, leveraging that\nmaterials with diverse reflectance functions reveal different lighting cues in\na single exposure. We train a deep neural network to regress from the LDR\nbackground image to HDR lighting by matching the LDR ground truth sphere images\nto those rendered with the predicted illumination using image-based relighting,\nwhich is differentiable. Our inference runs at interactive frame rates on a\nmobile device, enabling realistic rendering of virtual objects into real scenes\nfor mobile mixed reality. Training on automatically exposed and white-balanced\nvideos, we improve the realism of rendered objects compared to the state-of-the\nart methods for both indoor and outdoor scenes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:15:09 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["LeGendre", "Chloe", ""], ["Ma", "Wan-Chun", ""], ["Fyffe", "Graham", ""], ["Flynn", "John", ""], ["Charbonnel", "Laurent", ""], ["Busch", "Jay", ""], ["Debevec", "Paul", ""]]}, {"id": "1904.01178", "submitter": "Shahinur Alam", "authors": "Shahinur Alam, Mohammed Yeasin", "title": "Person Identification with Visual Summary for a Safe Access to a Smart\n  Home", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SafeAccess is an integrated system designed to provide easier and safer\naccess to a smart home for people with or without disabilities. The system is\ndesigned to enhance safety and promote the independence of people with\ndisability (i.e., visually impaired). The key functionality of the system\nincludes the detection and identification of human and generating contextual\nvisual summary from the real-time video streams obtained from the cameras\nplaced in strategic locations around the house. In addition, the system\nclassifies human into groups (i.e. friends/families/caregiver versus\nintruders/burglars/unknown). These features allow the user to grant/deny remote\naccess to the premises or ability to call emergency services. In this paper, we\nfocus on designing a prototype system for the smart home and building a robust\nrecognition engine that meets the system criteria and addresses speed,\naccuracy, deployment and environmental challenges under a wide variety of\npractical and real-life situations. To interact with the system, we implemented\na dialog enabled interface to create a personalized profile using face images\nor video of friend/families/caregiver. To improve computational efficiency, we\napply change detection to filter out frames and use Faster-RCNN to detect the\nhuman presence and extract faces using Multitask Cascaded Convolutional\nNetworks (MTCNN). Subsequently, we apply LBP/FaceNet to identify a person and\ngroups by matching extracted faces with the profile. SafeAccess sends a visual\nsummary to the users with an MMS containing a person's name if any match found\nor as \"Unknown\", scene image, facial description, and contextual information.\nSafeAccess identifies friends/families/caregiver versus intruders/unknown with\nan average F-score 0.97 and generates a visual summary from 10 classes with an\naverage accuracy of 98.01%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:25:31 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 23:24:00 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Alam", "Shahinur", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "1904.01186", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin\n  Shi, Chunjing Xu, Chao Xu, Qi Tian", "title": "Data-Free Learning of Student Networks", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning portable neural networks is very essential for computer vision for\nthe purpose that pre-trained heavy deep models can be well applied on edge\ndevices such as mobile phones and micro sensors. Most existing deep neural\nnetwork compression and speed-up methods are very effective for training\ncompact deep models, when we can directly access the training dataset. However,\ntraining data for the given deep network are often unavailable due to some\npractice problems (e.g. privacy, legal issue, and transmission), and the\narchitecture of the given network are also unknown except some interfaces. To\nthis end, we propose a novel framework for training efficient deep neural\nnetworks by exploiting generative adversarial networks (GANs). To be specific,\nthe pre-trained teacher networks are regarded as a fixed discriminator and the\ngenerator is utilized for derivating training samples which can obtain the\nmaximum response on the discriminator. Then, an efficient network with smaller\nmodel size and computational complexity is trained using the generated data and\nthe teacher network, simultaneously. Efficient student networks learned using\nthe proposed Data-Free Learning (DAFL) method achieve 92.22% and 74.47%\naccuracies using ResNet-18 without any training data on the CIFAR-10 and\nCIFAR-100 datasets, respectively. Meanwhile, our student network obtains an\n80.56% accuracy on the CelebA benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:00:06 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 06:50:22 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 06:54:30 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 06:58:35 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Yang", "Zhaohui", ""], ["Liu", "Chuanjian", ""], ["Shi", "Boxin", ""], ["Xu", "Chunjing", ""], ["Xu", "Chao", ""], ["Tian", "Qi", ""]]}, {"id": "1904.01189", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang, Cuiling Lan, Wenjun Zeng, Junliang Xing, Jianru Xue,\n  Nanning Zheng", "title": "Semantics-Guided Neural Networks for Efficient Skeleton-Based Human\n  Action Recognition", "comments": "Accepted by CVPR2020. The source code is available at\n  https://github.com/microsoft/SGN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition has attracted great interest thanks\nto the easy accessibility of the human skeleton data. Recently, there is a\ntrend of using very deep feedforward neural networks to model the 3D\ncoordinates of joints without considering the computational efficiency. In this\npaper, we propose a simple yet effective semantics-guided neural network (SGN)\nfor skeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability. In addition, we exploit the relationship\nof joints hierarchically through two modules, i.e., a joint-level module for\nmodeling the correlations of joints in the same frame and a framelevel module\nfor modeling the dependencies of frames by taking the joints in the same frame\nas a whole. A strong baseline is proposed to facilitate the study of this\nfield. With an order of magnitude smaller model size than most previous works,\nSGN achieves the state-of-the-art performance on the NTU60, NTU120, and SYSU\ndatasets. The source code is available at https://github.com/microsoft/SGN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:08:36 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:38:48 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 07:27:14 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Zhang", "Pengfei", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Xing", "Junliang", ""], ["Xue", "Jianru", ""], ["Zheng", "Nanning", ""]]}, {"id": "1904.01198", "submitter": "Poojan Oza", "authors": "Poojan Oza and Vishal M Patel", "title": "C2AE: Class Conditioned Auto-Encoder for Open-set Recognition", "comments": "CVPR2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models trained for classification often assume that all testing classes are\nknown while training. As a result, when presented with an unknown class during\ntesting, such closed-set assumption forces the model to classify it as one of\nthe known classes. However, in a real world scenario, classification models are\nlikely to encounter such examples. Hence, identifying those examples as unknown\nbecomes critical to model performance. A potential solution to overcome this\nproblem lies in a class of learning problems known as open-set recognition. It\nrefers to the problem of identifying the unknown classes during testing, while\nmaintaining performance on the known classes. In this paper, we propose an\nopen-set recognition algorithm using class conditioned auto-encoders with novel\ntraining and testing methodology. In contrast to previous methods, training\nprocedure is divided in two sub-tasks, 1. closed-set classification and, 2.\nopen-set identification (i.e. identifying a class as known or unknown). Encoder\nlearns the first task following the closed-set classification training\npipeline, whereas decoder learns the second task by reconstructing conditioned\non class identity. Furthermore, we model reconstruction errors using the\nExtreme Value Theory of statistical modeling to find the threshold for\nidentifying known/unknown class samples. Experiments performed on multiple\nimage classification datasets show proposed method performs significantly\nbetter than state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:47:39 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Oza", "Poojan", ""], ["Patel", "Vishal M", ""]]}, {"id": "1904.01201", "submitter": "Manolis Savva", "authors": "Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik\n  Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra\n  Malik, Devi Parikh, Dhruv Batra", "title": "Habitat: A Platform for Embodied AI Research", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Habitat, a platform for research in embodied artificial\nintelligence (AI). Habitat enables training embodied agents (virtual robots) in\nhighly efficient photorealistic 3D simulation. Specifically, Habitat consists\nof: (i) Habitat-Sim: a flexible, high-performance 3D simulator with\nconfigurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is\nfast -- when rendering a scene from Matterport3D, it achieves several thousand\nframes per second (fps) running single-threaded, and can reach over 10,000 fps\nmulti-process on a single GPU. (ii) Habitat-API: a modular high-level library\nfor end-to-end development of embodied AI algorithms -- defining tasks (e.g.,\nnavigation, instruction following, question answering), configuring, training,\nand benchmarking embodied agents.\n  These large-scale engineering contributions enable us to answer scientific\nquestions requiring experiments that were till now impracticable or 'merely'\nimpractical. Specifically, in the context of point-goal navigation: (1) we\nrevisit the comparison between learning and SLAM approaches from two recent\nworks and find evidence for the opposite conclusion -- that learning\noutperforms SLAM if scaled to an order of magnitude more experience than\nprevious investigations, and (2) we conduct the first cross-dataset\ngeneralization experiments {train, test} x {Matterport3D, Gibson} for multiple\nsensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors\ngeneralize across datasets. We hope that our open-source platform and these\nfindings will advance research in embodied AI.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:52:27 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 01:39:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Savva", "Manolis", ""], ["Kadian", "Abhishek", ""], ["Maksymets", "Oleksandr", ""], ["Zhao", "Yili", ""], ["Wijmans", "Erik", ""], ["Jain", "Bhavana", ""], ["Straub", "Julian", ""], ["Liu", "Jia", ""], ["Koltun", "Vladlen", ""], ["Malik", "Jitendra", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.01205", "submitter": "Rosalind Wang", "authors": "Mike Li, X. Rosalind Wang", "title": "Peak Alignment of Gas Chromatography-Mass Spectrometry Data with Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ChromAlignNet, a deep learning model for alignment of peaks in Gas\nChromatography-Mass Spectrometry (GC-MS) data. In GC-MS data, a compound's\nretention time (RT) may not stay fixed across multiple chromatograms. To use\nGC-MS data for biomarker discovery requires alignment of identical analyte's RT\nfrom different samples. Current methods of alignment are all based on a set of\nformal, mathematical rules. We present a solution to GC-MS alignment using deep\nlearning neural networks, which are more adept at complex, fuzzy data sets. We\ntested our model on several GC-MS data sets of various complexities and\nanalysed the alignment results quantitatively. We show the model has very good\nperformance (AUC $\\sim 1$ for simple data sets and AUC $\\sim 0.85$ for very\ncomplex data sets). Further, our model easily outperforms existing algorithms\non complex data sets. Compared with existing methods, ChromAlignNet is very\neasy to use as it requires no user input of reference chromatograms and\nparameters. This method can easily be adapted to other similar data such as\nthose from liquid chromatography. The source code is written in Python and\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 04:16:45 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 06:31:27 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 01:21:59 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Li", "Mike", ""], ["Wang", "X. Rosalind", ""]]}, {"id": "1904.01206", "submitter": "Dacheng Tao", "authors": "Zhe Chen, Jing Zhang, and Dacheng Tao", "title": "Progressive LiDAR Adaptation for Road Detection", "comments": null, "journal-ref": null, "doi": "10.1109/JAS.2019.1911459", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid developments in visual image-based road detection, robustly\nidentifying road areas in visual images remains challenging due to issues like\nillumination changes and blurry images. To this end, LiDAR sensor data can be\nincorporated to improve the visual image-based road detection, because LiDAR\ndata is less susceptible to visual noises. However, the main difficulty in\nintroducing LiDAR information into visual image-based road detection is that\nLiDAR data and its extracted features do not share the same space with the\nvisual data and visual features. Such gaps in spaces may limit the benefits of\nLiDAR information for road detection. To overcome this issue, we introduce a\nnovel Progressive LiDAR Adaptation-aided Road Detection (PLARD) approach to\nadapt LiDAR information into visual image-based road detection and improve\ndetection performance. In PLARD, progressive LiDAR adaptation consists of two\nsubsequent modules: 1) data space adaptation, which transforms the LiDAR data\nto the visual data space to align with the perspective view by applying\naltitude difference-based transformation; and 2) feature space adaptation,\nwhich adapts LiDAR features to visual features through a cascaded fusion\nstructure. Comprehensive empirical studies on the well-known KITTI road\ndetection benchmark demonstrate that PLARD takes advantage of both the visual\nand LiDAR information, achieving much more robust road detection even in\nchallenging urban scenes. In particular, PLARD outperforms other\nstate-of-the-art road detection models and is currently top of the publicly\naccessible benchmark leader-board.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 04:22:23 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Chen", "Zhe", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.01215", "submitter": "Prerana Mukherjee", "authors": "Prerana Mukherjee, Manoj Sharma, Megh Makwana, Ajay Pratap Singh,\n  Avinash Upadhyay, Akkshita Trivedi, Brejesh Lall and Santanu Chaudhury", "title": "DSAL-GAN: Denoising based Saliency Prediction with Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high quality saliency maps from noisy images is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing techniques for saliency detection cannot handle the noise\nperturbations smoothly and fail to delineate the salient objects present in the\ngiven scene. In this paper, we present a novel end-to-end coupled Denoising\nbased Saliency Prediction with Generative Adversarial Network (DSAL-GAN)\nframework to address the problem of salient object detection in noisy images.\nDSAL-GAN consists of two generative adversarial-networks (GAN) trained\nend-to-end to perform denoising and saliency prediction altogether in a\nholistic manner. The first GAN consists of a generator which denoises the noisy\ninput image, and in the discriminator counterpart we check whether the output\nis a denoised image or ground truth original image. The second GAN predicts the\nsaliency maps from raw pixels of the input denoised image using a data-driven\nmetric based on saliency prediction method with adversarial loss. Cycle\nconsistency loss is also incorporated to further improve salient region\nprediction. We demonstrate with comprehensive evaluation that the proposed\nframework outperforms several baseline saliency models on various performance\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 04:56:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Mukherjee", "Prerana", ""], ["Sharma", "Manoj", ""], ["Makwana", "Megh", ""], ["Singh", "Ajay Pratap", ""], ["Upadhyay", "Avinash", ""], ["Trivedi", "Akkshita", ""], ["Lall", "Brejesh", ""], ["Chaudhury", "Santanu", ""]]}, {"id": "1904.01219", "submitter": "Shruti Nagpal", "authors": "Shruti Nagpal, Maneet Singh, Richa Singh, Mayank Vatsa", "title": "Deep Learning for Face Recognition: Pride or Prejudiced?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do very high accuracies of deep networks suggest pride of effective AI or are\ndeep networks prejudiced? Do they suffer from in-group biases (own-race-bias\nand own-age-bias), and mimic the human behavior? Is in-group specific\ninformation being encoded sub-consciously by the deep networks?\n  This research attempts to answer these questions and presents an in-depth\nanalysis of `bias' in deep learning based face recognition systems. This is the\nfirst work which decodes if and where bias is encoded for face recognition.\nTaking cues from cognitive studies, we inspect if deep networks are also\naffected by social in- and out-group effect. Networks are analyzed for own-race\nand own-age bias, both of which have been well established in human beings. The\nsub-conscious behavior of face recognition models is examined to understand if\nthey encode race or age specific features for face recognition. Analysis is\nperformed based on 36 experiments conducted on multiple datasets. Four deep\nlearning networks either trained from scratch or pre-trained on over 10M images\nare used. Variations across class activation maps and feature visualizations\nprovide novel insights into the functioning of deep learning systems,\nsuggesting behavior similar to humans. It is our belief that a better\nunderstanding of state-of-the-art deep learning networks would enable\nresearchers to address the given challenge of bias in AI, and develop fairer\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 05:14:58 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 18:34:16 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Nagpal", "Shruti", ""], ["Singh", "Maneet", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1904.01231", "submitter": "Zhaohui Che", "authors": "Zhaohui Che, Ali Borji, Guangtao Zhai, Suiyi Ling, Guodong Guo,\n  Patrick Le Callet", "title": "Adversarial Attacks against Deep Saliency Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, a plethora of saliency models based on deep neural networks have\nled great breakthroughs in many complex high-level vision tasks (e.g. scene\ndescription, object detection). The robustness of these models, however, has\nnot yet been studied. In this paper, we propose a sparse feature-space\nadversarial attack method against deep saliency models for the first time. The\nproposed attack only requires a part of the model information, and is able to\ngenerate a sparser and more insidious adversarial perturbation, compared to\ntraditional image-space attacks. These adversarial perturbations are so subtle\nthat a human observer cannot notice their presences, but the model outputs will\nbe revolutionized. This phenomenon raises security threats to deep saliency\nmodels in practical applications. We also explore some intriguing properties of\nthe feature-space attack, e.g. 1) the hidden layers with bigger receptive\nfields generate sparser perturbations, 2) the deeper hidden layers achieve\nhigher attack success rates, and 3) different loss functions and different\nattacked layers will result in diverse perturbations. Experiments indicate that\nthe proposed method is able to successfully attack different model\narchitectures across various image scenes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 06:32:52 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Che", "Zhaohui", ""], ["Borji", "Ali", ""], ["Zhai", "Guangtao", ""], ["Ling", "Suiyi", ""], ["Guo", "Guodong", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1904.01241", "submitter": "Walid Abdullah Al", "authors": "Walid Abdullah Al, Il Dong Yun, Eun Ju Chun", "title": "Centerline Depth World Reinforcement Learning-based Left Atrial\n  Appendage Orifice Localization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Left atrial appendage (LAA) closure (LAAC) is a minimally invasive\nimplant-based method to prevent cardiovascular stroke in patients with\nnon-valvular atrial fibrillation. Assessing the LAA orifice in preoperative CT\nangiography plays a crucial role in choosing an appropriate LAAC implant size\nand a proper C-arm angulation. However, accurate orifice localization is hard\nbecause of the high anatomic variation of LAA, and unclear position and\norientation of the orifice in available CT views. Deep localization models also\nyield high error in localizing the orifice in CT image because of the tiny\nstructure of orifice compared to the vastness of CT image. In this paper, we\npropose a centerline depth-based reinforcement learning (RL) world for\neffective orifice localization in a small search space. In our scheme, an RL\nagent observes the centerline-to-surface distance and navigates through the LAA\ncenterline to localize the orifice. Thus, the search space is significantly\nreduced facilitating improved localization. The proposed formulation could\nresult in high localization accuracy comparing to the expert-annotations in 98\nCT images. Moreover, the localization process takes about 8 seconds which is 18\ntimes more efficient than the existing method. Therefore, this can be a useful\naid to physicians during the preprocedural planning of LAAC.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 06:56:11 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 01:28:42 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Al", "Walid Abdullah", ""], ["Yun", "Il Dong", ""], ["Chun", "Eun Ju", ""]]}, {"id": "1904.01258", "submitter": "Subhankar Roy", "authors": "Subhankar Roy and Enver Sangineto and Beg\\\"um Demir and Nicu Sebe", "title": "Metric-Learning based Deep Hashing Network for Content Based Retrieval\n  of Remote Sensing Images", "comments": "Accepted to IEEE Geoscience and Remote Sensing Letters. For code\n  visit: https://github.com/MLEnthusiast/MHCLN", "journal-ref": null, "doi": "10.1109/LGRS.2020.2974629", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods have been recently found very effective in retrieval of\nremote sensing (RS) images due to their computational efficiency and fast\nsearch speed. The traditional hashing methods in RS usually exploit\nhand-crafted features to learn hash functions to obtain binary codes, which can\nbe insufficient to optimally represent the information content of RS images. To\novercome this problem, in this paper we introduce a metric-learning based\nhashing network, which learns: 1) a semantic-based metric space for effective\nfeature representation; and 2) compact binary hash codes for fast archive\nsearch. Our network considers an interplay of multiple loss functions that\nallows to jointly learn a metric based semantic space facilitating similar\nimages to be clustered together in that target space and at the same time\nproducing compact final activations that lose negligible information when\nbinarized. Experiments carried out on two benchmark RS archives point out that\nthe proposed network significantly improves the retrieval performance under the\nsame retrieval time when compared to the state-of-the-art hashing methods in\nRS.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 07:43:01 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 10:30:30 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 09:43:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Roy", "Subhankar", ""], ["Sangineto", "Enver", ""], ["Demir", "Beg\u00fcm", ""], ["Sebe", "Nicu", ""]]}, {"id": "1904.01261", "submitter": "Lyuchen Cao", "authors": "Lvchen Cao, Huiqi Li, Yanjun Zhang, Liang Xu, Li Zhang", "title": "Hierarchical method for cataract grading based on retinal images using\n  improved Haar wavelet", "comments": "Under Review by Information Fusion (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cataracts, which are lenticular opacities that may occur at different lens\nlocations, are the leading cause of visual impairment worldwide. Accurate and\ntimely diagnosis can improve the quality of life of cataract patients. In this\npaper, a feature extraction-based method for grading cataract severity using\nretinal images is proposed. To obtain more appropriate features for the\nautomatic grading, the Haar wavelet is improved according to the\ncharacteristics of retinal images. Retinal images of non-cataract, as well as\nmild, moderate, and severe cataracts, are automatically recognized using the\nimproved Haar wavelet. A hierarchical strategy is used to transform the\nfour-class classification problem into three adjacent two-class classification\nproblems. Three sets of two-class classifiers based on a neural network are\ntrained individually and integrated together to establish a complete\nclassification system. The accuracies of the two-class classification (cataract\nand non-cataract) and four-class classification are 94.83% and 85.98%,\nrespectively. The performance analysis demonstrates that the improved Haar\nwavelet feature achieves higher accuracy than the original Haar wavelet\nfeature, and the fusion of three sets of two-class classifiers is superior to a\nsimple four-class classifier. The discussion indicates that the retinal\nimage-based method offers significant potential for cataract detection.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 07:47:53 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Cao", "Lvchen", ""], ["Li", "Huiqi", ""], ["Zhang", "Yanjun", ""], ["Xu", "Liang", ""], ["Zhang", "Li", ""]]}, {"id": "1904.01277", "submitter": "Alasdair Newson", "authors": "Sa\\\"id Ladjal, Alasdair Newson, Chi-Hieu Pham", "title": "A PCA-like Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autoencoder is a neural network which data projects to and from a lower\ndimensional latent space, where this data is easier to understand and model.\nThe autoencoder consists of two sub-networks, the encoder and the decoder,\nwhich carry out these transformations. The neural network is trained such that\nthe output is as close to the input as possible, the data having gone through\nan information bottleneck : the latent space. This tool bears significant\nressemblance to Principal Component Analysis (PCA), with two main differences.\nFirstly, the autoencoder is a non-linear transformation, contrary to PCA, which\nmakes the autoencoder more flexible and powerful. Secondly, the axes found by a\nPCA are orthogonal, and are ordered in terms of the amount of variability which\nthe data presents along these axes. This makes the interpretability of the PCA\nmuch greater than that of the autoencoder, which does not have these\nattributes. Ideally, then, we would like an autoencoder whose latent space\nconsists of independent components, ordered by decreasing importance to the\ndata. In this paper, we propose an algorithm to create such a network. We\ncreate an iterative algorithm which progressively increases the size of the\nlatent space, learning a new dimension at each step. Secondly, we propose a\ncovariance loss term to add to the standard autoencoder loss function, as well\nas a normalisation layer just before the latent space, which encourages the\nlatent space components to be statistically independent. We demonstrate the\nresults of this autoencoder on simple geometric shapes, and find that the\nalgorithm indeed finds a meaningful representation in the latent space. This\nmeans that subsequent interpolation in the latent space has meaning with\nrespect to the geometric properties of the images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:27:52 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ladjal", "Sa\u00efd", ""], ["Newson", "Alasdair", ""], ["Pham", "Chi-Hieu", ""]]}, {"id": "1904.01289", "submitter": "Daksh Thapar", "authors": "Daksh Thapar, Gaurav Jaswal, Aditya Nigam", "title": "FKIMNet: A Finger Dorsal Image Matching Network Comparing Component\n  (Major, Minor and Nail) Matching with Holistic (Finger Dorsal) Matching", "comments": "Accepted in IJCNN 2019", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8852390", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current finger knuckle image recognition systems, often require users to\nplace fingers' major or minor joints flatly towards the capturing sensor. To\nextend these systems for user non-intrusive application scenarios, such as\nconsumer electronics, forensic, defence etc, we suggest matching the full\ndorsal fingers, rather than the major/ minor region of interest (ROI) alone. In\nparticular, this paper makes a comprehensive study on the comparisons between\nfull finger and fusion of finger ROI's for finger knuckle image recognition.\nThese experiments suggest that using full-finger, provides a more elegant\nsolution. Addressing the finger matching problem, we propose a CNN\n(convolutional neural network) which creates a $128$-D feature embedding of an\nimage. It is trained via. triplet loss function, which enforces the L2 distance\nbetween the embeddings of the same subject to be approaching zero, whereas the\ndistance between any 2 embeddings of different subjects to be at least a\nmargin. For precise training of the network, we use dynamic adaptive margin,\ndata augmentation, and hard negative mining. In distinguished experiments, the\nindividual performance of finger, as well as weighted sum score level fusion of\nmajor knuckle, minor knuckle, and nail modalities have been computed,\njustifying our assumption to consider full finger as biometrics instead of its\ncounterparts. The proposed method is evaluated using two publicly available\nfinger knuckle image datasets i.e., PolyU FKP dataset and PolyU Contactless FKI\nDatasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:47:16 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Thapar", "Daksh", ""], ["Jaswal", "Gaurav", ""], ["Nigam", "Aditya", ""]]}, {"id": "1904.01293", "submitter": "Timo Stoffregen", "authors": "Timo Stoffregen and Guillermo Gallego and Tom Drummond and Lindsay\n  Kleeman and Davide Scaramuzza", "title": "Event-Based Motion Segmentation by Motion Compensation", "comments": "When viewed in Acrobat Reader, several of the figures animate. Video:\n  https://youtu.be/0q6ap_OSBAk", "journal-ref": "IEEE International Conference on Computer Vision 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to traditional cameras, whose pixels have a common exposure time,\nevent-based cameras are novel bio-inspired sensors whose pixels work\nindependently and asynchronously output intensity changes (called \"events\"),\nwith microsecond resolution. Since events are caused by the apparent motion of\nobjects, event-based cameras sample visual information based on the scene\ndynamics and are, therefore, a more natural fit than traditional cameras to\nacquire motion, especially at high speeds, where traditional cameras suffer\nfrom motion blur. However, distinguishing between events caused by different\nmoving objects and by the camera's ego-motion is a challenging task. We present\nthe first per-event segmentation method for splitting a scene into\nindependently moving objects. Our method jointly estimates the event-object\nassociations (i.e., segmentation) and the motion parameters of the objects (or\nthe background) by maximization of an objective function, which builds upon\nrecent results on event-based motion-compensation. We provide a thorough\nevaluation of our method on a public dataset, outperforming the\nstate-of-the-art by as much as 10%. We also show the first quantitative\nevaluation of a segmentation algorithm for event cameras, yielding around 90%\naccuracy at 4 pixels relative displacement.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:51:01 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 07:21:56 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 08:16:50 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2019 23:15:45 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Stoffregen", "Timo", ""], ["Gallego", "Guillermo", ""], ["Drummond", "Tom", ""], ["Kleeman", "Lindsay", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.01308", "submitter": "Guillaume Delorme", "authors": "Guillaume Delorme, Yihong Xu, Stephane Lathuili\\`ere, Radu Horaud,\n  Xavier Alameda-Pineda", "title": "CANU-ReID: A Conditional Adversarial Network for Unsupervised person\n  Re-IDentification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised person re-ID is the task of identifying people on a target data\nset for which the ID labels are unavailable during training. In this paper, we\npropose to unify two trends in unsupervised person re-ID: clustering &\nfine-tuning and adversarial learning. On one side, clustering groups training\nimages into pseudo-ID labels, and uses them to fine-tune the feature extractor.\nOn the other side, adversarial learning is used, inspired by domain adaptation,\nto match distributions from different domains. Since target data is distributed\nacross different camera viewpoints, we propose to model each camera as an\nindependent domain, and aim to learn domain-independent features.\nStraightforward adversarial learning yields negative transfer, we thus\nintroduce a conditioning vector to mitigate this undesirable effect. In our\nframework, the centroid of the cluster to which the visual sample belongs is\nused as conditioning vector of our conditional adversarial network, where the\nvector is permutation invariant (clusters ordering does not matter) and its\nsize is independent of the number of clusters. To our knowledge, we are the\nfirst to propose the use of conditional adversarial networks for unsupervised\nperson re-ID. We evaluate the proposed architecture on top of two\nstate-of-the-art clustering-based unsupervised person re-identification (re-ID)\nmethods on four different experimental settings with three different data sets\nand set the new state-of-the-art performance on all four of them. Our code and\nmodel will be made publicly available at\nhttps://team.inria.fr/perception/canu-reid/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 09:35:15 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 09:40:19 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Delorme", "Guillaume", ""], ["Xu", "Yihong", ""], ["Lathuili\u00e8re", "Stephane", ""], ["Horaud", "Radu", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "1904.01310", "submitter": "Minfeng Zhu", "authors": "Minfeng Zhu and Pingbo Pan and Wei Chen and Yi Yang", "title": "DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image\n  Synthesis", "comments": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on generating realistic images from text\ndescriptions. Current methods first generate an initial image with rough shape\nand color, and then refine the initial image to a high-resolution one. Most\nexisting text-to-image synthesis methods have two main problems. (1) These\nmethods depend heavily on the quality of the initial images. If the initial\nimage is not well initialized, the following processes can hardly refine the\nimage to a satisfactory quality. (2) Each word contributes a different level of\nimportance when depicting different image contents, however, unchanged text\nrepresentation is used in existing image refinement processes. In this paper,\nwe propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to\ngenerate high-quality images. The proposed method introduces a dynamic memory\nmodule to refine fuzzy image contents, when the initial images are not well\ngenerated. A memory writing gate is designed to select the important text\ninformation based on the initial image content, which enables our method to\naccurately generate images from the text description. We also utilize a\nresponse gate to adaptively fuse the information read from the memories and the\nimage features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200\ndataset and the Microsoft Common Objects in Context dataset. Experimental\nresults demonstrate that our DM-GAN model performs favorably against the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 09:43:23 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhu", "Minfeng", ""], ["Pan", "Pingbo", ""], ["Chen", "Wei", ""], ["Yang", "Yi", ""]]}, {"id": "1904.01318", "submitter": "Christian Rupprecht", "authors": "Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal", "title": "Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep reinforcement learning driven by visual perception becomes more\nwidely used there is a growing need to better understand and probe the learned\nagents. Understanding the decision making process and its relationship to\nvisual inputs can be very valuable to identify problems in learned behavior.\nHowever, this topic has been relatively under-explored in the research\ncommunity. In this work we present a method for synthesizing visual inputs of\ninterest for a trained agent. Such inputs or states could be situations in\nwhich specific actions are necessary. Further, critical states in which a very\nhigh or a very low reward can be achieved are often interesting to understand\nthe situational awareness of the system as they can correspond to risky states.\nTo this end, we learn a generative model over the state space of the\nenvironment and use its latent space to optimize a target function for the\nstate of interest. In our experiments we show that this method can generate\ninsights for a variety of environments and reinforcement learning methods. We\nexplore results in the standard Atari benchmark games as well as in an\nautonomous driving simulator. Based on the efficiency with which we have been\nable to identify behavioural weaknesses with this technique, we believe this\ngeneral approach could serve as an important tool for AI safety applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:21:23 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Rupprecht", "Christian", ""], ["Ibrahim", "Cyril", ""], ["Pal", "Christopher J.", ""]]}, {"id": "1904.01324", "submitter": "Saurabh Sharma", "authors": "Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal, Abhishek\n  Sharma, Arjun Jain", "title": "Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking", "comments": "In Proceedings of the 2019 IEEE International Conference on Computer\n  Vision (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D human-pose estimation from static images is a challenging\nproblem, due to the curse of dimensionality and the ill-posed nature of lifting\n2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder\nbased model that synthesizes diverse anatomically plausible 3D-pose samples\nconditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample\nset is consistent with the 2D-pose and helps tackling the inherent ambiguity in\n2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose-\n(a) depth-ordering/ordinal relations to score and weight-average the candidate\n3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle.\nWe report close to state of-the-art results on two benchmark datasets using\nOrdinalScore, and state-of-the-art results using the Oracle. We also show that\nour pipeline yields competitive results without paired image-to-3D annotations.\nThe training and evaluation code is available at\nhttps://github.com/ssfootball04/generative_pose.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:35:14 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 08:19:13 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Sharma", "Saurabh", ""], ["Varigonda", "Pavan Teja", ""], ["Bindal", "Prashast", ""], ["Sharma", "Abhishek", ""], ["Jain", "Arjun", ""]]}, {"id": "1904.01326", "submitter": "Thu Nguyen-Phuoc", "authors": "Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt,\n  Yong-Liang Yang", "title": "HoloGAN: Unsupervised learning of 3D representations from natural images", "comments": "International Conference on Computer Vision ICCV 2019. For project\n  page, see\n  https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generative adversarial network (GAN) for the task of\nunsupervised learning of 3D representations from natural images. Most\ngenerative models rely on 2D kernels to generate images and make few\nassumptions about the 3D world. These models therefore tend to create blurry\nimages or artefacts in tasks that require a strong 3D understanding, such as\nnovel-view synthesis. HoloGAN instead learns a 3D representation of the world,\nand to render this representation in a realistic manner. Unlike other GANs,\nHoloGAN provides explicit control over the pose of generated objects through\nrigid-body transformations of the learnt 3D features. Our experiments show that\nusing explicit 3D features enables HoloGAN to disentangle 3D pose and identity,\nwhich is further decomposed into shape and appearance, while still being able\nto generate images with similar or higher visual quality than other generative\nmodels. HoloGAN can be trained end-to-end from unlabelled 2D images only.\nParticularly, we do not require pose labels, 3D shapes, or multiple views of\nthe same objects. This shows that HoloGAN is the first generative model that\nlearns 3D representations from natural images in an entirely unsupervised\nmanner.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:36:01 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 10:41:28 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Nguyen-Phuoc", "Thu", ""], ["Li", "Chuan", ""], ["Theis", "Lucas", ""], ["Richardt", "Christian", ""], ["Yang", "Yong-Liang", ""]]}, {"id": "1904.01333", "submitter": "Yuting Liu", "authors": "Yuting Liu, Miaojing Shi, Qijun Zhao, Xiaofang Wang", "title": "Point in, Box out: Beyond Counting Persons in Crowds", "comments": "Accepted by CVPR2019(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern crowd counting methods usually employ deep neural networks (DNN) to\nestimate crowd counts via density regression. Despite their significant\nimprovements, the regression-based methods are incapable of providing the\ndetection of individuals in crowds. The detection-based methods, on the other\nhand, have not been largely explored in recent trends of crowd counting due to\nthe needs for expensive bounding box annotations. In this work, we instead\npropose a new deep detection network with only point supervision required. It\ncan simultaneously detect the size and location of human heads and count them\nin crowds. We first mine useful person size information from point-level\nannotations and initialize the pseudo ground truth bounding boxes. An online\nupdating scheme is introduced to refine the pseudo ground truth during\ntraining; while a locally-constrained regression loss is designed to provide\nadditional constraints on the size of the predicted boxes in a local\nneighborhood. In the end, we propose a curriculum learning strategy to train\nthe network from images of relatively accurate and easy pseudo ground truth\nfirst. Extensive experiments are conducted in both detection and counting tasks\non several standard benchmarks, e.g. ShanghaiTech, UCF_CC_50, WiderFace, and\nTRANCOS datasets, and the results show the superiority of our method over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:03:32 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:23:37 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Liu", "Yuting", ""], ["Shi", "Miaojing", ""], ["Zhao", "Qijun", ""], ["Wang", "Xiaofang", ""]]}, {"id": "1904.01334", "submitter": "Konstantin Posch", "authors": "Konstantin Posch, J\\\"urgen Pilz", "title": "Correlated Parameters to Accurately Measure Uncertainty in Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article a novel approach for training deep neural networks using\nBayesian techniques is presented. The Bayesian methodology allows for an easy\nevaluation of model uncertainty and additionally is robust to overfitting.\nThese are commonly the two main problems classical, i.e. non-Bayesian,\narchitectures have to struggle with. The proposed approach applies variational\ninference in order to approximate the intractable posterior distribution. In\nparticular, the variational distribution is defined as product of multiple\nmultivariate normal distributions with tridiagonal covariance matrices. Each\nsingle normal distribution belongs either to the weights, or to the biases\ncorresponding to one network layer. The layer-wise a posteriori variances are\ndefined based on the corresponding expectation values and further the\ncorrelations are assumed to be identical. Therefore, only a few additional\nparameters need to be optimized compared to non-Bayesian settings. The novel\napproach is successfully evaluated on basis of the popular benchmark datasets\nMNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:06:50 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Posch", "Konstantin", ""], ["Pilz", "J\u00fcrgen", ""]]}, {"id": "1904.01341", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod Kumar Kurmi and Vinay P. Namboodiri", "title": "Looking back at Labels: A Class based Domain Adaptation Technique", "comments": "IJCNN 2019 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we solve the problem of adapting classifiers across domains.\nWe consider the problem of domain adaptation for multi-class classification\nwhere we are provided a labeled set of examples in a source dataset and we are\nprovided a target dataset with no supervision. In this setting, we propose an\nadversarial discriminator based approach. While the approach based on\nadversarial discriminator has been previously proposed; in this paper, we\npresent an informed adversarial discriminator. Our observation relies on the\nanalysis that shows that if the discriminator has access to all the information\navailable including the class structure present in the source dataset, then it\ncan guide the transformation of features of the target set of classes to a more\nstructure adapted space. Using this formulation, we obtain state-of-the-art\nresults for the standard evaluation on benchmark datasets. We further provide\ndetailed analysis which shows that using all the labeled information results in\nan improved domain adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:28:19 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kurmi", "Vinod Kumar", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1904.01355", "submitter": "Chunhua Shen", "authors": "Zhi Tian, Chunhua Shen, Hao Chen, Tong He", "title": "FCOS: Fully Convolutional One-Stage Object Detection", "comments": "Accepted to Proc. Int. Conf. Computer Vision 2019. 13 pages. Code is\n  available at: https://github.com/tianzhi0549/FCOS/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a fully convolutional one-stage object detector (FCOS) to solve\nobject detection in a per-pixel prediction fashion, analogue to semantic\nsegmentation. Almost all state-of-the-art object detectors such as RetinaNet,\nSSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast,\nour proposed detector FCOS is anchor box free, as well as proposal free. By\neliminating the predefined set of anchor boxes, FCOS completely avoids the\ncomplicated computation related to anchor boxes such as calculating overlapping\nduring training. More importantly, we also avoid all hyper-parameters related\nto anchor boxes, which are often very sensitive to the final detection\nperformance. With the only post-processing non-maximum suppression (NMS), FCOS\nwith ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale\ntesting, surpassing previous one-stage detectors with the advantage of being\nmuch simpler. For the first time, we demonstrate a much simpler and flexible\ndetection framework achieving improved detection accuracy. We hope that the\nproposed FCOS framework can serve as a simple and strong alternative for many\nother instance-level tasks. Code is available at:Code is available at:\nhttps://tinyurl.com/FCOSv1\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:56:36 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 04:13:34 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 01:42:12 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 00:45:02 GMT"}, {"version": "v5", "created": "Tue, 20 Aug 2019 11:26:21 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["Chen", "Hao", ""], ["He", "Tong", ""]]}, {"id": "1904.01356", "submitter": "Omer Arshad", "authors": "Omer Arshad, Ignazio Gallo, Shah Nawaz, Alessandro Calefati", "title": "Aiding Intra-Text Representations with Visual Context for Multimodal\n  Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With massive explosion of social media such as Twitter and Instagram, people\ndaily share billions of multimedia posts, containing images and text.\nTypically, text in these posts is short, informal and noisy, leading to\nambiguities which can be resolved using images. In this paper we explore\ntext-centric Named Entity Recognition task on these multimedia posts. We\npropose an end to end model which learns a joint representation of a text and\nan image. Our model extends multi-dimensional self attention technique, where\nnow image helps to enhance relationship between words. Experiments show that\nour model is capable of capturing both textual and visual contexts with greater\naccuracy, achieving state-of-the-art results on Twitter multimodal Named Entity\nRecognition dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:57:40 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Arshad", "Omer", ""], ["Gallo", "Ignazio", ""], ["Nawaz", "Shah", ""], ["Calefati", "Alessandro", ""]]}, {"id": "1904.01357", "submitter": "Takahiro Kawashima", "authors": "Takahiro Kawashima, Hayaru Shouno", "title": "Fast Bayesian Restoration of Poisson Corrupted Images with INLA", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photon-limited images are often seen in fields such as medical imaging.\nAlthough the number of collected photons on an image sensor statistically\nfollows Poisson distribution, this type of noise is intractable, unlike\nGaussian noise. In this study, we propose a Bayesian restoration method of\nPoisson corrupted image using Integrated Nested Laplace Approximation (INLA),\nwhich is a computational method to evaluate marginalized posterior\ndistributions of latent Gaussian models (LGMs). When the original image can be\nregarded as ICAR (intrinsic conditional auto-regressive) model reasonably, our\nmethod performs very faster than well-known ones such as loopy belief\npropagation-based method and Markov chain Monte Carlo (MCMC) without decreasing\nthe accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:05:40 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kawashima", "Takahiro", ""], ["Shouno", "Hayaru", ""]]}, {"id": "1904.01375", "submitter": "Lu Yang", "authors": "Lu Yang, Fan Dang, Peng Wang, Hui Li, Zhen Li, Yanning Zhang", "title": "A Holistic Representation Guided Attention Network for Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading irregular scene text of arbitrary shape in natural images is still a\nchallenging problem, despite the progress made recently. Many existing\napproaches incorporate sophisticated network structures to handle various\nshapes, use extra annotations for stronger supervision, or employ hard-to-train\nrecurrent neural networks for sequence modeling. In this work, we propose a\nsimple yet strong approach for scene text recognition. With no need to convert\ninput images to sequence representations, we directly connect two-dimensional\nCNN features to an attention-based sequence decoder which guided by holistic\nrepresentation. The holistic representation can guide the attention-based\ndecoder focus on more accurate area. As no recurrent module is adopted, our\nmodel can be trained in parallel. It achieves 1.5x to 9.4x acceleration to\nbackward pass and 1.3x to 7.9x acceleration to forward pass, compared with the\nRNN counterparts. The proposed model is trained with only word-level\nannotations. With this simple design, our method achieves state-of-the-art or\ncompetitive recognition performance on the evaluated regular and irregular\nscene text benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:43:29 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 16:03:33 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 02:04:00 GMT"}, {"version": "v4", "created": "Sun, 5 Jul 2020 03:03:24 GMT"}, {"version": "v5", "created": "Tue, 30 Mar 2021 07:16:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yang", "Lu", ""], ["Dang", "Fan", ""], ["Wang", "Peng", ""], ["Li", "Hui", ""], ["Li", "Zhen", ""], ["Zhang", "Yanning", ""]]}, {"id": "1904.01376", "submitter": "Jindong Wang", "authors": "Jindong Wang, Yiqiang Chen, Han Yu, Meiyu Huang, Qiang Yang", "title": "Easy Transfer Learning By Exploiting Intra-domain Structures", "comments": "Camera-ready version of IEEE International Conference on Multimedia\n  and Expo (ICME) 2019; code available at\n  http://transferlearning.xyz/code/traditional/EasyTL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning aims at transferring knowledge from a well-labeled domain\nto a similar but different domain with limited or no labels. Unfortunately,\nexisting learning-based methods often involve intensive model selection and\nhyperparameter tuning to obtain good results. Moreover, cross-validation is not\npossible for tuning hyperparameters since there are often no labels in the\ntarget domain. This would restrict wide applicability of transfer learning\nespecially in computationally-constraint devices such as wearables. In this\npaper, we propose a practically Easy Transfer Learning (EasyTL) approach which\nrequires no model selection and hyperparameter tuning, while achieving\ncompetitive performance. By exploiting intra-domain structures, EasyTL is able\nto learn both non-parametric transfer features and classifiers. Extensive\nexperiments demonstrate that, compared to state-of-the-art traditional and deep\nmethods, EasyTL satisfies the Occam's Razor principle: it is extremely easy to\nimplement and use while achieving comparable or better performance in\nclassification accuracy and much better computational efficiency. Additionally,\nit is shown that EasyTL can increase the performance of existing transfer\nfeature learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:43:53 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 02:44:33 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Yu", "Han", ""], ["Huang", "Meiyu", ""], ["Yang", "Qiang", ""]]}, {"id": "1904.01382", "submitter": "Vlad Hosu", "authors": "Vlad Hosu, Bastian Goldlucke, Dietmar Saupe", "title": "Effective Aesthetics Prediction with Multi-level Spatially Pooled\n  Features", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective deep learning approach to aesthetics quality\nassessment that relies on a new type of pre-trained features, and apply it to\nthe AVA data set, the currently largest aesthetics database. While previous\napproaches miss some of the information in the original images, due to taking\nsmall crops, down-scaling or warping the originals during training, we propose\nthe first method that efficiently supports full resolution images as an input,\nand can be trained on variable input sizes. This allows us to significantly\nimprove upon the state of the art, increasing the Spearman rank-order\ncorrelation coefficient (SRCC) of ground-truth mean opinion scores (MOS) from\nthe existing best reported of 0.612 to 0.756. To achieve this performance, we\nextract multi-level spatially pooled (MLSP) features from all convolutional\nblocks of a pre-trained InceptionResNet-v2 network, and train a custom shallow\nConvolutional Neural Network (CNN) architecture on these new features.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:58:12 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hosu", "Vlad", ""], ["Goldlucke", "Bastian", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1904.01390", "submitter": "Shiv Ram Dubey", "authors": "Sai Prasanna Teja Reddy, Surya Teja Karri, Shiv Ram Dubey, Snehasis\n  Mukherjee", "title": "Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal\n  Convolutional Neural Networks", "comments": "Accepted in 2019 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition in videos is an active area of research in\ncomputer vision. However, fake facial expressions are difficult to be\nrecognized even by humans. On the other hand, facial micro-expressions\ngenerally represent the actual emotion of a person, as it is a spontaneous\nreaction expressed through human face. Despite of a few attempts made for\nrecognizing micro-expressions, still the problem is far from being a solved\nproblem, which is depicted by the poor rate of accuracy shown by the\nstate-of-the-art methods. A few CNN based approaches are found in the\nliterature to recognize micro-facial expressions from still images. Whereas, a\nspontaneous micro-expression video contains multiple frames that have to be\nprocessed together to encode both spatial and temporal information. This paper\nproposes two 3D-CNN methods: MicroExpSTCNN and MicroExpFuseNet, for spontaneous\nfacial micro-expression recognition by exploiting the spatiotemporal\ninformation in CNN framework. The MicroExpSTCNN considers the full spatial\ninformation, whereas the MicroExpFuseNet is based on the 3D-CNN feature fusion\nof the eyes and mouth regions. The experiments are performed over CAS(ME)^2 and\nSMIC micro-expression databases. The proposed MicroExpSTCNN model outperforms\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:45:49 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Reddy", "Sai Prasanna Teja", ""], ["Karri", "Surya Teja", ""], ["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1904.01400", "submitter": "Jiao Bingliang", "authors": "Peng Wang, Bingliang Jiao, Lu Yang, Yifei Yang, Shizhou Zhang, Wei\n  Wei, Yanning Zhang", "title": "Vehicle Re-identification in Aerial Imagery: Dataset and Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we construct a large-scale dataset for vehicle\nre-identification (ReID), which contains 137k images of 13k vehicle instances\ncaptured by UAV-mounted cameras. To our knowledge, it is the largest UAV-based\nvehicle ReID dataset. To increase intra-class variation, each vehicle is\ncaptured by at least two UAVs at different locations, with diverse view-angles\nand flight-altitudes. We manually label a variety of vehicle attributes,\nincluding vehicle type, color, skylight, bumper, spare tire and luggage rack.\nFurthermore, for each vehicle image, the annotator is also required to mark the\ndiscriminative parts that helps them to distinguish this particular vehicle\nfrom others. Besides the dataset, we also design a specific vehicle ReID\nalgorithm to make full use of the rich annotation information. It is capable of\nexplicitly detecting discriminative parts for each specific vehicle and\nsignificantly outperforms the evaluated baselines and state-of-the-art vehicle\nReID approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:24:16 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Peng", ""], ["Jiao", "Bingliang", ""], ["Yang", "Lu", ""], ["Yang", "Yifei", ""], ["Zhang", "Shizhou", ""], ["Wei", "Wei", ""], ["Zhang", "Yanning", ""]]}, {"id": "1904.01410", "submitter": "Guojun Yin", "authors": "Guojun Yin and Lu Sheng and Bin Liu and Nenghai Yu and Xiaogang Wang\n  and Jing Shao", "title": "Context and Attribute Grounded Dense Captioning", "comments": "12 pages, 9 figures, accepted as a POSTER at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense captioning aims at simultaneously localizing semantic regions and\ndescribing these regions-of-interest (ROIs) with short phrases or sentences in\nnatural language. Previous studies have shown remarkable progresses, but they\nare often vulnerable to the aperture problem that a caption generated by the\nfeatures inside one ROI lacks contextual coherence with its surrounding context\nin the input image. In this work, we investigate contextual reasoning based on\nmulti-scale message propagations from the neighboring contents to the target\nROIs. To this end, we design a novel end-to-end context and attribute grounded\ndense captioning framework consisting of 1) a contextual visual mining module\nand 2) a multi-level attribute grounded description generation module. Knowing\nthat captions often co-occur with the linguistic attributes (such as who, what\nand where), we also incorporate an auxiliary supervision from hierarchical\nlinguistic attributes to augment the distinctiveness of the learned captions.\nExtensive experiments and ablation studies on Visual Genome dataset demonstrate\nthe superiority of the proposed model in comparison to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:45:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yin", "Guojun", ""], ["Sheng", "Lu", ""], ["Liu", "Bin", ""], ["Yu", "Nenghai", ""], ["Wang", "Xiaogang", ""], ["Shao", "Jing", ""]]}, {"id": "1904.01416", "submitter": "Jens Behley", "authors": "Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke,\n  Cyrill Stachniss, Juergen Gall", "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR\n  Sequences", "comments": "ICCV2019. See teaser video at http://bit.ly/SemanticKITTI-teaser", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene understanding is important for various applications. In\nparticular, self-driving cars need a fine-grained understanding of the surfaces\nand objects in their vicinity. Light detection and ranging (LiDAR) provides\nprecise geometric information about the environment and is thus a part of the\nsensor suites of almost all self-driving cars. Despite the relevance of\nsemantic scene understanding for this application, there is a lack of a large\ndataset for this task which is based on an automotive LiDAR.\n  In this paper, we introduce a large dataset to propel research on laser-based\nsemantic segmentation. We annotated all sequences of the KITTI Vision Odometry\nBenchmark and provide dense point-wise annotations for the complete $360^{o}$\nfield-of-view of the employed automotive LiDAR. We propose three benchmark\ntasks based on this dataset: (i) semantic segmentation of point clouds using a\nsingle scan, (ii) semantic segmentation using multiple past scans, and (iii)\nsemantic scene completion, which requires to anticipate the semantic scene in\nthe future. We provide baseline experiments and show that there is a need for\nmore sophisticated models to efficiently tackle these tasks. Our dataset opens\nthe door for the development of more advanced methods, but also provides\nplentiful data to investigate new research directions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:53:16 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 22:09:13 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 09:30:52 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Behley", "Jens", ""], ["Garbade", "Martin", ""], ["Milioto", "Andres", ""], ["Quenzel", "Jan", ""], ["Behnke", "Sven", ""], ["Stachniss", "Cyrill", ""], ["Gall", "Juergen", ""]]}, {"id": "1904.01421", "submitter": "William Thong", "authors": "William Thong, Cees G.M. Snoek and Arnold W.M. Smeulders", "title": "Cooperative Embeddings for Instance, Attribute and Category Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to retrieve an image based on instance, attribute\nand category similarity notions. Different from existing works, which usually\naddress only one of these entities in isolation, we introduce a cooperative\nembedding to integrate them while preserving their specific level of semantic\nrepresentation. An algebraic structure defines a superspace filled with\ninstances. Attributes are axis-aligned to form subspaces, while categories\ninfluence the arrangement of similar instances. These relationships enable them\nto cooperate for their mutual benefits for image retrieval. We derive a\nproxy-based softmax embedding loss to learn simultaneously all similarity\nmeasures in both superspace and subspaces. We evaluate our model on datasets\nfrom two different domains. Experiments on image retrieval tasks show the\nbenefits of the cooperative embeddings for modeling multiple image\nsimilarities, and for discovering style evolution of instances between- and\nwithin-categories.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:55:47 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Thong", "William", ""], ["Snoek", "Cees G. M.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1904.01428", "submitter": "Lingjing Wang", "authors": "Lingjing Wang, Jianchun Chen, Xiang Li, Yi Fang", "title": "Non-Rigid Point Set Registration Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Point set registration is defined as a process to determine the spatial\ntransformation from the source point set to the target one. Existing methods\noften iteratively search for the optimal geometric transformation to register a\ngiven pair of point sets, driven by minimizing a predefined alignment loss\nfunction. In contrast, the proposed point registration neural network (PR-Net)\nactively learns the registration pattern as a parametric function from a\ntraining dataset, consequently predict the desired geometric transformation to\nalign a pair of point sets. PR-Net can transfer the learned knowledge (i.e.\nregistration pattern) from registering training pairs to testing ones without\nadditional iterative optimization. Specifically, in this paper, we develop\nnovel techniques to learn shape descriptors from point sets that help formulate\na clear correlation between source and target point sets. With the defined\ncorrelation, PR-Net tends to predict the transformation so that the source and\ntarget point sets can be statistically aligned, which in turn leads to an\noptimal spatial geometric registration. PR-Net achieves robust and superior\nperformance for non-rigid registration of point sets, even in presence of\nGaussian noise, outliers, and missing points, but requires much less time for\nregistering large number of pairs. More importantly, for a new pair of point\nsets, PR-Net is able to directly predict the desired transformation using the\nlearned model without repetitive iterative optimization routine. Our code is\navailable at https://github.com/Lingjing324/PR-Net.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:01:59 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Lingjing", ""], ["Chen", "Jianchun", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "1904.01475", "submitter": "Ali Furkan Biten", "authors": "Ali Furkan Biten, Lluis Gomez, Mar\\c{c}al Rusi\\~nol, Dimosthenis\n  Karatzas", "title": "Good News, Everyone! Context driven entity-aware captioning for news\n  images", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current image captioning systems perform at a merely descriptive level,\nessentially enumerating the objects in the scene and their relations. Humans,\non the contrary, interpret images by integrating several sources of prior\nknowledge of the world. In this work, we aim to take a step closer to producing\ncaptions that offer a plausible interpretation of the scene, by integrating\nsuch contextual information into the captioning pipeline. For this we focus on\nthe captioning of images used to illustrate news articles. We propose a novel\ncaptioning method that is able to leverage contextual information provided by\nthe text of news articles associated with an image. Our model is able to\nselectively draw information from the article guided by visual cues, and to\ndynamically extend the output dictionary to out-of-vocabulary named entities\nthat appear in the context source. Furthermore we introduce `GoodNews', the\nlargest news image captioning dataset in the literature and demonstrate\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:55:46 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Biten", "Ali Furkan", ""], ["Gomez", "Lluis", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1904.01480", "submitter": "Guojun Yin", "authors": "Guojun Yin and Bin Liu and Lu Sheng and Nenghai Yu and Xiaogang Wang\n  and Jing Shao", "title": "Semantics Disentangling for Text-to-Image Generation", "comments": "14 pages, 11 figures, accepted as an ORAL at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthesizing photo-realistic images from text descriptions is a challenging\nproblem. Previous studies have shown remarkable progresses on visual quality of\nthe generated images. In this paper, we consider semantics from the input text\ndescriptions in helping render photo-realistic images. However, diverse\nlinguistic expressions pose challenges in extracting consistent semantics even\nthey depict the same thing. To this end, we propose a novel photo-realistic\ntext-to-image generation model that implicitly disentangles semantics to both\nfulfill the high-level semantic consistency and low-level semantic diversity.\nTo be specific, we design (1) a Siamese mechanism in the discriminator to learn\nconsistent high-level semantics, and (2) a visual-semantic embedding strategy\nby semantic-conditioned batch normalization to find diverse low-level\nsemantics. Extensive experiments and ablation studies on CUB and MS-COCO\ndatasets demonstrate the superiority of the proposed method in comparison to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:08:51 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yin", "Guojun", ""], ["Liu", "Bin", ""], ["Sheng", "Lu", ""], ["Yu", "Nenghai", ""], ["Wang", "Xiaogang", ""], ["Shao", "Jing", ""]]}, {"id": "1904.01501", "submitter": "Riccardo de Lutio", "authors": "Riccardo de Lutio, Stefano D'Aronco, Jan Dirk Wegner, Konrad Schindler", "title": "Guided Super-Resolution as Pixel-to-Pixel Transformation", "comments": "Extended version, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided super-resolution is a unifying framework for several computer vision\ntasks where the inputs are a low-resolution source image of some target\nquantity (e.g., perspective depth acquired with a time-of-flight camera) and a\nhigh-resolution guide image from a different domain (e.g., a grey-scale image\nfrom a conventional camera); and the target output is a high-resolution version\nof the source (in our example, a high-res depth map). The standard way of\nlooking at this problem is to formulate it as a super-resolution task, i.e.,\nthe source image is upsampled to the target resolution, while transferring the\nmissing high-frequency details from the guide. Here, we propose to turn that\ninterpretation on its head and instead see it as a pixel-to-pixel mapping of\nthe guide image to the domain of the source image. The pixel-wise mapping is\nparametrised as a multi-layer perceptron, whose weights are learned by\nminimising the discrepancies between the source image and the downsampled\ntarget image. Importantly, our formulation makes it possible to regularise only\nthe mapping function, while avoiding regularisation of the outputs; thus\nproducing crisp, natural-looking images. The proposed method is unsupervised,\nusing only the specific source and guide images to fit the mapping. We evaluate\nour method on two different tasks, super-resolution of depth maps and of tree\nheight maps. In both cases, we clearly outperform recent baselines in\nquantitative comparisons, while delivering visually much sharper outputs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:41:44 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 14:48:19 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["de Lutio", "Riccardo", ""], ["D'Aronco", "Stefano", ""], ["Wegner", "Jan Dirk", ""], ["Schindler", "Konrad", ""]]}, {"id": "1904.01509", "submitter": "Jian Xue", "authors": "Yanfu Yan, Ke Lu, Jian Xue, Pengcheng Gao, Jiayi Lyu", "title": "FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D\n  Facial Animation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression analysis based on machine learning requires large number of\nwell-annotated data to reflect different changes in facial motion. Publicly\navailable datasets truly help to accelerate research in this area by providing\na benchmark resource, but all of these datasets, to the best of our knowledge,\nare limited to rough annotations for action units, including only their\nabsence, presence, or a five-level intensity according to the Facial Action\nCoding System. To meet the need for videos labeled in great detail, we present\na well-annotated dataset named FEAFA for Facial Expression Analysis and 3D\nFacial Animation. One hundred and twenty-two participants, including children,\nyoung adults and elderly people, were recorded in real-world conditions. In\naddition, 99,356 frames were manually labeled using Expression Quantitative\nTool developed by us to quantify 9 symmetrical FACS action units, 10\nasymmetrical (unilateral) FACS action units, 2 symmetrical FACS action\ndescriptors and 2 asymmetrical FACS action descriptors, and each action unit or\naction descriptor is well-annotated with a floating point number between 0 and\n1. To provide a baseline for use in future research, a benchmark for the\nregression of action unit values based on Convolutional Neural Networks are\npresented. We also demonstrate the potential of our FEAFA dataset for 3D facial\nanimation. Almost all state-of-the-art algorithms for facial animation are\nachieved based on 3D face reconstruction. We hence propose a novel method that\ndrives virtual characters only based on action unit value regression of the 2D\nvideo frames of source actors.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:50:11 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yan", "Yanfu", ""], ["Lu", "Ke", ""], ["Xue", "Jian", ""], ["Gao", "Pengcheng", ""], ["Lyu", "Jiayi", ""]]}, {"id": "1904.01538", "submitter": "Tianyu Wang", "authors": "Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, Rynson Lau", "title": "Spatial Attentive Single-Image Deraining with a High Quality Real Rain\n  Dataset", "comments": "Accepted by CVPR'19. Project page:\n  https://stevewongv.github.io/derain-project.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing rain streaks from a single image has been drawing considerable\nattention as rain streaks can severely degrade the image quality and affect the\nperformance of existing outdoor vision tasks. While recent CNN-based derainers\nhave reported promising performances, deraining remains an open problem for two\nreasons. First, existing synthesized rain datasets have only limited realism,\nin terms of modeling real rain characteristics such as rain shape, direction\nand intensity. Second, there are no public benchmarks for quantitative\ncomparisons on real rain images, which makes the current evaluation less\nobjective. The core challenge is that real world rain/clean image pairs cannot\nbe captured at the same time. In this paper, we address the single image rain\nremoval problem in two ways. First, we propose a semi-automatic method that\nincorporates temporal priors and human supervision to generate a high-quality\nclean image from each input sequence of real rain images. Using this method, we\nconstruct a large-scale dataset of $\\sim$$29.5K$ rain/rain-free image pairs\nthat covers a wide range of natural rain scenes. Second, to better cover the\nstochastic distribution of real rain streaks, we propose a novel SPatial\nAttentive Network (SPANet) to remove rain streaks in a local-to-global manner.\nExtensive experiments demonstrate that our network performs favorably against\nthe state-of-the-art deraining methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:52:29 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 08:31:04 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Tianyu", ""], ["Yang", "Xin", ""], ["Xu", "Ke", ""], ["Chen", "Shaozhe", ""], ["Zhang", "Qiang", ""], ["Lau", "Rynson", ""]]}, {"id": "1904.01569", "submitter": "Saining Xie", "authors": "Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He", "title": "Exploring Randomly Wired Neural Networks for Image Recognition", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks for image recognition have evolved through extensive manual\ndesign from simple chain-like models to structures with multiple wiring paths.\nThe success of ResNets and DenseNets is due in large part to their innovative\nwiring plans. Now, neural architecture search (NAS) studies are exploring the\njoint optimization of wiring and operation types, however, the space of\npossible wirings is constrained and still driven by manual design despite being\nsearched. In this paper, we explore a more diverse set of connectivity patterns\nthrough the lens of randomly wired neural networks. To do this, we first define\nthe concept of a stochastic network generator that encapsulates the entire\nnetwork generation process. Encapsulation provides a unified view of NAS and\nrandomly wired networks. Then, we use three classical random graph models to\ngenerate randomly wired graphs for networks. The results are surprising:\nseveral variants of these random generators yield network instances that have\ncompetitive accuracy on the ImageNet benchmark. These results suggest that new\nefforts focusing on designing better network generators may lead to new\nbreakthroughs by exploring less constrained search spaces with more room for\nnovel design.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 17:57:16 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 17:50:26 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Xie", "Saining", ""], ["Kirillov", "Alexander", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""]]}, {"id": "1904.01579", "submitter": "Feida Zhu", "authors": "Feida Zhu, Zhetong Liang, Xixi Jia, Lei Zhang and Yizhou Yu", "title": "A Benchmark for Edge-Preserving Image Smoothing", "comments": "Accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2908778", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-preserving image smoothing is an important step for many low-level\nvision problems. Though many algorithms have been proposed, there are several\ndifficulties hindering its further development. First, most existing algorithms\ncannot perform well on a wide range of image contents using a single parameter\nsetting. Second, the performance evaluation of edge-preserving image smoothing\nremains subjective, and there lacks a widely accepted datasets to objectively\ncompare the different algorithms. To address these issues and further advance\nthe state of the art, in this work we propose a benchmark for edge-preserving\nimage smoothing. This benchmark includes an image dataset with groundtruth\nimage smoothing results as well as baseline algorithms that can generate\ncompetitive edge-preserving smoothing results for a wide range of image\ncontents. The established dataset contains 500 training and testing images with\na number of representative visual object categories, while the baseline methods\nin our benchmark are built upon representative deep convolutional network\narchitectures, on top of which we design novel loss functions well suited for\nedge-preserving image smoothing. The trained deep networks run faster than most\nstate-of-the-art smoothing algorithms with leading smoothing results both\nqualitatively and quantitatively. The benchmark is publicly accessible via\nhttps://github.com/zhufeida/Benchmark_EPS.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:19:57 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zhu", "Feida", ""], ["Liang", "Zhetong", ""], ["Jia", "Xixi", ""], ["Zhang", "Lei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1904.01620", "submitter": "Imad Rida", "authors": "Imad Rida", "title": "Towards Human Body-Part Learning for Model-Free Gait Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gait based biometric aims to discriminate among people by the way or manner\nthey walk. It represents a biometric at distance which has many advantages over\nother biometric modalities. State-of-the-art methods require a limited\ncooperation from the individuals. Consequently, contrary to other modalities,\ngait is a non-invasive approach. As a behavioral analysis, gait is difficult to\ncircumvent. Moreover, gait can be performed without the subject being aware of\nit. Consequently, it is more difficult to try to tamper one own biometric\nsignature. In this paper we review different features and approaches used in\ngait recognition. A novel method able to learn the discriminative human\nbody-parts to improve the recognition accuracy will be introduced. Extensive\nexperiments will be performed on CASIA gait benchmark database and results will\nbe compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:49:14 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Rida", "Imad", ""]]}, {"id": "1904.01636", "submitter": "Eugene Vorontsov", "authors": "Eugene Vorontsov, Pavlo Molchanov, Christopher Beckham, Jan Kautz, and\n  Samuel Kadoury", "title": "Towards annotation-efficient segmentation via image-to-image translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in medical imaging, it is prohibitively challenging to produce enough\nboundary annotations to train deep neural networks for accurate tumor\nsegmentation. We propose the use of weak labels about whether an image presents\ntumor or whether it is absent to extend training over images that lack these\nannotations. Specifically, we propose a semi-supervised framework that employs\nunpaired image-to-image translation between two domains, presence vs. absence\nof cancer, as the unsupervised objective. We conjecture that translation helps\nsegmentation -- both require the target to be separated from the background. We\nencode images into two codes: one that is common to both domains and one that\nis unique to the presence domain. Decoding from the common code yields healthy\nimages; decoding with the addition of the unique code produces a residual\nchange to this image that adds cancer. Translation proceeds from presence to\nabsence and vice versa. In the first case, the tumor is re-added to the image\nand we successfully exploit the residual decoder to also perform segmentation.\nIn the second case, unique codes are sampled, producing a distribution of\npossible tumors. To validate the method, we created challenging synthetic tasks\nand tumor segmentation datasets from public BRATS (brain, MRI) and LitS (liver,\nCT) datasets. We show a clear improvement (0.83 Dice on brain, 0.74 on liver)\nover baseline semi-supervised training with autoencoding (0.73, 0.66) and a\nmean teacher approach (0.75, 0.69), demonstrating the ability to generalize\nfrom smaller distributions of annotated samples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:35:27 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 17:06:52 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 21:48:02 GMT"}, {"version": "v4", "created": "Sat, 12 Jun 2021 01:10:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Vorontsov", "Eugene", ""], ["Molchanov", "Pavlo", ""], ["Beckham", "Christopher", ""], ["Kautz", "Jan", ""], ["Kadoury", "Samuel", ""]]}, {"id": "1904.01638", "submitter": "Li Yao", "authors": "Li Yao, Jordan Prosky, Ben Covington, Kevin Lyman", "title": "A Strong Baseline for Domain Adaptation and Generalization in Medical\n  Imaging", "comments": "Extended abstract of a journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a strong baseline for the problem of multi-source\nmulti-target domain adaptation and generalization in medical imaging. Using a\ndiverse collection of ten chest X-ray datasets, we empirically demonstrate the\nbenefits of training medical imaging deep learning models on varied patient\npopulations for generalization to out-of-sample domains.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:38:34 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Yao", "Li", ""], ["Prosky", "Jordan", ""], ["Covington", "Ben", ""], ["Lyman", "Kevin", ""]]}, {"id": "1904.01645", "submitter": "Matthew Giamou", "authors": "Matthew Giamou, Filip Maric, Valentin Peretroukhin, Jonathan Kelly", "title": "Sparse Bounded Degree Sum of Squares Optimization for Certifiably\n  Globally Optimal Rotation Averaging", "comments": "Prior version contained an erroneous proof which has been removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating unknown rotations from noisy measurements is an important step in\nSfM and other 3D vision tasks. Typically, local optimization methods\nsusceptible to returning suboptimal local minima are used to solve the rotation\naveraging problem. A new wave of approaches that leverage convex relaxations\nhave provided the first formal guarantees of global optimality for state\nestimation techniques involving SO(3). However, most of these guarantees are\nonly applicable when the measurement error introduced by noise is within a\ncertain bound that depends on the problem instance's structure. In this paper,\nwe cast rotation averaging as a polynomial optimization problem over unit\nquaternions to produce the first rotation averaging method that is formally\nguaranteed to provide a certifiably globally optimal solution for \\textit{any}\nproblem instance. This is achieved by formulating and solving a sparse convex\nsum of squares (SOS) relaxation of the problem. We provide an open source\nimplementation of our algorithm and experiments, demonstrating the benefits of\nour globally optimal approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:02:14 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 19:25:09 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Giamou", "Matthew", ""], ["Maric", "Filip", ""], ["Peretroukhin", "Valentin", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1904.01648", "submitter": "Chiwoo Park", "authors": "Chiwoo Park, Peihua Qiu, Jennifer Carpena-N\\'u\\~nez, Rahul Rao,\n  Michael Susner and Benji Maruyama", "title": "Sequential Adaptive Design for Jump Regression Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting input variables or design points for statistical models has been of\ngreat interest in adaptive design and active learning. Motivated by two\nscientific examples, this paper presents a strategy of selecting the design\npoints for a regression model when the underlying regression function is\ndiscontinuous. The first example we undertook was for the purpose of\naccelerating imaging speed in a high resolution material imaging; the second\nwas use of sequential design for the purpose of mapping a chemical phase\ndiagram. In both examples, the underlying regression functions have\ndiscontinuities, so many of the existing design optimization approaches cannot\nbe applied because they mostly assume a continuous regression function.\nAlthough some existing adaptive design strategies developed from treed\nregression models can handle the discontinuities, the Bayesian approaches come\nwith computationally expensive Markov Chain Monte Carlo techniques for\nposterior inferences and subsequent design point selections, which is not\nappropriate for the first motivating example that requires computation at least\nfaster than the original imaging speed. In addition, the treed models are based\non the domain partitioning that are inefficient when the discontinuities occurs\nover complex sub-domain boundaries. We propose a simple and effective adaptive\ndesign strategy for a regression analysis with discontinuities: some\nstatistical properties with a fixed design will be presented first, and then\nthese properties will be used to propose a new criterion of selecting the\ndesign points for the regression analysis. Sequential design with the new\ncriterion will be presented with comprehensive simulated examples, and its\napplication to the two motivating examples will be presented.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:14:47 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 13:01:09 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 19:00:31 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 20:22:36 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Park", "Chiwoo", ""], ["Qiu", "Peihua", ""], ["Carpena-N\u00fa\u00f1ez", "Jennifer", ""], ["Rao", "Rahul", ""], ["Susner", "Michael", ""], ["Maruyama", "Benji", ""]]}, {"id": "1904.01649", "submitter": "Oncel Tuzel", "authors": "Vishwanath A. Sindagi, Yin Zhou and Oncel Tuzel", "title": "MVX-Net: Multimodal VoxelNet for 3D Object Detection", "comments": "7 pages", "journal-ref": "International Conference on Robotics and Automation (ICRA), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works on 3D object detection have focused on designing neural\nnetwork architectures that can consume point cloud data. While these approaches\ndemonstrate encouraging performance, they are typically based on a single\nmodality and are unable to leverage information from other modalities, such as\na camera. Although a few approaches fuse data from different modalities, these\nmethods either use a complicated pipeline to process the modalities\nsequentially, or perform late-fusion and are unable to learn interaction\nbetween different modalities at early stages. In this work, we present\nPointFusion and VoxelFusion: two simple yet effective early-fusion approaches\nto combine the RGB and point cloud modalities, by leveraging the recently\nintroduced VoxelNet architecture. Evaluation on the KITTI dataset demonstrates\nsignificant improvements in performance over approaches which only use point\ncloud data. Furthermore, the proposed method provides results competitive with\nthe state-of-the-art multimodal algorithms, achieving top-2 ranking in five of\nthe six bird's eye view and 3D detection categories on the KITTI benchmark, by\nusing a simple single stage network.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:15:07 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Zhou", "Yin", ""], ["Tuzel", "Oncel", ""]]}, {"id": "1904.01654", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Mehdi Moradi, Joy Wu, Tanveer Syeda-Mahmood", "title": "Identifying disease-free chest X-ray images with deep transfer learning", "comments": "SPIE Medical Imaging, 2019 (oral presentation)", "journal-ref": null, "doi": "10.1117/12.2513164", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays (CXRs) are among the most commonly used medical image\nmodalities. They are mostly used for screening, and an indication of disease\ntypically results in subsequent tests. As this is mostly a screening test used\nto rule out chest abnormalities, the requesting clinicians are often interested\nin whether a CXR is normal or not. A machine learning algorithm that can\naccurately screen out even a small proportion of the \"real normal\" exams out of\nall requested CXRs would be highly beneficial in reducing the workload for\nradiologists. In this work, we report a deep neural network trained for\nclassifying CXRs with the goal of identifying a large number of normal\n(disease-free) images without risking the discharge of sick patients. We use an\nImageNet-pretrained Inception-ResNet-v2 model to provide the image features,\nwhich are further used to train a model on CXRs labelled by expert\nradiologists. The probability threshold for classification is optimized for\n100% precision for the normal class, ensuring no sick patients are released. At\nthis threshold we report an average recall of 50%. This means that the proposed\nsolution has the potential to cut in half the number of disease-free CXRs\nexamined by radiologists, without risking the discharge of sick patients.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:26:53 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Moradi", "Mehdi", ""], ["Wu", "Joy", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1904.01665", "submitter": "Zhenheng Yang", "authors": "Zhenheng Yang, Dhruv Mahajan, Deepti Ghadiyaram, Ram Nevatia, Vignesh\n  Ramanathan", "title": "Activity Driven Weakly Supervised Object Detection", "comments": "CVPR'19 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection aims at reducing the amount of supervision\nrequired to train detection models. Such models are traditionally learned from\nimages/videos labelled only with the object class and not the object bounding\nbox. In our work, we try to leverage not only the object class labels but also\nthe action labels associated with the data. We show that the action depicted in\nthe image/video can provide strong cues about the location of the associated\nobject. We learn a spatial prior for the object dependent on the action (e.g.\n\"ball\" is closer to \"leg of the person\" in \"kicking ball\"), and incorporate\nthis prior to simultaneously train a joint object detection and action\nclassification model. We conducted experiments on both video datasets and image\ndatasets to evaluate the performance of our weakly supervised object detection\nmodel. Our approach outperformed the current state-of-the-art (SOTA) method by\nmore than 6% in mAP on the Charades video dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:52:39 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Yang", "Zhenheng", ""], ["Mahajan", "Dhruv", ""], ["Ghadiyaram", "Deepti", ""], ["Nevatia", "Ram", ""], ["Ramanathan", "Vignesh", ""]]}, {"id": "1904.01690", "submitter": "Jason Ku", "authors": "Jason Ku, Alex D. Pon, Steven L. Waslander", "title": "Monocular 3D Object Detection Leveraging Accurate Proposals and Shape\n  Reconstruction", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present MonoPSR, a monocular 3D object detection method that leverages\nproposals and shape reconstruction. First, using the fundamental relations of a\npinhole camera model, detections from a mature 2D object detector are used to\ngenerate a 3D proposal per object in a scene. The 3D location of these\nproposals prove to be quite accurate, which greatly reduces the difficulty of\nregressing the final 3D bounding box detection. Simultaneously, a point cloud\nis predicted in an object centered coordinate system to learn local scale and\nshape information. However, the key challenge is how to exploit shape\ninformation to guide 3D localization. As such, we devise aggregate losses,\nincluding a novel projection alignment loss, to jointly optimize these tasks in\nthe neural network to improve 3D localization accuracy. We validate our method\non the KITTI benchmark where we set new state-of-the-art results among\npublished monocular methods, including the harder pedestrian and cyclist\nclasses, while maintaining efficient run-time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:25:29 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ku", "Jason", ""], ["Pon", "Alex D.", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1904.01693", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Multigrid Predictive Filter Flow for Unsupervised Learning on Videos", "comments": "webpage (https://www.ics.uci.edu/~skong2/mgpff.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce multigrid Predictive Filter Flow (mgPFF), a framework for\nunsupervised learning on videos. The mgPFF takes as input a pair of frames and\noutputs per-pixel filters to warp one frame to the other. Compared to optical\nflow used for warping frames, mgPFF is more powerful in modeling sub-pixel\nmovement and dealing with corruption (e.g., motion blur). We develop a\nmultigrid coarse-to-fine modeling strategy that avoids the requirement of\nlearning large filters to capture large displacement. This allows us to train\nan extremely compact model (4.6MB) which operates in a progressive way over\nmultiple resolutions with shared weights. We train mgPFF on unsupervised,\nfree-form videos and show that mgPFF is able to not only estimate long-range\nflow for frame reconstruction and detect video shot transitions, but also\nreadily amendable for video object segmentation and pose tracking, where it\nsubstantially outperforms the published state-of-the-art without bells and\nwhistles. Moreover, owing to mgPFF's nature of per-pixel filter prediction, we\nhave the unique opportunity to visualize how each pixel is evolving during\nsolving these tasks, thus gaining better interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:41:48 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1904.01701", "submitter": "Pedro Miraldo", "authors": "G. Dias Pais, Srikumar Ramalingam, Venu Madhav Govindu, Jacinto C.\n  Nascimento, Rama Chellappa, and Pedro Miraldo", "title": "3DRegNet: A Deep Neural Network for 3D Point Registration", "comments": "15 pages, 8 figures, 6 tables", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 3DRegNet, a novel deep learning architecture for the registration\nof 3D scans. Given a set of 3D point correspondences, we build a deep neural\nnetwork to address the following two challenges: (i) classification of the\npoint correspondences into inliers/outliers, and (ii) regression of the motion\nparameters that align the scans into a common reference frame. With regard to\nregression, we present two alternative approaches: (i) a Deep Neural Network\n(DNN) registration and (ii) a Procrustes approach using SVD to estimate the\ntransformation. Our correspondence-based approach achieves a higher speedup\ncompared to competing baselines. We further propose the use of a refinement\nnetwork, which consists of a smaller 3DRegNet as a refinement to improve the\naccuracy of the registration. Extensive experiments on two challenging datasets\ndemonstrate that we outperform other methods and achieve state-of-the-art\nresults. The code is available.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:59:46 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:04:56 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Pais", "G. Dias", ""], ["Ramalingam", "Srikumar", ""], ["Govindu", "Venu Madhav", ""], ["Nascimento", "Jacinto C.", ""], ["Chellappa", "Rama", ""], ["Miraldo", "Pedro", ""]]}, {"id": "1904.01728", "submitter": "ShaoYing Wang", "authors": "Shaoying Wang, Haijiang Lai, Yifan Yang, and Jian Yin", "title": "Deep Policy Hashing Network with Listwise Supervision", "comments": "8 pages, accepted by ACM ICMR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-networks-based hashing has become a leading approach for large-scale\nimage retrieval, which learns a similarity-preserving network to map similar\nimages to nearby hash codes. The pairwise and triplet losses are two widely\nused similarity preserving manners for deep hashing. These manners ignore the\nfact that hashing is a prediction task on the list of binary codes. However,\nlearning deep hashing with listwise supervision is challenging in 1) how to\nobtain the rank list of whole training set when the batch size of the deep\nnetwork is always small and 2) how to utilize the listwise supervision. In this\npaper, we present a novel deep policy hashing architecture with two systems are\nlearned in parallel: a query network and a shared and slowly changing database\nnetwork. The following three steps are repeated until convergence: 1) the\ndatabase network encodes all training samples into binary codes to obtain a\nwhole rank list, 2) the query network is trained based on policy learning to\nmaximize a reward that indicates the performance of the whole ranking list of\nbinary codes, e.g., mean average precision (MAP), and 3) the database network\nis updated as the query network. Extensive evaluations on several benchmark\ndatasets show that the proposed method brings substantial improvements over\nstate-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:08:18 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Shaoying", ""], ["Lai", "Haijiang", ""], ["Yang", "Yifan", ""], ["Yin", "Jian", ""]]}, {"id": "1904.01739", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Guo-sen Xie, Yang Li, Sheng Li, Zi Huang", "title": "SADIH: Semantic-Aware DIscrete Hashing", "comments": "Accepted by The Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its low storage cost and fast query speed, hashing has been recognized\nto accomplish similarity search in large-scale multimedia retrieval\napplications. Particularly supervised hashing has recently received\nconsiderable research attention by leveraging the label information to preserve\nthe pairwise similarities of data points in the Hamming space. However, there\nstill remain two crucial bottlenecks: 1) the learning process of the full\npairwise similarity preservation is computationally unaffordable and unscalable\nto deal with big data; 2) the available category information of data are not\nwell-explored to learn discriminative hash functions. To overcome these\nchallenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH)\nframework, which aims to directly embed the transformed semantic information\ninto the asymmetric similarity approximation and discriminative hashing\nfunction learning. Specifically, a semantic-aware latent embedding is\nintroduced to asymmetrically preserve the full pairwise similarities while\nskillfully handle the cumbersome n times n pairwise similarity matrix.\nMeanwhile, a semantic-aware autoencoder is developed to jointly preserve the\ndata structures in the discriminative latent semantic space and perform data\nreconstruction. Moreover, an efficient alternating optimization algorithm is\nproposed to solve the resulting discrete optimization problem. Extensive\nexperimental results on multiple large-scale datasets demonstrate that our\nSADIH can clearly outperform the state-of-the-art baselines with the additional\nbenefit of lower computational costs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:45:05 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 04:51:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhang", "Zheng", ""], ["Xie", "Guo-sen", ""], ["Li", "Yang", ""], ["Li", "Sheng", ""], ["Huang", "Zi", ""]]}, {"id": "1904.01740", "submitter": "Javier Hernandez-Ortega", "authors": "Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Rudolf\n  Haraksim, Laurent Beslay", "title": "FaceQnet: Quality Assessment for Face Recognition based on Deep Learning", "comments": "Preprint version of a paper accepted at ICB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a Quality Assessment approach for face recognition\nbased on deep learning. The method consists of a Convolutional Neural Network,\nFaceQnet, that is used to predict the suitability of a specific input image for\nface recognition purposes. The training of FaceQnet is done using the VGGFace2\ndatabase. We employ the BioLab-ICAO framework for labeling the VGGFace2 images\nwith quality information related to their ICAO compliance level. The\ngroundtruth quality labels are obtained using FaceNet to generate comparison\nscores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making\nit capable of returning a numerical quality measure for each input image.\nFinally, we verify if the FaceQnet scores are suitable to predict the expected\nperformance when employing a specific image for face recognition with a COTS\nface recognition system. Several conclusions can be drawn from this work, most\nnotably: 1) we managed to employ an existing ICAO compliance framework and a\npretrained CNN to automatically label data with quality information, 2) we\ntrained FaceQnet for quality estimation by fine-tuning a pre-trained face\nrecognition network (ResNet-50), and 3) we have shown that the predictions from\nFaceQnet are highly correlated with the face recognition accuracy of a\nstate-of-the-art commercial system not used during development. FaceQnet is\npublicly available in GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 02:12:31 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 07:05:42 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Galbally", "Javier", ""], ["Fierrez", "Julian", ""], ["Haraksim", "Rudolf", ""], ["Beslay", "Laurent", ""]]}, {"id": "1904.01748", "submitter": "Sze Teng Liong", "authors": "Sze-Teng Liong, Y.S. Gan, Danna Zheng, Shu-Meng Lic, Hao-Xuan Xua,\n  Han-Zhe Zhang, Ran-Ke Lyu, Kun-Hong Liu", "title": "Evaluation of the Spatio-Temporal features and GAN for Micro-expression\n  Recognition System", "comments": "15 pages, 16 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the development and advancement of artificial intelligence, numerous\nworks were established in the human facial expression recognition system.\nMeanwhile, the detection and classification of micro-expressions are attracting\nattentions from various research communities in the recent few years. In this\npaper, we first review the processes of a conventional optical-flow-based\nrecognition system, which comprised of facial landmarks annotations, optical\nflow guided images computation, features extraction and emotion class\ncategorization. Secondly, a few approaches have been proposed to improve the\nfeature extraction part, such as exploiting GAN to generate more image samples.\nParticularly, several variations of optical flow are computed in order to\ngenerate optimal images to lead to high recognition accuracy. Next, GAN, a\ncombination of Generator and Discriminator, is utilized to generate new \"fake\"\nimages to increase the sample size. Thirdly, a modified state-of-the-art\nConvolutional neural networks is proposed. To verify the effectiveness of the\nthe proposed method, the results are evaluated on spontaneous micro-expression\ndatabases, namely SMIC, CASME II and SAMM. Both the F1-score and accuracy\nperformance metrics are reported in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 03:29:48 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Liong", "Sze-Teng", ""], ["Gan", "Y. S.", ""], ["Zheng", "Danna", ""], ["Lic", "Shu-Meng", ""], ["Xua", "Hao-Xuan", ""], ["Zhang", "Han-Zhe", ""], ["Lyu", "Ran-Ke", ""], ["Liu", "Kun-Hong", ""]]}, {"id": "1904.01749", "submitter": "Ting Sun", "authors": "Ting Sun, Lei Tai, Zhihan Gao, Ming Liu, Dit-Yan Yeung", "title": "Fully Using Classifiers for Weakly Supervised Semantic Segmentation with\n  Modified Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel weakly-supervised semantic segmentation method\nusing image-level label only. The class-specific activation maps from the\nwell-trained classifiers are used as cues to train a segmentation network. The\nwell-known defects of these cues are coarseness and incompleteness. We use\nsuper-pixel to refine them, and fuse the cues extracted from both a color image\ntrained classifier and a gray image trained classifier to compensate for their\nincompleteness. The conditional random field is adapted to regulate the\ntraining process and to refine the outputs further. Besides initializing the\nsegmentation network, the previously trained classifier is also used in the\ntesting phase to suppress the non-existing classes. Experimental results on the\nPASCAL VOC 2012 dataset illustrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 03:30:51 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 06:56:51 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Sun", "Ting", ""], ["Tai", "Lei", ""], ["Gao", "Zhihan", ""], ["Liu", "Ming", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1904.01755", "submitter": "Yang Wang", "authors": "Lin Wu, Richang Hong, Yang Wang, Meng Wang", "title": "Cross-Entropy Adversarial View Adaptation for Person Re-identification", "comments": "Appearing at IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is a task of matching pedestrians under\ndisjoint camera views. To recognise paired snapshots, it has to cope with large\ncross-view variations caused by the camera view shift. Supervised deep neural\nnetworks are effective in producing a set of non-linear projections that can\ntransform cross-view images into a common feature space. However, they\ntypically impose a symmetric architecture, yielding the network ill-conditioned\non its optimisation. In this paper, we learn view-invariant subspace for person\nre-ID, and its corresponding similarity metric using an adversarial view\nadaptation approach. The main contribution is to learn coupled asymmetric\nmappings regarding view characteristics which are adversarially trained to\naddress the view discrepancy by optimising the cross-entropy view confusion\nobjective. To determine the similarity value, the network is empowered with a\nsimilarity discriminator to promote features that are highly discriminant in\ndistinguishing positive and negative pairs. The other contribution includes an\nadaptive weighing on the most difficult samples to address the imbalance of\nwithin/between-identity pairs. Our approach achieves notable improved\nperformance in comparison to state-of-the-arts on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 03:52:21 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 02:25:53 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wu", "Lin", ""], ["Hong", "Richang", ""], ["Wang", "Yang", ""], ["Wang", "Meng", ""]]}, {"id": "1904.01759", "submitter": "Lipu Zhou", "authors": "Lipu Zhou, Shengze Wang, Jiamin Ye, Michael Kaess", "title": "Do not Omit Local Minimizer: a Complete Solution for Pose Estimation\n  from 3D Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating pose from given 3D correspondences, including point-to-point,\npoint-to-line and point-to-plane correspondences, is a fundamental task in\ncomputer vision with many applications. We present a complete solution for this\ntask, including a solution for the minimal problem and the least-squares\nproblem of this task. Previous works mainly focused on finding the global\nminimizer to address the least-squares problem. However, existing works that\nshow the ability to achieve global minimizer are still unsuitable for real-time\napplications. Furthermore, as one of contributions of this paper, we prove that\nthere exist ambiguous configurations for any number of lines and planes. These\nconfigurations have several solutions in theory, which makes the correct\nsolution may come from a local minimizer. Our algorithm is efficient and able\nto reveal local minimizers. We employ the Cayley-Gibbs-Rodriguez (CGR)\nparameterization of the rotation to derive a general rational cost for the\nthree cases of 3D correspondences. The main contribution of this paper is to\nsolve the resulting equation system of the minimal problem and the first-order\noptimality conditions of the least-squares problem, both of which are of\ncomplicated rational forms. The central idea of our algorithm is to introduce\nintermediate unknowns to simplify the problem. Extensive experimental results\nshow that our algorithm significantly outperforms previous algorithms when the\nnumber of correspondences is small. Besides, when the global minimizer is the\nsolution, our algorithm achieves the same accuracy as previous algorithms that\nhave guaranteed global optimality, but our algorithm is applicable to real-time\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 04:23:13 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 13:46:54 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Zhou", "Lipu", ""], ["Wang", "Shengze", ""], ["Ye", "Jiamin", ""], ["Kaess", "Michael", ""]]}, {"id": "1904.01766", "submitter": "Chen Sun", "authors": "Chen Sun and Austin Myers and Carl Vondrick and Kevin Murphy and\n  Cordelia Schmid", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning", "comments": "ICCV 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has become increasingly important to leverage the\nabundance of unlabeled data available on platforms like YouTube. Whereas most\nexisting approaches learn low-level representations, we propose a joint\nvisual-linguistic model to learn high-level features without any explicit\nsupervision. In particular, inspired by its recent success in language\nmodeling, we build upon the BERT model to learn bidirectional joint\ndistributions over sequences of visual and linguistic tokens, derived from\nvector quantization of video data and off-the-shelf speech recognition outputs,\nrespectively. We use VideoBERT in numerous tasks, including action\nclassification and video captioning. We show that it can be applied directly to\nopen-vocabulary classification, and confirm that large amounts of training data\nand cross-modal information are critical to performance. Furthermore, we\noutperform the state-of-the-art on video captioning, and quantitative results\nverify that the model learns high-level semantic features.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 04:40:16 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 19:52:54 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Sun", "Chen", ""], ["Myers", "Austin", ""], ["Vondrick", "Carl", ""], ["Murphy", "Kevin", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1904.01769", "submitter": "Peng Zhou", "authors": "Peng Zhou, Long Mai, Jianming Zhang, Ning Xu, Zuxuan Wu, Larry S.\n  Davis", "title": "M2KD: Multi-model and Multi-level Knowledge Distillation for Incremental\n  Learning", "comments": null, "journal-ref": "BMVC 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning targets at achieving good performance on new categories\nwithout forgetting old ones. Knowledge distillation has been shown critical in\npreserving the performance on old classes. Conventional methods, however,\nsequentially distill knowledge only from the last model, leading to performance\ndegradation on the old classes in later incremental learning steps. In this\npaper, we propose a multi-model and multi-level knowledge distillation\nstrategy. Instead of sequentially distilling knowledge only from the last\nmodel, we directly leverage all previous model snapshots. In addition, we\nincorporate an auxiliary distillation to further preserve knowledge encoded at\nthe intermediate feature levels. To make the model more memory efficient, we\nadapt mask based pruning to reconstruct all previous models with a small memory\nfootprint. Experiments on standard incremental learning benchmarks show that\nour method preserves the knowledge on old classes better and improves the\noverall performance over standard distillation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 04:54:01 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 04:41:31 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhou", "Peng", ""], ["Mai", "Long", ""], ["Zhang", "Jianming", ""], ["Xu", "Ning", ""], ["Wu", "Zuxuan", ""], ["Davis", "Larry S.", ""]]}, {"id": "1904.01772", "submitter": "Xin Li", "authors": "Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, Ming-Hsuan Yang", "title": "Target-Aware Deep Tracking", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep trackers mainly use convolutional neural networks pre-trained\nfor generic object recognition task for representations. Despite demonstrated\nsuccesses for numerous vision tasks, the contributions of using pre-trained\ndeep features for visual tracking are not as significant as that for object\nrecognition. The key issue is that in visual tracking the targets of interest\ncan be arbitrary object class with arbitrary forms. As such, pre-trained deep\nfeatures are less effective in modeling these targets of arbitrary forms for\ndistinguishing them from the background. In this paper, we propose a novel\nscheme to learn target-aware features, which can better recognize the targets\nundergoing significant appearance variations than pre-trained deep features. To\nthis end, we develop a regression loss and a ranking loss to guide the\ngeneration of target-active and scale-sensitive features. We identify the\nimportance of each convolutional filter according to the back-propagated\ngradients and select the target-aware features based on activations for\nrepresenting the targets. The target-aware features are integrated with a\nSiamese matching network for visual tracking. Extensive experimental results\nshow that the proposed algorithm performs favorably against the\nstate-of-the-art methods in terms of accuracy and speed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:06:39 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Li", "Xin", ""], ["Ma", "Chao", ""], ["Wu", "Baoyuan", ""], ["He", "Zhenyu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1904.01774", "submitter": "Atsuhiro Noguchi", "authors": "Atsuhiro Noguchi and Tatsuya Harada", "title": "Image Generation From Small Datasets via Batch Statistics Adaptation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the recent development of deep generative models, it is becoming\npossible to generate high-quality images with both fidelity and diversity.\nHowever, the training of such generative models requires a large dataset. To\nreduce the amount of data required, we propose a new method for transferring\nprior knowledge of the pre-trained generator, which is trained with a large\ndataset, to a small dataset in a different domain. Using such prior knowledge,\nthe model can generate images leveraging some common sense that cannot be\nacquired from a small dataset. In this work, we propose a novel method focusing\non the parameters for batch statistics, scale and shift, of the hidden layers\nin the generator. By training only these parameters in a supervised manner, we\nachieved stable training of the generator, and our method can generate higher\nquality images compared to previous methods without collapsing, even when the\ndataset is small (~100). Our results show that the diversity of the filters\nacquired in the pre-trained generator is important for the performance on the\ntarget domain. Our method makes it possible to add a new class or domain to a\npre-trained generator without disturbing the performance on the original\ndomain.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:24:02 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 15:53:48 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 10:03:23 GMT"}, {"version": "v4", "created": "Wed, 23 Oct 2019 08:28:38 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Noguchi", "Atsuhiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1904.01782", "submitter": "Rui Liu", "authors": "Rui Liu, Yu Liu, Xinyu Gong, Xiaogang Wang, Hongsheng Li", "title": "Conditional Adversarial Generative Flow for Controllable Image Synthesis", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models show great potential in image synthesis due to\nits reversible pipeline and exact log-likelihood target, yet it suffers from\nweak ability for conditional image synthesis, especially for multi-label or\nunaware conditions. This is because the potential distribution of image\nconditions is hard to measure precisely from its latent variable $z$. In this\npaper, based on modeling a joint probabilistic density of an image and its\nconditions, we propose a novel flow-based generative model named conditional\nadversarial generative flow (CAGlow). Instead of disentangling attributes from\nlatent space, we blaze a new trail for learning an encoder to estimate the\nmapping from condition space to latent space in an adversarial manner. Given a\nspecific condition $c$, CAGlow can encode it to a sampled $z$, and then enable\nrobust conditional image synthesis in complex situations like combining person\nidentity with multiple attributes. The proposed CAGlow can be implemented in\nboth supervised and unsupervised manners, thus can synthesize images with\nconditional information like categories, attributes, and even some unknown\nproperties. Extensive experiments show that CAGlow ensures the independence of\ndifferent conditions and outperforms regular Glow to a significant extent.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:58:01 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Liu", "Rui", ""], ["Liu", "Yu", ""], ["Gong", "Xinyu", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1904.01784", "submitter": "Yuning Chai", "authors": "Yuning Chai", "title": "Patchwork: A Patch-wise Attention Network for Efficient Object Detection\n  and Segmentation in Video Streams", "comments": "ICCV 2019 Camera Ready + Supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in single-frame object detection and segmentation techniques\nhave motivated a wide range of works to extend these methods to process video\nstreams. In this paper, we explore the idea of hard attention aimed for\nlatency-sensitive applications. Instead of reasoning about every frame\nseparately, our method selects and only processes a small sub-window of the\nframe. Our technique then makes predictions for the full frame based on the\nsub-windows from previous frames and the update from the current sub-window.\nThe latency reduction by this hard attention mechanism comes at the cost of\ndegraded accuracy. We made two contributions to address this. First, we propose\na specialized memory cell that recovers lost context when processing\nsub-windows. Secondly, we adopt a Q-learning-based policy training strategy\nthat enables our approach to intelligently select the sub-windows such that the\nstaleness in the memory hurts the performance the least. Our experiments\nsuggest that our approach reduces the latency by approximately four times\nwithout significantly sacrificing the accuracy on the ImageNet VID video object\ndetection dataset and the DAVIS video object segmentation dataset. We further\ndemonstrate that we can reinvest the saved computation into other parts of the\nnetwork, and thus resulting in an accuracy increase at a comparable\ncomputational cost as the original system and beating other recently proposed\nstate-of-the-art methods in the low latency range.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:58:42 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 17:11:31 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Chai", "Yuning", ""]]}, {"id": "1904.01786", "submitter": "Weikai Chen", "authors": "Shichen Liu, Tianye Li, Weikai Chen, Hao Li", "title": "Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning", "comments": "This is a substantially revised version of previous submission:\n  arXiv:1901.05567", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering bridges the gap between 2D vision and 3D scenes by simulating the\nphysical process of image formation. By inverting such renderer, one can think\nof a learning approach to infer 3D information from 2D images. However,\nstandard graphics renderers involve a fundamental discretization step called\nrasterization, which prevents the rendering process to be differentiable, hence\nable to be learned. Unlike the state-of-the-art differentiable renderers, which\nonly approximate the rendering gradient in the back propagation, we propose a\ntruly differentiable rendering framework that is able to (1) directly render\ncolorized mesh using differentiable functions and (2) back-propagate efficient\nsupervision signals to mesh vertices and their attributes from various forms of\nimage representations, including silhouette, shading and color images. The key\nto our framework is a novel formulation that views rendering as an aggregation\nfunction that fuses the probabilistic contributions of all mesh triangles with\nrespect to the rendered pixels. Such formulation enables our framework to flow\ngradients to the occluded and far-range vertices, which cannot be achieved by\nthe previous state-of-the-arts. We show that by using the proposed renderer,\none can achieve significant improvement in 3D unsupervised single-view\nreconstruction both qualitatively and quantitatively. Experiments also\ndemonstrate that our approach is able to handle the challenging tasks in\nimage-based shape fitting, which remain nontrivial to existing differentiable\nrenderers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:06:43 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Liu", "Shichen", ""], ["Li", "Tianye", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "1904.01795", "submitter": "Ty Nguyen", "authors": "Ty Nguyen, Shreyas S. Shivakumar, Ian D. Miller, James Keller, Elijah\n  S. Lee, Alex Zhou, Tolga Ozaslan, Giuseppe Loianno, Joseph H. Harwood,\n  Jennifer Wozencraft, Camillo J. Taylor, Vijay Kumar", "title": "MAVNet: an Effective Semantic Segmentation Micro-Network for MAV-based\n  Tasks", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic image segmentation on platforms subject to size, weight\nand power (SWaP) constraints is a key area of interest for air surveillance and\ninspection. In this work, we propose MAVNet: a small, light-weight, deep neural\nnetwork for real-time semantic segmentation on micro Aerial Vehicles (MAVs).\nMAVNet, inspired by ERFNet, features 400 times fewer parameters and achieves\ncomparable performance with some reference models in empirical experiments. Our\nmodel achieves a trade-off between speed and accuracy, achieving up to 48 FPS\non an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high\nresolution imagery. Additionally, we provide two novel datasets that represent\nchallenges in semantic segmentation for real-time MAV tracking and\ninfrastructure inspection tasks and verify MAVNet on these datasets. Our\nalgorithm and datasets are made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:36:26 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 23:03:57 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Nguyen", "Ty", ""], ["Shivakumar", "Shreyas S.", ""], ["Miller", "Ian D.", ""], ["Keller", "James", ""], ["Lee", "Elijah S.", ""], ["Zhou", "Alex", ""], ["Ozaslan", "Tolga", ""], ["Loianno", "Giuseppe", ""], ["Harwood", "Joseph H.", ""], ["Wozencraft", "Jennifer", ""], ["Taylor", "Camillo J.", ""], ["Kumar", "Vijay", ""]]}, {"id": "1904.01802", "submitter": "Baoyun Peng", "authors": "Baoyun Peng, Xiao Jin, Jiaheng Liu, Shunfeng Zhou, Yichao Wu, Yu Liu,\n  Dongsheng Li, Zhaoning Zhang", "title": "Correlation Congruence for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most teacher-student frameworks based on knowledge distillation (KD) depend\non a strong congruent constraint on instance level. However, they usually\nignore the correlation between multiple instances, which is also valuable for\nknowledge transfer. In this work, we propose a new framework named correlation\ncongruence for knowledge distillation (CCKD), which transfers not only the\ninstance-level information, but also the correlation between instances.\nFurthermore, a generalized kernel method based on Taylor series expansion is\nproposed to better capture the correlation between instances. Empirical\nexperiments and ablation studies on image classification tasks (including\nCIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face\nRecognition) show that the proposed CCKD substantially outperforms the original\nKD and achieves state-of-the-art accuracy compared with other SOTA KD-based\nmethods. The CCKD can be easily deployed in the majority of the teacher-student\nframework such as KD and hint-based learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:58:10 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Peng", "Baoyun", ""], ["Jin", "Xiao", ""], ["Liu", "Jiaheng", ""], ["Zhou", "Shunfeng", ""], ["Wu", "Yichao", ""], ["Liu", "Yu", ""], ["Li", "Dongsheng", ""], ["Zhang", "Zhaoning", ""]]}, {"id": "1904.01803", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Kuiyuan Yang", "title": "GFF: Gated Fully Fusion for Semantic Segmentation", "comments": "accepted by AAAI-2020(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation generates comprehensive understanding of scenes through\ndensely predicting the category for each pixel. High-level features from Deep\nConvolutional Neural Networks already demonstrate their effectiveness in\nsemantic segmentation tasks, however the coarse resolution of high-level\nfeatures often leads to inferior results for small/thin objects where detailed\ninformation is important. It is natural to consider importing low level\nfeatures to compensate for the lost detailed information in high-level\nfeatures.Unfortunately, simply combining multi-level features suffers from the\nsemantic gap among them. In this paper, we propose a new architecture, named\nGated Fully Fusion (GFF), to selectively fuse features from multiple levels\nusing gates in a fully connected way. Specifically, features at each level are\nenhanced by higher-level features with stronger semantics and lower-level\nfeatures with more details, and gates are used to control the propagation of\nuseful information which significantly reduces the noises during fusion. We\nachieve the state of the art results on four challenging scene parsing datasets\nincluding Cityscapes, Pascal Context, COCO-stuff and ADE20K.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:00:16 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 09:50:37 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Li", "Xiangtai", ""], ["Zhao", "Houlong", ""], ["Han", "Lei", ""], ["Tong", "Yunhai", ""], ["Yang", "Kuiyuan", ""]]}, {"id": "1904.01810", "submitter": "Dohyung Kim MR", "authors": "Junghyup Lee, Dohyung Kim, Jean Ponce, Bumsub Ham", "title": "SFNet: Learning Object-aware Semantic Correspondence", "comments": "cvpr 2019 oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semantic correspondence, that is, establishing a\ndense flow field between images depicting different instances of the same\nobject or scene category. We propose to use images annotated with binary\nforeground masks and subjected to synthetic geometric deformations to train a\nconvolutional neural network (CNN) for this task. Using these masks as part of\nthe supervisory signal offers a good compromise between semantic flow methods,\nwhere the amount of training data is limited by the cost of manually selecting\npoint correspondences, and semantic alignment ones, where the regression of a\nsingle global geometric transformation between images may be sensitive to\nimage-specific details such as background clutter. We propose a new CNN\narchitecture, dubbed SFNet, which implements this idea. It leverages a new and\ndifferentiable version of the argmax function for end-to-end training, with a\nloss that combines mask and flow consistency with smoothness terms.\nExperimental results demonstrate the effectiveness of our approach, which\nsignificantly outperforms the state of the art on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:33:21 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 01:10:03 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Lee", "Junghyup", ""], ["Kim", "Dohyung", ""], ["Ponce", "Jean", ""], ["Ham", "Bumsub", ""]]}, {"id": "1904.01828", "submitter": "Ning Wang", "authors": "Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li", "title": "Unsupervised Deep Tracking", "comments": "to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised visual tracking method in this paper. Different\nfrom existing approaches using extensive annotated data for supervised\nlearning, our CNN model is trained on large-scale unlabeled videos in an\nunsupervised manner. Our motivation is that a robust tracker should be\neffective in both the forward and backward predictions (i.e., the tracker can\nforward localize the target object in successive frames and backtrace to its\ninitial position in the first frame). We build our framework on a Siamese\ncorrelation filter network, which is trained using unlabeled raw videos.\nMeanwhile, we propose a multiple-frame validation method and a cost-sensitive\nloss to facilitate unsupervised learning. Without bells and whistles, the\nproposed unsupervised tracker achieves the baseline accuracy of fully\nsupervised trackers, which require complete and accurate labels during\ntraining. Furthermore, unsupervised framework exhibits a potential in\nleveraging unlabeled or weakly labeled data to further improve the tracking\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:14:11 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Ning", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Zhou", "Wengang", ""], ["Liu", "Wei", ""], ["Li", "Houqiang", ""]]}, {"id": "1904.01830", "submitter": "Yichao Yan", "authors": "Yichao Yan, Qiang Zhang, Bingbing Ni, Wendong Zhang, Minghao Xu,\n  Xiaokang Yang", "title": "Learning Context Graph for Person Search", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification has achieved great progress with deep convolutional\nneural networks. However, most previous methods focus on learning individual\nappearance feature embedding, and it is hard for the models to handle difficult\nsituations with different illumination, large pose variance and occlusion. In\nthis work, we take a step further and consider employing context information\nfor person search. For a probe-gallery pair, we first propose a contextual\ninstance expansion module, which employs a relative attention module to search\nand filter useful context information in the scene. We also build a graph\nlearning framework to effectively employ context pairs to update target\nsimilarity. These two modules are built on top of a joint detection and\ninstance feature learning framework, which improves the discriminativeness of\nthe learned features. The proposed framework achieves state-of-the-art\nperformance on two widely used person search datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:16:11 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Yan", "Yichao", ""], ["Zhang", "Qiang", ""], ["Ni", "Bingbing", ""], ["Zhang", "Wendong", ""], ["Xu", "Minghao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1904.01866", "submitter": "Byeongho Heo", "authors": "Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, Jin\n  Young Choi", "title": "A Comprehensive Overhaul of Feature Distillation", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the design aspects of feature distillation methods achieving\nnetwork compression and propose a novel feature distillation method in which\nthe distillation loss is designed to make a synergy among various aspects:\nteacher transform, student transform, distillation feature position and\ndistance function. Our proposed distillation loss includes a feature transform\nwith a newly designed margin ReLU, a new distillation feature position, and a\npartial L2 distance function to skip redundant information giving adverse\neffects to the compression of student. In ImageNet, our proposed method\nachieves 21.65% of top-1 error with ResNet50, which outperforms the performance\nof the teacher network, ResNet152. Our proposed method is evaluated on various\ntasks such as image classification, object detection and semantic segmentation\nand achieves a significant performance improvement in all tasks. The code is\navailable at https://sites.google.com/view/byeongho-heo/overhaul\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:17:59 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 05:07:09 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Heo", "Byeongho", ""], ["Kim", "Jeesoo", ""], ["Yun", "Sangdoo", ""], ["Park", "Hyojin", ""], ["Kwak", "Nojun", ""], ["Choi", "Jin Young", ""]]}, {"id": "1904.01870", "submitter": "Shanshan Zhao", "authors": "Shanshan Zhao, Huan Fu, Mingming Gong and Dacheng Tao", "title": "Geometry-Aware Symmetric Domain Adaptation for Monocular Depth\n  Estimation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised depth estimation has achieved high accuracy due to the advanced\ndeep network architectures. Since the groundtruth depth labels are hard to\nobtain, recent methods try to learn depth estimation networks in an\nunsupervised way by exploring unsupervised cues, which are effective but less\nreliable than true labels. An emerging way to resolve this dilemma is to\ntransfer knowledge from synthetic images with ground truth depth via domain\nadaptation techniques. However, these approaches overlook specific geometric\nstructure of the natural images in the target domain (i.e., real data), which\nis important for high-performing depth prediction. Motivated by the\nobservation, we propose a geometry-aware symmetric domain adaptation framework\n(GASDA) to explore the labels in the synthetic data and epipolar geometry in\nthe real data jointly. Moreover, by training two image style translators and\ndepth estimators symmetrically in an end-to-end network, our model achieves\nbetter image style transfer and generates high-quality depth maps. The\nexperimental results demonstrate the effectiveness of our proposed method and\ncomparable performance against the state-of-the-art. Code will be publicly\navailable at: https://github.com/sshan-zhao/GASDA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:23:03 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Zhao", "Shanshan", ""], ["Fu", "Huan", ""], ["Gong", "Mingming", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.01886", "submitter": "Tuan-Hung Vu", "authors": "Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord and Patrick\n  P\\'erez", "title": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation", "comments": "Accepted in ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) is important for applications where\nlarge scale annotation of representative data is challenging. For semantic\nsegmentation in particular, it helps deploy on real \"target domain\" data models\nthat are trained on annotated images from a different \"source domain\", notably\na virtual environment. To this end, most previous works consider semantic\nsegmentation as the only mode of supervision for source domain data, while\nignoring other, possibly available, information like depth. In this work, we\naim at exploiting at best such a privileged information while training the UDA\nmodel. We propose a unified depth-aware UDA framework that leverages in several\ncomplementary ways the knowledge of dense depth in the source domain. As a\nresult, the performance of the trained semantic segmentation model on the\ntarget domain is boosted. Our novel approach indeed achieves state-of-the-art\nperformance on different challenging synthetic-2-real benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:59:06 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 12:08:16 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 09:22:00 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Vu", "Tuan-Hung", ""], ["Jain", "Himalaya", ""], ["Bucher", "Maxime", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1904.01892", "submitter": "Fei Xue", "authors": "Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, Hongbin Zha", "title": "Beyond Tracking: Selecting Memory and Refining Poses for Deep Visual\n  Odometry", "comments": "Accepted to CVPR2019 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous learning-based visual odometry (VO) methods take VO as a pure\ntracking problem. In contrast, we present a VO framework by incorporating two\nadditional components called Memory and Refining. The Memory component\npreserves global information by employing an adaptive and efficient selection\nstrategy. The Refining component ameliorates previous results with the contexts\nstored in the Memory by adopting a spatial-temporal attention mechanism for\nfeature distilling. Experiments on the KITTI and TUM-RGBD benchmark datasets\ndemonstrate that our method outperforms state-of-the-art learning-based methods\nby a large margin and produces competitive results against classic monocular VO\napproaches. Especially, our model achieves outstanding performance in\nchallenging scenarios such as texture-less regions and abrupt motions, where\nclassic VO algorithms tend to fail.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 10:11:22 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 03:16:52 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Xue", "Fei", ""], ["Wang", "Xin", ""], ["Li", "Shunkai", ""], ["Wang", "Qiuyuan", ""], ["Wang", "Junqiu", ""], ["Zha", "Hongbin", ""]]}, {"id": "1904.01893", "submitter": "Xinjie Li", "authors": "Xinjie Li, Chun Yang, Songlu Chen, Chao Zhu, Xu-Cheng Yin", "title": "Semantic Bilinear Pooling for Fine-Grained Recognition", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturally, fine-grained recognition, e.g., vehicle identification or bird\nclassification, has specific hierarchical labels, where fine categories are\nalways harder to be classified than coarse categories. However, most of the\nrecent deep learning based methods neglect the semantic structure of\nfine-grained objects and do not take advantage of the traditional fine-grained\nrecognition techniques (e.g. coarse-to-fine classification). In this paper, we\npropose a novel framework with a two-branch network (coarse branch and fine\nbranch), i.e., semantic bilinear pooling, for fine-grained recognition with a\nhierarchical label tree. This framework can adaptively learn the semantic\ninformation from the hierarchical levels. Specifically, we design a generalized\ncross-entropy loss for the training of the proposed framework to fully exploit\nthe semantic priors via considering the relevance between adjacent levels and\nenlarge the distance between samples of different coarse classes. Furthermore,\nour method leverages only the fine branch when testing so that it adds no\noverhead to the testing time. Experimental results show that our proposed\nmethod achieves state-of-the-art performance on four public datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 10:14:57 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 01:53:53 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 10:09:58 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Li", "Xinjie", ""], ["Yang", "Chun", ""], ["Chen", "Songlu", ""], ["Zhu", "Chao", ""], ["Yin", "Xu-Cheng", ""]]}, {"id": "1904.01906", "submitter": "JeongHun Baek", "authors": "Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han,\n  Sangdoo Yun, Seong Joon Oh, Hwalsuk Lee", "title": "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and\n  Model Analysis", "comments": "Oral paper at ICCV'19. Our code is publicly available.\n  (https://github.com/clovaai/deep-text-recognition-benchmark)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many new proposals for scene text recognition (STR) models have been\nintroduced in recent years. While each claim to have pushed the boundary of the\ntechnology, a holistic and fair comparison has been largely missing in the\nfield due to the inconsistent choices of training and evaluation datasets. This\npaper addresses this difficulty with three major contributions. First, we\nexamine the inconsistencies of training and evaluation datasets, and the\nperformance gap results from inconsistencies. Second, we introduce a unified\nfour-stage STR framework that most existing STR models fit into. Using this\nframework allows for the extensive evaluation of previously proposed STR\nmodules and the discovery of previously unexplored module combinations. Third,\nwe analyze the module-wise contributions to performance in terms of accuracy,\nspeed, and memory demand, under one consistent set of training and evaluation\ndatasets. Such analyses clean up the hindrance on the current comparisons to\nunderstand the performance gain of the existing modules.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 10:45:29 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 09:20:35 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 05:31:18 GMT"}, {"version": "v4", "created": "Wed, 18 Dec 2019 11:40:03 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Baek", "Jeonghun", ""], ["Kim", "Geewook", ""], ["Lee", "Junyeop", ""], ["Park", "Sungrae", ""], ["Han", "Dongyoon", ""], ["Yun", "Sangdoo", ""], ["Oh", "Seong Joon", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "1904.01908", "submitter": "Pierre Falez", "authors": "Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne,\n  Pierre Boulet", "title": "Multi-layered Spiking Neural Network with Target Timestamp Threshold\n  Adaptation and STDP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are good candidates to produce\nultra-energy-efficient hardware. However, the performance of these models is\ncurrently behind traditional methods. Introducing multi-layered SNNs is a\npromising way to reduce this gap. We propose in this paper a new threshold\nadaptation system which uses a timestamp objective at which neurons should\nfire. We show that our method leads to state-of-the-art classification rates on\nthe MNIST dataset (98.60%) and the Faces/Motorbikes dataset (99.46%) with an\nunsupervised SNN followed by a linear SVM. We also investigate the sparsity\nlevel of the network by testing different inhibition policies and STDP rules.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 10:47:34 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Falez", "Pierre", ""], ["Tirilly", "Pierre", ""], ["Bilasco", "Ioan Marius", ""], ["Devienne", "Philippe", ""], ["Boulet", "Pierre", ""]]}, {"id": "1904.01909", "submitter": "Soumya Tripathy", "authors": "Soumya Tripathy, Juho Kannala and Esa Rahtu", "title": "ICface: Interpretable and Controllable Face Reenactment Using GANs", "comments": "Accepted in WACV-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generic face animator that is able to control the pose\nand expressions of a given face image. The animation is driven by human\ninterpretable control signals consisting of head pose angles and the Action\nUnit (AU) values. The control information can be obtained from multiple sources\nincluding external driving videos and manual controls. Due to the interpretable\nnature of the driving signal, one can easily mix the information between\nmultiple sources (e.g. pose from one image and expression from another) and\napply selective post-production editing. The proposed face animator is\nimplemented as a two-stage neural network model that is learned in a\nself-supervised manner using a large video collection. The proposed\nInterpretable and Controllable face reenactment network (ICface) is compared to\nthe state-of-the-art neural network-based face animation techniques in multiple\ntasks. The results indicate that ICface produces better visual quality while\nbeing more versatile than most of the comparison methods. The introduced model\ncould provide a lightweight and easy to use tool for a multitude of advanced\nimage and video editing tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 10:49:03 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 14:30:59 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Tripathy", "Soumya", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "1904.01920", "submitter": "Ahti Kalervo", "authors": "Ahti Kalervo, Juha Ylioinas, Markus H\\\"aiki\\\"o, Antti Karhu, Juho\n  Kannala", "title": "CubiCasa5K: A Dataset and an Improved Multi-Task Model for Floorplan\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Better understanding and modelling of building interiors and the emergence of\nmore impressive AR/VR technology has brought up the need for automatic parsing\nof floorplan images. However, there is a clear lack of representative datasets\nto investigate the problem further. To address this shortcoming, this paper\npresents a novel image dataset called CubiCasa5K, a large-scale floorplan image\ndataset containing 5000 samples annotated into over 80 floorplan object\ncategories. The dataset annotations are performed in a dense and versatile\nmanner by using polygons for separating the different objects. Diverging from\nthe classical approaches based on strong heuristics and low-level pixel\noperations, we present a method relying on an improved multi-task convolutional\nneural network. By releasing the novel dataset and our implementations, this\nstudy significantly boosts the research on automatic floorplan image analysis\nas it provides a richer set of tools for investigating the problem in a more\ncomprehensive manner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 11:23:59 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Kalervo", "Ahti", ""], ["Ylioinas", "Juha", ""], ["H\u00e4iki\u00f6", "Markus", ""], ["Karhu", "Antti", ""], ["Kannala", "Juho", ""]]}, {"id": "1904.01941", "submitter": "Youngmin Baek", "authors": "Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee", "title": "Character Region Awareness for Text Detection", "comments": "12 pages, 11 figures, Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection methods based on neural networks have emerged recently\nand have shown promising results. Previous methods trained with rigid\nword-level bounding boxes exhibit limitations in representing the text region\nin an arbitrary shape. In this paper, we propose a new scene text detection\nmethod to effectively detect text area by exploring each character and affinity\nbetween characters. To overcome the lack of individual character level\nannotations, our proposed framework exploits both the given character-level\nannotations for synthetic images and the estimated character-level\nground-truths for real images acquired by the learned interim model. In order\nto estimate affinity between characters, the network is trained with the newly\nproposed representation for affinity. Extensive experiments on six benchmarks,\nincluding the TotalText and CTW-1500 datasets which contain highly curved texts\nin natural images, demonstrate that our character-level text detection\nsignificantly outperforms the state-of-the-art detectors. According to the\nresults, our proposed method guarantees high flexibility in detecting\ncomplicated scene text images, such as arbitrarily-oriented, curved, or\ndeformed texts.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 12:00:33 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Baek", "Youngmin", ""], ["Lee", "Bado", ""], ["Han", "Dongyoon", ""], ["Yun", "Sangdoo", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "1904.01954", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Yujiang Wang, Pingchuan Ma, Zuwei Li, Maja Pantic", "title": "End-to-End Visual Speech Recognition for Small-Scale Datasets", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual speech recognition models traditionally consist of two stages, feature\nextraction and classification. Several deep learning approaches have been\nrecently presented aiming to replace the feature extraction stage by\nautomatically extracting features from mouth images. However, research on joint\nlearning of features and classification remains limited. In addition, most of\nthe existing methods require large amounts of data in order to achieve\nstate-of-the-art performance, otherwise they under-perform. In this work, we\npresent an end-to-end visual speech recognition system based on fully-connected\nlayers and Long-Short Memory (LSTM) networks which is suitable for small-scale\ndatasets. The model consists of two streams which extract features directly\nfrom the mouth and difference images, respectively. The temporal dynamics in\neach stream are modelled by a Bidirectional LSTM (BLSTM) and the fusion of the\ntwo streams takes place via another BLSTM. An absolute improvement of 0.6%,\n3.4%, 3.9%, 11.4% over the state-of-the-art is reported on the OuluVS2, CUAVE,\nAVLetters and AVLetters2 databases, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:57:51 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 16:10:22 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 17:58:41 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 17:42:11 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Petridis", "Stavros", ""], ["Wang", "Yujiang", ""], ["Ma", "Pingchuan", ""], ["Li", "Zuwei", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.01971", "submitter": "Hao Wang", "authors": "Hao Wang, Cheng Deng, Xinxu Xu, Wei Liu, Xinbo Gao, Dacheng Tao", "title": "Stacked Semantic-Guided Network for Zero-Shot Sketch-Based Image\n  Retrieval", "comments": "withdraw this paper due to minor problem in problem defination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot sketch-based image retrieval (ZS-SBIR) is a task of cross-domain\nimage retrieval from a natural image gallery with free-hand sketch under a\nzero-shot scenario. Previous works mostly focus on a generative approach that\ntakes a highly abstract and sparse sketch as input and then synthesizes the\ncorresponding natural image. However, the intrinsic visual sparsity and large\nintra-class variance of the sketch make the learning of the conditional decoder\nmore difficult and hence achieve unsatisfactory retrieval performance. In this\npaper, we propose a novel stacked semantic-guided network to address the unique\ncharacteristics of sketches in ZS-SBIR. Specifically, we devise multi-layer\nfeature fusion networks that incorporate different intermediate feature\nrepresentation information in a deep neural network to alleviate the intrinsic\nsparsity of sketches. In order to improve visual knowledge transfer from seen\nto unseen classes, we elaborate a coarse-to-fine conditional decoder that\ngenerates coarse-grained category-specific corresponding features first (taking\nauxiliary semantic information as conditional input) and then generates\nfine-grained instance-specific corresponding features (taking sketch\nrepresentation as conditional input). Furthermore, regression loss and\nclassification loss are utilized to preserve the semantic and discriminative\ninformation of the synthesized features respectively. Extensive experiments on\nthe large-scale Sketchy dataset and TU-Berlin dataset demonstrate that our\nproposed approach outperforms state-of-the-art methods by more than 20\\% in\nretrieval performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 12:33:47 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 02:31:42 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Wang", "Hao", ""], ["Deng", "Cheng", ""], ["Xu", "Xinxu", ""], ["Liu", "Wei", ""], ["Gao", "Xinbo", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.01975", "submitter": "Zhengping Che", "authors": "Zhengping Che, Guangyu Li, Tracy Li, Bo Jiang, Xuefeng Shi, Xinsheng\n  Zhang, Ying Lu, Guobin Wu, Yan Liu, Jieping Ye", "title": "D$^2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic\n  Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving datasets accelerate the development of intelligent driving and\nrelated computer vision technologies, while substantial and detailed\nannotations serve as fuels and powers to boost the efficacy of such datasets to\nimprove learning-based models. We propose D$^2$-City, a large-scale\ncomprehensive collection of dashcam videos collected by vehicles on DiDi's\nplatform. D$^2$-City contains more than 10000 video clips which deeply reflect\nthe diversity and complexity of real-world traffic scenarios in China. We also\nprovide bounding boxes and tracking annotations of 12 classes of objects in all\nframes of 1000 videos and detection annotations on keyframes for the remainder\nof the videos. Compared with existing datasets, D$^2$-City features data in\nvarying weather, road, and traffic conditions and a huge amount of elaborate\ndetection and tracking annotations. By bringing a diverse set of challenging\ncases to the community, we expect the D$^2$-City dataset will advance the\nperception and related areas of intelligent driving.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 12:40:08 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 06:42:25 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Che", "Zhengping", ""], ["Li", "Guangyu", ""], ["Li", "Tracy", ""], ["Jiang", "Bo", ""], ["Shi", "Xuefeng", ""], ["Zhang", "Xinsheng", ""], ["Lu", "Ying", ""], ["Wu", "Guobin", ""], ["Liu", "Yan", ""], ["Ye", "Jieping", ""]]}, {"id": "1904.01987", "submitter": "Javier Ruiz-Hidalgo", "authors": "Adri\\`a Ciurana, Albert Mosella-Montoro, Javier Ruiz-Hidalgo", "title": "Hybrid Cosine Based Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated their capability to\nsolve different kind of problems in a very huge number of applications.\nHowever, CNNs are limited for their computational and storage requirements.\nThese limitations make difficult to implement these kind of neural networks on\nembedded devices such as mobile phones, smart cameras or advanced driving\nassistance systems. In this paper, we present a novel layer named Hybrid Cosine\nBased Convolution that replaces standard convolutional layers using cosine\nbasis to generate filter weights. The proposed layers provide several\nadvantages: faster convergence in training, the receptive field can be\nincreased at no cost and substantially reduce the number of parameters. We\nevaluate our proposed layers on three competitive classification tasks where\nour proposed layers can achieve similar (and in some cases better) performances\nthan VGG and ResNet architectures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 13:06:43 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ciurana", "Adri\u00e0", ""], ["Mosella-Montoro", "Albert", ""], ["Ruiz-Hidalgo", "Javier", ""]]}, {"id": "1904.01990", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, Yi Yang", "title": "Invariance Matters: Exemplar Memory for Domain Adaptive Person\n  Re-identification", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the domain adaptive person re-identification (re-ID)\nproblem: learning a re-ID model from a labeled source domain and an unlabeled\ntarget domain. Conventional methods are mainly to reduce feature distribution\ngap between the source and target domains. However, these studies largely\nneglect the intra-domain variations in the target domain, which contain\ncritical factors influencing the testing performance on the target domain. In\nthis work, we comprehensively investigate into the intra-domain variations of\nthe target domain and propose to generalize the re-ID model w.r.t three types\nof the underlying invariance, i.e., exemplar-invariance, camera-invariance and\nneighborhood-invariance. To achieve this goal, an exemplar memory is introduced\nto store features of the target domain and accommodate the three invariance\nproperties. The memory allows us to enforce the invariance constraints over\nglobal training batch without significantly increasing computation cost.\nExperiment demonstrates that the three invariance properties and the proposed\nmemory are indispensable towards an effective domain adaptation system. Results\non three re-ID domains show that our domain adaptation accuracy outperforms the\nstate of the art by a large margin. Code is available at:\nhttps://github.com/zhunzhong07/ECN\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 13:11:59 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Zhong", "Zhun", ""], ["Zheng", "Liang", ""], ["Luo", "Zhiming", ""], ["Li", "Shaozi", ""], ["Yang", "Yi", ""]]}, {"id": "1904.01994", "submitter": "Nabeel Abdur Rehman", "authors": "Nabeel Abdur Rehman, Umar Saif, Rumi Chunara", "title": "Deep Landscape Features for Improving Vector-borne Disease Prediction", "comments": "10 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global population at risk of mosquito-borne diseases such as dengue,\nyellow fever, chikungunya and Zika is expanding. Infectious disease models\ncommonly incorporate environmental measures like temperature and precipitation.\nGiven increasing availability of high-resolution satellite imagery, here we\nconsider including landscape features from satellite imagery into infectious\ndisease prediction models. To do so, we implement a Convolutional Neural\nNetwork (CNN) model trained on Imagenet data and labelled landscape features in\nsatellite data from London. We then incorporate landscape features from\nsatellite image data from Pakistan, labelled using the CNN, in a well-known\nSusceptible-Infectious-Recovered epidemic model, alongside dengue case data\nfrom 2012-2016 in Pakistan. We study improvement of the prediction model for\neach of the individual landscape features, and assess the feasibility of using\nimage labels from a different place. We find that incorporating\nsatellite-derived landscape features can improve prediction of outbreaks, which\nis important for proactive and strategic surveillance and control programmes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 13:29:58 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Rehman", "Nabeel Abdur", ""], ["Saif", "Umar", ""], ["Chunara", "Rumi", ""]]}, {"id": "1904.02024", "submitter": "Maarten Vandersteegen", "authors": "Maarten Vandersteegen, Kristof Vanbeeck, Toon goedeme", "title": "Super accurate low latency object detection on a surveillance UAV", "comments": "Conference MVA2019, Source code: https://gitlab.com/EAVISE/jetnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones have proven to be useful in many industry segments such as security\nand surveillance, where e.g. on-board real-time object tracking is a necessity\nfor autonomous flying guards. Tracking and following suspicious objects is\ntherefore required in real-time on limited hardware. With an object detector in\nthe loop, low latency becomes extremely important. In this paper, we propose a\nsolution to make object detection for UAVs both fast and super accurate. We\npropose a multi-dataset learning strategy yielding top eye-sky object detection\naccuracy. Our model generalizes well on unseen data and can cope with different\nflying heights, optically zoomed-in shots and different viewing angles. We\napply optimization steps such that we achieve minimal latency on embedded\non-board hardware by fusing layers, quantizing calculations to 16-bit floats\nand 8-bit integers, with negligible loss in accuracy. We validate on NVIDIA's\nJetson TX2 and Jetson Xavier platforms where we achieve a speed-wise\nperformance boost of more than 10x.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:29:07 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Vandersteegen", "Maarten", ""], ["Vanbeeck", "Kristof", ""], ["goedeme", "Toon", ""]]}, {"id": "1904.02028", "submitter": "Jose M. Facil", "authors": "Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano,\n  Thomas Brox and Javier Civera", "title": "CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth", "comments": "Camera ready version for CVPR 2019. Project page:\n  http://webdiis.unizar.es/~jmfacil/camconvs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view depth estimation suffers from the problem that a network trained\non images from one camera does not generalize to images taken with a different\ncamera model. Thus, changing the camera model requires collecting an entirely\nnew training dataset. In this work, we propose a new type of convolution that\ncan take the camera parameters into account, thus allowing neural networks to\nlearn calibration-aware patterns. Experiments confirm that this improves the\ngeneralization capabilities of depth prediction networks considerably, and\nclearly outperforms the state of the art when the train and test images are\nacquired with different cameras.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:31:35 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Facil", "Jose M.", ""], ["Ummenhofer", "Benjamin", ""], ["Zhou", "Huizhong", ""], ["Montesano", "Luis", ""], ["Brox", "Thomas", ""], ["Civera", "Javier", ""]]}, {"id": "1904.02048", "submitter": "Qinbing Fu", "authors": "Qinbing Fu, Hongxin Wang, Cheng Hu, Shigang Yue", "title": "Towards Computational Models and Applications of Insect Visual Systems\n  for Motion Perception: A Review", "comments": "90 pages, 34 figures, a comprehensive review paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion perception is a critical capability determining a variety of aspects\nof insects' life, including avoiding predators, foraging and so forth. A good\nnumber of motion detectors have been identified in the insects' visual\npathways. Computational modelling of these motion detectors has not only been\nproviding effective solutions to artificial intelligence, but also benefiting\nthe understanding of complicated biological visual systems. These biological\nmechanisms through millions of years of evolutionary development will have\nformed solid modules for constructing dynamic vision systems for future\nintelligent machines. This article reviews the computational motion perception\nmodels originating from biological research of insects' visual systems in the\nliterature. These motion perception models or neural networks comprise the\nlooming sensitive neuronal models of lobula giant movement detectors (LGMDs) in\nlocusts, the translation sensitive neural systems of direction selective\nneurons (DSNs) in fruit flies, bees and locusts, as well as the small target\nmotion detectors (STMDs) in dragonflies and hover flies. We also review the\napplications of these models to robots and vehicles. Through these modelling\nstudies, we summarise the methodologies that generate different direction and\nsize selectivity in motion perception. At last, we discuss about multiple\nsystems integration and hardware realisation of these bio-inspired motion\nperception models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:10:29 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Fu", "Qinbing", ""], ["Wang", "Hongxin", ""], ["Hu", "Cheng", ""], ["Yue", "Shigang", ""]]}, {"id": "1904.02052", "submitter": "Philipp M. Maier", "authors": "Philipp M. Maier, Sina Keller", "title": "Estimating Chlorophyll a Concentrations of Several Inland Waters with\n  Hyperspectral Data and Machine Learning Models", "comments": "Accepted at ISPRS Geospatial Week 2019 in Enschede", "journal-ref": null, "doi": "10.5194/isprs-annals-IV-2-W5-609-2019", "report-no": null, "categories": "cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water is a key component of life, the natural environment and human health.\nFor monitoring the conditions of a water body, the chlorophyll a concentration\ncan serve as a proxy for nutrients and oxygen supply. In situ measurements of\nwater quality parameters are often time-consuming, expensive and limited in\nareal validity. Therefore, we apply remote sensing techniques. During field\ncampaigns, we collected hyperspectral data with a spectrometer and in situ\nmeasured chlorophyll a concentrations of 13 inland water bodies with different\nspectral characteristics. One objective of this study is to estimate\nchlorophyll a concentrations of these inland waters by applying three machine\nlearning regression models: Random Forest, Support Vector Machine and an\nArtificial Neural Network. Additionally, we simulate four different\nhyperspectral resolutions of the spectrometer data to investigate the effects\non the estimation performance. Furthermore, the application of first order\nderivatives of the spectra is evaluated in turn to the regression performance.\nThis study reveals the potential of combining machine learning approaches and\nremote sensing data for inland waters. Each machine learning model achieves an\nR2-score between 80 % to 90 % for the regression on chlorophyll a\nconcentrations. The random forest model benefits clearly from the applied\nderivatives of the spectra. In further studies, we will focus on the\napplication of machine learning models on spectral satellite data to enhance\nthe area-wide estimation of chlorophyll a concentration for inland waters.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:16:36 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Maier", "Philipp M.", ""], ["Keller", "Sina", ""]]}, {"id": "1904.02057", "submitter": "Kaidi Xu", "authors": "Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan,\n  Chuang Gan, Xue Lin", "title": "Interpreting Adversarial Examples by Activation Promotion and\n  Suppression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that convolutional neural networks (CNNs) are vulnerable\nto adversarial examples: images with imperceptible perturbations crafted to\nfool classifiers. However, interpretability of these perturbations is less\nexplored in the literature. This work aims to better understand the roles of\nadversarial perturbations and provide visual explanations from pixel, image and\nnetwork perspectives. We show that adversaries have a promotion-suppression\neffect (PSE) on neurons' activations and can be primarily categorized into\nthree types: i) suppression-dominated perturbations that mainly reduce the\nclassification score of the true label, ii) promotion-dominated perturbations\nthat focus on boosting the confidence of the target label, and iii) balanced\nperturbations that play a dual role in suppression and promotion. We also\nprovide image-level interpretability of adversarial examples. This links PSE of\npixel-level perturbations to class-specific discriminative image regions\nlocalized by class activation mapping (Zhou et al. 2016). Further, we examine\nthe adversarial effect through network dissection (Bau et al. 2017), which\noffers concept-level interpretability of hidden units. We show that there\nexists a tight connection between the units' sensitivity to adversarial attacks\nand their interpretability on semantic concepts. Lastly, we provide some new\ninsights from our interpretation to improve the adversarial robustness of\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:25:21 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 18:32:03 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Xu", "Kaidi", ""], ["Liu", "Sijia", ""], ["Zhang", "Gaoyuan", ""], ["Sun", "Mengshu", ""], ["Zhao", "Pu", ""], ["Fan", "Quanfu", ""], ["Gan", "Chuang", ""], ["Lin", "Xue", ""]]}, {"id": "1904.02074", "submitter": "Qinbing Fu", "authors": "Qinbing Fu, Nicola Bellotto, Huatian Wang, F. Claire Rind, Hongxin\n  Wang, Shigang Yue", "title": "A Visual Neural Network for Robust Collision Perception in Vehicle\n  Driving Scenarios", "comments": "12 pages, 7 figures, conference, springer format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research addresses the challenging problem of visual collision detection\nin very complex and dynamic real physical scenes, specifically, the vehicle\ndriving scenarios. This research takes inspiration from a large-field looming\nsensitive neuron, i.e., the lobula giant movement detector (LGMD) in the\nlocust's visual pathways, which represents high spike frequency to rapid\napproaching objects. Building upon our previous models, in this paper we\npropose a novel inhibition mechanism that is capable of adapting to different\nlevels of background complexity. This adaptive mechanism works effectively to\nmediate the local inhibition strength and tune the temporal latency of local\nexcitation reaching the LGMD neuron. As a result, the proposed model is\neffective to extract colliding cues from complex dynamic visual scenes. We\ntested the proposed method using a range of stimuli including simulated\nmovements in grating backgrounds and shifting of a natural panoramic scene, as\nwell as vehicle crash video sequences. The experimental results demonstrate the\nproposed method is feasible for fast collision perception in real-world\nsituations with potential applications in future autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:05:56 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Fu", "Qinbing", ""], ["Bellotto", "Nicola", ""], ["Wang", "Huatian", ""], ["Rind", "F. Claire", ""], ["Wang", "Hongxin", ""], ["Yue", "Shigang", ""]]}, {"id": "1904.02075", "submitter": "Xun Xu", "authors": "Xun Xu and Loong-Fah Cheong and Zhuwen Li", "title": "Learning for Multi-Type Subspace Clustering", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.10254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering has been extensively studied from the\nhypothesis-and-test, algebraic, and spectral clustering based perspectives.\nMost assume that only a single type/class of subspace is present.\nGeneralizations to multiple types are non-trivial, plagued by challenges such\nas choice of types and numbers of models, sampling imbalance and parameter\ntuning. In this work, we formulate the multi-type subspace clustering problem\nas one of learning non-linear subspace filters via deep multi-layer perceptrons\n(mlps). The response to the learnt subspace filters serve as the feature\nembedding that is clustering-friendly, i.e., points of the same clusters will\nbe embedded closer together through the network. For inference, we apply\nK-means to the network output to cluster the data. Experiments are carried out\non both synthetic and real world multi-type fitting problems, producing\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:08:38 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Xu", "Xun", ""], ["Cheong", "Loong-Fah", ""], ["Li", "Zhuwen", ""]]}, {"id": "1904.02077", "submitter": "Pengcheng Lin", "authors": "Peng-Cheng Lin and Wan-Lei Zhao", "title": "Graph based Nearest Neighbor Search: Promises and Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DS cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph based nearest neighbor search gets more and more popular on\nlarge-scale retrieval tasks. The attractiveness of this type of approaches lies\nin its superior performance over most of the known nearest neighbor search\napproaches as well as its genericness to various metrics. In this paper, the\nrole of two strategies, namely hierarchical structure and graph diversification\nthat are adopted as the key steps in the graph based approaches, is\ninvestigated. We find the hierarchical structure could not achieve \"much better\nlogarithmic complexity scaling\" as it was claimed in the original paper,\nparticularly on high dimensional cases. Moreover, we find that similar high\nsearch speed efficiency as the one with hierarchical structure could be\nachieved with the support of flat k-NN graph after graph diversification.\nFinally, we point out the difficulty, that is faced by most of the graph based\nsearch approaches, is directly linked to \"curse of dimensionality\".\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:12:55 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 09:51:07 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 09:01:31 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 14:23:49 GMT"}, {"version": "v5", "created": "Tue, 18 Jun 2019 09:07:06 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Lin", "Peng-Cheng", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1904.02082", "submitter": "Santiago Estrada", "authors": "Santiago Estrada, Ran Lu, Sailesh Conjeti, Ximena Orozco-Ruiz, Joana\n  Panos-Willuhn, Monique M.B Breteler and Martin Reuter", "title": "FatSegNet : A Fully Automated Deep Learning Pipeline for Adipose Tissue\n  Segmentation on Abdominal Dixon MRI", "comments": "Submitted to Magnetic Resonance in Medicine", "journal-ref": null, "doi": "10.1002/mrm.28022", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Development of a fast and fully automated deep learning pipeline\n(FatSegNet) to accurately identify, segment, and quantify abdominal adipose\ntissue on Dixon MRI from the Rhineland Study - a large prospective\npopulation-based study. Method: FatSegNet is composed of three stages: (i)\nconsistent localization of the abdominal region using two 2D-Competitive Dense\nFully Convolutional Networks (CDFNet), (ii) segmentation of adipose tissue on\nthree views by independent CDFNets, and (iii) view aggregation. FatSegNet is\ntrained with 33 manually annotated subjects, and validated by: 1) comparison of\nsegmentation accuracy against a testingset covering a wide range of body mass\nindex (BMI), 2) test-retest reliability, and 3) robustness in a large cohort\nstudy. Results: The CDFNet demonstrates increased robustness compared to\ntraditional deep learning networks. FatSegNet dice score outperforms manual\nraters on the abdominal visceral adipose tissue (VAT, 0.828 vs. 0.788), and\nproduces comparable results on subcutaneous adipose tissue (SAT, 0.973 vs.\n0.982). The pipeline has very small test-retest absolute percentage difference\nand excellent agreement between scan sessions (VAT: APD = 2.957%, ICC=0.998 and\nSAT: APD= 3.254%, ICC=0.996). Conclusion: FatSegNet can reliably analyze a 3D\nDixon MRI in1 min. It generalizes well to different body shapes, sensitively\nreplicates known VAT and SAT volume effects in a large cohort study, and\npermits localized analysis of fat compartments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:22:37 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 13:17:50 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Estrada", "Santiago", ""], ["Lu", "Ran", ""], ["Conjeti", "Sailesh", ""], ["Orozco-Ruiz", "Ximena", ""], ["Panos-Willuhn", "Joana", ""], ["Breteler", "Monique M. B", ""], ["Reuter", "Martin", ""]]}, {"id": "1904.02104", "submitter": "Wentong Liao", "authors": "Wentong Liao, Cuiling Lan, Wenjun Zeng, Michael Ying Yang, Bodo\n  Rosenhahn", "title": "Target-Tailored Source-Transformation for Scene Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation aims to provide a semantic and structural description\nof an image, denoting the objects (with nodes) and their relationships (with\nedges). The best performing works to date are based on exploiting the context\nsurrounding objects or relations,e.g., by passing information among objects. In\nthese approaches, to transform the representation of source objects is a\ncritical process for extracting information for the use by target objects. In\nthis work, we argue that a source object should give what tar-get object needs\nand give different objects different information rather than contributing\ncommon information to all targets. To achieve this goal, we propose a\nTarget-TailoredSource-Transformation (TTST) method to efficiently propagate\ninformation among object proposals and relations. Particularly, for a source\nobject proposal which will contribute information to other target objects, we\ntransform the source object feature to the target object feature domain by\nsimultaneously taking both the source and target into account. We further\nexplore more powerful representations by integrating language prior with the\nvisual context in the transformation for the scene graph generation. By doing\nso the target object is able to extract target-specific information from the\nsource object and source relation accordingly to refine its representation. Our\nframework is validated on the Visual Genome bench-mark and demonstrated its\nstate-of-the-art performance for the scene graph generation. The experimental\nresults show that the performance of object detection and visual relation-ship\ndetection are promoted mutually by our method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:59:49 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 14:33:40 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Liao", "Wentong", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1904.02113", "submitter": "Loic Landrieu", "authors": "Loic Landrieu, Mohamed Boussaha", "title": "Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new supervized learning framework for oversegmenting 3D point\nclouds into superpoints. We cast this problem as learning deep embeddings of\nthe local geometry and radiometry of 3D points, such that the border of objects\npresents high contrasts. The embeddings are computed using a lightweight neural\nnetwork operating on the points' local neighborhood. Finally, we formulate\npoint cloud oversegmentation as a graph partition problem with respect to the\nlearned embeddings.\n  This new approach allows us to set a new state-of-the-art in point cloud\noversegmentation by a significant margin, on a dense indoor dataset (S3DIS) and\na sparse outdoor one (vKITTI). Our best solution requires over five times fewer\nsuperpoints to reach similar performance than previously published methods on\nS3DIS. Furthermore, we show that our framework can be used to improve\nsuperpoint-based semantic segmentation algorithms, setting a new\nstate-of-the-art for this task as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:18:26 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Landrieu", "Loic", ""], ["Boussaha", "Mohamed", ""]]}, {"id": "1904.02199", "submitter": "Cathrin Elich", "authors": "Cathrin Elich, Francis Engelmann, Theodora Kontogianni, and Bastian\n  Leibe", "title": "3D-BEVIS: Bird's-Eye-View Instance Segmentation", "comments": "camera-ready version for GCPR '19", "journal-ref": null, "doi": "10.1007/978-3-030-33676-9_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning models achieve impressive results on 3D scene analysis\ntasks by operating directly on unstructured point clouds. A lot of progress was\nmade in the field of object classification and semantic segmentation. However,\nthe task of instance segmentation is less explored. In this work, we present\n3D-BEVIS, a deep learning framework for 3D semantic instance segmentation on\npoint clouds. Following the idea of previous proposal-free instance\nsegmentation approaches, our model learns a feature embedding and groups the\nobtained feature space into semantic instances. Current point-based methods\nscale linearly with the number of points by processing local sub-parts of a\nscene individually. However, to perform instance segmentation by clustering,\nglobally consistent features are required. Therefore, we propose to combine\nlocal point geometry with global context information from an intermediate\nbird's-eye view representation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:51:53 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 13:10:36 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 10:02:00 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Elich", "Cathrin", ""], ["Engelmann", "Francis", ""], ["Kontogianni", "Theodora", ""], ["Leibe", "Bastian", ""]]}, {"id": "1904.02201", "submitter": "Biao Jia", "authors": "Biao Jia, Chen Fang, Jonathan Brandt, Byungmoon Kim, Dinesh Manocha", "title": "PaintBot: A Reinforcement Learning Approach for Natural Media Painting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new automated digital painting framework, based on a painting\nagent trained through reinforcement learning. To synthesize an image, the agent\nselects a sequence of continuous-valued actions representing primitive painting\nstrokes, which are accumulated on a digital canvas. Action selection is guided\nby a given reference image, which the agent attempts to replicate subject to\nthe limitations of the action space and the agent's learned policy. The\npainting agent policy is determined using a variant of proximal policy\noptimization reinforcement learning. During training, our agent is presented\nwith patches sampled from an ensemble of reference images. To accelerate\ntraining convergence, we adopt a curriculum learning strategy, whereby\nreference patches are sampled according to how challenging they are using the\ncurrent policy. We experiment with differing loss functions, including\npixel-wise and perceptual loss, which have consequent differing effects on the\nlearned policy. We demonstrate that our painting agent can learn an effective\npolicy with a high dimensional continuous action space comprising pen pressure,\nwidth, tilt, and color, for a variety of painting styles. Through a\ncoarse-to-fine refinement process our agent can paint arbitrarily complex\nimages in the desired style.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:56:02 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Jia", "Biao", ""], ["Fang", "Chen", ""], ["Brandt", "Jonathan", ""], ["Kim", "Byungmoon", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1904.02203", "submitter": "Pravakar Roy", "authors": "Pravakar Roy, Nicolai H\\\"ani, Jun-Jee Chao, Volkan Isler", "title": "Semantics-Aware Image to Image Translation and Domain Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image to image translation is the problem of transferring an image from a\nsource domain to a different (but related) target domain. We present a new\nunsupervised image to image translation technique that leverages the underlying\nsemantic information for object transfiguration and domain transfer tasks.\nSpecifically, we present a generative adversarial learning approach that\njointly translates images and labels from a source domain to a target domain.\nOur main technical contribution is an encoder-decoder based network\narchitecture that jointly encodes the image and its underlying semantics and\ntranslates both individually to the target domain. Additionally, we propose\nobject transfiguration and cross-domain semantic consistency losses that\npreserve semantic labels. Through extensive experimental evaluation, we\ndemonstrate the effectiveness of our approach as compared to the\nstate-of-the-art methods on unsupervised image-to-image translation, domain\nadaptation, and object transfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:06:39 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 18:35:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Roy", "Pravakar", ""], ["H\u00e4ni", "Nicolai", ""], ["Chao", "Jun-Jee", ""], ["Isler", "Volkan", ""]]}, {"id": "1904.02204", "submitter": "Nadav Dym", "authors": "Nadav Dym and Shahar Ziv Kovalsky", "title": "Linearly Converging Quasi Branch and Bound Algorithms for Global Rigid\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several branch-and-bound (BnB) algorithms have been proposed\nto globally optimize rigid registration problems. In this paper, we suggest a\ngeneral framework to improve upon the BnB approach, which we name Quasi BnB.\nQuasi BnB replaces the linear lower bounds used in BnB algorithms with\nquadratic quasi-lower bounds which are based on the quadratic behavior of the\nenergy in the vicinity of the global minimum. While quasi-lower bounds are not\ntruly lower bounds, the Quasi-BnB algorithm is globally optimal. In fact we\nprove that it exhibits linear convergence -- it achieves $\\epsilon$-accuracy in\n$~O(\\log(1/\\epsilon)) $ time while the time complexity of other rigid\nregistration BnB algorithms is polynomial in $1/\\epsilon $. Our experiments\nverify that Quasi-BnB is significantly more efficient than state-of-the-art BnB\nalgorithms, especially for problems where high accuracy is desired.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:07:25 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 04:30:34 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Dym", "Nadav", ""], ["Kovalsky", "Shahar Ziv", ""]]}, {"id": "1904.02216", "submitter": "Hanchao Li", "authors": "Hanchao Li, Pengfei Xiong, Haoqiang Fan, Jian Sun", "title": "DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extremely efficient CNN architecture named DFANet\nfor semantic segmentation under resource constraints. Our proposed network\nstarts from a single lightweight backbone and aggregates discriminative\nfeatures through sub-network and sub-stage cascade respectively. Based on the\nmulti-scale feature propagation, DFANet substantially reduces the number of\nparameters, but still obtains sufficient receptive field and enhances the model\nlearning ability, which strikes a balance between the speed and segmentation\nperformance. Experiments on Cityscapes and CamVid datasets demonstrate the\nsuperior performance of DFANet with 8$\\times$ less FLOPs and 2$\\times$ faster\nthan the existing state-of-the-art real-time semantic segmentation methods\nwhile providing comparable accuracy. Specifically, it achieves 70.3\\% Mean IOU\non the Cityscapes test dataset with only 1.7 GFLOPs and a speed of 160 FPS on\none NVIDIA Titan X card, and 71.3\\% Mean IOU with 3.4 GFLOPs while inferring on\na higher resolution image.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:45:17 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Li", "Hanchao", ""], ["Xiong", "Pengfei", ""], ["Fan", "Haoqiang", ""], ["Sun", "Jian", ""]]}, {"id": "1904.02217", "submitter": "Peter Weiderer", "authors": "Peter Weiderer and Ana Maria Tom\\'e and Elmar Wolfgang Lang", "title": "Decomposing Temperature Time Series with Non-Negative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.app-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the fabrication of casting parts sensor data is typically\nautomatically recorded and accumulated for process monitoring and defect\ndiagnosis. As casting is a thermal process with many interacting process\nparameters, root cause analysis tends to be tedious and ineffective. We show\nhow a decomposition based on non-negative matrix factorization (NMF), which is\nguided by a knowledge-based initialization strategy, is able to extract\nphysical meaningful sources from temperature time series collected during a\nthermal manufacturing process. The approach assumes the time series to be\ngenerated by a superposition of several simultaneously acting component\nprocesses. NMF is able to reverse the superposition and to identify the hidden\ncomponent processes. The latter can be linked to ongoing physical phenomena and\nprocess variables, which cannot be monitored directly. Our approach provides\nnew insights into the underlying physics and offers a tool, which can assist in\ndiagnosing defect causes. We demonstrate our method by applying it to real\nworld data, collected in a foundry during the series production of casting\nparts for the automobile industry.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:46:56 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Weiderer", "Peter", ""], ["Tom\u00e9", "Ana Maria", ""], ["Lang", "Elmar Wolfgang", ""]]}, {"id": "1904.02225", "submitter": "Melanie Mitchell", "authors": "Erik Conser, Kennedy Hahn, Chandler M. Watson, Melanie Mitchell", "title": "Revisiting Visual Grounding", "comments": "To appear in Proceedings of the Workshop on Shortcomings in Vision\n  and Language, NAACL-2019, ACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a particular visual grounding method: the \"Image Retrieval Using\nScene Graphs\" (IRSG) system of Johnson et al. (2015). Our experiments indicate\nthat the system does not effectively use its learned object-relationship\nmodels. We also look closely at the IRSG dataset, as well as the widely used\nVisual Relationship Dataset (VRD) that is adapted from it. We find that these\ndatasets exhibit biases that allow methods that ignore relationships to perform\nrelatively well. We also describe several other problems with the IRSG dataset,\nand report on experiments using a subset of the dataset in which the biases and\nother problems are removed. Our studies contribute to a more general effort:\nthat of better understanding what machine learning methods that combine\nlanguage and vision actually learn and what popular datasets actually test.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 20:11:02 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Conser", "Erik", ""], ["Hahn", "Kennedy", ""], ["Watson", "Chandler M.", ""], ["Mitchell", "Melanie", ""]]}, {"id": "1904.02239", "submitter": "Valentin Khrulkov", "authors": "Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan\n  Oseledets, Victor Lempitsky", "title": "Hyperbolic Image Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision tasks such as image classification, image retrieval and\nfew-shot learning are currently dominated by Euclidean and spherical\nembeddings, so that the final decisions about class belongings or the degree of\nsimilarity are made using linear hyperplanes, Euclidean distances, or spherical\ngeodesic distances (cosine similarity). In this work, we demonstrate that in\nmany practical scenarios hyperbolic embeddings provide a better alternative.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:10:12 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 20:35:39 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Khrulkov", "Valentin", ""], ["Mirvakhabova", "Leyla", ""], ["Ustinova", "Evgeniya", ""], ["Oseledets", "Ivan", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1904.02242", "submitter": "Adam Nyberg", "authors": "Adam Nyberg, Abdelrahman Eldesokey, David Bergstr\\\"om, David\n  Gustafsson", "title": "Unpaired Thermal to Visible Spectrum Transfer using Adversarial Training", "comments": "Published in Computer Vision ECCV 2018 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal Infrared (TIR) cameras are gaining popularity in many computer vision\napplications due to their ability to operate under low-light conditions. Images\nproduced by TIR cameras are usually difficult for humans to perceive visually,\nwhich limits their usability. Several methods in the literature were proposed\nto address this problem by transforming TIR images into realistic visible\nspectrum (VIS) images. However, existing TIR-VIS datasets suffer from imperfect\nalignment between TIR-VIS image pairs which degrades the performance of\nsupervised methods. We tackle this problem by learning this transformation\nusing an unsupervised Generative Adversarial Network (GAN) which trains on\nunpaired TIR and VIS images. When trained and evaluated on KAIST-MS dataset,\nour proposed methods was shown to produce significantly more realistic and\nsharp VIS images than the existing state-of-the-art supervised methods. In\naddition, our proposed method was shown to generalize very well when evaluated\non a new dataset of new environments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:20:10 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Nyberg", "Adam", ""], ["Eldesokey", "Abdelrahman", ""], ["Bergstr\u00f6m", "David", ""], ["Gustafsson", "David", ""]]}, {"id": "1904.02251", "submitter": "Rohan Chabra", "authors": "Rohan Chabra, Julian Straub, Chris Sweeney, Richard Newcombe, Henry\n  Fuchs", "title": "StereoDRNet: Dilated Residual Stereo Net", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system that uses a convolution neural network (CNN) to estimate\ndepth from a stereo pair followed by volumetric fusion of the predicted depth\nmaps to produce a 3D reconstruction of a scene. Our proposed depth refinement\narchitecture, predicts view-consistent disparity and occlusion maps that helps\nthe fusion system to produce geometrically consistent reconstructions. We\nutilize 3D dilated convolutions in our proposed cost filtering network that\nyields better filtering while almost halving the computational cost in\ncomparison to state of the art cost filtering architectures.For feature\nextraction we use the Vortex Pooling architecture. The proposed method achieves\nstate of the art results in KITTI 2012, KITTI 2015 and ETH 3D stereo\nbenchmarks. Finally, we demonstrate that our system is able to produce high\nfidelity 3D scene reconstructions that outperforms the state of the art stereo\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:58:39 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 01:30:15 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 06:48:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chabra", "Rohan", ""], ["Straub", "Julian", ""], ["Sweeney", "Chris", ""], ["Newcombe", "Richard", ""], ["Fuchs", "Henry", ""]]}, {"id": "1904.02266", "submitter": "Maani Ghaffari Jadidi", "authors": "Maani Ghaffari, William Clark, Anthony Bloch, Ryan M. Eustice, Jessy\n  W. Grizzle", "title": "Continuous Direct Sparse Visual Odometry from RGB-D Images", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a novel formulation and evaluation of visual odometry\nfrom RGB-D images. Assuming a static scene, the developed theoretical framework\ngeneralizes the widely used direct energy formulation (photometric error\nminimization) technique for obtaining a rigid body transformation that aligns\ntwo overlapping RGB-D images to a continuous formulation. The continuity is\nachieved through functional treatment of the problem and representing the\nprocess models over RGB-D images in a reproducing kernel Hilbert space;\nconsequently, the registration is not limited to the specific image resolution\nand the framework is fully analytical with a closed-form derivation of the\ngradient. We solve the problem by maximizing the inner product between two\nfunctions defined over RGB-D images, while the continuous action of the rigid\nbody motion Lie group is captured through the integration of the flow in the\ncorresponding Lie algebra. Energy-based approaches have been extremely\nsuccessful and the developed framework in this paper shares many of their\ndesired properties such as the parallel structure on both CPUs and GPUs,\nsparsity, semi-dense tracking, avoiding explicit data association which is\ncomputationally expensive, and possible extensions to the simultaneous\nlocalization and mapping frameworks. The evaluations on experimental data and\ncomparison with the equivalent energy-based formulation of the problem confirm\nthe effectiveness of the proposed technique, especially, when the lack of\nstructure and texture in the environment is evident.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 23:25:01 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 03:08:07 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 23:21:18 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ghaffari", "Maani", ""], ["Clark", "William", ""], ["Bloch", "Anthony", ""], ["Eustice", "Ryan M.", ""], ["Grizzle", "Jessy W.", ""]]}, {"id": "1904.02296", "submitter": "Dacheng Tao", "authors": "Xinyuan Chen, Chang Xu, Xiaokang Yang, Li Song, and Dacheng Tao", "title": "Gated-GAN: Adversarial Gated Networks for Multi-Collection Style\n  Transfer", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2869695", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer describes the rendering of an image semantic content as\ndifferent artistic styles. Recently, generative adversarial networks (GANs)\nhave emerged as an effective approach in style transfer by adversarially\ntraining the generator to synthesize convincing counterfeits. However,\ntraditional GAN suffers from the mode collapse issue, resulting in unstable\ntraining and making style transfer quality difficult to guarantee. In addition,\nthe GAN generator is only compatible with one style, so a series of GANs must\nbe trained to provide users with choices to transfer more than one kind of\nstyle. In this paper, we focus on tackling these challenges and limitations to\nimprove style transfer. We propose adversarial gated networks (Gated GAN) to\ntransfer multiple styles in a single model. The generative networks have three\nmodules: an encoder, a gated transformer, and a decoder. Different styles can\nbe achieved by passing input images through different branches of the gated\ntransformer. To stabilize training, the encoder and decoder are combined as an\nautoencoder to reconstruct the input images. The discriminative networks are\nused to distinguish whether the input image is a stylized or genuine image. An\nauxiliary classifier is used to recognize the style categories of transferred\nimages, thereby helping the generative networks generate images in multiple\nstyles. In addition, Gated GAN makes it possible to explore a new style by\ninvestigating styles learned from artists or genres. Our extensive experiments\ndemonstrate the stability and effectiveness of the proposed model for\nmultistyle transfer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:20:52 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chen", "Xinyuan", ""], ["Xu", "Chang", ""], ["Yang", "Xiaokang", ""], ["Song", "Li", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02301", "submitter": "Dacheng Tao", "authors": "Meng Liu, Chang Xu, Yong Luo, Chao Xu, Yonggang Wen and Dacheng Tao", "title": "Cost-Sensitive Feature Selection by Optimizing F-Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is beneficial for improving the performance of general\nmachine learning tasks by extracting an informative subset from the\nhigh-dimensional features. Conventional feature selection methods usually\nignore the class imbalance problem, thus the selected features will be biased\ntowards the majority class. Considering that F-measure is a more reasonable\nperformance measure than accuracy for imbalanced data, this paper presents an\neffective feature selection algorithm that explores the class imbalance issue\nby optimizing F-measures. Since F-measure optimization can be decomposed into a\nseries of cost-sensitive classification problems, we investigate the\ncost-sensitive feature selection by generating and assigning different costs to\neach class with rigorous theory guidance. After solving a series of\ncost-sensitive feature selection problems, features corresponding to the best\nF-measure will be selected. In this way, the selected features will fully\nrepresent the properties of all classes. Experimental results on popular\nbenchmarks and challenging real-world data sets demonstrate the significance of\ncost-sensitive feature selection for the imbalanced data setting and validate\nthe effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:36:04 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Liu", "Meng", ""], ["Xu", "Chang", ""], ["Luo", "Yong", ""], ["Xu", "Chao", ""], ["Wen", "Yonggang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02302", "submitter": "Tengfei Zhang", "authors": "Tengfei Zhang, Yue Zhang, Xian Sun, Menglong Yan, Yaoling Wang, Kun Fu", "title": "A Training-free, One-shot Detection Framework For Geospatial Objects In\n  Remote Sensing Images", "comments": "5 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based object detection has achieved great success. However,\nthese supervised learning methods are data-hungry and time-consuming. This\nrestriction makes them unsuitable for limited data and urgent tasks, especially\nin the applications of remote sensing. Inspired by the ability of humans to\nquickly learn new visual concepts from very few examples, we propose a\ntraining-free, one-shot geospatial object detection framework for remote\nsensing images. It consists of (1) a feature extractor with remote sensing\ndomain knowledge, (2) a multi-level feature fusion method, (3) a novel\nsimilarity metric method, and (4) a 2-stage object detection pipeline.\nExperiments on sewage treatment plant and airport detections show that proposed\nmethod has achieved a certain effect. Our method can serve as a baseline for\ntraining-free, one-shot geospatial object detection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:36:51 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Zhang", "Tengfei", ""], ["Zhang", "Yue", ""], ["Sun", "Xian", ""], ["Yan", "Menglong", ""], ["Wang", "Yaoling", ""], ["Fu", "Kun", ""]]}, {"id": "1904.02307", "submitter": "Saeid Asgari Taghanaki", "authors": "Saied Asgari Taghanaki, Kumar Abhishek, Ghassan Hamarneh", "title": "Improved Inference via Deep Input Transfer", "comments": "Accepted to MICCAI 2019", "journal-ref": "MICCAI 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although numerous improvements have been made in the field of image\nsegmentation using convolutional neural networks, the majority of these\nimprovements rely on training with larger datasets, model architecture\nmodifications, novel loss functions, and better optimizers. In this paper, we\npropose a new segmentation performance boosting paradigm that relies on\noptimally modifying the network's input instead of the network itself. In\nparticular, we leverage the gradients of a trained segmentation network with\nrespect to the input to transfer it to a space where the segmentation accuracy\nimproves. We test the proposed method on three publicly available medical image\nsegmentation datasets: the ISIC 2017 Skin Lesion Segmentation dataset, the\nShenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, for which our method\nachieves improvements of 5.8%, 0.5%, and 4.8% in the average Dice scores,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:04:13 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 01:15:05 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 06:49:50 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 05:40:15 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Taghanaki", "Saied Asgari", ""], ["Abhishek", "Kumar", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1904.02317", "submitter": "Tengfei Zhang", "authors": "Tengfei Zhang, Yue Zhang, Xian Sun, Hao Sun, Menglong Yan, Xue Yang,\n  Kun Fu", "title": "Comparison Network for One-Shot Conditional Object Detection", "comments": "The paper is under revision now. Some problem are not well described.\n  However, this paper has spread out. I think the impact of an imperfect first\n  draft is not good, so we want to withdraw and revise", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current advances in object detection depend on large-scale datasets to\nget good performance. However, there may not always be sufficient samples in\nmany scenarios, which leads to the research on few-shot detection as well as\nits extreme variation one-shot detection. In this paper, the one-shot detection\nhas been formulated as a conditional probability problem. With this insight, a\nnovel one-shot conditional object detection (OSCD) framework, referred as\nComparison Network (ComparisonNet), has been proposed. Specifically, query and\ntarget image features are extracted through a Siamese network as mapped metrics\nof marginal probabilities. A two-stage detector for OSCD is introduced to\ncompare the extracted query and target features with the learnable metric to\napproach the optimized non-linear conditional probability. Once trained,\nComparisonNet can detect objects of both seen and unseen classes without\nfurther training, which also has the advantages including class-agnostic,\ntraining-free for unseen classes, and without catastrophic forgetting.\nExperiments show that the proposed approach achieves state-of-the-art\nperformance on the proposed datasets of Fashion-MNIST and PASCAL VOC.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:31:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 07:10:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhang", "Tengfei", ""], ["Zhang", "Yue", ""], ["Sun", "Xian", ""], ["Sun", "Hao", ""], ["Yan", "Menglong", ""], ["Yang", "Xue", ""], ["Fu", "Kun", ""]]}, {"id": "1904.02319", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti, Cherie Ho, Wenshan Wang, Sanjiban Choudhury,\n  Sebastian Scherer", "title": "Towards a Robust Aerial Cinematography Platform: Localizing and Tracking\n  Moving Targets in Unstructured Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of drones for aerial cinematography has revolutionized several\napplications and industries that require live and dynamic camera viewpoints\nsuch as entertainment, sports, and security. However, safely controlling a\ndrone while filming a moving target usually requires multiple expert human\noperators; hence the need for an autonomous cinematographer. Current approaches\nhave severe real-life limitations such as requiring fully scripted scenes,\nhigh-precision motion-capture systems or GPS tags to localize targets, and\nprior maps of the environment to avoid obstacles and plan for occlusion.\n  In this work, we overcome such limitations and propose a complete system for\naerial cinematography that combines: (1) a vision-based algorithm for target\nlocalization; (2) a real-time incremental 3D signed-distance map algorithm for\nocclusion and safety computation; and (3) a real-time camera motion planner\nthat optimizes smoothness, collisions, occlusions and artistic guidelines. We\nevaluate robustness and real-time performance in series of field experiments\nand simulations by tracking dynamic targets moving through unknown,\nunstructured environments. Finally, we verify that despite removing previous\nlimitations, our system achieves state-of-the-art performance. Videos of the\nsystem in action can be seen at https://youtu.be/ZE9MnCVmumc\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:37:05 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 03:29:55 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Ho", "Cherie", ""], ["Wang", "Wenshan", ""], ["Choudhury", "Sanjiban", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1904.02322", "submitter": "Youshan Zhang", "authors": "Youshan Zhang, Brian D. Davison", "title": "Modified Distribution Alignment for Domain Adaptation with Pre-trained\n  Inception ResNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely used in computer vision. There are\nseveral well trained deep neural networks for the ImageNet classification\nchallenge, which has played a significant role in image recognition. However,\nlittle work has explored pre-trained neural networks for image recognition in\ndomain adaption. In this paper, we are the first to extract better-represented\nfeatures from a pre-trained Inception ResNet model for domain adaptation. We\nthen present a modified distribution alignment method for classification using\nthe extracted features. We test our model using three benchmark datasets\n(Office+Caltech-10, Office-31, and Office-Home). Extensive experiments\ndemonstrate significant improvements (4.8%, 5.5%, and 10%) in classification\naccuracy over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:00:24 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 15:04:36 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "1904.02323", "submitter": "Fred Hohman", "authors": "Fred Hohman, Haekyu Park, Caleb Robinson, Duen Horng Chau", "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation\n  and Attribution Summarizations", "comments": "Published in IEEE Transactions on Visualization and Computer Graphics\n  2020, and presented at IEEE VAST 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is increasingly used in decision-making tasks. However,\nunderstanding how neural networks produce final predictions remains a\nfundamental challenge. Existing work on interpreting neural network predictions\nfor images often focuses on explaining predictions for single images or\nneurons. As predictions are often computed from millions of weights that are\noptimized over millions of images, such explanations can easily miss a bigger\npicture. We present Summit, an interactive system that scalably and\nsystematically summarizes and visualizes what features a deep learning model\nhas learned and how those features interact to make predictions. Summit\nintroduces two new scalable summarization techniques: (1) activation\naggregation discovers important neurons, and (2) neuron-influence aggregation\nidentifies relationships among such neurons. Summit combines these techniques\nto create the novel attribution graph that reveals and summarizes crucial\nneuron associations and substructures that contribute to a model's outcomes.\nSummit scales to large data, such as the ImageNet dataset with 1.2M images, and\nleverages neural network feature visualization and dataset examples to help\nusers distill large, complex neural network models into compact, interactive\nvisualizations. We present neural network exploration scenarios where Summit\nhelps us discover multiple surprising insights into a prevalent, large-scale\nimage classifier's learned representations and informs future neural network\narchitecture design. The Summit visualization runs in modern web browsers and\nis open-sourced.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:00:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 14:42:39 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 19:42:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hohman", "Fred", ""], ["Park", "Haekyu", ""], ["Robinson", "Caleb", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1904.02325", "submitter": "Yifan Yang", "authors": "Yifan Yang, Libing Geng, Hanjiang Lai, Yan Pan, Jian Yin", "title": "Feature Pyramid Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep-networks-based hashing has become a leading approach\nfor large-scale image retrieval. Most deep hashing approaches use the high\nlayer to extract the powerful semantic representations. However, these methods\nhave limited ability for fine-grained image retrieval because the semantic\nfeatures extracted from the high layer are difficult in capturing the subtle\ndifferences. To this end, we propose a novel two-pyramid hashing architecture\nto learn both the semantic information and the subtle appearance details for\nfine-grained image search. Inspired by the feature pyramids of convolutional\nneural network, a vertical pyramid is proposed to capture the high-layer\nfeatures and a horizontal pyramid combines multiple low-layer features with\nstructural information to capture the subtle differences. To fuse the low-level\nfeatures, a novel combination strategy, called consensus fusion, is proposed to\ncapture all subtle information from several low-layers for finer retrieval.\nExtensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford\nDogs demonstrate that the proposed method achieves significant performance\ncompared with the state-of-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:05:39 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Yang", "Yifan", ""], ["Geng", "Libing", ""], ["Lai", "Hanjiang", ""], ["Pan", "Yan", ""], ["Yin", "Jian", ""]]}, {"id": "1904.02340", "submitter": "Dacheng Tao", "authors": "Chang Xu, Dacheng Tao, Chao Xu", "title": "Multi-View Intact Space Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is practical to assume that an individual view is unlikely to be\nsufficient for effective multi-view learning. Therefore, integration of\nmulti-view information is both valuable and necessary. In this paper, we\npropose the Multi-view Intact Space Learning (MISL) algorithm, which integrates\nthe encoded complementary information in multiple views to discover a latent\nintact representation of the data. Even though each view on its own is\ninsufficient, we show theoretically that by combing multiple views we can\nobtain abundant information for latent intact space learning. Employing the\nCauchy loss (a technique used in statistical learning) as the error measurement\nstrengthens robustness to outliers. We propose a new definition of multi-view\nstability and then derive the generalization error bound based on multi-view\nstability and Rademacher complexity, and show that the complementarity between\nmultiple views is beneficial for the stability and generalization. MISL is\nefficiently optimized using a novel Iteratively Reweight Residuals (IRR)\ntechnique, whose convergence is theoretically analyzed. Experiments on\nsynthetic data and real-world datasets demonstrate that MISL is an effective\nand promising algorithm for practical applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:04:57 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Xu", "Chang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1904.02354", "submitter": "Zehang Lin", "authors": "Zhenguo Yang, Zehang Lin, Min Cheng, Qing Li and Wenyin Liu", "title": "MMED: A Multi-domain and Multi-modality Event Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we construct and release a multi-domain and multi-modality\nevent dataset (MMED), containing 25,165 textual news articles collected from\nhundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and\n76,516 image posts shared on Flickr social media, which are annotated according\nto 412 real-world events. The dataset is collected to explore the problem of\norganizing heterogeneous data contributed by professionals and amateurs in\ndifferent data domains, and the problem of transferring event knowledge\nobtained from one data domain to heterogeneous data domain, thus summarizing\nthe data with different contributors. We hope that the release of the MMED\ndataset can stimulate innovate research on related challenging problems, such\nas event discovery, cross-modal (event) retrieval, and visual question\nanswering, etc.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:27:10 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 12:05:49 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Yang", "Zhenguo", ""], ["Lin", "Zehang", ""], ["Cheng", "Min", ""], ["Li", "Qing", ""], ["Liu", "Wenyin", ""]]}, {"id": "1904.02358", "submitter": "Zheng Li", "authors": "Chaofeng Wang, Zheng Li and Jun Shi", "title": "Lightweight Image Super-Resolution with Adaptive Weighted Learning\n  Network", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied to the single-image\nsuper-resolution (SISR) task with great performance in recent years. However,\nmost convolutional neural network based SR models require heavy computation,\nwhich limit their real-world applications. In this work, a lightweight SR\nnetwork, named Adaptive Weighted Super-Resolution Network (AWSRN), is proposed\nfor SISR to address this issue. A novel local fusion block (LFB) is designed in\nAWSRN for efficient residual learning, which consists of stacked adaptive\nweighted residual units (AWRU) and a local residual fusion unit (LRFU).\nMoreover, an adaptive weighted multi-scale (AWMS) module is proposed to make\nfull use of features in reconstruction layer. AWMS consists of several\ndifferent scale convolutions, and the redundancy scale branch can be removed\naccording to the contribution of adaptive weights in AWMS for lightweight\nnetwork. The experimental results on the commonly used datasets show that the\nproposed lightweight AWSRN achieves superior performance on x2, x3, x4, and x8\nscale factors to state-of-the-art methods with similar parameters and\ncomputational overhead. Code is avaliable at:\nhttps://github.com/ChaofWang/AWSRN\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:44:32 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Wang", "Chaofeng", ""], ["Li", "Zheng", ""], ["Shi", "Jun", ""]]}, {"id": "1904.02361", "submitter": "Mehran Khodabandeh", "authors": "Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, William G. Macready", "title": "A Robust Learning Approach to Domain Adaptive Object Detection", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain shift is unavoidable in real-world applications of object detection.\nFor example, in self-driving cars, the target domain consists of unconstrained\nroad environments which cannot all possibly be observed in training data.\nSimilarly, in surveillance applications sufficiently representative training\ndata may be lacking due to privacy regulations. In this paper, we address the\ndomain adaptation problem from the perspective of robust learning and show that\nthe problem may be formulated as training with noisy labels. We propose a\nrobust object detection framework that is resilient to noise in bounding box\nclass labels, locations and size annotations. To adapt to the domain shift, the\nmodel is trained on the target domain using a set of noisy object bounding\nboxes that are obtained by a detection model trained only in the source domain.\nWe evaluate the accuracy of our approach in various source/target domain pairs\nand demonstrate that the model significantly improves the state-of-the-art on\nmultiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:50:10 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 05:24:59 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 05:43:00 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Khodabandeh", "Mehran", ""], ["Vahdat", "Arash", ""], ["Ranjbar", "Mani", ""], ["Macready", "William G.", ""]]}, {"id": "1904.02363", "submitter": "Longyin Wen", "authors": "Kai Xu, Longyin Wen, Guorong Li, Liefeng Bo, Qingming Huang", "title": "Spatiotemporal CNN for Video Object Segmentation", "comments": "10 pages, 3 figures, 6 tables, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a unified, end-to-end trainable spatiotemporal CNN\nmodel for VOS, which consists of two branches, i.e., the temporal coherence\nbranch and the spatial segmentation branch. Specifically, the temporal\ncoherence branch pretrained in an adversarial fashion from unlabeled video\ndata, is designed to capture the dynamic appearance and motion cues of video\nsequences to guide object segmentation. The spatial segmentation branch focuses\non segmenting objects accurately based on the learned appearance and motion\ncues. To obtain accurate segmentation results, we design a coarse-to-fine\nprocess to sequentially apply a designed attention module on multi-scale\nfeature maps, and concatenate them to produce the final prediction. In this\nway, the spatial segmentation branch is enforced to gradually concentrate on\nobject regions. These two branches are jointly fine-tuned on video segmentation\nsequences in an end-to-end manner. Several experiments are carried out on three\nchallenging datasets (i.e., DAVIS-2016, DAVIS-2017 and Youtube-Object) to show\nthat our method achieves favorable performance against the state-of-the-arts.\nCode is available at https://github.com/longyin880815/STCNN.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:53:15 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Xu", "Kai", ""], ["Wen", "Longyin", ""], ["Li", "Guorong", ""], ["Bo", "Liefeng", ""], ["Huang", "Qingming", ""]]}, {"id": "1904.02365", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Chunhua Shen, Ian Reid", "title": "Template-Based Automatic Search of Compact Semantic Segmentation\n  Architectures", "comments": "Updated runtime numbers on CityScapes. WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic search of neural architectures for various vision and natural\nlanguage tasks is becoming a prominent tool as it allows to discover\nhigh-performing structures on any dataset of interest. Nevertheless, on more\ndifficult domains, such as dense per-pixel classification, current automatic\napproaches are limited in their scope - due to their strong reliance on\nexisting image classifiers they tend to search only for a handful of additional\nlayers with discovered architectures still containing a large number of\nparameters. In contrast, in this work we propose a novel solution able to find\nlight-weight and accurate segmentation architectures starting from only few\nblocks of a pre-trained classification network. To this end, we progressively\nbuild up a methodology that relies on templates of sets of operations, predicts\nwhich template and how many times should be applied at each step, while also\ngenerating the connectivity structure and downsampling factors. All these\ndecisions are being made by a recurrent neural network that is rewarded based\non the score of the emitted architecture on the holdout set and trained using\nreinforcement learning. One discovered architecture achieves 63.2% mean IoU on\nCamVid and 67.8% on CityScapes having only 270K parameters. Pre-trained models\nand the search code are available at\nhttps://github.com/DrSleep/nas-segm-pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 06:06:32 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 03:25:48 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1904.02371", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Hao Chen, Chunhua Shen, Ian Reid", "title": "Architecture Search of Dynamic Cells for Semantic Video Segmentation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic video segmentation the goal is to acquire consistent dense\nsemantic labelling across image frames. To this end, recent approaches have\nbeen reliant on manually arranged operations applied on top of static semantic\nsegmentation networks - with the most prominent building block being the\noptical flow able to provide information about scene dynamics. Related to that\nis the line of research concerned with speeding up static networks by\napproximating expensive parts of them with cheaper alternatives, while\npropagating information from previous frames. In this work we attempt to come\nup with generalisation of those methods, and instead of manually designing\ncontextual blocks that connect per-frame outputs, we propose a neural\narchitecture search solution, where the choice of operations together with\ntheir sequential arrangement are being predicted by a separate neural network.\nWe showcase that such generalisation leads to stable and accurate results\nacross common benchmarks, such as CityScapes and CamVid datasets. Importantly,\nthe proposed methodology takes only 2 GPU-days, finds high-performing cells and\ndoes not rely on the expensive optical flow computation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 06:32:30 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Chen", "Hao", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1904.02375", "submitter": "Alexandre Boulch", "authors": "Alexandre Boulch", "title": "ConvPoint: Continuous Convolutions for Point Cloud Processing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are unstructured and unordered data, as opposed to images. Thus,\nmost machine learning approach developed for image cannot be directly\ntransferred to point clouds. In this paper, we propose a generalization of\ndiscrete convolutional neural networks (CNNs) in order to deal with point\nclouds by replacing discrete kernels by continuous ones. This formulation is\nsimple, allows arbitrary point cloud sizes and can easily be used for designing\nneural networks similarly to 2D CNNs. We present experimental results with\nvarious architectures, highlighting the flexibility of the proposed approach.\nWe obtain competitive results compared to the state-of-the-art on shape\nclassification, part segmentation and semantic segmentation for large-scale\npoint clouds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 06:51:56 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 09:26:51 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 12:19:38 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 07:11:42 GMT"}, {"version": "v5", "created": "Wed, 19 Feb 2020 10:49:28 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Boulch", "Alexandre", ""]]}, {"id": "1904.02382", "submitter": "Siyang Song", "authors": "Siyang Song, Enrique S\\'anchez-Lozano, Linlin Shen, Alan Johnston and\n  Michel Valstar", "title": "Inferring Dynamic Representations of Facial Actions from a Still Image", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial actions are spatio-temporal signals by nature, and therefore their\nmodeling is crucially dependent on the availability of temporal information. In\nthis paper, we focus on inferring such temporal dynamics of facial actions when\nno explicit temporal information is available, i.e. from still images. We\npresent a novel approach to capture multiple scales of such temporal dynamics,\nwith an application to facial Action Unit (AU) intensity estimation and\ndimensional affect estimation. In particular, 1) we propose a framework that\ninfers a dynamic representation (DR) from a still image, which captures the\nbi-directional flow of time within a short time-window centered at the input\nimage; 2) we show that we can train our method without the need of explicitly\ngenerating target representations, allowing the network to represent dynamics\nmore broadly; and 3) we propose to apply a multiple temporal scale approach\nthat infers DRs for different window lengths (MDR) from a still image. We\nempirically validate the value of our approach on the task of frame ranking,\nand show how our proposed MDR attains state of the art results on BP4D for AU\nintensity estimation and on SEMAINE for dimensional affect estimation, using\nonly still images at test time.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 07:15:53 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Song", "Siyang", ""], ["S\u00e1nchez-Lozano", "Enrique", ""], ["Shen", "Linlin", ""], ["Johnston", "Alan", ""], ["Valstar", "Michel", ""]]}, {"id": "1904.02420", "submitter": "Quentin Jodelet", "authors": "Quentin Jodelet, Vincent Gripon and Masafumi Hagiwara", "title": "Transfer Learning with Sparse Associative Memories", "comments": "Presented at the 28th International Conference on Artificial Neural\n  Networks (ICANN 2019)", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2019:\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\n  Science, vol 11727. Springer, Cham", "doi": "10.1007/978-3-030-30487-4_39", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel layer designed to be used as the output\nof pre-trained neural networks in the context of classification. Based on\nAssociative Memories, this layer can help design Deep Neural Networks which\nsupport incremental learning and that can be (partially) trained in real time\non embedded devices. Experiments on the ImageNet dataset and other different\ndomain specific datasets show that it is possible to design more flexible and\nfaster-to-train Neural Networks at the cost of a slight decrease in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:16:30 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 14:20:08 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 12:30:09 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Jodelet", "Quentin", ""], ["Gripon", "Vincent", ""], ["Hagiwara", "Masafumi", ""]]}, {"id": "1904.02422", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Neslihan Kose, Ahmet Gunduz, Gerhard Rigoll", "title": "Resource Efficient 3D Convolutional Neural Networks", "comments": "Accepted to ICCV 2019 workshop - Neural Architects", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks with 3D kernels (3D CNNs) have been\nvery popular in computer vision community as a result of their superior ability\nof extracting spatio-temporal features within video frames compared to 2D CNNs.\nAlthough there has been great advances recently to build resource efficient 2D\nCNN architectures considering memory and power budget, there is hardly any\nsimilar resource efficient architectures for 3D CNNs. In this paper, we have\nconverted various well-known resource efficient 2D CNNs to 3D CNNs and\nevaluated their performance on three major benchmarks in terms of\nclassification accuracy for different complexity levels. We have experimented\non (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester\ndataset to inspect their ability to capture motion patterns, and (3) UCF-101 to\ninspect the applicability of transfer learning. We have evaluated the run-time\nperformance of each model on a single Titan XP GPU and a Jetson TX2 embedded\nsystem. The results of this study show that these models can be utilized for\ndifferent types of real-world applications since they provide real-time\nperformance with considerable accuracies and memory usage. Our analysis on\ndifferent complexity levels shows that the resource efficient 3D CNNs should\nnot be designed too shallow or narrow in order to save complexity. The codes\nand pretrained models used in this work are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:19:19 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 12:46:55 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 08:34:32 GMT"}, {"version": "v4", "created": "Mon, 9 Sep 2019 09:39:50 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Kose", "Neslihan", ""], ["Gunduz", "Ahmet", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1904.02454", "submitter": "Xie De", "authors": "Cheng Deng, Yumeng Xue, Xianglong Liu, Chao Li, Dacheng Tao", "title": "Active Transfer Learning Network: A Unified Deep Joint Spectral-Spatial\n  Feature Learning Model For Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2868851", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently attracted significant attention in the field of\nhyperspectral images (HSIs) classification. However, the construction of an\nefficient deep neural network (DNN) mostly relies on a large number of labeled\nsamples being available. To address this problem, this paper proposes a unified\ndeep network, combined with active transfer learning that can be well-trained\nfor HSIs classification using only minimally labeled training data. More\nspecifically, deep joint spectral-spatial feature is first extracted through\nhierarchical stacked sparse autoencoder (SSAE) networks. Active transfer\nlearning is then exploited to transfer the pre-trained SSAE network and the\nlimited training samples from the source domain to the target domain, where the\nSSAE network is subsequently fine-tuned using the limited labeled samples\nselected from both source and target domain by corresponding active learning\nstrategies. The advantages of our proposed method are threefold: 1) the network\ncan be effectively trained using only limited labeled samples with the help of\nnovel active learning strategies; 2) the network is flexible and scalable\nenough to function across various transfer situations, including cross-dataset\nand intra-image; 3) the learned deep joint spectral-spatial feature\nrepresentation is more generic and robust than many joint spectral-spatial\nfeature representation. Extensive comparative evaluations demonstrate that our\nproposed method significantly outperforms many state-of-the-art approaches,\nincluding both traditional and deep network-based methods, on three popular\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 10:18:06 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Deng", "Cheng", ""], ["Xue", "Yumeng", ""], ["Liu", "Xianglong", ""], ["Li", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02459", "submitter": "Neeru Dubey", "authors": "Neeru Dubey, Shreya Ghosh, Abhinav Dhall", "title": "Unsupervised Learning of Eye Gaze Representation from the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic eye gaze estimation has interested researchers for a while now. In\nthis paper, we propose an unsupervised learning based method for estimating the\neye gaze region. To train the proposed network \"Ize-Net\" in self-supervised\nmanner, we collect a large `in the wild' dataset containing 1,54,251 images\nfrom the web. For the images in the database, we divide the gaze into three\nregions based on an automatic technique based on pupil-centers localization and\nthen use a feature-based technique to determine the gaze region. The\nperformance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning\nresults of Ize-Net for the task of eye gaze estimation. The feature\nrepresentation learned is also used to train traditional machine learning\nalgorithms for eye gaze estimation. The results demonstrate that the proposed\nmethod learns a rich data representation, which can be efficiently fine-tuned\nfor any eye gaze estimation dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 10:25:13 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Dubey", "Neeru", ""], ["Ghosh", "Shreya", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1904.02520", "submitter": "Xie De", "authors": "Cheng Deng, Zhao Li, Xinbo Gao, Dacheng Tao", "title": "Deep Multi-scale Discriminative Networks for Double JPEG Compression\n  Forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As JPEG is the most widely used image format, the importance of tampering\ndetection for JPEG images in blind forensics is self-evident. In this area,\nextracting effective statistical characteristics from a JPEG image for\nclassification remains a challenge. Effective features are designed manually in\ntraditional methods, suggesting that extensive labor-consuming research and\nderivation is required. In this paper, we propose a novel image tampering\ndetection method based on deep multi-scale discriminative networks (MSD-Nets).\nThe multi-scale module is designed to automatically extract multiple features\nfrom the discrete cosine transform (DCT) coefficient histograms of the JPEG\nimage. This module can capture the characteristic information in different\nscale spaces. In addition, a discriminative module is also utilized to improve\nthe detection effect of the networks in those difficult situations when the\nfirst compression quality (QF1) is higher than the second one (QF2). A special\nnetwork in this module is designed to distinguish the small statistical\ndifference between authentic and tampered regions in these cases. Finally, a\nprobability map can be obtained and the specific tampering area is located\nusing the last classification results. Extensive experiments demonstrate the\nsuperiority of our proposed method in both quantitative and qualitative metrics\nwhen compared with state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:44:57 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Deng", "Cheng", ""], ["Li", "Zhao", ""], ["Gao", "Xinbo", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02526", "submitter": "Eric Heim", "authors": "Eric Heim", "title": "Constrained Generative Adversarial Networks for Interactive Image\n  Generation", "comments": "To Appear in the Proceedings of the 2019 Conference on Computer\n  Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have received a great deal of\nattention due in part to recent success in generating original, high-quality\nsamples from visual domains. However, most current methods only allow for users\nto guide this image generation process through limited interactions. In this\nwork we develop a novel GAN framework that allows humans to be \"in-the-loop\" of\nthe image generation process. Our technique iteratively accepts relative\nconstraints of the form \"Generate an image more like image A than image B\".\nAfter each constraint is given, the user is presented with new outputs from the\nGAN, informing the next round of feedback. This feedback is used to constrain\nthe output of the GAN with respect to an underlying semantic space that can be\ndesigned to model a variety of different notions of similarity (e.g. classes,\nattributes, object relationships, color, etc.). In our experiments, we show\nthat our GAN framework is able to generate images that are of comparable\nquality to equivalent unsupervised GANs while satisfying a large number of the\nconstraints provided by users, effectively changing a GAN into one that allows\nusers interactive control over image generation without sacrificing image\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:59:41 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Heim", "Eric", ""]]}, {"id": "1904.02549", "submitter": "Arnaud Dapogny", "authors": "Arnaud Dapogny, K\\'evin Bailly and Matthieu Cord", "title": "DeCaFA: Deep Convolutional Cascade for Face Alignment In The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Alignment is an active computer vision domain, that consists in\nlocalizing a number of facial landmarks that vary across datasets.\nState-of-the-art face alignment methods either consist in end-to-end\nregression, or in refining the shape in a cascaded manner, starting from an\ninitial guess. In this paper, we introduce DeCaFA, an end-to-end deep\nconvolutional cascade architecture for face alignment. DeCaFA uses\nfully-convolutional stages to keep full spatial resolution throughout the\ncascade. Between each cascade stage, DeCaFA uses multiple chained transfer\nlayers with spatial softmax to produce landmark-wise attention maps for each of\nseveral landmark alignment tasks. Weighted intermediate supervision, as well as\nefficient feature fusion between the stages allow to learn to progressively\nrefine the attention maps in an end-to-end manner. We show experimentally that\nDeCaFA significantly outperforms existing approaches on 300W, CelebA and WFLW\ndatabases. In addition, we show that DeCaFA can learn fine alignment with\nreasonable accuracy from very few images using coarsely annotated data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 13:36:11 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Dapogny", "Arnaud", ""], ["Bailly", "K\u00e9vin", ""], ["Cord", "Matthieu", ""]]}, {"id": "1904.02553", "submitter": "Minye Wu", "authors": "Minye Wu, Haibin Ling, Ning Bi, Shenghua Gao, Hao Sheng, Jingyi Yu", "title": "Generic Multiview Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progresses in visual tracking have greatly improved the tracking\nperformance. However, challenges such as occlusion and view change remain\nobstacles in real world deployment. A natural solution to these challenges is\nto use multiple cameras with multiview inputs, though existing systems are\nmostly limited to specific targets (e.g. human), static cameras, and/or camera\ncalibration. To break through these limitations, we propose a generic multiview\ntracking (GMT) framework that allows camera movement, while requiring neither\nspecific object model nor camera calibration. A key innovation in our framework\nis a cross-camera trajectory prediction network (TPN), which implicitly and\ndynamically encodes camera geometric relations, and hence addresses missing\ntarget issues such as occlusion. Moreover, during tracking, we assemble\ninformation across different cameras to dynamically update a novel\ncollaborative correlation filter (CCF), which is shared among cameras to\nachieve robustness against view change. The two components are integrated into\na correlation filter tracking framework, where the features are trained offline\nusing existing single view tracking datasets. For evaluation, we first\ncontribute a new generic multiview tracking dataset (GMTD) with careful\nannotations, and then run experiments on GMTD and the PETS2009 datasets. On\nboth datasets, the proposed GMT algorithm shows clear advantages over\nstate-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 13:42:12 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Wu", "Minye", ""], ["Ling", "Haibin", ""], ["Bi", "Ning", ""], ["Gao", "Shenghua", ""], ["Sheng", "Hao", ""], ["Yu", "Jingyi", ""]]}, {"id": "1904.02566", "submitter": "Akihiro Nakamura", "authors": "Akihiro Nakamura, Michihiro Kobayashi", "title": "Noise-Level Estimation from Single Color Image Using Correlations\n  Between Textures in RGB Channels", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method for estimating noise level from a single color\nimage. In most image-denoising algorithms, an accurate noise-level estimate\nresults in good denoising performance; however, it is difficult to estimate\nnoise level from a single image because it is an ill-posed problem. We tackle\nthis problem by using prior knowledge that textures are highly correlated\nbetween RGB channels and noise is uncorrelated to other signals. We also\nextended our method for RAW images because they are available in almost all\ndigital cameras and often used in practical situations. Experiments show the\nhigh noise-estimation performance of our method in synthetic noisy images. We\nalso applied our method to natural images including RAW images and achieved\nbetter noise-estimation performance than conventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:12:10 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Nakamura", "Akihiro", ""], ["Kobayashi", "Michihiro", ""]]}, {"id": "1904.02575", "submitter": "Zhenzhen Dai", "authors": "Zhenzhen Dai, Eric Carver, Chang Liu, Joon Lee, Aharon Feldman, Weiwei\n  Zong, Milan Pantelic, Mohamed Elshaikh, Ning Wen", "title": "Segmentation of the Prostatic Gland and the Intraprostatic Lesions on\n  Multiparametic MRI Using Mask-RCNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer (PCa) is the most common cancer in men in the United States.\nMultiparametic magnetic resonance imaging (mp-MRI) has been explored by many\nresearchers to targeted prostate biopsies and radiation therapy. However,\nassessment on mp-MRI can be subjective, development of computer-aided diagnosis\nsystems to automatically delineate the prostate gland and the intraprostratic\nlesions (ILs) becomes important to facilitate with radiologists in clinical\npractice. In this paper, we first study the implementation of the Mask-RCNN\nmodel to segment the prostate and ILs. We trained and evaluated models on 120\npatients from two different cohorts of patients. We also used 2D U-Net and 3D\nU-Net as benchmarks to segment the prostate and compared the model's\nperformance. The contour variability of ILs using the algorithm was also\nbenchmarked against the interobserver variability between two different\nradiation oncologists on 19 patients. Our results indicate that the Mask-RCNN\nmodel is able to reach state-of-art performance in the prostate segmentation\nand outperforms several competitive baselines in ILs segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:25:14 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Dai", "Zhenzhen", ""], ["Carver", "Eric", ""], ["Liu", "Chang", ""], ["Lee", "Joon", ""], ["Feldman", "Aharon", ""], ["Zong", "Weiwei", ""], ["Pantelic", "Milan", ""], ["Elshaikh", "Mohamed", ""], ["Wen", "Ning", ""]]}, {"id": "1904.02587", "submitter": "Maria-Laura Torrente", "authors": "Mauro C. Beltrametti, Cristina Campi, Anna Maria Massone, Maria-Laura\n  Torrente", "title": "Geometry of the Hough transforms with applications to synthetic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of the Hough transform technique to detect curves in images,\nwe provide a bound for the number of Hough transforms to be considered for a\nsuccessful optimization of the accumulator function in the recognition\nalgorithm. Such a bound is consequence of geometrical arguments. We also show\nthe robustness of the results when applied to synthetic datasets strongly\nperturbed by noise. An algebraic approach, discussed in the appendix, leads to\na better bound of theoretical interest in the exact case.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:44:23 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Beltrametti", "Mauro C.", ""], ["Campi", "Cristina", ""], ["Massone", "Anna Maria", ""], ["Torrente", "Maria-Laura", ""]]}, {"id": "1904.02601", "submitter": "Xin Chen", "authors": "Xin Chen, Anqi Pang, Yang Wei, Lan Xui, Jingyi Yu", "title": "TightCap: 3D Human Shape Capture with Clothing Tightness", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present TightCap, a data-driven scheme to capture both the\nhuman shape and dressed garments accurately with only a single 3D human scan,\nwhich enables numerous applications such as virtual try-on, biometrics and body\nevaluation. To break the severe variations of the human poses and garments, we\npropose to model the clothing tightness - the displacements from the garments\nto the human shape implicitly in the global UV texturing domain. To this end,\nwe utilize an enhanced statistical human template and an effective multi-stage\nalignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on\nthis 2D representation, we propose a novel framework to predicted clothing\ntightness via a novel tightness formulation, as well as an effective\noptimization scheme to further reconstruct multi-layer human shape and garments\nunder various clothing categories and human postures. We further propose a new\nclothing tightness dataset (CTD) of human scans with a large variety of\nclothing styles, poses and corresponding ground-truth human shapes to stimulate\nfurther research. Extensive experiments demonstrate the effectiveness of our\nTightCap to achieve high-quality human shape and dressed garments\nreconstruction, as well as the further applications for clothing segmentation,\nretargeting and animation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:21:39 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 13:06:11 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 09:15:14 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Chen", "Xin", ""], ["Pang", "Anqi", ""], ["Wei", "Yang", ""], ["Xui", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "1904.02605", "submitter": "Zhang Chen", "authors": "Zhang Chen, Yu Ji, Mingyuan Zhou, Sing Bing Kang, Jingyi Yu", "title": "3D Face Reconstruction Using Color Photometric Stereo with Uncalibrated\n  Near Point Lights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new color photometric stereo (CPS) method that recovers high\nquality, detailed 3D face geometry in a single shot. Our system uses three\nuncalibrated near point lights of different colors and a single camera. For\nrobust self-calibration of the light sources, we use 3D morphable model (3DMM)\nand semantic segmentation of facial parts. We address the spectral ambiguity\nproblem by incorporating albedo consensus, albedo similarity, and proxy prior\ninto a unified framework. We avoid the need for spatial constancy of albedo;\ninstead, we use a new measure for albedo similarity that is based on the albedo\nnorm profile. Experiments show that our new approach produces state-of-the-art\nresults from single image with high-fidelity geometry that includes details\nsuch as wrinkles.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:24:32 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 10:18:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chen", "Zhang", ""], ["Ji", "Yu", ""], ["Zhou", "Mingyuan", ""], ["Kang", "Sing Bing", ""], ["Yu", "Jingyi", ""]]}, {"id": "1904.02614", "submitter": "Robert Hovden", "authors": "Yi Jiang, Elliot Padgett, Robert Hovden, David A. Muller", "title": "Sampling Limits for Electron Tomography with Sparsity-exploiting\n  Reconstructions", "comments": null, "journal-ref": "Ultramicroscopy 186, 94-103 (2018)", "doi": "10.1016/j.ultramic.2017.12.010", "report-no": null, "categories": "cs.CV physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electron tomography (ET) has become a standard technique for 3D\ncharacterization of materials at the nano-scale. Traditional reconstruction\nalgorithms such as weighted back projection suffer from disruptive artifacts\nwith insufficient projections. Popularized by compressed sensing,\nsparsity-exploiting algorithms have been applied to experimental ET data and\nshow promise for improving reconstruction quality or reducing the total beam\ndose applied to a specimen. Nevertheless, theoretical bounds for these methods\nhave been less explored in the context of ET applications. Here, we perform\nnumerical simulations to investigate performance of l_1-norm and\ntotal-variation (TV) minimization under various imaging conditions. From 36,100\ndifferent simulated structures, our results show specimens with more complex\nstructures generally require more projections for exact reconstruction.\nHowever, once sufficient data is acquired, dividing the beam dose over more\nprojections provides no improvements - analogous to the traditional\ndose-fraction theorem. Moreover, a limited tilt range of +-75 or less can\nresult in distorting artifacts in sparsity-exploiting reconstructions. The\ninfluence of optimization parameters on reconstructions is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:40:56 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Jiang", "Yi", ""], ["Padgett", "Elliot", ""], ["Hovden", "Robert", ""], ["Muller", "David A.", ""]]}, {"id": "1904.02616", "submitter": "Tongtong Yuan", "authors": "Tongtong Yuan, Weihong Deng, Jian Tang, Yinan Tang, Binghui Chen", "title": "Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning", "comments": "cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning, which learns discriminative features to process image\nclustering and retrieval tasks, has attracted extensive attention in recent\nyears. A number of deep metric learning methods, which ensure that similar\nexamples are mapped close to each other and dissimilar examples are mapped\nfarther apart, have been proposed to construct effective structures for loss\nfunctions and have shown promising results. In this paper, different from the\napproaches on learning the loss structures, we propose a robust SNR distance\nmetric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of\nimage pairs for deep metric learning. By exploring the properties of our SNR\ndistance metric from the view of geometry space and statistical theory, we\nanalyze the properties of our metric and show that it can preserve the semantic\nsimilarity between image pairs, which well justify its suitability for deep\nmetric learning. Compared with Euclidean distance metric, our SNR distance\nmetric can further jointly reduce the intra-class distances and enlarge the\ninter-class distances for learned features. Leveraging our SNR distance metric,\nwe propose Deep SNR-based Metric Learning (DSML) to generate discriminative\nfeature embeddings. By extensive experiments on three widely adopted\nbenchmarks, including CARS196, CUB200-2011 and CIFAR10, our DSML has shown its\nsuperiority over other state-of-the-art methods. Additionally, we extend our\nSNR distance metric to deep hashing learning, and conduct experiments on two\nbenchmarks, including CIFAR10 and NUS-WIDE, to demonstrate the effectiveness\nand generality of our SNR distance metric.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:42:58 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Yuan", "Tongtong", ""], ["Deng", "Weihong", ""], ["Tang", "Jian", ""], ["Tang", "Yinan", ""], ["Chen", "Binghui", ""]]}, {"id": "1904.02628", "submitter": "Silvio Olivastri", "authors": "Silvio Olivastri, Gurkirt Singh, Fabio Cuzzolin", "title": "End-to-End Video Captioning", "comments": "Accepted at Large Scale Holistic Video Understanding, ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building correspondences across different modalities, such as video and\nlanguage, has recently become critical in many visual recognition applications,\nsuch as video captioning. Inspired by machine translation, recent models tackle\nthis task using an encoder-decoder strategy. The (video) encoder is\ntraditionally a Convolutional Neural Network (CNN), while the decoding (for\nlanguage generation) is done using a Recurrent Neural Network (RNN). Current\nstate-of-the-art methods, however, train encoder and decoder separately. CNNs\nare pretrained on object and/or action recognition tasks and used to encode\nvideo-level features. The decoder is then optimised on such static features to\ngenerate the video's description. This disjoint setup is arguably sub-optimal\nfor input (video) to output (description) mapping. In this work, we propose to\noptimise both encoder and decoder simultaneously in an end-to-end fashion. In a\ntwo-stage training setting, we first initialise our architecture using\npre-trained encoders and decoders -- then, the entire network is trained\nend-to-end in a fine-tuning stage to learn the most relevant features for video\ncaption generation. In our experiments, we use GoogLeNet and\nInception-ResNet-v2 as encoders and an original Soft-Attention (SA-) LSTM as a\ndecoder. Analogously to gains observed in other computer vision problems, we\nshow that end-to-end training significantly improves over the traditional,\ndisjoint training process. We evaluate our End-to-End (EtENet) Networks on the\nMicrosoft Research Video Description (MSVD) and the MSR Video to Text (MSR-VTT)\nbenchmark datasets, showing how EtENet achieves state-of-the-art performance\nacross the board.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:57:23 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:28:48 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Olivastri", "Silvio", ""], ["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1904.02632", "submitter": "Raphael Gontijo Lopes", "authors": "Raphael Gontijo Lopes, David Ha, Douglas Eck, Jonathon Shlens", "title": "A Learned Representation for Scalable Vector Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic advances in generative models have resulted in near photographic\nquality for artificially rendered faces, animals and other objects in the\nnatural world. In spite of such advances, a higher level understanding of\nvision and imagery does not arise from exhaustively modeling an object, but\ninstead identifying higher-level attributes that best summarize the aspects of\nan object. In this work we attempt to model the drawing process of fonts by\nbuilding sequential generative models of vector graphics. This model has the\nbenefit of providing a scale-invariant representation for imagery whose latent\nrepresentation may be systematically manipulated and exploited to perform style\npropagation. We demonstrate these results on a large dataset of fonts and\nhighlight how such a model captures the statistical dependencies and richness\nof this dataset. We envision that our model can find use as a tool for graphic\ndesigners to facilitate font design.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:04:03 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Lopes", "Raphael Gontijo", ""], ["Ha", "David", ""], ["Eck", "Douglas", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1904.02633", "submitter": "Tzu-Ming Harry Hsu", "authors": "Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag,\n  Wei-Hung Weng, Peter Szolovits, Marzyeh Ghassemi", "title": "Clinically Accurate Chest X-Ray Report Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic generation of radiology reports given medical radiographs has\nsignificant potential to operationally and improve clinical patient care. A\nnumber of prior works have focused on this problem, employing advanced methods\nfrom computer vision and natural language generation to produce readable\nreports. However, these works often fail to account for the particular nuances\nof the radiology domain, and, in particular, the critical importance of\nclinical accuracy in the resulting generated reports. In this work, we present\na domain-aware automatic chest X-ray radiology report generation system which\nfirst predicts what topics will be discussed in the report, then conditionally\ngenerates sentences corresponding to these topics. The resulting system is\nfine-tuned using reinforcement learning, considering both readability and\nclinical accuracy, as assessed by the proposed Clinically Coherent Reward. We\nverify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that\nour model offers marked improvements on both language generation metrics and\nCheXpert assessed accuracy over a variety of competitive baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:04:30 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 04:15:47 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liu", "Guanxiong", ""], ["Hsu", "Tzu-Ming Harry", ""], ["McDermott", "Matthew", ""], ["Boag", "Willie", ""], ["Weng", "Wei-Hung", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1904.02639", "submitter": "Dong Gong", "authors": "Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda\n  Mansour, Svetha Venkatesh, Anton van den Hengel", "title": "Memorizing Normality to Detect Anomaly: Memory-augmented Deep\n  Autoencoder for Unsupervised Anomaly Detection", "comments": "Accepted to appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoencoder has been extensively used for anomaly detection. Training on\nthe normal data, the autoencoder is expected to produce higher reconstruction\nerror for the abnormal inputs than the normal ones, which is adopted as a\ncriterion for identifying anomalies. However, this assumption does not always\nhold in practice. It has been observed that sometimes the autoencoder\n\"generalizes\" so well that it can also reconstruct anomalies well, leading to\nthe miss detection of anomalies. To mitigate this drawback for autoencoder\nbased anomaly detector, we propose to augment the autoencoder with a memory\nmodule and develop an improved autoencoder called memory-augmented autoencoder,\ni.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder\nand then uses it as a query to retrieve the most relevant memory items for\nreconstruction. At the training stage, the memory contents are updated and are\nencouraged to represent the prototypical elements of the normal data. At the\ntest stage, the learned memory will be fixed, and the reconstruction is\nobtained from a few selected memory records of the normal data. The\nreconstruction will thus tend to be close to a normal sample. Thus the\nreconstructed errors on anomalies will be strengthened for anomaly detection.\nMemAE is free of assumptions on the data type and thus general to be applied to\ndifferent tasks. Experiments on various datasets prove the excellent\ngeneralization and high effectiveness of the proposed MemAE.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:16:50 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 08:20:46 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Gong", "Dong", ""], ["Liu", "Lingqiao", ""], ["Le", "Vuong", ""], ["Saha", "Budhaditya", ""], ["Mansour", "Moussa Reda", ""], ["Venkatesh", "Svetha", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1904.02643", "submitter": "H. Sebastian Seung", "authors": "Eric Mitchell and Stefan Keselj and Sergiy Popovych and Davit\n  Buniatyan and H. Sebastian Seung", "title": "Siamese Encoding and Alignment by Multiscale Learning with\n  Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of aligning a source image to a target image, where the\ntransform is specified by a dense vector field. The two images are encoded as\nfeature hierarchies by siamese convolutional nets. Then a hierarchy of aligner\nmodules computes the transform in a coarse-to-fine recursion. Each module\nreceives as input the transform that was computed by the module at the level\nabove, aligns the source and target encodings at the same level of the\nhierarchy, and then computes an improved approximation to the transform using a\nconvolutional net. The entire architecture of encoder and aligner nets is\ntrained in a self-supervised manner to minimize the squared error between\nsource and target remaining after alignment. We show that siamese encoding\nenables more accurate alignment than the image pyramids of SPyNet, a previous\ndeep learning approach to coarse-to-fine alignment. Furthermore,\nself-supervision applies even without target values for the transform, unlike\nthe strongly supervised SPyNet. We also show that our approach outperforms\none-shot approaches to alignment, because the fine pathways in the latter\napproach may fail to contribute to alignment accuracy when displacements are\nlarge. As shown by previous one-shot approaches, good results from\nself-supervised learning require that the loss function additionally penalize\nnon-smooth transforms. We demonstrate that \"masking out\" the penalty function\nnear discontinuities leads to correct recovery of non-smooth transforms. Our\nclaims are supported by empirical comparisons using images from serial section\nelectron microscopy of brain tissue.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:31:01 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Mitchell", "Eric", ""], ["Keselj", "Stefan", ""], ["Popovych", "Sergiy", ""], ["Buniatyan", "Davit", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1904.02654", "submitter": "Jindong Wang", "authors": "Chaohui Yu, Jindong Wang, Yiqiang Chen, Zijing Wu", "title": "Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel\n  Pruning", "comments": "Accepted by International Joint Conference on Neural Networks (IJCNN)\n  2019; 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep unsupervised domain adaptation (UDA) has recently received increasing\nattention from researchers. However, existing methods are computationally\nintensive due to the computation cost of Convolutional Neural Networks (CNN)\nadopted by most work. To date, there is no effective network compression method\nfor accelerating these models. In this paper, we propose a unified Transfer\nChannel Pruning (TCP) approach for accelerating UDA models. TCP is capable of\ncompressing the deep UDA model by pruning less important channels while\nsimultaneously learning transferable features by reducing the cross-domain\ndistribution divergence. Therefore, it reduces the impact of negative transfer\nand maintains competitive performance on the target task. To the best of our\nknowledge, TCP is the first approach that aims at accelerating deep UDA models.\nTCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two\ncommon backbone networks-VGG16 and ResNet50. Experimental results demonstrate\nthat TCP achieves comparable or better classification accuracy than other\ncomparison methods while significantly reducing the computational cost. To be\nmore specific, in VGG16, we get even higher accuracy after pruning 26% floating\npoint operations (FLOPs); in ResNet50, we also get higher accuracy on half of\nthe tasks after pruning 12% FLOPs. We hope that TCP will open a new door for\nfuture research on accelerating transfer learning models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 07:32:30 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Yu", "Chaohui", ""], ["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Wu", "Zijing", ""]]}, {"id": "1904.02657", "submitter": "Georg Pichler", "authors": "Georg Pichler and Jose Dolz and Ismail Ben Ayed and Pablo Piantanida", "title": "On Direct Distribution Matching for Adapting Segmentation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimization of distribution matching losses is a principled approach to\ndomain adaptation in the context of image classification. However, it is\nlargely overlooked in adapting segmentation networks, which is currently\ndominated by adversarial models. We propose a class of loss functions, which\nencourage direct kernel density matching in the network-output space, up to\nsome geometric transformations computed from unlabeled inputs. Rather than\nusing an intermediate domain discriminator, our direct approach unifies\ndistribution matching and segmentation in a single loss. Therefore, it\nsimplifies segmentation adaptation by avoiding extra adversarial steps, while\nimproving both the quality, stability and efficiency of training. We juxtapose\nour approach to state-of-the-art segmentation adaptation via adversarial\ntraining in the network-output space. In the challenging task of adapting brain\nsegmentation across different magnetic resonance images (MRI) modalities, our\napproach achieves significantly better results both in terms of accuracy and\nstability.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:49:18 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Pichler", "Georg", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Piantanida", "Pablo", ""]]}, {"id": "1904.02663", "submitter": "Yoni Kasten", "authors": "Yoni Kasten, Amnon Geifman, Meirav Galun, Ronen Basri", "title": "Algebraic Characterization of Essential Matrices and Their Averaging in\n  Multiview Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Essential matrix averaging, i.e., the task of recovering camera locations and\norientations in calibrated, multiview settings, is a first step in global\napproaches to Euclidean structure from motion. A common approach to essential\nmatrix averaging is to separately solve for camera orientations and\nsubsequently for camera positions. This paper presents a novel approach that\nsolves simultaneously for both camera orientations and positions. We offer a\ncomplete characterization of the algebraic conditions that enable a unique\nEuclidean reconstruction of $n$ cameras from a collection of $(^n_2)$ essential\nmatrices. We next use these conditions to formulate essential matrix averaging\nas a constrained optimization problem, allowing us to recover a consistent set\nof essential matrices given a (possibly partial) set of measured essential\nmatrices computed independently for pairs of images. We finally use the\nrecovered essential matrices to determine the global positions and orientations\nof the $n$ cameras. We test our method on common SfM datasets, demonstrating\nhigh accuracy while maintaining efficiency and robustness, compared to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:00:00 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 19:14:06 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Kasten", "Yoni", ""], ["Geifman", "Amnon", ""], ["Galun", "Meirav", ""], ["Basri", "Ronen", ""]]}, {"id": "1904.02672", "submitter": "John Lin", "authors": "John Lin, Mohamed El Amine Seddik, Mohamed Tamaazousti, Youssef\n  Tamaazousti and Adrien Bartoli", "title": "Deep Multi-class Adversarial Specularity Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning approach, in the form of a fully-convolutional\nneural network (CNN), which automatically and consistently removes specular\nhighlights from a single image by generating its diffuse component. To train\nthe generative network, we define an adversarial loss on a discriminative\nnetwork as in the GAN framework and combined it with a content loss. In\ncontrast to existing GAN approaches, we implemented the discriminator to be a\nmulti-class classifier instead of a binary one, to find more constraining\nfeatures. This helps the network pinpoint the diffuse manifold by providing two\nmore gradient terms. We also rendered a synthetic dataset designed to help the\nnetwork generalize well. We show that our model performs well across various\nsynthetic and real images and outperforms the state-of-the-art in consistency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:14:14 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Lin", "John", ""], ["Seddik", "Mohamed El Amine", ""], ["Tamaazousti", "Mohamed", ""], ["Tamaazousti", "Youssef", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1904.02675", "submitter": "Jionghao Wu", "authors": "Wu Jionghao", "title": "UU-Nets Connecting Discriminator and Generator for Image to Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial generative model have successfully manifest itself in image\nsynthesis. However, the performance deteriorate and unstable, because\ndiscriminator is far stable than generator, and it is hard to control the game\nbetween the two modules. Various methods have been introduced to tackle the\nproblem such as WGAN, Relativistic GAN and their successors by adding or\nrestricting the loss function, which certainly help balance the min-max game,\nbut they all focused on the loss function ignoring the intrinsic structure\nlimitation. We present a UU-Net architecture inspired by U-net bridging the\nencoder and the decoder, UU-Net composed by two U-Net liked modules\nrespectively served as generator and discriminator. Because the modules in\nU-net are symmetrical, therefore it shares weights easily between all four\ncomponents. Thanks to UU-net's modules identical and symmetric property, we\ncould not only carried the features from inner generator's encoder to its\ndecoder, but also to the discriminator's encoder and decoder. By this design,\nit give us more control and condition flexibility to intervene the process\nbetween the generator and the discriminator.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:25:21 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Jionghao", "Wu", ""]]}, {"id": "1904.02683", "submitter": "Zongmian Li", "authors": "Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev, Nicolas\n  Mansard and Josef Sivic", "title": "Estimating 3D Motion and Forces of Person-Object Interactions from\n  Monocular Video", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2019.00884", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a method to automatically reconstruct the 3D\nmotion of a person interacting with an object from a single RGB video. Our\nmethod estimates the 3D poses of the person and the object, contact positions,\nand forces and torques actuated by the human limbs. The main contributions of\nthis work are three-fold. First, we introduce an approach to jointly estimate\nthe motion and the actuation forces of the person on the manipulated object by\nmodeling contacts and the dynamics of their interactions. This is cast as a\nlarge-scale trajectory optimization problem. Second, we develop a method to\nautomatically recognize from the input video the position and timing of\ncontacts between the person and the object or the ground, thereby significantly\nsimplifying the complexity of the optimization. Third, we validate our approach\non a recent MoCap dataset with ground truth contact forces and demonstrate its\nperformance on a new dataset of Internet videos showing people manipulating a\nvariety of tools in unconstrained environments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:43:35 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 11:45:10 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Li", "Zongmian", ""], ["Sedlar", "Jiri", ""], ["Carpentier", "Justin", ""], ["Laptev", "Ivan", ""], ["Mansard", "Nicolas", ""], ["Sivic", "Josef", ""]]}, {"id": "1904.02689", "submitter": "Daniel Bolya", "authors": "Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee", "title": "YOLACT: Real-time Instance Segmentation", "comments": "Updated for ICCV 2019 and added appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, fully-convolutional model for real-time instance\nsegmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a\nsingle Titan Xp, which is significantly faster than any previous competitive\napproach. Moreover, we obtain this result after training on only one GPU. We\naccomplish this by breaking instance segmentation into two parallel subtasks:\n(1) generating a set of prototype masks and (2) predicting per-instance mask\ncoefficients. Then we produce instance masks by linearly combining the\nprototypes with the mask coefficients. We find that because this process\ndoesn't depend on repooling, this approach produces very high-quality masks and\nexhibits temporal stability for free. Furthermore, we analyze the emergent\nbehavior of our prototypes and show they learn to localize instances on their\nown in a translation variant manner, despite being fully-convolutional.\nFinally, we also propose Fast NMS, a drop-in 12 ms faster replacement for\nstandard NMS that only has a marginal performance penalty.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:46:12 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 18:09:40 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Bolya", "Daniel", ""], ["Zhou", "Chong", ""], ["Xiao", "Fanyi", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1904.02698", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic", "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order\n  Tensor", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent findings indicate that over-parametrization, while crucial for\nsuccessfully training deep neural networks, also introduces large amounts of\nredundancy. Tensor methods have the potential to efficiently parametrize\nover-complete representations by leveraging this redundancy. In this paper, we\npropose to fully parametrize Convolutional Neural Networks (CNNs) with a single\nhigh-order, low-rank tensor. Previous works on network tensorization have\nfocused on parametrizing individual layers (convolutional or fully connected)\nonly, and perform the tensorization layer-by-layer separately. In contrast, we\npropose to jointly capture the full structure of a neural network by\nparametrizing it with a single high-order tensor, the modes of which represent\neach of the architectural design parameters of the network (e.g. number of\nconvolutional blocks, depth, number of stacks, input features, etc). This\nparametrization allows to regularize the whole network and drastically reduce\nthe number of parameters. Our model is end-to-end trainable and the low-rank\nstructure imposed on the weight tensor acts as an implicit regularization. We\nstudy the case of networks with rich structure, namely Fully Convolutional\nNetworks (FCNs), which we propose to parametrize with a single 8th-order\ntensor. We show that our approach can achieve superior performance with small\ncompression rates, and attain high compression rates with negligible drop in\naccuracy for the challenging task of human pose estimation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:55:37 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kossaifi", "Jean", ""], ["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.02701", "submitter": "Jiangmiao Pang", "authors": "Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang,\n  Dahua Lin", "title": "Libra R-CNN: Towards Balanced Learning for Object Detection", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with model architectures, the training process, which is also\ncrucial to the success of detectors, has received relatively less attention in\nobject detection. In this work, we carefully revisit the standard training\npractice of detectors, and find that the detection performance is often limited\nby the imbalance during the training process, which generally consists in three\nlevels - sample level, feature level, and objective level. To mitigate the\nadverse effects caused thereby, we propose Libra R-CNN, a simple but effective\nframework towards balanced learning for object detection. It integrates three\nnovel components: IoU-balanced sampling, balanced feature pyramid, and balanced\nL1 loss, respectively for reducing the imbalance at sample, feature, and\nobjective level. Benefitted from the overall balanced design, Libra R-CNN\nsignificantly improves the detection performance. Without bells and whistles,\nit achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN\nFaster R-CNN and RetinaNet respectively on MSCOCO.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:58:22 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Pang", "Jiangmiao", ""], ["Chen", "Kai", ""], ["Shi", "Jianping", ""], ["Feng", "Huajun", ""], ["Ouyang", "Wanli", ""], ["Lin", "Dahua", ""]]}, {"id": "1904.02734", "submitter": "Shane Steinert-Threlkeld", "authors": "Lewis O'Sullivan and Shane Steinert-Threlkeld", "title": "Neural Models of the Psychosemantics of `Most'", "comments": "to appear at 9th Workshop on Cognitive Modeling and Computational\n  Linguistics (CMCL2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How are the meanings of linguistic expressions related to their use in\nconcrete cognitive tasks? Visual identification tasks show human speakers can\nexhibit considerable variation in their understanding, representation and\nverification of certain quantifiers. This paper initiates an investigation into\nneural models of these psycho-semantic tasks. We trained two types of network\n-- a convolutional neural network (CNN) model and a recurrent model of visual\nattention (RAM) -- on the \"most\" verification task from \\citet{Pietroski2009},\nmanipulating the visual scene and novel notions of task duration. Our results\nqualitatively mirror certain features of human performance (such as sensitivity\nto the ratio of set sizes, indicating a reliance on approximate number) while\ndiffering in interesting ways (such as exhibiting a subtly different pattern\nfor the effect of image type). We conclude by discussing the prospects for\nusing neural models as cognitive models of this and other psychosemantic tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 18:14:23 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["O'Sullivan", "Lewis", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "1904.02749", "submitter": "Lei Yang", "authors": "Lei Yang, Xiaohang Zhan, Dapeng Chen, Junjie Yan, Chen Change Loy,\n  Dahua Lin", "title": "Learning to Cluster Faces on an Affinity Graph", "comments": "8 pages, 8 figures, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition sees remarkable progress in recent years, and its\nperformance has reached a very high level. Taking it to a next level requires\nsubstantially larger data, which would involve prohibitive annotation cost.\nHence, exploiting unlabeled data becomes an appealing alternative. Recent works\nhave shown that clustering unlabeled faces is a promising approach, often\nleading to notable performance gains. Yet, how to effectively cluster,\nespecially on a large-scale (i.e. million-level or above) dataset, remains an\nopen question. A key challenge lies in the complex variations of cluster\npatterns, which make it difficult for conventional clustering methods to meet\nthe needed accuracy. This work explores a novel approach, namely, learning to\ncluster instead of relying on hand-crafted criteria. Specifically, we propose a\nframework based on graph convolutional network, which combines a detection and\na segmentation module to pinpoint face clusters. Experiments show that our\nmethod yields significantly more accurate face clusters, which, as a result,\nalso lead to further performance gain in face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:01:35 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 08:41:15 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Yang", "Lei", ""], ["Zhan", "Xiaohang", ""], ["Chen", "Dapeng", ""], ["Yan", "Junjie", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1904.02750", "submitter": "Sergey Zakharov", "authors": "Sergey Zakharov, Wadim Kehl, Slobodan Ilic", "title": "DeceptionNet: Network-Driven Domain Randomization", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to tackle domain adaptation between synthetic and\nreal data. Instead, of employing \"blind\" domain randomization, i.e., augmenting\nsynthetic renderings with random backgrounds or changing illumination and\ncolorization, we leverage the task network as its own adversarial guide toward\nuseful augmentations that maximize the uncertainty of the output. To this end,\nwe design a min-max optimization scheme where a given task competes against a\nspecial deception network to minimize the task error subject to the specific\nconstraints enforced by the deceiver. The deception network samples from a\nfamily of differentiable pixel-level perturbations and exploits the task\narchitecture to find the most destructive augmentations. Unlike GAN-based\napproaches that require unlabeled data from the target domain, our method\nachieves robust mappings that scale well to multiple target distributions from\nsource data alone. We apply our framework to the tasks of digit recognition on\nenhanced MNIST variants, classification and object pose estimation on the\nCropped LineMOD dataset as well as semantic segmentation on the Cityscapes\ndataset and compare it to a number of domain adaptation approaches, thereby\ndemonstrating similar results with superior generalization capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:01:42 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 16:38:36 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zakharov", "Sergey", ""], ["Kehl", "Wadim", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1904.02756", "submitter": "Amir Hertz", "authors": "Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "Blind Visual Motif Removal from a Single Image", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many images shared over the web include overlaid objects, or visual motifs,\nsuch as text, symbols or drawings, which add a description or decoration to the\nimage. For example, decorative text that specifies where the image was taken,\nrepeatedly appears across a variety of different images. Often, the reoccurring\nvisual motif, is semantically similar, yet, differs in location, style and\ncontent (e.g. text placement, font and letters). This work proposes a deep\nlearning based technique for blind removal of such objects. In the blind\nsetting, the location and exact geometry of the motif are unknown. Our approach\nsimultaneously estimates which pixels contain the visual motif, and synthesizes\nthe underlying latent image. It is applied to a single input image, without any\nuser assistance in specifying the location of the motif, achieving\nstate-of-the-art results for blind removal of both opaque and semi-transparent\nvisual motifs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:17:05 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hertz", "Amir", ""], ["Fogel", "Sharon", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1904.02762", "submitter": "Cicero Nogueira dos Santos", "authors": "Cicero Nogueira dos Santos, Youssef Mroueh, Inkit Padhi, Pierre Dognin", "title": "Learning Implicit Generative Models by Matching Perceptual Features", "comments": "16 pages", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual features (PFs) have been used with great success in tasks such as\ntransfer learning, style transfer, and super-resolution. However, the efficacy\nof PFs as key source of information for learning generative models is not well\nstudied. We investigate here the use of PFs in the context of learning implicit\ngenerative models through moment matching (MM). More specifically, we propose a\nnew effective MM approach that learns implicit generative models by performing\nmean and covariance matching of features extracted from pretrained ConvNets.\nOur proposed approach improves upon existing MM methods by: (1) breaking away\nfrom the problematic min/max game of adversarial learning; (2) avoiding online\nlearning of kernel functions; and (3) being efficient with respect to both\nnumber of used moments and required minibatch size. Our experimental results\ndemonstrate that, due to the expressiveness of PFs from pretrained deep\nConvNets, our method achieves state-of-the-art results for challenging\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:34:23 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Mroueh", "Youssef", ""], ["Padhi", "Inkit", ""], ["Dognin", "Pierre", ""]]}, {"id": "1904.02768", "submitter": "Kristian Muri Knausg{\\aa}rd", "authors": "Erlend Olsvik, Christian M. D. Trinh, Kristian Muri Knausg{\\aa}rd,\n  Arne Wiklund, Tonje Knutsen S{\\o}rdalen, Alf Ring Kleiven, Lei Jiao and\n  Morten Goodwin", "title": "Biometric Fish Classification of Temperate Species Using Convolutional\n  Neural Network with Squeeze-and-Excitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding and ability to effectively monitor and manage coastal\necosystems are severely limited by observation methods. Automatic recognition\nof species in natural environment is a promising tool which would revolutionize\nvideo and image analysis for a wide range of applications in marine ecology.\nHowever, classifying fish from images captured by underwater cameras is in\ngeneral very challenging due to noise and illumination variations in water.\nPrevious classification methods in the literature relies on filtering the\nimages to separate the fish from the background or sharpening the images by\nremoving background noise. This pre-filtering process may negatively impact the\nclassification accuracy. In this work, we propose a Convolutional Neural\nNetwork (CNN) using the Squeeze-and-Excitation (SE) architecture for\nclassifying images of fish without pre-filtering. Different from conventional\nschemes, this scheme is divided into two steps. The first step is to train the\nfish classifier via a public data set, i.e., Fish4Knowledge, without using\nimage augmentation, named as pre-training. The second step is to train the\nclassifier based on a new data set consisting of species that we are interested\nin for classification, named as post-training. The weights obtained from\npre-training are applied to post-training as a priori. This is also known as\ntransfer learning. Our solution achieves the state-of-the-art accuracy of\n99.27% accuracy on the pre-training. The accuracy on the post-training is\n83.68%. Experiments on the post-training with image augmentation yields an\naccuracy of 87.74%, indicating that the solution is viable with a larger data\nset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:50:49 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Olsvik", "Erlend", ""], ["Trinh", "Christian M. D.", ""], ["Knausg\u00e5rd", "Kristian Muri", ""], ["Wiklund", "Arne", ""], ["S\u00f8rdalen", "Tonje Knutsen", ""], ["Kleiven", "Alf Ring", ""], ["Jiao", "Lei", ""], ["Goodwin", "Morten", ""]]}, {"id": "1904.02774", "submitter": "Viresh Ranjan", "authors": "Viresh Ranjan, Mubarak Shah, Minh Hoai Nguyen", "title": "Crowd Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of Crowd Counting, and present a crowd\ndensity estimation based approach for obtaining the crowd count. Most of the\nexisting crowd counting approaches rely on local features for estimating the\ncrowd density map. In this work, we investigate the usefulness of combining\nlocal with non-local features for crowd counting. We use convolution layers for\nextracting local features, and a type of self-attention mechanism for\nextracting non-local features. We combine the local and the non-local features,\nand use it for estimating crowd density map. We conduct experiments on three\npublicly available Crowd Counting datasets, and achieve significant improvement\nover the previous approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 20:04:39 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ranjan", "Viresh", ""], ["Shah", "Mubarak", ""], ["Nguyen", "Minh Hoai", ""]]}, {"id": "1904.02794", "submitter": "Manoj Acharya", "authors": "Manoj Acharya, Karan Jariwala, Christopher Kanan", "title": "VQD: Visual Query Detection in Natural Scenes", "comments": "To appear in NAACL 2019 ( To download the dataset please go to\n  http://www.manojacharya.com/ )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Visual Query Detection (VQD), a new visual grounding task. In VQD,\na system is guided by natural language to localize a variable number of objects\nin an image. VQD is related to visual referring expression recognition, where\nthe task is to localize only one object. We describe the first dataset for VQD\nand we propose baseline algorithms that demonstrate the difficulty of the task\ncompared to referring expression recognition.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:12:37 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 19:59:37 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Acharya", "Manoj", ""], ["Jariwala", "Karan", ""], ["Kanan", "Christopher", ""]]}, {"id": "1904.02805", "submitter": "Arturo Deza", "authors": "Arturo Deza, Amit Surana, Miguel P. Eckstein", "title": "Assessment of Faster R-CNN in Man-Machine collaborative search", "comments": "To be presented at CVPR 2019 in Long Beach, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of modern expert systems driven by deep learning that\nsupplement human experts (e.g. radiologists, dermatologists, surveillance\nscanners), we analyze how and when do such expert systems enhance human\nperformance in a fine-grained small target visual search task. We set up a 2\nsession factorial experimental design in which humans visually search for a\ntarget with and without a Deep Learning (DL) expert system. We evaluate human\nchanges of target detection performance and eye-movements in the presence of\nthe DL system. We find that performance improvements with the DL system\n(computed via a Faster R-CNN with a VGG16) interacts with observer's perceptual\nabilities (e.g., sensitivity). The main results include: 1) The DL system\nreduces the False Alarm rate per Image on average across observer groups of\nboth high/low sensitivity; 2) Only human observers with high sensitivity\nperform better than the DL system, while the low sensitivity group does not\nsurpass individual DL system performance, even when aided with the DL system\nitself; 3) Increases in number of trials and decrease in viewing time were\nmainly driven by the DL system only for the low sensitivity group. 4) The DL\nsystem aids the human observer to fixate at a target by the 3rd fixation. These\nresults provide insights of the benefits and limitations of deep learning\nsystems that are collaborative or competitive with humans.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:03:53 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Deza", "Arturo", ""], ["Surana", "Amit", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1904.02811", "submitter": "Du Tran", "authors": "Du Tran and Heng Wang and Lorenzo Torresani and Matt Feiszli", "title": "Video Classification with Channel-Separated Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group convolution has been shown to offer great computational savings in\nvarious 2D convolutional architectures for image classification. It is natural\nto ask: 1) if group convolution can help to alleviate the high computational\ncost of video classification networks; 2) what factors matter the most in 3D\ngroup convolutional networks; and 3) what are good computation/accuracy\ntrade-offs with 3D group convolutional networks.\n  This paper studies the effects of different design choices in 3D group\nconvolutional networks for video classification. We empirically demonstrate\nthat the amount of channel interactions plays an important role in the accuracy\nof 3D group convolutional networks. Our experiments suggest two main findings.\nFirst, it is a good practice to factorize 3D convolutions by separating channel\ninteractions and spatiotemporal interactions as this leads to improved accuracy\nand lower computational cost. Second, 3D channel-separated convolutions provide\na form of regularization, yielding lower training accuracy but higher test\naccuracy compared to 3D convolutions. These two empirical findings lead us to\ndesign an architecture -- Channel-Separated Convolutional Network (CSN) --\nwhich is simple, efficient, yet accurate. On Sports1M, Kinetics, and\nSomething-Something, our CSNs are comparable with or better than the\nstate-of-the-art while being 2-3 times more efficient.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:28:24 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 00:15:33 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 17:04:52 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 22:30:49 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Tran", "Du", ""], ["Wang", "Heng", ""], ["Torresani", "Lorenzo", ""], ["Feiszli", "Matt", ""]]}, {"id": "1904.02823", "submitter": "Ruizhou Ding", "authors": "Ruizhou Ding, Ting-Wu Chin, Zeye Liu, Diana Marculescu", "title": "Regularizing Activation Distribution for Training Binarized Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized Neural Networks (BNNs) can significantly reduce the inference\nlatency and energy consumption in resource-constrained devices due to their\npure-logical computation and fewer memory accesses. However, training BNNs is\ndifficult since the activation flow encounters degeneration, saturation, and\ngradient mismatch problems. Prior work alleviates these issues by increasing\nactivation bits and adding floating-point scaling factors, thereby sacrificing\nBNN's energy efficiency. In this paper, we propose to use distribution loss to\nexplicitly regularize the activation flow, and develop a framework to\nsystematically formulate the loss. Our experiments show that the distribution\nloss can consistently improve the accuracy of BNNs without losing their energy\nbenefits. Moreover, equipped with the proposed regularization, BNN training is\nshown to be robust to the selection of hyper-parameters including optimizer and\nlearning rate.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:20:09 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ding", "Ruizhou", ""], ["Chin", "Ting-Wu", ""], ["Liu", "Zeye", ""], ["Marculescu", "Diana", ""]]}, {"id": "1904.02832", "submitter": "Dacheng Tao", "authors": "Chen Gong, Tongliang Liu, Yuanyan Tang, Jian Yang, Jie Yang, Dacheng\n  Tao", "title": "A Regularization Approach for Instance-Based Superset Label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from the traditional supervised learning in which each training\nexample has only one explicit label, superset label learning (SLL) refers to\nthe problem that a training example can be associated with a set of candidate\nlabels, and only one of them is correct. Existing SLL methods are either\nregularization-based or instance-based, and the latter of which has achieved\nstate-of-the-art performance. This is because the latest instance-based methods\ncontain an explicit disambiguation operation that accurately picks up the\ngroundtruth label of each training example from its ambiguous candidate labels.\nHowever, such disambiguation operation does not fully consider the mutually\nexclusive relationship among different candidate labels, so the disambiguated\nlabels are usually generated in a nondiscriminative way, which is unfavorable\nfor the instance-based methods to obtain satisfactory performance. To address\nthis defect, we develop a novel regularization approach for instance-based\nsuperset label (RegISL) learning so that our instance-based method also\ninherits the good discriminative ability possessed by the regularization\nscheme. Specifically, we employ a graph to represent the training set, and\nrequire the examples that are adjacent on the graph to obtain similar labels.\nMore importantly, a discrimination term is proposed to enlarge the gap of\nvalues between possible labels and unlikely labels for every training example.\nAs a result, the intrinsic constraints among different candidate labels are\ndeployed, and the disambiguated labels generated by RegISL are more\ndiscriminative and accurate than those output by existing instance-based\nalgorithms. The experimental results on various tasks convincingly demonstrate\nthe superiority of our RegISL to other typical SLL methods in terms of both\ntraining accuracy and test accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 00:22:26 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Gong", "Chen", ""], ["Liu", "Tongliang", ""], ["Tang", "Yuanyan", ""], ["Yang", "Jian", ""], ["Yang", "Jie", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02835", "submitter": "Ruizhou Ding", "authors": "Ruizhou Ding, Zeye Liu, Ting-Wu Chin, Diana Marculescu, and R. D.\n  (Shawn) Blanton", "title": "FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and\n  Accurate Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the throughput and energy efficiency of Deep Neural Networks\n(DNNs) on customized hardware, lightweight neural networks constrain the\nweights of DNNs to be a limited combination (denoted as $k\\in\\{1,2\\}$) of\npowers of 2. In such networks, the multiply-accumulate operation can be\nreplaced with a single shift operation, or two shifts and an add operation. To\nprovide even more design flexibility, the $k$ for each convolutional filter can\nbe optimally chosen instead of being fixed for every filter. In this paper, we\nformulate the selection of $k$ to be differentiable, and describe model\ntraining for determining $k$-based weights on a per-filter basis. Over 46\nFPGA-design experiments involving eight configurations and four data sets\nreveal that lightweight neural networks with a flexible $k$ value (dubbed\nFLightNNs) fully utilize the hardware resources on Field Programmable Gate\nArrays (FPGAs), our experimental results show that FLightNNs can achieve\n2$\\times$ speedup when compared to lightweight NNs with $k=2$, with only 0.1\\%\naccuracy degradation. Compared to a 4-bit fixed-point quantization, FLightNNs\nachieve higher accuracy and up to 2$\\times$ inference speedup, due to their\nlightweight shift operations. In addition, our experiments also demonstrate\nthat FLightNNs can achieve higher computational energy efficiency for ASIC\nimplementation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 00:27:16 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ding", "Ruizhou", "", "Shawn"], ["Liu", "Zeye", "", "Shawn"], ["Chin", "Ting-Wu", "", "Shawn"], ["Marculescu", "Diana", "", "Shawn"], ["D.", "R.", "", "Shawn"], ["Blanton", "", ""]]}, {"id": "1904.02843", "submitter": "Jong Chul Ye", "authors": "Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Deep Learning-based Universal Beamformer for Ultrasound Imaging", "comments": "Accepted for MICCAI 2019. arXiv admin note: substantial text overlap\n  with arXiv:1901.01706", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ultrasound (US) imaging, individual channel RF measurements are\nback-propagated and accumulated to form an image after applying specific\ndelays. While this time reversal is usually implemented using a hardware- or\nsoftware-based delay-and-sum (DAS) beamformer, the performance of DAS decreases\nrapidly in situations where data acquisition is not ideal. Herein, for the\nfirst time, we demonstrate that a single data-driven adaptive beamformer\ndesigned as a deep neural network can generate high quality images robustly for\nvarious detector channel configurations and subsampling rates. The proposed\ndeep beamformer is evaluated for two distinct acquisition schemes: focused\nultrasound imaging and planewave imaging. Experimental results showed that the\nproposed deep beamformer exhibit significant performance gain for both focused\nand planar imaging schemes, in terms of contrast-to-noise ratio and structural\nsimilarity.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 01:40:52 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:07:27 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1904.02860", "submitter": "Yaojie Liu", "authors": "Yaojie Liu and Joel Stehouwer and Amin Jourabloo and Xiaoming Liu", "title": "Deep Tree Learning for Zero-shot Face Anti-Spoofing", "comments": "To appear at CVPR 2019 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is designed to keep face recognition systems from\nrecognizing fake faces as the genuine users. While advanced face anti-spoofing\nmethods are developed, new types of spoof attacks are also being created and\nbecoming a threat to all existing systems. We define the detection of unknown\nspoof attacks as Zero-Shot Face Anti-spoofing (ZSFA). Previous works of ZSFA\nonly study 1-2 types of spoof attacks, such as print/replay attacks, which\nlimits the insight of this problem. In this work, we expand the ZSFA problem to\na wide range of 13 types of spoof attacks, including print attack, replay\nattack, 3D mask attacks, and so on. A novel Deep Tree Network (DTN) is proposed\nto tackle the ZSFA. The tree is learned to partition the spoof samples into\nsemantic sub-groups in an unsupervised fashion. When a data sample arrives,\nbeing know or unknown attacks, DTN routes it to the most similar spoof cluster,\nand make the binary decision. In addition, to enable the study of ZSFA, we\nintroduce the first face anti-spoofing database that contains diverse types of\nspoof attacks. Experiments show that our proposed method achieves the state of\nthe art on multiple testing protocols of ZSFA.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 03:23:16 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 16:27:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Liu", "Yaojie", ""], ["Stehouwer", "Joel", ""], ["Jourabloo", "Amin", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1904.02865", "submitter": "Damien Teney", "authors": "Damien Teney, Anton van den Hengel", "title": "Actively Seeking and Learning from Live Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key limitations of traditional machine learning methods is their\nrequirement for training data that exemplifies all the information to be\nlearned. This is a particular problem for visual question answering methods,\nwhich may be asked questions about virtually anything. The approach we propose\nis a step toward overcoming this limitation by searching for the information\nrequired at test time. The resulting method dynamically utilizes data from an\nexternal source, such as a large set of questions/answers or images/captions.\nConcretely, we learn a set of base weights for a simple VQA model, that are\nspecifically adapted to a given question with the information specifically\nretrieved for this question. The adaptation process leverages recent advances\nin gradient-based meta learning and contributions for efficient retrieval and\ncross-domain adaptation. We surpass the state-of-the-art on the VQA-CP v2\nbenchmark and demonstrate our approach to be intrinsically more robust to\nout-of-distribution test data. We demonstrate the use of external non-VQA data\nusing the MS COCO captioning dataset to support the answering process. This\napproach opens a new avenue for open-domain VQA systems that interface with\ndiverse sources of data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 04:23:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Teney", "Damien", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1904.02870", "submitter": "Sheng Li", "authors": "Sheng Li, Fengxiang He, Bo Du, Lefei Zhang, Yonghao Xu, Dacheng Tao", "title": "Fast Spatio-Temporal Residual Network for Video Super-Resolution", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning based video super-resolution (SR) methods have\nachieved promising performance. To simultaneously exploit the spatial and\ntemporal information of videos, employing 3-dimensional (3D) convolutions is a\nnatural approach. However, straight utilizing 3D convolutions may lead to an\nexcessively high computational complexity which restricts the depth of video SR\nmodels and thus undermine the performance. In this paper, we present a novel\nfast spatio-temporal residual network (FSTRN) to adopt 3D convolutions for the\nvideo SR task in order to enhance the performance while maintaining a low\ncomputational load. Specifically, we propose a fast spatio-temporal residual\nblock (FRB) that divide each 3D filter to the product of two 3D filters, which\nhave considerably lower dimensions. Furthermore, we design a cross-space\nresidual learning that directly links the low-resolution space and the\nhigh-resolution space, which can greatly relieve the computational burden on\nthe feature fusion and up-scaling parts. Extensive evaluations and comparisons\non benchmark datasets validate the strengths of the proposed approach and\ndemonstrate that the proposed network significantly outperforms the current\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:08:00 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Li", "Sheng", ""], ["He", "Fengxiang", ""], ["Du", "Bo", ""], ["Zhang", "Lefei", ""], ["Xu", "Yonghao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02872", "submitter": "Jong Chul Ye", "authors": "Boah Kim and Jong Chul Ye", "title": "Mumford-Shah Loss Functional for Image Segmentation with Deep Learning", "comments": "Accepted for IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2941265", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art image segmentation algorithms are mostly based on\ndeep neural networks, thanks to their high performance and fast computation\ntime. However, these methods are usually trained in a supervised manner, which\nrequires large number of high quality ground-truth segmentation masks. On the\nother hand, classical image segmentation approaches such as level-set methods\nare formulated in a self-supervised manner by minimizing energy functions such\nas Mumford-Shah functional, so they are still useful to help generation of\nsegmentation masks without labels. Unfortunately, these algorithms are usually\ncomputationally expensive and often have limitation in semantic segmentation.\nIn this paper, we propose a novel loss function based on Mumford-Shah\nfunctional that can be used in deep-learning based image segmentation without\nor with small labeled data. This loss function is based on the observation that\nthe softmax layer of deep neural networks has striking similarity to the\ncharacteristic function in the Mumford-Shah functional. We show that the new\nloss function enables semi-supervised and unsupervised segmentation. In\naddition, our loss function can be also used as a regularized function to\nenhance supervised semantic segmentation algorithms. Experimental results on\nmultiple datasets demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:17:18 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 09:14:39 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Kim", "Boah", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1904.02877", "submitter": "Dimitrios Stamoulis", "authors": "Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos,\n  Bodhi Priyantha, Jie Liu, Diana Marculescu", "title": "Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4\n  Hours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:49:41 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Stamoulis", "Dimitrios", ""], ["Ding", "Ruizhou", ""], ["Wang", "Di", ""], ["Lymberopoulos", "Dimitrios", ""], ["Priyantha", "Bodhi", ""], ["Liu", "Jie", ""], ["Marculescu", "Diana", ""]]}, {"id": "1904.02884", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu", "title": "Evading Defenses to Transferable Adversarial Examples by\n  Translation-Invariant Attacks", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, which can\nmislead classifiers by adding imperceptible perturbations. An intriguing\nproperty of adversarial examples is their good transferability, making\nblack-box attacks feasible in real-world applications. Due to the threat of\nadversarial attacks, many methods have been proposed to improve the robustness.\nSeveral state-of-the-art defenses are shown to be robust against transferable\nadversarial examples. In this paper, we propose a translation-invariant attack\nmethod to generate more transferable adversarial examples against the defense\nmodels. By optimizing a perturbation over an ensemble of translated images, the\ngenerated adversarial example is less sensitive to the white-box model being\nattacked and has better transferability. To improve the efficiency of attacks,\nwe further show that our method can be implemented by convolving the gradient\nat the untranslated image with a pre-defined kernel. Our method is generally\napplicable to any gradient-based attack method. Extensive experiments on the\nImageNet dataset validate the effectiveness of the proposed method. Our best\nattack fools eight state-of-the-art defenses at an 82% success rate on average\nbased only on the transferability, demonstrating the insecurity of the current\ndefense techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:15:51 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Dong", "Yinpeng", ""], ["Pang", "Tianyu", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "1904.02887", "submitter": "Yadan Luo", "authors": "Yadan Luo, Ziwei Wang, Zi Huang, Yang Yang, Huimin Lu", "title": "Snap and Find: Deep Discrete Cross-domain Garment Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of online stores, there is a pressing need for\nintelligent search systems to understand the item photos snapped by customers\nand search against large-scale product databases to find their desired items.\nHowever, it is challenging for conventional retrieval systems to match up the\nitem photos captured by customers and the ones officially released by stores,\nespecially for garment images. To bridge the customer- and store- provided\ngarment photos, existing studies have been widely exploiting the clothing\nattributes (\\textit{e.g.,} black) and landmarks (\\textit{e.g.,} collar) to\nlearn a common embedding space for garment representations. Unfortunately they\nomit the sequential correlation of attributes and consume large quantity of\nhuman labors to label the landmarks. In this paper, we propose a deep\nmulti-task cross-domain hashing termed \\textit{DMCH}, in which cross-domain\nembedding and sequential attribute learning are modeled simultaneously.\nSequential attribute learning not only provides the semantic guidance for\nembedding, but also generates rich attention on discriminative local details\n(\\textit{e.g.,} black buttons) of clothing items without requiring extra\nlandmark labels. This leads to promising performance and 306$\\times$ boost on\nefficiency when compared with the state-of-the-art models, which is\ndemonstrated through rigorous experiments on two public fashion datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:30:22 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Luo", "Yadan", ""], ["Wang", "Ziwei", ""], ["Huang", "Zi", ""], ["Yang", "Yang", ""], ["Lu", "Huimin", ""]]}, {"id": "1904.02904", "submitter": "Cosmin Ancuti", "authors": "Codruta O. Ancuti and Cosmin Ancuti and Mateu Sbert and Radu Timofte", "title": "Dense Haze: A benchmark for image dehazing with dense-haze and haze-free\n  images", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image dehazing is an ill-posed problem that has recently drawn\nimportant attention. Despite the significant increase in interest shown for\ndehazing over the past few years, the validation of the dehazing methods\nremains largely unsatisfactory, due to the lack of pairs of real hazy and\ncorresponding haze-free reference images. To address this limitation, we\nintroduce Dense-Haze - a novel dehazing dataset. Characterized by dense and\nhomogeneous hazy scenes, Dense-Haze contains 33 pairs of real hazy and\ncorresponding haze-free images of various outdoor scenes. The hazy scenes have\nbeen recorded by introducing real haze, generated by professional haze\nmachines. The hazy and haze-free corresponding scenes contain the same visual\ncontent captured under the same illumination parameters. Dense-Haze dataset\naims to push significantly the state-of-the-art in single-image dehazing by\npromoting robust methods for real and various hazy scenes. We also provide a\ncomprehensive qualitative and quantitative evaluation of state-of-the-art\nsingle image dehazing techniques based on the Dense-Haze dataset. Not\nsurprisingly, our study reveals that the existing dehazing techniques perform\npoorly for dense homogeneous hazy scenes and that there is still much room for\nimprovement.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:27:32 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ancuti", "Codruta O.", ""], ["Ancuti", "Cosmin", ""], ["Sbert", "Mateu", ""], ["Timofte", "Radu", ""]]}, {"id": "1904.02909", "submitter": "Woonsung Park", "authors": "Woonsung Park and Munchurl Kim", "title": "Deep Predictive Video Compression with Bi-directional Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep image compression has shown a big progress in terms of coding\nefficiency and image quality improvement. However, relatively less attention\nhas been put on video compression using deep learning networks. In the paper,\nwe first propose a deep learning based bi-predictive coding network, called\nBP-DVC Net, for video compression. Learned from the lesson of the conventional\nvideo coding, a B-frame coding structure is incorporated in our BP-DVC Net.\nWhile the bi-predictive coding in the conventional video codecs requires to\ntransmit to decoder sides the motion vectors for block motion and the residues\nfrom prediction, our BP-DVC Net incorporates optical flow estimation networks\nin both encoder and decoder sides so as not to transmit the motion information\nto the decoder sides for coding efficiency improvement. Also, a bi-prediction\nnetwork in the BP-DVC Net is proposed and used to precisely predict the current\nframe and to yield the resulting residues as small as possible. Furthermore,\nour BP-DVC Net allows for the compressive feature maps to be entropy-coded\nusing the temporal context among the feature maps of adjacent frames. The\nBP-DVC Net has an end-to-end video compression architecture with newly designed\nflow and prediction losses. Experimental results show that the compression\nperformance of our proposed method is comparable to those of H.264, HEVC in\nterms of PSNR and MS-SSIM.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:43:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Park", "Woonsung", ""], ["Kim", "Munchurl", ""]]}, {"id": "1904.02910", "submitter": "Jong Chul Ye", "authors": "Sungjun Lim, Sang-Eun Lee, Sunghoe Chang, Jong Chul Ye", "title": "Blind Deconvolution Microscopy Using Cycle Consistent CNN with Explicit\n  PSF Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution microscopy has been extensively used to improve the resolution\nof the widefield fluorescent microscopy. Conventional approaches, which usually\nrequire the point spread function (PSF) measurement or blind estimation, are\nhowever computationally expensive. Recently, CNN based approaches have been\nexplored as a fast and high performance alternative. In this paper, we present\na novel unsupervised deep neural network for blind deconvolution based on cycle\nconsistency and PSF modeling layers. In contrast to the recent CNN approaches\nfor similar problem, the explicit PSF modeling layers improve the robustness of\nthe algorithm. Experimental results confirm the efficacy of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:43:34 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Lim", "Sungjun", ""], ["Lee", "Sang-Eun", ""], ["Chang", "Sunghoe", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1904.02912", "submitter": "Yen-Chi Cheng", "authors": "Tsun-Hsuan Wang, Yen-Chi Cheng, Chieh Hubert Lin, Hwann-Tzong Chen,\n  Min Sun", "title": "Point-to-Point Video Generation", "comments": "To appear in ICCV 2019. The first two authors contributed equally to\n  this work. 16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While image manipulation achieves tremendous breakthroughs (e.g., generating\nrealistic faces) in recent years, video generation is much less explored and\nharder to control, which limits its applications in the real world. For\ninstance, video editing requires temporal coherence across multiple clips and\nthus poses both start and end constraints within a video sequence. We introduce\npoint-to-point video generation that controls the generation process with two\ncontrol points: the targeted start- and end-frames. The task is challenging\nsince the model not only generates a smooth transition of frames, but also\nplans ahead to ensure that the generated end-frame conforms to the targeted\nend-frame for videos of various length. We propose to maximize the modified\nvariational lower bound of conditional data likelihood under a skip-frame\ntraining strategy. Our model can generate sequences such that their end-frame\nis consistent with the targeted end-frame without loss of quality and\ndiversity. Extensive experiments are conducted on Stochastic Moving MNIST,\nWeizmann Human Action, and Human3.6M to evaluate the effectiveness of the\nproposed method. We demonstrate our method under a series of scenarios (e.g.,\ndynamic length generation) and the qualitative results showcase the potential\nand merits of point-to-point generation. For project page, see\nhttps://zswang666.github.io/P2PVG-Project-Page/\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:43:55 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 09:14:55 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Wang", "Tsun-Hsuan", ""], ["Cheng", "Yen-Chi", ""], ["Lin", "Chieh Hubert", ""], ["Chen", "Hwann-Tzong", ""], ["Sun", "Min", ""]]}, {"id": "1904.02917", "submitter": "Tsun-Hsuan Wang", "authors": "Tsun-Hsuan Wang, Hou-Ning Hu, Chieh Hubert Lin, Yi-Hsuan Tsai,\n  Wei-Chen Chiu, Min Sun", "title": "3D LiDAR and Stereo Fusion using Stereo Matching Network with\n  Conditional Cost Volume Normalization", "comments": "ver.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complementary characteristics of active and passive depth sensing\ntechniques motivate the fusion of the Li-DAR sensor and stereo camera for\nimproved depth perception. Instead of directly fusing estimated depths across\nLiDAR and stereo modalities, we take advantages of the stereo matching network\nwith two enhanced techniques: Input Fusion and Conditional Cost Volume\nNormalization (CCVNorm) on the LiDAR information. The proposed framework is\ngeneric and closely integrated with the cost volume component that is commonly\nutilized in stereo matching neural networks. We experimentally verify the\nefficacy and robustness of our method on the KITTI Stereo and Depth Completion\ndatasets, obtaining favorable performance against various fusion strategies.\nMoreover, we demonstrate that, with a hierarchical extension of CCVNorm, the\nproposed method brings only slight overhead to the stereo matching network in\nterms of computation time and model size. For project page, see\nhttps://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:53:42 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Wang", "Tsun-Hsuan", ""], ["Hu", "Hou-Ning", ""], ["Lin", "Chieh Hubert", ""], ["Tsai", "Yi-Hsuan", ""], ["Chiu", "Wei-Chen", ""], ["Sun", "Min", ""]]}, {"id": "1904.02920", "submitter": "Simon Vandenhende", "authors": "Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere and Luc\n  Van Gool", "title": "Branched Multi-Task Networks: Deciding What Layers To Share", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of multi-task learning, neural networks with branched\narchitectures have often been employed to jointly tackle the tasks at hand.\nSuch ramified networks typically start with a number of shared layers, after\nwhich different tasks branch out into their own sequence of layers.\nUnderstandably, as the number of possible network configurations is\ncombinatorially large, deciding what layers to share and where to branch out\nbecomes cumbersome. Prior works have either relied on ad hoc methods to\ndetermine the level of layer sharing, which is suboptimal, or utilized neural\narchitecture search techniques to establish the network design, which is\nconsiderably expensive. In this paper, we go beyond these limitations and\npropose an approach to automatically construct branched multi-task networks, by\nleveraging the employed tasks' affinities. Given a specific budget, i.e. number\nof learnable parameters, the proposed approach generates architectures, in\nwhich shallow layers are task-agnostic, whereas deeper ones gradually grow more\ntask-specific. Extensive experimental analysis across numerous, diverse\nmulti-tasking datasets shows that, for a given budget, our method consistently\nyields networks with the highest performance, while for a certain performance\nthreshold it requires the least amount of learnable parameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 08:00:32 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 10:03:52 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 19:01:42 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2020 08:32:29 GMT"}, {"version": "v5", "created": "Thu, 13 Aug 2020 06:44:45 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Vandenhende", "Simon", ""], ["Georgoulis", "Stamatios", ""], ["De Brabandere", "Bert", ""], ["Van Gool", "Luc", ""]]}, {"id": "1904.02948", "submitter": "Wei Liu", "authors": "Wei Liu, Irtiza Hasan, Shengcai Liao", "title": "Center and Scale Prediction: A Box-free Approach for Pedestrian and Face\n  Detection", "comments": "An extension of the paper accepted by CVPR2019, the title is changed\n  to 'Center and Scale Prediction: A Box-free Approach for Pedestrian and Face\n  Detection'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection generally requires sliding-window classifiers in tradition\nor anchor box based predictions in modern deep learning approaches. However,\neither of these approaches requires tedious configurations in boxes. In this\npaper, we provide a new perspective where detecting objects is motivated as a\nhigh-level semantic feature detection task. Like edges, corners, blobs and\nother feature detectors, the proposed detector scans for feature points all\nover the image, for which the convolution is naturally suited. However, unlike\nthese traditional low-level features, the proposed detector goes for a\nhigher-level abstraction, that is, we are looking for central points where\nthere are objects, and modern deep models are already capable of such a\nhigh-level semantic abstraction. Besides, like blob detection, we also predict\nthe scales of the central points, which is also a straightforward convolution.\nTherefore, in this paper, pedestrian and face detection is simplified as a\nstraightforward center and scale prediction task through convolutions. This\nway, the proposed method enjoys a box-free setting. Though structurally simple,\nit presents competitive accuracy on several challenging benchmarks, including\npedestrian detection and face detection. Furthermore, a cross-dataset\nevaluation is performed, demonstrating a superior generalization ability of the\nproposed method\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:14:57 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 14:42:58 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 13:32:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Wei", ""], ["Hasan", "Irtiza", ""], ["Liao", "Shengcai", ""]]}, {"id": "1904.02957", "submitter": "Alessio Tonioni", "authors": "Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano,\n  Thalaiyasingam Ajanthan and Philip H. S. Torr", "title": "Learning to Adapt for Stereo", "comments": "Accepted at CVPR2019. Code available at\n  https://github.com/CVLAB-Unibo/Learning2AdaptForStereo", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2019, pp. 9661-9670", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world applications of stereo depth estimation require models that are\nrobust to dynamic variations in the environment. Even though deep learning\nbased stereo methods are successful, they often fail to generalize to unseen\nvariations in the environment, making them less suitable for practical\napplications such as autonomous driving. In this work, we introduce a\n\"learning-to-adapt\" framework that enables deep stereo methods to continuously\nadapt to new target domains in an unsupervised manner. Specifically, our\napproach incorporates the adaptation procedure into the learning objective to\nobtain a base set of parameters that are better suited for unsupervised online\nadaptation. To further improve the quality of the adaptation, we learn a\nconfidence measure that effectively masks the errors introduced during the\nunsupervised adaptation. We evaluate our method on synthetic and real-world\nstereo datasets and our experiments evidence that learning-to-adapt is, indeed\nbeneficial for online adaptation on vastly different domains.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:37:27 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Tonioni", "Alessio", ""], ["Rahnama", "Oscar", ""], ["Joy", "Thomas", ""], ["Di Stefano", "Luigi", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1904.02969", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon,\n  Kwanghoon Sohn", "title": "Semantic Attribute Matching Networks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semantic attribute matching networks (SAM-Net) for jointly\nestablishing correspondences and transferring attributes across semantically\nsimilar images, which intelligently weaves the advantages of the two tasks\nwhile overcoming their limitations. SAM-Net accomplishes this through an\niterative process of establishing reliable correspondences by reducing the\nattribute discrepancy between the images and synthesizing attribute transferred\nimages using the learned correspondences. To learn the networks using weak\nsupervisions in the form of image pairs, we present a semantic attribute\nmatching loss based on the matching similarity between an attribute transferred\nsource feature and a warped target feature. With SAM-Net, the state-of-the-art\nperformance is attained on several benchmarks for semantic matching and\nattribute transfer.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 10:03:43 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Kim", "Seungryong", ""], ["Min", "Dongbo", ""], ["Jeong", "Somi", ""], ["Kim", "Sunok", ""], ["Jeon", "Sangryul", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1904.02998", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, Zhibo Chen", "title": "Relation-Aware Global Attention for Person Re-identification", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For person re-identification (re-id), attention mechanisms have become\nattractive as they aim at strengthening discriminative features and suppressing\nirrelevant ones, which matches well the key of re-id, i.e., discriminative\nfeature learning. Previous approaches typically learn attention using local\nconvolutions, ignoring the mining of knowledge from global structure patterns.\nIntuitively, the affinities among spatial positions/nodes in the feature map\nprovide clustering-like information and are helpful for inferring semantics and\nthus attention, especially for person images where the feasible human poses are\nconstrained. In this work, we propose an effective Relation-Aware Global\nAttention (RGA) module which captures the global structural information for\nbetter attention learning. Specifically, for each feature position, in order to\ncompactly grasp the structural information of global scope and local appearance\ninformation, we propose to stack the relations, i.e., its pairwise\ncorrelations/affinities with all the feature positions (e.g., in raster scan\norder), and the feature itself together to learn the attention with a shallow\nconvolutional model. Extensive ablation studies demonstrate that our RGA can\nsignificantly enhance the feature representation power and help achieve the\nstate-of-the-art performance on several popular benchmarks. The source code is\navailable at\nhttps://github.com/microsoft/Relation-Aware-Global-Attention-Networks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 11:31:37 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 20:19:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Jin", "Xin", ""], ["Chen", "Zhibo", ""]]}, {"id": "1904.03000", "submitter": "Yaser Souri", "authors": "Johann Sawatzky, Yaser Souri, Christian Grund, Juergen Gall", "title": "What Object Should I Use? - Task Driven Object Detection", "comments": "CVPR 2019. The first two authors contributed equally, ordered\n  alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans have to solve everyday tasks, they simply pick the objects that\nare most suitable. While the question which object should one use for a\nspecific task sounds trivial for humans, it is very difficult to answer for\nrobots or other autonomous systems. This issue, however, is not addressed by\ncurrent benchmarks for object detection that focus on detecting object\ncategories. We therefore introduce the COCO-Tasks dataset which comprises about\n40,000 images where the most suitable objects for 14 tasks have been annotated.\nWe furthermore propose an approach that detects the most suitable objects for a\ngiven task. The approach builds on a Gated Graph Neural Network to exploit the\nappearance of each object as well as the global context of all present objects\nin the scene. In our experiments, we show that the proposed approach\noutperforms other approaches that are evaluated on the dataset like\nclassification or ranking approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 11:36:07 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Sawatzky", "Johann", ""], ["Souri", "Yaser", ""], ["Grund", "Christian", ""], ["Gall", "Juergen", ""]]}, {"id": "1904.03011", "submitter": "Gjorgji Strezoski", "authors": "Gjorgji Strezoski, Nanne van Noord, Marcel Worring", "title": "Learning Task Relatedness in Multi-Task Learning for Images in Context", "comments": "To appear in ICMR 2019 (Oral + Lightning Talk + Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia applications often require concurrent solutions to multiple tasks.\nThese tasks hold clues to each-others solutions, however as these relations can\nbe complex this remains a rarely utilized property. When task relations are\nexplicitly defined based on domain knowledge multi-task learning (MTL) offers\nsuch concurrent solutions, while exploiting relatedness between multiple tasks\nperformed over the same dataset. In most cases however, this relatedness is not\nexplicitly defined and the domain expert knowledge that defines it is not\navailable. To address this issue, we introduce Selective Sharing, a method that\nlearns the inter-task relatedness from secondary latent features while the\nmodel trains. Using this insight, we can automatically group tasks and allow\nthem to share knowledge in a mutually beneficial way. We support our method\nwith experiments on 5 datasets in classification, regression, and ranking tasks\nand compare to strong baselines and state-of-the-art approaches showing a\nconsistent improvement in terms of accuracy and parameter counts. In addition,\nwe perform an activation region analysis showing how Selective Sharing affects\nthe learned representation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 12:08:16 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Strezoski", "Gjorgji", ""], ["van Noord", "Nanne", ""], ["Worring", "Marcel", ""]]}, {"id": "1904.03014", "submitter": "Yu Cheng", "authors": "Duo Wang, Yu Cheng, Mo Yu, Xiaoxiao Guo, Tao Zhang", "title": "A Hybrid Approach with Optimization and Metric-based Meta-Learner for\n  Few-Shot Learning", "comments": "Accepted to Neurocomputing journal, code will be released soon. arXiv\n  admin note: text overlap with arXiv:1901.09890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to learn classifiers for new classes with only a few\ntraining examples per class. Most existing few-shot learning approaches belong\nto either metric-based meta-learning or optimization-based meta-learning\ncategory, both of which have achieved successes in the simplified \"$k$-shot\n$N$-way\" image classification settings. Specifically, the optimization-based\napproaches train a meta-learner to predict the parameters of the task-specific\nclassifiers. The task-specific classifiers are required to be\nhomogeneous-structured to ease the parameter prediction, so the meta-learning\napproaches could only handle few-shot learning problems where the tasks share a\nuniform number of classes. The metric-based approaches learn one task-invariant\nmetric for all the tasks. Even though the metric-learning approaches allow\ndifferent numbers of classes, they require the tasks all coming from a similar\ndomain such that there exists a uniform metric that could work across tasks. In\nthis work, we propose a hybrid meta-learning model called Meta-Metric-Learner\nwhich combines the merits of both optimization- and metric-based approaches.\nOur meta-metric-learning approach consists of two components, a task-specific\nmetric-based learner as a base model, and a meta-learner that learns and\nspecifies the base model. Thus our model is able to handle flexible numbers of\nclasses as well as generate more generalized metrics for classification across\ntasks. We test our approach in the standard \"$k$-shot $N$-way\" few-shot\nlearning setting following previous works and a new realistic few-shot setting\nwith flexible class numbers in both single-source form and multi-source forms.\nExperiments show that our approach can obtain superior performance in all\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 07:31:34 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 01:35:22 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wang", "Duo", ""], ["Cheng", "Yu", ""], ["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Zhang", "Tao", ""]]}, {"id": "1904.03041", "submitter": "Richard McKinley", "authors": "Richard McKinley, Lorenz Grunder, Rik Wepfer, Fabian Aschwanden, Tim\n  Fischer, Christoph Friedli, Raphaela Muri, Christian Rummel, Rajeev Verma,\n  Christian Weisstanner, Mauricio Reyes, Anke Salmen, Andrew Chan, Roland\n  Wiest, Franca Wagner", "title": "Automatic detection of lesion load change in Multiple Sclerosis using\n  convolutional neural networks with segmentation confidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of new or enlarged white-matter lesions in multiple sclerosis\nis a vital task in the monitoring of patients undergoing disease-modifying\ntreatment for multiple sclerosis. However, the definition of 'new or enlarged'\nis not fixed, and it is known that lesion-counting is highly subjective, with\nhigh degree of inter- and intra-rater variability. Automated methods for lesion\nquantification hold the potential to make the detection of new and enlarged\nlesions consistent and repeatable. However, the majority of lesion segmentation\nalgorithms are not evaluated for their ability to separate progressive from\nstable patients, despite this being a pressing clinical use-case. In this paper\nwe show that change in volumetric measurements of lesion load alone is not a\ngood method for performing this separation, even for highly performing\nsegmentation methods. Instead, we propose a method for identifying lesion\nchanges of high certainty, and establish on a dataset of longitudinal multiple\nsclerosis cases that this method is able to separate progressive from stable\ntimepoints with a very high level of discrimination (AUC = 0.99), while changes\nin lesion volume are much less able to perform this separation (AUC = 0.71).\nValidation of the method on a second external dataset confirms that the method\nis able to generalize beyond the setting in which it was trained, achieving an\naccuracy of 83% in separating stable and progressive timepoints. Both lesion\nvolume and count have previously been shown to be strong predictors of disease\ncourse across a population. However, we demonstrate that for individual\npatients, changes in these measures are not an adequate means of establishing\nno evidence of disease activity. Meanwhile, directly detecting tissue which\nchanges, with high confidence, from non-lesion to lesion is a feasible\nmethodology for identifying radiologically active patients.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 12:59:58 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["McKinley", "Richard", ""], ["Grunder", "Lorenz", ""], ["Wepfer", "Rik", ""], ["Aschwanden", "Fabian", ""], ["Fischer", "Tim", ""], ["Friedli", "Christoph", ""], ["Muri", "Raphaela", ""], ["Rummel", "Christian", ""], ["Verma", "Rajeev", ""], ["Weisstanner", "Christian", ""], ["Reyes", "Mauricio", ""], ["Salmen", "Anke", ""], ["Chan", "Andrew", ""], ["Wiest", "Roland", ""], ["Wagner", "Franca", ""]]}, {"id": "1904.03075", "submitter": "Basel Alyafi", "authors": "Md. Kamrul Hasan, Basel Alyafi, Fakrul Islam Tushar", "title": "Comparative Analysis of Automatic Skin Lesion Segmentation with Two\n  Different Implementations", "comments": "4 pages, 4 figures, 4 tables, 4 sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion segmentation from the surrounding skin is the first task for\ndeveloping automatic Computer-Aided Diagnosis of skin cancer. Variant features\nof lesion like uneven distribution of color, irregular shape, border and\ntexture make this task challenging. The contribution of this paper is to\npresent and compare two different approaches to skin lesion segmentation. The\nfirst approach uses watershed, while the second approach uses mean-shift.\nPre-processing steps were performed in both approaches for removing hair and\ndark borders of microscopic images. The Evaluation of the proposed approaches\nwas performed using Jaccard Index (Intersection over Union or IoU). An\nadditional contribution of this paper is to present pipelines for performing\npre-processing and segmentation applying existing segmentation and\nmorphological algorithms which led to promising results. On average, the first\napproach showed better performance than the second one with average Jaccard\nIndex over 200 ISIC-2017 challenge images are 89.16% and 76.94% respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:05:24 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Alyafi", "Basel", ""], ["Tushar", "Fakrul Islam", ""]]}, {"id": "1904.03076", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Oliver Wasenm\\\"uller, Christian Unger, Didier\n  Stricker", "title": "SDC - Stacked Dilated Convolution: A Unified Descriptor Network for\n  Dense Matching Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense pixel matching is important for many computer vision tasks such as\ndisparity and flow estimation. We present a robust, unified descriptor network\nthat considers a large context region with high spatial variance. Our network\nhas a very large receptive field and avoids striding layers to maintain spatial\nresolution. These properties are achieved by creating a novel neural network\nlayer that consists of multiple, parallel, stacked dilated convolutions (SDC).\nSeveral of these layers are combined to form our SDC descriptor network. In our\nexperiments, we show that our SDC features outperform state-of-the-art feature\ndescriptors in terms of accuracy and robustness. In addition, we demonstrate\nthe superior performance of SDC in state-of-the-art stereo matching, optical\nflow and scene flow algorithms on several famous public benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:07:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Unger", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.03081", "submitter": "Thomas M\\\"ollenhoff", "authors": "Michael Moeller, Thomas M\\\"ollenhoff, Daniel Cremers", "title": "Controlling Neural Networks via Energy Dissipation", "comments": "Published as a conference paper at ICCV 2019, Seoul", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has shown a tremendous success in solving various computer\nvision problems with the help of deep learning techniques. Lately, many works\nhave demonstrated that learning-based approaches with suitable network\narchitectures even exhibit superior performance for the solution of (ill-posed)\nimage reconstruction problems such as deblurring, super-resolution, or medical\nimage reconstruction. The drawback of purely learning-based methods, however,\nis that they cannot provide provable guarantees for the trained network to\nfollow a given data formation process during inference. In this work we propose\nenergy dissipating networks that iteratively compute a descent direction with\nrespect to a given cost function or energy at the currently estimated\nreconstruction. Therefore, an adaptive step size rule such as a line-search,\nalong with a suitable number of iterations can guarantee the reconstruction to\nfollow a given data formation model encoded in the energy to arbitrary\nprecision, and hence control the model's behavior even during test time. We\nprove that under standard assumptions, descent using the direction predicted by\nthe network converges (linearly) to the global minimum of the energy. We\nillustrate the effectiveness of the proposed approach in experiments on single\nimage super resolution and computed tomography (CT) reconstruction, and further\nillustrate extensions to convex feasibility problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:13:55 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:54:46 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Moeller", "Michael", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Cremers", "Daniel", ""]]}, {"id": "1904.03086", "submitter": "Yen-Chi Cheng", "authors": "Chun-Hung Chao, Yen-Chi Cheng, Hsien-Tzu Cheng, Chi-Wen Huang,\n  Tsung-Ying Ho, Chen-Kan Tseng, Le Lu, Min Sun", "title": "Radiotherapy Target Contouring with Convolutional Gated Graph Neural\n  Network", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018. Version\n  2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomography medical imaging is essential in the clinical workflow of modern\ncancer radiotherapy. Radiation oncologists identify cancerous tissues, applying\ndelineation on treatment regions throughout all image slices. This kind of task\nis often formulated as a volumetric segmentation task by means of 3D\nconvolutional networks with considerable computational cost. Instead, inspired\nby the treating methodology of considering meaningful information across\nslices, we used Gated Graph Neural Network to frame this problem more\nefficiently. More specifically, we propose convolutional recurrent Gated Graph\nPropagator (GGP) to propagate high-level information through image slices, with\nlearnable adjacency weighted matrix. Furthermore, as physicians often\ninvestigate a few specific slices to refine their decision, we model this\nslice-wise interaction procedure to further improve our segmentation result.\nThis can be set by editing any slice effortlessly as updating predictions of\nother slices using GGP. To evaluate our method, we collect an Esophageal Cancer\nRadiotherapy Target Treatment Contouring dataset of 81 patients which includes\ntomography images with radiotherapy target. On this dataset, our convolutional\ngraph network produces state-of-the-art results and outperforms the baselines.\nWith the addition of interactive setting, performance is improved even further.\nOur method has the potential to be easily applied to diverse kinds of medical\ntasks with volumetric images. Incorporating both the ability to make a feasible\nprediction and to consider the human interactive input, the proposed method is\nsuitable for clinical scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:28:55 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Chao", "Chun-Hung", ""], ["Cheng", "Yen-Chi", ""], ["Cheng", "Hsien-Tzu", ""], ["Huang", "Chi-Wen", ""], ["Ho", "Tsung-Ying", ""], ["Tseng", "Chen-Kan", ""], ["Lu", "Le", ""], ["Sun", "Min", ""]]}, {"id": "1904.03110", "submitter": "Magdalini Paschali", "authors": "Magdalini Paschali, Stefano Gasperini, Abhijit Guha Roy, Michael Y.-S.\n  Fang, Nassir Navab", "title": "3DQ: Compact Quantized Neural Networks for Volumetric Whole Brain\n  Segmentation", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model architectures have been dramatically increasing in size, improving\nperformance at the cost of resource requirements. In this paper we propose 3DQ,\na ternary quantization method, applied for the first time to 3D Fully\nConvolutional Neural Networks (F-CNNs), enabling 16x model compression while\nmaintaining performance on par with full precision models. We extensively\nevaluate 3DQ on two datasets for the challenging task of whole brain\nsegmentation. Additionally, we showcase our method's ability to generalize on\ntwo common 3D architectures, namely 3D U-Net and V-Net. Outperforming a variety\nof baselines, the proposed method is capable of compressing large 3D models to\na few MBytes, alleviating the storage needs in space critical applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:09:07 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 08:01:14 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 11:07:17 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Paschali", "Magdalini", ""], ["Gasperini", "Stefano", ""], ["Roy", "Abhijit Guha", ""], ["Fang", "Michael Y. -S.", ""], ["Navab", "Nassir", ""]]}, {"id": "1904.03116", "submitter": "Yaser Souri", "authors": "Yaser Souri, Mohsen Fayyaz, Luca Minciullo, Gianpiero Francesca,\n  Juergen Gall", "title": "Fast Weakly Supervised Action Segmentation Using Mutual Consistency", "comments": "Accepted for publication at TPAMI (IEEE Transactions on Pattern\n  Analysis and Machine Intelligence) in 2021. First two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action segmentation is the task of predicting the actions for each frame of a\nvideo. As obtaining the full annotation of videos for action segmentation is\nexpensive, weakly supervised approaches that can learn only from transcripts\nare appealing. In this paper, we propose a novel end-to-end approach for weakly\nsupervised action segmentation based on a two-branch neural network. The two\nbranches of our network predict two redundant but different representations for\naction segmentation and we propose a novel mutual consistency (MuCon) loss that\nenforces the consistency of the two redundant representations. Using the MuCon\nloss together with a loss for transcript prediction, our proposed approach\nachieves the accuracy of state-of-the-art approaches while being $14$ times\nfaster to train and $20$ times faster during inference. The MuCon loss proves\nbeneficial even in the fully supervised setting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:19:35 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 11:55:42 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 16:44:09 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 21:31:13 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Souri", "Yaser", ""], ["Fayyaz", "Mohsen", ""], ["Minciullo", "Luca", ""], ["Francesca", "Gianpiero", ""], ["Gall", "Juergen", ""]]}, {"id": "1904.03124", "submitter": "Hannah Dee", "authors": "Jonathan Bell, Hannah M. Dee", "title": "Leaf segmentation through the classification of edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:39:00 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Bell", "Jonathan", ""], ["Dee", "Hannah M.", ""]]}, {"id": "1904.03127", "submitter": "Magdalini Paschali", "authors": "Magdalini Paschali, Muhammad Ferjad Naeem, Walter Simson, Katja\n  Steiger, Martin Mollenhauer, Nassir Navab", "title": "Deep Learning Under the Microscope: Improving the Interpretability of\n  Medical Imaging Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel interpretation method tailored to\nhistological Whole Slide Image (WSI) processing. A Deep Neural Network (DNN),\ninspired by Bag-of-Features models is equipped with a Multiple Instance\nLearning (MIL) branch and trained with weak supervision for WSI classification.\nMIL avoids label ambiguity and enhances our model's expressive power without\nguiding its attention. We utilize a fine-grained logit heatmap of the models\nactivations to interpret its decision-making process. The proposed method is\nquantitatively and qualitatively evaluated on two challenging histology\ndatasets, outperforming a variety of baselines. In addition, two expert\npathologists were consulted regarding the interpretability provided by our\nmethod and acknowledged its potential for integration into several clinical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:41:12 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 08:01:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Paschali", "Magdalini", ""], ["Naeem", "Muhammad Ferjad", ""], ["Simson", "Walter", ""], ["Steiger", "Katja", ""], ["Mollenhauer", "Martin", ""], ["Navab", "Nassir", ""]]}, {"id": "1904.03137", "submitter": "Oleksiy Ostapenko", "authors": "Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick J\\\"ahnichen,\n  Moin Nabi", "title": "Learning to Remember: A Synaptic Plasticity Driven Framework for\n  Continual Learning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models trained in the context of continual learning (CL) should be able to\nlearn from a stream of data over an undefined period of time. The main\nchallenges herein are: 1) maintaining old knowledge while simultaneously\nbenefiting from it when learning new tasks, and 2) guaranteeing model\nscalability with a growing amount of data to learn from. In order to tackle\nthese challenges, we introduce Dynamic Generative Memory (DGM) - a synaptic\nplasticity driven framework for continual learning. DGM relies on conditional\ngenerative adversarial networks with learnable connection plasticity realized\nwith neural masking. Specifically, we evaluate two variants of neural masking:\napplied to (i) layer activations and (ii) to connection weights directly.\nFurthermore, we propose a dynamic network expansion mechanism that ensures\nsufficient model capacity to accommodate for continually incoming tasks. The\namount of added capacity is determined dynamically from the learned binary\nmask. We evaluate DGM in the continual class-incremental setup on visual\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 16:02:15 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 07:23:22 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 16:28:09 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 14:46:07 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ostapenko", "Oleksiy", ""], ["Puscas", "Mihai", ""], ["Klein", "Tassilo", ""], ["J\u00e4hnichen", "Patrick", ""], ["Nabi", "Moin", ""]]}, {"id": "1904.03141", "submitter": "Te Qi", "authors": "Te Qi (1), Bayram Bayramli (1), Usman Ali (1), Qinchuan Zhang (1),\n  Hongtao Lu (1) ((1) Shanghai Jiao Tong University)", "title": "Spatial Shortcut Network for Human Pose Estimation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many computer vision problems, human pose estimation is a challenging\nproblem in that recognizing a body part requires not only information from\nlocal area but also from areas with large spatial distance. In order to\nspatially pass information, large convolutional kernels and deep layers have\nbeen normally used, introducing high computation cost and large parameter\nspace. Luckily for pose estimation, human body is geometrically structured in\nimages, enabling modeling of spatial dependency. In this paper, we propose a\nspatial shortcut network for pose estimation task, where information is easier\nto flow spatially. We evaluate our model with detailed analyses and present its\noutstanding performance with smaller structure.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 16:06:58 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Qi", "Te", "", "Shanghai Jiao Tong University"], ["Bayramli", "Bayram", "", "Shanghai Jiao Tong University"], ["Ali", "Usman", "", "Shanghai Jiao Tong University"], ["Zhang", "Qinchuan", "", "Shanghai Jiao Tong University"], ["Lu", "Hongtao", "", "Shanghai Jiao Tong University"]]}, {"id": "1904.03148", "submitter": "Huy Vo Van", "authors": "Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick\n  Perez, Jean Ponce", "title": "Unsupervised Image Matching and Object Discovery as Optimization", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with complete or partial supervision is powerful but relies on\never-growing human annotation efforts. As a way to mitigate this serious\nproblem, as well as to serve specific applications, unsupervised learning has\nemerged as an important field of research. In computer vision, unsupervised\nlearning comes in various guises. We focus here on the unsupervised discovery\nand matching of object categories among images in a collection, following the\nwork of Cho et al. 2015. We show that the original approach can be reformulated\nand solved as a proper optimization problem. Experiments on several benchmarks\nestablish the merit of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 16:29:44 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Vo", "Huy V.", ""], ["Bach", "Francis", ""], ["Cho", "Minsu", ""], ["Han", "Kai", ""], ["LeCun", "Yann", ""], ["Perez", "Patrick", ""], ["Ponce", "Jean", ""]]}, {"id": "1904.03167", "submitter": "Sergey Zakharov", "authors": "Roman Kaskman, Sergey Zakharov, Ivan Shugurov, Slobodan Ilic", "title": "HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D Objects", "comments": "ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the most important prerequisites for creating and evaluating 6D object\npose detectors are datasets with labeled 6D poses. With the advent of deep\nlearning, demand for such datasets is growing continuously. Despite the fact\nthat some of exist, they are scarce and typically have restricted setups, such\nas a single object per sequence, or they focus on specific object types, such\nas textureless industrial parts. Besides, two significant components are often\nignored: training using only available 3D models instead of real data and\nscalability, i.e. training one method to detect all objects rather than\ntraining one detector per object. Other challenges, such as occlusions,\nchanging light conditions and changes in object appearance, as well precisely\ndefined benchmarks are either not present or are scattered among different\ndatasets. In this paper we present a dataset for 6D pose estimation that covers\nthe above-mentioned challenges, mainly targeting training from 3D models (both\ntextured and textureless), scalability, occlusions, and changes in light\nconditions and object appearance. The dataset features 33 objects (17 toy, 8\nhousehold and 8 industry-relevant objects) over 13 scenes of various\ndifficulty. We also present a set of benchmarks to test various desired\ndetector properties, particularly focusing on scalability with respect to the\nnumber of objects and resistance to changing light conditions, occlusions and\nclutter. We also set a baseline for the presented benchmarks using a\nstate-of-the-art DPOD detector. Considering the difficulty of making such\ndatasets, we plan to release the code allowing other researchers to extend this\ndataset or make their own datasets in the future.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:16:09 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 18:49:11 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Kaskman", "Roman", ""], ["Zakharov", "Sergey", ""], ["Shugurov", "Ivan", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1904.03175", "submitter": "Moein Shakeri", "authors": "Moein Shakeri, Hong Zhang", "title": "Moving Object Detection under Discontinuous Change in Illumination Using\n  Tensor Low-Rank and Invariant Sparse Decomposition", "comments": "14 pages, 14 figures, including supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although low-rank and sparse decomposition based methods have been\nsuccessfully applied to the problem of moving object detection using structured\nsparsity-inducing norms, they are still vulnerable to significant illumination\nchanges that arise in certain applications. We are interested in moving object\ndetection in applications involving time-lapse image sequences for which\ncurrent methods mistakenly group moving objects and illumination changes into\nforeground. Our method relies on the multilinear (tensor) data low-rank and\nsparse decomposition framework to address the weaknesses of existing methods.\nThe key to our proposed method is to create first a set of prior maps that can\ncharacterize the changes in the image sequence due to illumination. We show\nthat they can be detected by a k-support norm. To deal with concurrent, two\ntypes of changes, we employ two regularization terms, one for detecting moving\nobjects and the other for accounting for illumination changes, in the tensor\nlow-rank and sparse decomposition formulation. Through comprehensive\nexperiments using challenging datasets, we show that our method demonstrates a\nremarkable ability to detect moving objects under discontinuous change in\nillumination, and outperforms the state-of-the-art solutions to this\nchallenging problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:49:37 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 01:10:09 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Shakeri", "Moein", ""], ["Zhang", "Hong", ""]]}, {"id": "1904.03181", "submitter": "Ankan Bansal", "authors": "Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama\n  Chellappa", "title": "Detecting Human-Object Interactions via Functional Generalization", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for detecting human-object interactions (HOIs) in\nimages, based on the idea that humans interact with functionally similar\nobjects in a similar manner. The proposed model is simple and efficiently uses\nthe data, visual features of the human, relative spatial orientation of the\nhuman and the object, and the knowledge that functionally similar objects take\npart in similar interactions with humans. We provide extensive experimental\nvalidation for our approach and demonstrate state-of-the-art results for HOI\ndetection. On the HICO-Det dataset our method achieves a gain of over 2.5%\nabsolute points in mean average precision (mAP) over state-of-the-art. We also\nshow that our approach leads to significant performance gains for zero-shot HOI\ndetection in the seen object setting. We further demonstrate that using a\ngeneric object detector, our model can generalize to interactions involving\npreviously unseen objects.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:58:54 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 03:47:43 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 06:28:11 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Bansal", "Ankan", ""], ["Rambhatla", "Sai Saketh", ""], ["Shrivastava", "Abhinav", ""], ["Chellappa", "Rama", ""]]}, {"id": "1904.03182", "submitter": "Valentin Peretroukhin", "authors": "Valentin Peretroukhin, Brandon Wagstaff, Matthew Giamou and Jonathan\n  Kelly", "title": "Probabilistic Regression of Rotations using Quaternion Averaging and a\n  Deep Multi-Headed Network", "comments": "A shortened version of this work appears in the Proceedings of the\n  IEEE Conference on Computer Vision and Pattern Recognition (CVPR'19) Workshop\n  on Uncertainty and Robustness in Deep Visual Learning, Long Beach,\n  California, USA, Jun. 16-20 2019, pp. 83-86", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimates of rotation are crucial to vision-based motion estimation\nin augmented reality and robotics. In this work, we present a method to extract\nprobabilistic estimates of rotation from deep regression models. First, we\nbuild on prior work and argue that a multi-headed network structure we name\nHydraNet provides better calibrated uncertainty estimates than methods that\nrely on stochastic forward passes. Second, we extend HydraNet to targets that\nbelong to the rotation group, SO(3), by regressing unit quaternions and using\nthe tools of rotation averaging and uncertainty injection onto the manifold to\nproduce three-dimensional covariances. Finally, we present results and analysis\non a synthetic dataset, learn consistent orientation estimates on the 7-Scenes\ndataset, and show how we can use our learned covariances to fuse deep estimates\nof relative orientation with classical stereo visual odometry to improve\nlocalization on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 19:39:09 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 16:27:56 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Peretroukhin", "Valentin", ""], ["Wagstaff", "Brandon", ""], ["Giamou", "Matthew", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1904.03189", "submitter": "Rameen Abdal", "authors": "Rameen Abdal, Yipeng Qin, Peter Wonka", "title": "Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?", "comments": "Accepted for oral presentation at ICCV 2019, \"For videos visit\n  https://youtu.be/RnTXLXw9o_I , https://youtu.be/zJoYY2eHAF0 and\n  https://youtu.be/bA893L-PjbI\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm to embed a given image into the latent\nspace of StyleGAN. This embedding enables semantic image editing operations\nthat can be applied to existing photographs. Taking the StyleGAN trained on the\nFFHQ dataset as an example, we show results for image morphing, style transfer,\nand expression transfer. Studying the results of the embedding algorithm\nprovides valuable insights into the structure of the StyleGAN latent space. We\npropose a set of experiments to test what class of images can be embedded, how\nthey are embedded, what latent space is suitable for embedding, and if the\nembedding is semantically meaningful.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:31:56 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 15:37:25 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Wonka", "Peter", ""]]}, {"id": "1904.03208", "submitter": "Qing Liu", "authors": "Qing Liu, Lingxi Xie, Huiyu Wang, Alan Yuille", "title": "Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image\n  Retrieval", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based image retrieval (SBIR) is widely recognized as an important\nvision problem which implies a wide range of real-world applications. Recently,\nresearch interests arise in solving this problem under the more realistic and\nchallenging setting of zero-shot learning. In this paper, we investigate this\nproblem from the viewpoint of domain adaptation which we show is critical in\nimproving feature embedding in the zero-shot scenario. Based on a framework\nwhich starts with a pre-trained model on ImageNet and fine-tunes it on the\ntraining set of SBIR benchmark, we advocate the importance of preserving\npreviously acquired knowledge, e.g., the rich discriminative features learned\nfrom ImageNet, to improve the model's transfer ability. For this purpose, we\ndesign an approach named Semantic-Aware Knowledge prEservation (SAKE), which\nfine-tunes the pre-trained model in an economical way and leverages semantic\ninformation, e.g., inter-class relationship, to achieve the goal of knowledge\npreservation. Zero-shot experiments on two extended SBIR datasets, TU-Berlin\nand Sketchy, verify the superior performance of our approach. Extensive\ndiagnostic experiments validate that knowledge preserved benefits SBIR in\nzero-shot settings, as a large fraction of the performance gain is from the\nmore properly structured feature embedding for photo images. Code is available\nat: https://github.com/qliu24/SAKE.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 18:04:40 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 03:33:21 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 18:14:45 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Liu", "Qing", ""], ["Xie", "Lingxi", ""], ["Wang", "Huiyu", ""], ["Yuille", "Alan", ""]]}, {"id": "1904.03215", "submitter": "Hermann Blum", "authors": "Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland Siegwart, Cesar\n  Cadena", "title": "The Fishyscapes Benchmark: Measuring Blind Spots in Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled impressive progress in the accuracy of semantic\nsegmentation. Yet, the ability to estimate uncertainty and detect failure is\nkey for safety-critical applications like autonomous driving. Existing\nuncertainty estimates have mostly been evaluated on simple tasks, and it is\nunclear whether these methods generalize to more complex scenarios. We present\nFishyscapes, the first public benchmark for uncertainty estimation in a\nreal-world task of semantic segmentation for urban driving. It evaluates\npixel-wise uncertainty estimates towards the detection of anomalous objects in\nfront of the vehicle. We~adapt state-of-the-art methods to recent semantic\nsegmentation models and compare approaches based on softmax confidence,\nBayesian learning, and embedding density. Our results show that anomaly\ndetection is far from solved even for ordinary situations, while our benchmark\nallows measuring advancements beyond the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 18:17:25 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 11:44:12 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 08:59:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Blum", "Hermann", ""], ["Sarlin", "Paul-Edouard", ""], ["Nieto", "Juan", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1904.03239", "submitter": "Weicheng Kuo", "authors": "Weicheng Kuo, Anelia Angelova, Jitendra Malik, Tsung-Yi Lin", "title": "ShapeMask: Learning to Segment Novel Objects by Refining Shape Priors", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 9207-9216", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation aims to detect and segment individual objects in a\nscene. Most existing methods rely on precise mask annotations of every\ncategory. However, it is difficult and costly to segment objects in novel\ncategories because a large number of mask annotations is required. We introduce\nShapeMask, which learns the intermediate concept of object shape to address the\nproblem of generalization in instance segmentation to novel categories.\nShapeMask starts with a bounding box detection and gradually refines it by\nfirst estimating the shape of the detected object through a collection of shape\npriors. Next, ShapeMask refines the coarse shape into an instance level mask by\nlearning instance embeddings. The shape priors provide a strong cue for\nobject-like prediction, and the instance embeddings model the instance specific\nappearance information. ShapeMask significantly outperforms the\nstate-of-the-art by 6.4 and 3.8 AP when learning across categories, and obtains\ncompetitive performance in the fully supervised setting. It is also robust to\ninaccurate detections, decreased model capacity, and small training data.\nMoreover, it runs efficiently with 150ms inference time and trains within 11\nhours on TPUs. With a larger backbone model, ShapeMask increases the gap with\nstate-of-the-art to 9.4 and 6.2 AP across categories. Code will be released.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:03:26 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kuo", "Weicheng", ""], ["Angelova", "Anelia", ""], ["Malik", "Jitendra", ""], ["Lin", "Tsung-Yi", ""]]}, {"id": "1904.03249", "submitter": "Miao Liu", "authors": "Miao Liu, Xin Chen, Yun Zhang, Yin Li, James M. Rehg", "title": "Attention Distillation for Learning Video Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of learning motion representations using\ndeep models for video recognition. To this end, we make use of attention\nmodules that learn to highlight regions in the video and aggregate features for\nrecognition. Specifically, we propose to leverage output attention maps as a\nvehicle to transfer the learned representation from a motion (flow) network to\nan RGB network. We systematically study the design of attention modules, and\ndevelop a novel method for attention distillation. Our method is evaluated on\nmajor action benchmarks, and consistently improves the performance of the\nbaseline RGB network by a significant margin. Moreover, we demonstrate that our\nattention maps can leverage motion cues in learning to identify the location of\nactions in video frames. We believe our method provides a step towards learning\nmotion-aware representations in deep models. Our project page is available at\nhttps://aptx4869lm.github.io/AttentionDistillation/\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:43:08 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 19:42:37 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Miao", ""], ["Chen", "Xin", ""], ["Zhang", "Yun", ""], ["Li", "Yin", ""], ["Rehg", "James M.", ""]]}, {"id": "1904.03259", "submitter": "Stephen Odaibo", "authors": "Stephen G. Odaibo", "title": "Is 'Unsupervised Learning' a Misconceived Term?", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is all of machine learning supervised to some degree? The field of machine\nlearning has traditionally been categorized pedagogically into\n$supervised~vs~unsupervised~learning$; where supervised learning has typically\nreferred to learning from labeled data, while unsupervised learning has\ntypically referred to learning from unlabeled data. In this paper, we assert\nthat all machine learning is in fact supervised to some degree, and that the\nscope of supervision is necessarily commensurate to the scope of learning\npotential. In particular, we argue that clustering algorithms such as k-means,\nand dimensionality reduction algorithms such as principal component analysis,\nvariational autoencoders, and deep belief networks are each internally\nsupervised by the data itself to learn their respective representations of its\nfeatures. Furthermore, these algorithms are not capable of external inference\nuntil their respective outputs (clusters, principal components, or\nrepresentation codes) have been identified and externally labeled in effect. As\nsuch, they do not suffice as examples of unsupervised learning. We propose that\nthe categorization `supervised vs unsupervised learning' be dispensed with, and\ninstead, learning algorithms be categorized as either\n$internally~or~externally~supervised$ (or both). We believe this change in\nperspective will yield new fundamental insights into the structure and\ncharacter of data and of learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:05:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Odaibo", "Stephen G.", ""]]}, {"id": "1904.03273", "submitter": "Nazanin Mehrasa", "authors": "Nazanin Mehrasa, Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid\n  Sigal, Greg Mori", "title": "A Variational Auto-Encoder Model for Stochastic Point Processes", "comments": "CVPR 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic generative model for action sequences. The\nmodel is termed the Action Point Process VAE (APP-VAE), a variational\nauto-encoder that can capture the distribution over the times and categories of\naction sequences. Modeling the variety of possible action sequences is a\nchallenge, which we show can be addressed via the APP-VAE's use of latent\nrepresentations and non-linear functions to parameterize distributions over\nwhich event is likely to occur next in a sequence and at what time. We\nempirically validate the efficacy of APP-VAE for modeling action sequences on\nthe MultiTHUMOS and Breakfast datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:49:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mehrasa", "Nazanin", ""], ["Jyothi", "Akash Abdu", ""], ["Durand", "Thibaut", ""], ["He", "Jiawei", ""], ["Sigal", "Leonid", ""], ["Mori", "Greg", ""]]}, {"id": "1904.03278", "submitter": "Naureen Mahmood", "authors": "Naureen Mahmood (Meshcapade GmbH), Nima Ghorbani (MPI for Intelligent\n  Systems), Nikolaus F. Troje (York University), Gerard Pons-Moll (MPI for\n  Informatics) and Michael J. Black (MPI for Intelligent Systems)", "title": "AMASS: Archive of Motion Capture as Surface Shapes", "comments": "12 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets are the cornerstone of recent advances in computer vision\nusing deep learning. In contrast, existing human motion capture (mocap)\ndatasets are small and the motions limited, hampering progress on learning\nmodels of human motion. While there are many different datasets available, they\neach use a different parameterization of the body, making it difficult to\nintegrate them into a single meta dataset. To address this, we introduce AMASS,\na large and varied database of human motion that unifies 15 different optical\nmarker-based mocap datasets by representing them within a common framework and\nparameterization. We achieve this using a new method, MoSh++, that converts\nmocap data into realistic 3D human meshes represented by a rigged body model;\nhere we use SMPL [doi:10.1145/2816795.2818013], which is widely used and\nprovides a standard skeletal representation as well as a fully rigged surface\nmesh. The method works for arbitrary marker sets, while recovering soft-tissue\ndynamics and realistic hand motion. We evaluate MoSh++ and tune its\nhyperparameters using a new dataset of 4D body scans that are jointly recorded\nwith marker-based mocap. The consistent representation of AMASS makes it\nreadily useful for animation, visualization, and generating training data for\ndeep learning. Our dataset is significantly richer than previous human motion\ncollections, having more than 40 hours of motion data, spanning over 300\nsubjects, more than 11,000 motions, and will be publicly available to the\nresearch community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:00:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mahmood", "Naureen", "", "Meshcapade GmbH"], ["Ghorbani", "Nima", "", "MPI for Intelligent\n  Systems"], ["Troje", "Nikolaus F.", "", "York University"], ["Pons-Moll", "Gerard", "", "MPI for\n  Informatics"], ["Black", "Michael J.", "", "MPI for Intelligent Systems"]]}, {"id": "1904.03280", "submitter": "Yihui He", "authors": "Jianren Wang, Yihui He, Xiaobo Wang, Xinjia Yu, Xia Chen", "title": "Prediction-Tracking-Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a prediction driven method for visual tracking and segmentation\nin videos. Instead of solely relying on matching with appearance cues for\ntracking, we build a predictive model which guides finding more accurate\ntracking regions efficiently. With the proposed prediction mechanism, we\nimprove the model robustness against distractions and occlusions during\ntracking. We demonstrate significant improvements over state-of-the-art methods\nnot only on visual tracking tasks (VOT 2016 and VOT 2018) but also on video\nsegmentation datasets (DAVIS 2016 and DAVIS 2017).\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:05:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Jianren", ""], ["He", "Yihui", ""], ["Wang", "Xiaobo", ""], ["Yu", "Xinjia", ""], ["Chen", "Xia", ""]]}, {"id": "1904.03282", "submitter": "Niluthpol Chowdhury Mithun", "authors": "Niluthpol Chowdhury Mithun, Sujoy Paul, Amit K. Roy-Chowdhury", "title": "Weakly Supervised Video Moment Retrieval From Text Queries", "comments": "Revised Table 1 in Page 6, A small bug related to rounding resulted\n  in a slightly improved score in the previous version. Our conclusion remains\n  the same after the update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been a few recent methods proposed in text to video moment\nretrieval using natural language queries, but requiring full supervision during\ntraining. However, acquiring a large number of training videos with temporal\nboundary annotations for each text description is extremely time-consuming and\noften not scalable. In order to cope with this issue, in this work, we\nintroduce the problem of learning from weak labels for the task of text to\nvideo moment retrieval. The weak nature of the supervision is because, during\ntraining, we only have access to the video-text pairs rather than the temporal\nextent of the video to which different text descriptions relate. We propose a\njoint visual-semantic embedding based framework that learns the notion of\nrelevant segments from video using only video-level sentence descriptions.\nSpecifically, our main idea is to utilize latent alignment between video frames\nand sentence descriptions using Text-Guided Attention (TGA). TGA is then used\nduring the test phase to retrieve relevant moments. Experiments on two\nbenchmark datasets demonstrate that our method achieves comparable performance\nto state-of-the-art fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:11:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 23:03:18 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Paul", "Sujoy", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1904.03285", "submitter": "Arijit Ray", "authors": "Arijit Ray, Yi Yao, Rakesh Kumar, Ajay Divakaran, Giedrius Burachas", "title": "Can You Explain That? Lucid Explanations Help Human-AI Collaborative\n  Image Retrieval", "comments": "2019 AAAI Conference on Human Computation and Crowdsourcing", "journal-ref": "2019 AAAI Conference on Human Computation and Crowdsourcing", "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there have been many proposals on making AI algorithms explainable, few\nhave attempted to evaluate the impact of AI-generated explanations on human\nperformance in conducting human-AI collaborative tasks. To bridge the gap, we\npropose a Twenty-Questions style collaborative image retrieval game,\nExplanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy\nof explanations (visual evidence or textual justification) in the context of\nVisual Question Answering (VQA). In our proposed ExAG, a human user needs to\nguess a secret image picked by the VQA agent by asking natural language\nquestions to it. We show that overall, when AI explains its answers, users\nsucceed more often in guessing the secret image correctly. Notably, a few\ncorrect explanations can readily improve human performance when VQA answers are\nmostly incorrect as compared to no-explanation games. Furthermore, we also show\nthat while explanations rated as \"helpful\" significantly improve human\nperformance, \"incorrect\" and \"unhelpful\" explanations can degrade performance\nas compared to no-explanation games. Our experiments, therefore, demonstrate\nthat ExAG is an effective means to evaluate the efficacy of AI-generated\nexplanations on a human-AI collaborative task.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:26:39 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 16:45:38 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 21:52:10 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 17:13:50 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ray", "Arijit", ""], ["Yao", "Yi", ""], ["Kumar", "Rakesh", ""], ["Divakaran", "Ajay", ""], ["Burachas", "Giedrius", ""]]}, {"id": "1904.03289", "submitter": "Ikhsanul Habibie", "authors": "Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Gerard Pons-Moll,\n  Christian Theobalt", "title": "In the Wild Human Pose Estimation Using Explicit 2D Features and\n  Intermediate 3D Representations", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network based approaches for monocular 3D human pose\nestimation usually require a large amount of training images with 3D pose\nannotations. While it is feasible to provide 2D joint annotations for large\ncorpora of in-the-wild images with humans, providing accurate 3D annotations to\nsuch in-the-wild corpora is hardly feasible in practice. Most existing 3D\nlabelled data sets are either synthetically created or feature in-studio\nimages. 3D pose estimation algorithms trained on such data often have limited\nability to generalize to real world scene diversity. We therefore propose a new\ndeep learning based method for monocular 3D human pose estimation that shows\nhigh accuracy and generalizes better to in-the-wild scenes. It has a network\narchitecture that comprises a new disentangled hidden space encoding of\nexplicit 2D and 3D features, and uses supervision by a new learned projection\nmodel from predicted 3D pose. Our algorithm can be jointly trained on image\ndata with 3D labels and image data with only 2D labels. It achieves\nstate-of-the-art accuracy on challenging in-the-wild data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:37:55 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Habibie", "Ikhsanul", ""], ["Xu", "Weipeng", ""], ["Mehta", "Dushyant", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "1904.03303", "submitter": "Francesco Pittaluga", "authors": "Francesco Pittaluga, Sanjeev J. Koppal, Sing Bing Kang, Sudipta N.\n  Sinha", "title": "Revealing Scenes by Inverting Structure from Motion Reconstructions", "comments": "10 pages, 8 figures, to be published in IEEE Conference on Computer\n  Vision and Pattern Recognition 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many 3D vision systems localize cameras within a scene using 3D point clouds.\nSuch point clouds are often obtained using structure from motion (SfM), after\nwhich the images are discarded to preserve privacy. In this paper, we show, for\nthe first time, that such point clouds retain enough information to reveal\nscene appearance and compromise privacy. We present a privacy attack that\nreconstructs color images of the scene from the point cloud. Our method is\nbased on a cascaded U-Net that takes as input, a 2D multichannel image of the\npoints rendered from a specific viewpoint containing point depth and optionally\ncolor and SIFT descriptors and outputs a color image of the scene from that\nviewpoint. Unlike previous feature inversion methods, we deal with highly\nsparse and irregular 2D point distributions and inputs where many point\nattributes are missing, namely keypoint orientation and scale, the descriptor\nimage source and the 3D point visibility. We evaluate our attack algorithm on\npublic datasets and analyze the significance of the point cloud attributes.\nFinally, we show that novel views can also be generated thereby enabling\ncompelling virtual tours of the underlying scene.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 22:03:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Pittaluga", "Francesco", ""], ["Koppal", "Sanjeev J.", ""], ["Kang", "Sing Bing", ""], ["Sinha", "Sudipta N.", ""]]}, {"id": "1904.03308", "submitter": "Sina Mokhtarzadeh Azar", "authors": "Sina Mokhtarzadeh Azar, Mina Ghadimi Atigh, Ahmad Nickabadi, Alexandre\n  Alahi", "title": "Convolutional Relational Machine for Group Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end deep Convolutional Neural Network called\nConvolutional Relational Machine (CRM) for recognizing group activities that\nutilizes the information in spatial relations between individual persons in\nimage or video. It learns to produce an intermediate spatial representation\n(activity map) based on individual and group activities. A multi-stage\nrefinement component is responsible for decreasing the incorrect predictions in\nthe activity map. Finally, an aggregation component uses the refined\ninformation to recognize group activities. Experimental results demonstrate the\nconstructive contribution of the information extracted and represented in the\nform of the activity map. CRM shows advantages over state-of-the-art models on\nVolleyball and Collective Activity datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 22:26:47 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Azar", "Sina Mokhtarzadeh", ""], ["Atigh", "Mina Ghadimi", ""], ["Nickabadi", "Ahmad", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1904.03326", "submitter": "Julius Surya Sumantri", "authors": "Julius Surya Sumantri, In Kyu Park", "title": "360 Panorama Synthesis from a Sparse Set of Images with Unknown Field of\n  View", "comments": "Presented in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  360 images represent scenes captured in all possible viewing directions and\nenable viewers to navigate freely around the scene thereby providing an\nimmersive experience. Conversely, conventional images represent scenes in a\nsingle viewing direction with a small or limited field of view (FOV). As a\nresult, only certain parts of the scenes are observed, and valuable information\nabout the surroundings is lost. In this paper, a learning-based approach that\nreconstructs the scene in 360 x 180 from a sparse set of conventional images\n(typically 4 images) is proposed. The proposed approach first estimates the FOV\nof input images relative to the panorama. The estimated FOV is then used as the\nprior for synthesizing a high-resolution 360 panoramic output. The proposed\nmethod overcomes the difficulty of learning-based approach in synthesizing high\nresolution images (up to 512$\\times$1024). Experimental results demonstrate\nthat the proposed method produces 360 panorama with reasonable quality. Results\nalso show that the proposed method outperforms the alternative method and can\nbe generalized for non-panoramic scenes and images captured by a smartphone\ncamera.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 01:00:58 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 04:48:45 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 04:14:35 GMT"}, {"version": "v4", "created": "Sun, 22 Dec 2019 13:07:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Sumantri", "Julius Surya", ""], ["Park", "In Kyu", ""]]}, {"id": "1904.03345", "submitter": "Long Zhao", "authors": "Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris N. Metaxas", "title": "Semantic Graph Convolutional Networks for 3D Human Pose Regression", "comments": "In CVPR 2019 (13 pages including supplementary material). The code\n  can be found at https://github.com/garyzhao/SemGCN", "journal-ref": null, "doi": "10.1109/CVPR.2019.00354", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning Graph Convolutional Networks\n(GCNs) for regression. Current architectures of GCNs are limited to the small\nreceptive field of convolution filters and shared transformation matrix for\neach node. To address these limitations, we propose Semantic Graph\nConvolutional Networks (SemGCN), a novel neural network architecture that\noperates on regression tasks with graph-structured data. SemGCN learns to\ncapture semantic information such as local and global node relationships, which\nis not explicitly represented in the graph. These semantic relationships can be\nlearned through end-to-end training from the ground truth without additional\nsupervision or hand-crafted rules. We further investigate applying SemGCN to 3D\nhuman pose regression. Our formulation is intuitive and sufficient since both\n2D and 3D human poses can be represented as a structured graph encoding the\nrelationships between joints in the skeleton of a human body. We carry out\ncomprehensive studies to validate our method. The results prove that SemGCN\noutperforms state of the art while using 90% fewer parameters.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 02:52:02 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 01:15:49 GMT"}, {"version": "v3", "created": "Sun, 8 Mar 2020 21:56:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhao", "Long", ""], ["Peng", "Xi", ""], ["Tian", "Yu", ""], ["Kapadia", "Mubbasir", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1904.03349", "submitter": "Zhen Zhu", "authors": "Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei Wang, Xiang Bai", "title": "Progressive Pose Attention Transfer for Person Image Generation", "comments": "To appear in CVPR 2019, oral presentation (21 pages, 15 figures\n  including the supplementary materials)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a new generative adversarial network for pose transfer,\ni.e., transferring the pose of a given person to a target pose. The generator\nof the network comprises a sequence of Pose-Attentional Transfer Blocks that\neach transfers certain regions it attends to, generating the person image\nprogressively. Compared with those in previous works, our generated person\nimages possess better appearance consistency and shape consistency with the\ninput images, thus significantly more realistic-looking. The efficacy and\nefficiency of the proposed network are validated both qualitatively and\nquantitatively on Market-1501 and DeepFashion. Furthermore, the proposed\narchitecture can generate training images for person re-identification,\nalleviating data insufficiency. Codes and models are available at:\nhttps://github.com/tengteng95/Pose-Transfer.git.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 03:10:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 01:41:54 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 06:45:54 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhu", "Zhen", ""], ["Huang", "Tengteng", ""], ["Shi", "Baoguang", ""], ["Yu", "Miao", ""], ["Wang", "Bofei", ""], ["Bai", "Xiang", ""]]}, {"id": "1904.03355", "submitter": "Chen Chen", "authors": "Chen Chen, Xiaopeng Liu, Meng Ding, Junfeng Zheng, Jiangyun Li", "title": "3D Dilated Multi-Fiber Network for Real-time Brain Tumor Segmentation in\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumor segmentation plays a pivotal role in medical image processing. In\nthis work, we aim to segment brain MRI volumes. 3D convolution neural networks\n(CNN) such as 3D U-Net and V-Net employing 3D convolutions to capture the\ncorrelation between adjacent slices have achieved impressive segmentation\nresults. However, these 3D CNN architectures come with high computational\noverheads due to multiple layers of 3D convolutions, which may make these\nmodels prohibitive for practical large-scale applications. To this end, we\npropose a highly efficient 3D CNN to achieve real-time dense volumetric\nsegmentation. The network leverages the 3D multi-fiber unit which consists of\nan ensemble of lightweight 3D convolutional networks to significantly reduce\nthe computational cost. Moreover, 3D dilated convolutions are used to build\nmulti-scale feature representations. Extensive experimental results on the\nBraTS-2018 challenge dataset show that the proposed architecture greatly\nreduces computation cost while maintaining high accuracy for brain tumor\nsegmentation. The source code can be found at\nhttps://github.com/China-LiuXiaopeng/BraTS-DMFNet\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 03:55:42 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 08:39:10 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 01:06:06 GMT"}, {"version": "v4", "created": "Sat, 24 Aug 2019 16:54:24 GMT"}, {"version": "v5", "created": "Sat, 21 Sep 2019 02:19:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chen", "Chen", ""], ["Liu", "Xiaopeng", ""], ["Ding", "Meng", ""], ["Zheng", "Junfeng", ""], ["Li", "Jiangyun", ""]]}, {"id": "1904.03358", "submitter": "Wanhua Li", "authors": "Wanhua Li, Jiwen Lu, Jianjiang Feng, Chunjing Xu, Jie Zhou, Qi Tian", "title": "BridgeNet: A Continuity-Aware Probabilistic Network for Age Estimation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation is an important yet very challenging problem in computer\nvision. Existing methods for age estimation usually apply a divide-and-conquer\nstrategy to deal with heterogeneous data caused by the non-stationary aging\nprocess. However, the facial aging process is also a continuous process, and\nthe continuity relationship between different components has not been\neffectively exploited. In this paper, we propose BridgeNet for age estimation,\nwhich aims to mine the continuous relation between age labels effectively. The\nproposed BridgeNet consists of local regressors and gating networks. Local\nregressors partition the data space into multiple overlapping subspaces to\ntackle heterogeneous data and gating networks learn continuity aware weights\nfor the results of local regressors by employing the proposed bridge-tree\nstructure, which introduces bridge connections into tree models to enforce the\nsimilarity between neighbor nodes. Moreover, these two components of BridgeNet\ncan be jointly learned in an end-to-end way. We show experimental results on\nthe MORPH II, FG-NET and Chalearn LAP 2015 datasets and find that BridgeNet\noutperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 04:21:23 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Wanhua", ""], ["Lu", "Jiwen", ""], ["Feng", "Jianjiang", ""], ["Xu", "Chunjing", ""], ["Zhou", "Jie", ""], ["Tian", "Qi", ""]]}, {"id": "1904.03373", "submitter": "Muming Zhao", "authors": "Muming Zhao, Jian Zhang, Chongyang Zhang, Wenjun Zhang", "title": "Towards Locally Consistent Object Counting with Constrained Multi-stage\n  Convolutional Neural Networks", "comments": "Accepted by ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-density object counting in surveillance scenes is challenging mainly due\nto the drastic variation of object scales. The prevalence of deep learning has\nlargely boosted the object counting accuracy on several benchmark datasets.\nHowever, does the global counts really count? Armed with this question we dive\ninto the predicted density map whose summation over the whole regions reports\nthe global counts for more in-depth analysis. We observe that the object\ndensity map generated by most existing methods usually lacks of local\nconsistency, i.e., counting errors in local regions exist unexpectedly even\nthough the global count seems to well match with the ground-truth. Towards this\nproblem, in this paper we propose a constrained multi-stage Convolutional\nNeural Networks (CNNs) to jointly pursue locally consistent density map from\ntwo aspects. Different from most existing methods that mainly rely on the\nmulti-column architectures of plain CNNs, we exploit a stacking formulation of\nplain CNNs. Benefited from the internal multi-stage learning process, the\nfeature map could be repeatedly refined, allowing the density map to approach\nthe ground-truth density distribution. For further refinement of the density\nmap, we also propose a grid loss function. With finer local-region-based\nsupervisions, the underlying model is constrained to generate locally\nconsistent density values to minimize the training errors considering both the\nglobal and local counts accuracy. Experiments on two widely-tested object\ncounting benchmarks with overall significant results compared with\nstate-of-the-art methods demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 06:21:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhao", "Muming", ""], ["Zhang", "Jian", ""], ["Zhang", "Chongyang", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1904.03375", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu,\n  Mengdie Zhou, Qi Tian", "title": "Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling", "comments": "CVPR'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric deep learning is increasingly important thanks to the popularity of\n3D sensors. Inspired by the recent advances in NLP domain, the self-attention\ntransformer is introduced to consume the point clouds. We develop Point\nAttention Transformers (PATs), using a parameter-efficient Group Shuffle\nAttention (GSA) to replace the costly Multi-Head Attention. We demonstrate its\nability to process size-varying inputs, and prove its permutation equivariance.\nBesides, prior work uses heuristics dependence on the input data (e.g.,\nFurthest Point Sampling) to hierarchically select subsets of input points.\nThereby, we for the first time propose an end-to-end learnable and\ntask-agnostic sampling operation, named Gumbel Subset Sampling (GSS), to select\na representative subset of input points. Equipped with Gumbel-Softmax, it\nproduces a \"soft\" continuous subset in training phase, and a \"hard\" discrete\nsubset in test phase. By selecting representative subsets in a hierarchical\nfashion, the networks learn a stronger representation of the input sets with\nlower computation cost. Experiments on classification and segmentation\nbenchmarks show the effectiveness and efficiency of our methods. Furthermore,\nwe propose a novel application, to process event camera stream as point clouds,\nand achieve a state-of-the-art performance on DVS128 Gesture Dataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 06:25:41 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yang", "Jiancheng", ""], ["Zhang", "Qiang", ""], ["Ni", "Bingbing", ""], ["Li", "Linguo", ""], ["Liu", "Jinxian", ""], ["Zhou", "Mengdie", ""], ["Tian", "Qi", ""]]}, {"id": "1904.03377", "submitter": "Jinjin Gu", "authors": "Jinjin Gu, Hannan Lu, Wangmeng Zuo, Chao Dong", "title": "Blind Super-Resolution With Iterative Kernel Correction", "comments": "Accepted by CVPR 2019. The Table 1 in the CVF camera ready version is\n  corrected in this pre-print version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have dominated super-resolution (SR) field due to\ntheir remarkable performance in terms of effectiveness and efficiency. Most of\nthese methods assume that the blur kernel during downsampling is\npredefined/known (e.g., bicubic). However, the blur kernels involved in real\napplications are complicated and unknown, resulting in severe performance drop\nfor the advanced SR methods. In this paper, we propose an Iterative Kernel\nCorrection (IKC) method for blur kernel estimation in blind SR problem, where\nthe blur kernels are unknown. We draw the observation that kernel mismatch\ncould bring regular artifacts (either over-sharpening or over-smoothing), which\ncan be applied to correct inaccurate blur kernels. Thus we introduce an\niterative correction scheme -- IKC that achieves better results than direct\nkernel estimation. We further propose an effective SR network architecture\nusing spatial feature transform (SFT) layers to handle multiple blur kernels,\nnamed SFTMD. Extensive experiments on synthetic and real-world images show that\nthe proposed IKC method with SFTMD can provide visually favorable SR results\nand the state-of-the-art performance in blind SR problem.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 06:51:23 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 09:08:31 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gu", "Jinjin", ""], ["Lu", "Hannan", ""], ["Zuo", "Wangmeng", ""], ["Dong", "Chao", ""]]}, {"id": "1904.03378", "submitter": "Chang Chen", "authors": "Chang Chen, Zhiwei Xiong, Xinmei Tian, Zheng-Jun Zha, Feng Wu", "title": "Camera Lens Super-Resolution", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for single image super-resolution (SR) are typically\nevaluated with synthetic degradation models such as bicubic or Gaussian\ndownsampling. In this paper, we investigate SR from the perspective of camera\nlenses, named as CameraSR, which aims to alleviate the intrinsic tradeoff\nbetween resolution (R) and field-of-view (V) in realistic imaging systems.\nSpecifically, we view the R-V degradation as a latent model in the SR process\nand learn to reverse it with realistic low- and high-resolution image pairs. To\nobtain the paired images, we propose two novel data acquisition strategies for\ntwo representative imaging systems (i.e., DSLR and smartphone cameras),\nrespectively. Based on the obtained City100 dataset, we quantitatively analyze\nthe performance of commonly-used synthetic degradation models, and demonstrate\nthe superiority of CameraSR as a practical solution to boost the performance of\nexisting SR methods. Moreover, CameraSR can be readily generalized to different\ncontent and devices, which serves as an advanced digital zoom tool in realistic\nimaging systems. Codes and datasets are available at\nhttps://github.com/ngchc/CameraSR.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 07:04:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chen", "Chang", ""], ["Xiong", "Zhiwei", ""], ["Tian", "Xinmei", ""], ["Zha", "Zheng-Jun", ""], ["Wu", "Feng", ""]]}, {"id": "1904.03379", "submitter": "Sijie Song", "authors": "Sijie Song, Wei Zhang, Jiaying Liu, Tao Mei", "title": "Unsupervised Person Image Generation with Semantic Parsing\n  Transformation", "comments": "Accepted to CVPR 2019 (Oral). Our project is available at\n  https://github.com/SijieSong/person_generation_spt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address unsupervised pose-guided person image generation,\nwhich is known challenging due to non-rigid deformation. Unlike previous\nmethods learning a rock-hard direct mapping between human bodies, we propose a\nnew pathway to decompose the hard mapping into two more accessible subtasks,\nnamely, semantic parsing transformation and appearance generation. Firstly, a\nsemantic generative network is proposed to transform between semantic parsing\nmaps, in order to simplify the non-rigid deformation learning. Secondly, an\nappearance generative network learns to synthesize semantic-aware textures.\nThirdly, we demonstrate that training our framework in an end-to-end manner\nfurther refines the semantic maps and final results accordingly. Our method is\ngeneralizable to other semantic-aware person image generation tasks, eg,\nclothing texture transfer and controlled image manipulation. Experimental\nresults demonstrate the superiority of our method on DeepFashion and\nMarket-1501 datasets, especially in keeping the clothing attributes and better\nbody shapes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 07:19:03 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 13:12:53 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Song", "Sijie", ""], ["Zhang", "Wei", ""], ["Liu", "Jiaying", ""], ["Mei", "Tao", ""]]}, {"id": "1904.03380", "submitter": "Junjie Hu", "authors": "Junjie Hu, Yan Zhang, Takayuki Okatani", "title": "Visualization of Convolutional Neural Networks for Monocular Depth\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have shown great success on\nthe task of monocular depth estimation. A fundamental yet unanswered question\nis: how CNNs can infer depth from a single image. Toward answering this\nquestion, we consider visualization of inference of a CNN by identifying\nrelevant pixels of an input image to depth estimation. We formulate it as an\noptimization problem of identifying the smallest number of image pixels from\nwhich the CNN can estimate a depth map with the minimum difference from the\nestimate from the entire image. To cope with a difficulty with optimization\nthrough a deep CNN, we propose to use another network to predict those relevant\nimage pixels in a forward computation. In our experiments, we first show the\neffectiveness of this approach, and then apply it to different depth estimation\nnetworks on indoor and outdoor scene datasets. The results provide several\nfindings that help exploration of the above question.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 07:29:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hu", "Junjie", ""], ["Zhang", "Yan", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1904.03385", "submitter": "Jian Wang", "authors": "Jian Wang and Yunshan Zhong and Yachun Li and Chi Zhang and Yichen Wei", "title": "Re-Identification Supervised Texture Generation", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of 3D human body pose and shape from a single image has been\nextensively studied in recent years. However, the texture generation problem\nhas not been fully discussed. In this paper, we propose an end-to-end learning\nstrategy to generate textures of human bodies under the supervision of person\nre-identification. We render the synthetic images with textures extracted from\nthe inputs and maximize the similarity between the rendered and input images by\nusing the re-identification network as the perceptual metrics. Experiment\nresults on pedestrian images show that our model can generate the texture from\na single image and demonstrate that our textures are of higher quality than\nthose generated by other available methods. Furthermore, we extend the\napplication scope to other categories and explore the possible utilization of\nour generated textures.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 08:10:33 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Jian", ""], ["Zhong", "Yunshan", ""], ["Li", "Yachun", ""], ["Zhang", "Chi", ""], ["Wei", "Yichen", ""]]}, {"id": "1904.03391", "submitter": "Hazrat Ali", "authors": "Sulaiman Khan, Hazrat Ali, Zahid Ullah, Nasru Minallah, Shahid\n  Maqsood, Abdul Hafeez", "title": "KNN and ANN-based Recognition of Handwritten Pashto Letters using Zoning\n  Features", "comments": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications,", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), 9(10), June 2018", "doi": "10.14569/IJACSA.2018.091070", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a recognition system for handwritten Pashto letters.\nHowever, handwritten character recognition is a challenging task. These letters\nnot only differ in shape and style but also vary among individuals. The\nrecognition becomes further daunting due to the lack of standard datasets for\ninscribed Pashto letters. In this work, we have designed a database of moderate\nsize, which encompasses a total of 4488 images, stemming from 102\ndistinguishing samples for each of the 44 letters in Pashto. The recognition\nframework uses zoning feature extractor followed by K-Nearest Neighbour (KNN)\nand Neural Network (NN) classifiers for classifying individual letter. Based on\nthe evaluation of the proposed system, an overall classification accuracy of\napproximately 70.05% is achieved by using KNN while 72% is achieved by using\nNN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:10:55 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 15:49:26 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Khan", "Sulaiman", ""], ["Ali", "Hazrat", ""], ["Ullah", "Zahid", ""], ["Minallah", "Nasru", ""], ["Maqsood", "Shahid", ""], ["Hafeez", "Abdul", ""]]}, {"id": "1904.03392", "submitter": "Cai Shaofeng", "authors": "Shaofeng Cai, Yao Shu, Gang Chen, Beng Chin Ooi, Wei Wang, Meihui\n  Zhang", "title": "Effective and Efficient Dropout for Deep Convolutional Neural Networks", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural networks (CNNs) based applications have become\nubiquitous, where proper regularization is greatly needed. To prevent large\nneural network models from overfitting, dropout has been widely used as an\nefficient regularization technique in practice. However, many recent works show\nthat the standard dropout is ineffective or even detrimental to the training of\nCNNs. In this paper, we revisit this issue and examine various dropout variants\nin an attempt to improve existing dropout-based regularization techniques for\nCNNs. We attribute the failure of standard dropout to the conflict between the\nstochasticity of dropout and its following Batch Normalization (BN), and\npropose to reduce the conflict by placing dropout operations right before the\nconvolutional operation instead of BN, or totally address this issue by\nreplacing BN with Group Normalization (GN). We further introduce a structurally\nmore suited dropout variant Drop-Conv2d, which provides more efficient and\neffective regularization for deep CNNs. These dropout variants can be readily\nintegrated into the building blocks of CNNs and implemented in existing deep\nlearning platforms. Extensive experiments on benchmark datasets including\nCIFAR, SVHN and ImageNet are conducted to compare the existing building blocks\nand the proposed ones with dropout training. Results show that our building\nblocks improve over state-of-the-art CNNs significantly, which is mainly due to\nthe better regularization and implicit model ensemble effect.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:17:51 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 15:08:27 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 04:17:42 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2020 14:28:54 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 17:30:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Cai", "Shaofeng", ""], ["Shu", "Yao", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""]]}, {"id": "1904.03405", "submitter": "Jin Zeng", "authors": "Jin Zeng, Yanfeng Tong, Yunmu Huang, Qiong Yan, Wenxiu Sun, Jing Chen,\n  Yongtian Wang", "title": "Deep Surface Normal Estimation with Hierarchical RGB-D Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing availability of commodity RGB-D cameras has boosted the\napplications in the field of scene understanding. However, as a fundamental\nscene understanding task, surface normal estimation from RGB-D data lacks\nthorough investigation. In this paper, a hierarchical fusion network with\nadaptive feature re-weighting is proposed for surface normal estimation from a\nsingle RGB-D image. Specifically, the features from color image and depth are\nsuccessively integrated at multiple scales to ensure global surface smoothness\nwhile preserving visually salient details. Meanwhile, the depth features are\nre-weighted with a confidence map estimated from depth before merging into the\ncolor branch to avoid artifacts caused by input depth corruption. Additionally,\na hybrid multi-scale loss function is designed to learn accurate normal\nestimation given noisy ground-truth dataset. Extensive experimental results\nvalidate the effectiveness of the fusion strategy and the loss design,\noutperforming state-of-the-art normal estimation schemes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:54:38 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 07:12:19 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Zeng", "Jin", ""], ["Tong", "Yanfeng", ""], ["Huang", "Yunmu", ""], ["Yan", "Qiong", ""], ["Sun", "Wenxiu", ""], ["Chen", "Jing", ""], ["Wang", "Yongtian", ""]]}, {"id": "1904.03419", "submitter": "Enric Corona", "authors": "Enric Corona, Albert Pumarola, Guillem Aleny\\`a, Francesc\n  Moreno-Noguer", "title": "Context-aware Human Motion Prediction", "comments": "Accepted at CVPR20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of predicting human motion given a sequence of past observations\nis at the core of many applications in robotics and computer vision. Current\nstate-of-the-art formulate this problem as a sequence-to-sequence task, in\nwhich a historical of 3D skeletons feeds a Recurrent Neural Network (RNN) that\npredicts future movements, typically in the order of 1 to 2 seconds. However,\none aspect that has been obviated so far, is the fact that human motion is\ninherently driven by interactions with objects and/or other humans in the\nenvironment. In this paper, we explore this scenario using a novel\ncontext-aware motion prediction architecture. We use a semantic-graph model\nwhere the nodes parameterize the human and objects in the scene and the edges\ntheir mutual interactions. These interactions are iteratively learned through a\ngraph attention layer, fed with the past observations, which now include both\nobject and human body motions. Once this semantic graph is learned, we inject\nit to a standard RNN to predict future movements of the human/s and object/s.\nWe consider two variants of our architecture, either freezing the contextual\ninteractions in the future of updating them. A thorough evaluation in the\n\"Whole-Body Human Motion Database\" shows that in both cases, our context-aware\nnetworks clearly outperform baselines in which the context information is not\nconsidered.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 11:42:32 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 09:52:57 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 21:19:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Corona", "Enric", ""], ["Pumarola", "Albert", ""], ["Aleny\u00e0", "Guillem", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1904.03425", "submitter": "Lei Qi", "authors": "Lei Qi, Lei Wang, Jing Huo, Luping Zhou, Yinghuan Shi and Yang Gao", "title": "A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person\n  Re-identification", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised cross-domain person re-identification (Re-ID) faces two key\nissues. One is the data distribution discrepancy between source and target\ndomains, and the other is the lack of labelling information in target domain.\nThey are addressed in this paper from the perspective of representation\nlearning. For the first issue, we highlight the presence of camera-level\nsub-domains as a unique characteristic of person Re-ID, and develop\ncamera-aware domain adaptation to reduce the discrepancy not only between\nsource and target domains but also across these sub-domains. For the second\nissue, we exploit the temporal continuity in each camera of target domain to\ncreate discriminative information. This is implemented by dynamically\ngenerating online triplets within each batch, in order to maximally take\nadvantage of the steadily improved feature representation in training process.\nTogether, the above two methods give rise to a novel unsupervised deep domain\nadaptation framework for person Re-ID. Experiments and ablation studies on\nbenchmark datasets demonstrate its superiority and interesting properties.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 12:12:23 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 14:24:30 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Qi", "Lei", ""], ["Wang", "Lei", ""], ["Huo", "Jing", ""], ["Zhou", "Luping", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "1904.03436", "submitter": "Mang Ye", "authors": "Mang Ye, Xu Zhang, Pong C. Yuen, Shih-Fu Chang", "title": "Unsupervised Embedding Learning via Invariant and Spreading Instance\n  Feature", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the unsupervised embedding learning problem, which\nrequires an effective similarity measurement between samples in low-dimensional\nembedding space. Motivated by the positive concentrated and negative separated\nproperties observed from category-wise supervised learning, we propose to\nutilize the instance-wise supervision to approximate these properties, which\naims at learning data augmentation invariant and instance spread-out features.\nTo achieve this goal, we propose a novel instance based softmax embedding\nmethod, which directly optimizes the `real' instance features on top of the\nsoftmax function. It achieves significantly faster learning speed and higher\naccuracy than all existing methods. The proposed method performs well for both\nseen and unseen testing categories with cosine similarity. It also achieves\ncompetitive performance even without pre-trained network over samples from\nfine-grained categories.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 12:58:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ye", "Mang", ""], ["Zhang", "Xu", ""], ["Yuen", "Pong C.", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1904.03441", "submitter": "Lei Huang", "authors": "Lei Huang, Yi Zhou, Fan Zhu, Li Liu and Ling Shao", "title": "Iterative Normalization: Beyond Standardization towards Efficient\n  Whitening", "comments": "Accepted to CVPR 2019. The Code is available at\n  https://github.com/huangleiBuaa/IterNorm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is ubiquitously employed for accelerating neural\nnetwork training and improving the generalization capability by performing\nstandardization within mini-batches. Decorrelated Batch Normalization (DBN)\nfurther boosts the above effectiveness by whitening. However, DBN relies\nheavily on either a large batch size, or eigen-decomposition that suffers from\npoor efficiency on GPUs. We propose Iterative Normalization (IterNorm), which\nemploys Newton's iterations for much more efficient whitening, while\nsimultaneously avoiding the eigen-decomposition. Furthermore, we develop a\ncomprehensive study to show IterNorm has better trade-off between optimization\nand generalization, with theoretical and experimental support. To this end, we\nexclusively introduce Stochastic Normalization Disturbance (SND), which\nmeasures the inherent stochastic uncertainty of samples when applied to\nnormalization operations. With the support of SND, we provide natural\nexplanations to several phenomena from the perspective of optimization, e.g.,\nwhy group-wise whitening of DBN generally outperforms full-whitening and why\nthe accuracy of BN degenerates with reduced batch sizes. We demonstrate the\nconsistently improved performance of IterNorm with extensive experiments on\nCIFAR-10 and ImageNet over BN and DBN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 13:10:20 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Huang", "Lei", ""], ["Zhou", "Yi", ""], ["Zhu", "Fan", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""]]}, {"id": "1904.03451", "submitter": "Pau Riba", "authors": "Sounak Dey, Pau Riba, Anjan Dutta, Josep Llados and Yi-Zhe Song", "title": "Doodle to Search: Practical Zero-Shot Sketch-based Image Retrieval", "comments": "Oral paper in CVPR 2019", "journal-ref": "2019 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "doi": "10.1109/CVPR.2019.00228", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we investigate the problem of zero-shot sketch-based image\nretrieval (ZS-SBIR), where human sketches are used as queries to conduct\nretrieval of photos from unseen categories. We importantly advance prior arts\nby proposing a novel ZS-SBIR scenario that represents a firm step forward in\nits practical application. The new setting uniquely recognizes two important\nyet often neglected challenges of practical ZS-SBIR, (i) the large domain gap\nbetween amateur sketch and photo, and (ii) the necessity for moving towards\nlarge-scale retrieval. We first contribute to the community a novel ZS-SBIR\ndataset, QuickDraw-Extended, that consists of 330,000 sketches and 204,000\nphotos spanning across 110 categories. Highly abstract amateur human sketches\nare purposefully sourced to maximize the domain gap, instead of ones included\nin existing datasets that can often be semi-photorealistic. We then formulate a\nZS-SBIR framework to jointly model sketches and photos into a common embedding\nspace. A novel strategy to mine the mutual information among domains is\nspecifically engineered to alleviate the domain gap. External semantic\nknowledge is further embedded to aid semantic transfer. We show that, rather\nsurprisingly, retrieval performance significantly outperforms that of\nstate-of-the-art on existing datasets that can already be achieved using a\nreduced version of our model. We further demonstrate the superior performance\nof our full model by comparing with a number of alternatives on the newly\nproposed dataset. The new dataset, plus all training and testing code of our\nmodel, will be publicly released to facilitate future research\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 14:02:50 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 08:05:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Dey", "Sounak", ""], ["Riba", "Pau", ""], ["Dutta", "Anjan", ""], ["Llados", "Josep", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "1904.03461", "submitter": "Erik Wijmans", "authors": "Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia\n  Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra", "title": "Embodied Question Answering in Photorealistic Environments with Point\n  Cloud Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help bridge the gap between internet vision-style problems and the goal of\nvision for embodied perception we instantiate a large-scale navigation task --\nEmbodied Question Answering [1] in photo-realistic environments (Matterport\n3D). We thoroughly study navigation policies that utilize 3D point clouds, RGB\nimages, or their combination. Our analysis of these models reveals several key\nfindings. We find that two seemingly naive navigation baselines, forward-only\nand random, are strong navigators and challenging to outperform, due to the\nspecific choice of the evaluation setting presented by [1]. We find a novel\nloss-weighting scheme we call Inflection Weighting to be important when\ntraining recurrent models for navigation with behavior cloning and are able to\nout perform the baselines with this technique. We find that point clouds\nprovide a richer signal than RGB images for learning obstacle avoidance,\nmotivating the use (and continued study) of 3D deep learning models for\nembodied navigation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 14:50:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wijmans", "Erik", ""], ["Datta", "Samyak", ""], ["Maksymets", "Oleksandr", ""], ["Das", "Abhishek", ""], ["Gkioxari", "Georgia", ""], ["Lee", "Stefan", ""], ["Essa", "Irfan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.03468", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang, Yuchao Dai, Hongdong Li and Piotr Koniusz", "title": "Deep Stacked Hierarchical Multi-patch Network for Image Deblurring", "comments": "IEEE Conference on Computer Vision and Pattern Recognition 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep end-to-end learning methods have shown their superiority in\nremoving non-uniform motion blur, there still exist major challenges with the\ncurrent multi-scale and scale-recurrent models: 1) Deconvolution/upsampling\noperations in the coarse-to-fine scheme result in expensive runtime; 2) Simply\nincreasing the model depth with finer-scale levels cannot improve the quality\nof deblurring. To tackle the above problems, we present a deep hierarchical\nmulti-patch network inspired by Spatial Pyramid Matching to deal with blurry\nimages via a fine-to-coarse hierarchical representation. To deal with the\nperformance saturation w.r.t. depth, we propose a stacked version of our\nmulti-patch model. Our proposed basic multi-patch model achieves the\nstate-of-the-art performance on the GoPro dataset while enjoying a 40x faster\nruntime compared to current multi-scale methods. With 30ms to process an image\nat 1280x720 resolution, it is the first real-time deep motion deblurring model\nfor 720p images at 30fps. For stacked networks, significant improvements (over\n1.2dB) are achieved on the GoPro dataset by increasing the network depth.\nMoreover, by varying the depth of the stacked model, one can adapt the\nperformance and runtime of the same network for different application\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 15:15:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Hongguang", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1904.03472", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang, Jing Zhang and Piotr Koniusz", "title": "Few-Shot Learning via Saliency-guided Hallucination of Samples", "comments": "IEEE Conference on Computer Vision and Pattern Recognition 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning new concepts from a few of samples is a standard challenge in\ncomputer vision. The main directions to improve the learning ability of\nfew-shot training models include (i) a robust similarity learning and (ii)\ngenerating or hallucinating additional data from the limited existing samples.\nIn this paper, we follow the latter direction and present a novel data\nhallucination model. Currently, most datapoint generators contain a specialized\nnetwork (i.e., GAN) tasked with hallucinating new datapoints, thus requiring\nlarge numbers of annotated data for their training in the first place. In this\npaper, we propose a novel less-costly hallucination method for few-shot\nlearning which utilizes saliency maps. To this end, we employ a saliency\nnetwork to obtain the foregrounds and backgrounds of available image samples\nand feed the resulting maps into a two-stream network to hallucinate datapoints\ndirectly in the feature space from viable foreground-background combinations.\nTo the best of our knowledge, we are the first to leverage saliency maps for\nsuch a task and we demonstrate their usefulness in hallucinating additional\ndatapoints for few-shot learning. Our proposed network achieves the state of\nthe art on publicly available datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 15:33:39 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Hongguang", ""], ["Zhang", "Jing", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1904.03483", "submitter": "Huu Le", "authors": "Huu Le, Thanh-Toan Do, Tuan Hoang, Ngai-Man Cheung", "title": "SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud\n  Registration without Correspondences", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel randomized algorithm for robust point cloud\nregistration without correspondences. Most existing registration approaches\nrequire a set of putative correspondences obtained by extracting invariant\ndescriptors. However, such descriptors could become unreliable in noisy and\ncontaminated settings. In these settings, methods that directly handle input\npoint sets are preferable. Without correspondences, however, conventional\nrandomized techniques require a very large number of samples in order to reach\nsatisfactory solutions. In this paper, we propose a novel approach to address\nthis problem. In particular, our work enables the use of randomized methods for\npoint cloud registration without the need of putative correspondences. By\nconsidering point cloud alignment as a special instance of graph matching and\nemploying an efficient semi-definite relaxation, we propose a novel sampling\nmechanism, in which the size of the sampled subsets can be larger-than-minimal.\nOur tight relaxation scheme enables fast rejection of the outliers in the\nsampled sets, resulting in high-quality hypotheses. We conduct extensive\nexperiments to demonstrate that our approach outperforms other state-of-the-art\nmethods. Importantly, our proposed method serves as a generic framework which\ncan be extended to problems with known correspondences.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:07:09 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 09:50:49 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Le", "Huu", ""], ["Do", "Thanh-Toan", ""], ["Hoang", "Tuan", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1904.03485", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui\n  Shi, Thomas Huang", "title": "When AWGN-based Denoiser Meets Real Noises", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative learning-based image denoisers have achieved promising\nperformance on synthetic noises such as Additive White Gaussian Noise (AWGN).\nThe synthetic noises adopted in most previous work are pixel-independent, but\nreal noises are mostly spatially/channel-correlated and\nspatially/channel-variant. This domain gap yields unsatisfied performance on\nimages with real noises if the model is only trained with AWGN. In this paper,\nwe propose a novel approach to boost the performance of a real image denoiser\nwhich is trained only with synthetic pixel-independent noise data dominated by\nAWGN. First, we train a deep model that consists of a noise estimator and a\ndenoiser with mixed AWGN and Random Value Impulse Noise (RVIN). We then\ninvestigate Pixel-shuffle Down-sampling (PD) strategy to adapt the trained\nmodel to real noises. Extensive experiments demonstrate the effectiveness and\ngeneralization of the proposed approach. Notably, our method achieves\nstate-of-the-art performance on real sRGB images in the DND benchmark among\nmodels trained with synthetic noises. Codes are available at\nhttps://github.com/yzhouas/PD-Denoising-pytorch.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:16:49 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 03:21:47 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zhou", "Yuqian", ""], ["Jiao", "Jianbo", ""], ["Huang", "Haibin", ""], ["Wang", "Yang", ""], ["Wang", "Jue", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas", ""]]}, {"id": "1904.03486", "submitter": "Themos Stafylakis", "authors": "Themos Stafylakis, Johan Rohdin, Oldrich Plchot, Petr Mizera, Lukas\n  Burget", "title": "Self-supervised speaker embeddings", "comments": "Preprint. Submitted to Interspeech 2019. Updated results compared to\n  first version and minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to i-vectors, speaker embeddings such as x-vectors are incapable of\nleveraging unlabelled utterances, due to the classification loss over training\nspeakers. In this paper, we explore an alternative training strategy to enable\nthe use of unlabelled utterances in training. We propose to train speaker\nembedding extractors via reconstructing the frames of a target speech segment,\ngiven the inferred embedding of another speech segment of the same utterance.\nWe do this by attaching to the standard speaker embedding extractor a decoder\nnetwork, which we feed not merely with the speaker embedding, but also with the\nestimated phone sequence of the target frame sequence. The reconstruction loss\ncan be used either as a single objective, or be combined with the standard\nspeaker classification loss. In the latter case, it acts as a regularizer,\nencouraging generalizability to speakers unseen during training. In all cases,\nthe proposed architectures are trained from scratch and in an end-to-end\nfashion. We demonstrate the benefits from the proposed approach on VoxCeleb and\nSpeakers in the wild, and we report notable improvements over the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:32:12 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 13:27:29 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Stafylakis", "Themos", ""], ["Rohdin", "Johan", ""], ["Plchot", "Oldrich", ""], ["Mizera", "Petr", ""], ["Burget", "Lukas", ""]]}, {"id": "1904.03493", "submitter": "Xin Wang", "authors": "Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang\n  Wang", "title": "VATEX: A Large-Scale, High-Quality Multilingual Dataset for\n  Video-and-Language Research", "comments": "ICCV 2019 Oral. 17 pages, 14 figures, 6 tables (updated the VATEX\n  website link: vatex-challenge.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new large-scale multilingual video description dataset, VATEX,\nwhich contains over 41,250 videos and 825,000 captions in both English and\nChinese. Among the captions, there are over 206,000 English-Chinese parallel\ntranslation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is\nmultilingual, larger, linguistically complex, and more diverse in terms of both\nvideo and natural language descriptions. We also introduce two tasks for\nvideo-and-language research based on VATEX: (1) Multilingual Video Captioning,\naimed at describing a video in various languages with a compact unified\ncaptioning model, and (2) Video-guided Machine Translation, to translate a\nsource language description into the target language using the video\ninformation as additional spatiotemporal context. Extensive experiments on the\nVATEX dataset show that, first, the unified multilingual model can not only\nproduce both English and Chinese descriptions for a video more efficiently, but\nalso offer improved performance over the monolingual models. Furthermore, we\ndemonstrate that the spatiotemporal video context can be effectively utilized\nto align source and target languages and thus assist machine translation. In\nthe end, we discuss the potentials of using VATEX for other video-and-language\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:50:31 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 06:29:53 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 16:47:55 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wang", "Xin", ""], ["Wu", "Jiawei", ""], ["Chen", "Junkun", ""], ["Li", "Lei", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.03498", "submitter": "Sudhakar Kumawat", "authors": "Sudhakar Kumawat and Shanmuganathan Raman", "title": "LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional 3D Convolutional Neural Networks (CNNs) are computationally\nexpensive, memory intensive, prone to overfit, and most importantly, there is a\nneed to improve their feature learning capabilities. To address these issues,\nwe propose Rectified Local Phase Volume (ReLPV) block, an efficient alternative\nto the standard 3D convolutional layer. The ReLPV block extracts the phase in a\n3D local neighborhood (e.g., 3x3x3) of each position of the input map to obtain\nthe feature maps. The phase is extracted by computing 3D Short Term Fourier\nTransform (STFT) at multiple fixed low frequency points in the 3D local\nneighborhood of each position. These feature maps at different frequency points\nare then linearly combined after passing them through an activation function.\nThe ReLPV block provides significant parameter savings of at least, 3^3 to 13^3\ntimes compared to the standard 3D convolutional layer with the filter sizes\n3x3x3 to 13x13x13, respectively. We show that the feature learning capabilities\nof the ReLPV block are significantly better than the standard 3D convolutional\nlayer. Furthermore, it produces consistently better results across different 3D\ndata representations. We achieve state-of-the-art accuracy on the volumetric\nModelNet10 and ModelNet40 datasets while utilizing only 11% parameters of the\ncurrent state-of-the-art. We also improve the state-of-the-art on the UCF-101\nsplit-1 action recognition dataset by 5.68% (when trained from scratch) while\nusing only 15% of the parameters of the state-of-the-art. The project webpage\nis available at https://sites.google.com/view/lp-3dcnn/home.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 17:26:15 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kumawat", "Sudhakar", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1904.03501", "submitter": "Yuemeng Li", "authors": "Yuemeng Li, Yong Fan", "title": "DeepSEED: 3D Squeeze-and-Excitation Encoder-Decoder Convolutional Neural\n  Networks for Pulmonary Nodule Detection", "comments": "Accepted by 2020 IEEE International Symposium on Biomedical Imaging\n  (ISBI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary nodule detection plays an important role in lung cancer screening\nwith low-dose computed tomography (CT) scans. It remains challenging to build\nnodule detection deep learning models with good generalization performance due\nto unbalanced positive and negative samples. In order to overcome this problem\nand further improve state-of-the-art nodule detection methods, we develop a\nnovel deep 3D convolutional neural network with an Encoder-Decoder structure in\nconjunction with a region proposal network. Particularly, we utilize a\ndynamically scaled cross entropy loss to reduce the false positive rate and\ncombat the sample imbalance problem associated with nodule detection. We adopt\nthe squeeze-and-excitation structure to learn effective image features and\nutilize inter-dependency information of different feature maps. We have\nvalidated our method based on publicly available CT scans with manually\nlabelled ground-truth obtained from LIDC/IDRI dataset and its subset LUNA16\nwith thinner slices. Ablation studies and experimental results have\ndemonstrated that our method could outperform state-of-the-art nodule detection\nmethods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 17:40:47 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 19:33:54 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Li", "Yuemeng", ""], ["Fan", "Yong", ""]]}, {"id": "1904.03508", "submitter": "Hwann-Tzong Chen", "authors": "Chih-Yao Chiu, Hwann-Tzong Chen, Tyng-Luh Liu", "title": "C2S2: Cost-aware Channel Sparse Selection for Progressive Network\n  Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a channel-selection approach for simplifying deep neural\nnetworks. Specifically, we propose a new type of generic network layer, called\npruning layer, to seamlessly augment a given pre-trained model for compression.\nEach pruning layer, comprising $1 \\times 1$ depth-wise kernels, is represented\nwith a dual format: one is real-valued and the other is binary. The former\nenables a two-phase optimization process of network pruning to operate with an\nend-to-end differentiable network, and the latter yields the mask information\nfor channel selection. Our method progressively performs the pruning task\nlayer-wise, and achieves channel selection according to a sparsity criterion to\nfavor pruning more channels. We also develop a cost-aware mechanism to prevent\nthe compression from sacrificing the expected network performance. Our results\nfor compressing several benchmark deep networks on image classification and\nsemantic segmentation are comparable to those by state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 18:40:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chiu", "Chih-Yao", ""], ["Chen", "Hwann-Tzong", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1904.03516", "submitter": "Hwann-Tzong Chen", "authors": "Songhao Jia, Ding-Jie Chen, Hwann-Tzong Chen", "title": "Instance-Level Meta Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a normalization mechanism called Instance-Level Meta\nNormalization (ILM~Norm) to address a learning-to-normalize problem. ILM~Norm\nlearns to predict the normalization parameters via both the feature\nfeed-forward and the gradient back-propagation paths. ILM~Norm provides a meta\nnormalization mechanism and has several good properties. It can be easily\nplugged into existing instance-level normalization schemes such as Instance\nNormalization, Layer Normalization, or Group Normalization. ILM~Norm normalizes\neach instance individually and therefore maintains high performance even when\nsmall mini-batch is used. The experimental results show that ILM~Norm well\nadapts to different network architectures and tasks, and it consistently\nimproves the performance of the original models. The code is available at\nurl{https://github.com/Gasoonjia/ILM-Norm.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:37:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jia", "Songhao", ""], ["Chen", "Ding-Jie", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1904.03525", "submitter": "Jiankang Deng", "authors": "Yuxiang Zhou, Jiankang Deng, Irene Kotsia, Stefanos Zafeiriou", "title": "Dense 3D Face Decoding over 2500FPS: Joint Texture & Shape Convolutional\n  Mesh Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Morphable Models (3DMMs) are statistical models that represent facial\ntexture and shape variations using a set of linear bases and more particular\nPrincipal Component Analysis (PCA). 3DMMs were used as statistical priors for\nreconstructing 3D faces from images by solving non-linear least square\noptimization problems. Recently, 3DMMs were used as generative models for\ntraining non-linear mappings (\\ie, regressors) from image to the parameters of\nthe models via Deep Convolutional Neural Networks (DCNNs). Nevertheless, all of\nthe above methods use either fully connected layers or 2D convolutions on\nparametric unwrapped UV spaces leading to large networks with many parameters.\nIn this paper, we present the first, to the best of our knowledge, non-linear\n3DMMs by learning joint texture and shape auto-encoders using direct mesh\nconvolutions. We demonstrate how these auto-encoders can be used to train very\nlight-weight models that perform Coloured Mesh Decoding (CMD) in-the-wild at a\nspeed of over 2500 FPS.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 20:22:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhou", "Yuxiang", ""], ["Deng", "Jiankang", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1904.03534", "submitter": "Ali Sadeghian", "authors": "Ali Sadeghian, Deoksu Lim, Johan Karlsson, Jian Li", "title": "Automatic Target Recognition Using Discrimination Based on Optimal\n  Transport", "comments": null, "journal-ref": "2015 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of distances based on optimal transportation has recently shown\npromise for discrimination of power spectra. In particular, spectral estimation\nmethods based on l1 regularization as well as covariance based methods can be\nshown to be robust with respect to such distances. These transportation\ndistances provide a geometric framework where geodesics corresponds to smooth\ntransition of spectral mass, and have been useful for tracking. In this paper,\nwe investigate the use of these distances for automatic target recognition. We\nstudy the use of the Monge-Kantorovich distance compared to the standard l2\ndistance for classifying civilian vehicles based on SAR images. We use a\nversion of the Monge-Kantorovich distance that applies also for the case where\nthe spectra may have different total mass, and we formulate the optimization\nproblem as a minimum flow problem that can be computed using efficient\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:43:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sadeghian", "Ali", ""], ["Lim", "Deoksu", ""], ["Karlsson", "Johan", ""], ["Li", "Jian", ""]]}, {"id": "1904.03537", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Peter Ochs, Thomas Pock, Shoham Sabach", "title": "Convex-Concave Backtracking for Inertial Bregman Proximal Gradient\n  Algorithms in Non-Convex Optimization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backtracking line-search is an old yet powerful strategy for finding a better\nstep sizes to be used in proximal gradient algorithms. The main principle is to\nlocally find a simple convex upper bound of the objective function, which in\nturn controls the step size that is used. In case of inertial proximal gradient\nalgorithms, the situation becomes much more difficult and usually leads to very\nrestrictive rules on the extrapolation parameter. In this paper, we show that\nthe extrapolation parameter can be controlled by locally finding also a simple\nconcave lower bound of the objective function. This gives rise to a double\nconvex-concave backtracking procedure which allows for an adaptive choice of\nboth the step size and extrapolation parameters. We apply this procedure to the\nclass of inertial Bregman proximal gradient methods, and prove that any\nsequence generated by these algorithms converges globally to a critical point\nof the function at hand. Numerical experiments on a number of challenging\nnon-convex problems in image processing and machine learning were conducted and\nshow the power of combining inertial step and double backtracking strategy in\nachieving improved performances.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:59:02 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 11:16:22 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Ochs", "Peter", ""], ["Pock", "Thomas", ""], ["Sabach", "Shoham", ""]]}, {"id": "1904.03551", "submitter": "Kanji Tanaka", "authors": "Hiroki Tomoe, Tanaka Kanji", "title": "Long-Term Vehicle Localization by Recursive Knowledge Distillation", "comments": "5 pages, 3 figures, technical report. arXiv admin note: text overlap\n  with arXiv:1709.05470 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current state-of-the-art frameworks for cross-season visual place\nrecognition (CS-VPR) focus on domain adaptation (DA) to a single specific\nseason. From the viewpoint of long-term CS-VPR, such frameworks do not scale\nwell to sequential multiple domains (e.g., spring - summer - autumn - winter -\n... ). The goal of this study is to develop a novel long-term ensemble learning\n(LEL) framework that allows for a constant cost retraining in long-term\nsequential-multi-domain CS-VPR (SMD-VPR), which only requires the memorization\nof a small constant number of deep convolutional neural networks (CNNs) and can\nretrain the CNN ensemble of every season at a small constant time/space cost.\nWe frame our task as the multi-teacher multi-student knowledge distillation\n(MTMS-KD), which recursively compresses all the previous season's knowledge\ninto a current CNN ensemble. We further address the issue of\nteacher-student-assignment (TSA) to achieve a good\ngeneralization/specialization tradeoff. Experimental results on SMD-VPR tasks\nvalidate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:16:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Tomoe", "Hiroki", ""], ["Kanji", "Tanaka", ""]]}, {"id": "1904.03552", "submitter": "Kanji Tanaka", "authors": "Kojima Yusuke, Tanaka Kanji, Yang Naiming, Hirota Yuji", "title": "Scalable Change Retrieval Using Deep 3D Neural Codes", "comments": "5 pages, 1 figure, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel scalable framework for image change detection (ICD) from\nan on-board 3D imagery system. We argue that existing ICD systems are\nconstrained by the time required to align a given query image with individual\nreference image coordinates. We utilize an invariant coordinate system (ICS) to\nreplace the time-consuming image alignment with an offline pre-processing\nprocedure. Our key contribution is an extension of the traditional image\ncomparison-based ICD tasks to setups of the image retrieval (IR) task. We\nreplace each component of the 3D ICD system, i.e., (1) image modeling, (2)\nimage alignment, and (3) image differencing, with significantly efficient\nvariants from the bag-of-words (BoW) IR paradigm. Further, we train a deep 3D\nfeature extractor in an unsupervised manner using an unsupervised Siamese\nnetwork and automatically collected training data. We conducted experiments on\na challenging cross-season ICD task using a publicly available dataset and\nthereby validate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:26:32 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yusuke", "Kojima", ""], ["Kanji", "Tanaka", ""], ["Naiming", "Yang", ""], ["Yuji", "Hirota", ""]]}, {"id": "1904.03555", "submitter": "Kanji Tanaka", "authors": "Yamaguchi Kousuke, Tanaka Kanji, Sugimoto Takuma, Ide Rino, Takeda\n  Koji", "title": "Place-specific Background Modeling Using Recursive Autoencoders", "comments": "6 pages, 3 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image change detection (ICD) to detect changed objects in front of a vehicle\nwith respect to a place-specific background model using an on-board monocular\nvision system is a fundamental problem in intelligent vehicle (IV). From the\nperspective of recent large-scale IV applications, it can be impractical in\nterms of space/time efficiency to train place-specific background models for\nevery possible place. To address these issues, we introduce a new autoencoder\n(AE) based efficient ICD framework that combines the advantages of AE-based\nanomaly detection (AD) and AE-based image compression (IC). We propose a method\nthat uses AE reconstruction errors as a single unified measure for training a\nminimal set of place-specific AEs and maintains detection accuracy. We\nintroduce an efficient incremental recursive AE (rAE) training framework that\nrecursively summarizes a large collection of background images into the AE set.\nThe results of experiments on challenging cross-season ICD tasks validate the\nefficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:32:05 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kousuke", "Yamaguchi", ""], ["Kanji", "Tanaka", ""], ["Takuma", "Sugimoto", ""], ["Rino", "Ide", ""], ["Koji", "Takeda", ""]]}, {"id": "1904.03567", "submitter": "Chuanmin Jia", "authors": "Siwei Ma, Xinfeng Zhang, Chuanmin Jia, Zhenghui Zhao, Shiqi Wang,\n  Shanshe Wang", "title": "Image and Video Compression with Neural Networks: A Review", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (T-CSVT) as transactions paper", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  2019", "doi": "10.1109/TCSVT.2019.2910119", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the image and video coding technologies have advanced by\nleaps and bounds. However, due to the popularization of image and video\nacquisition devices, the growth rate of image and video data is far beyond the\nimprovement of the compression ratio. In particular, it has been widely\nrecognized that there are increasing challenges of pursuing further coding\nperformance improvement within the traditional hybrid coding framework. Deep\nconvolution neural network (CNN) which makes the neural network resurge in\nrecent years and has achieved great success in both artificial intelligent and\nsignal processing fields, also provides a novel and promising solution for\nimage and video compression. In this paper, we provide a systematic,\ncomprehensive and up-to-date review of neural network based image and video\ncompression techniques. The evolution and development of neural network based\ncompression methodologies are introduced for images and video respectively.\nMore specifically, the cutting-edge video coding techniques by leveraging deep\nlearning and HEVC framework are presented and discussed, which promote the\nstate-of-the-art video coding performance substantially. Moreover, the\nend-to-end image and video coding frameworks based on neural networks are also\nreviewed, revealing interesting explorations on next generation image and video\ncoding frameworks/standards. The most significant research works on the image\nand video coding related topics using neural networks are highlighted, and\nfuture trends are also envisioned. In particular, the joint compression on\nsemantic and visual information is tentatively explored to formulate high\nefficiency signal representation structure for both human vision and machine\nvision, which are the two dominant signal receptor in the age of artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 01:59:57 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 09:40:37 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ma", "Siwei", ""], ["Zhang", "Xinfeng", ""], ["Jia", "Chuanmin", ""], ["Zhao", "Zhenghui", ""], ["Wang", "Shiqi", ""], ["Wang", "Shanshe", ""]]}, {"id": "1904.03571", "submitter": "Sheng Yang", "authors": "Sheng Yang, Guosheng Lin, Qiuping Jiang, Weisi Lin", "title": "A Dilated Inception Network for Visual Saliency Prediction", "comments": "Accepted by IEEE Transactions on Multimedia. The source codes are\n  available at https://github.com/ysyscool/DINet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the advent of deep convolutional neural networks (DCNN), the\nimprovements in visual saliency prediction research are impressive. One\npossible direction to approach the next improvement is to fully characterize\nthe multi-scale saliency-influential factors with a computationally-friendly\nmodule in DCNN architectures. In this work, we proposed an end-to-end dilated\ninception network (DINet) for visual saliency prediction. It captures\nmulti-scale contextual features effectively with very limited extra parameters.\nInstead of utilizing parallel standard convolutions with different kernel sizes\nas the existing inception module, our proposed dilated inception module (DIM)\nuses parallel dilated convolutions with different dilation rates which can\nsignificantly reduce the computation load while enriching the diversity of\nreceptive fields in feature maps. Moreover, the performance of our saliency\nmodel is further improved by using a set of linear normalization-based\nprobability distribution distance metrics as loss functions. As such, we can\nformulate saliency prediction as a probability distribution prediction task for\nglobal saliency inference instead of a typical pixel-wise regression problem.\nExperimental results on several challenging saliency benchmark datasets\ndemonstrate that our DINet with proposed loss functions can achieve\nstate-of-the-art performance with shorter inference time.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 02:41:44 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 02:34:24 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yang", "Sheng", ""], ["Lin", "Guosheng", ""], ["Jiang", "Qiuping", ""], ["Lin", "Weisi", ""]]}, {"id": "1904.03579", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Keze Wang, Liang Lin", "title": "Adaptively Connected Neural Networks", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel adaptively connected neural network (ACNet) to\nimprove the traditional convolutional neural networks (CNNs) {in} two aspects.\nFirst, ACNet employs a flexible way to switch global and local inference in\nprocessing the internal feature representations by adaptively determining the\nconnection status among the feature nodes (e.g., pixels of the feature maps)\n\\footnote{In a computer vision domain, a node refers to a pixel of a feature\nmap{, while} in {the} graph domain, a node denotes a graph node.}. We can show\nthat existing CNNs, the classical multilayer perceptron (MLP), and the recently\nproposed non-local network (NLN) \\cite{nonlocalnn17} are all special cases of\nACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive\nexperimental analyses on {a variety of benchmarks (i.e.,} ImageNet-1k\nclassification, COCO 2017 detection and segmentation, CUHK03 person\nre-identification, CIFAR analysis, and Cora document categorization)\ndemonstrate that {ACNet} cannot only achieve state-of-the-art performance but\nalso overcome the limitation of the conventional MLP and CNN\n\\footnote{Corresponding author: Liang Lin (linliang@ieee.org)}. The code is\navailable at\n\\url{https://github.com/wanggrun/Adaptively-Connected-Neural-Networks}.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 04:01:27 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Guangrun", ""], ["Wang", "Keze", ""], ["Lin", "Liang", ""]]}, {"id": "1904.03580", "submitter": "Huaxi Huang", "authors": "Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, Jingsong Xu", "title": "Compare More Nuanced:Pairwise Alignment Bilinear Network For Few-shot\n  Fine-grained Learning", "comments": "ICME 2019 Oral", "journal-ref": null, "doi": "10.1109/ICME.2019.00024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition ability of human beings is developed in a progressive way.\nUsually, children learn to discriminate various objects from coarse to\nfine-grained with limited supervision. Inspired by this learning process, we\npropose a simple yet effective model for the Few-Shot Fine-Grained (FSFG)\nrecognition, which tries to tackle the challenging fine-grained recognition\ntask using meta-learning. The proposed method, named Pairwise Alignment\nBilinear Network (PABN), is an end-to-end deep neural network. Unlike\ntraditional deep bilinear networks for fine-grained classification, which adopt\nthe self-bilinear pooling to capture the subtle features of images, the\nproposed model uses a novel pairwise bilinear pooling to compare the nuanced\ndifferences between base images and query images for learning a deep distance\nmetric. In order to match base image features with query image features, we\ndesign feature alignment losses before the proposed pairwise bilinear pooling.\nExperiment results on four fine-grained classification datasets and one generic\nfew-shot dataset demonstrate that the proposed model outperforms both the\nstate-ofthe-art few-shot fine-grained and general few-shot methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 04:01:52 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 10:50:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Huang", "Huaxi", ""], ["Zhang", "Junjie", ""], ["Zhang", "Jian", ""], ["Wu", "Qiang", ""], ["Xu", "Jingsong", ""]]}, {"id": "1904.03582", "submitter": "Xiu-Shen Wei", "authors": "Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, Yanwen Guo", "title": "Multi-Label Image Recognition with Graph Convolutional Networks", "comments": "To appear at CVPR 2019 (Source codes have been released on\n  https://github.com/chenzhaomin123/ML_GCN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multi-label image recognition is to predict a set of object\nlabels that present in an image. As objects normally co-occur in an image, it\nis desirable to model the label dependencies to improve the recognition\nperformance. To capture and explore such important dependencies, we propose a\nmulti-label classification model based on Graph Convolutional Network (GCN).\nThe model builds a directed graph over the object labels, where each node\n(label) is represented by word embeddings of a label, and GCN is learned to map\nthis label graph into a set of inter-dependent object classifiers. These\nclassifiers are applied to the image descriptors extracted by another sub-net,\nenabling the whole network to be end-to-end trainable. Furthermore, we propose\na novel re-weighted scheme to create an effective label correlation matrix to\nguide information propagation among the nodes in GCN. Experiments on two\nmulti-label image recognition datasets show that our approach obviously\noutperforms other existing state-of-the-art methods. In addition, visualization\nanalyses reveal that the classifiers learned by our model maintain meaningful\nsemantic topology.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 04:36:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chen", "Zhao-Min", ""], ["Wei", "Xiu-Shen", ""], ["Wang", "Peng", ""], ["Guo", "Yanwen", ""]]}, {"id": "1904.03589", "submitter": "Zhiyuan Fang", "authors": "Zhiyuan Fang, Shu Kong, Charless Fowlkes, Yezhou Yang", "title": "Modularized Textual Grounding for Counterfactual Resilience", "comments": "13 pages, 12 figures, IEEE Conference on Computer Vision and Pattern\n  Recognition, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision applications often require a textual grounding module with\nprecision, interpretability, and resilience to counterfactual inputs/queries.\nTo achieve high grounding precision, current textual grounding methods heavily\nrely on large-scale training data with manual annotations at the pixel level.\nSuch annotations are expensive to obtain and thus severely narrow the model's\nscope of real-world applications. Moreover, most of these methods sacrifice\ninterpretability, generalizability, and they neglect the importance of being\nresilient to counterfactual inputs. To address these issues, we propose a\nvisual grounding system which is 1) end-to-end trainable in a weakly supervised\nfashion with only image-level annotations, and 2) counterfactually resilient\nowing to the modular design. Specifically, we decompose textual descriptions\ninto three levels: entity, semantic attribute, color information, and perform\ncompositional grounding progressively. We validate our model through a series\nof experiments and demonstrate its improvement over the state-of-the-art\nmethods. In particular, our model's performance not only surpasses other\nweakly/un-supervised methods and even approaches the strongly supervised ones,\nbut also is interpretable for decision making and performs much better in face\nof counterfactual classes than all the others.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 05:59:04 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 04:42:34 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Kong", "Shu", ""], ["Fowlkes", "Charless", ""], ["Yang", "Yezhou", ""]]}, {"id": "1904.03597", "submitter": "Jiangliu Wang", "authors": "Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu and\n  Wei Liu", "title": "Self-supervised Spatio-temporal Representation Learning for Videos by\n  Predicting Motion and Appearance Statistics", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of video representation learning without\nhuman-annotated labels. While previous efforts address the problem by designing\nnovel self-supervised tasks using video data, the learned features are merely\non a frame-by-frame basis, which are not applicable to many video analytic\ntasks where spatio-temporal features are prevailing. In this paper we propose a\nnovel self-supervised approach to learn spatio-temporal features for video\nrepresentation. Inspired by the success of two-stream approaches in video\nclassification, we propose to learn visual features by regressing both motion\nand appearance statistics along spatial and temporal dimensions, given only the\ninput video data. Specifically, we extract statistical concepts (fast-motion\nregion and the corresponding dominant direction, spatio-temporal color\ndiversity, dominant color, etc.) from simple patterns in both spatial and\ntemporal domains. Unlike prior puzzles that are even hard for humans to solve,\nthe proposed approach is consistent with human inherent visual habits and\ntherefore easy to answer. We conduct extensive experiments with C3D to validate\nthe effectiveness of our proposed approach. The experiments show that our\napproach can significantly improve the performance of C3D when applied to video\nclassification tasks. Code is available at\nhttps://github.com/laura-wang/video_repres_mas.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 07:27:37 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Jiangliu", ""], ["Jiao", "Jianbo", ""], ["Bao", "Linchao", ""], ["He", "Shengfeng", ""], ["Liu", "Yunhui", ""], ["Liu", "Wei", ""]]}, {"id": "1904.03608", "submitter": "Shaohui Liu", "authors": "Shaohui Liu, Xiao Zhang, Jianqiao Wangni, Jianbo Shi", "title": "Normalized Diversification", "comments": "12 pages, 9 figures, to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating diverse yet specific data is the goal of the generative\nadversarial network (GAN), but it suffers from the problem of mode collapse. We\nintroduce the concept of normalized diversity which force the model to preserve\nthe normalized pairwise distance between the sparse samples from a latent\nparametric distribution and their corresponding high-dimensional outputs. The\nnormalized diversification aims to unfold the manifold of unknown topology and\nnon-uniform distribution, which leads to safe interpolation between valid\nlatent variables. By alternating the maximization over the pairwise distance\nand updating the total distance (normalizer), we encourage the model to\nactively explore in the high-dimensional output space. We demonstrate that by\ncombining the normalized diversity loss and the adversarial loss, we generate\ndiverse data without suffering from mode collapsing. Experimental results show\nthat our method achieves consistent improvement on unsupervised image\ngeneration, conditional image generation and hand pose estimation over strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 09:00:35 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 21:19:59 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Liu", "Shaohui", ""], ["Zhang", "Xiao", ""], ["Wangni", "Jianqiao", ""], ["Shi", "Jianbo", ""]]}, {"id": "1904.03612", "submitter": "Fatemeh Shiri", "authors": "Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, Piotr Koniusz", "title": "Recovering Faces from Portraits with Auxiliary Facial Attributes", "comments": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": null, "doi": "10.1109/WACV.2019.00049", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a photorealistic face from an artistic portrait is a challenging\ntask since crucial facial details are often distorted or completely lost in\nartistic compositions. To handle this loss, we propose an Attribute-guided Face\nRecovery from Portraits (AFRP) that utilizes a Face Recovery Network (FRN) and\na Discriminative Network (DN). FRN consists of an autoencoder with residual\nblock-embedded skip-connections and incorporates facial attribute vectors into\nthe feature maps of input portraits at the bottleneck of the autoencoder. DN\nhas multiple convolutional and fully-connected layers, and its role is to\nenforce FRN to generate authentic face images with corresponding facial\nattributes dictated by the input attribute vectors. %Leveraging on the spatial\ntransformer networks, FRN automatically compensates for misalignments of\nportraits. % and generates aligned face images. For the preservation of\nidentities, we impose the recovered and ground-truth faces to share similar\nvisual features. Specifically, DN determines whether the recovered image looks\nlike a real face and checks if the facial attributes extracted from the\nrecovered image are consistent with given attributes. %Our method can recover\nhigh-quality photorealistic faces from unaligned portraits while preserving the\nidentity of the face images as well as it can reconstruct a photorealistic face\nimage with a desired set of attributes. Our method can recover photorealistic\nidentity-preserving faces with desired attributes from unseen stylized\nportraits, artistic paintings, and hand-drawn sketches. On large-scale\nsynthesized and sketch datasets, we demonstrate that our face recovery method\nachieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 09:29:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Shiri", "Fatemeh", ""], ["Yu", "Xin", ""], ["Porikli", "Fatih", ""], ["Hartley", "Richard", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1904.03616", "submitter": "Beibin Li", "authors": "Beibin Li, Sachin Mehta, Deepali Aneja, Claire Foster, Pamela Ventola,\n  Frederick Shic, Linda Shapiro", "title": "A Facial Affect Analysis System for Autism Spectrum Disorder", "comments": "5 pages (including 1 page for reference), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an end-to-end machine learning-based system for\nclassifying autism spectrum disorder (ASD) using facial attributes such as\nexpressions, action units, arousal, and valence. Our system classifies ASD\nusing representations of different facial attributes from convolutional neural\nnetworks, which are trained on images in the wild. Our experimental results\nshow that different facial attributes used in our system are statistically\nsignificant and improve sensitivity, specificity, and F1 score of ASD\nclassification by a large margin. In particular, the addition of different\nfacial attributes improves the performance of ASD classification by about 7%\nwhich achieves a F1 score of 76%.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 10:08:35 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Beibin", ""], ["Mehta", "Sachin", ""], ["Aneja", "Deepali", ""], ["Foster", "Claire", ""], ["Ventola", "Pamela", ""], ["Shic", "Frederick", ""], ["Shapiro", "Linda", ""]]}, {"id": "1904.03624", "submitter": "Lu Yu", "authors": "Lu Yu, Vacit Oguz Yazici, Xialei Liu, Joost van de Weijer, Yongmei\n  Cheng, Arnau Ramisa", "title": "Learning Metrics from Teachers: Compact Networks for Image Embedding", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metric learning networks are used to compute image embeddings, which are\nwidely used in many applications such as image retrieval and face recognition.\nIn this paper, we propose to use network distillation to efficiently compute\nimage embeddings with small networks. Network distillation has been\nsuccessfully applied to improve image classification, but has hardly been\nexplored for metric learning. To do so, we propose two new loss functions that\nmodel the communication of a deep teacher network to a small student network.\nWe evaluate our system in several datasets, including CUB-200-2011, Cars-196,\nStanford Online Products and show that embeddings computed using small student\nnetworks perform significantly better than those computed using standard\nnetworks of similar size. Results on a very compact network (MobileNet-0.25),\nwhich can be used on mobile devices, show that the proposed method can greatly\nimprove Recall@1 results from 27.5\\% to 44.6\\%. Furthermore, we investigate\nvarious aspects of distillation for embeddings, including hint and attention\nlayers, semi-supervised learning and cross quality distillation. (Code is\navailable at https://github.com/yulu0724/EmbeddingDistillation.)\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 11:12:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yu", "Lu", ""], ["Yazici", "Vacit Oguz", ""], ["Liu", "Xialei", ""], ["van de Weijer", "Joost", ""], ["Cheng", "Yongmei", ""], ["Ramisa", "Arnau", ""]]}, {"id": "1904.03629", "submitter": "Songtao Liu", "authors": "Songtao Liu, Di Huang and Yunhong Wang", "title": "Adaptive NMS: Refining Pedestrian Detection in a Crowd", "comments": "To appear at CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrian detection in a crowd is a very challenging issue. This paper\naddresses this problem by a novel Non-Maximum Suppression (NMS) algorithm to\nbetter refine the bounding boxes given by detectors. The contributions are\nthreefold: (1) we propose adaptive-NMS, which applies a dynamic suppression\nthreshold to an instance, according to the target density; (2) we design an\nefficient subnetwork to learn density scores, which can be conveniently\nembedded into both the single-stage and two-stage detectors; and (3) we achieve\nstate of the art results on the CityPersons and CrowdHuman benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 11:50:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Liu", "Songtao", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""]]}, {"id": "1904.03632", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li, Fa-Ting Hong and Wei-Shi Zheng", "title": "Learning to Learn Relation for Important People Detection in Still\n  Images", "comments": "Important people detection, Relation Network, POINT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily recognize the importance of people in social event images,\nand they always focus on the most important individuals. However, learning to\nlearn the relation between people in an image, and inferring the most important\nperson based on this relation, remains undeveloped. In this work, we propose a\ndeep imPOrtance relatIon NeTwork (POINT) that combines both relation modeling\nand feature learning. In particular, we infer two types of interaction modules:\nthe person-person interaction module that learns the interaction between people\nand the event-person interaction module that learns to describe how a person is\ninvolved in the event occurring in an image. We then estimate the importance\nrelations among people from both interactions and encode the relation feature\nfrom the importance relations. In this way, POINT automatically learns several\ntypes of relation features in parallel, and we aggregate these relation\nfeatures and the person's feature to form the importance feature for important\npeople classification. Extensive experimental results show that our method is\neffective for important people detection and verify the efficacy of learning to\nlearn relations for important people detection.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 12:11:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Wei-Hong", ""], ["Hong", "Fa-Ting", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1904.03639", "submitter": "Siyuan Liu", "authors": "Siyuan Liu, Kim-Han Thung, Weili Lin, Pew-Thian Yap and Dinggang Shen", "title": "Real-Time Quality Assessment of Pediatric MRI via Semi-Supervised Deep\n  Nonlocal Residual Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2992079", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce an image quality assessment (IQA) method for\npediatric T1- and T2-weighted MR images. IQA is first performed slice-wise\nusing a nonlocal residual neural network (NR-Net) and then volume-wise by\nagglomerating the slice QA results using random forest. Our method requires\nonly a small amount of quality-annotated images for training and is designed to\nbe robust to annotation noise that might occur due to rater errors and the\ninevitable mix of good and bad slices in an image volume. Using a small set of\nquality-assessed images, we pre-train NR-Net to annotate each image slice with\nan initial quality rating (i.e., pass, questionable, fail), which we then\nrefine by semi-supervised learning and iterative self-training. Experimental\nresults demonstrate that our method, trained using only samples of modest size,\nexhibit great generalizability, capable of real-time (milliseconds per volume)\nlarge-scale IQA with near-perfect accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 12:37:13 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 23:12:31 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Liu", "Siyuan", ""], ["Thung", "Kim-Han", ""], ["Lin", "Weili", ""], ["Yap", "Pew-Thian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1904.03668", "submitter": "Thanh Huy Nguyen", "authors": "Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes and\n  Jean-Marc Le Caillec", "title": "Robust Building-based Registration of Airborne LiDAR Data and Optical\n  Imagery on Urban Scenes", "comments": "Copyright 2019 IEEE. Published in the IEEE 2019 International\n  Geoscience & Remote Sensing Symposium (IGARSS 2019), scheduled for July 28 -\n  August 2, 2019 in Yokohama, Japan", "journal-ref": "IGARSS 2019 - 2019 IEEE International Geoscience and Remote\n  Sensing Symposium, Yokohama, Japan, 2019, pp. 8474-8477", "doi": "10.1109/IGARSS.2019.8898612", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this paper is to address the problem of registering\nairborne LiDAR data and optical aerial or satellite imagery acquired from\ndifferent platforms, at different times, with different points of view and\nlevels of detail. In this paper, we present a robust registration method based\non building regions, which are extracted from optical images using mean shift\nsegmentation, and from LiDAR data using a 3D point cloud filtering process. The\nmatching of the extracted building segments is then carried out using Graph\nTransformation Matching (GTM) which allows to determine a common pattern of\nrelative positions of segment centers. Thanks to this registration, the\nrelative shifts between the data sets are significantly reduced, which enables\na subsequent fine registration and a resulting high-quality data fusion.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 15:13:58 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Nguyen", "Thanh Huy", ""], ["Daniel", "Sylvie", ""], ["Gueriot", "Didier", ""], ["Sintes", "Christophe", ""], ["Caillec", "Jean-Marc Le", ""]]}, {"id": "1904.03692", "submitter": "Michael Ying Yang", "authors": "Dayan Guan, Xing Luo, Yanpeng Cao, Jiangxin Yang, Yanlong Cao, George\n  Vosselman, Michael Ying Yang", "title": "Unsupervised Domain Adaptation for Multispectral Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal information (e.g., visible and thermal) can generate robust\npedestrian detections to facilitate around-the-clock computer vision\napplications, such as autonomous driving and video surveillance. However, it\nstill remains a crucial challenge to train a reliable detector working well in\ndifferent multispectral pedestrian datasets without manual annotations. In this\npaper, we propose a novel unsupervised domain adaptation framework for\nmultispectral pedestrian detection, by iteratively generating pseudo\nannotations and updating the parameters of our designed multispectral\npedestrian detector on target domain. Pseudo annotations are generated using\nthe detector trained on source domain, and then updated by fixing the\nparameters of detector and minimizing the cross entropy loss without\nback-propagation. Training labels are generated using the pseudo annotations by\nconsidering the characteristics of similarity and complementarity between\nwell-aligned visible and infrared image pairs. The parameters of detector are\nupdated using the generated labels by minimizing our defined multi-detection\nloss function with back-propagation. The optimal parameters of detector can be\nobtained after iteratively updating the pseudo annotations and parameters.\nExperimental results show that our proposed unsupervised multimodal domain\nadaptation method achieves significantly higher detection performance than the\napproach without domain adaptation, and is competitive with the supervised\nmultispectral pedestrian detectors.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 17:24:28 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Guan", "Dayan", ""], ["Luo", "Xing", ""], ["Cao", "Yanpeng", ""], ["Yang", "Jiangxin", ""], ["Cao", "Yanlong", ""], ["Vosselman", "George", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1904.03693", "submitter": "Carlos Mastalli", "authors": "Carlos Mastalli, Ioannis Havoutis, Alexander W. Winkler, Darwin G.\n  Caldwell and Claudio Semini", "title": "On-line and on-board planning and perception for quadrupedal locomotion", "comments": "7 pages, International Conference on Technologies for Practical Robot\n  Applications", "journal-ref": "published 2015", "doi": "10.1109/TePRA.2015.7219685", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a legged motion planning approach for quadrupedal locomotion over\nchallenging terrain. We decompose the problem into body action planning and\nfootstep planning. We use a lattice representation together with a set of\ndefined body movement primitives for computing a body action plan. The lattice\nrepresentation allows us to plan versatile movements that ensure feasibility\nfor every possible plan. To this end, we propose a set of rules that define the\nfootstep search regions and footstep sequence given a body action. We use\nAnytime Repairing A* (ARA*) search that guarantees bounded suboptimal plans.\nOur main contribution is a planning approach that generates on-line versatile\nmovements. Experimental trials demonstrate the performance of our planning\napproach in a set of challenging terrain conditions. The terrain information\nand plans are computed on-line and on-board.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 17:27:14 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mastalli", "Carlos", ""], ["Havoutis", "Ioannis", ""], ["Winkler", "Alexander W.", ""], ["Caldwell", "Darwin G.", ""], ["Semini", "Claudio", ""]]}, {"id": "1904.03699", "submitter": "Chongyang Wang", "authors": "Min Peng, Chongyang Wang, Tao Bi, Tong Chen, XiangDong Zhou, Yu shi", "title": "A Novel Apex-Time Network for Cross-Dataset Micro-Expression Recognition", "comments": "6 pages, 3 figures, 3 tables, code available, accepted in ACII 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic recognition of micro-expression has been boosted ever since the\nsuccessful introduction of deep learning approaches. As researchers working on\nsuch topics are moving to learn from the nature of micro-expression, the\npractice of using deep learning techniques has evolved from processing the\nentire video clip of micro-expression to the recognition on apex frame. Using\nthe apex frame is able to get rid of redundant video frames, but the relevant\ntemporal evidence of micro-expression would be thereby left out. This paper\nproposes a novel Apex-Time Network (ATNet) to recognize micro-expression based\non spatial information from the apex frame as well as on temporal information\nfrom the respective-adjacent frames. Through extensive experiments on three\nbenchmarks, we demonstrate the improvement achieved by learning such temporal\ninformation. Specially, the model with such temporal information is more robust\nin cross-dataset validations.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 17:59:04 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 13:54:00 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 08:55:23 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 08:50:51 GMT"}, {"version": "v5", "created": "Fri, 12 Jul 2019 11:51:39 GMT"}, {"version": "v6", "created": "Wed, 17 Jul 2019 12:19:23 GMT"}, {"version": "v7", "created": "Sat, 17 Aug 2019 12:41:09 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Peng", "Min", ""], ["Wang", "Chongyang", ""], ["Bi", "Tao", ""], ["Chen", "Tong", ""], ["Zhou", "XiangDong", ""], ["shi", "Yu", ""]]}, {"id": "1904.03710", "submitter": "Kuldeep Purohit", "authors": "Kuldeep Purohit, Subeesh Vasu, M. Purnachandra Rao, A. N. Rajagopalan", "title": "Planar Geometry and Latest Scene Recovery from a Single Motion Blurred\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on motion deblurring either ignore the effects of\ndepth-dependent blur or work with the assumption of a multi-layered scene\nwherein each layer is modeled in the form of fronto-parallel plane. In this\nwork, we consider the case of 3D scenes with piecewise planar structure i.e., a\nscene that can be modeled as a combination of multiple planes with arbitrary\norientations. We first propose an approach for estimation of normal of a planar\nscene from a single motion blurred observation. We then develop an algorithm\nfor automatic recovery of a number of planes, the parameters corresponding to\neach plane, and camera motion from a single motion blurred image of a\nmultiplanar 3D scene. Finally, we propose a first-of-its-kind approach to\nrecover the planar geometry and latent image of the scene by adopting an\nalternating minimization framework built on our findings. Experiments on\nsynthetic and real data reveal that our proposed method achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 18:58:47 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 07:17:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Purohit", "Kuldeep", ""], ["Vasu", "Subeesh", ""], ["Rao", "M. Purnachandra", ""], ["Rajagopalan", "A. N.", ""]]}, {"id": "1904.03714", "submitter": "Andres Asensio Ramos", "authors": "A. Asensio Ramos (1,2), C. Diaz Baso (3) ((1) Instituto de Astrofisica\n  de Canarias, (2) Universidad de La Laguna, (3) Institute for Solar Physics,\n  Dept. of Astronomy, Stockholm University)", "title": "Stokes Inversion based on Convolutional Neural Networks", "comments": "18 pages, 14 figures, accepted for publication in Astronomy &\n  Astrophysics", "journal-ref": null, "doi": "10.1051/0004-6361/201935628", "report-no": null, "categories": "astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectropolarimetric inversions are routinely used in the field of Solar\nPhysics for the extraction of physical information from observations. The\napplication to two-dimensional fields of view often requires the use of\nsupercomputers with parallelized inversion codes. Even in this case, the\ncomputing time spent on the process is still very large. Our aim is to develop\na new inversion code based on the application of convolutional neural networks\nthat can quickly provide a three-dimensional cube of thermodynamical and\nmagnetic properties from the interpretation of two-dimensional maps of Stokes\nprofiles. We train two different architectures of fully convolutional neural\nnetworks. To this end, we use the synthetic Stokes profiles obtained from two\nsnapshots of three-dimensional magneto-hydrodynamic numerical simulations of\ndifferent structures of the solar atmosphere. We provide an extensive analysis\nof the new inversion technique, showing that it infers the thermodynamical and\nmagnetic properties with a precision comparable to that of standard inversion\ntechniques. However, it provides several key improvements: our method is around\none million times faster, it returns a three-dimensional view of the physical\nproperties of the region of interest in geometrical height, it provides\nquantities that cannot be obtained otherwise (pressure and Wilson depression)\nand the inferred properties are decontaminated from the blurring effect of\ninstrumental point spread functions for free. The code is provided for free on\na specific repository, with options for training and evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:03:45 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 09:42:38 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ramos", "A. Asensio", ""], ["Baso", "C. Diaz", ""]]}, {"id": "1904.03734", "submitter": "Samuel Grieggs", "authors": "Samuel Grieggs, Bingyu Shen, Greta Rauch, Pei Li, Jiaqi Ma, David\n  Chiang, Brian Price, Walter J. Scheirer", "title": "Measuring Human Perception to Improve Handwritten Document Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subtleties of human perception, as measured by vision scientists through\nthe use of psychophysics, are important clues to the internal workings of\nvisual recognition. For instance, measured reaction time can indicate whether a\nvisual stimulus is easy for a subject to recognize, or whether it is hard. In\nthis paper, we consider how to incorporate psychophysical measurements of\nvisual perception into the loss function of a deep neural network being trained\nfor a recognition task, under the assumption that such information can enforce\nconsistency with human behavior. As a case study to assess the viability of\nthis approach, we look at the problem of handwritten document transcription.\nWhile good progress has been made towards automatically transcribing modern\nhandwriting, significant challenges remain in transcribing historical\ndocuments. Here we describe a general enhancement strategy, underpinned by the\nnew loss formulation, which can be applied to the training regime of any deep\nlearning-based document transcription system. Through experimentation, reliable\nperformance improvement is demonstrated for the standard IAM and RIMES datasets\nfor three different network architectures. Further, we go on to show\nfeasibility for our approach on a new dataset of digitized Latin manuscripts,\noriginally produced by scribes in the Cloister of St. Gall in the the 9th\ncentury.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:23:31 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 04:28:09 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 02:03:55 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 15:58:39 GMT"}, {"version": "v5", "created": "Tue, 22 Jun 2021 22:00:27 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Grieggs", "Samuel", ""], ["Shen", "Bingyu", ""], ["Rauch", "Greta", ""], ["Li", "Pei", ""], ["Ma", "Jiaqi", ""], ["Chiang", "David", ""], ["Price", "Brian", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1904.03750", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Zhewei Yao and Michael W. Mahoney", "title": "JumpReLU: A Retrofit Defense Strategy for Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated that very simple attacks can fool\nhighly-sophisticated neural network architectures. In particular, so-called\nadversarial examples, constructed from perturbations of input data that are\nsmall or imperceptible to humans but lead to different predictions, may lead to\nan enormous risk in certain critical applications. In light of this, there has\nbeen a great deal of work on developing adversarial training strategies to\nimprove model robustness. These training strategies are very expensive, in both\nhuman and computational time. To complement these approaches, we propose a very\nsimple and inexpensive strategy which can be used to ``retrofit'' a\npreviously-trained network to improve its resilience to adversarial attacks.\nMore concretely, we propose a new activation function---the JumpReLU---which,\nwhen used in place of a ReLU in an already-trained model, leads to a trade-off\nbetween predictive accuracy and robustness. This trade-off is controlled by the\njump size, a hyper-parameter which can be tuned during the validation stage.\nOur empirical results demonstrate that this increases model robustness,\nprotecting against adversarial attacks with substantially increased levels of\nperturbations. This is accomplished simply by retrofitting existing networks\nwith our JumpReLU activation function, without the need for retraining the\nmodel. Additionally, we demonstrate that adversarially trained (robust) models\ncan greatly benefit from retrofitting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:47:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Yao", "Zhewei", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1904.03751", "submitter": "Matthias M\\\"uller", "authors": "Guohao Li, Matthias M\\\"uller, Ali Thabet, Bernard Ghanem", "title": "DeepGCNs: Can GCNs Go as Deep as CNNs?", "comments": "First two authors contributed equally. Accepted to ICCV'19 as oral\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) achieve impressive performance in a wide\nvariety of fields. Their success benefited from a massive boost when very deep\nCNN models were able to be reliably trained. Despite their merits, CNNs fail to\nproperly address problems with non-Euclidean data. To overcome this challenge,\nGraph Convolutional Networks (GCNs) build graphs to represent non-Euclidean\ndata, borrow concepts from CNNs, and apply them in training. GCNs show\npromising results, but they are usually limited to very shallow models due to\nthe vanishing gradient problem. As a result, most state-of-the-art GCN models\nare no deeper than 3 or 4 layers. In this work, we present new ways to\nsuccessfully train very deep GCNs. We do this by borrowing concepts from CNNs,\nspecifically residual/dense connections and dilated convolutions, and adapting\nthem to GCN architectures. Extensive experiments show the positive effect of\nthese deep GCN frameworks. Finally, we use these new concepts to build a very\ndeep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU\nover state-of-the-art) in the task of point cloud semantic segmentation. We\nbelieve that the community can greatly benefit from this work, as it opens up\nmany opportunities for advancing GCN-based research.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:49:26 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 12:53:49 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Li", "Guohao", ""], ["M\u00fcller", "Matthias", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.03754", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt, Ankur Handa, James Hays, Dieter Fox", "title": "ContactGrasp: Functional Multi-finger Grasp Synthesis from Contact", "comments": "IROS 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping and manipulating objects is an important human skill. Since most\nobjects are designed to be manipulated by human hands, anthropomorphic hands\ncan enable richer human-robot interaction. Desirable grasps are not only\nstable, but also functional: they enable post-grasp actions with the object.\nHowever, functional grasp synthesis for high degree-of-freedom anthropomorphic\nhands from object shape alone is challenging because of the large optimization\nspace. We present ContactGrasp, a framework for functional grasp synthesis from\nobject shape and contact on the object surface. Contact can be manually\nspecified or obtained through demonstrations. Our contact representation is\nobject-centric and allows functional grasp synthesis even for hand models\ndifferent than the one used for demonstration. Using a dataset of contact\ndemonstrations from humans grasping diverse household objects, we synthesize\nfunctional grasps for three hand models and two functional intents. The project\nwebpage is https://contactdb.cc.gatech.edu/contactgrasp.html.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:57:17 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 03:28:58 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 05:04:43 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Handa", "Ankur", ""], ["Hays", "James", ""], ["Fox", "Dieter", ""]]}, {"id": "1904.03758", "submitter": "Kwonjoon Lee", "authors": "Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, Stefano Soatto", "title": "Meta-Learning with Differentiable Convex Optimization", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many meta-learning approaches for few-shot learning rely on simple base\nlearners such as nearest-neighbor classifiers. However, even in the few-shot\nregime, discriminatively trained linear predictors can offer better\ngeneralization. We propose to use these predictors as base learners to learn\nrepresentations for few-shot learning and show they offer better tradeoffs\nbetween feature size and performance across a range of few-shot recognition\nbenchmarks. Our objective is to learn feature embeddings that generalize well\nunder a linear classification rule for novel categories. To efficiently solve\nthe objective, we exploit two properties of linear classifiers: implicit\ndifferentiation of the optimality conditions of the convex problem and the dual\nformulation of the optimization problem. This allows us to use high-dimensional\nembeddings with improved generalization at a modest increase in computational\noverhead. Our approach, named MetaOptNet, achieves state-of-the-art performance\non miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learning\nbenchmarks. Our code is available at https://github.com/kjunelee/MetaOptNet.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 22:23:42 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 17:59:19 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Lee", "Kwonjoon", ""], ["Maji", "Subhransu", ""], ["Ravichandran", "Avinash", ""], ["Soatto", "Stefano", ""]]}, {"id": "1904.03775", "submitter": "Yunyang Xiong", "authors": "Yunyang Xiong, Hyunwoo J. Kim, Varsha Hedau", "title": "ANTNets: Mobile Convolutional Neural Networks for Resource Efficient\n  Image Classification", "comments": "CVPR 2019 Workshop on Efficient Deep Learning for Computer Vision\n  (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have achieved remarkable success in\ncomputer vision. However, deep neural networks require large computing\nresources to achieve high performance. Although depthwise separable convolution\ncan be an efficient module to approximate a standard convolution, it often\nleads to reduced representational power of networks. In this paper, under\nbudget constraints such as computational cost (MAdds) and the parameter count,\nwe propose a novel basic architectural block, ANTBlock. It boosts the\nrepresentational power by modeling, in a high dimensional space,\ninterdependency of channels between a depthwise convolution layer and a\nprojection layer in the ANTBlocks. Our experiments show that ANTNet built by a\nsequence of ANTBlocks, consistently outperforms state-of-the-art low-cost\nmobile convolutional neural networks across multiple datasets. On CIFAR100, our\nmodel achieves 75.7% top-1 accuracy, which is 1.5% higher than MobileNetV2 with\n8.3% fewer parameters and 19.6% less computational cost. On ImageNet, our model\nachieves 72.8% top-1 accuracy, which is 0.8% improvement, with 157.7ms (20%\nfaster) on iPhone 5s over MobileNetV2.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 23:43:24 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 18:53:34 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Xiong", "Yunyang", ""], ["Kim", "Hyunwoo J.", ""], ["Hedau", "Varsha", ""]]}, {"id": "1904.03786", "submitter": "Yunyang Xiong", "authors": "Yunyang Xiong, Ronak Mehta, Vikas Singh", "title": "Resource Constrained Neural Network Architecture Search: Will a\n  Submodularity Assumption Help?", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural network architectures is frequently either based on\nhuman expertise using trial/error and empirical feedback or tackled via large\nscale reinforcement learning strategies performed over distinct discrete\narchitecture choices. In the latter case, the optimization is often\nnon-differentiable and also not very amenable to derivative-free optimization\nmethods. Most methods in use today require sizable computational resources. And\nif we want networks that additionally satisfy resource constraints, the above\nchallenges are exacerbated because the search must now balance accuracy with\ncertain budget constraints on resources. We formulate this problem as the\noptimization of a set function -- we find that the empirical behavior of this\nset function often (but not always) satisfies marginal gain and monotonicity\nprinciples -- properties central to the idea of submodularity. Based on this\nobservation, we adapt algorithms within discrete optimization to obtain\nheuristic schemes for neural network architecture search, where we have\nresource constraints on the architecture. This simple scheme when applied on\nCIFAR-100 and ImageNet, identifies resource-constrained architectures with\nquantifiably better performance than current state-of-the-art models designed\nfor mobile devices. Specifically, we find high-performing architectures with\nfewer parameters and computations by a search method that is much faster.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 00:39:27 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 19:07:43 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Xiong", "Yunyang", ""], ["Mehta", "Ronak", ""], ["Singh", "Vikas", ""]]}, {"id": "1904.03797", "submitter": "Tao Kong", "authors": "Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Lei Li, Jianbo Shi", "title": "FoveaBox: Beyond Anchor-based Object Detector", "comments": "IEEE Transactions on Image Processing, code at:\n  https://github.com/taokong/FoveaBox", "journal-ref": "IEEE Trans. Image Process. pp. 7389-7398 (2020)", "doi": "10.1109/TIP.2020.3002345", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FoveaBox, an accurate, flexible, and completely anchor-free\nframework for object detection. While almost all state-of-the-art object\ndetectors utilize predefined anchors to enumerate possible locations, scales\nand aspect ratios for the search of the objects, their performance and\ngeneralization ability are also limited to the design of anchors. Instead,\nFoveaBox directly learns the object existing possibility and the bounding box\ncoordinates without anchor reference. This is achieved by: (a) predicting\ncategory-sensitive semantic maps for the object existing possibility, and (b)\nproducing category-agnostic bounding box for each position that potentially\ncontains an object. The scales of target boxes are naturally associated with\nfeature pyramid representations. In FoveaBox, an instance is assigned to\nadjacent feature levels to make the model more accurate.We demonstrate its\neffectiveness on standard benchmarks and report extensive experimental\nanalysis. Without bells and whistles, FoveaBox achieves state-of-the-art single\nmodel performance on the standard COCO and Pascal VOC object detection\nbenchmark. More importantly, FoveaBox avoids all computation and\nhyper-parameters related to anchor boxes, which are often sensitive to the\nfinal detection performance. We believe the simple and effective approach will\nserve as a solid baseline and help ease future research for object detection.\nThe code has been made publicly available at\nhttps://github.com/taokong/FoveaBox .\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 01:43:48 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:38:26 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Kong", "Tao", ""], ["Sun", "Fuchun", ""], ["Liu", "Huaping", ""], ["Jiang", "Yuning", ""], ["Li", "Lei", ""], ["Shi", "Jianbo", ""]]}, {"id": "1904.03803", "submitter": "Tianxin Shi", "authors": "Tianxin Shi, Shuhan Shen, Xiang Gao, Lingjie Zhu", "title": "Visual Localization Using Sparse Semantic 3D Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust visual localization under a wide range of viewing\ncondition variations including season and illumination changes, as well as\nweather and day-night variations, is the key component for many computer vision\nand robotics applications. Under these conditions, most traditional methods\nwould fail to locate the camera. In this paper we present a visual localization\nalgorithm that combines structure-based method and image-based method with\nsemantic information. Given semantic information about the query and database\nimages, the retrieved images are scored according to the semantic consistency\nof the 3D model and the query image. Then the semantic matching score is used\nas weight for RANSAC's sampling and the pose is solved by a standard PnP\nsolver. Experiments on the challenging long-term visual localization benchmark\ndataset demonstrate that our method has significant improvement compared with\nthe state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 02:36:58 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 01:24:21 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Shi", "Tianxin", ""], ["Shen", "Shuhan", ""], ["Gao", "Xiang", ""], ["Zhu", "Lingjie", ""]]}, {"id": "1904.03816", "submitter": "Sungjoo Ha", "authors": "Seokjun Seo, Seungwoo Choi, Martin Kersner, Beomjun Shin, Hyungsuk\n  Yoon, Hyeongmin Byun, Sungjoo Ha", "title": "Towards Real-Time Automatic Portrait Matting on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of automatic portrait matting on mobile devices. The\nproposed model is aimed at attaining real-time inference on mobile devices with\nminimal degradation of model performance. Our model MMNet, based on\nmulti-branch dilated convolution with linear bottleneck blocks, outperforms the\nstate-of-the-art model and is orders of magnitude faster. The model can be\naccelerated four times to attain 30 FPS on Xiaomi Mi 5 device with moderate\nincrease in the gradient error. Under the same conditions, our model has an\norder of magnitude less number of parameters and is faster than Mobile\nDeepLabv3 while maintaining comparable performance. The accompanied\nimplementation can be found at \\url{https://github.com/hyperconnect/MMNet}.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:21:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Seo", "Seokjun", ""], ["Choi", "Seungwoo", ""], ["Kersner", "Martin", ""], ["Shin", "Beomjun", ""], ["Yoon", "Hyungsuk", ""], ["Byun", "Hyeongmin", ""], ["Ha", "Sungjoo", ""]]}, {"id": "1904.03828", "submitter": "Dacheng Tao", "authors": "Chen Gong, Dacheng Tao, Xiaojun Chang, Jian Yang", "title": "Ensemble Teaching for Hybrid Label Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label propagation aims to iteratively diffuse the label information from\nlabeled examples to unlabeled examples over a similarity graph. Current label\npropagation algorithms cannot consistently yield satisfactory performance due\nto two reasons: one is the instability of single propagation method in dealing\nwith various practical data, and the other one is the improper propagation\nsequence ignoring the labeling difficulties of different examples. To remedy\nabove defects, this paper proposes a novel propagation algorithm called hybrid\ndiffusion under ensemble teaching (HyDEnT). Specifically, HyDEnT integrates\nmultiple propagation methods as base learners to fully exploit their individual\nwisdom, which helps HyDEnT to be stable and obtain consistent encouraging\nresults. More importantly, HyDEnT conducts propagation under the guidance of an\nensemble of teachers. That is to say, in every propagation round the simplest\ncurriculum examples are wisely designated by a teaching algorithm, so that\ntheir labels can be reliably and accurately decided by the learners. To\noptimally choose these simplest examples, every teacher in the ensemble should\ncomprehensively consider the examples' difficulties from its own viewpoint, as\nwell as the common knowledge shared by all the teachers. This is accomplished\nby a designed optimization problem, which can be efficiently solved via the\nblock coordinate descent method. Thanks to the efforts of the teachers, all the\nunlabeled examples are logically propagated from simple to difficult, leading\nto better propagation quality of HyDEnT than the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:10:40 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gong", "Chen", ""], ["Tao", "Dacheng", ""], ["Chang", "Xiaojun", ""], ["Yang", "Jian", ""]]}, {"id": "1904.03832", "submitter": "Jingke Meng", "authors": "Jingke Meng, Sheng Wu, Wei-Shi Zheng", "title": "Weakly Supervised Person Re-Identification", "comments": "to appear at CVPR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the conventional person re-id setting, it is assumed that the labeled\nimages are the person images within the bounding box for each individual; this\nlabeling across multiple nonoverlapping camera views from raw video\nsurveillance is costly and time-consuming. To overcome this difficulty, we\nconsider weakly supervised person re-id modeling. The weak setting refers to\nmatching a target person with an untrimmed gallery video where we only know\nthat the identity appears in the video without the requirement of annotating\nthe identity in any frame of the video during the training procedure. Hence,\nfor a video, there could be multiple video-level labels. We cast this weakly\nsupervised person re-id challenge into a multi-instance multi-label learning\n(MIML) problem. In particular, we develop a Cross-View MIML (CV-MIML) method\nthat is able to explore potential intraclass person images from all the camera\nviews by incorporating the intra-bag alignment and the cross-view bag\nalignment. Finally, the CV-MIML method is embedded into an existing deep neural\nnetwork for developing the Deep Cross-View MIML (Deep CV-MIML) model. We have\nperformed extensive experiments to show the feasibility of the proposed weakly\nsupervised setting and verify the effectiveness of our method compared to\nrelated methods on four weakly labeled datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:21:54 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 00:46:11 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Meng", "Jingke", ""], ["Wu", "Sheng", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1904.03837", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han", "title": "Centripetal SGD for Pruning Very Deep Convolutional Networks with\n  Complicated Structure", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The redundancy is widely recognized in Convolutional Neural Networks (CNNs),\nwhich enables to remove unimportant filters from convolutional layers so as to\nslim the network with acceptable performance drop. Inspired by the linear and\ncombinational properties of convolution, we seek to make some filters\nincreasingly close and eventually identical for network slimming. To this end,\nwe propose Centripetal SGD (C-SGD), a novel optimization method, which can\ntrain several filters to collapse into a single point in the parameter\nhyperspace. When the training is completed, the removal of the identical\nfilters can trim the network with NO performance loss, thus no finetuning is\nneeded. By doing so, we have partly solved an open problem of constrained\nfilter pruning on CNNs with complicated structure, where some layers must be\npruned following others. Our experimental results on CIFAR-10 and ImageNet have\njustified the effectiveness of C-SGD-based filter pruning. Moreover, we have\nprovided empirical evidences for the assumption that the redundancy in deep\nneural networks helps the convergence of training by showing that a redundant\nCNN trained using C-SGD outperforms a normally trained counterpart with the\nequivalent width.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:48:02 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ding", "Xiaohan", ""], ["Ding", "Guiguang", ""], ["Guo", "Yuchen", ""], ["Han", "Jungong", ""]]}, {"id": "1904.03845", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang Lai, Zhengtao\n  Yu, and Liang Lin", "title": "Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A\n  New Benchmark", "comments": "Accepted by TNNLS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) benefits greatly from the accurate\nannotations of existing datasets (e.g., CUHK03 [1] and Market-1501 [2]), which\nare quite expensive because each image in these datasets has to be assigned\nwith a proper label. In this work, we ease the annotation of Re-ID by replacing\nthe accurate annotation with inaccurate annotation, i.e., we group the images\ninto bags in terms of time and assign a bag-level label for each bag. This\ngreatly reduces the annotation effort and leads to the creation of a\nlarge-scale Re-ID benchmark called SYSU-30$k$. The new benchmark contains $30k$\nindividuals, which is about $20$ times larger than CUHK03 ($1.3k$ individuals)\nand Market-1501 ($1.5k$ individuals), and $30$ times larger than ImageNet ($1k$\ncategories). It sums up to 29,606,918 images. Learning a Re-ID model with\nbag-level annotation is called the weakly supervised Re-ID problem. To solve\nthis problem, we introduce a differentiable graphical model to capture the\ndependencies from all images in a bag and generate a reliable pseudo label for\neach person image. The pseudo label is further used to supervise the learning\nof the Re-ID model. When compared with the fully supervised Re-ID models, our\nmethod achieves state-of-the-art performance on SYSU-30$k$ and other datasets.\nThe code, dataset, and pretrained model will be available at\n\\url{https://github.com/wanggrun/SYSU-30k}.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 05:27:53 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 16:01:41 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 08:16:31 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wang", "Guangrun", ""], ["Wang", "Guangcong", ""], ["Zhang", "Xujie", ""], ["Lai", "Jianhuang", ""], ["Yu", "Zhengtao", ""], ["Lin", "Liang", ""]]}, {"id": "1904.03846", "submitter": "Dacheng Tao", "authors": "Yong Luo, Tongliang Liu, Dacheng Tao, Chao Xu", "title": "Decomposition-Based Transfer Distance Metric Learning for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) is a critical factor for image analysis and\npattern recognition. To learn a robust distance metric for a target task, we\nneed abundant side information (i.e., the similarity/dissimilarity pairwise\nconstraints over the labeled data), which is usually unavailable in practice\ndue to the high labeling cost. This paper considers the transfer learning\nsetting by exploiting the large quantity of side information from certain\nrelated, but different source tasks to help with target metric learning (with\nonly a little side information). The state-of-the-art metric learning\nalgorithms usually fail in this setting because the data distributions of the\nsource task and target task are often quite different. We address this problem\nby assuming that the target distance metric lies in the space spanned by the\neigenvectors of the source metrics (or other randomly generated bases). The\ntarget metric is represented as a combination of the base metrics, which are\ncomputed using the decomposed components of the source metrics (or simply a set\nof random bases); we call the proposed method, decomposition-based transfer DML\n(DTDML). In particular, DTDML learns a sparse combination of the base metrics\nto construct the target metric by forcing the target metric to be close to an\nintegration of the source metrics. The main advantage of the proposed method\ncompared with existing transfer metric learning approaches is that we directly\nlearn the base metric coefficients instead of the target metric. To this end,\nfar fewer variables need to be learned. We therefore obtain more reliable\nsolutions given the limited side information and the optimization tends to be\nfaster. Experiments on the popular handwritten image (digit, letter)\nclassification and challenge natural image annotation tasks demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 05:39:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1904.03848", "submitter": "Yiran Zhong", "authors": "Yiran Zhong, Pan Ji, Jianyuan Wang, Yuchao Dai, Hongdong Li", "title": "Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes", "comments": "Accepted at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised deep learning for optical flow computation has achieved\npromising results. Most existing deep-net based methods rely on image\nbrightness consistency and local smoothness constraint to train the networks.\nTheir performance degrades at regions where repetitive textures or occlusions\noccur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical\nflow method which incorporates global geometric constraints into network\nlearning. In particular, we investigate multiple ways of enforcing the epipolar\nconstraint in flow estimation. To alleviate a ``chicken-and-egg'' type of\nproblem encountered in dynamic scenes where multiple motions may be present, we\npropose a low-rank constraint as well as a union-of-subspaces constraint for\ntraining. Experimental results on various benchmarking datasets show that our\nmethod achieves competitive performance compared with supervised methods and\noutperforms state-of-the-art unsupervised deep-learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 05:41:48 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhong", "Yiran", ""], ["Ji", "Pan", ""], ["Wang", "Jianyuan", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1904.03868", "submitter": "Xuelian Cheng", "authors": "Xuelian Cheng, Yiran Zhong, Yuchao Dai, Pan Ji, Hongdong Li", "title": "Noise-Aware Unsupervised Deep Lidar-Stereo Fusion", "comments": "Accepted at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present LidarStereoNet, the first unsupervised Lidar-stereo\nfusion network, which can be trained in an end-to-end manner without the need\nof ground truth depth maps. By introducing a novel \"Feedback Loop'' to connect\nthe network input with output, LidarStereoNet could tackle both noisy Lidar\npoints and misalignment between sensors that have been ignored in existing\nLidar-stereo fusion studies. Besides, we propose to incorporate a piecewise\nplanar model into network learning to further constrain depths to conform to\nthe underlying 3D geometry. Extensive quantitative and qualitative evaluations\non both real and synthetic datasets demonstrate the superiority of our method,\nwhich outperforms state-of-the-art stereo matching, depth completion and\nLidar-Stereo fusion approaches significantly.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 07:16:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Cheng", "Xuelian", ""], ["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Ji", "Pan", ""], ["Li", "Hongdong", ""]]}, {"id": "1904.03870", "submitter": "Jonghwan Mun", "authors": "Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, Bohyung Han", "title": "Streamlined Dense Video Captioning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense video captioning is an extremely challenging task since accurate and\ncoherent description of events in a video requires holistic understanding of\nvideo contents as well as contextual reasoning of individual events. Most\nexisting approaches handle this problem by first detecting event proposals from\na video and then captioning on a subset of the proposals. As a result, the\ngenerated sentences are prone to be redundant or inconsistent since they fail\nto consider temporal dependency between events. To tackle this challenge, we\npropose a novel dense video captioning framework, which models temporal\ndependency across events in a video explicitly and leverages visual and\nlinguistic context from prior events for coherent storytelling. This objective\nis achieved by 1) integrating an event sequence generation network to select a\nsequence of event proposals adaptively, and 2) feeding the sequence of event\nproposals to our sequential video captioning network, which is trained by\nreinforcement learning with two-level rewards at both event and episode levels\nfor better context modeling. The proposed technique achieves outstanding\nperformances on ActivityNet Captions dataset in most metrics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 07:17:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mun", "Jonghwan", ""], ["Yang", "Linjie", ""], ["Ren", "Zhou", ""], ["Xu", "Ning", ""], ["Han", "Bohyung", ""]]}, {"id": "1904.03885", "submitter": "Peratham Wiriyathammabhum Mr.", "authors": "Peratham Wiriyathammabhum, Abhinav Shrivastava, Vlad I. Morariu, Larry\n  S. Davis", "title": "Referring to Objects in Videos using Spatio-Temporal Identifying\n  Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new task, the grounding of spatio-temporal identifying\ndescriptions in videos. Previous work suggests potential bias in existing\ndatasets and emphasizes the need for a new data creation schema to better model\nlinguistic structure. We introduce a new data collection scheme based on\ngrammatical constraints for surface realization to enable us to investigate the\nproblem of grounding spatio-temporal identifying descriptions in videos. We\nthen propose a two-stream modular attention network that learns and grounds\nspatio-temporal identifying descriptions based on appearance and motion. We\nshow that motion modules help to ground motion-related words and also help to\nlearn in appearance modules because modular neural networks resolve task\ninterference between modules. Finally, we propose a future challenge and a need\nfor a robust system arising from replacing ground truth visual annotations with\nautomatic video object detector and temporal event localization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 08:28:54 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wiriyathammabhum", "Peratham", ""], ["Shrivastava", "Abhinav", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1904.03892", "submitter": "Taibou Birgui Sekou", "authors": "Taibou Birgui Sekou and Moncef Hidane and Julien Olivier and Hubert\n  Cardot", "title": "From Patch to Image Segmentation using Fully Convolutional Networks --\n  Application to Retinal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based models, generally, require a large number of samples for\nappropriate training, a requirement that is difficult to satisfy in the medical\nfield. This issue can usually be avoided with a proper initialization of the\nweights. On the task of medical image segmentation in general, two techniques\nare oftentimes employed to tackle the training of a deep network $f_T$. The\nfirst one consists in reusing some weights of a network $f_S$ pre-trained on a\nlarge scale database ($e.g.$ ImageNet). This procedure, also known as\n$transfer$ $learning$, happens to reduce the flexibility when it comes to new\nnetwork design since $f_T$ is constrained to match some parts of $f_S$. The\nsecond commonly used technique consists in working on image patches to benefit\nfrom the large number of available patches. This paper brings together these\ntwo techniques and propose to train $arbitrarily$ $designed$ $networks$ that\nsegment an image in one forward pass, with a focus on relatively small\ndatabases. An experimental work have been carried out on the tasks of retinal\nblood vessel segmentation and the optic disc one, using four publicly available\ndatabases. Furthermore, three types of network are considered, going from a\nvery light weighted network to a densely connected one. The final results show\nthe efficiency of the proposed framework along with state of the art results on\nall the databases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 08:59:12 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 11:14:20 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Sekou", "Taibou Birgui", ""], ["Hidane", "Moncef", ""], ["Olivier", "Julien", ""], ["Cardot", "Hubert", ""]]}, {"id": "1904.03895", "submitter": "Fengda Zhu", "authors": "Fengda Zhu, Linchao Zhu, Yi Yang", "title": "Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation", "comments": "Paper has been accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing interest in 3D indoor navigation, where a robot\nin an environment moves to a target according to an instruction. To deploy a\nrobot for navigation in the physical world, lots of training data is required\nto learn an effective policy. It is quite labour intensive to obtain sufficient\nreal environment data for training robots while synthetic data is much easier\nto construct by rendering. Though it is promising to utilize the synthetic\nenvironments to facilitate navigation training in the real world, real\nenvironment are heterogeneous from synthetic environment in two aspects. First,\nthe visual representation of the two environments have significant variances.\nSecond, the houseplans of these two environments are quite different. Therefore\ntwo types of information, i.e. visual representation and policy behavior, need\nto be adapted in the reinforcement model. The learning procedure of visual\nrepresentation and that of policy behavior are presumably reciprocal. We\npropose to jointly adapt visual representation and policy behavior to leverage\nthe mutual impacts of environment and policy. Specifically, our method employs\nan adversarial feature adaptation model for visual representation transfer and\na policy mimic strategy for policy behavior imitation. Experiment shows that\nour method outperforms the baseline by 19.47% without any additional human\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:01:48 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 11:00:57 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zhu", "Fengda", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1904.03901", "submitter": "Yong Luo", "authors": "Yong Luo, Tongliang Liu, Dacheng Tao, Chao Xu", "title": "Multi-View Matrix Completion for Multi-Label Image Classification", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (Volume: 24, Issue: 8, Aug.\n  2015)", "doi": "10.1109/TIP.2015.2421309", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in multi-label image classification due to its\ncritical role in web-based image analytics-based applications, such as\nlarge-scale image retrieval and browsing. Matrix completion has recently been\nintroduced as a method for transductive (semi-supervised) multi-label\nclassification, and has several distinct advantages, including robustness to\nmissing data and background noise in both feature and label space. However, it\nis limited by only considering data represented by a single-view feature, which\ncannot precisely characterize images containing several semantic concepts. To\nutilize multiple features taken from different views, we have to concatenate\nthe different features as a long vector. But this concatenation is prone to\nover-fitting and often leads to very high time complexity in MC based image\nclassification. Therefore, we propose to weightedly combine the MC outputs of\ndifferent views, and present the multi-view matrix completion (MVMC) framework\nfor transductive multi-label image classification. To learn the view\ncombination weights effectively, we apply a cross validation strategy on the\nlabeled set. In the learning process, we adopt the average precision (AP) loss,\nwhich is particular suitable for multi-label image classification. A least\nsquares loss formulation is also presented for the sake of efficiency, and the\nrobustness of the algorithm based on the AP loss compared with the other losses\nis investigated. Experimental evaluation on two real world datasets (PASCAL\nVOC' 07 and MIR Flickr) demonstrate the effectiveness of MVMC for transductive\n(semi-supervised) multi-label image classification, and show that MVMC can\nexploit complementary properties of different features and output-consistent\nlabels for improved multi-label image classification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:17:56 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1904.03910", "submitter": "Peter Csermely", "authors": "Bence Agg, Andrea Csaszar, Mate Szalay-Beko, Daniel V. Veres, Reka\n  Mizsei, Peter Ferdinandy, Peter Csermely and Istvan A. Kovacs", "title": "The EntOptLayout Cytoscape plug-in for the efficient visualization of\n  major protein complexes in protein-protein interaction and signalling\n  networks", "comments": null, "journal-ref": "Bioinformatics 2019 35, 4490-4492", "doi": "10.1093/bioinformatics/btz257", "report-no": null, "categories": "q-bio.MN cond-mat.dis-nn cs.CV physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Network visualizations of complex biological datasets usually\nresult in 'hairball' images, which do not discriminate network modules.\nResults: We present the EntOptLayout Cytoscape plug-in based on a recently\ndeveloped network representation theory. The plug-in provides an efficient\nvisualization of network modules, which represent major protein complexes in\nprotein-protein interaction and signalling networks. Importantly, the tool\ngives a quality score of the network visualization by calculating the\ninformation loss between the input data and the visual representation showing a\n3- to 25-fold improvement over conventional methods. Availability and\nimplementation: The plug-in (running on Windows, Linux, or Mac OS) and its\ntutorial (both in written and video forms) can be downloaded freely under the\nterms of the MIT license from: http://apps.cytoscape.org/apps/entoptlayout.\nSupplementary data are available at Bioinformatics online. Contact:\ncsermely.peter@med.semmelweis-univ.hu\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:35:20 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 11:47:10 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Agg", "Bence", ""], ["Csaszar", "Andrea", ""], ["Szalay-Beko", "Mate", ""], ["Veres", "Daniel V.", ""], ["Mizsei", "Reka", ""], ["Ferdinandy", "Peter", ""], ["Csermely", "Peter", ""], ["Kovacs", "Istvan A.", ""]]}, {"id": "1904.03911", "submitter": "Soumyadeep Ghosh", "authors": "Soumyadeep Ghosh, Richa Singh, Mayank Vatsa", "title": "On Learning Density Aware Embeddings", "comments": "Accepted in IEEE CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep metric learning algorithms have been utilized to learn discriminative\nand generalizable models which are effective for classifying unseen classes. In\nthis paper, a novel noise tolerant deep metric learning algorithm is proposed.\nThe proposed method, termed as Density Aware Metric Learning, enforces the\nmodel to learn embeddings that are pulled towards the most dense region of the\nclusters for each class. It is achieved by iteratively shifting the estimate of\nthe center towards the dense region of the cluster thereby leading to faster\nconvergence and higher generalizability. In addition to this, the approach is\nrobust to noisy samples in the training data, often present as outliers.\nDetailed experiments and analysis on two challenging cross-modal face\nrecognition databases and two popular object recognition databases exhibit the\nefficacy of the proposed approach. It has superior convergence, requires lesser\ntraining time, and yields better accuracies than several popular deep metric\nlearning methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:35:23 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ghosh", "Soumyadeep", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1904.03921", "submitter": "Yong Luo", "authors": "Yong Luo, Dacheng Tao, Chang Xu, Chao Xu, Hong Liu, Yonggang Wen", "title": "Multi-view Vector-valued Manifold Regularization for Multi-label Image\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (Volume:\n  24, Issue: 5, May 2013)", "doi": "10.1109/TNNLS.2013.2238682", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, image datasets used for classification are naturally\nassociated with multiple labels and comprised of multiple views, because each\nimage may contain several objects (e.g. pedestrian, bicycle and tree) and is\nproperly characterized by multiple visual features (e.g. color, texture and\nshape). Currently available tools ignore either the label relationship or the\nview complementary. Motivated by the success of the vector-valued function that\nconstructs matrix-valued kernels to explore the multi-label structure in the\noutput space, we introduce multi-view vector-valued manifold regularization\n(MV$\\mathbf{^3}$MR) to integrate multiple features. MV$\\mathbf{^3}$MR exploits\nthe complementary property of different features and discovers the intrinsic\nlocal geometry of the compact support shared by different features under the\ntheme of manifold regularization. We conducted extensive experiments on two\nchallenging, but popular datasets, PASCAL VOC' 07 (VOC) and MIR Flickr (MIR),\nand validated the effectiveness of the proposed MV$\\mathbf{^3}$MR for image\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:57:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Tao", "Dacheng", ""], ["Xu", "Chang", ""], ["Xu", "Chao", ""], ["Liu", "Hong", ""], ["Wen", "Yonggang", ""]]}, {"id": "1904.03936", "submitter": "Bharath Bhushan Damodaran", "authors": "Kilian Fatras, Bharath Bhushan Damodaran, Sylvain Lobry, R\\'emi\n  Flamary, Devis Tuia, Nicolas Courty", "title": "Wasserstein Adversarial Regularization (WAR) on label noise", "comments": "In Press, IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (PAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noisy labels often occur in vision datasets, especially when they are\nobtained from crowdsourcing or Web scraping. We propose a new regularization\nmethod, which enables learning robust classifiers in presence of noisy data. To\nachieve this goal, we propose a new adversarial regularization scheme based on\nthe Wasserstein distance. Using this distance allows taking into account\nspecific relations between classes by leveraging the geometric properties of\nthe labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a\nselective regularization, which promotes smoothness of the classifier between\nsome classes, while preserving sufficient complexity of the decision boundary\nbetween others. We first discuss how and why adversarial regularization can be\nused in the context of label noise and then show the effectiveness of our\nmethod on five datasets corrupted with noisy labels: in both benchmarks and\nreal datasets, WAR outperforms the state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:28:12 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:21:53 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 07:45:32 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Fatras", "Kilian", ""], ["Damodaran", "Bharath Bhushan", ""], ["Lobry", "Sylvain", ""], ["Flamary", "R\u00e9mi", ""], ["Tuia", "Devis", ""], ["Courty", "Nicolas", ""]]}, {"id": "1904.03941", "submitter": "Pedro Miraldo", "authors": "Pedro Miraldo, Surojit Saha, and Srikumar Ramalingam", "title": "Minimal Solvers for Mini-Loop Closures in 3D Multi-Scan Alignment", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scan registration is a classical, yet a highly useful problem in the\ncontext of 3D sensors such as Kinect and Velodyne. While there are several\nexisting methods, the techniques are usually incremental where adjacent scans\nare registered first to obtain the initial poses, followed by motion averaging\nand bundle-adjustment refinement. In this paper, we take a different approach\nand develop minimal solvers for jointly computing the initial poses of cameras\nin small loops such as 3-, 4-, and 5-cycles. Note that the classical\nregistration of 2 scans can be done using a minimum of 3 point matches to\ncompute 6 degrees of relative motion. On the other hand, to jointly compute the\n3D registrations in n-cycles, we take 2 point matches between the first n-1\nconsecutive pairs (i.e., Scan 1 & Scan 2, ... , and Scan n-1 & Scan n) and 1 or\n2 point matches between Scan 1 and Scan n. Overall, we use 5, 7, and 10 point\nmatches for 3-, 4-, and 5-cycles, and recover 12, 18, and 24 degrees of\ntransformation variables, respectively. Using simulations and real-data we show\nthat the 3D registration using mini n-cycles are computationally efficient, and\ncan provide alternate and better initial poses compared to standard pairwise\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:46:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Miraldo", "Pedro", ""], ["Saha", "Surojit", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1904.03942", "submitter": "Bjoern Haefner", "authors": "Bjoern Haefner and Zhenzhang Ye and Maolin Gao and Tao Wu and Yvain\n  Qu\\'eau and Daniel Cremers", "title": "Variational Uncalibrated Photometric Stereo under General Lighting", "comments": "Haefner and Ye contributed equally", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric stereo (PS) techniques nowadays remain constrained to an ideal\nlaboratory setup where modeling and calibration of lighting is amenable. To\neliminate such restrictions, we propose an efficient principled variational\napproach to uncalibrated PS under general illumination. To this end, the\nLambertian reflectance model is approximated through a spherical harmonic\nexpansion, which preserves the spatial invariance of the lighting. The joint\nrecovery of shape, reflectance and illumination is then formulated as a single\nvariational problem. There the shape estimation is carried out directly in\nterms of the underlying perspective depth map, thus implicitly ensuring\nintegrability and bypassing the need for a subsequent normal integration. To\ntackle the resulting nonconvex problem numerically, we undertake a two-phase\nprocedure to initialize a balloon-like perspective depth map, followed by a\n\"lagged\" block coordinate descent scheme. The experiments validate efficiency\nand robustness of this approach. Across a variety of evaluations, we are able\nto reduce the mean angular error consistently by a factor of 2-3 compared to\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:48:53 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 18:38:49 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Haefner", "Bjoern", ""], ["Ye", "Zhenzhang", ""], ["Gao", "Maolin", ""], ["Wu", "Tao", ""], ["Qu\u00e9au", "Yvain", ""], ["Cremers", "Daniel", ""]]}, {"id": "1904.03949", "submitter": "Alessandro Bianchi", "authors": "Alessandro Bianchi, Moreno Raimondo Vendra, Pavlos Protopapas, Marco\n  Brambilla", "title": "Improving Image Classification Robustness through Selective CNN-Filters\n  Fine-Tuning", "comments": "arXiv admin note: text overlap with arXiv:1705.02406 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality plays a big role in CNN-based image classification performance.\nFine-tuning the network with distorted samples may be too costly for large\nnetworks. To solve this issue, we propose a transfer learning approach\noptimized to keep into account that in each layer of a CNN some filters are\nmore susceptible to image distortion than others. Our method identifies the\nmost susceptible filters and applies retraining only to the filters that show\nthe highest activation maps distance between clean and distorted images.\nFilters are ranked using the Borda count election method and then only the most\naffected filters are fine-tuned. This significantly reduces the number of\nparameters to retrain. We evaluate this approach on the CIFAR-10 and CIFAR-100\ndatasets, testing it on two different models and two different types of\ndistortion. Results show that the proposed transfer learning technique recovers\nmost of the lost performance due to input data distortion, at a considerably\nfaster pace with respect to existing methods, thanks to the reduced number of\nparameters to fine-tune. When few noisy samples are provided for training, our\nfilter-level fine tuning performs particularly well, also outperforming state\nof the art layer-level transfer learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:02:05 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bianchi", "Alessandro", ""], ["Vendra", "Moreno Raimondo", ""], ["Protopapas", "Pavlos", ""], ["Brambilla", "Marco", ""]]}, {"id": "1904.03955", "submitter": "Chen Wang", "authors": "Chen Wang and Jianfei Yang and Lihua Xie and Junsong Yuan", "title": "Kervolutional Neural Networks", "comments": "oral paper in CVPR 2019", "journal-ref": "2019 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2019) 31-40", "doi": "10.1109/CVPR.2019.00012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have enabled the state-of-the-art\nperformance in many computer vision tasks. However, little effort has been\ndevoted to establishing convolution in non-linear space. Existing works mainly\nleverage on the activation layers, which can only provide point-wise\nnon-linearity. To solve this problem, a new operation, kervolution (kernel\nconvolution), is introduced to approximate complex behaviors of human\nperception systems leveraging on the kernel trick. It generalizes convolution,\nenhances the model capacity, and captures higher order interactions of\nfeatures, via patch-wise kernel functions, but without introducing additional\nparameters. Extensive experiments show that kervolutional neural networks (KNN)\nachieve higher accuracy and faster convergence than baseline CNN.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:10:51 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 21:27:01 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wang", "Chen", ""], ["Yang", "Jianfei", ""], ["Xie", "Lihua", ""], ["Yuan", "Junsong", ""]]}, {"id": "1904.03961", "submitter": "Yang He", "authors": "Yang He, Ping Liu, Linchao Zhu, Yi Yang", "title": "Meta Filter Pruning to Accelerate Deep Convolutional Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods usually utilize pre-defined criterions, such as p-norm, to\nprune unimportant filters. There are two major limitations in these methods.\nFirst, the relations of the filters are largely ignored. The filters usually\nwork jointly to make an accurate prediction in a collaborative way. Similar\nfilters will have equivalent effects on the network prediction, and the\nredundant filters can be further pruned. Second, the pruning criterion remains\nunchanged during training. As the network updated at each iteration, the filter\ndistribution also changes continuously. The pruning criterions should also be\nadaptively switched. In this paper, we propose Meta Filter Pruning (MFP) to\nsolve the above problems. First, as a complement to the existing p-norm\ncriterion, we introduce a new pruning criterion considering the filter relation\nvia filter distance. Additionally, we build a meta pruning framework for filter\npruning, so that our method could adaptively select the most appropriate\npruning criterion as the filter distribution changes. Experiments validate our\napproach on two image classification benchmarks. Notably, on ILSVRC-2012, our\nMFP reduces more than 50% FLOPs on ResNet-50 with only 0.44% top-5 accuracy\nloss.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:24:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["He", "Yang", ""], ["Liu", "Ping", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1904.03973", "submitter": "Tao Lei", "authors": "Tao Lei, Xiaohong Jia, Tongliang Liu, Shigang Liu, Hongying Meng, and\n  Asoke K. Nandi", "title": "Adaptive Morphological Reconstruction for Seeded Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2920514", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological reconstruction (MR) is often employed by seeded image\nsegmentation algorithms such as watershed transform and power watershed as it\nis able to filter seeds (regional minima) to reduce over-segmentation. However,\nMR might mistakenly filter meaningful seeds that are required for generating\naccurate segmentation and it is also sensitive to the scale because a\nsingle-scale structuring element is employed. In this paper, a novel adaptive\nmorphological reconstruction (AMR) operation is proposed that has three\nadvantages. Firstly, AMR can adaptively filter useless seeds while preserving\nmeaningful ones. Secondly, AMR is insensitive to the scale of structuring\nelements because multiscale structuring elements are employed. Finally, AMR has\ntwo attractive properties: monotonic increasingness and convergence that help\nseeded segmentation algorithms to achieve a hierarchical segmentation.\nExperiments clearly demonstrate that AMR is useful for improving algorithms of\nseeded image segmentation and seed-based spectral segmentation. Compared to\nseveral state-of-the-art algorithms, the proposed algorithms provide better\nsegmentation results requiring less computing time. Source code is available at\nhttps://github.com/SUST-reynole/AMR.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:56:07 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Lei", "Tao", ""], ["Jia", "Xiaohong", ""], ["Liu", "Tongliang", ""], ["Liu", "Shigang", ""], ["Meng", "Hongying", ""], ["Nandi", "Asoke K.", ""]]}, {"id": "1904.03977", "submitter": "Prerana Mukherjee", "authors": "Divyam Madaan, Radhika Dua, Prerana Mukherjee and Brejesh Lall", "title": "VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution is the leading environmental health hazard globally due to\nvarious sources which include factory emissions, car exhaust and cooking\nstoves. As a precautionary measure, air pollution forecast serves as the basis\nfor taking effective pollution control measures, and accurate air pollution\nforecasting has become an important task. In this paper, we forecast\nfine-grained ambient air quality information for 5 prominent locations in Delhi\nbased on the historical and real-time ambient air quality and meteorological\ndata reported by Central Pollution Control board. We present VayuAnukulani\nsystem, a novel end-to-end solution to predict air quality for next 24 hours by\nestimating the concentration and level of different air pollutants including\nnitrogen dioxide ($NO_2$), particulate matter ($PM_{2.5}$ and $PM_{10}$) for\nDelhi. Extensive experiments on data sources obtained in Delhi demonstrate that\nthe proposed adaptive attention based Bidirectional LSTM Network outperforms\nseveral baselines for classification and regression models. The accuracy of the\nproposed adaptive system is $\\sim 15 - 20\\%$ better than the same offline\ntrained model. We compare the proposed methodology on several competing\nbaselines, and show that the network outperforms conventional methods by $\\sim\n3 - 5 \\%$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:02:03 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Madaan", "Divyam", ""], ["Dua", "Radhika", ""], ["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""]]}, {"id": "1904.03982", "submitter": "Lefei Zhang", "authors": "Lefei Zhang, Qian Zhang, Bo Du, Xin Huang, Yuan Yan Tang, Dacheng Tao", "title": "Simultaneous Spectral-Spatial Feature Selection and Extraction for\n  Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hyperspectral remote sensing data mining, it is important to take into\naccount of both spectral and spatial information, such as the spectral\nsignature, texture feature and morphological property, to improve the\nperformances, e.g., the image classification accuracy. In a feature\nrepresentation point of view, a nature approach to handle this situation is to\nconcatenate the spectral and spatial features into a single but high\ndimensional vector and then apply a certain dimension reduction technique\ndirectly on that concatenated vector before feed it into the subsequent\nclassifier. However, multiple features from various domains definitely have\ndifferent physical meanings and statistical properties, and thus such\nconcatenation hasn't efficiently explore the complementary properties among\ndifferent features, which should benefit for boost the feature\ndiscriminability. Furthermore, it is also difficult to interpret the\ntransformed results of the concatenated vector. Consequently, finding a\nphysically meaningful consensus low dimensional feature representation of\noriginal multiple features is still a challenging task. In order to address the\nthese issues, we propose a novel feature learning framework, i.e., the\nsimultaneous spectral-spatial feature selection and extraction algorithm, for\nhyperspectral images spectral-spatial feature representation and\nclassification. Specifically, the proposed method learns a latent low\ndimensional subspace by projecting the spectral-spatial feature into a common\nfeature space, where the complementary information has been effectively\nexploited, and simultaneously, only the most significant original features have\nbeen transformed. Encouraging experimental results on three public available\nhyperspectral remote sensing datasets confirm that our proposed method is\neffective and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:05:59 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Lefei", ""], ["Zhang", "Qian", ""], ["Du", "Bo", ""], ["Huang", "Xin", ""], ["Tang", "Yuan Yan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.03983", "submitter": "Ccsd", "authors": "Adrien Nivaggioli, Hicham Randrianarivo (CEDRIC)", "title": "Weakly Supervised Semantic Segmentation of Satellite Images", "comments": null, "journal-ref": "Joint Urban Remote Sensing Event (JURSE), May 2019, Vannes, France", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When one wants to train a neural network to perform semantic segmentation,\ncreating pixel-level annotations for each of the images in the database is a\ntedious task. If he works with aerial or satellite images, which are usually\nvery large, it is even worse. With that in mind, we investigate how to use\nimage-level annotations in order to perform semantic segmentation. Image-level\nannotations are much less expensive to acquire than pixel-level annotations,\nbut we lose a lot of information for the training of the model. From the\nannotations of the images, the model must find by itself how to classify the\ndifferent regions of the image. In this work, we use the method proposed by Anh\nand Kwak [1] to produce pixel-level annotation from image level annotation. We\ncompare the overall quality of our generated dataset with the original dataset.\nIn addition, we propose an adaptation of the AffinityNet that allows us to\ndirectly perform a semantic segmentation. Our results show that the generated\nlabels lead to the same performances for the training of several segmentation\nnetworks. Also, the quality of semantic segmentation performed directly by the\nAffinityNet and the Random Walk is close to the one of the best\nfully-supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:09:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Nivaggioli", "Adrien", "", "CEDRIC"], ["Randrianarivo", "Hicham", "", "CEDRIC"]]}, {"id": "1904.04036", "submitter": "Jia Liu", "authors": "Jia Liu, Maoguo Gong, Haibo He", "title": "Nucleus Neural Network: A Data-driven Self-organized Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial neural networks which are inspired from the learning mechanism of\nbrain have achieved great successes in many problems, especially those with\ndeep layers. In this paper, we propose a nucleus neural network (NNN) and\ncorresponding connecting architecture learning method. In a nucleus, there are\nno regular layers, i.e., a neuron may connect to all the neurons in the\nnucleus. This type of architecture gets rid of layer limitation and may lead to\nmore powerful learning capability. It is crucial to determine the connections\nbetween them given numerous neurons. Based on the principle that more relevant\ninput and output neuron pair deserves higher connecting density, we propose an\nefficient architecture learning model for the nucleus. Moreover, we improve the\nlearning method for connecting weights and biases given the optimized\narchitecture. We find that this novel architecture is robust to irrelevant\ncomponents in test data. So we reconstruct a new dataset based on the MNIST\ndataset where the types of digital backgrounds in training and test sets are\ndifferent. Experiments demonstrate that the proposed learner achieves\nsignificant improvement over traditional learners on the reconstructed data\nset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:03:50 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 14:06:05 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Liu", "Jia", ""], ["Gong", "Maoguo", ""], ["He", "Haibo", ""]]}, {"id": "1904.04061", "submitter": "Yong Luo", "authors": "Yong Luo, Yonggang Wen, Tongliang Liu, Dacheng Tao", "title": "Transferring Knowledge Fragments for Learning Distance Metric from A\n  Heterogeneous Domain", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (Volume: 41, Issue: 4, April 1 2019)", "doi": "10.1109/TPAMI.2018.2824309", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of transfer learning is to improve the performance of target\nlearning task by leveraging information (or transferring knowledge) from other\nrelated tasks. In this paper, we examine the problem of transfer distance\nmetric learning (DML), which usually aims to mitigate the label information\ndeficiency issue in the target DML. Most of the current Transfer DML (TDML)\nmethods are not applicable to the scenario where data are drawn from\nheterogeneous domains. Some existing heterogeneous transfer learning (HTL)\napproaches can learn target distance metric by usually transforming the samples\nof source and target domain into a common subspace. However, these approaches\nlack flexibility in real-world applications, and the learned transformations\nare often restricted to be linear. This motivates us to develop a general\nflexible heterogeneous TDML (HTDML) framework. In particular, any\n(linear/nonlinear) DML algorithms can be employed to learn the source metric\nbeforehand. Then the pre-learned source metric is represented as a set of\nknowledge fragments to help target metric learning. We show how generalization\nerror in the target domain could be reduced using the proposed transfer\nstrategy, and develop novel algorithm to learn either linear or nonlinear\ntarget metric. Extensive experiments on various applications demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:44:22 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.04084", "submitter": "Zixin Luo", "authors": "Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li,\n  Tian Fang, Long Quan", "title": "ContextDesc: Local Descriptor Augmentation with Cross-Modality Context", "comments": "Accepted to CVPR 2019 (oral), supplementary materials included.\n  (https://github.com/lzx551402/contextdesc)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing studies on learning local features focus on the patch-based\ndescriptions of individual keypoints, whereas neglecting the spatial relations\nestablished from their keypoint locations. In this paper, we go beyond the\nlocal detail representation by introducing context awareness to augment\noff-the-shelf local feature descriptors. Specifically, we propose a unified\nlearning framework that leverages and aggregates the cross-modality contextual\ninformation, including (i) visual context from high-level image representation,\nand (ii) geometric context from 2D keypoint distribution. Moreover, we propose\nan effective N-pair loss that eschews the empirical hyper-parameter search and\nimproves the convergence. The proposed augmentation scheme is lightweight\ncompared with the raw local feature description, meanwhile improves remarkably\non several large-scale benchmarks with diversified scenes, which demonstrates\nboth strong practicality and generalization ability in geometric matching\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:12:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Zixin", ""], ["Shen", "Tianwei", ""], ["Zhou", "Lei", ""], ["Zhang", "Jiahui", ""], ["Yao", "Yao", ""], ["Li", "Shiwei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1904.04088", "submitter": "Yong Luo", "authors": "Yong Luo, Yonggang Wen, Dacheng Tao, Jie Gui, Chao Xu", "title": "Large Margin Multi-modal Multi-task Feature Extraction for Image\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (Volume: 25, Issue: 1, Jan.\n  2016)", "doi": "10.1109/TIP.2015.2495116", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The features used in many image analysis-based applications are frequently of\nvery high dimension. Feature extraction offers several advantages in\nhigh-dimensional cases, and many recent studies have used multi-task feature\nextraction approaches, which often outperform single-task feature extraction\napproaches. However, most of these methods are limited in that they only\nconsider data represented by a single type of feature, even though features\nusually represent images from multiple modalities. We therefore propose a novel\nlarge margin multi-modal multi-task feature extraction (LM3FE) framework for\nhandling multi-modal features for image classification. In particular, LM3FE\nsimultaneously learns the feature extraction matrix for each modality and the\nmodality combination coefficients. In this way, LM3FE not only handles\ncorrelated and noisy features, but also utilizes the complementarity of\ndifferent modalities to further help reduce feature redundancy in each\nmodality. The large margin principle employed also helps to extract strongly\npredictive features so that they are more suitable for prediction (e.g.,\nclassification). An alternating algorithm is developed for problem optimization\nand each sub-problem can be efficiently solved. Experiments on two challenging\nreal-world image datasets demonstrate the effectiveness and superiority of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:14:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Tao", "Dacheng", ""], ["Gui", "Jie", ""], ["Xu", "Chao", ""]]}, {"id": "1904.04092", "submitter": "Jingjing Li", "authors": "Jingjing Li, Mengmeng Jin, Ke Lu, Zhengming Ding, Lei Zhu, Zi Huang", "title": "Leveraging the Invariant Side of Generative Zero-Shot Learning", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional zero-shot learning (ZSL) methods generally learn an embedding,\ne.g., visual-semantic mapping, to handle the unseen visual samples via an\nindirect manner. In this paper, we take the advantage of generative adversarial\nnetworks (GANs) and propose a novel method, named leveraging invariant side GAN\n(LisGAN), which can directly generate the unseen features from random noises\nwhich are conditioned by the semantic descriptions. Specifically, we train a\nconditional Wasserstein GANs in which the generator synthesizes fake unseen\nfeatures from noises and the discriminator distinguishes the fake from real via\na minimax game. Considering that one semantic description can correspond to\nvarious synthesized visual samples, and the semantic description, figuratively,\nis the soul of the generated features, we introduce soul samples as the\ninvariant side of generative zero-shot learning in this paper. A soul sample is\nthe meta-representation of one class. It visualizes the most\nsemantically-meaningful aspects of each sample in the same category. We\nregularize that each generated sample (the varying side of generative ZSL)\nshould be close to at least one soul sample (the invariant side) which has the\nsame class label with it. At the zero-shot recognition stage, we propose to use\ntwo classifiers, which are deployed in a cascade way, to achieve a\ncoarse-to-fine result. Experiments on five popular benchmarks verify that our\nproposed approach can outperform state-of-the-art methods with significant\nimprovements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:28:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Jingjing", ""], ["Jin", "Mengmeng", ""], ["Lu", "Ke", ""], ["Ding", "Zhengming", ""], ["Zhu", "Lei", ""], ["Huang", "Zi", ""]]}, {"id": "1904.04094", "submitter": "David Griffiths Mr", "authors": "David Griffiths, Jan Boehm", "title": "Weighted Point Cloud Augmentation for Neural Network Training Data\n  Class-Imbalance", "comments": "7 pages, 6 figures, submitted for ISPRS Geospatial Week conference\n  2019", "journal-ref": null, "doi": "10.5194/isprs-archives-XLII-2-W13-981-2019", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent developments in the field of deep learning for 3D data have\ndemonstrated promising potential for end-to-end learning directly from point\nclouds. However, many real-world point clouds contain a large class im-balance\ndue to the natural class im-balance observed in nature. For example, a 3D scan\nof an urban environment will consist mostly of road and facade, whereas other\nobjects such as poles will be under-represented. In this paper we address this\nissue by employing a weighted augmentation to increase classes that contain\nfewer points. By mitigating the class im-balance present in the data we\ndemonstrate that a standard PointNet++ deep neural network can achieve higher\nperformance at inference on validation data. This was observed as an increase\nof F1 score of 19% and 25% on two test benchmark datasets; ScanNet and\nSemantic3D respectively where no class im-balance pre-processing had been\nperformed. Our networks performed better on both highly-represented and\nunder-represented classes, which indicates that the network is learning more\nrobust and meaningful features when the loss function is not overly exposed to\nonly a few classes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:32:27 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 07:31:37 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Griffiths", "David", ""], ["Boehm", "Jan", ""]]}, {"id": "1904.04144", "submitter": "Fabio Tosi", "authors": "Fabio Tosi, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia", "title": "Learning monocular depth estimation infusing traditional stereo\n  knowledge", "comments": "accepted at CVPR 2019. Code available at\n  https://github.com/fabiotosi92/monoResMatch-Tensorflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from a single image represents a fascinating, yet\nchallenging problem with countless applications. Recent works proved that this\ntask could be learned without direct supervision from ground truth labels\nleveraging image synthesis on sequences or stereo pairs. Focusing on this\nsecond case, in this paper we leverage stereo matching in order to improve\nmonocular depth estimation. To this aim we propose monoResMatch, a novel deep\narchitecture designed to infer depth from a single input image by synthesizing\nfeatures from a different point of view, horizontally aligned with the input\nimage, performing stereo matching between the two cues. In contrast to previous\nworks sharing this rationale, our network is the first trained end-to-end from\nscratch. Moreover, we show how obtaining proxy ground truth annotation through\ntraditional stereo algorithms, such as Semi-Global Matching, enables more\naccurate monocular depth estimation still countering the need for expensive\ndepth labels by keeping a self-supervised approach. Exhaustive experimental\nresults prove how the synergy between i) the proposed monoResMatch architecture\nand ii) proxy-supervision attains state-of-the-art for self-supervised\nmonocular depth estimation. The code is publicly available at\nhttps://github.com/fabiotosi92/monoResMatch-Tensorflow.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 15:59:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tosi", "Fabio", ""], ["Aleotti", "Filippo", ""], ["Poggi", "Matteo", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1904.04158", "submitter": "Mohammad Tofighi", "authors": "Yuelong Li, Mohammad Tofighi, and Vishal Monga", "title": "Robust Alignment for Panoramic Stitching via an Exact Rank Constraint", "comments": "Accepted for publication in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2909800", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of image alignment for panoramic stitching. Unlike most\nexisting approaches that are feature-based, our algorithm works on pixels\ndirectly, and accounts for errors across the whole images globally.\nTechnically, we formulate the alignment problem as rank-1 and sparse matrix\ndecomposition over transformed images, and develop an efficient algorithm for\nsolving this challenging non-convex optimization problem. The algorithm reduces\nto solving a sequence of subproblems, where we analytically establish exact\nrecovery conditions, convergence and optimality, together with convergence rate\nand complexity. We generalize it to simultaneously align multiple images and\nrecover multiple homographies, extending its application scope towards vast\nmajority of practical scenarios. Experimental results demonstrate that the\nproposed algorithm is capable of more accurately aligning the images and\ngenerating higher quality stitched images than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:25:08 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Li", "Yuelong", ""], ["Tofighi", "Mohammad", ""], ["Monga", "Vishal", ""]]}, {"id": "1904.04166", "submitter": "Yu Wu", "authors": "Yu Wu, Lu Jiang, Yi Yang", "title": "Revisiting EmbodiedQA: A Simple Baseline and Beyond", "comments": "Accepted to IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2020.2967584", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Embodied Question Answering (EmbodiedQA), an agent interacts with an\nenvironment to gather necessary information for answering user questions.\nExisting works have laid a solid foundation towards solving this interesting\nproblem. But the current performance, especially in navigation, suggests that\nEmbodiedQA might be too challenging for the contemporary approaches. In this\npaper, we empirically study this problem and introduce 1) a simple yet\neffective baseline that achieves promising performance; 2) an easier and\npractical setting for EmbodiedQA where an agent has a chance to adapt the\ntrained model to a new environment before it actually answers users questions.\nIn this new setting, we randomly place a few objects in new environments, and\nupgrade the agent policy by a distillation network to retain the generalization\nability from the trained model. On the EmbodiedQA v1 benchmark, under the\nstandard setting, our simple baseline achieves very competitive results to\nthe-state-of-the-art; in the new setting, we found the introduced small change\nin settings yields a notable gain in navigation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:23:24 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 15:19:25 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Wu", "Yu", ""], ["Jiang", "Lu", ""], ["Yang", "Yi", ""]]}, {"id": "1904.04175", "submitter": "Michael Kellman", "authors": "Michael Kellman, Emrah Bostan, Michael Chen, Laura Waller", "title": "Data-Driven Design for Fourier Ptychographic Microscopy", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier Ptychographic Microscopy (FPM) is a computational imaging method that\nis able to super-resolve features beyond the diffraction-limit set by the\nobjective lens of a traditional microscope. This is accomplished by using\nsynthetic aperture and phase retrieval algorithms to combine many measurements\ncaptured by an LED array microscope with programmable source patterns. FPM\nprovides simultaneous large field-of-view and high resolution imaging, but at\nthe cost of reduced temporal resolution, thereby limiting live cell\napplications. In this work, we learn LED source pattern designs that compress\nthe many required measurements into only a few, with negligible loss in\nreconstruction quality or resolution. This is accomplished by recasting the\nsuper-resolution reconstruction as a Physics-based Neural Network and learning\nthe experimental design to optimize the network's overall performance.\nSpecifically, we learn LED patterns for different applications (e.g. amplitude\ncontrast and quantitative phase imaging) and show that the designs we learn\nthrough simulation generalize well in the experimental setting. Further, we\ndiscuss a context-specific loss function, practical memory limitations, and\ninterpretability of our learned designs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:30:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kellman", "Michael", ""], ["Bostan", "Emrah", ""], ["Chen", "Michael", ""], ["Waller", "Laura", ""]]}, {"id": "1904.04189", "submitter": "Anna Kukleva", "authors": "Anna Kukleva, Hilde Kuehne, Fadime Sener, Juergen Gall", "title": "Unsupervised learning of action classes with continuous temporal\n  embedding", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of temporally detecting and segmenting actions in untrimmed videos\nhas seen an increased attention recently. One problem in this context arises\nfrom the need to define and label action boundaries to create annotations for\ntraining which is very time and cost intensive. To address this issue, we\npropose an unsupervised approach for learning action classes from untrimmed\nvideo sequences. To this end, we use a continuous temporal embedding of\nframewise features to benefit from the sequential nature of activities. Based\non the latent space created by the embedding, we identify clusters of temporal\nsegments across all videos that correspond to semantic meaningful action\nclasses. The approach is evaluated on three challenging datasets, namely the\nBreakfast dataset, YouTube Instructions, and the 50Salads dataset. While\nprevious works assumed that the videos contain the same high level activity, we\nfurthermore show that the proposed approach can also be applied to a more\ngeneral setting where the content of the videos is unknown.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:05:31 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kukleva", "Anna", ""], ["Kuehne", "Hilde", ""], ["Sener", "Fadime", ""], ["Gall", "Juergen", ""]]}, {"id": "1904.04195", "submitter": "Hao Tan", "authors": "Hao Tan, Licheng Yu, Mohit Bansal", "title": "Learning to Navigate Unseen Environments: Back Translation with\n  Environmental Dropout", "comments": "NAACL 2019 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grand goal in AI is to build a robot that can accurately navigate based on\nnatural language instructions, which requires the agent to perceive the scene,\nunderstand and ground language, and act in the real-world environment. One key\nchallenge here is to learn to navigate in new environments that are unseen\nduring training. Most of the existing approaches perform dramatically worse in\nunseen environments as compared to seen ones. In this paper, we present a\ngeneralizable navigational agent. Our agent is trained in two stages. The first\nstage is training via mixed imitation and reinforcement learning, combining the\nbenefits from both off-policy and on-policy optimization. The second stage is\nfine-tuning via newly-introduced 'unseen' triplets (environment, path,\ninstruction). To generate these unseen triplets, we propose a simple but\neffective 'environmental dropout' method to mimic unseen environments, which\novercomes the problem of limited seen environment variability. Next, we apply\nsemi-supervised learning (via back-translation) on these dropped-out\nenvironments to generate new paths and instructions. Empirically, we show that\nour agent is substantially better at generalizability when fine-tuned with\nthese triplets, outperforming the state-of-art approaches by a large margin on\nthe private unseen test set of the Room-to-Room task, and achieving the top\nrank on the leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:14:52 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tan", "Hao", ""], ["Yu", "Licheng", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.04196", "submitter": "Seungryul Baek", "authors": "Seungryul Baek, Kwang In Kim, Tae-Kyun Kim", "title": "Pushing the Envelope for RGB-based Dense 3D Hand Pose Estimation via\n  Neural Rendering", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D hand meshes from single RGB images is challenging, due to\nintrinsic 2D-3D mapping ambiguities and limited training data. We adopt a\ncompact parametric 3D hand model that represents deformable and articulated\nhand meshes. To achieve the model fitting to RGB images, we investigate and\ncontribute in three ways: 1) Neural rendering: inspired by recent work on human\nbody, our hand mesh estimator (HME) is implemented by a neural network and a\ndifferentiable renderer, supervised by 2D segmentation masks and 3D skeletons.\nHME demonstrates good performance for estimating diverse hand shapes and\nimproves pose estimation accuracies. 2) Iterative testing refinement: Our\nfitting function is differentiable. We iteratively refine the initial estimate\nusing the gradients, in the spirit of iterative model fitting methods like ICP.\nThe idea is supported by the latest research on human body. 3) Self-data\naugmentation: collecting sized RGB-mesh (or segmentation mask)-skeleton\ntriplets for training is a big hurdle. Once the model is successfully fitted to\ninput RGB images, its meshes i.e. shapes and articulations, are realistic, and\nwe augment view-points on top of estimated dense hand poses. Experiments using\nthree RGB-based benchmarks show that our framework offers beyond\nstate-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D\nhand shapes. Each technical component above meaningfully improves the accuracy\nin the ablation study.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:15:55 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 15:57:08 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Baek", "Seungryul", ""], ["Kim", "Kwang In", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1904.04202", "submitter": "Lex Fridman", "authors": "Jack Terwilliger, Michael Glazer, Henri Schmidt, Josh Domeyer,\n  Heishiro Toyoda, Bruce Mehler, Bryan Reimer, Lex Fridman", "title": "Dynamics of Pedestrian Crossing Decisions Based on Vehicle Trajectories\n  in Large-Scale Simulated and Real-World Data", "comments": "Will appear in Proceedings of 2019 Driving Assessment Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans, as both pedestrians and drivers, generally skillfully navigate\ntraffic intersections. Despite the uncertainty, danger, and the non-verbal\nnature of communication commonly found in these interactions, there are\nsurprisingly few collisions considering the total number of interactions. As\nthe role of automation technology in vehicles grows, it becomes increasingly\ncritical to understand the relationship between pedestrian and driver behavior:\nhow pedestrians perceive the actions of a vehicle/driver and how pedestrians\nmake crossing decisions. The relationship between time-to-arrival (TTA) and\npedestrian gap acceptance (i.e., whether a pedestrian chooses to cross under a\ngiven window of time to cross) has been extensively investigated. However, the\ndynamic nature of vehicle trajectories in the context of non-verbal\ncommunication has not been systematically explored. Our work provides evidence\nthat trajectory dynamics, such as changes in TTA, can be powerful signals in\nthe non-verbal communication between drivers and pedestrians. Moreover, we\ninvestigate these effects in both simulated and real-world datasets, both\nlarger than have previously been considered in literature to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:19:54 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Terwilliger", "Jack", ""], ["Glazer", "Michael", ""], ["Schmidt", "Henri", ""], ["Domeyer", "Josh", ""], ["Toyoda", "Heishiro", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""], ["Fridman", "Lex", ""]]}, {"id": "1904.04205", "submitter": "Hoel Kervadec", "authors": "Hoel Kervadec, Jose Dolz, Jing Yuan, Christian Desrosiers, Eric\n  Granger, Ismail Ben Ayed", "title": "Constrained Deep Networks: Lagrangian Optimization via Log-Barrier\n  Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the optimization aspects of imposing hard inequality\nconstraints on the outputs of CNNs. In the context of deep networks,\nconstraints are commonly handled with penalties for their simplicity, and\ndespite their well-known limitations. Lagrangian-dual optimization has been\nlargely avoided, except for a few recent works, mainly due to the computational\ncomplexity and stability/convergence issues caused by alternating explicit dual\nupdates/projections and stochastic optimization. Several studies showed that,\nsurprisingly for deep CNNs, the theoretical and practical advantages of\nLagrangian optimization over penalties do not materialize in practice. We\npropose log-barrier extensions, which approximate Lagrangian optimization of\nconstrained-CNN problems with a sequence of unconstrained losses. Unlike\nstandard interior-point and log-barrier methods, our formulation does not need\nan initial feasible solution. Furthermore, we provide a new technical result,\nwhich shows that the proposed extensions yield an upper bound on the duality\ngap. This generalizes the duality-gap result of standard log-barriers, yielding\nsub-optimality certificates for feasible solutions. While sub-optimality is not\nguaranteed for non-convex problems, our result shows that log-barrier\nextensions are a principled way to approximate Lagrangian optimization for\nconstrained CNNs via implicit dual variables. We report comprehensive weakly\nsupervised segmentation experiments, with various constraints, showing that our\nformulation outperforms substantially the existing constrained-CNN methods,\nboth in terms of accuracy, constraint satisfaction and training stability, more\nso when dealing with a large number of constraints.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:25:46 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 20:27:22 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 10:34:39 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 22:05:40 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Kervadec", "Hoel", ""], ["Dolz", "Jose", ""], ["Yuan", "Jing", ""], ["Desrosiers", "Christian", ""], ["Granger", "Eric", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1904.04218", "submitter": "Niladri Das", "authors": "Sk. Miraj Ahmed, Niladri Ranjan Das and Kunal Narayan Chaudhury", "title": "Least-squares registration of point sets over SE (d) using closed-form\n  projections", "comments": "To appear in Computer Vision and Image Understanding (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of registering multiple point sets in some\n$d$-dimensional space using rotations and translations. Assume that there are\nsets with common points, and moreover the pairwise correspondences are known\nfor such sets. We consider a least-squares formulation of this problem, where\nthe variables are the transforms associated with the point sets. The present\nnovelty is that we reduce this nonconvex problem to an optimization over the\npositive semidefinite cone, where the objective is linear but the constraints\nare nevertheless nonconvex. We propose to solve this using variable splitting\nand the alternating directions method of multipliers (ADMM). Due to the\nlinearity of the objective and the structure of constraints, the ADMM\nsubproblems are given by projections with closed-form solutions. In particular,\nfor $m$ point sets, the dominant cost per iteration is the partial\neigendecomposition of an $md \\times md$ matrix, and $m-1$ singular value\ndecompositions of $d \\times d$ matrices. We empirically show that for\nappropriate parameter settings, the proposed solver has a large convergence\nbasin and is stable under perturbations. As applications, we use our method for\n$2$D shape matching and $3$D multiview registration. In either application, we\nmodel the shapes/scans as point sets and determine the pairwise correspondences\nusing ICP. In particular, our algorithm compares favorably with existing\nmethods for multiview reconstruction in terms of timing and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:38:37 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 10:12:50 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Ahmed", "Sk. Miraj", ""], ["Das", "Niladri Ranjan", ""], ["Chaudhury", "Kunal Narayan", ""]]}, {"id": "1904.04231", "submitter": "Chen Sun", "authors": "Chen Sun and Abhinav Shrivastava and Carl Vondrick and Rahul\n  Sukthankar and Kevin Murphy and Cordelia Schmid", "title": "Relational Action Forecasting", "comments": "CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on multi-person action forecasting in videos. More\nprecisely, given a history of H previous frames, the goal is to detect actors\nand to predict their future actions for the next T frames. Our approach jointly\nmodels temporal and spatial interactions among different actors by constructing\na recurrent graph, using actor proposals obtained with Faster R-CNN as nodes.\nOur method learns to select a subset of discriminative relations without\nrequiring explicit supervision, thus enabling us to tackle challenging visual\ndata. We refer to our model as Discriminative Relational Recurrent Network\n(DRRN). Evaluation of action prediction on AVA demonstrates the effectiveness\nof our proposed method compared to simpler baselines. Furthermore, we\nsignificantly improve performance on the task of early action classification on\nJ-HMDB, from the previous SOTA of 48% to 60%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:57:27 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sun", "Chen", ""], ["Shrivastava", "Abhinav", ""], ["Vondrick", "Carl", ""], ["Sukthankar", "Rahul", ""], ["Murphy", "Kevin", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1904.04232", "submitter": "Jia-Bin Huang", "authors": "Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin\n  Huang", "title": "A Closer Look at Few-shot Classification", "comments": "ICLR 2019. Code: https://github.com/wyharveychen/CloserLookFewShot .\n  Project: https://sites.google.com/view/a-closer-look-at-few-shot/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification aims to learn a classifier to recognize unseen\nclasses during training with limited labeled examples. While significant\nprogress has been made, the growing complexity of network designs,\nmeta-learning algorithms, and differences in implementation details make a fair\ncomparison difficult. In this paper, we present 1) a consistent comparative\nanalysis of several representative few-shot classification algorithms, with\nresults showing that deeper backbones significantly reduce the performance\ndifferences among methods on datasets with limited domain differences, 2) a\nmodified baseline method that surprisingly achieves competitive performance\nwhen compared with the state-of-the-art on both the \\miniI and the CUB\ndatasets, and 3) a new experimental setting for evaluating the cross-domain\ngeneralization ability for few-shot classification algorithms. Our results\nreveal that reducing intra-class variation is an important factor when the\nfeature backbone is shallow, but not as critical when using deeper backbones.\nIn a realistic cross-domain evaluation setting, we show that a baseline method\nwith a standard fine-tuning practice compares favorably against other\nstate-of-the-art few-shot learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:59:07 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 16:25:06 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Chen", "Wei-Yu", ""], ["Liu", "Yen-Cheng", ""], ["Kira", "Zsolt", ""], ["Wang", "Yu-Chiang Frank", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1904.04241", "submitter": "Fatemeh Shiri", "authors": "Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, Piotr Koniusz", "title": "Identity-preserving Face Recovery from Stylized Portraits", "comments": "International Journal of Computer Vision 2019. arXiv admin note:\n  substantial text overlap with arXiv:1801.02279", "journal-ref": null, "doi": "10.1007/s11263-019-01169-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an artistic portrait, recovering the latent photorealistic face that\npreserves the subject's identity is challenging because the facial details are\noften distorted or fully lost in artistic portraits. We develop an\nIdentity-preserving Face Recovery from Portraits (IFRP) method that utilizes a\nStyle Removal network (SRN) and a Discriminative Network (DN). Our SRN,\ncomposed of an autoencoder with residual block-embedded skip connections, is\ndesigned to transfer feature maps of stylized images to the feature maps of the\ncorresponding photorealistic faces. Owing to the Spatial Transformer Network\n(STN), SRN automatically compensates for misalignments of stylized portraits to\noutput aligned realistic face images. To ensure the identity preservation, we\npromote the recovered and ground truth faces to share similar visual features\nvia a distance measure which compares features of recovered and ground truth\nfaces extracted from a pre-trained FaceNet network. DN has multiple\nconvolutional and fully-connected layers, and its role is to enforce recovered\nfaces to be similar to authentic faces. Thus, we can recover high-quality\nphotorealistic faces from unaligned portraits while preserving the identity of\nthe face in an image. By conducting extensive evaluations on a large-scale\nsynthesized dataset and a hand-drawn sketch dataset, we demonstrate that our\nmethod achieves superior face recovery and attains state-of-the-art results. In\naddition, our method can recover photorealistic faces from unseen stylized\nportraits, artistic paintings, and hand-drawn sketches.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 09:18:59 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Shiri", "Fatemeh", ""], ["Yu", "Xin", ""], ["Porikli", "Fatih", ""], ["Hartley", "Richard", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1904.04272", "submitter": "Martin Engilberge", "authors": "Martin Engilberge, Louis Chevallier, Patrick P\\'erez, Matthieu Cord", "title": "SoDeep: a Sorting Deep net to learn ranking loss surrogates", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several tasks in machine learning are evaluated using non-differentiable\nmetrics such as mean average precision or Spearman correlation. However, their\nnon-differentiability prevents from using them as objective functions in a\nlearning framework. Surrogate and relaxation methods exist but tend to be\nspecific to a given metric.\n  In the present work, we introduce a new method to learn approximations of\nsuch non-differentiable objective functions. Our approach is based on a deep\narchitecture that approximates the sorting of arbitrary sets of scores. It is\ntrained virtually for free using synthetic data. This sorting deep (SoDeep) net\ncan then be combined in a plug-and-play manner with existing deep\narchitectures. We demonstrate the interest of our approach in three different\ntasks that require ranking: Cross-modal text-image retrieval, multi-label image\nclassification and visual memorability ranking. Our approach yields very\ncompetitive results on these three tasks, which validates the merit and the\nflexibility of SoDeep as a proxy for sorting operation in ranking-based losses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:02:43 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Engilberge", "Martin", ""], ["Chevallier", "Louis", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "1904.04281", "submitter": "Tolga Birdal", "authors": "Haowen Deng and Tolga Birdal and Slobodan Ilic", "title": "3D Local Features for Direct Pairwise Registration", "comments": "To appear in CVPR 2019. 16 pages, identical to the camera ready\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, data driven approach for solving the problem of\nregistration of two point cloud scans. Our approach is direct in the sense that\na single pair of corresponding local patches already provides the necessary\ntransformation cue for the global registration. To achieve that, we first endow\nthe state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling,\nwhere the discrepancy between the two leads to pose-specific descriptors. Based\nupon this, we introduce RelativeNet, a relative pose estimation network to\nassign correspondence-specific orientations to the keypoints, eliminating any\nlocal reference frame computations. Finally, we devise a simple yet effective\nhypothesize-and-verify algorithm to quickly use the predictions and align two\npoint sets. Our extensive quantitative and qualitative experiments suggests\nthat our approach outperforms the state of the art in challenging real datasets\nof pairwise registration and that augmenting the keypoints with local pose\ninformation leads to better generalization and a dramatic speed-up.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:17:36 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Deng", "Haowen", ""], ["Birdal", "Tolga", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1904.04289", "submitter": "Bruno Korbar", "authors": "Bruno Korbar, Du Tran, Lorenzo Torresani", "title": "SCSampler: Sampling Salient Clips from Video for Efficient Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many action recognition datasets consist of collections of brief,\ntrimmed videos each containing a relevant action, videos in the real-world\n(e.g., on YouTube) exhibit very different properties: they are often several\nminutes long, where brief relevant clips are often interleaved with segments of\nextended duration containing little change. Applying densely an action\nrecognition system to every temporal clip within such videos is prohibitively\nexpensive. Furthermore, as we show in our experiments, this results in\nsuboptimal recognition accuracy as informative predictions from relevant clips\nare outnumbered by meaningless classification outputs over long uninformative\nsections of the video. In this paper we introduce a lightweight \"clip-sampling\"\nmodel that can efficiently identify the most salient temporal clips within a\nlong video. We demonstrate that the computational cost of action recognition on\nuntrimmed videos can be dramatically reduced by invoking recognition only on\nthese most salient clips. Furthermore, we show that this yields significant\ngains in recognition accuracy compared to analysis of all clips or\nrandomly/uniformly selected clips. On Sports1M, our clip sampling scheme\nelevates the accuracy of an already state-of-the-art action classifier by 7%\nand reduces by more than 15 times its computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:28:12 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 12:15:46 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Korbar", "Bruno", ""], ["Tran", "Du", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1904.04290", "submitter": "Ricardo Martin Brualla", "authors": "Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit\n  Pandey, Noah Snavely, Ricardo Martin-Brualla", "title": "Neural Rerendering in the Wild", "comments": "To be presented at CVPR 2019 (oral). Supplementary video available at\n  http://youtu.be/E1crWQn_kmY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore total scene capture -- recording, modeling, and rerendering a\nscene under varying appearance such as season and time of day. Starting from\ninternet photos of a tourist landmark, we apply traditional 3D reconstruction\nto register the photos and approximate the scene as a point cloud. For each\nphoto, we render the scene points into a deep framebuffer, and train a neural\nnetwork to learn the mapping of these initial renderings to the actual photos.\nThis rerendering network also takes as input a latent appearance vector and a\nsemantic mask indicating the location of transient objects like pedestrians.\nThe model is evaluated on several datasets of publicly available images\nspanning a broad range of illumination conditions. We create short videos\ndemonstrating realistic manipulation of the image viewpoint, appearance, and\nsemantic labeling. We also compare results with prior work on scene\nreconstruction from internet photos.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:30:14 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Meshry", "Moustafa", ""], ["Goldman", "Dan B", ""], ["Khamis", "Sameh", ""], ["Hoppe", "Hugues", ""], ["Pandey", "Rohit", ""], ["Snavely", "Noah", ""], ["Martin-Brualla", "Ricardo", ""]]}, {"id": "1904.04297", "submitter": "Naoufel Werghi Dr.", "authors": "Bilal Taha, Munawar Hayat, Stefano Berretti, Naoufel Werghi", "title": "Learned 3D Shape Representations Using Fused Geometrically Augmented\n  Images: Application to Facial Expression and Action Unit Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to learn generic multi-modal mesh surface\nrepresentations using a novel scheme for fusing texture and geometric data. Our\napproach defines an inverse mapping between different geometric descriptors\ncomputed on the mesh surface or its down-sampled version, and the corresponding\n2D texture image of the mesh, allowing the construction of fused geometrically\naugmented images (FGAI). This new fused modality enables us to learn feature\nrepresentations from 3D data in a highly efficient manner by simply employing\nstandard convolutional neural networks in a transfer-learning mode. In contrast\nto existing methods, the proposed approach is both computationally and memory\nefficient, preserves intrinsic geometric information and learns highly\ndiscriminative feature representation by effectively fusing shape and texture\ninformation at data level. The efficacy of our approach is demonstrated for the\ntasks of facial action unit detection and expression classification. The\nextensive experiments conducted on the Bosphorus and BU-4DFE datasets, show\nthat our method produces a significant boost in the performance when compared\nto state-of-the-art solutions\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:05:57 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Taha", "Bilal", ""], ["Hayat", "Munawar", ""], ["Berretti", "Stefano", ""], ["Werghi", "Naoufel", ""]]}, {"id": "1904.04317", "submitter": "Yan Luo", "authors": "Yan Luo, Yongkang Wong, Mohan Kankanhalli, and Qi Zhao", "title": "$\\mathcal{G}$-softmax: Improving Intra-class Compactness and Inter-class\n  Separability of Features", "comments": "15 pages, published in TNNLS", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems in 2019", "doi": "10.1109/TNNLS.2019.2909737", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-class compactness and inter-class separability are crucial indicators\nto measure the effectiveness of a model to produce discriminative features,\nwhere intra-class compactness indicates how close the features with the same\nlabel are to each other and inter-class separability indicates how far away the\nfeatures with different labels are. In this work, we investigate intra-class\ncompactness and inter-class separability of features learned by convolutional\nnetworks and propose a Gaussian-based softmax ($\\mathcal{G}$-softmax) function\nthat can effectively improve intra-class compactness and inter-class\nseparability. The proposed function is simple to implement and can easily\nreplace the softmax function. We evaluate the proposed $\\mathcal{G}$-softmax\nfunction on classification datasets (i.e., CIFAR-10, CIFAR-100, and Tiny\nImageNet) and on multi-label classification datasets (i.e., MS COCO and\nNUS-WIDE). The experimental results show that the proposed\n$\\mathcal{G}$-softmax function improves the state-of-the-art models across all\nevaluated datasets. In addition, analysis of the intra-class compactness and\ninter-class separability demonstrates the advantages of the proposed function\nover the softmax function, which is consistent with the performance\nimprovement. More importantly, we observe that high intra-class compactness and\ninter-class separability are linearly correlated to average precision on MS\nCOCO and NUS-WIDE. This implies that improvement of intra-class compactness and\ninter-class separability would lead to improvement of average precision.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:31:25 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 15:21:07 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Luo", "Yan", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan", ""], ["Zhao", "Qi", ""]]}, {"id": "1904.04329", "submitter": "Xiaowei Jia", "authors": "Xiaowei Jia, Ankush Khandelwal, Vipin Kumar", "title": "Automated Monitoring Cropland Using Remote Sensing Data: Challenges and\n  Opportunities for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of how recent advances in machine learning\nand the availability of data from earth observing satellites can dramatically\nimprove our ability to automatically map croplands over long period and over\nlarge regions. It discusses three applications in the domain of crop monitoring\nwhere ML approaches are beginning to show great promise. For each application,\nit highlights machine learning challenges, proposed approaches, and recent\nresults. The paper concludes with discussion of major challenges that need to\nbe addressed before ML approaches will reach their full potential for this\nproblem of great societal relevance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:54:27 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Jia", "Xiaowei", ""], ["Khandelwal", "Ankush", ""], ["Kumar", "Vipin", ""]]}, {"id": "1904.04335", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Haibin Ling", "title": "End-to-end Projector Photometric Compensation", "comments": "To appear in the 2019 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR). Source code and dataset are available at\n  https://github.com/BingyaoHuang/compennet", "journal-ref": null, "doi": "10.1109/CVPR.2019.00697", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projector photometric compensation aims to modify a projector input image\nsuch that it can compensate for disturbance from the appearance of projection\nsurface. In this paper, for the first time, we formulate the compensation\nproblem as an end-to-end learning problem and propose a convolutional neural\nnetwork, named CompenNet, to implicitly learn the complex compensation\nfunction. CompenNet consists of a UNet-like backbone network and an autoencoder\nsubnet. Such architecture encourages rich multi-level interactions between the\ncamera-captured projection surface image and the input image, and thus captures\nboth photometric and environment information of the projection surface. In\naddition, the visual details and interaction information are carried to deeper\nlayers along the multi-level skip convolution layers. The architecture is of\nparticular importance for the projector compensation task, for which only a\nsmall training dataset is allowed in practice. Another contribution we make is\na novel evaluation benchmark, which is independent of system setup and thus\nquantitatively verifiable. Such benchmark is not previously available, to our\nbest knowledge, due to the fact that conventional evaluation requests the\nhardware system to actually project the final results. Our key idea, motivated\nfrom our end-to-end problem formulation, is to use a reasonable surrogate to\navoid such projection process so as to be setup-independent. Our method is\nevaluated carefully on the benchmark, and the results show that our end-to-end\nlearning solution outperforms state-of-the-arts both qualitatively and\nquantitatively by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:07:27 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "1904.04336", "submitter": "Eric K. Tokuda", "authors": "Eric K. Tokuda, Claudio T. Silva, Roberto M. Cesar-Jr", "title": "Quantifying the presence of graffiti in urban environments", "comments": "This article was presented at the IEEE Big Data and Smart Computing\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graffiti is a common phenomenon in urban scenarios. Differently from urban\nart, graffiti tagging is a vandalism act and many local governments are putting\ngreat effort to combat it. The graffiti map of a region can be a very useful\nresource because it may allow one to potentially combat vandalism in locations\nwith high level of graffiti and also to cleanup saturated regions to discourage\nfuture acts. There is currently no automatic way of obtaining a graffiti map of\na region and it is obtained by manual inspection by the police or by popular\nparticipation. In this sense, we describe an ongoing work where we propose an\nautomatic way of obtaining a graffiti map of a neighbourhood. It consists of\nthe systematic collection of street view images followed by the identification\nof graffiti tags in the collected dataset and finally, in the calculation of\nthe proposed graffiti level of that location. We validate the proposed method\nby evaluating the geographical distribution of graffiti in a city known to have\nhigh concentration of graffiti -- Sao Paulo, Brazil.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:08:09 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Tokuda", "Eric K.", ""], ["Silva", "Claudio T.", ""], ["Cesar-Jr", "Roberto M.", ""]]}, {"id": "1904.04339", "submitter": "Heda Song", "authors": "Heda Song, Mercedes Torres Torres, Ender \\\"Ozcan, Isaac Triguero", "title": "L2AE-D: Learning to Aggregate Embeddings for Few-shot Learning with\n  Meta-level Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning focuses on learning a new visual concept with very limited\nlabelled examples. A successful approach to tackle this problem is to compare\nthe similarity between examples in a learned metric space based on\nconvolutional neural networks. However, existing methods typically suffer from\nmeta-level overfitting due to the limited amount of training tasks and do not\nnormally consider the importance of the convolutional features of different\nexamples within the same channel. To address these limitations, we make the\nfollowing two contributions: (a) We propose a novel meta-learning approach for\naggregating useful convolutional features and suppressing noisy ones based on a\nchannel-wise attention mechanism to improve class representations. The proposed\nmodel does not require fine-tuning and can be trained in an end-to-end manner.\nThe main novelty lies in incorporating a shared weight generation module that\nlearns to assign different weights to the feature maps of different examples\nwithin the same channel. (b) We also introduce a simple meta-level dropout\ntechnique that reduces meta-level overfitting in several few-shot learning\napproaches. In our experiments, we find that this simple technique\nsignificantly improves the performance of the proposed method as well as\nvarious state-of-the-art meta-learning algorithms. Applying our method to\nfew-shot image recognition using Omniglot and miniImageNet datasets shows that\nit is capable of delivering a state-of-the-art classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:11:39 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Song", "Heda", ""], ["Torres", "Mercedes Torres", ""], ["\u00d6zcan", "Ender", ""], ["Triguero", "Isaac", ""]]}, {"id": "1904.04346", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar and Brendan Tran Morris", "title": "What and How Well You Performed? A Multitask Learning Approach to Action\n  Quality Assessment", "comments": "CVPR 2019. Dataset temporarily made available at\n  https://github.com/ParitoshParmar/MTL-AQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can performance on the task of action quality assessment (AQA) be improved by\nexploiting a description of the action and its quality? Current AQA and skills\nassessment approaches propose to learn features that serve only one task -\nestimating the final score. In this paper, we propose to learn spatio-temporal\nfeatures that explain three related tasks - fine-grained action recognition,\ncommentary generation, and estimating the AQA score. A new multitask-AQA\ndataset, the largest to date, comprising of 1412 diving samples was collected\nto evaluate our approach (https://github.com/ParitoshParmar/MTL-AQA). We show\nthat our MTL approach outperforms STL approach using two different kinds of\narchitectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new\nstate-of-the-art performance with a rank correlation of 90.44%. Detailed\nexperiments were performed to show that MTL offers better generalization than\nSTL, and representations from action recognition models are not sufficient for\nthe AQA task and instead should be learned.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:43:52 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 16:46:51 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Parmar", "Paritosh", ""], ["Morris", "Brendan Tran", ""]]}, {"id": "1904.04354", "submitter": "Neslisah Torosdagli", "authors": "Neslisah Torosdagli, Mary McIntosh, Denise K. Liberton, Payal Verma,\n  Murat Sincan, Wade W. Han, Janice S. Lee, and Ulas Bagci", "title": "Relational Reasoning Network (RRN) for Anatomical Landmarking", "comments": "10 pages, 6 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately identifying anatomical landmarks is a crucial step in deformation\nanalysis and surgical planning for craniomaxillofacial (CMF) bones. Available\nmethods require segmentation of the object of interest for precise landmarking.\nUnlike those, our purpose in this study is to perform anatomical landmarking\nusing the inherent relation of CMF bones without explicitly segmenting them. We\npropose a new deep network architecture, called relational reasoning network\n(RRN), to accurately learn the local and the global relations of the landmarks.\nSpecifically, we are interested in learning landmarks in CMF region: mandible,\nmaxilla, and nasal bones. The proposed RRN works in an end-to-end manner,\nutilizing learned relations of the landmarks based on dense-block units and\nwithout the need for segmentation. For a given a few landmarks as input, the\nproposed system accurately and efficiently localizes the remaining landmarks on\nthe aforementioned bones. For a comprehensive evaluation of RRN, we used\ncone-beam computed tomography (CBCT) scans of 250 patients. The proposed system\nidentifies the landmark locations very accurately even when there are severe\npathologies or deformations in the bones. The proposed RRN has also revealed\nunique relationships among the landmarks that help us infer several reasoning\nabout informativeness of the landmark points. RRN is invariant to order of\nlandmarks and it allowed us to discover the optimal configurations (number and\nlocation) for landmarks to be localized within the object of interest\n(mandible) or nearby objects (maxilla and nasal). To the best of our knowledge,\nthis is the first of its kind algorithm finding anatomical relations of the\nobjects using deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:58:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Torosdagli", "Neslisah", ""], ["McIntosh", "Mary", ""], ["Liberton", "Denise K.", ""], ["Verma", "Payal", ""], ["Sincan", "Murat", ""], ["Han", "Wade W.", ""], ["Lee", "Janice S.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1904.04357", "submitter": "Chenyou Fan", "authors": "Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, Heng\n  Huang", "title": "Heterogeneous Memory Enhanced Multimodal Attention Model for Video\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end trainable Video Question\nAnswering (VideoQA) framework with three major components: 1) a new\nheterogeneous memory which can effectively learn global context information\nfrom appearance and motion features; 2) a redesigned question memory which\nhelps understand the complex semantics of question and highlights queried\nsubjects; and 3) a new multimodal fusion layer which performs multi-step\nreasoning by attending to relevant visual and textual hints with self-updated\nattention. Our VideoQA model firstly generates the global context-aware visual\nand textual features respectively by interacting current inputs with memory\ncontents. After that, it makes the attentional fusion of the multimodal visual\nand textual representations to infer the correct answer. Multiple cycles of\nreasoning can be made to iteratively refine attention weights of the multimodal\ndata and improve the final representation of the QA pair. Experimental results\ndemonstrate our approach achieves state-of-the-art performance on four VideoQA\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:10:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Fan", "Chenyou", ""], ["Zhang", "Xiaofan", ""], ["Zhang", "Shu", ""], ["Wang", "Wensheng", ""], ["Zhang", "Chi", ""], ["Huang", "Heng", ""]]}, {"id": "1904.04363", "submitter": "Hongxin Wang", "authors": "Hongxin Wang, Jigen Peng, Xuqiang Zheng, Shigang Yue", "title": "A Robust Visual System for Small Target Motion Detection Against\n  Cluttered Moving Backgrounds", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2910418", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring small objects against cluttered moving backgrounds is a huge\nchallenge to future robotic vision systems. As a source of inspiration, insects\nare quite apt at searching for mates and tracking prey -- which always appear\nas small dim speckles in the visual field. The exquisite sensitivity of insects\nfor small target motion, as revealed recently, is coming from a class of\nspecific neurons called small target motion detectors (STMDs). Although a few\nSTMD-based models have been proposed, these existing models only use motion\ninformation for small target detection and cannot discriminate small targets\nfrom small-target-like background features (named as fake features). To address\nthis problem, this paper proposes a novel visual system model (STMD+) for small\ntarget motion detection, which is composed of four subsystems -- ommatidia,\nmotion pathway, contrast pathway and mushroom body. Compared to existing\nSTMD-based models, the additional contrast pathway extracts directional\ncontrast from luminance signals to eliminate false positive background motion.\nThe directional contrast and the extracted motion information by the motion\npathway are integrated in the mushroom body for small target discrimination.\nExtensive experiments showed the significant and consistent improvements of the\nproposed visual system model over existing STMD-based models against fake\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:21:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wang", "Hongxin", ""], ["Peng", "Jigen", ""], ["Zheng", "Xuqiang", ""], ["Yue", "Shigang", ""]]}, {"id": "1904.04370", "submitter": "Abby Stylianou", "authors": "Hong Xuan, Abby Stylianou and Robert Pless", "title": "Improved Embeddings with Easy Positive Triplet Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning seeks to define an embedding where semantically similar\nimages are embedded to nearby locations, and semantically dissimilar images are\nembedded to distant locations. Substantial work has focused on loss functions\nand strategies to learn these embeddings by pushing images from the same class\nas close together in the embedding space as possible. In this paper, we propose\nan alternative, loosened embedding strategy that requires the embedding\nfunction only map each training image to the most similar examples from the\nsame class, an approach we call \"Easy Positive\" mining. We provide a collection\nof experiments and visualizations that highlight that this Easy Positive mining\nleads to embeddings that are more flexible and generalize better to new unseen\ndata. This simple mining strategy yields recall performance that exceeds state\nof the art approaches (including those with complicated loss functions and\nensemble methods) on image retrieval datasets including CUB, Stanford Online\nProducts, In-Shop Clothes and Hotels-50K.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:41:28 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 18:58:28 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Xuan", "Hong", ""], ["Stylianou", "Abby", ""], ["Pless", "Robert", ""]]}, {"id": "1904.04375", "submitter": "Rodolfo Valiente", "authors": "Rodolfo Valiente, Mahdi Zaman, Sedat Ozer, Yaser P. Fallah", "title": "Controlling Steering Angle for Cooperative Self-driving Vehicles\n  utilizing CNN and LSTM-based Deep Networks", "comments": "Accepted in IV 2019, 6 pages, 9 figures", "journal-ref": null, "doi": "10.1109/IVS.2019.8814260", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in autonomous vehicles is adjusting the steering\nangle at different road conditions. Recent state-of-the-art solutions\naddressing this challenge include deep learning techniques as they provide\nend-to-end solution to predict steering angles directly from the raw input\nimages with higher accuracy. Most of these works ignore the temporal\ndependencies between the image frames. In this paper, we tackle the problem of\nutilizing multiple sets of images shared between two autonomous vehicles to\nimprove the accuracy of controlling the steering angle by considering the\ntemporal dependencies between the image frames. This problem has not been\nstudied in the literature widely. We present and study a new deep architecture\nto predict the steering angle automatically by using Long-Short-Term-Memory\n(LSTM) in our deep architecture. Our deep architecture is an end-to-end network\nthat utilizes CNN, LSTM and fully connected (FC) layers and it uses both\npresent and futures images (shared by a vehicle ahead via Vehicle-to-Vehicle\n(V2V) communication) as input to control the steering angle. Our model\ndemonstrates the lowest error when compared to the other existing approaches in\nthe literature.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:51:49 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 18:32:10 GMT"}, {"version": "v3", "created": "Sun, 12 May 2019 02:38:13 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Valiente", "Rodolfo", ""], ["Zaman", "Mahdi", ""], ["Ozer", "Sedat", ""], ["Fallah", "Yaser P.", ""]]}, {"id": "1904.04402", "submitter": "Xudong Wang", "authors": "Xudong Wang, Zhaowei Cai, Dashan Gao and Nuno Vasconcelos", "title": "Towards Universal Object Detection by Domain Attention", "comments": "11 pages, 6 figures. Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite increasing efforts on universal representations for visual\nrecognition, few have addressed object detection. In this paper, we develop an\neffective and efficient universal object detection system that is capable of\nworking on various image domains, from human faces and traffic signs to medical\nCT images. Unlike multi-domain models, this universal model does not require\nprior knowledge of the domain of interest. This is achieved by the introduction\nof a new family of adaptation layers, based on the principles of squeeze and\nexcitation, and a new domain-attention mechanism. In the proposed universal\ndetector, all parameters and computations are shared across domains, and a\nsingle network processes all domains all the time. Experiments, on a newly\nestablished universal object detection benchmark of 11 diverse datasets, show\nthat the proposed detector outperforms a bank of individual detectors, a\nmulti-domain detector, and a baseline universal detector, with a 1.3x parameter\nincrease over a single-domain baseline detector. The code and benchmark will be\nreleased at http://www.svcl.ucsd.edu/projects/universal-detection/.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 00:11:12 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 02:10:00 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 23:24:45 GMT"}, {"version": "v4", "created": "Sat, 6 Jul 2019 02:43:56 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wang", "Xudong", ""], ["Cai", "Zhaowei", ""], ["Gao", "Dashan", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1904.04404", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Zhile Ren, Mingze Xu, Xinlei Chen, David Crandall, Devi\n  Parikh, Dhruv Batra", "title": "Embodied Visual Recognition", "comments": "14 pages, 13 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive visual systems typically fail to recognize objects in the amodal\nsetting where they are heavily occluded. In contrast, humans and other embodied\nagents have the ability to move in the environment, and actively control the\nviewing angle to better understand object shapes and semantics. In this work,\nwe introduce the task of Embodied Visual Recognition (EVR): An agent is\ninstantiated in a 3D environment close to an occluded target object, and is\nfree to move in the environment to perform object classification, amodal object\nlocalization, and amodal object segmentation. To address this, we develop a new\nmodel called Embodied Mask R-CNN, for agents to learn to move strategically to\nimprove their visual recognition abilities. We conduct experiments using the\nHouse3D environment. Experimental results show that: 1) agents with embodiment\n(movement) achieve better visual recognition performance than passive ones; 2)\nin order to improve visual recognition abilities, agents can learn strategical\nmoving paths that are different from shortest paths.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 00:33:17 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Yang", "Jianwei", ""], ["Ren", "Zhile", ""], ["Xu", "Mingze", ""], ["Chen", "Xinlei", ""], ["Crandall", "David", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.04406", "submitter": "Sujoy Paul", "authors": "Mahmudul Hasan, Sujoy Paul, Anastasios I. Mourikis, Amit K.\n  Roy-Chowdhury", "title": "Context-Aware Query Selection for Active Learning in Event Recognition", "comments": "To appear in Transactions of Pattern Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2878696", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition is a challenging problem with many practical\napplications. In addition to the visual features, recent approaches have\nbenefited from the use of context, e.g., inter-relationships among the\nactivities and objects. However, these approaches require data to be labeled,\nentirely available beforehand, and not designed to be updated continuously,\nwhich make them unsuitable for surveillance applications. In contrast, we\npropose a continuous-learning framework for context-aware activity recognition\nfrom unlabeled video, which has two distinct advantages over existing methods.\nFirst, it employs a novel active-learning technique that not only exploits the\ninformativeness of the individual activities but also utilizes their contextual\ninformation during query selection; this leads to significant reduction in\nexpensive manual annotation effort. Second, the learned models can be adapted\nonline as more data is available. We formulate a conditional random field model\nthat encodes the context and devise an information-theoretic approach that\nutilizes entropy and mutual information of the nodes to compute the set of most\ninformative queries, which are labeled by a human. These labels are combined\nwith graphical inference techniques for incremental updates. We provide a\ntheoretical formulation of the active learning framework with an analytic\nsolution. Experiments on six challenging datasets demonstrate that our\nframework achieves superior performance with significantly less manual\nlabeling.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 00:58:23 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hasan", "Mahmudul", ""], ["Paul", "Sujoy", ""], ["Mourikis", "Anastasios I.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1904.04412", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Riyadh Al-Raoush, Olivier Monga,\n  Patricia Garnier, Sebti Foufou, Abdelaziz Bouras, Alexandros Iosifidis,\n  Moncef Gabbouj, Philippe C. Baveye", "title": "3D Quantum Cuts for Automatic Segmentation of Porous Media in Tomography\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binary segmentation of volumetric images of porous media is a crucial step\ntowards gaining a deeper understanding of the factors governing biogeochemical\nprocesses at minute scales. Contemporary work primarily revolves around\nprimitive techniques based on global or local adaptive thresholding that have\nknown common drawbacks in image segmentation. Moreover, absence of a unified\nbenchmark prohibits quantitative evaluation, which further clouds the impact of\nexisting methodologies. In this study, we tackle the issue on both fronts.\nFirstly, by drawing parallels with natural image segmentation, we propose a\nnovel, and automatic segmentation technique, 3D Quantum Cuts (QCuts-3D)\ngrounded on a state-of-the-art spectral clustering technique. Secondly, we\ncurate and present a publicly available dataset of 68 multiphase volumetric\nimages of porous media with diverse solid geometries, along with voxel-wise\nground truth annotations for each constituting phase. We provide comparative\nevaluations between QCuts-3D and the current state-of-the-art over this dataset\nacross a variety of evaluation metrics. The proposed systematic approach\nachieves a 26% increase in AUROC while achieving a substantial reduction of the\ncomputational complexity of the state-of-the-art competitors. Moreover,\nstatistical analysis reveals that the proposed method exhibits significant\nrobustness against the compositional variations of porous media.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 01:43:24 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 06:38:04 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Al-Raoush", "Riyadh", ""], ["Monga", "Olivier", ""], ["Garnier", "Patricia", ""], ["Foufou", "Sebti", ""], ["Bouras", "Abdelaziz", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""], ["Baveye", "Philippe C.", ""]]}, {"id": "1904.04419", "submitter": "Tingfung Lau", "authors": "Tingfung Lau, Nathan Ng, Julian Gingold, Nina Desai, Julian McAuley,\n  Zachary C. Lipton", "title": "Embryo staging with weakly-supervised region selection and\n  dynamically-decoded predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To optimize clinical outcomes, fertility clinics must strategically select\nwhich embryos to transfer. Common selection heuristics are formulas expressed\nin terms of the durations required to reach various developmental milestones,\nquantities historically annotated manually by experienced embryologists based\non time-lapse EmbryoScope videos. We propose a new method for automatic embryo\nstaging that exploits several sources of structure in this time-lapse data.\nFirst, noting that in each image the embryo occupies a small subregion, we\njointly train a region proposal network with the downstream classifier to\nisolate the embryo. Notably, because we lack ground-truth bounding boxes, our\nwe weakly supervise the region proposal network optimizing its parameters via\nreinforcement learning to improve the downstream classifier's loss. Moreover,\nnoting that embryos reaching the blastocyst stage progress monotonically\nthrough earlier stages, we develop a dynamic-programming-based decoder that\npost-processes our predictions to select the most likely monotonic sequence of\ndevelopmental stages. Our methods outperform vanilla residual networks and\nrival the best numbers in contemporary papers, as measured by both per-frame\naccuracy and transition prediction error, despite operating on smaller data\nthan many.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:03:08 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Lau", "Tingfung", ""], ["Ng", "Nathan", ""], ["Gingold", "Julian", ""], ["Desai", "Nina", ""], ["McAuley", "Julian", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1904.04421", "submitter": "Xiaofan Zhang", "authors": "Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle\n  Rupnow, Wen-mei Hwu, Deming Chen", "title": "FPGA/DNN Co-Design: An Efficient Design Methodology for IoT Intelligence\n  on the Edge", "comments": "Accepted by Design Automation Conference (DAC'2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While embedded FPGAs are attractive platforms for DNN acceleration on\nedge-devices due to their low latency and high energy efficiency, the scarcity\nof resources of edge-scale FPGA devices also makes it challenging for DNN\ndeployment. In this paper, we propose a simultaneous FPGA/DNN co-design\nmethodology with both bottom-up and top-down approaches: a bottom-up\nhardware-oriented DNN model search for high accuracy, and a top-down FPGA\naccelerator design considering DNN-specific characteristics. We also build an\nautomatic co-design flow, including an Auto-DNN engine to perform\nhardware-oriented DNN model search, as well as an Auto-HLS engine to generate\nsynthesizable C code of the FPGA accelerator for explored DNNs. We demonstrate\nour co-design approach on an object detection task using PYNQ-Z1 FPGA. Results\nshow that our proposed DNN model and accelerator outperform the\nstate-of-the-art FPGA designs in all aspects including Intersection-over-Union\n(IoU) (6.2% higher), frames per second (FPS) (2.48X higher), power consumption\n(40% lower), and energy efficiency (2.5X higher). Compared to GPU-based\nsolutions, our designs deliver similar accuracy but consume far less energy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:06:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hao", "Cong", ""], ["Zhang", "Xiaofan", ""], ["Li", "Yuhong", ""], ["Huang", "Sitao", ""], ["Xiong", "Jinjun", ""], ["Rupnow", "Kyle", ""], ["Hwu", "Wen-mei", ""], ["Chen", "Deming", ""]]}, {"id": "1904.04427", "submitter": "Chaojing Duan", "authors": "Chaojing Duan, Siheng Chen, Jelena Kovacevic", "title": "3D Point Cloud Denoising via Deep Neural Network based Local Surface\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural-network-based architecture for 3D point cloud denoising\ncalled neural projection denoising (NPD). In our previous work, we proposed a\ntwo-stage denoising algorithm, which first estimates reference planes and\nfollows by projecting noisy points to estimated reference planes. Since the\nestimated reference planes are inevitably noisy, multi-projection is applied to\nstabilize the denoising performance. NPD algorithm uses a neural network to\nestimate reference planes for points in noisy point clouds. With more accurate\nestimations of reference planes, we are able to achieve better denoising\nperformances with only one-time projection. To the best of our knowledge, NPD\nis the first work to denoise 3D point clouds with deep learning techniques. To\nconduct the experiments, we sample 40000 point clouds from the 3D data in\nShapeNet to train a network and sample 350 point clouds from the 3D data in\nModelNet10 to test. Experimental results show that our algorithm can estimate\nnormal vectors of points in noisy point clouds. Comparing to five competitive\nmethods, the proposed algorithm achieves better denoising performance and\nproduces much smaller variances.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:29:39 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Duan", "Chaojing", ""], ["Chen", "Siheng", ""], ["Kovacevic", "Jelena", ""]]}, {"id": "1904.04429", "submitter": "Maozheng Zhao", "authors": "Maozheng Zhao, Le Hou, Han Le, Dimitris Samaras, Nebojsa Jojic,\n  Danielle Fassler, Tahsin Kurc, Rajarsi Gupta, Kolya Malkin, Shroyer Kenneth,\n  and Joel Saltz", "title": "Label Super Resolution with Inter-Instance Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of semantic segmentation, high-resolution (pixel-level) ground\ntruth is very expensive to collect, especially for high resolution images such\nas gigapixel pathology images. On the other hand, collecting low resolution\nlabels (labels for a block of pixels) for these high resolution images is much\nmore cost efficient. Conventional methods trained on these low-resolution\nlabels are only capable of giving low-resolution predictions. The existing\nstate-of-the-art label super resolution (LSR) method is capable of predicting\nhigh resolution labels, using only low-resolution supervision, given the joint\ndistribution between low resolution and high resolution labels. However, it\ndoes not consider the inter-instance variance which is crucial in the ideal\nmathematical formulation. In this work, we propose a novel loss function\nmodeling the inter-instance variance. We test our method on a real world\napplication: infiltrating breast cancer region segmentation in histopathology\nslides. Experimental results show the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:35:03 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 21:50:57 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhao", "Maozheng", ""], ["Hou", "Le", ""], ["Le", "Han", ""], ["Samaras", "Dimitris", ""], ["Jojic", "Nebojsa", ""], ["Fassler", "Danielle", ""], ["Kurc", "Tahsin", ""], ["Gupta", "Rajarsi", ""], ["Malkin", "Kolya", ""], ["Kenneth", "Shroyer", ""], ["Saltz", "Joel", ""]]}, {"id": "1904.04433", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang,\n  Jun Zhu", "title": "Efficient Decision-based Black-box Adversarial Attacks on Face\n  Recognition", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has obtained remarkable progress in recent years due to the\ngreat improvement of deep convolutional neural networks (CNNs). However, deep\nCNNs are vulnerable to adversarial examples, which can cause fateful\nconsequences in real-world face recognition applications with\nsecurity-sensitive purposes. Adversarial attacks are widely studied as they can\nidentify the vulnerability of the models before they are deployed. In this\npaper, we evaluate the robustness of state-of-the-art face recognition models\nin the decision-based black-box attack setting, where the attackers have no\naccess to the model parameters and gradients, but can only acquire hard-label\npredictions by sending queries to the target model. This attack setting is more\npractical in real-world face recognition systems. To improve the efficiency of\nprevious methods, we propose an evolutionary attack algorithm, which can model\nthe local geometries of the search directions and reduce the dimension of the\nsearch space. Extensive experiments demonstrate the effectiveness of the\nproposed method that induces a minimum perturbation to an input face image with\nfewer queries. We also apply the proposed method to attack a real-world face\nrecognition system successfully.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:45:35 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Wu", "Baoyuan", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""], ["Zhu", "Jun", ""]]}, {"id": "1904.04441", "submitter": "Hui Zeng", "authors": "Hui Zeng, Lida Li, Zisheng Cao and Lei Zhang", "title": "Reliable and Efficient Image Cropping: A Grid Anchor based Approach", "comments": "To appear in CVPR 2019. Code and dataset are released", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image cropping aims to improve the composition as well as aesthetic quality\nof an image by removing extraneous content from it. Existing image cropping\ndatabases provide only one or several human-annotated bounding boxes as the\ngroundtruth, which cannot reflect the non-uniqueness and flexibility of image\ncropping in practice. The employed evaluation metrics such as\nintersection-over-union cannot reliably reflect the real performance of\ncropping models, either. This work revisits the problem of image cropping, and\npresents a grid anchor based formulation by considering the special properties\nand requirements (e.g., local redundancy, content preservation, aspect ratio)\nof image cropping. Our formulation reduces the searching space of candidate\ncrops from millions to less than one hundred. Consequently, a grid anchor based\ncropping benchmark is constructed, where all crops of each image are annotated\nand more reliable evaluation metrics are defined. We also design an effective\nand lightweight network module, which simultaneously considers the region of\ninterest and region of discard for more accurate image cropping. Our model can\nstably output visually pleasing crops for images of different scenes and run at\na speed of 125 FPS. Code and dataset are available at:\nhttps://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:18:20 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zeng", "Hui", ""], ["Li", "Lida", ""], ["Cao", "Zisheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1904.04443", "submitter": "Yulun Zhang", "authors": "Yulun Zhang, Chen Fang, Yilin Wang, Zhaowen Wang, Zhe Lin, Yun Fu,\n  Jimei Yang", "title": "Multimodal Style Transfer via Graph Cuts", "comments": "Accepted to ICCV 2019. Typos in Eqs. (11) and (12) have been fixed in\n  arXiv V2 and this version (V6). Code: https://github.com/yulunzhang/MST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An assumption widely used in recent neural style transfer methods is that\nimage styles can be described by global statics of deep features like Gram or\ncovariance matrices. Alternative approaches have represented styles by\ndecomposing them into local pixel or neural patches. Despite the recent\nprogress, most existing methods treat the semantic patterns of style image\nuniformly, resulting unpleasing results on complex styles. In this paper, we\nintroduce a more flexible and general universal style transfer technique:\nmultimodal style transfer (MST). MST explicitly considers the matching of\nsemantic patterns in content and style images. Specifically, the style image\nfeatures are clustered into sub-style components, which are matched with local\ncontent features under a graph cut formulation. A reconstruction network is\ntrained to transfer each sub-style and render the final stylized result. We\nalso generalize MST to improve some existing methods. Extensive experiments\ndemonstrate the superior effectiveness, robustness, and flexibility of MST.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:23:20 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 18:39:22 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 05:42:10 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 18:37:53 GMT"}, {"version": "v5", "created": "Mon, 23 Dec 2019 21:07:16 GMT"}, {"version": "v6", "created": "Tue, 7 Jan 2020 15:03:47 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhang", "Yulun", ""], ["Fang", "Chen", ""], ["Wang", "Yilin", ""], ["Wang", "Zhaowen", ""], ["Lin", "Zhe", ""], ["Fu", "Yun", ""], ["Yang", "Jimei", ""]]}, {"id": "1904.04445", "submitter": "Artsiom Sanakoyeu", "authors": "Yauhen Babakhin, Artsiom Sanakoyeu, Hirotoshi Kitamura", "title": "Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an\n  Ensemble of Convolutional Neural Networks", "comments": "Accepted at GCPR 2019, Source code:\n  https://github.com/ybabakhin/kaggle_salt_bes_phalanx", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic image analysis plays a crucial role in a wide range of industrial\napplications and has been receiving significant attention. One of the essential\nchallenges of seismic imaging is detecting subsurface salt structure which is\nindispensable for identification of hydrocarbon reservoirs and drill path\nplanning. Unfortunately, exact identification of large salt deposits is\nnotoriously difficult and professional seismic imaging often requires expert\nhuman interpretation of salt bodies. Convolutional neural networks (CNNs) have\nbeen successfully applied in many fields, and several attempts have been made\nin the field of seismic imaging. But the high cost of manual annotations by\ngeophysics experts and scarce publicly available labeled datasets hinder the\nperformance of the existing CNN-based methods. In this work, we propose a\nsemi-supervised method for segmentation (delineation) of salt bodies in seismic\nimages which utilizes unlabeled data for multi-round self-training. To reduce\nerror amplification during self-training we propose a scheme which uses an\nensemble of CNNs. We show that our approach outperforms state-of-the-art on the\nTGS Salt Identification Challenge dataset and is ranked the first among the\n3234 competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:25:21 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 23:18:38 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 16:44:47 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Babakhin", "Yauhen", ""], ["Sanakoyeu", "Artsiom", ""], ["Kitamura", "Hirotoshi", ""]]}, {"id": "1904.04449", "submitter": "Jia Li", "authors": "Kui Fu, Peipei Shi, Yafei Song, Shiming Ge, Xiangju Lu and Jia Li", "title": "Ultrafast Video Attention Prediction with Coupled Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large convolutional neural network models have recently demonstrated\nimpressive performance on video attention prediction. Conventionally, these\nmodels are with intensive computation and large memory. To address these\nissues, we design an extremely light-weight network with ultrafast speed, named\nUVA-Net. The network is constructed based on depth-wise convolutions and takes\nlow-resolution images as input. However, this straight-forward acceleration\nmethod will decrease performance dramatically. To this end, we propose a\ncoupled knowledge distillation strategy to augment and train the network\neffectively. With this strategy, the model can further automatically discover\nand emphasize implicit useful cues contained in the data. Both spatial and\ntemporal knowledge learned by the high-resolution complex teacher networks also\ncan be distilled and transferred into the proposed low-resolution light-weight\nspatiotemporal network. Experimental results show that the performance of our\nmodel is comparable to 11 state-of-the-art models in video attention\nprediction, while it costs only 0.68 MB memory footprint, runs about 10,106 FPS\non GPU and 404 FPS on CPU, which is 206 times faster than previous models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:32:08 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 07:35:44 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Fu", "Kui", ""], ["Shi", "Peipei", ""], ["Song", "Yafei", ""], ["Ge", "Shiming", ""], ["Lu", "Xiangju", ""], ["Li", "Jia", ""]]}, {"id": "1904.04452", "submitter": "Chong Luo", "authors": "Guangting Wang, Chong Luo, Zhiwei Xiong and Wenjun Zeng", "title": "SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object\n  Tracking", "comments": "to appear in CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greatest challenge facing visual object tracking is the simultaneous\nrequirements on robustness and discrimination power. In this paper, we propose\na SiamFC-based tracker, named SPM-Tracker, to tackle this challenge. The basic\nidea is to address the two requirements in two separate matching stages.\nRobustness is strengthened in the coarse matching (CM) stage through\ngeneralized training while discrimination power is enhanced in the fine\nmatching (FM) stage through a distance learning network. The two stages are\nconnected in series as the input proposals of the FM stage are generated by the\nCM stage. They are also connected in parallel as the matching scores and box\nlocation refinements are fused to generate the final results. This innovative\nseries-parallel structure takes advantage of both stages and results in\nsuperior performance. The proposed SPM-Tracker, running at 120fps on GPU,\nachieves an AUC of 0.687 on OTB-100 and an EAO of 0.434 on VOT-16, exceeding\nother real-time trackers by a notable margin.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:36:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wang", "Guangting", ""], ["Luo", "Chong", ""], ["Xiong", "Zhiwei", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1904.04466", "submitter": "Zixiang Cai", "authors": "Yuan Gao, Zixiang Cai, Lei Yu", "title": "Intra-Ensemble in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving model performance is always the key problem in machine learning\nincluding deep learning. However, stand-alone neural networks always suffer\nfrom marginal effect when stacking more layers. At the same time, ensemble is\nan useful technique to further enhance model performance. Nevertheless,\ntraining several independent deep neural networks for ensemble costs multiple\nresources. If so, is it possible to utilize ensemble in only one neural\nnetwork? In this work, we propose Intra-Ensemble, an end-to-end ensemble\nstrategy with stochastic channel recombination operations to train several\nsub-networks simultaneously within one neural network. Additional parameter\nsize is marginal since the majority of parameters are mutually shared.\nMeanwhile, stochastic channel recombination significantly increases the\ndiversity of sub-networks, which finally enhances ensemble performance.\nExtensive experiments and ablation studies prove the applicability of\nintra-ensemble on various kinds of datasets and network architectures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:53:17 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 02:09:23 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gao", "Yuan", ""], ["Cai", "Zixiang", ""], ["Yu", "Lei", ""]]}, {"id": "1904.04473", "submitter": "Fanzi Wu", "authors": "Fanzi Wu, Linchao Bao, Yajing Chen, Yonggen Ling, Yibing Song, Songnan\n  Li, King Ngi Ngan, Wei Liu", "title": "MVF-Net: Multi-View 3D Face Morphable Model Regression", "comments": "2019 Conference on Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of recovering the 3D geometry of a human face from a\nset of facial images in multiple views. While recent studies have shown\nimpressive progress in 3D Morphable Model (3DMM) based facial reconstruction,\nthe settings are mostly restricted to a single view. There is an inherent\ndrawback in the single-view setting: the lack of reliable 3D constraints can\ncause unresolvable ambiguities. We in this paper explore 3DMM-based shape\nrecovery in a different setting, where a set of multi-view facial images are\ngiven as input. A novel approach is proposed to regress 3DMM parameters from\nmulti-view inputs with an end-to-end trainable Convolutional Neural Network\n(CNN). Multiview geometric constraints are incorporated into the network by\nestablishing dense correspondences between different views leveraging a novel\nself-supervised view alignment loss. The main ingredient of the view alignment\nloss is a differentiable dense optical flow estimator that can backpropagate\nthe alignment errors between an input view and a synthetic rendering from\nanother input view, which is projected to the target view through the 3D shape\nto be inferred. Through minimizing the view alignment loss, better 3D shapes\ncan be recovered such that the synthetic projections from one view to another\ncan better align with the observed image. Extensive experiments demonstrate the\nsuperiority of the proposed method over other 3DMM methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 05:42:11 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wu", "Fanzi", ""], ["Bao", "Linchao", ""], ["Chen", "Yajing", ""], ["Ling", "Yonggen", ""], ["Song", "Yibing", ""], ["Li", "Songnan", ""], ["Ngan", "King Ngi", ""], ["Liu", "Wei", ""]]}, {"id": "1904.04474", "submitter": "Wenhan Yang", "authors": "Ye Yuan, Wenhan Yang, Wenqi Ren, Jiaying Liu, Walter J. Scheirer,\n  Zhangyang Wang", "title": "UG$^{2+}$ Track 2: A Collective Benchmark Effort for Evaluating and\n  Advancing Image Understanding in Poor Visibility Environments", "comments": "A summary paper on datasets, fact sheets, baseline results, challenge\n  results, and winning methods in UG$^{2+}$ Challenge (Track 2). More materials\n  are provided in http://www.ug2challenge.org/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UG$^{2+}$ challenge in IEEE CVPR 2019 aims to evoke a comprehensive\ndiscussion and exploration about how low-level vision techniques can benefit\nthe high-level automatic visual recognition in various scenarios. In its second\ntrack, we focus on object or face detection in poor visibility enhancements\ncaused by bad weathers (haze, rain) and low light conditions. While existing\nenhancement methods are empirically expected to help the high-level end task,\nthat is observed to not always be the case in practice. To provide a more\nthorough examination and fair comparison, we introduce three benchmark sets\ncollected in real-world hazy, rainy, and low-light conditions, respectively,\nwith annotate objects/faces annotated. To our best knowledge, this is the first\nand currently largest effort of its kind. Baseline results by cascading\nexisting enhancement and detection models are reported, indicating the highly\nchallenging nature of our new data as well as the large room for further\ntechnical innovations. We expect a large participation from the broad research\ncommunity to address these challenges together.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 05:48:15 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 23:31:09 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 09:40:01 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 10:13:20 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yuan", "Ye", ""], ["Yang", "Wenhan", ""], ["Ren", "Wenqi", ""], ["Liu", "Jiaying", ""], ["Scheirer", "Walter J.", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1904.04492", "submitter": "Tanzila Rahman", "authors": "Tanzila Rahman, Mrigank Rochan and Yang Wang", "title": "Convolutional Temporal Attention Model for Video-based Person\n  Re-identification", "comments": "6 pages, 4 figures, ICME 2019", "journal-ref": "ICME 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of video-based person re-identification is to match two input\nvideos, so that the distance of the two videos is small if two videos contain\nthe same person. A common approach for person re-identification is to first\nextract image features for all frames in the video, then aggregate all the\nfeatures to form a video-level feature. The video-level features of two videos\ncan then be used to calculate the distance of the two videos. In this paper, we\npropose a temporal attention approach for aggregating frame-level features into\na video-level feature vector for re-identification. Our method is motivated by\nthe fact that not all frames in a video are equally informative. We propose a\nfully convolutional temporal attention model for generating the attention\nscores. Fully convolutional network (FCN) has been widely used in semantic\nsegmentation for generating 2D output maps. In this paper, we formulate video\nbased person re-identification as a sequence labeling problem like semantic\nsegmentation. We establish a connection between them and modify FCN to generate\nattention scores to represent the importance of each frame. Extensive\nexperiments on three different benchmark datasets (i.e. iLIDS-VID, PRID-2011\nand SDU-VID) show that our proposed method outperforms other state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 07:03:53 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 07:19:33 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Rahman", "Tanzila", ""], ["Rochan", "Mrigank", ""], ["Wang", "Yang", ""]]}, {"id": "1904.04514", "submitter": "Jingdong Wang", "authors": "Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao, Dong Liu,\n  Yadong Mu, Xinggang Wang, Wenyu Liu, Jingdong Wang", "title": "High-Resolution Representations for Labeling Pixels and Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution representation learning plays an essential role in many\nvision problems, e.g., pose estimation and semantic segmentation. The\nhigh-resolution network (HRNet)~\\cite{SunXLW19}, recently developed for human\npose estimation, maintains high-resolution representations through the whole\nprocess by connecting high-to-low resolution convolutions in \\emph{parallel}\nand produces strong high-resolution representations by repeatedly conducting\nfusions across parallel convolutions.\n  In this paper, we conduct a further study on high-resolution representations\nby introducing a simple yet effective modification and apply it to a wide range\nof vision tasks. We augment the high-resolution representation by aggregating\nthe (upsampled) representations from all the parallel convolutions rather than\nonly the representation from the high-resolution convolution as done\nin~\\cite{SunXLW19}. This simple modification leads to stronger representations,\nevidenced by superior results. We show top results in semantic segmentation on\nCityscapes, LIP, and PASCAL Context, and facial landmark detection on AFLW,\nCOFW, $300$W, and WFLW. In addition, we build a multi-level representation from\nthe high-resolution representation and apply it to the Faster R-CNN object\ndetection framework and the extended frameworks. The proposed approach achieves\nsuperior results to existing single-model networks on COCO object detection.\nThe code and models have been publicly available at\n\\url{https://github.com/HRNet}.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:08:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Sun", "Ke", ""], ["Zhao", "Yang", ""], ["Jiang", "Borui", ""], ["Cheng", "Tianheng", ""], ["Xiao", "Bin", ""], ["Liu", "Dong", ""], ["Mu", "Yadong", ""], ["Wang", "Xinggang", ""], ["Liu", "Wenyu", ""], ["Wang", "Jingdong", ""]]}, {"id": "1904.04516", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann and Marius Schubert", "title": "Uncertainty Measures and Prediction Quality Rating for the Semantic\n  Segmentation of Nested Multi Resolution Street Scene Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the semantic segmentation of street scenes the reliability of the\nprediction and therefore uncertainty measures are of highest interest. We\npresent a method that generates for each input image a hierarchy of nested\ncrops around the image center and presents these, all re-scaled to the same\nsize, to a neural network for semantic segmentation. The resulting softmax\noutputs are then post processed such that we can investigate mean and variance\nover all image crops as well as mean and variance of uncertainty heat maps\nobtained from pixel-wise uncertainty measures, like the entropy, applied to\neach crop's softmax output. In our tests, we use the publicly available\nDeepLabv3+ MobilenetV2 network (trained on the Cityscapes dataset) and\ndemonstrate that the incorporation of crops improves the quality of the\nprediction and that we obtain more reliable uncertainty measures. These are\nthen aggregated over predicted segments for either classifying between IoU=0\nand IoU>0 (meta classification) or predicting the IoU via linear regression\n(meta regression). The latter yields reliable performance estimates for\nsegmentation networks, in particular useful in the absence of ground truth. For\nthe task of meta classification we obtain a classification accuracy of\n$81.93\\%$ and an AUROC of $89.89\\%$. For meta regression we obtain an $R^2$\nvalue of $84.77\\%$. These results yield significant improvements compared to\nother approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:14:09 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Rottmann", "Matthias", ""], ["Schubert", "Marius", ""]]}, {"id": "1904.04520", "submitter": "Mara Graziani Miss", "authors": "Mara Graziani, Vincent Andrearczyk and Henning M\\\"uller", "title": "Regression Concept Vectors for Bidirectional Explanations in\n  Histopathology", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": "Understanding and Interpreting Machine Learning in Medical Image\n  Computing Applications: First International Workshops, Proceedings. Vol.\n  11038. Springer, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations for deep neural network predictions in terms of domain-related\nconcepts can be valuable in medical applications, where justifications are\nimportant for confidence in the decision-making. In this work, we propose a\nmethodology to exploit continuous concept measures as Regression Concept\nVectors (RCVs) in the activation space of a layer. The directional derivative\nof the decision function along the RCVs represents the network sensitivity to\nincreasing values of a given concept measure. When applied to breast cancer\ngrading, nuclei texture emerges as a relevant concept in the detection of tumor\ntissue in breast lymph node samples. We evaluate score robustness and\nconsistency by statistical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:26:02 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Graziani", "Mara", ""], ["Andrearczyk", "Vincent", ""], ["M\u00fcller", "Henning", ""]]}, {"id": "1904.04536", "submitter": "Ke Gong", "authors": "Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, Liang Lin", "title": "Graphonomy: Universal Human Parsing via Graph Transfer Learning", "comments": "Accepted to CVPR 2019. The Code is available at\n  https://github.com/Gaoyiminggithub/Graphonomy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior highly-tuned human parsing models tend to fit towards each dataset in a\nspecific domain or with discrepant label granularity, and can hardly be adapted\nto other human parsing tasks without extensive re-training. In this paper, we\naim to learn a single universal human parsing model that can tackle all kinds\nof human parsing needs by unifying label annotations from different domains or\nat various levels of granularity. This poses many fundamental learning\nchallenges, e.g. discovering underlying semantic structures among different\nlabel granularity, performing proper transfer learning across different image\ndomains, and identifying and utilizing label redundancies across related tasks.\n  To address these challenges, we propose a new universal human parsing agent,\nnamed \"Graphonomy\", which incorporates hierarchical graph transfer learning\nupon the conventional parsing network to encode the underlying label semantic\nstructures and propagate relevant semantic information. In particular,\nGraphonomy first learns and propagates compact high-level graph representation\namong the labels within one dataset via Intra-Graph Reasoning, and then\ntransfers semantic information across multiple datasets via Inter-Graph\nTransfer. Various graph transfer dependencies (\\eg, similarity, linguistic\nknowledge) between different datasets are analyzed and encoded to enhance graph\ntransfer capability. By distilling universal semantic graph representation to\neach specific task, Graphonomy is able to predict all levels of parsing labels\nin one system without piling up the complexity. Experimental results show\nGraphonomy effectively achieves the state-of-the-art results on three human\nparsing benchmarks as well as advantageous universal human parsing performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:49:18 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Gong", "Ke", ""], ["Gao", "Yiming", ""], ["Liang", "Xiaodan", ""], ["Shen", "Xiaohui", ""], ["Wang", "Meng", ""], ["Lin", "Liang", ""]]}, {"id": "1904.04547", "submitter": "Anirban Santara", "authors": "Anirban Santara, Jayeeta Datta, Sourav Sarkar, Ankur Garg, Kirti\n  Padia, Pabitra Mitra", "title": "PUNCH: Positive UNlabelled Classification based information retrieval in\n  Hyperspectral images", "comments": "9 pages, under review at ACMMM-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images of land-cover captured by airborne or satellite-mounted\nsensors provide a rich source of information about the chemical composition of\nthe materials present in a given place. This makes hyperspectral imaging an\nimportant tool for earth sciences, land-cover studies, and military and\nstrategic applications. However, the scarcity of labeled training examples and\nspatial variability of spectral signature are two of the biggest challenges\nfaced by hyperspectral image classification. In order to address these issues,\nwe aim to develop a framework for material-agnostic information retrieval in\nhyperspectral images based on Positive-Unlabelled (PU) classification. Given a\nhyperspectral scene, the user labels some positive samples of a material he/she\nis looking for and our goal is to retrieve all the remaining instances of the\nquery material in the scene. Additionally, we require the system to work\nequally well for any material in any scene without the user having to disclose\nthe identity of the query material. This material-agnostic nature of the\nframework provides it with superior generalization abilities. We explore two\nalternative approaches to solve the hyperspectral image classification problem\nwithin this framework. The first approach is an adaptation of non-negative risk\nestimation based PU learning for hyperspectral data. The second approach is\nbased on one-versus-all positive-negative classification where the negative\nclass is approximately sampled using a novel spectral-spatial retrieval model.\nWe propose two annotator models - uniform and blob - that represent the\nlabelling patterns of a human annotator. We compare the performances of the\nproposed algorithms for each annotator model on three benchmark hyperspectral\nimage datasets - Indian Pines, Pavia University and Salinas.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:00:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Santara", "Anirban", ""], ["Datta", "Jayeeta", ""], ["Sarkar", "Sourav", ""], ["Garg", "Ankur", ""], ["Padia", "Kirti", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1904.04552", "submitter": "Paul Voigtlaender", "authors": "Paul Voigtlaender and Jonathon Luiten and Bastian Leibe", "title": "BoLTVOS: Box-Level Tracking for Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approach video object segmentation (VOS) by splitting the task into two\nsub-tasks: bounding box level tracking, followed by bounding box segmentation.\nFollowing this paradigm, we present BoLTVOS (Box-Level Tracking for VOS), which\nconsists of an R-CNN detector conditioned on the first-frame bounding box to\ndetect the object of interest, a temporal consistency rescoring algorithm, and\na Box2Seg network that converts bounding boxes to segmentation masks. BoLTVOS\nperforms VOS using only the firstframe bounding box without the mask. We\nevaluate our approach on DAVIS 2017 and YouTube-VOS, and show that it\noutperforms all methods that do not perform first-frame fine-tuning. We further\npresent BoLTVOS-ft, which learns to segment the object in question using the\nfirst-frame mask while it is being tracked, without increasing the runtime.\nBoLTVOS-ft outperforms PReMVOS, the previously best performing VOS method on\nDAVIS 2016 and YouTube-VOS, while running up to 45 times faster. Our bounding\nbox tracker also outperforms all previous short-term and longterm trackers on\nthe bounding box level tracking datasets OTB 2015 and LTB35. A newer version of\nthis work can be found at arXiv:1911.12836.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:16:26 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 16:47:22 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Voigtlaender", "Paul", ""], ["Luiten", "Jonathon", ""], ["Leibe", "Bastian", ""]]}, {"id": "1904.04555", "submitter": "Bruno Ferrarini", "authors": "Bruno Ferrarini (1), Shoaib Ehsan (1), Adrien Bartoli (2), Ale\\v{s}\n  Leonardis (3), Klaus D. McDonald-Maier (1) ((1) University of Essex, CSEE,\n  Wivenhoe Park, Colchester CO4 3SQ, UK (2) Facult\\'e de M\\'edecine, 28 Place\n  Henri Dunant, 63000 Clermont-Ferrand, France (3) University of Birmingham,\n  School of Computer Science, Birmingham B15 2TT, UK)", "title": "Assessing Capsule Networks With Biased Data", "comments": "15 pages, 4 figures, 2 tables, Capsule Networks, Evaluation, Biased\n  Data", "journal-ref": null, "doi": "10.1007/978-3-030-20205-7_8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning based methods achieves impressive results in object\nclassification and detection. Utilizing representative data of the visual world\nduring the training phase is crucial to achieve good performance with such data\ndriven approaches. However, it not always possible to access bias-free datasets\nthus, robustness to biased data is a desirable property for a learning system.\nCapsule Networks have been introduced recently and their tolerance to biased\ndata has received little attention. This paper aims to fill this gap and\nproposes two experimental scenarios to assess the tolerance to imbalanced\ntraining data and to determine the generalization performance of a model with\nunfamiliar affine transformations of the images. This paper assesses dynamic\nrouting and EM routing based Capsule Networks and proposes a comparison with\nConvolutional Neural Networks in the two tested scenarios. The presented\nresults provide new insights into the behaviour of capsule networks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:20:29 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ferrarini", "Bruno", ""], ["Ehsan", "Shoaib", ""], ["Bartoli", "Adrien", ""], ["Leonardis", "Ale\u0161", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1904.04562", "submitter": "Eunwoo Kim", "authors": "Eunwoo Kim, Chanho Ahn, Philip H.S. Torr, Songhwai Oh", "title": "Deep Virtual Networks for Memory Efficient Inference of Multiple Tasks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks consume a large amount of memory by their nature. A natural\nquestion arises can we reduce that memory requirement whilst maintaining\nperformance. In particular, in this work we address the problem of memory\nefficient learning for multiple tasks. To this end, we propose a novel network\narchitecture producing multiple networks of different configurations, termed\ndeep virtual networks (DVNs), for different tasks. Each DVN is specialized for\na single task and structured hierarchically. The hierarchical structure, which\ncontains multiple levels of hierarchy corresponding to different numbers of\nparameters, enables multiple inference for different memory budgets. The\nbuilding block of a deep virtual network is based on a disjoint collection of\nparameters of a network, which we call a unit. The lowest level of hierarchy in\na deep virtual network is a unit, and higher levels of hierarchy contain lower\nlevels' units and other additional units. Given a budget on the number of\nparameters, a different level of a deep virtual network can be chosen to\nperform the task. A unit can be shared by different DVNs, allowing multiple\nDVNs in a single network. In addition, shared units provide assistance to the\ntarget task with additional knowledge learned from another tasks. This\ncooperative configuration of DVNs makes it possible to handle different tasks\nin a memory-aware manner. Our experiments show that the proposed method\noutperforms existing approaches for multiple tasks. Notably, ours is more\nefficient than others as it allows memory-aware inference for all tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:34:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kim", "Eunwoo", ""], ["Ahn", "Chanho", ""], ["Torr", "Philip H. S.", ""], ["Oh", "Songhwai", ""]]}, {"id": "1904.04571", "submitter": "Albert Pumarola", "authors": "Albert Pumarola and Jordi Sanchez and Gary P. T. Choi and Alberto\n  Sanfeliu and Francesc Moreno-Noguer", "title": "3DPeople: Modeling the Geometry of Dressed Humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D human shape estimation build upon parametric\nrepresentations that model very well the shape of the naked body, but are not\nappropriate to represent the clothing geometry. In this paper, we present an\napproach to model dressed humans and predict their geometry from single images.\nWe contribute in three fundamental aspects of the problem, namely, a new\ndataset, a novel shape parameterization algorithm and an end-to-end deep\ngenerative network for predicting shape.\n  First, we present 3DPeople, a large-scale synthetic dataset with 2.5 Million\nphoto-realistic images of 80 subjects performing 70 activities and wearing\ndiverse outfits. Besides providing textured 3D meshes for clothes and body, we\nannotate the dataset with segmentation masks, skeletons, depth, normal maps and\noptical flow. All this together makes 3DPeople suitable for a plethora of\ntasks.\n  We then represent the 3D shapes using 2D geometry images. To build these\nimages we propose a novel spherical area-preserving parameterization algorithm\nbased on the optimal mass transportation method. We show this approach to\nimprove existing spherical maps which tend to shrink the elongated parts of the\nfull body models such as the arms and legs, making the geometry images\nincomplete.\n  Finally, we design a multi-resolution deep generative network that, given an\ninput image of a dressed human, predicts his/her geometry image (and thus the\nclothed body shape) in an end-to-end manner. We obtain very promising results\nin jointly capturing body pose and clothing shape, both for synthetic\nvalidation and on the wild images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:57:04 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Pumarola", "Albert", ""], ["Sanchez", "Jordi", ""], ["Choi", "Gary P. T.", ""], ["Sanfeliu", "Alberto", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1904.04605", "submitter": "Huangxing Lin", "authors": "Huangxing Lin, Yanlong Li, Xinghao Ding, Weihong Zeng, Yue Huang, John\n  Paisley", "title": "Rain O'er Me: Synthesizing real rain to derain with data distillation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3005517", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised technique for learning to remove rain from images\nwithout using synthetic rain software. The method is based on a two-stage data\ndistillation approach: 1) A rainy image is first paired with a coarsely\nderained version using on a simple filtering technique (\"rain-to-clean\"). 2)\nThen a clean image is randomly matched with the rainy soft-labeled pair.\nThrough a shared deep neural network, the rain that is removed from the first\nimage is then added to the clean image to generate a second pair\n(\"clean-to-rain\"). The neural network simultaneously learns to map both images\nsuch that high resolution structure in the clean images can inform the\nderaining of the rainy images. Demonstrations show that this approach can\naddress those visual characteristics of rain not easily synthesized by software\nin the usual way.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 11:38:24 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 02:47:39 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lin", "Huangxing", ""], ["Li", "Yanlong", ""], ["Ding", "Xinghao", ""], ["Zeng", "Weihong", ""], ["Huang", "Yue", ""], ["Paisley", "John", ""]]}, {"id": "1904.04612", "submitter": "Mike Papadakis", "authors": "Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon", "title": "Automated Search for Configurations of Deep Neural Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are intensively used to solve a wide variety of\ncomplex problems. Although powerful, such systems require manual configuration\nand tuning. To this end, we view DNNs as configurable systems and propose an\nend-to-end framework that allows the configuration, evaluation and automated\nsearch for DNN architectures. Therefore, our contribution is threefold. First,\nwe model the variability of DNN architectures with a Feature Model (FM) that\ngeneralizes over existing architectures. Each valid configuration of the FM\ncorresponds to a valid DNN model that can be built and trained. Second, we\nimplement, on top of Tensorflow, an automated procedure to deploy, train and\nevaluate the performance of a configured model. Third, we propose a method to\nsearch for configurations and demonstrate that it leads to good DNN models. We\nevaluate our method by applying it on image classification tasks (MNIST,\nCIFAR-10) and show that, with limited amount of computation and training, our\nmethod can identify high-performing architectures (with high accuracy). We also\ndemonstrate that we outperform existing state-of-the-art architectures\nhandcrafted by ML researchers. Our FM and framework have been released %and are\npublicly available to support replication and future research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 11:56:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ghamizi", "Salah", ""], ["Cordy", "Maxime", ""], ["Papadakis", "Mike", ""], ["Traon", "Yves Le", ""]]}, {"id": "1904.04620", "submitter": "Jiwoong Choi", "authors": "Jiwoong Choi, Dayoung Chun, Hyun Kim, Hyuk-Jae Lee", "title": "Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization\n  Uncertainty for Autonomous Driving", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of object detection algorithms is becoming increasingly important in\nautonomous vehicles, and object detection at high accuracy and a fast inference\nspeed is essential for safe autonomous driving. A false positive (FP) from a\nfalse localization during autonomous driving can lead to fatal accidents and\nhinder safe and efficient driving. Therefore, a detection algorithm that can\ncope with mislocalizations is required in autonomous driving applications. This\npaper proposes a method for improving the detection accuracy while supporting a\nreal-time operation by modeling the bounding box (bbox) of YOLOv3, which is the\nmost representative of one-stage detectors, with a Gaussian parameter and\nredesigning the loss function. In addition, this paper proposes a method for\npredicting the localization uncertainty that indicates the reliability of bbox.\nBy using the predicted localization uncertainty during the detection process,\nthe proposed schemes can significantly reduce the FP and increase the true\npositive (TP), thereby improving the accuracy. Compared to a conventional\nYOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average\nprecision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD)\ndatasets, respectively. Nevertheless, the proposed algorithm is capable of\nreal-time detection at faster than 42 frames per second (fps) and shows a\nhigher accuracy than previous approaches with a similar fps. Therefore, the\nproposed algorithm is the most suitable for autonomous driving applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:23:55 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 11:11:02 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Choi", "Jiwoong", ""], ["Chun", "Dayoung", ""], ["Kim", "Hyun", ""], ["Lee", "Hyuk-Jae", ""]]}, {"id": "1904.04621", "submitter": "Abdullah Hamdi", "authors": "Abdullah Hamdi, Bernard Ghanem", "title": "Towards Analyzing Semantic Robustness of Deep Neural Networks", "comments": "Presented at European conference on computer vision (ECCV 2020)\n  Workshop on Adversarial Robustness in the Real World (\n  https://eccv20-adv-workshop.github.io/ ) [best paper award]. The code is\n  available at https://github.com/ajhamdi/semantic-robustness", "journal-ref": "ECCV 2020 Workshops", "doi": "10.1007/978-3-030-66415-2_2", "report-no": "10", "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive performance of Deep Neural Networks (DNNs) on various\nvision tasks, they still exhibit erroneous high sensitivity toward semantic\nprimitives (e.g. object pose). We propose a theoretically grounded analysis for\nDNN robustness in the semantic space. We qualitatively analyze different DNNs'\nsemantic robustness by visualizing the DNN global behavior as semantic maps and\nobserve interesting behavior of some DNNs. Since generating these semantic maps\ndoes not scale well with the dimensionality of the semantic space, we develop a\nbottom-up approach to detect robust regions of DNNs. To achieve this, we\nformalize the problem of finding robust semantic regions of the network as\noptimizing integral bounds and we develop expressions for update directions of\nthe region bounds. We use our developed formulations to quantitatively evaluate\nthe semantic robustness of different popular network architectures. We show\nthrough extensive experimentation that several networks, while trained on the\nsame dataset and enjoying comparable accuracy, do not necessarily perform\nsimilarly in semantic robustness. For example, InceptionV3 is more accurate\ndespite being less semantically robust than ResNet50. We hope that this tool\nwill serve as a milestone towards understanding the semantic robustness of\nDNNs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:26:55 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 18:59:10 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 20:06:36 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 15:53:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hamdi", "Abdullah", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.04661", "submitter": "Ke Yan", "authors": "Ke Yan, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu,\n  Ronald M. Summers", "title": "Holistic and Comprehensive Annotation of Clinically Significant Findings\n  on Diverse CT Images: Learning from Radiology Reports and Label Ontology", "comments": "CVPR 2019 oral, main paper + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radiologists' routine work, one major task is to read a medical image,\ne.g., a CT scan, find significant lesions, and describe them in the radiology\nreport. In this paper, we study the lesion description or annotation problem.\nGiven a lesion image, our aim is to predict a comprehensive set of relevant\nlabels, such as the lesion's body part, type, and attributes, which may assist\ndownstream fine-grained diagnosis. To address this task, we first design a deep\nlearning module to extract relevant semantic labels from the radiology reports\nassociated with the lesion images. With the images and text-mined labels, we\npropose a lesion annotation network (LesaNet) based on a multilabel\nconvolutional neural network (CNN) to learn all labels holistically.\nHierarchical relations and mutually exclusive relations between the labels are\nleveraged to improve the label prediction accuracy. The relations are utilized\nin a label expansion strategy and a relational hard example mining algorithm.\nWe also attach a simple score propagation layer on LesaNet to enhance recall\nand explore implicit relation between labels. Multilabel metric learning is\ncombined with classification to enable interpretable prediction. We evaluated\nLesaNet on the public DeepLesion dataset, which contains over 32K diverse\nlesion images. Experiments show that LesaNet can precisely annotate the lesions\nusing an ontology of 171 fine-grained labels with an average AUC of 0.9344.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:34:31 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 00:45:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yan", "Ke", ""], ["Peng", "Yifan", ""], ["Sandfort", "Veit", ""], ["Bagheri", "Mohammadhadi", ""], ["Lu", "Zhiyong", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1904.04663", "submitter": "Yabin Zhang", "authors": "Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan", "title": "Domain-Symmetric Networks for Adversarial Domain Adaptation", "comments": "CVPR 2019 camera ready. Codes are available at:\n  https://github.com/YBZh/SymNets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to learn a model of classifier for\nunlabeled samples on the target domain, given training data of labeled samples\non the source domain. Impressive progress is made recently by learning\ninvariant features via domain-adversarial training of deep networks. In spite\nof the recent progress, domain adaptation is still limited in achieving the\ninvariance of feature distributions at a finer category level. To this end, we\npropose in this paper a new domain adaptation method called Domain-Symmetric\nNetworks (SymNets). The proposed SymNet is based on a symmetric design of\nsource and target task classifiers, based on which we also construct an\nadditional classifier that shares with them its layer neurons. To train the\nSymNet, we propose a novel adversarial learning objective whose key design is\nbased on a two-level domain confusion scheme, where the category-level\nconfusion loss improves over the domain-level one by driving the learning of\nintermediate network features to be invariant at the corresponding categories\nof the two domains. Both domain discrimination and domain confusion are\nimplemented based on the constructed additional classifier. Since target\nsamples are unlabeled, we also propose a scheme of cross-domain training to\nhelp learn the target classifier. Careful ablation studies show the efficacy of\nour proposed method. In particular, based on commonly used base networks, our\nSymNets achieve the new state of the art on three benchmark domain adaptation\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:40:38 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 11:07:23 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zhang", "Yabin", ""], ["Tang", "Hui", ""], ["Jia", "Kui", ""], ["Tan", "Mingkui", ""]]}, {"id": "1904.04671", "submitter": "Selim Arikan", "authors": "Selim Arikan, Kiran Varanasi, Didier Stricker", "title": "Surface Defect Classification in Real-Time Using Convolutional Neural\n  Networks", "comments": "Supplementary material will follow", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface inspection systems are an important application domain for computer\nvision, as they are used for defect detection and classification in the\nmanufacturing industry. Existing systems use hand-crafted features which\nrequire extensive domain knowledge to create. Even though Convolutional neural\nnetworks (CNNs) have proven successful in many large-scale challenges,\nindustrial inspection systems have yet barely realized their potential due to\ntwo significant challenges: real-time processing speed requirements and\nspecialized narrow domain-specific datasets which are sometimes limited in\nsize. In this paper, we propose CNN models that are specifically designed to\nhandle capacity and real-time speed requirements of surface inspection systems.\nTo train and evaluate our network models, we created a surface image dataset\ncontaining more than 22000 labeled images with many types of surface materials\nand achieved 98.0% accuracy in binary defect classification. To solve the class\nimbalance problem in our datasets, we introduce neural data augmentation\nmethods which are also applicable to similar domains that suffer from the same\nproblem. Our results show that deep learning based methods are feasible to be\nused in surface inspection systems and outperform traditional methods in\naccuracy and inference time by considerable margins.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:22:38 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Arikan", "Selim", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.04686", "submitter": "Licheng Yu", "authors": "Licheng Yu and Xinlei Chen and Georgia Gkioxari and Mohit Bansal and\n  Tamara L. Berg and Dhruv Batra", "title": "Multi-Target Embodied Question Answering", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied Question Answering (EQA) is a relatively new task where an agent is\nasked to answer questions about its environment from egocentric perception. EQA\nmakes the fundamental assumption that every question, e.g., \"what color is the\ncar?\", has exactly one target (\"car\") being inquired about. This assumption\nputs a direct limitation on the abilities of the agent. We present a\ngeneralization of EQA - Multi-Target EQA (MT-EQA). Specifically, we study\nquestions that have multiple targets in them, such as \"Is the dresser in the\nbedroom bigger than the oven in the kitchen?\", where the agent has to navigate\nto multiple locations (\"dresser in bedroom\", \"oven in kitchen\") and perform\ncomparative reasoning (\"dresser\" bigger than \"oven\") before it can answer a\nquestion. Such questions require the development of entirely new modules or\ncomponents in the agent. To address this, we propose a modular architecture\ncomposed of a program generator, a controller, a navigator, and a VQA module.\nThe program generator converts the given question into sequential executable\nsub-programs; the navigator guides the agent to multiple locations pertinent to\nthe navigation-related sub-programs; and the controller learns to select\nrelevant observations along its path. These observations are then fed to the\nVQA module to predict the answer. We perform detailed analysis for each of the\nmodel components and show that our joint model can outperform previous methods\nand strong baselines by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:10:40 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Yu", "Licheng", ""], ["Chen", "Xinlei", ""], ["Gkioxari", "Georgia", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.04689", "submitter": "Davide Moltisanti", "authors": "Davide Moltisanti, Sanja Fidler, Dima Damen", "title": "Action Recognition from Single Timestamp Supervision in Untrimmed Videos", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising actions in videos relies on labelled supervision during training,\ntypically the start and end times of each action instance. This supervision is\nnot only subjective, but also expensive to acquire. Weak video-level\nsupervision has been successfully exploited for recognition in untrimmed\nvideos, however it is challenged when the number of different actions in\ntraining videos increases. We propose a method that is supervised by single\ntimestamps located around each action instance, in untrimmed videos. We replace\nexpensive action bounds with sampling distributions initialised from these\ntimestamps. We then use the classifier's response to iteratively update the\nsampling distributions. We demonstrate that these distributions converge to the\nlocation and extent of discriminative action segments. We evaluate our method\non three datasets for fine-grained recognition, with increasing number of\ndifferent actions per video, and show that single timestamps offer a reasonable\ncompromise between recognition performance and labelling effort, performing\ncomparably to full temporal supervision. Our update method improves top-1 test\naccuracy by up to 5.4%. across the evaluated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:11:48 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Moltisanti", "Davide", ""], ["Fidler", "Sanja", ""], ["Damen", "Dima", ""]]}, {"id": "1904.04691", "submitter": "Muhammad Usman Ghani", "authors": "Muhammad Usman Ghani, W. Clem Karl", "title": "Fast Enhanced CT Metal Artifact Reduction using Data Domain Deep\n  Learning", "comments": "Accepted for publication in IEEE Transactions on Computational\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtered back projection (FBP) is the most widely used method for image\nreconstruction in X-ray computed tomography (CT) scanners. The presence of\nhyper-dense materials in a scene, such as metals, can strongly attenuate\nX-rays, producing severe streaking artifacts in the reconstruction. These metal\nartifacts can greatly limit subsequent object delineation and information\nextraction from the images, restricting their diagnostic value. This problem is\nparticularly acute in the security domain, where there is great heterogeneity\nin the objects that can appear in a scene, highly accurate decisions must be\nmade quickly. The standard practical approaches to reducing metal artifacts in\nCT imagery are either simplistic non-adaptive interpolation-based projection\ndata completion methods or direct image post-processing methods. These standard\napproaches have had limited success. Motivated primarily by security\napplications, we present a new deep-learning-based metal artifact reduction\n(MAR) approach that tackles the problem in the projection data domain. We treat\nthe projection data corresponding to metal objects as missing data and train an\nadversarial deep network to complete the missing data in the projection domain.\nThe subsequent complete projection data is then used with FBP to reconstruct\nimage intended to be free of artifacts. This new approach results in an\nend-to-end MAR algorithm that is computationally efficient so practical and\nfits well into existing CT workflows allowing easy adoption in existing\nscanners. Training deep networks can be challenging, and another contribution\nof our work is to demonstrate that training data generated using an accurate\nX-ray simulation can be used to successfully train the deep network when\ncombined with transfer learning using limited real data sets. We demonstrate\nthe effectiveness and potential of our algorithm on simulated and real\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:13:41 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 01:11:12 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 16:16:35 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ghani", "Muhammad Usman", ""], ["Karl", "W. Clem", ""]]}, {"id": "1904.04696", "submitter": "Walter Simson Iv", "authors": "Walter Simson, R\\\"udiger G\\\"obl, Magdalini Paschali, Markus Kr\\\"onke,\n  Klemens Scheidhauer, Wolfgang Weber, Nassir Navab", "title": "End-to-End Learning-Based Ultrasound Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging is caught between the quest for the highest image quality,\nand the necessity for clinical usability. Our contribution is two-fold: First,\nwe propose a novel fully convolutional neural network for ultrasound\nreconstruction. Second, a custom loss function tailored to the modality is\nemployed for end-to-end training of the network. We demonstrate that training a\nnetwork to map time-delayed raw data to a minimum variance ground truth offers\nperformance increases in a clinical environment. In doing so, a path is\nexplored towards improved clinically viable ultrasound reconstruction. The\nproposed method displays both promising image reconstruction quality and\nacquisition frequency when integrated for live ultrasound scanning. A clinical\nevaluation is conducted to verify the diagnostic usefulness of the proposed\nmethod in a clinical setting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:24:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Simson", "Walter", ""], ["G\u00f6bl", "R\u00fcdiger", ""], ["Paschali", "Magdalini", ""], ["Kr\u00f6nke", "Markus", ""], ["Scheidhauer", "Klemens", ""], ["Weber", "Wolfgang", ""], ["Navab", "Nassir", ""]]}, {"id": "1904.04717", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen and Giorgos Tolias and Yannis Avrithis and Ondrej Chum", "title": "Label Propagation for Deep Semi-supervised Learning", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is becoming increasingly important because it can\ncombine data carefully labeled by humans with abundant unlabeled data to train\ndeep neural networks. Classic methods on semi-supervised learning that have\nfocused on transductive learning have not been fully exploited in the inductive\nframework followed by modern deep learning. The same holds for the manifold\nassumption---that similar examples should get the same prediction. In this\nwork, we employ a transductive label propagation method that is based on the\nmanifold assumption to make predictions on the entire dataset and use these\npredictions to generate pseudo-labels for the unlabeled data and train a deep\nneural network. At the core of the transductive method lies a nearest neighbor\ngraph of the dataset that we create based on the embeddings of the same\nnetwork.Therefore our learning process iterates between these two steps. We\nimprove performance on several datasets especially in the few labels regime and\nshow that our work is complementary to current state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:55:48 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Chum", "Ondrej", ""]]}, {"id": "1904.04741", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh", "title": "Generative Models for Novelty Detection: Applications in abnormal event\n  and situational change detection from data series", "comments": "PhD thesis Feb. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is a process for distinguishing the observations that\ndiffer in some respect from the observations that the model is trained on.\nNovelty detection is one of the fundamental requirements of a good\nclassification or identification system since sometimes the test data contains\nobservations that were not known at the training time. In other words, the\nnovelty class is often is not presented during the training phase or not well\ndefined.\n  In light of the above, one-class classifiers and generative methods can\nefficiently model such problems. However, due to the unavailability of data\nfrom the novelty class, training an end-to-end model is a challenging task\nitself. Therefore, detecting the Novel classes in unsupervised and\nsemi-supervised settings is a crucial step in such tasks.\n  In this thesis, we propose several methods to model the novelty detection\nproblem in unsupervised and semi-supervised fashion. The proposed frameworks\napplied to different related applications of anomaly and outlier detection\ntasks. The results show the superior of our proposed methods in compare to the\nbaselines and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:40:15 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""]]}, {"id": "1904.04744", "submitter": "Pierluigi Zama Ramirez", "authors": "Pierluigi Zama Ramirez, Alessio Tonioni, Samuele Salti, Luigi Di\n  Stefano", "title": "Learning Across Tasks and Domains", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have proven that many relevant visual tasks are closely related\none to another. Yet, this connection is seldom deployed in practice due to the\nlack of practical methodologies to transfer learned concepts across different\ntraining processes. In this work, we introduce a novel adaptation framework\nthat can operate across both task and domains. Our framework learns to transfer\nknowledge across tasks in a fully supervised domain (e.g., synthetic data) and\nuse this knowledge on a different domain where we have only partial supervision\n(e.g., real data). Our proposal is complementary to existing domain adaptation\ntechniques and extends them to cross tasks scenarios providing additional\nperformance gains. We prove the effectiveness of our framework across two\nchallenging tasks (i.e., monocular depth estimation and semantic segmentation)\nand four different domains (Synthia, Carla, Kitti, and Cityscapes).\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:48:44 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 15:27:16 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ramirez", "Pierluigi Zama", ""], ["Tonioni", "Alessio", ""], ["Salti", "Samuele", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1904.04745", "submitter": "Linwei Ye", "authors": "Linwei Ye, Mrigank Rochan, Zhi Liu and Yang Wang", "title": "Cross-Modal Self-Attention Network for Referring Image Segmentation", "comments": "Accepted to CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of referring image segmentation. Given an input image\nand a natural language expression, the goal is to segment the object referred\nby the language expression in the image. Existing works in this area treat the\nlanguage expression and the input image separately in their representations.\nThey do not sufficiently capture long-range correlations between these two\nmodalities. In this paper, we propose a cross-modal self-attention (CMSA)\nmodule that effectively captures the long-range dependencies between linguistic\nand visual features. Our model can adaptively focus on informative words in the\nreferring expression and important regions in the input image. In addition, we\npropose a gated multi-level fusion module to selectively integrate\nself-attentive cross-modal features corresponding to different levels in the\nimage. This module controls the information flow of features at different\nlevels. We validate the proposed approach on four evaluation datasets. Our\nproposed approach consistently outperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:51:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ye", "Linwei", ""], ["Rochan", "Mrigank", ""], ["Liu", "Zhi", ""], ["Wang", "Yang", ""]]}, {"id": "1904.04747", "submitter": "Rafael Rodrigues", "authors": "Rafael Rodrigues and Antonio M. G. Pinheiro", "title": "Segmentation of Skeletal Muscle in Thigh Dixon MRI Based on Texture\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of skeletal muscles in Magnetic Resonance Images (MRI) is\nessential for the study of muscle physiology and diagnosis of muscular\npathologies. However, manual segmentation of large MRI volumes is a\ntime-consuming task. The state-of-the-art on algorithms for muscle segmentation\nin MRI is still not very extensive and is somewhat database-dependent. In this\npaper, an automated segmentation method based on AdaBoost classification of\nlocal texture features is presented. The texture descriptor consists of the\nHistogram of Oriented Gradients (HOG), Wavelet-based features, and a set of\nstatistical measures computed from both the original and the Laplacian of\nGaussian filtering of the grayscale MRI. The classifier performance suggests\nthat texture analysis may be a helpful tool for designing a generalized and\nautomated MRI muscle segmentation framework. Furthermore, an atlas-based\napproach to individual muscle segmentation is also described in this paper. The\natlas is obtained by overlaying the muscle segmentation ground truth, provided\nby a radiologist, after image alignment using an appropriate affine\ntransformation. Then, it is used to define the muscle labels upon the AdaBoost\nbinary segmentation. The developed atlas method provides reasonable results\nwhen an accurate muscle tissue segmentation was obtained.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:56:06 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Rodrigues", "Rafael", ""], ["Pinheiro", "Antonio M. G.", ""]]}, {"id": "1904.04751", "submitter": "Aibek Alanov", "authors": "Aibek Alanov, Max Kochurov, Denis Volkhonskiy, Daniil Yashkov, Evgeny\n  Burnaev, Dmitry Vetrov", "title": "User-Controllable Multi-Texture Synthesis with Generative Adversarial\n  Networks", "comments": "8 pages paper, 17 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-texture synthesis model based on generative\nadversarial networks (GANs) with a user-controllable mechanism. The user\ncontrol ability allows to explicitly specify the texture which should be\ngenerated by the model. This property follows from using an encoder part which\nlearns a latent representation for each texture from the dataset. To ensure a\ndataset coverage, we use an adversarial loss function that penalizes for\nincorrect reproductions of a given texture. In experiments, we show that our\nmodel can learn descriptive texture manifolds for large datasets and from raw\ndata such as a collection of high-resolution photos. Moreover, we apply our\nmethod to produce 3D textures and show that it outperforms existing baselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:59:16 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:07:22 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Alanov", "Aibek", ""], ["Kochurov", "Max", ""], ["Volkhonskiy", "Denis", ""], ["Yashkov", "Daniil", ""], ["Burnaev", "Evgeny", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1904.04772", "submitter": "James Oldfield", "authors": "James Oldfield, Yannis Panagakis, Mihalis A. Nicolaou", "title": "Adversarial Learning of Disentangled and Generalizable Representations\n  for Visual Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a multitude of methods for image-to-image translation have\ndemonstrated impressive results on problems such as multi-domain or\nmulti-attribute transfer. The vast majority of such works leverages the\nstrengths of adversarial learning and deep convolutional autoencoders to\nachieve realistic results by well-capturing the target data distribution.\nNevertheless, the most prominent representatives of this class of methods do\nnot facilitate semantic structure in the latent space, and usually rely on\nbinary domain labels for test-time transfer. This leads to rigid models, unable\nto capture the variance of each domain label. In this light, we propose a novel\nadversarial learning method that (i) facilitates the emergence of latent\nstructure by semantically disentangling sources of variation, and (ii)\nencourages learning generalizable, continuous, and transferable latent codes\nthat enable flexible attribute mixing. This is achieved by introducing a novel\nloss function that encourages representations to result in uniformly\ndistributed class posteriors for disentangled attributes. In tandem with an\nalgorithm for inducing generalizable properties, the resulting representations\ncan be utilized for a variety of tasks such as intensity-preserving\nmulti-attribute image translation and synthesis, without requiring labelled\ntest data. We demonstrate the merits of the proposed method by a set of\nqualitative and quantitative experiments on popular databases such as MultiPIE,\nRaFD, and BU-3DFE, where our method outperforms other, state-of-the-art methods\nin tasks such as intensity-preserving multi-attribute transfer and synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:35:21 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 08:44:45 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 14:16:30 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Oldfield", "James", ""], ["Panagakis", "Yannis", ""], ["Nicolaou", "Mihalis A.", ""]]}, {"id": "1904.04776", "submitter": "Tianyang Zhao", "authors": "Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker,\n  Yibiao Zhao, Yizhou Wang, Ying Nian Wu", "title": "Multi-Agent Tensor Fusion for Contextual Trajectory Prediction", "comments": "Presented in CVPR 19:\n  http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Multi-Agent_Tensor_Fusion_for_Contextual_Trajectory_Prediction_CVPR_2019_paper.html\n  ; Architecture details available:\n  https://github.com/programmingLearner/MATF-architecture-details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of others' trajectories is essential for autonomous\ndriving. Trajectory prediction is challenging because it requires reasoning\nabout agents' past movements, social interactions among varying numbers and\nkinds of agents, constraints from the scene context, and the stochasticity of\nhuman behavior. Our approach models these interactions and constraints jointly\nwithin a novel Multi-Agent Tensor Fusion (MATF) network. Specifically, the\nmodel encodes multiple agents' past trajectories and the scene context into a\nMulti-Agent Tensor, then applies convolutional fusion to capture multiagent\ninteractions while retaining the spatial structure of agents and the scene\ncontext. The model decodes recurrently to multiple agents' future trajectories,\nusing adversarial loss to learn stochastic predictions. Experiments on both\nhighway driving and pedestrian crowd datasets show that the model achieves\nstate-of-the-art prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:38:11 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 13:38:00 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhao", "Tianyang", ""], ["Xu", "Yifei", ""], ["Monfort", "Mathew", ""], ["Choi", "Wongun", ""], ["Baker", "Chris", ""], ["Zhao", "Yibiao", ""], ["Wang", "Yizhou", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1904.04794", "submitter": "Ushasi Chaudhuri", "authors": "Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, and Mihai Datcu", "title": "CMIR-NET : A Deep Learning Based Model For Cross-Modal Retrieval In\n  Remote Sensing", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.02.006", "report-no": null, "categories": "eess.IV cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of cross-modal information retrieval in the domain of\nremote sensing. In particular, we are interested in two application scenarios:\ni) cross-modal retrieval between panchromatic (PAN) and multi-spectral imagery,\nand ii) multi-label image retrieval between very high resolution (VHR) images\nand speech based label annotations. Notice that these multi-modal retrieval\nscenarios are more challenging than the traditional uni-modal retrieval\napproaches given the inherent differences in distributions between the\nmodalities. However, with the growing availability of multi-source remote\nsensing data and the scarcity of enough semantic annotations, the task of\nmulti-modal retrieval has recently become extremely important. In this regard,\nwe propose a novel deep neural network based architecture which is considered\nto learn a discriminative shared feature space for all the input modalities,\nsuitable for semantically coherent information retrieval. Extensive experiments\nare carried out on the benchmark large-scale PAN - multi-spectral DSRSID\ndataset and the multi-label UC-Merced dataset. Together with the Merced\ndataset, we generate a corpus of speech signals corresponding to the labels.\nSuperior performance with respect to the current state-of-the-art is observed\nin all the cases.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:16:54 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 06:19:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chaudhuri", "Ushasi", ""], ["Banerjee", "Biplab", ""], ["Bhattacharya", "Avik", ""], ["Datcu", "Mihai", ""]]}, {"id": "1904.04812", "submitter": "Ching-Hang Chen", "authors": "Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover, Rohith MV,\n  Stefan Stojanov, and James M. Rehg", "title": "Unsupervised 3D Pose Estimation with Geometric Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an unsupervised learning approach to recover 3D human pose from 2D\nskeletal joints extracted from a single image. Our method does not require any\nmulti-view image data, 3D skeletons, correspondences between 2D-3D points, or\nuse previously learned 3D priors during training. A lifting network accepts 2D\nlandmarks as inputs and generates a corresponding 3D skeleton estimate. During\ntraining, the recovered 3D skeleton is reprojected on random camera viewpoints\nto generate new \"synthetic\" 2D poses. By lifting the synthetic 2D poses back to\n3D and re-projecting them in the original camera view, we can define\nself-consistency loss both in 3D and in 2D. The training can thus be self\nsupervised by exploiting the geometric self-consistency of the\nlift-reproject-lift process. We show that self-consistency alone is not\nsufficient to generate realistic skeletons, however adding a 2D pose\ndiscriminator enables the lifter to output valid 3D poses. Additionally, to\nlearn from 2D poses \"in the wild\", we train an unsupervised 2D domain adapter\nnetwork to allow for an expansion of 2D data. This improves results and\ndemonstrates the usefulness of 2D pose data for unsupervised 3D lifting.\nResults on Human3.6M dataset for 3D human pose estimation demonstrate that our\napproach improves upon the previous unsupervised methods by 30% and outperforms\nmany weakly supervised approaches that explicitly use 3D data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:53:50 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chen", "Ching-Hang", ""], ["Tyagi", "Ambrish", ""], ["Agrawal", "Amit", ""], ["Drover", "Dylan", ""], ["MV", "Rohith", ""], ["Stojanov", "Stefan", ""], ["Rehg", "James M.", ""]]}, {"id": "1904.04817", "submitter": "Logan Courtney", "authors": "Logan Courtney, Ramavarapu Sreenivas", "title": "Learning from Videos with Deep Convolutional LSTM Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of convolution LSTMs to simultaneously learn\nspatial- and temporal-information in videos. A deep network of convolutional\nLSTMs allows the model to access the entire range of temporal information at\nall spatial scales of the data. We describe our experiments involving\nconvolution LSTMs for lipreading that demonstrate the model is capable of\nselectively choosing which spatiotemporal scales are most relevant for a\nparticular dataset. The proposed deep architecture also holds promise in other\napplications where spatiotemporal features play a vital role without having to\nspecifically cater the design of the network for the particular spatiotemporal\nfeatures existent within the problem. For the Lip Reading in the Wild (LRW)\ndataset, our model slightly outperforms the previous state of the art (83.4%\nvs. 83.0%) and sets the new state of the art at 85.2% when the model is\npretrained on the Lip Reading Sentences (LRS2) dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:57:17 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Courtney", "Logan", ""], ["Sreenivas", "Ramavarapu", ""]]}, {"id": "1904.04821", "submitter": "Kai Chen", "authors": "Yuhang Cao, Kai Chen, Chen Change Loy, Dahua Lin", "title": "Prime Sample Attention in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a common paradigm in object detection frameworks to treat all samples\nequally and target at maximizing the performance on average. In this work, we\nrevisit this paradigm through a careful study on how different samples\ncontribute to the overall performance measured in terms of mAP. Our study\nsuggests that the samples in each mini-batch are neither independent nor\nequally important, and therefore a better classifier on average does not\nnecessarily mean higher mAP. Motivated by this study, we propose the notion of\nPrime Samples, those that play a key role in driving the detection performance.\nWe further develop a simple yet effective sampling and learning strategy called\nPrIme Sample Attention (PISA) that directs the focus of the training process\ntowards such samples. Our experiments demonstrate that it is often more\neffective to focus on prime samples than hard samples when training a detector.\nParticularly, On the MSCOCO dataset, PISA outperforms the random sampling\nbaseline and hard mining schemes, e.g., OHEM and Focal Loss, consistently by\naround 2% on both single-stage and two-stage detectors, even with a strong\nbackbone ResNeXt-101.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:59:18 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 15:40:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Cao", "Yuhang", ""], ["Chen", "Kai", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1904.04850", "submitter": "Yonatan Svirsky", "authors": "Yonatan Svirsky, Andrei Sharf", "title": "A Non-linear Differential CNN-Rendering Module for 3D Data Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a differential rendering module which allows neural\nnetworks to efficiently process cluttered data. The module is composed of\ncontinuous piecewise differentiable functions defined as a sensor array of\ncells embedded in 3D space. Our module is learnable and can be easily\nintegrated into neural networks allowing to optimize data rendering towards\nspecific learning tasks using gradient based methods in an end-to-end fashion.\nEssentially, the module's sensor cells are allowed to transform independently\nand locally focus and sense different parts of the 3D data. Thus, through their\noptimization process, cells learn to focus on important parts of the data,\nbypassing occlusions, clutter and noise. Since sensor cells originally lie on a\ngrid, this equals to a highly non-linear rendering of the scene into a 2D\nimage. Our module performs especially well in presence of clutter and\nocclusions. Similarly, it deals well with non-linear deformations and improves\nclassification accuracy through proper rendering of the data. In our\nexperiments, we apply our module to demonstrate efficient localization and\nclassification tasks in cluttered data both 2D and 3D.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:04:47 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Svirsky", "Yonatan", ""], ["Sharf", "Andrei", ""]]}, {"id": "1904.04854", "submitter": "Sergey Zakharov", "authors": "Sergey Zakharov, Wadim Kehl, Benjamin Planche, Andreas Hutter, and\n  Slobodan Ilic", "title": "3D Object Instance Recognition and Pose Estimation Using Triplet Loss\n  with Dynamic Margin", "comments": null, "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), pp. 552-559. IEEE, 2017", "doi": "10.1109/IROS.2017.8202207", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of 3D object instance recognition and\npose estimation of localized objects in cluttered environments using\nconvolutional neural networks. Inspired by the descriptor learning approach of\nWohlhart et al., we propose a method that introduces the dynamic margin in the\nmanifold learning triplet loss function. Such a loss function is designed to\nmap images of different objects under different poses to a lower-dimensional,\nsimilarity-preserving descriptor space on which efficient nearest neighbor\nsearch algorithms can be applied. Introducing the dynamic margin allows for\nfaster training times and better accuracy of the resulting low-dimensional\nmanifolds. Furthermore, we contribute the following: adding in-plane rotations\n(ignored by the baseline method) to the training, proposing new background\nnoise types that help to better mimic realistic scenarios and improve accuracy\nwith respect to clutter, adding surface normals as another powerful image\nmodality representing an object surface leading to better performance than\nmerely depth, and finally implementing an efficient online batch generation\nthat allows for better variability during the training phase. We perform an\nexhaustive evaluation to demonstrate the effects of our contributions.\nAdditionally, we assess the performance of the algorithm on the large BigBIRD\ndataset to demonstrate good scalability properties of the pipeline with respect\nto the number of models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:09:12 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zakharov", "Sergey", ""], ["Kehl", "Wadim", ""], ["Planche", "Benjamin", ""], ["Hutter", "Andreas", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1904.04858", "submitter": "Pedro Miraldo", "authors": "Joao Campos, Joao R. Cardoso, and Pedro Miraldo", "title": "POSEAMM: A Unified Framework for Solving Pose Problems using an\n  Alternating Minimization Method", "comments": "12 pages, 5 figures", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2019", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation is one of the most important problems in computer vision. It\ncan be divided in two different categories -- absolute and relative -- and may\ninvolve two different types of camera models: central and non-central.\nState-of-the-art methods have been designed to solve separately these problems.\nThis paper presents a unified framework that is able to solve any pose problem\nby alternating optimization techniques between two set of parameters, rotation\nand translation. In order to make this possible, it is necessary to define an\nobjective function that captures the problem at hand. Since the objective\nfunction will depend on the rotation and translation it is not possible to\nsolve it as a simple minimization problem. Hence the use of Alternating\nMinimization methods, in which the function will be alternatively minimized\nwith respect to the rotation and the translation. We show how to use our\nframework in three distinct pose problems. Our methods are then benchmarked\nwith both synthetic and real data, showing their better balance between\ncomputational time and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:31:18 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Campos", "Joao", ""], ["Cardoso", "Joao R.", ""], ["Miraldo", "Pedro", ""]]}, {"id": "1904.04862", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Bita Darvish Rouhani, Farinaz Koushanfar", "title": "SWNet: Small-World Neural Networks and Rapid Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large and highly accurate deep learning (DL) models is\ncomputationally costly. This cost is in great part due to the excessive number\nof trained parameters, which are well-known to be redundant and compressible\nfor the execution phase. This paper proposes a novel transformation which\nchanges the topology of the DL architecture such that it reaches an optimal\ncross-layer connectivity. This transformation leverages our important\nobservation that for a set level of accuracy, convergence is fastest when\nnetwork topology reaches the boundary of a Small-World Network. Small-world\ngraphs are known to possess a specific connectivity structure that enables\nenhanced signal propagation among nodes. Our small-world models, called SWNets,\nprovide several intriguing benefits: they facilitate data (gradient) flow\nwithin the network, enable feature-map reuse by adding long-range connections\nand accommodate various network architectures/datasets. Compared to densely\nconnected networks (e.g., DenseNets), SWNets require a substantially fewer\nnumber of training parameters while maintaining a similar level of\nclassification accuracy. We evaluate our networks on various DL model\narchitectures and image classification datasets, namely, CIFAR10, CIFAR100, and\nILSVRC (ImageNet). Our experiments demonstrate an average of ~2.1x improvement\nin convergence speed to the desired accuracy\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:41:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Rouhani", "Bita Darvish", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1904.04868", "submitter": "Vinh Tran", "authors": "Vinh Tran, Yang Wang, Minh Hoai", "title": "Back to the Future: Knowledge Distillation for Human Action Anticipation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of training a neural network to anticipate human actions\nin video. This task is challenging given the complexity of video data, the\nstochastic nature of the future, and the limited amount of annotated training\ndata. In this paper, we propose a novel knowledge distillation framework that\nuses an action recognition network to supervise the training of an action\nanticipation network, guiding the latter to attend to the relevant information\nneeded for correctly anticipating the future actions. This framework is\npossible thanks to a novel loss function to account for positional shifts of\nsemantic concepts in a dynamic video. The knowledge distillation framework is a\nform of self-supervised learning, and it takes advantage of unlabeled data.\nExperimental results on JHMDB and EPIC-KITCHENS dataset show the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:55:44 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Tran", "Vinh", ""], ["Wang", "Yang", ""], ["Hoai", "Minh", ""]]}, {"id": "1904.04875", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Yu Ji, Yuqi Ding, Jinwei Ye, S. Susan Young, Jingyi Yu", "title": "Non-Lambertian Surface Shape and Reflectance Reconstruction Using\n  Concentric Multi-Spectral Light Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the shape and reflectance of non-Lambertian surfaces remains a\nchallenging problem in computer vision since the view-dependent appearance\ninvalidates traditional photo-consistency constraint. In this paper, we\nintroduce a novel concentric multi-spectral light field (CMSLF) design that is\nable to recover the shape and reflectance of surfaces with arbitrary material\nin one shot. Our CMSLF system consists of an array of cameras arranged on\nconcentric circles where each ring captures a specific spectrum. Coupled with a\nmulti-spectral ring light, we are able to sample viewpoint and lighting\nvariations in a single shot via spectral multiplexing. We further show that\nsuch concentric camera/light setting results in a unique pattern of specular\nchanges across views that enables robust depth estimation. We formulate a\nphysical-based reflectance model on CMSLF to estimate depth and multi-spectral\nreflectance map without imposing any surface prior. Extensive synthetic and\nreal experiments show that our method outperforms state-of-the-art light\nfield-based techniques, especially in non-Lambertian scenes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 19:12:29 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 04:48:51 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Ji", "Yu", ""], ["Ding", "Yuqi", ""], ["Ye", "Jinwei", ""], ["Young", "S. Susan", ""], ["Yu", "Jingyi", ""]]}, {"id": "1904.04882", "submitter": "Yang Wang", "authors": "Supreeth Narasimhaswamy, Zhengwei Wei, Yang Wang, Justin Zhang, Minh\n  Hoai", "title": "Contextual Attention for Hand Detection in the Wild", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hand-CNN, a novel convolutional network architecture for detecting\nhand masks and predicting hand orientations in unconstrained images. Hand-CNN\nextends MaskRCNN with a novel attention mechanism to incorporate contextual\ncues in the detection process. This attention mechanism can be implemented as\nan efficient network module that captures non-local dependencies between\nfeatures. This network module can be inserted at different stages of an object\ndetection network, and the entire detector can be trained end-to-end.\n  We also introduce a large-scale annotated hand dataset containing hands in\nunconstrained images for training and evaluation. We show that Hand-CNN\noutperforms existing methods on several datasets, including our hand detection\nbenchmark and the publicly available PASCAL VOC human layout challenge. We also\nconduct ablation studies on hand detection to show the effectiveness of the\nproposed contextual attention module.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 19:45:42 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Narasimhaswamy", "Supreeth", ""], ["Wei", "Zhengwei", ""], ["Wang", "Yang", ""], ["Zhang", "Justin", ""], ["Hoai", "Minh", ""]]}, {"id": "1904.04925", "submitter": "Ziyuan Zhang", "authors": "Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu, Jian Wan\n  and Nanxin Wang", "title": "Gait Recognition via Disentangled Representation Learning", "comments": "To appear at CVPR 2019 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait, the walking pattern of individuals, is one of the most important\nbiometrics modalities. Most of the existing gait recognition methods take\nsilhouettes or articulated body models as the gait features. These methods\nsuffer from degraded recognition performance when handling confounding\nvariables, such as clothing, carrying and view angle. To remedy this issue, we\npropose a novel AutoEncoder framework to explicitly disentangle pose and\nappearance features from RGB imagery and the LSTM-based integration of pose\nfeatures over time produces the gait feature. In addition, we collect a\nFrontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view\nwalking, which is a challenging problem since it contains minimal gait cues\ncompared to other views. FVG also includes other important variations, e.g.,\nwalking speed, carrying, and clothing. With extensive experiments on CASIA-B,\nUSF and FVG datasets, our method demonstrates superior performance to the state\nof the arts quantitatively, the ability of feature disentanglement\nqualitatively, and promising computational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 21:49:58 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhang", "Ziyuan", ""], ["Tran", "Luan", ""], ["Yin", "Xi", ""], ["Atoum", "Yousef", ""], ["Liu", "Xiaoming", ""], ["Wan", "Jian", ""], ["Wang", "Nanxin", ""]]}, {"id": "1904.04933", "submitter": "Luan Tran", "authors": "Luan Tran, Feng Liu and Xiaoming Liu", "title": "Towards High-fidelity Nonlinear 3D Face Morphable Model", "comments": "CVPR 2019. Project webpage:\n  http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding 3D morphable basis functions into deep neural networks opens great\npotential for models with better representation power. However, to faithfully\nlearn those models from an image collection, it requires strong regularization\nto overcome ambiguities involved in the learning process. This critically\nprevents us from learning high fidelity face models which are needed to\nrepresent face images in high level of details. To address this problem, this\npaper presents a novel approach to learn additional proxies as means to\nside-step strong regularizations, as well as, leverages to promote detailed\nshape/albedo. To ease the learning, we also propose to use a dual-pathway\nnetwork, a carefully-designed architecture that brings a balance between global\nand local-based models. By improving the nonlinear 3D morphable model in both\nlearning objective and network architecture, we present a model which is\nsuperior in capturing higher level of details than the linear or its precedent\nnonlinear counterparts. As a result, our model achieves state-of-the-art\nperformance on 3D face reconstruction by solely optimizing latent\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 22:05:35 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Tran", "Luan", ""], ["Liu", "Feng", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1904.04957", "submitter": "Tristan Hascoet", "authors": "Tristan Hascoet, Yasuo Ariki and Tetsuya Takiguchi", "title": "On zero-shot recognition of generic objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in computer vision are the result of a healthy\ncompetition among researchers on high quality, task-specific, benchmarks. After\na decade of active research, zero-shot learning (ZSL) models accuracy on the\nImagenet benchmark remains far too low to be considered for practical object\nrecognition applications. In this paper, we argue that the main reason behind\nthis apparent lack of progress is the poor quality of this benchmark. We\nhighlight major structural flaws of the current benchmark and analyze different\nfactors impacting the accuracy of ZSL models. We show that the actual\nclassification accuracy of existing ZSL models is significantly higher than was\npreviously thought as we account for these flaws. We then introduce the notion\nof structural bias specific to ZSL datasets. We discuss how the presence of\nthis new form of bias allows for a trivial solution to the standard benchmark\nand conclude on the need for a new benchmark. We then detail the semi-automated\nconstruction of a new benchmark to address these flaws.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:04:35 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Hascoet", "Tristan", ""], ["Ariki", "Yasuo", ""], ["Takiguchi", "Tetsuya", ""]]}, {"id": "1904.04960", "submitter": "Jia Li", "authors": "Kaiwen Yu, Jia Li, Yu Zhang, Yifan Zhao, Long Xu", "title": "Image Quality Assessment for Omnidirectional Cross-reference Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the development of virtual reality (VR), omnidirectional images\nplay an important role in producing multimedia content with immersive\nexperience. However, despite various existing approaches for omnidirectional\nimage stitching, how to quantitatively assess the quality of stitched images is\nstill insufficiently explored. To address this problem, we establish a novel\nomnidirectional image dataset containing stitched images as well as\ndual-fisheye images captured from standard quarters of 0$^\\circ$, 90$^\\circ$,\n180$^\\circ$ and 270$^\\circ$. In this manner, when evaluating the quality of an\nimage stitched from a pair of fisheye images (e.g., 0$^\\circ$ and 180$^\\circ$),\nthe other pair of fisheye images (e.g., 90$^\\circ$ and 270$^\\circ$) can be used\nas the cross-reference to provide ground-truth observations of the stitching\nregions. Based on this dataset, we further benchmark six widely used stitching\nmodels with seven evaluation metrics for IQA. To the best of our knowledge, it\nis the first dataset that focuses on assessing the stitching quality of\nomnidirectional images.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:11:07 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 01:03:02 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Yu", "Kaiwen", ""], ["Li", "Jia", ""], ["Zhang", "Yu", ""], ["Zhao", "Yifan", ""], ["Xu", "Long", ""]]}, {"id": "1904.04971", "submitter": "Jiquan Ngiam", "authors": "Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam", "title": "CondConv: Conditionally Parameterized Convolutions for Efficient\n  Inference", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional layers are one of the basic building blocks of modern deep\nneural networks. One fundamental assumption is that convolutional kernels\nshould be shared for all examples in a dataset. We propose conditionally\nparameterized convolutions (CondConv), which learn specialized convolutional\nkernels for each example. Replacing normal convolutions with CondConv enables\nus to increase the size and capacity of a network, while maintaining efficient\ninference. We demonstrate that scaling networks with CondConv improves the\nperformance and inference cost trade-off of several existing convolutional\nneural network architectures on both classification and detection tasks. On\nImageNet classification, our CondConv approach applied to EfficientNet-B0\nachieves state-of-the-art performance of 78.3% accuracy with only 413M\nmultiply-adds. Code and checkpoints for the CondConv Tensorflow layer and\nCondConv-EfficientNet models are available at:\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:46:48 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 17:58:56 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 00:53:44 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Yang", "Brandon", ""], ["Bender", "Gabriel", ""], ["Le", "Quoc V.", ""], ["Ngiam", "Jiquan", ""]]}, {"id": "1904.04972", "submitter": "Zhifeng Li", "authors": "Hao Wang, Dihong Gong, Zhifeng Li and Wei Liu", "title": "Decorrelated Adversarial Learning for Age-Invariant Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing research interest in age-invariant face\nrecognition. However, matching faces with big age gaps remains a challenging\nproblem, primarily due to the significant discrepancy of face appearances\ncaused by aging. To reduce such a discrepancy, in this paper we propose a novel\nalgorithm to remove age-related components from features mixed with both\nidentity and age information. Specifically, we factorize a mixed face feature\ninto two uncorrelated components: identity-dependent component and\nage-dependent component, where the identity-dependent component includes\ninformation that is useful for face recognition. To implement this idea, we\npropose the Decorrelated Adversarial Learning (DAL) algorithm, where a\nCanonical Mapping Module (CMM) is introduced to find the maximum correlation\nbetween the paired features generated by a backbone network, while the backbone\nnetwork and the factorization module are trained to generate features reducing\nthe correlation. Thus, the proposed model learns the decomposed features of age\nand identity whose correlation is significantly reduced. Simultaneously, the\nidentity-dependent feature and the age-dependent feature are respectively\nsupervised by ID and age preserving signals to ensure that they both contain\nthe correct information. Extensive experiments are conducted on popular\npublic-domain face aging datasets (FG-NET, MORPH Album 2, and CACD-VS) to\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:47:09 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Wang", "Hao", ""], ["Gong", "Dihong", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""]]}, {"id": "1904.04975", "submitter": "He Lingxiao", "authors": "Lingxiao He, Yinggang Wang, Wu Liu, Xingyu Liao, He Zhao, Zhenan Sun,\n  Jiashi Feng", "title": "Foreground-aware Pyramid Reconstruction for Alignment-free Occluded\n  Person Re-identification", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-identifying a person across multiple disjoint camera views is important\nfor intelligent video surveillance, smart retailing and many other\napplications. However, existing person re-identification (ReID) methods are\nchallenged by the ubiquitous occlusion over persons and suffer from performance\ndegradation. This paper proposes a novel occlusion-robust and alignment-free\nmodel for occluded person ReID and extends its application to realistic and\ncrowded scenarios. The proposed model first leverages the full convolution\nnetwork (FCN) and pyramid pooling to extract spatial pyramid features. Then an\nalignment-free matching approach, namely Foreground-aware Pyramid\nReconstruction (FPR), is developed to accurately compute matching scores\nbetween occluded persons, despite their different scales and sizes. FPR uses\nthe error from robust reconstruction over spatial pyramid features to measure\nsimilarities between two persons. More importantly, we design an\nocclusion-sensitive foreground probability generator that focuses more on clean\nhuman body parts to refine the similarity computation with less contamination\nfrom occlusion. The FPR is easily embedded into any end-to-end person ReID\nmodels. The effectiveness of the proposed method is clearly demonstrated by the\nexperimental results (Rank-1 accuracy) on three occluded person datasets:\nPartial REID (78.30\\%), Partial iLIDS (68.08\\%) and Occluded REID (81.00\\%);\nand three benchmark person datasets: Market1501 (95.42\\%), DukeMTMC (88.64\\%)\nand CUHK03 (76.08\\%)\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 02:04:24 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 03:46:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["He", "Lingxiao", ""], ["Wang", "Yinggang", ""], ["Liu", "Wu", ""], ["Liao", "Xingyu", ""], ["Zhao", "He", ""], ["Sun", "Zhenan", ""], ["Feng", "Jiashi", ""]]}, {"id": "1904.04978", "submitter": "Dawei Du", "authors": "Congcong Li, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Qi Tian,\n  Longyin Wen, Siwei Lyu", "title": "Data Priming Network for Automatic Check-Out", "comments": "Accepted to ACM MM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Check-Out (ACO) receives increased interests in recent years. An\nimportant component of the ACO system is the visual item counting, which\nrecognizes the categories and counts of the items chosen by the customers.\nHowever, the training of such a system is challenged by the domain adaptation\nproblem, in which the training data are images from isolated items while the\ntesting images are for collections of items. Existing methods solve this\nproblem with data augmentation using synthesized images, but the image\nsynthesis leads to unreal images that affect the training process. In this\npaper, we propose a new data priming method to solve the domain adaptation\nproblem. Specifically, we first use pre-augmentation data priming, in which we\nremove distracting background from the training images using the coarse-to-fine\nstrategy and select images with realistic view angles by the pose pruning\nmethod. In the post-augmentation step, we train a data priming network using\ndetection and counting collaborative learning, and select more reliable images\nfrom testing data to fine-tune the final visual item tallying network.\nExperiments on the large scale Retail Product Checkout (RPC) dataset\ndemonstrate the superiority of the proposed method, i.e., we achieve 80.51%\ncheckout accuracy compared with 56.68% of the baseline methods. The source\ncodes can be found in https://isrc.iscas.ac.cn/gitlab/research/acm-mm-2019-ACO.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 02:12:48 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 17:12:56 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 03:04:32 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Li", "Congcong", ""], ["Du", "Dawei", ""], ["Zhang", "Libo", ""], ["Luo", "Tiejian", ""], ["Wu", "Yanjun", ""], ["Tian", "Qi", ""], ["Wen", "Longyin", ""], ["Lyu", "Siwei", ""]]}, {"id": "1904.04985", "submitter": "Noa Garcia", "authors": "Noa Garcia, Benjamin Renoust, Yuta Nakashima", "title": "Context-Aware Embeddings for Automatic Art Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3323873.3325028", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic art analysis aims to classify and retrieve artistic representations\nfrom a collection of images by using computer vision and machine learning\ntechniques. In this work, we propose to enhance visual representations from\nneural networks with contextual artistic information. Whereas visual\nrepresentations are able to capture information about the content and the style\nof an artwork, our proposed context-aware embeddings additionally encode\nrelationships between different artistic attributes, such as author, school, or\nhistorical period. We design two different approaches for using context in\nautomatic art analysis. In the first one, contextual data is obtained through a\nmulti-task learning model, in which several attributes are trained together to\nfind visual relationships between elements. In the second approach, context is\nobtained through an art-specific knowledge graph, which encodes relationships\nbetween artistic attributes. An exhaustive evaluation of both of our models in\nseveral art analysis problems, such as author identification, type\nclassification, or cross-modal retrieval, show that performance is improved by\nup to 7.3% in art classification and 37.24% in retrieval when context-aware\nembeddings are used.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 02:37:49 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Garcia", "Noa", ""], ["Renoust", "Benjamin", ""], ["Nakashima", "Yuta", ""]]}, {"id": "1904.04986", "submitter": "Zhexiong Shang", "authors": "Zhexiong Shang, Chongsheng Cheng, Zhigang Shen", "title": "A Data Fusion Platform for Supporting Bridge Deck Condition Monitoring\n  by Merging Aerial and Ground Inspection Imagery", "comments": "8 pages, 5 figures, submitted to i3ce 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UAVs showed great efficiency on scanning bridge decks surface by taking a\nsingle shot or through stitching a couple of overlaid still images. If\npotential surface deficits are identified through aerial images, subsequent\nground inspections can be scheduled. This two-phase inspection procedure showed\ngreat potentials on increasing field inspection productivity. Since aerial and\nground inspection images are taken at different scales, a tool to properly fuse\nthese multi-scale images is needed for improving the current bridge deck\ncondition monitoring practice. In response to this need a data fusion platform\nis introduced in this study. Using this proposed platform multi-scale images\ntaken by different inspection devices can be fused through geo-referencing. As\npart of the platform, a web-based user interface is developed to organize and\nvisualize those images with inspection notes under users queries. For\nillustration purpose, a case study involving multi-scale optical and infrared\nimages from UAV and ground inspector, and its implementation using the proposed\nplatform is presented.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 02:40:45 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Shang", "Zhexiong", ""], ["Cheng", "Chongsheng", ""], ["Shen", "Zhigang", ""]]}, {"id": "1904.04987", "submitter": "Zhexiong Shang", "authors": "Zhexiong Shang, Zhigang Shen", "title": "Vision-model-based Real-time Localization of Unmanned Aerial Vehicle for\n  Autonomous Structure Inspection under GPS-denied Environment", "comments": "8 pages, 5 figures, submitted to i3ce 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UAVs have been widely used in visual inspections of buildings, bridges and\nother structures. In either outdoor autonomous or semi-autonomous flights\nmissions strong GPS signal is vital for UAV to locate its own positions.\nHowever, strong GPS signal is not always available, and it can degrade or fully\nloss underneath large structures or close to power lines, which can cause\nserious control issues or even UAV crashes. Such limitations highly restricted\nthe applications of UAV as a routine inspection tool in various domains. In\nthis paper a vision-model-based real-time self-positioning method is proposed\nto support autonomous aerial inspection without the need of GPS support.\nCompared to other localization methods that requires additional onboard\nsensors, the proposed method uses a single camera to continuously estimate the\ninflight poses of UAV. Each step of the proposed method is discussed in detail,\nand its performance is tested through an indoor test case.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 02:43:52 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Shang", "Zhexiong", ""], ["Shen", "Zhigang", ""]]}, {"id": "1904.04989", "submitter": "Peng Chu", "authors": "Peng Chu and Haibin Ling", "title": "FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional\n  Assignment for Online Multiple Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association-based multiple object tracking (MOT) involves multiple\nseparated modules processed or optimized differently, which results in complex\nmethod design and requires non-trivial tuning of parameters. In this paper, we\npresent an end-to-end model, named FAMNet, where Feature extraction, Affinity\nestimation and Multi-dimensional assignment are refined in a single network.\nAll layers in FAMNet are designed differentiable thus can be optimized jointly\nto learn the discriminative features and higher-order affinity model for robust\nMOT, which is supervised by the loss directly from the assignment ground truth.\nWe also integrate single object tracking technique and a dedicated target\nmanagement scheme into the FAMNet-based tracking system to further recover\nfalse negatives and inhibit noisy target candidates generated by the external\ndetector. The proposed method is evaluated on a diverse set of benchmarks\nincluding MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising\nperformance on all of them in comparison with state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:07:00 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chu", "Peng", ""], ["Ling", "Haibin", ""]]}, {"id": "1904.04992", "submitter": "Jia Li", "authors": "Jia Li, Kui Fu, Shengwei Zhao, Shiming Ge", "title": "Spatiotemporal Knowledge Distillation for Efficient Estimation of Aerial\n  Video Saliency", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2946102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of video saliency estimation techniques has achieved\nsignificant advances along with the rapid development of Convolutional Neural\nNetworks (CNNs). However, devices like cameras and drones may have limited\ncomputational capability and storage space so that the direct deployment of\ncomplex deep saliency models becomes infeasible. To address this problem, this\npaper proposes a dynamic saliency estimation approach for aerial videos via\nspatiotemporal knowledge distillation. In this approach, five components are\ninvolved, including two teachers, two students and the desired spatiotemporal\nmodel. The knowledge of spatial and temporal saliency is first separately\ntransferred from the two complex and redundant teachers to their simple and\ncompact students, and the input scenes are also degraded from high-resolution\nto low-resolution to remove the probable data redundancy so as to greatly speed\nup the feature extraction process. After that, the desired spatiotemporal model\nis further trained by distilling and encoding the spatial and temporal saliency\nknowledge of two students into a unified network. In this manner, the\ninter-model redundancy can be further removed for the effective estimation of\ndynamic saliency on aerial videos. Experimental results show that the proposed\napproach outperforms ten state-of-the-art models in estimating visual saliency\non aerial videos, while its speed reaches up to 28,738 FPS on the GPU platform.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:41:10 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Li", "Jia", ""], ["Fu", "Kui", ""], ["Zhao", "Shengwei", ""], ["Ge", "Shiming", ""]]}, {"id": "1904.04998", "submitter": "Ariel Gordon", "authors": "Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova", "title": "Depth from Videos in the Wild: Unsupervised Monocular Depth Learning\n  from Unknown Cameras", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 8977-8986", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for simultaneous learning of depth, egomotion,\nobject motion, and camera intrinsics from monocular videos, using only\nconsistency across neighboring video frames as supervision signal. Similarly to\nprior work, our method learns by applying differentiable warping to frames and\ncomparing the result to adjacent ones, but it provides several improvements: We\naddress occlusions geometrically and differentiably, directly using the depth\nmaps as predicted during training. We introduce randomized layer normalization,\na novel powerful regularizer, and we account for object motion relative to the\nscene. To the best of our knowledge, our work is the first to learn the camera\nintrinsic parameters, including lens distortion, from video in an unsupervised\nmanner, thereby allowing us to extract accurate depth and motion from arbitrary\nvideos of unknown origin at scale. We evaluate our results on the Cityscapes,\nKITTI and EuRoC datasets, establishing new state of the art on depth prediction\nand odometry, and demonstrate qualitatively that depth prediction can be\nlearned from a collection of YouTube videos.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 04:16:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Gordon", "Ariel", ""], ["Li", "Hanhan", ""], ["Jonschkowski", "Rico", ""], ["Angelova", "Anelia", ""]]}, {"id": "1904.05003", "submitter": "Wenbing Huang", "authors": "Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, Junzhou Huang", "title": "Semi-Supervised Graph Classification: A Hierarchical Graph Perspective", "comments": "12 pages, WWW-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node classification and graph classification are two graph learning problems\nthat predict the class label of a node and the class label of a graph\nrespectively. A node of a graph usually represents a real-world entity, e.g., a\nuser in a social network, or a protein in a protein-protein interaction\nnetwork. In this work, we consider a more challenging but practically useful\nsetting, in which a node itself is a graph instance. This leads to a\nhierarchical graph perspective which arises in many domains such as social\nnetwork, biological network and document collection. For example, in a social\nnetwork, a group of people with shared interests forms a user group, whereas a\nnumber of user groups are interconnected via interactions or common members. We\nstudy the node classification problem in the hierarchical graph where a `node'\nis a graph instance, e.g., a user group in the above example. As labels are\nusually limited in real-world data, we design two novel semi-supervised\nsolutions named \\underline{SE}mi-supervised gr\\underline{A}ph\nc\\underline{L}assification via \\underline{C}autious/\\underline{A}ctive\n\\underline{I}teration (or SEAL-C/AI in short). SEAL-C/AI adopt an iterative\nframework that takes turns to build or update two classifiers, one working at\nthe graph instance level and the other at the hierarchical graph level. To\nsimplify the representation of the hierarchical graph, we propose a novel\nsupervised, self-attentive graph embedding method called SAGE, which embeds\ngraph instances of arbitrary size into fixed-length vectors. Through\nexperiments on synthetic data and Tencent QQ group data, we demonstrate that\nSEAL-C/AI not only outperform competing methods by a significant margin in\nterms of accuracy/Macro-F1, but also generate meaningful interpretations of the\nlearned representations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 04:53:20 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Jia", ""], ["Rong", "Yu", ""], ["Cheng", "Hong", ""], ["Meng", "Helen", ""], ["Huang", "Wenbing", ""], ["Huang", "Junzhou", ""]]}, {"id": "1904.05005", "submitter": "Xun Yang", "authors": "Xun Yang, Meng Wang, Dacheng Tao", "title": "Person Re-identification with Metric Learning using Privileged\n  Information", "comments": "Accepted for IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the promising progress made in recent years, person re-identification\nremains a challenging task due to complex variations in human appearances from\ndifferent camera views. This paper presents a logistic discriminant metric\nlearning method for this challenging problem. Different with most existing\nmetric learning algorithms, it exploits both original data and auxiliary data\nduring training, which is motivated by the new machine learning paradigm -\nLearning Using Privileged Information. Such privileged information is a kind of\nauxiliary knowledge which is only available during training. Our goal is to\nlearn an optimal distance function by constructing a locally adaptive decision\nrule with the help of privileged information. We jointly learn two distance\nmetrics by minimizing the empirical loss penalizing the difference between the\ndistance in the original space and that in the privileged space. In our\nsetting, the distance in the privileged space functions as a local decision\nthreshold, which guides the decision making in the original space like a\nteacher. The metric learned from the original space is used to compute the\ndistance between a probe image and a gallery image during testing. In addition,\nwe extend the proposed approach to a multi-view setting which is able to\nexplore the complementation of multiple feature representations. In the\nmulti-view setting, multiple metrics corresponding to different original\nfeatures are jointly learned, guided by the same privileged information.\nBesides, an effective iterative optimization scheme is introduced to\nsimultaneously optimize the metrics and the assigned metric weights. Experiment\nresults on several widely-used datasets demonstrate that the proposed approach\nis superior to global decision threshold based methods and outperforms most\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 05:01:28 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Yang", "Xun", ""], ["Wang", "Meng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.05008", "submitter": "Ushasi Chaudhuri", "authors": "Ushasi Chaudhuri, Partha Bhowmick, Jayanta Mukhopadhyay", "title": "Efficient Retrieval of Logos Using Rough Set Reducts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for similar logos in the registered logo database is a very\nimportant and tedious task at the trademark office. Speed and accuracy are two\naspects that one must attend to while developing a system for retrieval of\nlogos. In this paper, we propose a rough-set based method to quantify the\nstructural information in a logo image that can be used to efficiently index an\nimage. A logo is split into a number of polygons, and for each polygon, we\ncompute the tight upper and lower approximations based on the principles of a\nrough set. This representation is used for forming feature vectors for\nretrieval of logos. Experimentation on a standard data set shows the usefulness\nof the proposed technique. It is computationally efficient and also provides\nretrieval results at high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 05:34:04 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chaudhuri", "Ushasi", ""], ["Bhowmick", "Partha", ""], ["Mukhopadhyay", "Jayanta", ""]]}, {"id": "1904.05019", "submitter": "Yurun Tian", "authors": "Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, Vassileios\n  Balntas", "title": "SOSNet: Second Order Similarity Regularization for Local Descriptor\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that Second Order Similarity (SOS) has been used with\nsignificant success in tasks such as graph matching and clustering, it has not\nbeen exploited for learning local descriptors. In this work, we explore the\npotential of SOS in the field of descriptor learning by building upon the\nintuition that a positive pair of matching points should exhibit similar\ndistances with respect to other points in the embedding space. Thus, we propose\na novel regularization term, named Second Order Similarity Regularization\n(SOSR), that follows this principle. By incorporating SOSR into training, our\nlearned descriptor achieves state-of-the-art performance on several challenging\nbenchmarks containing distinct tasks ranging from local patch retrieval to\nstructure from motion. Furthermore, by designing a von Mises-Fischer\ndistribution based evaluation method, we link the utilization of the descriptor\nspace to the matching performance, thus demonstrating the effectiveness of our\nproposed SOSR. Extensive experimental results, empirical evidence, and in-depth\nanalysis are provided, indicating that SOSR can significantly boost the\nmatching performance of the learned descriptor.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 06:33:28 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 23:37:59 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Tian", "Yurun", ""], ["Yu", "Xin", ""], ["Fan", "Bin", ""], ["Wu", "Fuchao", ""], ["Heijnen", "Huub", ""], ["Balntas", "Vassileios", ""]]}, {"id": "1904.05020", "submitter": "Jiajie Tian", "authors": "Jiajie Tian, Zhu Teng, Rui Li, Yan Li, Baopeng Zhang, Jianping Fan", "title": "Imitating Targets from all sides: An Unsupervised Transfer Learning\n  method for Person Re-identification", "comments": "The author and result of model have changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) models usually show a limited performance\nwhen they are trained on one dataset and tested on another dataset due to the\ninter-dataset bias (e.g. completely different identities and backgrounds) and\nthe intra-dataset difference (e.g. camera invariance). In terms of this issue,\ngiven a labelled source training set and an unlabelled target training set, we\npropose an unsupervised transfer learning method characterized by 1) bridging\ninter-dataset bias and intra-dataset difference via a proposed ImitateModel\nsimultaneously; 2) regarding the unsupervised person Re-ID problem as a\nsemi-supervised learning problem formulated by a dual classification loss to\nlearn a discriminative representation across domains; 3) exploiting the\nunderlying commonality across different domains from the class-style space to\nimprove the generalization ability of re-ID models. Extensive experiments are\nconducted on two widely employed benchmarks, including Market-1501 and\nDukeMTMC-reID, and experimental results demonstrate that the proposed method\ncan achieve a competitive performance against other state-of-the-art\nunsupervised Re-ID approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 06:33:39 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 02:59:40 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Tian", "Jiajie", ""], ["Teng", "Zhu", ""], ["Li", "Rui", ""], ["Li", "Yan", ""], ["Zhang", "Baopeng", ""], ["Fan", "Jianping", ""]]}, {"id": "1904.05022", "submitter": "Ping-Rong Chen", "authors": "Ping-Rong Chen, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin", "title": "DSNet: An Efficient CNN for Road Scene Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road scene understanding is a critical component in an autonomous driving\nsystem. Although the deep learning-based road scene segmentation can achieve\nvery high accuracy, its complexity is also very high for developing real-time\napplications. It is challenging to design a neural net with high accuracy and\nlow computational complexity. To address this issue, we investigate the\nadvantages and disadvantages of several popular CNN architectures in terms of\nspeed, storage and segmentation accuracy. We start from the Fully Convolutional\nNetwork (FCN) with VGG, and then we study ResNet and DenseNet. Through detailed\nexperiments, we pick up the favorable components from the existing\narchitectures and at the end, we construct a light-weight network architecture\nbased on the DenseNet. Our proposed network, called DSNet, demonstrates a\nreal-time testing (inferencing) ability (on the popular GPU platform) and it\nmaintains an accuracy comparable with most previous systems. We test our system\non several datasets including the challenging Cityscapes dataset (resolution of\n1024x512) with an mIoU of about 69.1 % and runtime of 0.0147 second per image\non a single GTX 1080Ti. We also design a more accurate model but at the price\nof a slower speed, which has an mIoU of about 72.6 % on the CamVid dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 06:46:04 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chen", "Ping-Rong", ""], ["Hang", "Hsueh-Ming", ""], ["Chan", "Sheng-Wei", ""], ["Lin", "Jing-Jhih", ""]]}, {"id": "1904.05034", "submitter": "Chen Zhao", "authors": "Chen Zhao and Bernard Ghanem", "title": "ThumbNet: One Thumbnail Image Contains All You Need for Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural networks (CNNs) have achieved great\nsuccess in computer vision tasks, its real-world application is still impeded\nby its voracious demand of computational resources. Current works mostly seek\nto compress the network by reducing its parameters or parameter-incurred\ncomputation, neglecting the influence of the input image on the system\ncomplexity. Based on the fact that input images of a CNN contain substantial\nredundancy, in this paper, we propose a unified framework, dubbed as ThumbNet,\nto simultaneously accelerate and compress CNN models by enabling them to infer\non one thumbnail image. We provide three effective strategies to train\nThumbNet. In doing so, ThumbNet learns an inference network that performs\nequally well on small images as the original-input network on large images.\nWith ThumbNet, not only do we obtain the thumbnail-input inference network that\ncan drastically reduce computation and memory requirements, but also we obtain\nan image downscaler that can generate thumbnail images for generic\nclassification tasks. Extensive experiments show the effectiveness of ThumbNet,\nand demonstrate that the thumbnail-input inference network learned by ThumbNet\ncan adequately retain the accuracy of the original-input network even when the\ninput images are downscaled 16 times.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:44:09 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 09:01:45 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 12:32:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhao", "Chen", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.05044", "submitter": "Jiwoon Ahn", "authors": "Jiwoon Ahn, Sunghyun Cho, Suha Kwak", "title": "Weakly Supervised Learning of Instance Segmentation with Inter-pixel\n  Relations", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for learning instance segmentation with\nimage-level class labels as supervision. Our approach generates pseudo instance\nsegmentation labels of training images, which are used to train a fully\nsupervised model. For generating the pseudo labels, we first identify confident\nseed areas of object classes from attention maps of an image classification\nmodel, and propagate them to discover the entire instance areas with accurate\nboundaries. To this end, we propose IRNet, which estimates rough areas of\nindividual instances and detects boundaries between different object classes.\nIt thus enables to assign instance labels to the seeds and to propagate them\nwithin the boundaries so that the entire areas of instances can be estimated\naccurately. Furthermore, IRNet is trained with inter-pixel relations on the\nattention maps, thus no extra supervision is required. Our method with IRNet\nachieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing\nnot only previous state-of-the-art trained with the same level of supervision,\nbut also some of previous models relying on stronger supervision.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:02:35 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 13:29:08 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 01:46:17 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ahn", "Jiwoon", ""], ["Cho", "Sunghyun", ""], ["Kwak", "Suha", ""]]}, {"id": "1904.05049", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis,\n  Marcus Rohrbach, Shuicheng Yan, Jiashi Feng", "title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural\n  Networks with Octave Convolution", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural images, information is conveyed at different frequencies where\nhigher frequencies are usually encoded with fine details and lower frequencies\nare usually encoded with global structures. Similarly, the output feature maps\nof a convolution layer can also be seen as a mixture of information at\ndifferent frequencies. In this work, we propose to factorize the mixed feature\nmaps by their frequencies, and design a novel Octave Convolution (OctConv)\noperation to store and process feature maps that vary spatially \"slower\" at a\nlower spatial resolution reducing both memory and computation cost. Unlike\nexisting multi-scale methods, OctConv is formulated as a single, generic,\nplug-and-play convolutional unit that can be used as a direct replacement of\n(vanilla) convolutions without any adjustments in the network architecture. It\nis also orthogonal and complementary to methods that suggest better topologies\nor reduce channel-wise redundancy like group or depth-wise convolutions. We\nexperimentally show that by simply replacing convolutions with OctConv, we can\nconsistently boost accuracy for both image and video recognition tasks, while\nreducing memory and computational cost. An OctConv-equipped ResNet-152 can\nachieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2\nGFLOPs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:15:00 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 11:25:14 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 08:21:46 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chen", "Yunpeng", ""], ["Fan", "Haoqi", ""], ["Xu", "Bing", ""], ["Yan", "Zhicheng", ""], ["Kalantidis", "Yannis", ""], ["Rohrbach", "Marcus", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1904.05050", "submitter": "Ruoteng Li", "authors": "Ruotent Li, Loong Fah Cheong, Robby T. Tan", "title": "Heavy Rain Image Restoration: Integrating Physics Model and Conditional\n  Adversarial Learning", "comments": "CVPR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deraining works focus on rain streaks removal but they cannot deal\nadequately with heavy rain images. In heavy rain, streaks are strongly visible,\ndense rain accumulation or rain veiling effect significantly washes out the\nimage, further scenes are relatively more blurry, etc. In this paper, we\npropose a novel method to address these problems. We put forth a 2-stage\nnetwork: a physics-based backbone followed by a depth-guided GAN refinement.\nThe first stage estimates the rain streaks, the transmission, and the\natmospheric light governed by the underlying physics. To tease out these\ncomponents more reliably, a guided filtering framework is used to decompose the\nimage into its low- and high-frequency components. This filtering is guided by\na rain-free residue image --- its content is used to set the passbands for the\ntwo channels in a spatially-variant manner so that the background details do\nnot get mixed up with the rain-streaks. For the second stage, the refinement\nstage, we put forth a depth-guided GAN to recover the background details failed\nto be retrieved by the first stage, as well as correcting artefacts introduced\nby that stage. We have evaluated our method against the state of the art\nmethods. Extensive experiments show that our method outperforms them on real\nrain image data, recovering visually clean images with good details.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:15:59 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Ruotent", ""], ["Cheong", "Loong Fah", ""], ["Tan", "Robby T.", ""]]}, {"id": "1904.05059", "submitter": "Chao Zhang", "authors": "Chao Zhang, Shuaicheng Liu, Xun Xu, Ce Zhu", "title": "C3AE: Exploring the Limits of Compact Model for Age Estimation", "comments": "accepted by cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation is a classic learning problem in computer vision. Many larger\nand deeper CNNs have been proposed with promising performance, such as AlexNet,\nVggNet, GoogLeNet and ResNet. However, these models are not practical for the\nembedded/mobile devices. Recently, MobileNets and ShuffleNets have been\nproposed to reduce the number of parameters, yielding lightweight models.\nHowever, their representation has been weakened because of the adoption of\ndepth-wise separable convolution. In this work, we investigate the limits of\ncompact model for small-scale image and propose an extremely Compact yet\nefficient Cascade Context-based Age Estimation model(C3AE). This model\npossesses only 1/9 and 1/2000 parameters compared with MobileNets/ShuffleNets\nand VggNet, while achieves competitive performance. In particular, we re-define\nage estimation problem by two-points representation, which is implemented by a\ncascade model. Moreover, to fully utilize the facial context information,\nmulti-branch CNN network is proposed to aggregate multi-scale context.\nExperiments are carried out on three age estimation datasets. The\nstate-of-the-art performance on compact model has been achieved with a\nrelatively large margin.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:33:14 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 15:24:36 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zhang", "Chao", ""], ["Liu", "Shuaicheng", ""], ["Xu", "Xun", ""], ["Zhu", "Ce", ""]]}, {"id": "1904.05065", "submitter": "Shangchen Zhou", "authors": "Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Haozhe Xie, Jinshan Pan,\n  Jimmy Ren", "title": "DAVANet: Stereo Deblurring with View Aggregation", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays stereo cameras are more commonly adopted in emerging devices such as\ndual-lens smartphones and unmanned aerial vehicles. However, they also suffer\nfrom blurry images in dynamic scenes which leads to visual discomfort and\nhampers further image processing. Previous works have succeeded in monocular\ndeblurring, yet there are few studies on deblurring for stereoscopic images. By\nexploiting the two-view nature of stereo images, we propose a novel stereo\nimage deblurring network with Depth Awareness and View Aggregation, named\nDAVANet. In our proposed network, 3D scene cues from the depth and varying\ninformation from two views are incorporated, which help to remove complex\nspatially-varying blur in dynamic scenes. Specifically, with our proposed\nfusion network, we integrate the bidirectional disparities estimation and\ndeblurring into a unified framework. Moreover, we present a large-scale\nmulti-scene dataset for stereo deblurring, containing 20,637 blurry-sharp\nstereo image pairs from 135 diverse sequences and their corresponding\nbidirectional disparities. The experimental results on our dataset demonstrate\nthat DAVANet outperforms state-of-the-art methods in terms of accuracy, speed,\nand model size.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:47:56 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhou", "Shangchen", ""], ["Zhang", "Jiawei", ""], ["Zuo", "Wangmeng", ""], ["Xie", "Haozhe", ""], ["Pan", "Jinshan", ""], ["Ren", "Jimmy", ""]]}, {"id": "1904.05068", "submitter": "Wonpyo Park", "authors": "Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho", "title": "Relational Knowledge Distillation", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation aims at transferring knowledge acquired in one model\n(a teacher) to another model (a student) that is typically smaller. Previous\napproaches can be expressed as a form of training the student to mimic output\nactivations of individual data examples represented by the teacher. We\nintroduce a novel approach, dubbed relational knowledge distillation (RKD),\nthat transfers mutual relations of data examples instead. For concrete\nrealizations of RKD, we propose distance-wise and angle-wise distillation\nlosses that penalize structural differences in relations. Experiments conducted\non different tasks show that the proposed method improves educated student\nmodels with a significant margin. In particular for metric learning, it allows\nstudents to outperform their teachers' performance, achieving the state of the\narts on standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:52:14 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 09:36:42 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Park", "Wonpyo", ""], ["Kim", "Dongju", ""], ["Lu", "Yan", ""], ["Cho", "Minsu", ""]]}, {"id": "1904.05092", "submitter": "Desmond Elliott", "authors": "Spandana Gella, Desmond Elliott, Frank Keller", "title": "Cross-lingual Visual Verb Sense Disambiguation", "comments": "NAACL 2019; fix typo in author name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that visual context improves cross-lingual sense\ndisambiguation for nouns. We extend this line of work to the more challenging\ntask of cross-lingual verb sense disambiguation, introducing the MultiSense\ndataset of 9,504 images annotated with English, German, and Spanish verbs. Each\nimage in MultiSense is annotated with an English verb and its translation in\nGerman or Spanish. We show that cross-lingual verb sense disambiguation models\nbenefit from visual context, compared to unimodal baselines. We also show that\nthe verb sense predicted by our best disambiguation model can improve the\nresults of a text-only machine translation system when used for a multimodal\ntranslation task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 09:43:06 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 12:52:07 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gella", "Spandana", ""], ["Elliott", "Desmond", ""], ["Keller", "Frank", ""]]}, {"id": "1904.05118", "submitter": "Xingran Zhou", "authors": "Xingran Zhou, Siyu Huang, Bin Li, Yingming Li, Jiachen Li, Zhongfei\n  Zhang", "title": "Text Guided Person Image Synthesis", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method to manipulate the visual appearance (pose\nand attribute) of a person image according to natural language descriptions.\nOur method can be boiled down to two stages: 1) text guided pose generation and\n2) visual appearance transferred image synthesis. In the first stage, our\nmethod infers a reasonable target human pose based on the text. In the second\nstage, our method synthesizes a realistic and appearance transferred person\nimage according to the text in conjunction with the target pose. Our method\nextracts sufficient information from the text and establishes a mapping between\nthe image space and the language space, making generating and editing images\ncorresponding to the description possible. We conduct extensive experiments to\nreveal the effectiveness of our method, as well as using the VQA Perceptual\nScore as a metric for evaluating the method. It shows for the first time that\nwe can automatically edit the person image from the natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 11:40:15 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhou", "Xingran", ""], ["Huang", "Siyu", ""], ["Li", "Bin", ""], ["Li", "Yingming", ""], ["Li", "Jiachen", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1904.05124", "submitter": "Phong Nguyen-Ha", "authors": "Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Janne Heikkila", "title": "Predicting Novel Views Using Generative Adversarial Query Network", "comments": "12 pages, 4 figures, accepted for presentation at the Scandinavian\n  Conference on Image Analysis 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of predicting a novel view of the scene using an arbitrary number\nof observations is a challenging problem for computers as well as for humans.\nThis paper introduces the Generative Adversarial Query Network (GAQN), a\ngeneral learning framework for novel view synthesis that combines Generative\nQuery Network (GQN) and Generative Adversarial Networks (GANs). The\nconventional GQN encodes input views into a latent representation that is used\nto generate a new view through a recurrent variational decoder. The proposed\nGAQN builds on this work by adding two novel aspects: First, we extend the\ncurrent GQN architecture with an adversarial loss function for improving the\nvisual quality and convergence speed. Second, we introduce a feature-matching\nloss function for stabilizing the training procedure. The experiments\ndemonstrate that GAQN is able to produce high-quality results and faster\nconvergence compared to the conventional approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 11:49:25 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nguyen-Ha", "Phong", ""], ["Huynh", "Lam", ""], ["Rahtu", "Esa", ""], ["Heikkila", "Janne", ""]]}, {"id": "1904.05126", "submitter": "Nikita Araslanov", "authors": "Nikita Araslanov, Constantin Rothkopf, Stefan Roth", "title": "Actor-Critic Instance Segmentation", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches to visual scene analysis have emphasised parallel processing\nof the image elements. However, one area in which the sequential nature of\nvision is apparent, is that of segmenting multiple, potentially similar and\npartially occluded objects in a scene. In this work, we revisit the recurrent\nformulation of this challenging problem in the context of reinforcement\nlearning. Motivated by the limitations of the global max-matching assignment of\nthe ground-truth segments to the recurrent states, we develop an actor-critic\napproach in which the actor recurrently predicts one instance mask at a time\nand utilises the gradient from a concurrently trained critic network. We\nformulate the state, action, and the reward such as to let the critic model\nlong-term effects of the current prediction and incorporate this information\ninto the gradient signal. Furthermore, to enable effective exploration in the\ninherently high-dimensional action space of instance masks, we learn a compact\nrepresentation using a conditional variational auto-encoder. We show that our\nactor-critic model consistently provides accuracy benefits over the recurrent\nbaseline on standard instance segmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 12:08:13 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Araslanov", "Nikita", ""], ["Rothkopf", "Constantin", ""], ["Roth", "Stefan", ""]]}, {"id": "1904.05159", "submitter": "Hyung Jin Chang", "authors": "Kwang In Kim, Hyung Jin Chang", "title": "Joint Manifold Diffusion for Combining Predictions on Decoupled\n  Observations", "comments": "Published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new predictor combination algorithm that improves a given task\npredictor based on potentially relevant reference predictors. Existing\napproaches are limited in that, to discover the underlying task dependence,\nthey either require known parametric forms of all predictors or access to a\nsingle fixed dataset on which all predictors are jointly evaluated. To overcome\nthese limitations, we design a new non-parametric task dependence estimation\nprocedure that automatically aligns evaluations of heterogeneous predictors\nacross disjoint feature sets. Our algorithm is instantiated as a robust\nmanifold diffusion process that jointly refines the estimated predictor\nalignments and the corresponding task dependence. We apply this algorithm to\nthe relative attributes ranking problem and demonstrate that it not only\nbroadens the application range of predictor combination approaches but also\noutperforms existing methods even when applied to classical predictor\ncombination settings.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:07:48 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kim", "Kwang In", ""], ["Chang", "Hyung Jin", ""]]}, {"id": "1904.05160", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong,\n  Stella X. Yu", "title": "Large-Scale Long-Tailed Recognition in an Open World", "comments": "To appear in CVPR 2019 as an oral presentation. Code, datasets and\n  models are available at https://liuziwei7.github.io/projects/LongTail.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world data often have a long-tailed and open-ended distribution. A\npractical recognition system must classify among majority and minority classes,\ngeneralize from a few known instances, and acknowledge novelty upon a never\nseen instance. We define Open Long-Tailed Recognition (OLTR) as learning from\nsuch naturally distributed data and optimizing the classification accuracy over\na balanced test set which include head, tail, and open classes. OLTR must\nhandle imbalanced classification, few-shot learning, and open-set recognition\nin one integrated algorithm, whereas existing classification approaches focus\nonly on one aspect and deliver poorly over the entire class spectrum. The key\nchallenges are how to share visual knowledge between head and tail classes and\nhow to reduce confusion between tail and open classes. We develop an integrated\nOLTR algorithm that maps an image to a feature space such that visual concepts\ncan easily relate to each other based on a learned metric that respects the\nclosed-world classification while acknowledging the novelty of the open world.\nOur so-called dynamic meta-embedding combines a direct image feature and an\nassociated memory feature, with the feature norm indicating the familiarity to\nknown classes. On three large-scale OLTR datasets we curate from object-centric\nImageNet, scene-centric Places, and face-centric MS1M data, our method\nconsistently outperforms the state-of-the-art. Our code, datasets, and models\nenable future OLTR research and are publicly available at\nhttps://liuziwei7.github.io/projects/LongTail.html.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:09:42 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 14:17:39 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Liu", "Ziwei", ""], ["Miao", "Zhongqi", ""], ["Zhan", "Xiaohang", ""], ["Wang", "Jiayun", ""], ["Gong", "Boqing", ""], ["Yu", "Stella X.", ""]]}, {"id": "1904.05181", "submitter": "Linxi Jiang", "authors": "Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang", "title": "Black-box Adversarial Attacks on Video Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known for their vulnerability to adversarial\nexamples. These are examples that have undergone small, carefully crafted\nperturbations, and which can easily fool a DNN into making misclassifications\nat test time. Thus far, the field of adversarial research has mainly focused on\nimage models, under either a white-box setting, where an adversary has full\naccess to model parameters, or a black-box setting where an adversary can only\nquery the target model for probabilities or labels. Whilst several white-box\nattacks have been proposed for video models, black-box video attacks are still\nunexplored. To close this gap, we propose the first black-box video attack\nframework, called V-BAD. V-BAD utilizes tentative perturbations transferred\nfrom image models, and partition-based rectifications found by the NES on\npartitions (patches) of tentative perturbations, to obtain good adversarial\ngradient estimates with fewer queries to the target model. V-BAD is equivalent\nto estimating the projection of an adversarial gradient on a selected subspace.\nUsing three benchmark video datasets, we demonstrate that V-BAD can craft both\nuntargeted and targeted attacks to fool two state-of-the-art deep video\nrecognition models. For the targeted attack, it achieves $>$93\\% success rate\nusing only an average of $3.4 \\sim 8.4 \\times 10^4$ queries, a similar number\nof queries to state-of-the-art black-box image attacks. This is despite the\nfact that videos often have two orders of magnitude higher dimensionality than\nstatic images. We believe that V-BAD is a promising new tool to evaluate and\nimprove the robustness of video recognition models to black-box adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:41:02 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:22:54 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jiang", "Linxi", ""], ["Ma", "Xingjun", ""], ["Chen", "Shaoxiang", ""], ["Bailey", "James", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1904.05191", "submitter": "Julia Rackerseder", "authors": "Beatrice Demiray, Julia Rackerseder, Stevica Bozhinoski, Nassir Navab", "title": "Weakly-Supervised White and Grey Matter Segmentation in 3D Brain\n  Ultrasound", "comments": "* Beatrice Demiray and Julia Rackerseder contributed equally to this\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the segmentation of brain structures in ultrasound helps initialize\nimage based registration, assist brain shift compensation, and provides\ninterventional decision support, the task of segmenting grey and white matter\nin cranial ultrasound is very challenging and has not been addressed yet. We\ntrain a multi-scale fully convolutional neural network simultaneously for two\nclasses in order to segment real clinical 3D ultrasound data. Parallel pathways\nworking at different levels of resolution account for high frequency speckle\nnoise and global 3D image features. To ensure reproducibility, the publicly\navailable RESECT dataset is utilized for training and cross-validation. Due to\nthe absence of a ground truth, we train with weakly annotated label. We\nimplement label transfer from MRI to US, which is prone to a residual but\ninevitable registration error. To further improve results, we perform transfer\nlearning using synthetic US data. The resulting method leads to excellent Dice\nscores of 0.7080, 0.8402 and 0.9315 for grey matter, white matter and\nbackground. Our proposed methodology sets an unparalleled standard for white\nand grey matter segmentation in 3D intracranial ultrasound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:55:10 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 08:10:11 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 07:53:34 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Demiray", "Beatrice", ""], ["Rackerseder", "Julia", ""], ["Bozhinoski", "Stevica", ""], ["Navab", "Nassir", ""]]}, {"id": "1904.05200", "submitter": "Xie De", "authors": "Cheng Deng, Xianglong Liu, Chao Li, Dacheng Tao", "title": "Active Multi-Kernel Domain Adaptation for Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the quick progress of the hyperspectral images\n(HSI) classification. Most of existing studies either heavily rely on the\nexpensive label information using the supervised learning or can hardly exploit\nthe discriminative information borrowed from related domains. To address this\nissues, in this paper we show a novel framework addressing HSI classification\nbased on the domain adaptation (DA) with active learning (AL). The main idea of\nour method is to retrain the multi-kernel classifier by utilizing the available\nlabeled samples from source domain, and adding minimum number of the most\ninformative samples with active queries in the target domain. The proposed\nmethod adaptively combines multiple kernels, forming a DA classifier that\nminimizes the bias between the source and target domains. Further equipped with\nthe nested actively updating process, it sequentially expands the training set\nand gradually converges to a satisfying level of classification performance. We\nstudy this active adaptation framework with the Margin Sampling (MS) strategy\nin the HSI classification task. Our experimental results on two popular HSI\ndatasets demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:07:25 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Deng", "Cheng", ""], ["Liu", "Xianglong", ""], ["Li", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.05236", "submitter": "Hoel Kervadec", "authors": "Hoel Kervadec, Jose Dolz, Eric Granger, Ismail Ben Ayed", "title": "Curriculum semi-supervised segmentation", "comments": "Accepted as paper as MICCAI 2O19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates a curriculum-style strategy for semi-supervised CNN\nsegmentation, which devises a regression network to learn image-level\ninformation such as the size of a target region. These regressions are used to\neffectively regularize the segmentation network, constraining softmax\npredictions of the unlabeled images to match the inferred label distributions.\nOur framework is based on inequality constraints that tolerate uncertainties\nwith inferred knowledge, e.g., regressed region size, and can be employed for a\nlarge variety of region attributes. We evaluated our proposed strategy for left\nventricle segmentation in magnetic resonance images (MRI), and compared it to\nstandard proposal-based semi-supervision strategies. Our strategy leverages\nunlabeled data in more efficiently, and achieves very competitive results,\napproaching the performance of full-supervision.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:14:47 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 13:29:51 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kervadec", "Hoel", ""], ["Dolz", "Jose", ""], ["Granger", "Eric", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1904.05244", "submitter": "Djamila Aouada", "authors": "Konstantinos Papadopoulos and Girum Demisse and Enjie Ghorbel and\n  Michel Antunes and Djamila Aouada and Bj\\\"orn Ottersten", "title": "Localized Trajectories for 2D and 3D Action Recognition", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Dense Trajectories concept is one of the most successful approaches in\naction recognition, suitable for scenarios involving a significant amount of\nmotion. However, due to noise and background motion, many generated\ntrajectories are irrelevant to the actual human activity and can potentially\nlead to performance degradation. In this paper, we propose Localized\nTrajectories as an improved version of Dense Trajectories where motion\ntrajectories are clustered around human body joints provided by RGB-D cameras\nand then encoded by local Bag-of-Words. As a result, the Localized Trajectories\nconcept provides a more discriminative representation of actions as compared to\nDense Trajectories. Moreover, we generalize Localized Trajectories to 3D by\nusing the modalities offered by RGB-D cameras. One of the main advantages of\nusing RGB-D data to generate trajectories is that they include radial\ndisplacements that are perpendicular to the image plane. Extensive experiments\nand analysis are carried out on five different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:34:25 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Papadopoulos", "Konstantinos", ""], ["Demisse", "Girum", ""], ["Ghorbel", "Enjie", ""], ["Antunes", "Michel", ""], ["Aouada", "Djamila", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "1904.05250", "submitter": "Antonino Furnari", "authors": "Antonino Furnari, Sebastiano Battiato, Kristen Grauman, Giovanni Maria\n  Farinella", "title": "Next-Active-Object prediction from Egocentric Videos", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation, Volume\n  49, 2017, Pages 401-411, ISSN 1047-3203", "doi": "10.1016/j.jvcir.2017.10.004", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although First Person Vision systems can sense the environment from the\nuser's perspective, they are generally unable to predict his intentions and\ngoals. Since human activities can be decomposed in terms of atomic actions and\ninteractions with objects, intelligent wearable systems would benefit from the\nability to anticipate user-object interactions. Even if this task is not\ntrivial, the First Person Vision paradigm can provide important cues to address\nthis challenge. We propose to exploit the dynamics of the scene to recognize\nnext-active-objects before an object interaction begins. We train a classifier\nto discriminate trajectories leading to an object activation from all others\nand forecast next-active-objects by analyzing fixed-length trajectory segments\nwithin a temporal sliding window. The proposed method compares favorably with\nrespect to several baselines on the Activity of Daily Living (ADL) egocentric\ndataset comprising 10 hours of videos acquired by 20 subjects while performing\nunconstrained interactions with several objects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:39:19 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Furnari", "Antonino", ""], ["Battiato", "Sebastiano", ""], ["Grauman", "Kristen", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "1904.05257", "submitter": "Victor Kulikov", "authors": "Victor Kulikov and Victor Lempitsky", "title": "Instance Segmentation of Biological Images Using Harmonic Embeddings", "comments": "Accepted as oral to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new instance segmentation approach tailored to biological\nimages, where instances may correspond to individual cells, organisms or plant\nparts. Unlike instance segmentation for user photographs or road scenes, in\nbiological data object instances may be particularly densely packed, the\nappearance variation may be particularly low, the processing power may be\nrestricted, while, on the other hand, the variability of sizes of individual\ninstances may be limited. The proposed approach successfully addresses these\npeculiarities.\n  Our approach describes each object instance using an expectation of a limited\nnumber of sine waves with frequencies and phases adjusted to particular object\nsizes and densities. At train time, a fully-convolutional network is learned to\npredict the object embeddings at each pixel using a simple pixelwise regression\nloss, while at test time the instances are recovered using clustering in the\nembedding space. In the experiments, we show that our approach outperforms\nprevious embedding-based instance segmentation approaches on a number of\nbiological datasets, achieving state-of-the-art on a popular CVPPP benchmark.\nThis excellent performance is combined with computational efficiency that is\nneeded for deployment to domain specialists.\n  The source code of the approach is available at\nhttps://github.com/kulikovv/harmonic\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:56:16 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 15:26:52 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Kulikov", "Victor", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1904.05264", "submitter": "Antonino Furnari", "authors": "Francesco Ragusa, Antonino Furnari, Sebastiano Battiato, Giovanni\n  Signorello, Giovanni Maria Farinella", "title": "Egocentric Visitors Localization in Cultural Sites", "comments": "To appear in ACM Journal on Computing and Cultural Heritage (JOCCH),\n  2019", "journal-ref": "ACM Journal on Computing and Cultural Heritage (JOCCH), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of localizing visitors in a cultural site from\negocentric (first person) images. Localization information can be useful both\nto assist the user during his visit (e.g., by suggesting where to go and what\nto see next) and to provide behavioral information to the manager of the\ncultural site (e.g., how much time has been spent by visitors at a given\nlocation? What has been liked most?). To tackle the problem, we collected a\nlarge dataset of egocentric videos using two cameras: a head-mounted HoloLens\ndevice and a chest-mounted GoPro. Each frame has been labeled according to the\nlocation of the visitor and to what he was looking at. The dataset is freely\navailable in order to encourage research in this domain. The dataset is\ncomplemented with baseline experiments performed considering a state-of-the-art\nmethod for location-based temporal segmentation of egocentric videos.\nExperiments show that compelling results can be achieved to extract useful\ninformation for both the visitor and the site-manager.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:05:43 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Ragusa", "Francesco", ""], ["Furnari", "Antonino", ""], ["Battiato", "Sebastiano", ""], ["Signorello", "Giovanni", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "1904.05265", "submitter": "Peng Jiang Dr.", "authors": "Bin Liu, Qian Guo, Shucai Li, Benchao Liu, Yuxiao Ren, Yonghao Pang,\n  Xu Guo, Lanbo Liu, Peng Jiang", "title": "Deep Learning Inversion of Electrical Resistivity Data", "comments": "IEEE Transactions on Geoscience and Remote Sensing, 2020", "journal-ref": null, "doi": "10.1109/TGRS.2020.2969040", "report-no": null, "categories": "cs.CV cs.AI physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse problem of electrical resistivity surveys (ERSs) is difficult\nbecause of its nonlinear and ill-posed nature. For this task, traditional\nlinear inversion methods still face challenges such as suboptimal approximation\nand initial model selection. Inspired by the remarkable nonlinear mapping\nability of deep learning approaches, in this article, we propose to build the\nmapping from apparent resistivity data (input) to resistivity model (output)\ndirectly by convolutional neural networks (CNNs). However, the vertically\nvarying characteristic of patterns in the apparent resistivity data may cause\nambiguity when using CNNs with the weight sharing and effective receptive field\nproperties. To address the potential issue, we supply an additional tier\nfeature map to CNNs to help those aware of the relationship between input and\noutput. Based on the prevalent U-Net architecture, we design our network\n(ERSInvNet) that can be trained end-to-end and can reach a very fast inference\nspeed during testing. We further introduce a depth weighting function and a\nsmooth constraint into loss function to improve inversion accuracy for the deep\nregion and suppress false anomalies. Six groups of experiments are considered\nto demonstrate the feasibility and efficiency of the proposed methods.\nAccording to the comprehensive qualitative analysis and quantitative\ncomparison, ERSInvNet with tier feature map, smooth constraints, and depth\nweighting function together achieve the best performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:06:29 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:14:16 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Liu", "Bin", ""], ["Guo", "Qian", ""], ["Li", "Shucai", ""], ["Liu", "Benchao", ""], ["Ren", "Yuxiao", ""], ["Pang", "Yonghao", ""], ["Guo", "Xu", ""], ["Liu", "Lanbo", ""], ["Jiang", "Peng", ""]]}, {"id": "1904.05290", "submitter": "Junhwa Hur", "authors": "Junhwa Hur and Stefan Roth", "title": "Iterative Residual Refinement for Joint Optical Flow and Occlusion\n  Estimation", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to optical flow estimation have seen rapid progress\nover the recent years. One common trait of many networks is that they refine an\ninitial flow estimate either through multiple stages or across the levels of a\ncoarse-to-fine representation. While leading to more accurate results, the\ndownside of this is an increased number of parameters. Taking inspiration from\nboth classical energy minimization approaches as well as residual networks, we\npropose an iterative residual refinement (IRR) scheme based on weight sharing\nthat can be combined with several backbone networks. It reduces the number of\nparameters, improves the accuracy, or even achieves both. Moreover, we show\nthat integrating occlusion prediction and bi-directional flow estimation into\nour IRR scheme can further boost the accuracy. Our full network achieves\nstate-of-the-art results for both optical flow and occlusion estimation across\nseveral standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:50:38 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "1904.05304", "submitter": "Samet Akcay", "authors": "Yona Falinie A. Gaus, Neelanjan Bhowmik, Samet Ak\\c{c}ay, Paolo M.\n  Guillen-Garcia, Jack W. Barker, Toby P. Breckon", "title": "Evaluation of a Dual Convolutional Neural Network Architecture for\n  Object-wise Anomaly Detection in Cluttered X-ray Security Imagery", "comments": "IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray baggage security screening is widely used to maintain aviation and\ntransport security. Of particular interest is the focus on automated security\nX-ray analysis for particular classes of object such as electronics, electrical\nitems, and liquids. However, manual inspection of such items is challenging\nwhen dealing with potentially anomalous items. Here we present a dual\nconvolutional neural network (CNN) architecture for automatic anomaly detection\nwithin complex security X-ray imagery. We leverage recent advances in\nregion-based (R-CNN), mask-based CNN (Mask R-CNN) and detection architectures\nsuch as RetinaNet to provide object localisation variants for specific object\nclasses of interest. Subsequently, leveraging a range of established CNN object\nand fine-grained category classification approaches we formulate within object\nanomaly detection as a two-class problem (anomalous or benign). While the best\nperforming object localisation method is able to perform with 97.9% mean\naverage precision (mAP) over a six-class X-ray object detection problem,\nsubsequent two-class anomaly/benign classification is able to achieve 66%\nperformance for within object anomaly detection. Overall, this performance\nillustrates both the challenge and promise of object-wise anomaly detection\nwithin the context of cluttered X-ray security imagery.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:11:50 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Gaus", "Yona Falinie A.", ""], ["Bhowmik", "Neelanjan", ""], ["Ak\u00e7ay", "Samet", ""], ["Guillen-Garcia", "Paolo M.", ""], ["Barker", "Jack W.", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1904.05343", "submitter": "Matthew Tancik", "authors": "Matthew Tancik, Ben Mildenhall, Ren Ng", "title": "StegaStamp: Invisible Hyperlinks in Physical Photographs", "comments": "CVPR 2020, Project page: http://www.matthewtancik.com/stegastamp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Printed and digitally displayed photos have the ability to hide imperceptible\ndigital data that can be accessed through internet-connected imaging systems.\nAnother way to think about this is physical photographs that have unique QR\ncodes invisibly embedded within them. This paper presents an architecture,\nalgorithms, and a prototype implementation addressing this vision. Our key\ntechnical contribution is StegaStamp, a learned steganographic algorithm to\nenable robust encoding and decoding of arbitrary hyperlink bitstrings into\nphotos in a manner that approaches perceptual invisibility. StegaStamp\ncomprises a deep neural network that learns an encoding/decoding algorithm\nrobust to image perturbations approximating the space of distortions resulting\nfrom real printing and photography. We demonstrates real-time decoding of\nhyperlinks in photos from in-the-wild videos that contain variation in\nlighting, shadows, perspective, occlusion and viewing distance. Our prototype\nsystem robustly retrieves 56 bit hyperlinks after error correction - sufficient\nto embed a unique code within every photo on the internet.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:53:38 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 02:51:10 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Tancik", "Matthew", ""], ["Mildenhall", "Ben", ""], ["Ng", "Ren", ""]]}, {"id": "1904.05349", "submitter": "Bugra Tekin", "authors": "Bugra Tekin, Federica Bogo, Marc Pollefeys", "title": "H+O: Unified Egocentric Recognition of 3D Hand-Object Poses and\n  Interactions", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for understanding 3D hand and object\ninteractions in raw image sequences from egocentric RGB cameras. Given a single\nRGB image, our model jointly estimates the 3D hand and object poses, models\ntheir interactions, and recognizes the object and action classes with a single\nfeed-forward pass through a neural network. We propose a single architecture\nthat does not rely on external detection algorithms but rather is trained\nend-to-end on single images. We further merge and propagate information in the\ntemporal domain to infer interactions between hand and object trajectories and\nrecognize actions. The complete model takes as input a sequence of frames and\noutputs per-frame 3D hand and object pose predictions along with the estimates\nof object and action categories for the entire sequence. We demonstrate\nstate-of-the-art performance of our algorithm even in comparison to the\napproaches that work on depth data and ground-truth annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:58:32 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Tekin", "Bugra", ""], ["Bogo", "Federica", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1904.05373", "submitter": "Hang Su", "authors": "Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller,\n  and Jan Kautz", "title": "Pixel-Adaptive Convolutional Neural Networks", "comments": "CVPR 2019. Video introduction: https://youtu.be/gsQZbHuR64o", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutions are the fundamental building block of CNNs. The fact that their\nweights are spatially shared is one of the main reasons for their widespread\nuse, but it also is a major limitation, as it makes convolutions content\nagnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet\neffective modification of standard convolutions, in which the filter weights\nare multiplied with a spatially-varying kernel that depends on learnable, local\npixel features. PAC is a generalization of several popular filtering techniques\nand thus can be used for a wide range of use cases. Specifically, we\ndemonstrate state-of-the-art performance when PAC is used for deep joint image\nupsampling. PAC also offers an effective alternative to fully-connected CRF\n(Full-CRF), called PAC-CRF, which performs competitively, while being\nconsiderably faster. In addition, we also demonstrate that PAC can be used as a\ndrop-in replacement for convolution layers in pre-trained networks, resulting\nin consistent performance improvements.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:02:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Su", "Hang", ""], ["Jampani", "Varun", ""], ["Sun", "Deqing", ""], ["Gallo", "Orazio", ""], ["Learned-Miller", "Erik", ""], ["Kautz", "Jan", ""]]}, {"id": "1904.05404", "submitter": "Shuai Liao", "authors": "Shuai Liao, Efstratios Gavves, Cees G. M. Snoek", "title": "Spherical Regression: Learning Viewpoints, Surface Normals and 3D\n  Rotations on n-Spheres", "comments": "CVPR 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision challenges require continuous outputs, but tend to be\nsolved by discrete classification. The reason is classification's natural\ncontainment within a probability $n$-simplex, as defined by the popular softmax\nactivation function. Regular regression lacks such a closed geometry, leading\nto unstable training and convergence to suboptimal local minima. Starting from\nthis insight we revisit regression in convolutional neural networks. We observe\nmany continuous output problems in computer vision are naturally contained in\nclosed geometrical manifolds, like the Euler angles in viewpoint estimation or\nthe normals in surface normal estimation. A natural framework for posing such\ncontinuous output problems are $n$-spheres, which are naturally closed\ngeometric manifolds defined in the $\\mathbb{R}^{(n+1)}$ space. By introducing a\nspherical exponential mapping on $n$-spheres at the regression output, we\nobtain well-behaved gradients, leading to stable training. We show how our\nspherical regression can be utilized for several computer vision challenges,\nspecifically viewpoint estimation, surface normal estimation and 3D rotation\nestimation. For all these problems our experiments demonstrate the benefit of\nspherical regression. All paper resources are available at\nhttps://github.com/leoshine/Spherical_Regression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:33:59 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Liao", "Shuai", ""], ["Gavves", "Efstratios", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1904.05408", "submitter": "Jiqing Wu", "authors": "Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda\n  Pani Paudel, Luc Van Gool", "title": "Sliced Wasserstein Generative Models", "comments": "This paper is submitted to arxiv twice, thus withdraw one of the\n  versions. See arXiv:1706.02631 instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In generative modeling, the Wasserstein distance (WD) has emerged as a useful\nmetric to measure the discrepancy between generated and real data\ndistributions. Unfortunately, it is challenging to approximate the WD of\nhigh-dimensional distributions. In contrast, the sliced Wasserstein distance\n(SWD) factorizes high-dimensional distributions into their multiple\none-dimensional marginal distributions and is thus easier to approximate. In\nthis paper, we introduce novel approximations of the primal and dual SWD.\nInstead of using a large number of random projections, as it is done by\nconventional SWD approximation methods, we propose to approximate SWDs with a\nsmall number of parameterized orthogonal projections in an end-to-end deep\nlearning fashion. As concrete applications of our SWD approximations, we design\ntwo types of differentiable SWD blocks to equip modern generative\nframeworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In\nthe experiments, we not only show the superiority of the proposed generative\nmodels on standard image synthesis benchmarks, but also demonstrate the\nstate-of-the-art performance on challenging high resolution image and video\ngeneration in an unsupervised manner.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:49:43 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 06:53:02 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wu", "Jiqing", ""], ["Huang", "Zhiwu", ""], ["Acharya", "Dinesh", ""], ["Li", "Wen", ""], ["Thoma", "Janine", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "1904.05410", "submitter": "Yang Wang", "authors": "Yang Wang, Vinh Tran, Gedas Bertasius, Lorenzo Torresani, Minh Hoai", "title": "Attentive Action and Context Factorization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for human action recognition, one that can localize the\nspatiotemporal regions that `define' the actions. This is a challenging task\ndue to the subtlety of human actions in video and the co-occurrence of\ncontextual elements. To address this challenge, we utilize conjugate samples of\nhuman actions, which are video clips that are contextually similar to human\naction samples but do not contain the action. We introduce a novel attentional\nmechanism that can spatially and temporally separate human actions from the\nco-occurring contextual factors. The separation of the action and context\nfactors is weakly supervised, eliminating the need for laboriously detailed\nannotation of these two factors in training samples. Our method can be used to\nbuild human action classifiers with higher accuracy and better\ninterpretability. Experiments on several human action recognition datasets\ndemonstrate the quantitative and qualitative benefits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:50:45 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wang", "Yang", ""], ["Tran", "Vinh", ""], ["Bertasius", "Gedas", ""], ["Torresani", "Lorenzo", ""], ["Hoai", "Minh", ""]]}, {"id": "1904.05443", "submitter": "Alejandro Pardo", "authors": "Alejandro Pardo, Mengmeng Xu, Ali Thabet, Pablo Arbelaez, Bernard\n  Ghanem", "title": "BAOD: Budget-Aware Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of object detection from a novel perspective in which\nannotation budget constraints are taken into consideration, appropriately\ncoined Budget Aware Object Detection (BAOD). When provided with a fixed budget,\nwe propose a strategy for building a diverse and informative dataset that can\nbe used to optimally train a robust detector. We investigate both optimization\nand learning-based methods to sample which images to annotate and what type of\nannotation (strongly or weakly supervised) to annotate them with. We adopt a\nhybrid supervised learning framework to train the object detector from both\nthese types of annotation. We conduct a comprehensive empirical study showing\nthat a handcrafted optimization method outperforms other selection techniques\nincluding random sampling, uncertainty sampling and active learning. By\ncombining an optimal image/annotation selection scheme with hybrid supervised\nlearning to solve the BAOD problem, we show that one can achieve the\nperformance of a strongly supervised detector on PASCAL-VOC 2007 while saving\n12.8% of its original annotation budget. Furthermore, when $100\\%$ of the\nbudget is used, it surpasses this performance by 2.0 mAP percentage points.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:13:08 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Pardo", "Alejandro", ""], ["Xu", "Mengmeng", ""], ["Thabet", "Ali", ""], ["Arbelaez", "Pablo", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.05448", "submitter": "Soraia Musse", "authors": "Cliceres dal Bianco, Soraia Raupp Musse", "title": "Predicting Future Pedestrian Motion in Video Sequences using Crowd\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While human and group analysis have become an important area in last decades,\nsome current and relevant applications involve to estimate future motion of\npedestrians in real video sequences. This paper presents a method to provide\nmotion estimation of real pedestrians in next seconds, using crowd simulation.\nOur method is based on Physics and heuristics and use BioCrowds as crowd\nsimulation methodology to estimate future positions of people in video\nsequences. Results show that our method for estimation works well even for\ncomplex videos where events can happen. The maximum achieved average error is\n$2.72$cm when estimating the future motion of 32 pedestrians with more than 2\nseconds in advance. This paper discusses this and other results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:25:24 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Bianco", "Cliceres dal", ""], ["Musse", "Soraia Raupp", ""]]}, {"id": "1904.05449", "submitter": "Mengyu Dai", "authors": "Mengyu Dai, Zhengwu Zhang, and Anuj Srivastava", "title": "Analyzing Dynamical Brain Functional Connectivity As Trajectories on\n  Space of Covariance Matrices", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": "Published in IEEE Transactions on Medical Imaging, 2019", "doi": "10.1109/TMI.2019.2931708", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human brain functional connectivity (FC) is often measured as the similarity\nof functional MRI responses across brain regions when a brain is either resting\nor performing a task. This paper aims to statistically analyze the dynamic\nnature of FC by representing the collective time-series data, over a set of\nbrain regions, as a trajectory on the space of covariance matrices, or\nsymmetric-positive definite matrices (SPDMs). We use a recently developed\nmetric on the space of SPDMs for quantifying differences across FC\nobservations, and for clustering and classification of FC trajectories. To\nfacilitate large scale and high-dimensional data analysis, we propose a novel,\nmetric-based dimensionality reduction technique to reduce data from large SPDMs\nto small SPDMs. We illustrate this comprehensive framework using data from the\nHuman Connectome Project (HCP) database for multiple subjects and tasks, with\ntask classification rates that match or outperform state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:27:42 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 18:49:33 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Dai", "Mengyu", ""], ["Zhang", "Zhengwu", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1904.05457", "submitter": "Guanqing Hu", "authors": "Guanqing Hu and James J. Clark", "title": "Instance Segmentation based Semantic Matting for Compositing\n  Applications", "comments": "16th Conference on Computer and Robot Vision (CRV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compositing is a key step in film making and image editing that aims to\nsegment a foreground object and combine it with a new background. Automatic\nimage compositing can be done easily in a studio using chroma-keying when the\nbackground is pure blue or green. However, image compositing in natural scenes\nwith complex backgrounds remains a tedious task, requiring experienced artists\nto hand-segment. In order to achieve automatic compositing in natural scenes,\nwe propose a fully automated method that integrates instance segmentation and\nimage matting processes to generate high-quality semantic mattes that can be\nused for image editing task. Our approach can be seen both as a refinement of\nexisting instance segmentation algorithms and as a fully automated semantic\nimage matting method. It extends automatic image compositing techniques such as\nchroma-keying to scenes with complex natural backgrounds without the need for\nany kind of user interaction. The output of our approach can be considered as\nboth refined instance segmentations and alpha mattes with semantic meanings. We\nprovide experimental results which show improved performance results as\ncompared to existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:48:34 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Hu", "Guanqing", ""], ["Clark", "James J.", ""]]}, {"id": "1904.05475", "submitter": "Shashank Tripathi", "authors": "Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi,\n  James M. Rehg and Visesh Chari", "title": "Learning to Generate Synthetic Data via Compositing", "comments": "Accepted to CVPR 2019, supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a task-aware approach to synthetic data generation. Our framework\nemploys a trainable synthesizer network that is optimized to produce meaningful\ntraining samples by assessing the strengths and weaknesses of a `target'\nnetwork. The synthesizer and target networks are trained in an adversarial\nmanner wherein each network is updated with a goal to outdo the other.\nAdditionally, we ensure the synthesizer generates realistic data by pairing it\nwith a discriminator trained on real-world images. Further, to make the target\nclassifier invariant to blending artefacts, we introduce these artefacts to\nbackground regions of the training images so the target does not over-fit to\nthem.\n  We demonstrate the efficacy of our approach by applying it to different\ntarget networks including a classification network on AffNIST, and two object\ndetection networks (SSD, Faster-RCNN) on different datasets. On the AffNIST\nbenchmark, our approach is able to surpass the baseline results with just half\nthe training examples. On the VOC person detection benchmark, we show\nimprovements of up to 2.7% as a result of our data augmentation. Similarly on\nthe GMU detection benchmark, we report a performance boost of 3.5% in mAP over\nthe baseline method, outperforming the previous state of the art approaches by\nup to 7.5% on specific categories.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 23:22:09 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 22:15:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Tripathi", "Shashank", ""], ["Chandra", "Siddhartha", ""], ["Agrawal", "Amit", ""], ["Tyagi", "Ambrish", ""], ["Rehg", "James M.", ""], ["Chari", "Visesh", ""]]}, {"id": "1904.05478", "submitter": "Naama Hammel", "authors": "Boris Babenko, Siva Balasubramanian, Katy E. Blumer, Greg S. Corrado,\n  Lily Peng, Dale R. Webster, Naama Hammel, Avinash V. Varadarajan", "title": "Predicting Progression of Age-related Macular Degeneration from Fundus\n  Images using Deep Learning", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Patients with neovascular age-related macular degeneration (AMD)\ncan avoid vision loss via certain therapy. However, methods to predict the\nprogression to neovascular age-related macular degeneration (nvAMD) are\nlacking. Purpose: To develop and validate a deep learning (DL) algorithm to\npredict 1-year progression of eyes with no, early, or intermediate AMD to\nnvAMD, using color fundus photographs (CFP). Design: Development and validation\nof a DL algorithm. Methods: We trained a DL algorithm to predict 1-year\nprogression to nvAMD, and used 10-fold cross-validation to evaluate this\napproach on two groups of eyes in the Age-Related Eye Disease Study (AREDS):\nnone/early/intermediate AMD, and intermediate AMD (iAMD) only. We compared the\nDL algorithm to the manually graded 4-category and 9-step scales in the AREDS\ndataset. Main outcome measures: Performance of the DL algorithm was evaluated\nusing the sensitivity at 80% specificity for progression to nvAMD. Results: The\nDL algorithm's sensitivity for predicting progression to nvAMD from\nnone/early/iAMD (78+/-6%) was higher than manual grades from the 9-step scale\n(67+/-8%) or the 4-category scale (48+/-3%). For predicting progression\nspecifically from iAMD, the DL algorithm's sensitivity (57+/-6%) was also\nhigher compared to the 9-step grades (36+/-8%) and the 4-category grades\n(20+/-0%). Conclusions: Our DL algorithm performed better in predicting\nprogression to nvAMD than manual grades. Future investigations are required to\ntest the application of this DL algorithm in a real-world clinical setting.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 23:31:01 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Babenko", "Boris", ""], ["Balasubramanian", "Siva", ""], ["Blumer", "Katy E.", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""], ["Hammel", "Naama", ""], ["Varadarajan", "Avinash V.", ""]]}, {"id": "1904.05509", "submitter": "Chongsheng Cheng", "authors": "Chongsheng Cheng, Zhexiong Shang, and Zhigang Shen", "title": "CNN-Based Deep Architecture for Reinforced Concrete Delamination\n  Segmentation Through Thermography", "comments": "Accepted for the 2019 ASCE International Conference on Computing in\n  Civil Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delamination assessment of the bridge deck plays a vital role for bridge\nhealth monitoring. Thermography as one of the nondestructive technologies for\ndelamination detection has the advantage of efficient data acquisition. But\nthere are challenges on the interpretation of data for accurate delamination\nshape profiling. Due to the environmental variation and the irregular presence\nof delamination size and depth, conventional processing methods based on\ntemperature contrast fall short in accurate segmentation of delamination.\nInspired by the recent development of deep learning architecture for image\nsegmentation, the Convolutional Neural Network (CNN) based framework was\ninvestigated for the applicability of delamination segmentation under\nvariations in temperature contrast and shape diffusion. The models were\ndeveloped based on Dense Convolutional Network (DenseNet) and trained on\nthermal images collected for mimicked delamination in concrete slabs with\ndifferent depths under experimental setup. The results suggested satisfactory\nperformance of accurate profiling the delamination shapes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 03:13:41 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cheng", "Chongsheng", ""], ["Shang", "Zhexiong", ""], ["Shen", "Zhigang", ""]]}, {"id": "1904.05512", "submitter": "Luyang Wang", "authors": "Luyang Wang and Yan Chen and Zhenhua Guo and Keyuan Qian and Mude Lin\n  and Hongsheng Li and Jimmy S. Ren", "title": "Generalizing Monocular 3D Human Pose Estimation in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of the large-scale labeled 3D poses in the Human3.6M dataset\nplays an important role in advancing the algorithms for 3D human pose\nestimation from a still image. We observe that recent innovation in this area\nmainly focuses on new techniques that explicitly address the generalization\nissue when using this dataset, because this database is constructed in a highly\ncontrolled environment with limited human subjects and background variations.\nDespite such efforts, we can show that the results of the current methods are\nstill error-prone especially when tested against the images taken in-the-wild.\nIn this paper, we aim to tackle this problem from a different perspective. We\npropose a principled approach to generate high quality 3D pose ground truth\ngiven any in-the-wild image with a person inside. We achieve this by first\ndevising a novel stereo inspired neural network to directly map any 2D pose to\nhigh quality 3D counterpart. We then perform a carefully designed geometric\nsearching scheme to further refine the joints. Based on this scheme, we build a\nlarge-scale dataset with 400,000 in-the-wild images and their corresponding 3D\npose ground truth. This enables the training of a high quality neural network\nmodel, without specialized training scheme and auxiliary loss function, which\nperforms favorably against the state-of-the-art 3D pose estimation methods. We\nalso evaluate the generalization ability of our model both quantitatively and\nqualitatively. Results show that our approach convincingly outperforms the\nprevious methods. We make our dataset and code publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 03:18:15 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wang", "Luyang", ""], ["Chen", "Yan", ""], ["Guo", "Zhenhua", ""], ["Qian", "Keyuan", ""], ["Lin", "Mude", ""], ["Li", "Hongsheng", ""], ["Ren", "Jimmy S.", ""]]}, {"id": "1904.05514", "submitter": "Vishnu Naresh Boddeti", "authors": "Proteek Chandan Roy and Vishnu Naresh Boddeti", "title": "Mitigating Information Leakage in Image Representations: A Maximum\n  Entropy Approach", "comments": "Accepted for oral presentation at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image recognition systems have demonstrated tremendous progress over the past\nfew decades thanks, in part, to our ability of learning compact and robust\nrepresentations of images. As we witness the wide spread adoption of these\nsystems, it is imperative to consider the problem of unintended leakage of\ninformation from an image representation, which might compromise the privacy of\nthe data owner. This paper investigates the problem of learning an image\nrepresentation that minimizes such leakage of user information. We formulate\nthe problem as an adversarial non-zero sum game of finding a good embedding\nfunction with two competing goals: to retain as much task dependent\ndiscriminative image information as possible, while simultaneously minimizing\nthe amount of information, as measured by entropy, about other sensitive\nattributes of the user. We analyze the stability and convergence dynamics of\nthe proposed formulation using tools from non-linear systems theory and compare\nto that of the corresponding adversarial zero-sum game formulation that\noptimizes likelihood as a measure of information content. Numerical experiments\non UCI, Extended Yale B, CIFAR-10 and CIFAR-100 datasets indicate that our\nproposed approach is able to learn image representations that exhibit high task\nperformance while mitigating leakage of predefined sensitive information.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 03:34:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Roy", "Proteek Chandan", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1904.05519", "submitter": "Uttaran Bhattacharya", "authors": "Uttaran Bhattacharya and Venu Madhav Govindu", "title": "Efficient and Robust Registration on the 3D Special Euclidean Group", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accurate, robust and fast method for registration of 3D scans.\nOur motion estimation optimizes a robust cost function on the intrinsic\nrepresentation of rigid motions, i.e., the Special Euclidean group\n$\\mathbb{SE}(3)$. We exploit the geometric properties of Lie groups as well as\nthe robustness afforded by an iteratively reweighted least squares\noptimization. We also generalize our approach to a joint multiview method that\nsimultaneously solves for the registration of a set of scans. We demonstrate\nthe efficacy of our approach by thorough experimental validation. Our approach\nsignificantly outperforms the state-of-the-art robust 3D registration method\nbased on a line process in terms of both speed and accuracy. We also show that\nthis line process method is a special case of our principled geometric\nsolution. Finally, we also present scenarios where global registration based on\nfeature correspondences fails but multiview ICP based on our robust motion\nestimation is successful.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 03:52:42 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Bhattacharya", "Uttaran", ""], ["Govindu", "Venu Madhav", ""]]}, {"id": "1904.05521", "submitter": "Hao Wu", "authors": "Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun,\n  Wei-Ying Ma", "title": "UniVSE: Robust Visual Semantic Embeddings via Structured Semantic\n  Representations", "comments": "v1 is the full version which is accepted by CVPR 2019. v2 is the\n  short version accepted by NAACL 2019 SpLU-RoboNLP workshop (in non-archival\n  proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Unified Visual-Semantic Embeddings (UniVSE) for learning a joint\nspace of visual and textual concepts. The space unifies the concepts at\ndifferent levels, including objects, attributes, relations, and full scenes. A\ncontrastive learning approach is proposed for the fine-grained alignment from\nonly image-caption pairs. Moreover, we present an effective approach for\nenforcing the coverage of semantic components that appear in the sentence. We\ndemonstrate the robustness of Unified VSE in defending text-domain adversarial\nattacks on cross-modal retrieval tasks. Such robustness also empowers the use\nof visual cues to resolve word dependencies in novel sentences.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:04:06 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 03:21:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wu", "Hao", ""], ["Mao", "Jiayuan", ""], ["Zhang", "Yufeng", ""], ["Jiang", "Yuning", ""], ["Li", "Lei", ""], ["Sun", "Weiwei", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1904.05537", "submitter": "Leonid Keselman", "authors": "Leonid Keselman, Martial Hebert", "title": "Direct Fitting of Gaussian Mixture Models", "comments": "Accepted to the Conference on Computer and Robot Vision 2019. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When fitting Gaussian Mixture Models to 3D geometry, the model is typically\nfit to point clouds, even when the shapes were obtained as 3D meshes. Here we\npresent a formulation for fitting Gaussian Mixture Models (GMMs) directly to a\ntriangular mesh instead of using points sampled from its surface. Part of this\nwork analyzes a general formulation for evaluating likelihood of geometric\nobjects. This modification enables fitting higher-quality GMMs under a wider\nrange of initialization conditions. Additionally, models obtained from this\nfitting method are shown to produce an improvement in 3D registration for both\nmeshes and RGB-D frames. This result is general and applicable to arbitrary\ngeometric objects, including representing uncertainty from sensor measurements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 05:19:52 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 02:34:47 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Keselman", "Leonid", ""], ["Hebert", "Martial", ""]]}, {"id": "1904.05544", "submitter": "Zhuo Lei", "authors": "Zhuo Lei, Chao Zhang, Qian Zhang and Guoping Qiu", "title": "FrameRank: A Text Processing Approach to Video Summarization", "comments": "accepted by ICME 2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization has been extensively studied in the past decades.\nHowever, user-generated video summarization is much less explored since there\nlack large-scale video datasets within which human-generated video summaries\nare unambiguously defined and annotated. Toward this end, we propose a\nuser-generated video summarization dataset - UGSum52 - that consists of 52\nvideos (207 minutes). In constructing the dataset, because of the subjectivity\nof user-generated video summarization, we manually annotate 25 summaries for\neach video, which are in total 1300 summaries. To the best of our knowledge, it\nis currently the largest dataset for user-generated video summarization.\n  Based on this dataset, we present FrameRank, an unsupervised video\nsummarization method that employs a frame-to-frame level affinity graph to\nidentify coherent and informative frames to summarize a video. We use the\nKullback-Leibler(KL)-divergence-based graph to rank temporal segments according\nto the amount of semantic information contained in their frames. We illustrate\nthe effectiveness of our method by applying it to three datasets SumMe, TVSum\nand UGSum52 and show it achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:16:17 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 10:45:19 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lei", "Zhuo", ""], ["Zhang", "Chao", ""], ["Zhang", "Qian", ""], ["Qiu", "Guoping", ""]]}, {"id": "1904.05547", "submitter": "Chen Li", "authors": "Chen Li, Gim Hee Lee", "title": "Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture\n  Density Network", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose estimation from a monocular image or 2D joints is an ill-posed\nproblem because of depth ambiguity and occluded joints. We argue that 3D human\npose estimation from a monocular input is an inverse problem where multiple\nfeasible solutions can exist. In this paper, we propose a novel approach to\ngenerate multiple feasible hypotheses of the 3D pose from 2D joints.In contrast\nto existing deep learning approaches which minimize a mean square error based\non an unimodal Gaussian distribution, our method is able to generate multiple\nfeasible hypotheses of 3D pose based on a multimodal mixture density networks.\nOur experiments show that the 3D poses estimated by our approach from an input\nof 2D joints are consistent in 2D reprojections, which supports our argument\nthat multiple solutions exist for the 2D-to-3D inverse problem. Furthermore, we\nshow state-of-the-art performance on the Human3.6M dataset in both best\nhypothesis and multi-view settings, and we demonstrate the generalization\ncapacity of our model by testing on the MPII and MPI-INF-3DHP datasets. Our\ncode is available at the project website.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:26:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Li", "Chen", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1904.05548", "submitter": "Zilong Zheng", "authors": "Zilong Zheng, Wenguan Wang, Siyuan Qi, Song-Chun Zhu", "title": "Reasoning Visual Dialogs with Structural and Partial Observations", "comments": "CVPR 2019 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model to address the task of Visual Dialog which exhibits\ncomplex dialog structures. To obtain a reasonable answer based on the current\nquestion and the dialog history, the underlying semantic dependencies between\ndialog entities are essential. In this paper, we explicitly formalize this task\nas inference in a graphical model with partially observed nodes and unknown\ngraph structures (relations in dialog). The given dialog entities are viewed as\nthe observed nodes. The answer to a given question is represented by a node\nwith missing value. We first introduce an Expectation Maximization algorithm to\ninfer both the underlying dialog structures and the missing node values\n(desired answers). Based on this, we proceed to propose a differentiable graph\nneural network (GNN) solution that approximates this process. Experiment\nresults on the VisDial and VisDial-Q datasets show that our model outperforms\ncomparative methods. It is also observed that our method can infer the\nunderlying dialog structure for better dialog reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:46:15 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 23:40:33 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zheng", "Zilong", ""], ["Wang", "Wenguan", ""], ["Qi", "Siyuan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1904.05562", "submitter": "Huawei Wei", "authors": "Huawei Wei, Shuang Liang, Yichen Wei", "title": "3D Dense Face Alignment via Graph Convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D face reconstruction and face alignment tasks are gradually\ncombined into one task: 3D dense face alignment. Its goal is to reconstruct the\n3D geometric structure of face with pose information. In this paper, we propose\na graph convolution network to regress 3D face coordinates. Our method directly\nperforms feature learning on the 3D face mesh, where the geometric structure\nand details are well preserved. Extensive experiments show that our approach\ngains superior performance over state-of-the-art methods on several challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 07:19:07 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wei", "Huawei", ""], ["Liang", "Shuang", ""], ["Wei", "Yichen", ""]]}, {"id": "1904.05578", "submitter": "Qian Zhang", "authors": "Qian Zhang, Li Wang, Xiaopeng Zong, Weili Lin, Gang Li and Dinggang\n  Shen", "title": "FRNET: Flattened Residual Network for Infant MRI Skull Stripping", "comments": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI)", "journal-ref": null, "doi": "10.1109/ISBI.2019.8759167", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skull stripping for brain MR images is a basic segmentation task. Although\nmany methods have been proposed, most of them focused mainly on the adult MR\nimages. Skull stripping for infant MR images is more challenging due to the\nsmall size and dynamic intensity changes of brain tissues during the early\nages. In this paper, we propose a novel CNN based framework to robustly extract\nbrain region from infant MR image without any human assistance. Specifically,\nwe propose a simplified but more robust flattened residual network architecture\n(FRnet). We also introduce a new boundary loss function to highlight ambiguous\nand low contrast regions between brain and non-brain regions. To make the whole\nframework more robust to MR images with different imaging quality, we further\nintroduce an artifact simulator for data augmentation. We have trained and\ntested our proposed framework on a large dataset (N=343), covering newborns to\n48-month-olds, and obtained performance better than the state-of-the-art\nmethods in all age groups.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 08:41:19 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zhang", "Qian", ""], ["Wang", "Li", ""], ["Zong", "Xiaopeng", ""], ["Lin", "Weili", ""], ["Li", "Gang", ""], ["Shen", "Dinggang", ""]]}, {"id": "1904.05582", "submitter": "Iulia Duta", "authors": "Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu", "title": "Recurrent Space-time Graph Neural Networks", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32 {NeurIPS\n  2019} pages 12838-1285", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in the space-time domain remains a very challenging problem in\nmachine learning and computer vision. Current computational models for\nunderstanding spatio-temporal visual data are heavily rooted in the classical\nsingle-image based paradigm. It is not yet well understood how to integrate\ninformation in space and time into a single, general model. We propose a neural\ngraph model, recurrent in space and time, suitable for capturing both the local\nappearance and the complex higher-level interactions of different entities and\nobjects within the changing world scene. Nodes and edges in our graph have\ndedicated neural networks for processing information. Nodes operate over\nfeatures extracted from local parts in space and time and previous memory\nstates. Edges process messages between connected nodes at different locations\nand spatial scales or between past and present time. Messages are passed\niteratively in order to transmit information globally and establish long range\ninteractions. Our model is general and could learn to recognize a variety of\nhigh level spatio-temporal concepts and be applied to different learning tasks.\nWe demonstrate, through extensive experiments and ablation studies, that our\nmodel outperforms strong baselines and top published methods on recognizing\ncomplex activities in video. Moreover, we obtain state-of-the-art performance\non the challenging Something-Something human-object interaction dataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 08:51:48 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 13:40:23 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 15:12:41 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2019 15:18:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nicolicioiu", "Andrei", ""], ["Duta", "Iulia", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1904.05586", "submitter": "Vignesh Srinivasan", "authors": "Vignesh Srinivasan, Ercan E. Kuruoglu, Klaus-Robert M\\\"uller, Wojciech\n  Samek and Shinichi Nakajima", "title": "Black-Box Decision based Adversarial Attack with Symmetric\n  $\\alpha$-stable Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing techniques for adversarial attack and defense is an important\nresearch field for establishing reliable machine learning and its applications.\nMany existing methods employ Gaussian random variables for exploring the data\nspace to find the most adversarial (for attacking) or least adversarial (for\ndefense) point. However, the Gaussian distribution is not necessarily the\noptimal choice when the exploration is required to follow the complicated\nstructure that most real-world data distributions exhibit. In this paper, we\ninvestigate how statistics of random variables affect such random walk\nexploration. Specifically, we generalize the Boundary Attack, a\nstate-of-the-art black-box decision based attacking strategy, and propose the\nL\\'evy-Attack, where the random walk is driven by symmetric $\\alpha$-stable\nrandom variables. Our experiments on MNIST and CIFAR10 datasets show that the\nL\\'evy-Attack explores the image data space more efficiently, and significantly\nimproves the performance. Our results also give an insight into the recently\nfound fact in the whitebox attacking scenario that the choice of the norm for\nmeasuring the amplitude of the adversarial patterns is essential.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 09:02:30 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Srinivasan", "Vignesh", ""], ["Kuruoglu", "Ercan E.", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""], ["Nakajima", "Shinichi", ""]]}, {"id": "1904.05614", "submitter": "Inbar Huberman", "authors": "Inbar Huberman and Raanan Fattal", "title": "Reducing Lateral Visual Biases in Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system is composed of multiple physiological components that\napply multiple mechanisms in order to cope with the rich visual content it\nencounters. The complexity of this system leads to non-trivial relations\nbetween what we see and what we perceive, and in particular, between the raw\nintensities of an image that we display and the ones we perceive where various\nvisual biases and illusions are introduced. In this paper we describe a method\nfor reducing a large class of biases related to the lateral inhibition\nmechanism in the human retina where neurons suppress the activity of\nneighboring receptors. Among these biases are the well-known Mach bands and\nhalos that appear around smooth and sharp image gradients as well as the\nappearance of false contrasts between identical regions. The new method removes\nthese visual biases by computing an image that contains counter biases such\nthat when this laterally-compensated image is viewed on a display, the inserted\nbiases cancel the ones created in the retina.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 10:30:11 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Huberman", "Inbar", ""], ["Fattal", "Raanan", ""]]}, {"id": "1904.05629", "submitter": "Inbar Huberman-Spiegelglas", "authors": "Inbar Huberman and Raanan Fattal", "title": "Detecting Repeating Objects using Patch Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a new method for detecting and counting a repeating\nobject in an image. While the method relies on a fairly sophisticated\ndeformable part model, unlike existing techniques it estimates the model\nparameters in an unsupervised fashion thus alleviating the need for a\nuser-annotated training data and avoiding the associated specificity. This\nautomatic fitting process is carried out by exploiting the recurrence of small\nimage patches associated with the repeating object and analyzing their spatial\ncorrelation. The analysis allows us to reject outlier patches, recover the\nvisual and shape parameters of the part model, and detect the object instances\nefficiently. In order to achieve a practical system which is able to cope with\ndiverse images, we describe a simple and intuitive active-learning procedure\nthat updates the object classification by querying the user on very few\ncarefully chosen marginal classifications. Evaluation of the new method against\nthe state-of-the-art techniques demonstrates its ability to achieve higher\naccuracy through a better user experience.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:17:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Huberman", "Inbar", ""], ["Fattal", "Raanan", ""]]}, {"id": "1904.05644", "submitter": "Ning Tan", "authors": "Yun Jiang, Ning Tan, Tingting Peng, Hai Zhang", "title": "Retinal Vessels Segmentation Based on Dilated Multi-Scale Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of retinal vessels is a basic step in Diabetic\nretinopathy(DR) detection. Most methods based on deep convolutional neural\nnetwork (DCNN) have small receptive fields, and hence they are unable to\ncapture global context information of larger regions, with difficult to\nidentify lesions. The final segmented retina vessels contain more noise with\nlow classification accuracy. Therefore, in this paper, we propose a DCNN\nstructure named as D-Net. In the proposed D-Net, the dilation convolution is\nused in the backbone network to obtain a larger receptive field without losing\nspatial resolution, so as to reduce the loss of feature information and to\nreduce the difficulty of tiny thin vessels segmentation. The large receptive\nfield can better distinguished between the lesion area and the blood vessel\narea. In the proposed Multi-Scale Information Fusion module (MSIF), parallel\nconvolution layers with different dilation rates are used, so that the model\ncan obtain more dense feature information and better capture retinal vessel\ninformation of different sizes. In the decoding module, the skip layer\nconnection is used to propagate context information to higher resolution\nlayers, so as to prevent low-level information from passing the entire network\nstructure. Finally, our method was verified on DRIVE, STARE and CHASE dataset.\nThe experimental results show that our network structure outperforms some\nstate-of-art method, such as N4-fields, U-Net, and DRIU in terms of accuracy,\nsensitivity, specificity, and AUCROC. Particularly, D-Net outperforms U-Net by\n1.04%, 1.23% and 2.79% in DRIVE, STARE, and CHASE three dataset, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:47:58 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Jiang", "Yun", ""], ["Tan", "Ning", ""], ["Peng", "Tingting", ""], ["Zhang", "Hai", ""]]}, {"id": "1904.05647", "submitter": "Fang Wan", "authors": "Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao and Qixiang Ye", "title": "C-MIL: Continuation Multiple Instance Learning for Weakly Supervised\n  Object Detection", "comments": "Accept by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD) is a challenging task when provided\nwith image category supervision but required to simultaneously learn object\nlocations and object detectors. Many WSOD approaches adopt multiple instance\nlearning (MIL) and have non-convex loss functions which are prone to get stuck\ninto local minima (falsely localize object parts) while missing full object\nextent during training. In this paper, we introduce a continuation optimization\nmethod into MIL and thereby creating continuation multiple instance learning\n(C-MIL), with the intention of alleviating the non-convexity problem in a\nsystematic way. We partition instances into spatially related and class related\nsubsets, and approximate the original loss function with a series of smoothed\nloss functions defined within the subsets. Optimizing smoothed loss functions\nprevents the training procedure falling prematurely into local minima and\nfacilitates the discovery of Stable Semantic Extremal Regions (SSERs) which\nindicate full object extent. On the PASCAL VOC 2007 and 2012 datasets, C-MIL\nimproves the state-of-the-art of weakly supervised object detection and weakly\nsupervised object localization with large margins.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:59:41 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wan", "Fang", ""], ["Liu", "Chang", ""], ["Ke", "Wei", ""], ["Ji", "Xiangyang", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "1904.05651", "submitter": "Arnesh Sen", "authors": "Arnesh Sen, Kaustav Sen, Jayoti Das", "title": "Software Based Higher Order Structural Foot Abnormality Detection Using\n  Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entire movement of human body undergoes through a periodic process named\nGait Cycle. The structure of human foot is the key element to complete the\ncycle successfully. Abnormality of this foot structure is an alarming form of\ncongenital disorder which results a classification based on the geometry of the\nhuman foot print image. Image processing is one of the most efficient way to\ndetermine a number of footprint parameter to detect the severeness of disorder.\nThis paper aims to detect the Flatfoot and High Arch foot abnormalities using\none of the footprint parameters named Modified Brucken Index by biomedical\nimage processing.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 12:09:31 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sen", "Arnesh", ""], ["Sen", "Kaustav", ""], ["Das", "Jayoti", ""]]}, {"id": "1904.05673", "submitter": "Thomas Boulay", "authors": "Thomas Boulay, Said El-Hachimi, Mani Kumar Surisetti, Pullarao Maddu,\n  Saranya Kandan", "title": "YUVMultiNet: Real-time YUV multi-task CNN for autonomous driving", "comments": "This paper is accepted for CVPR workshop demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-task convolutional neural network (CNN)\narchitecture optimized for a low power automotive grade SoC. We introduce a\nnetwork based on a unified architecture where the encoder is shared among the\ntwo tasks namely detection and segmentation. The pro-posed network runs at\n25FPS for 1280x800 resolution. We briefly discuss the methods used to optimize\nthe network architecture such as using native YUV image directly, optimization\nof layers & feature maps and applying quantization. We also focus on memory\nbandwidth in our design as convolutions are data intensives and most SOCs are\nbandwidth bottlenecked. We then demonstrate the efficiency of our proposed\nnetwork for a dedicated CNN accelerators presenting the key performance\nindicators (KPI) for the detection and segmentation tasks obtained from the\nhardware execution and the corresponding run-time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:08:05 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Boulay", "Thomas", ""], ["El-Hachimi", "Said", ""], ["Surisetti", "Mani Kumar", ""], ["Maddu", "Pullarao", ""], ["Kandan", "Saranya", ""]]}, {"id": "1904.05677", "submitter": "Muhammad Haris", "authors": "Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita", "title": "Deep Back-Projection Networks for Single Image Super-resolution", "comments": "To appear in TPAMI 2020. The code is available at\n  https://github.com/alterzero/DBPN-Pytorch arXiv admin note: substantial text\n  overlap with arXiv:1803.02735", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous feed-forward architectures of recently proposed deep\nsuper-resolution networks learn the features of low-resolution inputs and the\nnon-linear mapping from those to a high-resolution output. However, this\napproach does not fully address the mutual dependencies of low- and\nhigh-resolution images. We propose Deep Back-Projection Networks (DBPN), the\nwinner of two image super-resolution challenges (NTIRE2018 and PIRM2018), that\nexploit iterative up- and down-sampling layers. These layers are formed as a\nunit providing an error feedback mechanism for projection errors. We construct\nmutually-connected up- and down-sampling units each of which represents\ndifferent types of low- and high-resolution components. We also show that\nextending this idea to demonstrate a new insight towards more efficient network\ndesign substantially, such as parameter sharing on the projection module and\ntransition layer on projection step. The experimental results yield superior\nresults and in particular establishing new state-of-the-art results across\nmultiple data sets, especially for large scaling factors such as 8x.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:32:53 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 03:56:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Haris", "Muhammad", ""], ["Shakhnarovich", "Greg", ""], ["Ukita", "Norimichi", ""]]}, {"id": "1904.05709", "submitter": "Luis Pineda", "authors": "Luis Pineda, Amaia Salvador, Michal Drozdzal, Adriana Romero", "title": "Elucidating image-to-set prediction: An analysis of models, losses and\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify an important reproducibility challenge in the\nimage-to-set prediction literature that impedes proper comparisons among\npublished methods, namely, researchers use different evaluation protocols to\nassess their contributions. To alleviate this issue, we introduce an\nimage-to-set prediction benchmark suite built on top of five public datasets of\nincreasing task complexity that are suitable for multi-label classification\n(VOC, COCO, NUS-WIDE, ADE20k and Recipe1M). Using the benchmark, we provide an\nin-depth analysis where we study the key components of current models, namely\nthe choice of the image representation backbone as well as the set predictor\ndesign. Our results show that (1) exploiting better image representation\nbackbones leads to higher performance boosts than enhancing set predictors, and\n(2) modeling both the label co-occurrences and ordering has a slight positive\nimpact in terms of performance, whereas explicit cardinality prediction only\nhelps when training on complex datasets, such as Recipe1M. To facilitate future\nimage-to-set prediction research, we make the code, best models and dataset\nsplits publicly available at: https://github.com/facebookresearch/image-to-set.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:10:53 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 04:02:31 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Pineda", "Luis", ""], ["Salvador", "Amaia", ""], ["Drozdzal", "Michal", ""], ["Romero", "Adriana", ""]]}, {"id": "1904.05712", "submitter": "Nick Moran", "authors": "Nick Moran and Chiraag Juvekar", "title": "Reconstructing Network Inputs with Additive Perturbation Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this work, we present preliminary results demonstrating the ability to\nrecover a significant amount of information about secret model inputs given\nonly very limited access to model outputs and the ability evaluate the model on\nadditive perturbations to the input.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:17:52 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Moran", "Nick", ""], ["Juvekar", "Chiraag", ""]]}, {"id": "1904.05729", "submitter": "Xiang Chen", "authors": "Xiang Chen, Lingbo Qing, Xiaohai He, Xiaodong Luo, Yining Xu", "title": "FTGAN: A Fully-trained Generative Adversarial Networks for Text to Face\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a sub-domain of text-to-image synthesis, text-to-face generation has huge\npotentials in public safety domain. With lack of dataset, there are almost no\nrelated research focusing on text-to-face synthesis. In this paper, we propose\na fully-trained Generative Adversarial Network (FTGAN) that trains the text\nencoder and image decoder at the same time for fine-grained text-to-face\ngeneration. With a novel fully-trained generative network, FTGAN can synthesize\nhigher-quality images and urge the outputs of the FTGAN are more relevant to\nthe input sentences. In addition, we build a dataset called SCU-Text2face for\ntext-to-face synthesis. Through extensive experiments, the FTGAN shows its\nsuperiority in boosting both generated images' quality and similarity to the\ninput descriptions. The proposed FTGAN outperforms the previous state of the\nart, boosting the best reported Inception Score to 4.63 on the CUB dataset. On\nSCU-text2face, the face images generated by our proposed FTGAN just based on\nthe input descriptions is of average 59% similarity to the ground-truth, which\nset a baseline for text-to-face synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:38:35 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Chen", "Xiang", ""], ["Qing", "Lingbo", ""], ["He", "Xiaohai", ""], ["Luo", "Xiaodong", ""], ["Xu", "Yining", ""]]}, {"id": "1904.05730", "submitter": "Yuansheng Hua", "authors": "Lichao Mou, Yuansheng Hua, Xiao Xiang Zhu", "title": "A Relation-Augmented Fully Convolutional Network for Semantic\n  Segmentation in Aerial Scenes", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current semantic segmentation approaches fall back on deep convolutional\nneural networks (CNNs). However, their use of convolution operations with local\nreceptive fields causes failures in modeling contextual spatial relations.\nPrior works have sought to address this issue by using graphical models or\nspatial propagation modules in networks. But such models often fail to capture\nlong-range spatial relationships between entities, which leads to spatially\nfragmented predictions. Moreover, recent works have demonstrated that\nchannel-wise information also acts a pivotal part in CNNs. In this work, we\nintroduce two simple yet effective network units, the spatial relation module\nand the channel relation module, to learn and reason about global relationships\nbetween any two spatial positions or feature maps, and then produce\nrelation-augmented feature representations. The spatial and channel relation\nmodules are general and extensible, and can be used in a plug-and-play fashion\nwith the existing fully convolutional network (FCN) framework. We evaluate\nrelation module-equipped networks on semantic segmentation tasks using two\naerial image datasets, which fundamentally depend on long-range spatial\nrelational reasoning. The networks achieve very competitive results, bringing\nsignificant improvements over baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:40:03 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 11:34:40 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 12:19:05 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Mou", "Lichao", ""], ["Hua", "Yuansheng", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1904.05767", "submitter": "Yana Hasson", "authors": "Yana Hasson, G\\\"ul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael\n  J. Black, Ivan Laptev, Cordelia Schmid", "title": "Learning joint reconstruction of hands and manipulated objects", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating hand-object manipulations is essential for interpreting and\nimitating human actions. Previous work has made significant progress towards\nreconstruction of hand poses and object shapes in isolation. Yet,\nreconstructing hands and objects during manipulation is a more challenging task\ndue to significant occlusions of both the hand and object. While presenting\nchallenges, manipulations may also simplify the problem since the physics of\ncontact restricts the space of valid hand-object configurations. For example,\nduring manipulation, the hand and object should be in contact but not\ninterpenetrate. In this work, we regularize the joint reconstruction of hands\nand objects with manipulation constraints. We present an end-to-end learnable\nmodel that exploits a novel contact loss that favors physically plausible\nhand-object constellations. Our approach improves grasp quality metrics over\nbaselines, using RGB images as input. To train and evaluate the model, we also\npropose a new large-scale synthetic dataset, ObMan, with hand-object\nmanipulations. We demonstrate the transferability of ObMan-trained models to\nreal data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 15:21:33 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Hasson", "Yana", ""], ["Varol", "G\u00fcl", ""], ["Tzionas", "Dimitrios", ""], ["Kalevatykh", "Igor", ""], ["Black", "Michael J.", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1904.05773", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Rasoul Sali, Marium N. Khan, William Adorno, S. Asad\n  Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Sana Syed, Donald E. Brown", "title": "Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy\n  Images Using Color Balancing on Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of\nmalnutrition and adversely impact normal childhood development. CD is an\nautoimmune disorder that is prevalent worldwide and is caused by an increased\nsensitivity to gluten. Gluten exposure destructs the small intestinal\nepithelial barrier, resulting in nutrient mal-absorption and childhood\nunder-nutrition. EE also results in barrier dysfunction but is thought to be\ncaused by an increased vulnerability to infections. EE has been implicated as\nthe predominant cause of under-nutrition, oral vaccine failure, and impaired\ncognitive development in low-and-middle-income countries. Both conditions\nrequire a tissue biopsy for diagnosis, and a major challenge of interpreting\nclinical biopsy images to differentiate between these gastrointestinal diseases\nis striking histopathologic overlap between them. In the current study, we\npropose a convolutional neural network (CNN) to classify duodenal biopsy images\nfrom subjects with CD, EE, and healthy controls. We evaluated the performance\nof our proposed model using a large cohort containing 1000 biopsy images. Our\nevaluations show that the proposed model achieves an area under ROC of 0.99,\n1.00, and 0.97 for CD, EE, and healthy controls, respectively. These results\ndemonstrate the discriminative power of the proposed model in duodenal biopsies\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:24:32 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 02:34:20 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 19:22:31 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 16:22:32 GMT"}, {"version": "v5", "created": "Wed, 9 Oct 2019 16:24:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kowsari", "Kamran", ""], ["Sali", "Rasoul", ""], ["Khan", "Marium N.", ""], ["Adorno", "William", ""], ["Ali", "S. Asad", ""], ["Moore", "Sean R.", ""], ["Amadi", "Beatrice C.", ""], ["Kelly", "Paul", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "1904.05802", "submitter": "Yukai Shi", "authors": "Jinghui Qin, Ziwei Xie, Yukai Shi, Wushao Wen", "title": "Difficulty-aware Image Super Resolution via Deep Adaptive Dual-Network", "comments": "ICME2019(Oral), code and results are available at:\n  https://github.com/xzwlx/Difficulty-SR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently, deep learning based single image super-resolution(SR) approaches\nhave achieved great development. The state-of-the-art SR methods usually adopt\na feed-forward pipeline to establish a non-linear mapping between low-res(LR)\nand high-res(HR) images. However, due to treating all image regions equally\nwithout considering the difficulty diversity, these approaches meet an upper\nbound for optimization. To address this issue, we propose a novel SR approach\nthat discriminately processes each image region within an image by its\ndifficulty. Specifically, we propose a dual-way SR network that one way is\ntrained to focus on easy image regions and another is trained to handle hard\nimage regions. To identify whether a region is easy or hard, we propose a novel\nimage difficulty recognition network based on PSNR prior. Our SR approach that\nuses the region mask to adaptively enforce the dual-way SR network yields\nsuperior results. Extensive experiments on several standard benchmarks (e.g.,\nSet5, Set14, BSD100, and Urban100) show that our approach achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:00:57 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 04:15:54 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Qin", "Jinghui", ""], ["Xie", "Ziwei", ""], ["Shi", "Yukai", ""], ["Wen", "Wushao", ""]]}, {"id": "1904.05814", "submitter": "Tolga Birdal", "authors": "Tolga Birdal and Umut \\c{S}im\\c{s}ekli", "title": "Probabilistic Permutation Synchronization using the Riemannian Structure\n  of the Birkhoff Polytope", "comments": "To appear as oral presentation at CVPR 2019. 20 pages including the\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an entirely new geometric and probabilistic approach to\nsynchronization of correspondences across multiple sets of objects or images.\nIn particular, we present two algorithms: (1) Birkhoff-Riemannian L-BFGS for\noptimizing the relaxed version of the combinatorially intractable cycle\nconsistency loss in a principled manner, (2) Birkhoff-Riemannian Langevin Monte\nCarlo for generating samples on the Birkhoff Polytope and estimating the\nconfidence of the found solutions. To this end, we first introduce the very\nrecently developed Riemannian geometry of the Birkhoff Polytope. Next, we\nintroduce a new probabilistic synchronization model in the form of a Markov\nRandom Field (MRF). Finally, based on the first order retraction operators, we\nformulate our problem as simulating a stochastic differential equation and\ndevise new integrators. We show on both synthetic and real datasets that we\nachieve high quality multi-graph matching results with faster convergence and\nreliable confidence/uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:12:50 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Birdal", "Tolga", ""], ["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1904.05822", "submitter": "Rahul Garg", "authors": "Rahul Garg, Neal Wadhwa, Sameer Ansari, Jonathan T. Barron", "title": "Learning Single Camera Depth Estimation using Dual-Pixels", "comments": "Accepted to ICCV 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have enabled rapid progress in monocular depth\nestimation, but their quality is limited by the ill-posed nature of the problem\nand the scarcity of high quality datasets. We estimate depth from a single\ncamera by leveraging the dual-pixel auto-focus hardware that is increasingly\ncommon on modern camera sensors. Classic stereo algorithms and prior\nlearning-based depth estimation techniques under-perform when applied on this\ndual-pixel data, the former due to too-strong assumptions about RGB image\nmatching, and the latter due to not leveraging the understanding of optics of\ndual-pixel image formation. To allow learning based methods to work well on\ndual-pixel imagery, we identify an inherent ambiguity in the depth estimated\nfrom dual-pixel cues, and develop an approach to estimate depth up to this\nambiguity. Using our approach, existing monocular depth estimation techniques\ncan be effectively applied to dual-pixel data, and much smaller models can be\nconstructed that still infer high quality depth. To demonstrate this, we\ncapture a large dataset of in-the-wild 5-viewpoint RGB images paired with\ncorresponding dual-pixel data, and show how view supervision with this data can\nbe used to learn depth up to the unknown ambiguities. On our new task, our\nmodel is 30% more accurate than any prior work on learning-based monocular or\nstereoscopic depth estimation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:25:43 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 01:19:03 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 17:52:05 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Garg", "Rahul", ""], ["Wadhwa", "Neal", ""], ["Ansari", "Sameer", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1904.05835", "submitter": "Zhenwen Dai", "authors": "Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen\n  Dai", "title": "Variational Information Distillation for Knowledge Transfer", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from a teacher neural network pretrained on the same\nor a similar task to a student neural network can significantly improve the\nperformance of the student neural network. Existing knowledge transfer\napproaches match the activations or the corresponding hand-crafted features of\nthe teacher and the student networks. We propose an information-theoretic\nframework for knowledge transfer which formulates knowledge transfer as\nmaximizing the mutual information between the teacher and the student networks.\nWe compare our method with existing knowledge transfer methods on both\nknowledge distillation and transfer learning tasks and show that our method\nconsistently outperforms existing methods. We further demonstrate the strength\nof our method on knowledge transfer across heterogeneous network architectures\nby transferring knowledge from a convolutional neural network (CNN) to a\nmulti-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly\noutperforms the-state-of-the-art methods and it achieves similar performance to\nthe CNN with a single convolutional layer.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:39:19 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Hu", "Shell Xu", ""], ["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""], ["Dai", "Zhenwen", ""]]}, {"id": "1904.05847", "submitter": "Juan Leon", "authors": "Juan Leon Alcazar, Maria A. Bravo, Ali K. Thabet, Guillaume Jeanneret,\n  Thomas Brox, Pablo Arbelaez, Bernard Ghanem", "title": "MAIN: Multi-Attention Instance Network for Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-level video segmentation requires a solid integration of spatial and\ntemporal information. However, current methods rely mostly on domain-specific\ninformation (online learning) to produce accurate instance-level segmentations.\nWe propose a novel approach that relies exclusively on the integration of\ngeneric spatio-temporal attention cues. Our strategy, named Multi-Attention\nInstance Network (MAIN), overcomes challenging segmentation scenarios over\narbitrary videos without modelling sequence- or instance-specific knowledge. We\ndesign MAIN to segment multiple instances in a single forward pass, and\noptimize it with a novel loss function that favors class agnostic predictions\nand assigns instance-specific penalties. We achieve state-of-the-art\nperformance on the challenging Youtube-VOS dataset and benchmark, improving the\nunseen Jaccard and F-Metric by 6.8% and 12.7% respectively, while operating at\nreal-time (30.3 FPS).\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:59:22 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Alcazar", "Juan Leon", ""], ["Bravo", "Maria A.", ""], ["Thabet", "Ali K.", ""], ["Jeanneret", "Guillaume", ""], ["Brox", "Thomas", ""], ["Arbelaez", "Pablo", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.05866", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart,\n  Ahmed A. A. Osman, Dimitrios Tzionas and Michael J. Black", "title": "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate the analysis of human actions, interactions and emotions, we\ncompute a 3D model of human body pose, hand pose, and facial expression from a\nsingle monocular image. To achieve this, we use thousands of 3D scans to train\na new, unified, 3D model of the human body, SMPL-X, that extends SMPL with\nfully articulated hands and an expressive face. Learning to regress the\nparameters of SMPL-X directly from images is challenging without paired images\nand 3D ground truth. Consequently, we follow the approach of SMPLify, which\nestimates 2D features and then optimizes model parameters to fit the features.\nWe improve on SMPLify in several significant ways: (1) we detect 2D features\ncorresponding to the face, hands, and feet and fit the full SMPL-X model to\nthese; (2) we train a new neural network pose prior using a large MoCap\ndataset; (3) we define a new interpenetration penalty that is both fast and\naccurate; (4) we automatically detect gender and the appropriate body models\n(male, female, or neutral); (5) our PyTorch implementation achieves a speedup\nof more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to\nboth controlled images and images in the wild. We evaluate 3D accuracy on a new\ncurated dataset comprising 100 images with pseudo ground-truth. This is a step\ntowards automatic expressive human capture from monocular RGB data. The models,\ncode, and data are available for research purposes at\nhttps://smpl-x.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:47:37 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Choutas", "Vasileios", ""], ["Ghorbani", "Nima", ""], ["Bolkart", "Timo", ""], ["Osman", "Ahmed A. A.", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Michael J.", ""]]}, {"id": "1904.05868", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos and Jean Kossaifi and Maja\n  Pantic", "title": "Improved training of binary networks for human pose estimation and image\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big neural networks trained on large datasets have advanced the\nstate-of-the-art for a large variety of challenging problems, improving\nperformance by a large margin. However, under low memory and limited\ncomputational power constraints, the accuracy on the same problems drops\nconsiderable. In this paper, we propose a series of techniques that\nsignificantly improve the accuracy of binarized neural networks (i.e networks\nwhere both the features and the weights are binary). We evaluate the proposed\nimprovements on two diverse tasks: fine-grained recognition (human pose\nestimation) and large-scale image recognition (ImageNet classification).\nSpecifically, we introduce a series of novel methodological changes including:\n(a) more appropriate activation functions, (b) reverse-order initialization,\n(c) progressive quantization, and (d) network stacking and show that these\nadditions improve existing state-of-the-art network binarization techniques,\nsignificantly. Additionally, for the first time, we also investigate the extent\nto which network binarization and knowledge distillation can be combined. When\ntested on the challenging MPII dataset, our method shows a performance\nimprovement of more than 4% in absolute terms. Finally, we further validate our\nfindings by applying the proposed techniques for large-scale object recognition\non the Imagenet dataset, on which we report a reduction of error rate by 4%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:55:06 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""], ["Kossaifi", "Jean", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.05869", "submitter": "Oleh Rybkin", "authors": "Karl Pertsch, Oleh Rybkin, Jingyun Yang, Shenghao Zhou, Konstantinos\n  G. Derpanis, Kostas Daniilidis, Joseph Lim, Andrew Jaegle", "title": "Keyframing the Future: Keyframe Discovery for Visual Prediction and\n  Planning", "comments": "Conference on Learning for Dynamics and Control, 2020. Website:\n  https://sites.google.com/view/keyin/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal observations such as videos contain essential information about the\ndynamics of the underlying scene, but they are often interleaved with\ninessential, predictable details. One way of dealing with this problem is by\nfocusing on the most informative moments in a sequence. We propose a model that\nlearns to discover these important events and the times when they occur and\nuses them to represent the full sequence. We do so using a hierarchical\nKeyframe-Inpainter (KeyIn) model that first generates a video's keyframes and\nthen inpaints the rest by generating the frames at the intervening times. We\npropose a fully differentiable formulation to efficiently learn this procedure.\nWe show that KeyIn finds informative keyframes in several datasets with\ndifferent dynamics and visual properties. KeyIn outperforms other recent\nhierarchical predictive models for planning. For more details, please see the\nproject website at \\url{https://sites.google.com/view/keyin}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:55:09 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 00:53:23 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Pertsch", "Karl", ""], ["Rybkin", "Oleh", ""], ["Yang", "Jingyun", ""], ["Zhou", "Shenghao", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""], ["Lim", "Joseph", ""], ["Jaegle", "Andrew", ""]]}, {"id": "1904.05871", "submitter": "Hengduo Li", "authors": "Hengduo Li, Bharat Singh, Mahyar Najibi, Zuxuan Wu, Larry S. Davis", "title": "An Analysis of Pre-Training on Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a detailed analysis of convolutional neural networks which are\npre-trained on the task of object detection. To this end, we train detectors on\nlarge datasets like OpenImagesV4, ImageNet Localization and COCO. We analyze\nhow well their features generalize to tasks like image classification, semantic\nsegmentation and object detection on small datasets like PASCAL-VOC,\nCaltech-256, SUN-397, Flowers-102 etc. Some important conclusions from our\nanalysis are --- 1) Pre-training on large detection datasets is crucial for\nfine-tuning on small detection datasets, especially when precise localization\nis needed. For example, we obtain 81.1% mAP on the PASCAL-VOC dataset at 0.7\nIoU after pre-training on OpenImagesV4, which is 7.6% better than the recently\nproposed DeformableConvNetsV2 which uses ImageNet pre-training. 2) Detection\npre-training also benefits other localization tasks like semantic segmentation\nbut adversely affects image classification. 3) Features for images (like avg.\npooled Conv5) which are similar in the object detection feature space are\nlikely to be similar in the image classification feature space but the converse\nis not true. 4) Visualization of features reveals that detection neurons have\nactivations over an entire object, while activations for classification\nnetworks typically focus on parts. Therefore, detection networks are poor at\nclassification when multiple instances are present in an image or when an\ninstance only covers a small fraction of an image.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:58:23 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Li", "Hengduo", ""], ["Singh", "Bharat", ""], ["Najibi", "Mahyar", ""], ["Wu", "Zuxuan", ""], ["Davis", "Larry S.", ""]]}, {"id": "1904.05873", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, Jifeng Dai", "title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have become a popular component in deep neural networks,\nyet there has been little examination of how different influencing factors and\nmethods for computing attention from these factors affect performance. Toward a\nbetter general understanding of attention mechanisms, we present an empirical\nstudy that ablates various spatial attention elements within a generalized\nattention formulation, encompassing the dominant Transformer attention as well\nas the prevalent deformable convolution and dynamic convolution modules.\nConducted on a variety of applications, the study yields significant findings\nabout spatial attention in deep networks, some of which run counter to\nconventional understanding. For example, we find that the query and key content\ncomparison in Transformer attention is negligible for self-attention, but vital\nfor encoder-decoder attention. A proper combination of deformable convolution\nwith key content only saliency achieves the best accuracy-efficiency tradeoff\nin self-attention. Our results suggest that there exists much room for\nimprovement in the design of attention mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:58:37 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zhu", "Xizhou", ""], ["Cheng", "Dazhi", ""], ["Zhang", "Zheng", ""], ["Lin", "Stephen", ""], ["Dai", "Jifeng", ""]]}, {"id": "1904.05876", "submitter": "Idan Schwartz", "authors": "Idan Schwartz, Alexander Schwing and Tamir Hazan", "title": "A Simple Baseline for Audio-Visual Scene-Aware Dialog", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed audio-visual scene-aware dialog task paves the way to a\nmore data-driven way of learning virtual assistants, smart speakers and car\nnavigation systems. However, very little is known to date about how to\neffectively extract meaningful information from a plethora of sensors that\npound the computational engine of those devices. Therefore, in this paper, we\nprovide and carefully analyze a simple baseline for audio-visual scene-aware\ndialog which is trained end-to-end. Our method differentiates in a data-driven\nmanner useful signals from distracting ones using an attention mechanism. We\nevaluate the proposed approach on the recently introduced and challenging\naudio-visual scene-aware dataset, and demonstrate the key features that permit\nto outperform the current state-of-the-art by more than 20\\% on CIDEr.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:51 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Schwartz", "Idan", ""], ["Schwing", "Alexander", ""], ["Hazan", "Tamir", ""]]}, {"id": "1904.05877", "submitter": "Yuan-Ting Hu", "authors": "Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui,\n  Sanmi Koyejo, Zhizhen Zhao, David Forsyth, Alexander Schwing", "title": "Max-Sliced Wasserstein Distance and its use for GANs", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) and variational auto-encoders have\nsignificantly improved our distribution modeling capabilities, showing promise\nfor dataset augmentation, image-to-image translation and feature learning.\nHowever, to model high-dimensional distributions, sequential training and\nstacked architectures are common, increasing the number of tunable\nhyper-parameters as well as the training time. Nonetheless, the sample\ncomplexity of the distance metrics remains one of the factors affecting GAN\ntraining. We first show that the recently proposed sliced Wasserstein distance\nhas compelling sample complexity properties when compared to the Wasserstein\ndistance. To further improve the sliced Wasserstein distance we then analyze\nits `projection complexity' and develop the max-sliced Wasserstein distance\nwhich enjoys compelling sample complexity while reducing projection complexity,\nalbeit necessitating a max estimation. We finally illustrate that the proposed\ndistance trains GANs on high-dimensional images up to a resolution of 256x256\neasily.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Deshpande", "Ishan", ""], ["Hu", "Yuan-Ting", ""], ["Sun", "Ruoyu", ""], ["Pyrros", "Ayis", ""], ["Siddiqui", "Nasir", ""], ["Koyejo", "Sanmi", ""], ["Zhao", "Zhizhen", ""], ["Forsyth", "David", ""], ["Schwing", "Alexander", ""]]}, {"id": "1904.05879", "submitter": "Unnat Jain", "authors": "Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana\n  Lazebnik, Ali Farhadi, Alexander Schwing and Aniruddha Kembhavi", "title": "Two Body Problem: Collaborative Visual Task Completion", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration is a necessary skill to perform tasks that are beyond one\nagent's capabilities. Addressed extensively in both conventional and modern AI,\nmulti-agent collaboration has often been studied in the context of simple grid\nworlds. We argue that there are inherently visual aspects to collaboration\nwhich should be studied in visually rich environments. A key element in\ncollaboration is communication that can be either explicit, through messages,\nor implicit, through perception of the other agents and the visual world.\nLearning to collaborate in a visual environment entails learning (1) to perform\nthe task, (2) when and what to communicate, and (3) how to act based on these\ncommunications and the perception of the visual world. In this paper we study\nthe problem of learning to collaborate directly from pixels in AI2-THOR and\ndemonstrate the benefits of explicit and implicit modes of communication to\nperform visual tasks. Refer to our project page for more details:\nhttps://prior.allenai.org/projects/two-body-problem\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Jain", "Unnat", ""], ["Weihs", "Luca", ""], ["Kolve", "Eric", ""], ["Rastegari", "Mohammad", ""], ["Lazebnik", "Svetlana", ""], ["Farhadi", "Ali", ""], ["Schwing", "Alexander", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "1904.05880", "submitter": "Idan Schwartz", "authors": "Idan Schwartz and Seunghak Yu and Tamir Hazan and Alexander Schwing", "title": "Factor Graph Attention", "comments": "Accepted to CVPR 2019; revised version includes bottom-up features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog is an effective way to exchange information, but subtle details and\nnuances are extremely important. While significant progress has paved a path to\naddress visual dialog with algorithms, details and nuances remain a challenge.\nAttention mechanisms have demonstrated compelling results to extract details in\nvisual question answering and also provide a convincing framework for visual\ndialog due to their interpretability and effectiveness. However, the many data\nutilities that accompany visual dialog challenge existing attention techniques.\nWe address this issue and develop a general attention mechanism for visual\ndialog which operates on any number of data utilities. To this end, we design a\nfactor graph based attention mechanism which combines any number of utility\nrepresentations. We illustrate the applicability of the proposed approach on\nthe challenging and recently introduced VisDial datasets, outperforming recent\nstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on\nMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:58 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 20:05:12 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 23:35:13 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Schwartz", "Idan", ""], ["Yu", "Seunghak", ""], ["Hazan", "Tamir", ""], ["Schwing", "Alexander", ""]]}, {"id": "1904.05916", "submitter": "Sara Beery", "authors": "Sara Beery, Yang Liu, Dan Morris, Jim Piavis, Ashish Kapoor, Markus\n  Meister, Neel Joshi, Pietro Perona", "title": "Synthetic Examples Improve Generalization for Rare Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect and classify rare occurrences in images has important\napplications - for example, counting rare and endangered species when studying\nbiodiversity, or detecting infrequent traffic scenarios that pose a danger to\nself-driving cars. Few-shot learning is an open problem: current computer\nvision systems struggle to categorize objects they have seen only rarely during\ntraining, and collecting a sufficient number of training examples of rare\nevents is often challenging and expensive, and sometimes outright impossible.\nWe explore in depth an approach to this problem: complementing the few\navailable training images with ad-hoc simulated data.\n  Our testbed is animal species classification, which has a real-world\nlong-tailed distribution. We analyze the effect of different axes of variation\nin simulation, such as pose, lighting, model, and simulation method, and we\nprescribe best practices for efficiently incorporating simulated data for\nreal-world performance gain. Our experiments reveal that synthetic data can\nconsiderably reduce error rates for classes that are rare, that as the amount\nof simulated data is increased, accuracy on the target class improves, and that\nhigh variation of simulated data provides maximum performance gain.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:28:43 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 04:27:29 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Beery", "Sara", ""], ["Liu", "Yang", ""], ["Morris", "Dan", ""], ["Piavis", "Jim", ""], ["Kapoor", "Ashish", ""], ["Meister", "Markus", ""], ["Joshi", "Neel", ""], ["Perona", "Pietro", ""]]}, {"id": "1904.05939", "submitter": "Salman Khan Dr.", "authors": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Fahad Shahbaz Khan, Ling\n  Shao", "title": "Learning Digital Camera Pipeline for Extreme Low-Light Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low-light conditions, a conventional camera imaging pipeline produces\nsub-optimal images that are usually dark and noisy due to a low photon count\nand low signal-to-noise ratio (SNR). We present a data-driven approach that\nlearns the desired properties of well-exposed images and reflects them in\nimages that are captured in extremely low ambient light environments, thereby\nsignificantly improving the visual quality of these low-light images. We\npropose a new loss function that exploits the characteristics of both\npixel-wise and perceptual metrics, enabling our deep neural network to learn\nthe camera processing pipeline to transform the short-exposure, low-light RAW\nsensor data to well-exposed sRGB images. The results show that our method\noutperforms the state-of-the-art according to psychophysical tests as well as\npixel-wise standard metrics and recent learning-based perceptual image quality\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 19:49:31 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zamir", "Syed Waqas", ""], ["Arora", "Aditya", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "1904.05947", "submitter": "M\\'arton V\\'eges", "authors": "M\\'arton V\\'eges, Andr\\'as L\\H{o}rincz", "title": "Absolute Human Pose Estimation with Depth Prediction Network", "comments": "Accepted to IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common approach to 3D human pose estimation is predicting the body joint\ncoordinates relative to the hip. This works well for a single person but is\ninsufficient in the case of multiple interacting people. Methods predicting\nabsolute coordinates first estimate a root-relative pose then calculate the\ntranslation via a secondary optimization task. We propose a neural network that\npredicts joints in a camera centered coordinate system instead of a\nroot-relative one. Unlike previous methods, our network works in a single step\nwithout any post-processing. Our network beats previous methods on the\nMuPoTS-3D dataset and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 20:26:37 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["V\u00e9ges", "M\u00e1rton", ""], ["L\u0151rincz", "Andr\u00e1s", ""]]}, {"id": "1904.05956", "submitter": "Sunyi Zheng", "authors": "Sunyi Zheng, Jiapan Guo, Xiaonan Cui, Raymond N. J. Veldhuis, Matthijs\n  Oudkerk, and Peter M.A.van Ooijen", "title": "Automatic Pulmonary Nodule Detection in CT Scans Using Convolutional\n  Neural Networks Based on Maximum Intensity Projection", "comments": "Submitted to IEEE TMI", "journal-ref": null, "doi": "10.1109/TMI.2019.2935553", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate pulmonary nodule detection is a crucial step in lung cancer\nscreening. Computer-aided detection (CAD) systems are not routinely used by\nradiologists for pulmonary nodule detection in clinical practice despite their\npotential benefits. Maximum intensity projection (MIP) images improve the\ndetection of pulmonary nodules in radiological evaluation with computed\ntomography (CT) scans. Inspired by the clinical methodology of radiologists, we\naim to explore the feasibility of applying MIP images to improve the\neffectiveness of automatic lung nodule detection using convolutional neural\nnetworks (CNNs). We propose a CNN-based approach that takes MIP images of\ndifferent slab thicknesses (5 mm, 10 mm, 15 mm) and 1 mm axial section slices\nas input. Such an approach augments the two-dimensional (2-D) CT slice images\nwith more representative spatial information that helps discriminate nodules\nfrom vessels through their morphologies. Our proposed method achieves\nsensitivity of 92.67% with 1 false positive per scan and sensitivity of 94.19%\nwith 2 false positives per scan for lung nodule detection on 888 scans in the\nLIDC-IDRI dataset. The use of thick MIP images helps the detection of small\npulmonary nodules (3 mm-10 mm) and results in fewer false positives.\nExperimental results show that utilizing MIP images can increase the\nsensitivity and lower the number of false positives, which demonstrates the\neffectiveness and significance of the proposed MIP-based CNNs framework for\nautomatic pulmonary nodule detection in CT scans. The proposed method also\nshows the potential that CNNs could gain benefits for nodule detection by\ncombining the clinical procedure.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 21:12:33 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 09:54:55 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zheng", "Sunyi", ""], ["Guo", "Jiapan", ""], ["Cui", "Xiaonan", ""], ["Veldhuis", "Raymond N. J.", ""], ["Oudkerk", "Matthijs", ""], ["van Ooijen", "Peter M. A.", ""]]}, {"id": "1904.05967", "submitter": "Xin Wang", "authors": "Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, Joseph E. Gonzalez", "title": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning good feature embeddings for images often requires substantial\ntraining data. As a consequence, in settings where training data is limited\n(e.g., few-shot and zero-shot learning), we are typically forced to use a\ngeneric feature embedding across various tasks. Ideally, we want to construct\nfeature embeddings that are tuned for the given task. In this work, we propose\nTask-Aware Feature Embedding Networks (TAFE-Nets) to learn how to adapt the\nimage representation to a new task in a meta learning fashion. Our network is\ncomposed of a meta learner and a prediction network. Based on a task input, the\nmeta learner generates parameters for the feature layers in the prediction\nnetwork so that the feature embedding can be accurately adjusted for that task.\nWe show that TAFE-Net is highly effective in generalizing to new tasks or\nconcepts and evaluate the TAFE-Net on a range of benchmarks in zero-shot and\nfew-shot learning. Our model matches or exceeds the state-of-the-art on all\ntasks. In particular, our approach improves the prediction accuracy of unseen\nattribute-object pairs by 4 to 15 points on the challenging visual\nattribute-object composition task.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 22:14:45 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Xin", ""], ["Yu", "Fisher", ""], ["Wang", "Ruth", ""], ["Darrell", "Trevor", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1904.05979", "submitter": "Hang Zhao", "authors": "Hang Zhao, Chuang Gan, Wei-Chiu Ma, Antonio Torralba", "title": "The Sound of Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sounds originate from object motions and vibrations of surrounding air.\nInspired by the fact that humans is capable of interpreting sound sources from\nhow objects move visually, we propose a novel system that explicitly captures\nsuch motion cues for the task of sound localization and separation. Our system\nis composed of an end-to-end learnable model called Deep Dense Trajectory\n(DDT), and a curriculum learning scheme. It exploits the inherent coherence of\naudio-visual signals from a large quantities of unlabeled videos. Quantitative\nand qualitative evaluations show that comparing to previous models that rely on\nvisual appearance cues, our motion based system improves performance in\nseparating musical instrument sounds. Furthermore, it separates sound\ncomponents from duets of the same category of instruments, a challenging\nproblem that has not been addressed before.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:05:52 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhao", "Hang", ""], ["Gan", "Chuang", ""], ["Ma", "Wei-Chiu", ""], ["Torralba", "Antonio", ""]]}, {"id": "1904.05982", "submitter": "Jon Hoffman", "authors": "Jon Hoffman", "title": "Cramnet: Layer-wise Deep Neural Network Compression with Knowledge\n  Transfer from a Teacher Network", "comments": "Thesis for Masters degree", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Networks accomplish amazing things, but they suffer from computational\nand memory bottlenecks that restrict their usage. Nowhere can this be better\nseen than in the mobile space, where specialized hardware is being created just\nto satisfy the demand for neural networks. Previous studies have shown that\nneural networks have vastly more connections than they actually need to do\ntheir work. This thesis develops a method that can compress networks to less\nthan 10% of memory and less than 25% of computational power, without loss of\naccuracy, and without creating sparse networks that require special code to\nrun.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:28:05 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Hoffman", "Jon", ""]]}, {"id": "1904.05986", "submitter": "Sara Beery", "authors": "Sara Beery, Grant van Horn, Oisin Mac Aodha, Pietro Perona", "title": "The iWildCam 2018 Challenge Dataset", "comments": "Challenge hosted at the fifth Fine-Grained Visual Categorization\n  Workshop (FGVC5) at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera traps are a valuable tool for studying biodiversity, but research\nusing this data is limited by the speed of human annotation. With the vast\namounts of data now available it is imperative that we develop automatic\nsolutions for annotating camera trap data in order to allow this research to\nscale. A promising approach is based on deep networks trained on\nhuman-annotated images. We provide a challenge dataset to explore whether such\nsolutions generalize to novel locations, since systems that are trained once\nand may be deployed to operate automatically in new locations would be most\nuseful.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:48:19 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 20:21:23 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Beery", "Sara", ""], ["van Horn", "Grant", ""], ["Mac Aodha", "Oisin", ""], ["Perona", "Pietro", ""]]}, {"id": "1904.05992", "submitter": "Hamed Alqahtani Mr", "authors": "Hamed Alqahtani, Manolya Kavakli-Thorne, and Charles Z. Liu", "title": "An Introduction to Person Re-identification with Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a basic subject in the field of computer vision.\nThe traditional methods have several limitations in solving the problems of\nperson illumination like occlusion, pose variation and feature variation under\ncomplex background. Fortunately, deep learning paradigm opens new ways of the\nperson re-identification research and becomes a hot spot in this field.\nGenerative Adversarial Nets (GANs) in the past few years attracted lots of\nattention in solving these problems. This paper reviews the GAN based methods\nfor person re-identification focuses on the related papers about different GAN\nbased frameworks and discusses their advantages and disadvantages. Finally, it\nproposes the direction of future research, especially the prospect of person\nre-identification methods based on GANs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 00:33:05 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 23:27:48 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Alqahtani", "Hamed", ""], ["Kavakli-Thorne", "Manolya", ""], ["Liu", "Charles Z.", ""]]}, {"id": "1904.06008", "submitter": "Pengju Zhang", "authors": "Qiuyu Zhu, Pengju Zhang, Xin Ye", "title": "A New Loss Function for CNN Classifier Based on Pre-defined\n  Evenly-Distributed Class Centroids", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of convolutional neural networks (CNNs) in recent years,\nthe network structure has become more and more complex and varied, and has\nachieved very good results in pattern recognition, image classification, object\ndetection and tracking. For CNNs used for image classification, in addition to\nthe network structure, more and more research is now focusing on the\nimprovement of the loss function, so as to enlarge the inter-class feature\ndifferences, and reduce the intra-class feature variations as soon as possible.\nBesides the traditional Softmax, typical loss functions include L-Softmax,\nAM-Softmax, ArcFace, and Center loss, etc. Based on the concept of predefined\nevenly-distributed class centroids (PEDCC) in CSAE network, this paper proposes\na PEDCC-based loss function called PEDCC-Loss, which can make the inter-class\ndistance maximal and intra-class distance small enough in hidden feature space.\nMultiple experiments on image classification and face recognition have proved\nthat our method achieve the best recognition accuracy, and network training is\nstable and easy to converge. Code is available in\nhttps://github.com/ZLeopard/PEDCC-Loss\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 02:19:45 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 11:50:59 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Zhu", "Qiuyu", ""], ["Zhang", "Pengju", ""], ["Ye", "Xin", ""]]}, {"id": "1904.06017", "submitter": "Rui Fan", "authors": "Rui Fan, Jianhao Jiao, Jie Pan, Huaiyang Huang, Shaojie Shen, Ming Liu", "title": "Real-Time Dense Stereo Embedded in A UAV for Road Inspection", "comments": "9 pages, 8 figures, In Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR) Workshops, June 16-20, 2019, Long\n  Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condition assessment of road surfaces is essential to ensure their\nserviceability while still providing maximum road traffic safety. This paper\npresents a robust stereo vision system embedded in an unmanned aerial vehicle\n(UAV). The perspective view of the target image is first transformed into the\nreference view, and this not only improves the disparity accuracy, but also\nreduces the algorithm's computational complexity. The cost volumes generated\nfrom stereo matching are then filtered using a bilateral filter. The latter has\nbeen proved to be a feasible solution for the functional minimisation problem\nin a fully connected Markov random field model. Finally, the disparity maps are\ntransformed by minimising an energy function with respect to the roll angle and\ndisparity projection model. This makes the damaged road areas more\ndistinguishable from the road surface. The proposed system is implemented on an\nNVIDIA Jetson TX2 GPU with CUDA for real-time purposes. It is demonstrated\nthrough experiments that the damaged road areas can be easily distinguished\nfrom the transformed disparity maps.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 02:45:32 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Fan", "Rui", ""], ["Jiao", "Jianhao", ""], ["Pan", "Jie", ""], ["Huang", "Huaiyang", ""], ["Shen", "Shaojie", ""], ["Liu", "Ming", ""]]}, {"id": "1904.06024", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xinchao Wang, Xiaojun Bi, Dacheng Tao", "title": "A Light Dual-Task Neural Network for Haze Removal", "comments": "6 pages, 4 figures", "journal-ref": "IEEE Signal Processing Letters, 2018, 25(8): 1231-1235", "doi": "10.1109/LSP.2018.2849681", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image dehazing is a challenging problem due to its ill-posed nature.\nExisting methods rely on a suboptimal two-step approach, where an intermediate\nproduct like a depth map is estimated, based on which the haze-free image is\nsubsequently generated using an artificial prior formula. In this paper, we\npropose a light dual-task Neural Network called LDTNet that restores the\nhaze-free image in one shot. We use transmission map estimation as an auxiliary\ntask to assist the main task, haze removal, in feature extraction and to\nenhance the generalization of the network. In LDTNet, the haze-free image and\nthe transmission map are produced simultaneously. As a result, the artificial\nprior is reduced to the smallest extent. Extensive experiments demonstrate that\nour algorithm achieves superior performance against the state-of-the-art\nmethods on both synthetic and real-world images.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:51:56 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zhang", "Yu", ""], ["Wang", "Xinchao", ""], ["Bi", "Xiaojun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06026", "submitter": "Kai Qiao", "authors": "Lingyun Jiang, Kai Qiao, Ruoxi Qin, Linyuan Wang, Jian Chen, Haibing\n  Bu, Bin Yan", "title": "Cycle-Consistent Adversarial GAN: the integration of adversarial attack\n  and defense", "comments": "13 pages,7 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification of deep learning, adversarial examples where inputs\nintended to add small magnitude perturbations may mislead deep neural networks\n(DNNs) to incorrect results, which means DNNs are vulnerable to them. Different\nattack and defense strategies have been proposed to better research the\nmechanism of deep learning. However, those research in these networks are only\nfor one aspect, either an attack or a defense, not considering that attacks and\ndefenses should be interdependent and mutually reinforcing, just like the\nrelationship between spears and shields. In this paper, we propose\nCycle-Consistent Adversarial GAN (CycleAdvGAN) to generate adversarial\nexamples, which can learn and approximate the distribution of original\ninstances and adversarial examples. For CycleAdvGAN, once the Generator and are\ntrained, can generate adversarial perturbations efficiently for any instance,\nso as to make DNNs predict wrong, and recovery adversarial examples to clean\ninstances, so as to make DNNs predict correct. We apply CycleAdvGAN under\nsemi-white box and black-box settings on two public datasets MNIST and CIFAR10.\nUsing the extensive experiments, we show that our method has achieved the\nstate-of-the-art adversarial attack method and also efficiently improve the\ndefense ability, which make the integration of adversarial attack and defense\ncome true. In additional, it has improved attack effect only trained on the\nadversarial dataset generated by any kind of adversarial attack.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 04:03:42 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Jiang", "Lingyun", ""], ["Qiao", "Kai", ""], ["Qin", "Ruoxi", ""], ["Wang", "Linyuan", ""], ["Chen", "Jian", ""], ["Bu", "Haibing", ""], ["Yan", "Bin", ""]]}, {"id": "1904.06031", "submitter": "Saurabh Singh", "authors": "Saurabh Singh, Abhinav Shrivastava", "title": "EvalNorm: Estimating Batch Normalization Statistics for Evaluation", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Batch normalization (BN) has been very effective for deep learning and is\nwidely used. However, when training with small minibatches, models using BN\nexhibit a significant degradation in performance. In this paper we study this\npeculiar behavior of BN to gain a better understanding of the problem, and\nidentify a cause. We propose 'EvalNorm' to address the issue by estimating\ncorrected normalization statistics to use for BN during evaluation. EvalNorm\nsupports online estimation of the corrected statistics while the model is being\ntrained, and does not affect the training scheme of the model. As a result,\nEvalNorm can also be used with existing pre-trained models allowing them to\nbenefit from our method. EvalNorm yields large gains for models trained with\nsmaller batches. Our experiments show that EvalNorm performs 6.18% (absolute)\nbetter than vanilla BN for a batchsize of 2 on ImageNet validation set and from\n1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across\na variety of setups.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 04:54:56 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 22:17:03 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Singh", "Saurabh", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "1904.06038", "submitter": "Ravi Shekhar", "authors": "Ravi Shekhar, Ece Takmaz, Raquel Fern\\'andez, Raffaella Bernardi", "title": "Evaluating the Representational Hub of Language and Vision Models", "comments": "Accepted to IWCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The multimodal models used in the emerging field at the intersection of\ncomputational linguistics and computer vision implement the bottom-up\nprocessing of the `Hub and Spoke' architecture proposed in cognitive science to\nrepresent how the brain processes and combines multi-sensory inputs. In\nparticular, the Hub is implemented as a neural network encoder. We investigate\nthe effect on this encoder of various vision-and-language tasks proposed in the\nliterature: visual question answering, visual reference resolution, and\nvisually grounded dialogue. To measure the quality of the representations\nlearned by the encoder, we use two kinds of analyses. First, we evaluate the\nencoder pre-trained on the different vision-and-language tasks on an existing\ndiagnostic task designed to assess multimodal semantic understanding. Second,\nwe carry out a battery of analyses aimed at studying how the encoder merges and\nexploits the two modalities.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:18:35 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Shekhar", "Ravi", ""], ["Takmaz", "Ece", ""], ["Fern\u00e1ndez", "Raquel", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1904.06040", "submitter": "Hiroki Tokunaga", "authors": "Hiroki Tokunaga, Yuki Teramoto, Akihiko Yoshizawa and Ryoma Bise", "title": "Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in\n  Pathology", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated digital histopathology image segmentation is an important task to\nhelp pathologists diagnose tumors and cancer subtypes. For pathological\ndiagnosis of cancer subtypes, pathologists usually change the magnification of\nwhole-slide images (WSI) viewers. A key assumption is that the importance of\nthe magnifications depends on the characteristics of the input image, such as\ncancer subtypes. In this paper, we propose a novel semantic segmentation\nmethod, called Adaptive-Weighting-Multi-Field-of-View-CNN (AWMF-CNN), that can\nadaptively use image features from images with different magnifications to\nsegment multiple cancer subtype regions in the input image. The proposed method\naggregates several expert CNNs for images of different magnifications by\nadaptively changing the weight of each expert depending on the input image. It\nleverages information in the images with different magnifications that might be\nuseful for identifying the subtypes. It outperformed other state-of-the-art\nmethods in experiments.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:32:33 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Tokunaga", "Hiroki", ""], ["Teramoto", "Yuki", ""], ["Yoshizawa", "Akihiko", ""], ["Bise", "Ryoma", ""]]}, {"id": "1904.06044", "submitter": "Bilal Lodhi", "authors": "Bilal Ahmed Lodhi", "title": "Unsupervised Method to Localize Masses in Mammograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the most common and prevalent type of cancer that\nmainly affects the women population. chances of effective treatment increases\nwith early diagnosis. Mammography is considered one of the effective and proven\ntechniques for early diagnosis of breast cancer. Tissues around masses look\nidentical in mammogram, which makes automatic detection process a very\nchallenging task. They are indistinguishable from the surrounding parenchyma.\nIn this paper, we present an efficient and automated approach to segment masses\nin mammograms. The proposed method uses hierarchical clustering to isolate the\nsalient area, and then features are extracted to reject false detection. We\napplied our method on two popular publicly available datasets (mini-MIAS and\nDDSM). A total of 56 images from mini-mias database, and 76 images from DDSM\nwere randomly selected. Results are explained in-terms of ROC (Receiver\nOperating Characteristics) curves and compared with the other techniques.\nExperimental results demonstrate the efficiency and advantages of the proposed\nsystem in automatic mass identification in mammograms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:50:51 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lodhi", "Bilal Ahmed", ""]]}, {"id": "1904.06062", "submitter": "Jayakorn Vongkulbhisal", "authors": "Jayakorn Vongkulbhisal, Phongtharin Vinayavekhin, Marco\n  Visentini-Scarzanella", "title": "Unifying Heterogeneous Classifiers with Distillation", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of unifying knowledge from a set of\nclassifiers with different architectures and target classes into a single\nclassifier, given only a generic set of unlabelled data. We call this problem\nUnifying Heterogeneous Classifiers (UHC). This problem is motivated by\nscenarios where data is collected from multiple sources, but the sources cannot\nshare their data, e.g., due to privacy concerns, and only privately trained\nmodels can be shared. In addition, each source may not be able to gather data\nto train all classes due to data availability at each source, and may not be\nable to train the same classification model due to different computational\nresources. To tackle this problem, we propose a generalisation of knowledge\ndistillation to merge HCs. We derive a probabilistic relation between the\noutputs of HCs and the probability over all classes. Based on this relation, we\npropose two classes of methods based on cross-entropy minimisation and matrix\nfactorisation, which allow us to estimate soft labels over all classes from\nunlabelled samples and use them in lieu of ground truth labels to train a\nunified classifier. Our extensive experiments on ImageNet, LSUN, and Places365\ndatasets show that our approaches significantly outperform a naive extension of\ndistillation and can achieve almost the same accuracy as classifiers that are\ntrained in a centralised, supervised manner.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 06:51:41 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Vongkulbhisal", "Jayakorn", ""], ["Vinayavekhin", "Phongtharin", ""], ["Visentini-Scarzanella", "Marco", ""]]}, {"id": "1904.06074", "submitter": "John Chiverton Dr", "authors": "Mahmoud Al-Faris, John P. Chiverton, Yanyan Yang and David L. Ndzi", "title": "Multi-View Region Adaptive Multi-temporal DMM and RGB Action Recognition", "comments": "14 pages, 6 figures, 13 tables. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition remains an important yet challenging task. This work\nproposes a novel action recognition system. It uses a novel Multiple View\nRegion Adaptive Multi-resolution in time Depth Motion Map (MV-RAMDMM)\nformulation combined with appearance information. Multiple stream 3D\nConvolutional Neural Networks (CNNs) are trained on the different views and\ntime resolutions of the region adaptive Depth Motion Maps. Multiple views are\nsynthesised to enhance the view invariance. The region adaptive weights, based\non localised motion, accentuate and differentiate parts of actions possessing\nfaster motion. Dedicated 3D CNN streams for multi-time resolution appearance\ninformation (RGB) are also included. These help to identify and differentiate\nbetween small object interactions. A pre-trained 3D-CNN is used here with\nfine-tuning for each stream along with multiple class Support Vector Machines\n(SVM)s. Average score fusion is used on the output. The developed approach is\ncapable of recognising both human action and human-object interaction. Three\npublic domain datasets including: MSR 3D Action,Northwestern UCLA multi-view\nactions and MSR 3D daily activity are used to evaluate the proposed solution.\nThe experimental results demonstrate the robustness of this approach compared\nwith state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 07:23:30 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Al-Faris", "Mahmoud", ""], ["Chiverton", "John P.", ""], ["Yang", "Yanyan", ""], ["Ndzi", "David L.", ""]]}, {"id": "1904.06090", "submitter": "Hamed R. Tavakoli", "authors": "Hamed R. Tavakoli, Esa Rahtu, Juho Kannala, Ali Borji", "title": "Digging Deeper into Egocentric Gaze Prediction", "comments": "presented at WACV 2019", "journal-ref": null, "doi": "10.1109/WACV.2019.00035", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper digs deeper into factors that influence egocentric gaze. Instead\nof training deep models for this purpose in a blind manner, we propose to\ninspect factors that contribute to gaze guidance during daily tasks. Bottom-up\nsaliency and optical flow are assessed versus strong spatial prior baselines.\nTask-specific cues such as vanishing point, manipulation point, and hand\nregions are analyzed as representatives of top-down information. We also look\ninto the contribution of these factors by investigating a simple recurrent\nneural model for ego-centric gaze prediction. First, deep features are\nextracted for all input video frames. Then, a gated recurrent unit is employed\nto integrate information over time and to predict the next fixation. We also\npropose an integrated model that combines the recurrent model with several\ntop-down and bottom-up cues. Extensive experiments over multiple datasets\nreveal that (1) spatial biases are strong in egocentric videos, (2) bottom-up\nsaliency models perform poorly in predicting gaze and underperform spatial\nbiases, (3) deep features perform better compared to traditional features, (4)\nas opposed to hand regions, the manipulation point is a strong influential cue\nfor gaze prediction, (5) combining the proposed recurrent model with bottom-up\ncues, vanishing points and, in particular, manipulation point results in the\nbest gaze prediction accuracy over egocentric videos, (6) the knowledge\ntransfer works best for cases where the tasks or sequences are similar, and (7)\ntask and activity recognition can benefit from gaze prediction. Our findings\nsuggest that (1) there should be more emphasis on hand-object interaction and\n(2) the egocentric vision community should consider larger datasets including\ndiverse stimuli and more subjects.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:08:12 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Tavakoli", "Hamed R.", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""], ["Borji", "Ali", ""]]}, {"id": "1904.06097", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee", "title": "Evaluating Robustness of Deep Image Super-Resolution against Adversarial\n  Attacks", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution aims to generate a high-resolution version of a\nlow-resolution image, which serves as an essential component in many computer\nvision applications. This paper investigates the robustness of deep\nlearning-based super-resolution methods against adversarial attacks, which can\nsignificantly deteriorate the super-resolved images without noticeable\ndistortion in the attacked low-resolution images. It is demonstrated that\nstate-of-the-art deep super-resolution methods are highly vulnerable to\nadversarial attacks. Different levels of robustness of different methods are\nanalyzed theoretically and experimentally. We also present analysis on\ntransferability of attacks, and feasibility of targeted attacks and universal\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:37:17 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 05:11:06 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Zhang", "Huan", ""], ["Kim", "Jun-Hyuk", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1904.06109", "submitter": "In Kyu Park", "authors": "Xiaowei Yuan, In Kyu Park", "title": "Face De-occlusion using 3D Morphable Model and Generative Adversarial\n  Network", "comments": "Presented in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent decades, 3D morphable model (3DMM) has been commonly used in\nimage-based photorealistic 3D face reconstruction. However, face images are\noften corrupted by serious occlusion by non-face objects including eyeglasses,\nmasks, and hands. Such objects block the correct capture of landmarks and\nshading information. Therefore, the reconstructed 3D face model is hardly\nreusable. In this paper, a novel method is proposed to restore de-occluded face\nimages based on inverse use of 3DMM and generative adversarial network. We\nutilize the 3DMM prior to the proposed adversarial network and combine a global\nand local adversarial convolutional neural network to learn face de-occlusion\nmodel. The 3DMM serves not only as geometric prior but also proposes the face\nregion for the local discriminator. Experiment results confirm the\neffectiveness and robustness of the proposed algorithm in removing challenging\ntypes of occlusions with various head poses and illumination. Furthermore, the\nproposed method reconstructs the correct 3D face model with de-occluded\ntextures.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:06:47 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 14:29:45 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Yuan", "Xiaowei", ""], ["Park", "In Kyu", ""]]}, {"id": "1904.06116", "submitter": "Ren\\'e Schuster", "authors": "Rohan Saxena, Ren\\'e Schuster, Oliver Wasenm\\\"uller, Didier Stricker", "title": "PWOC-3D: Deep Occlusion-Aware End-to-End Scene Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, convolutional neural networks (CNNs) have demonstrated\nincreasing success at learning many computer vision tasks including dense\nestimation problems such as optical flow and stereo matching. However, the\njoint prediction of these tasks, called scene flow, has traditionally been\ntackled using slow classical methods based on primitive assumptions which fail\nto generalize. The work presented in this paper overcomes these drawbacks\nefficiently (in terms of speed and accuracy) by proposing PWOC-3D, a compact\nCNN architecture to predict scene flow from stereo image sequences in an\nend-to-end supervised setting. Further, large motion and occlusions are\nwell-known problems in scene flow estimation. PWOC-3D employs specialized\ndesign decisions to explicitly model these challenges. In this regard, we\npropose a novel self-supervised strategy to predict occlusions from images\n(learned without any labeled occlusion data). Leveraging several such\nconstructs, our network achieves competitive results on the KITTI benchmark and\nthe challenging FlyingThings3D dataset. Especially on KITTI, PWOC-3D achieves\nthe second place among end-to-end deep learning methods with 48 times fewer\nparameters than the top-performing method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:20:51 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Saxena", "Rohan", ""], ["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.06145", "submitter": "Ari Heljakka", "authors": "Ari Heljakka, Arno Solin, Juho Kannala", "title": "Towards Photographic Image Manipulation with Balanced Growing of\n  Generative Autoencoders", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative autoencoder that provides fast encoding, faithful\nreconstructions (eg. retaining the identity of a face), sharp\ngenerated/reconstructed samples in high resolutions, and a well-structured\nlatent space that supports semantic manipulation of the inputs. There are no\ncurrent autoencoder or GAN models that satisfactorily achieve all of these. We\nbuild on the progressively growing autoencoder model PIONEER, for which we\ncompletely alter the training dynamics based on a careful analysis of recently\nintroduced normalization schemes. We show significantly improved visual and\nquantitative results for face identity conservation in CelebAHQ. Our model\nachieves state-of-the-art disentanglement of latent space, both quantitatively\nand via realistic image attribute manipulations. On the LSUN Bedrooms dataset,\nwe improve the disentanglement performance of the vanilla PIONEER, despite\nhaving a simpler model. Overall, our results indicate that the PIONEER networks\nprovide a way towards photorealistic face manipulation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 10:31:45 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 18:36:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Heljakka", "Ari", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1904.06167", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Oliver Wasenm\\\"uller, Christian Unger, Didier\n  Stricker", "title": "An Empirical Evaluation Study on the Training of SDC Features for Dense\n  Pixel Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network is a non-trivial task. Not only the tuning of\nhyperparameters, but also the gathering and selection of training data, the\ndesign of the loss function, and the construction of training schedules is\nimportant to get the most out of a model. In this study, we perform a set of\nexperiments all related to these issues. The model for which different training\nstrategies are investigated is the recently presented SDC descriptor network\n(stacked dilated convolution). It is used to describe images on pixel-level for\ndense matching tasks. Our work analyzes SDC in more detail, validates some best\npractices for training deep neural networks, and provides insights into\ntraining with multiple domain data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 11:50:07 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Unger", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.06194", "submitter": "Ze-Feng Gao", "authors": "Ze-Feng Gao, Song Cheng, Rong-Qiang He, Z. Y. Xie, Hui-Hai Zhao,\n  Zhong-Yi Lu, Tao Xiang", "title": "Compressing deep neural networks by matrix product operators", "comments": "8+9 pages, 3+7 figures, 2+11 tables", "journal-ref": "Phys. Rev. Research 2, 023300 (2020)", "doi": "10.1103/PhysRevResearch.2.023300", "report-no": null, "categories": "cs.LG cs.CV cs.NE physics.comp-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network is a parametrization of a multilayer mapping of signals\nin terms of many alternatively arranged linear and nonlinear transformations.\nThe linear transformations, which are generally used in the fully connected as\nwell as convolutional layers, contain most of the variational parameters that\nare trained and stored. Compressing a deep neural network to reduce its number\nof variational parameters but not its prediction power is an important but\nchallenging problem toward the establishment of an optimized scheme in training\nefficiently these parameters and in lowering the risk of overfitting. Here we\nshow that this problem can be effectively solved by representing linear\ntransformations with matrix product operators (MPOs), which is a tensor network\noriginally proposed in physics to characterize the short-range entanglement in\none-dimensional quantum states. We have tested this approach in five typical\nneural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\nwidely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\nrepresentation indeed sets up a faithful and efficient mapping between input\nand output signals, which can keep or even improve the prediction accuracy with\na dramatically reduced number of parameters. Our method greatly simplifies the\nrepresentations in deep learning, and opens a possible route toward\nestablishing a framework of modern neural networks which might be simpler and\ncheaper, but more efficient.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:00 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 03:26:01 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gao", "Ze-Feng", ""], ["Cheng", "Song", ""], ["He", "Rong-Qiang", ""], ["Xie", "Z. Y.", ""], ["Zhao", "Hui-Hai", ""], ["Lu", "Zhong-Yi", ""], ["Xiang", "Tao", ""]]}, {"id": "1904.06210", "submitter": "Rocio Gonzalez-Diaz", "authors": "Javier Lamar-Leon, Rocio Gonzalez-Diaz, Edel Garcia-Reyes", "title": "Topological signature for periodic motion recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.06982", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm that computes the topological\nsignature for a given periodic motion sequence. Such signature consists of a\nvector obtained by persistent homology which captures the topological and\ngeometric changes of the object that models the motion. Two topological\nsignatures are compared simply by the angle between the corresponding vectors.\nWith respect to gait recognition, we have tested our method using only the\nlowest fourth part of the body's silhouette. In this way, the impact of\nvariations in the upper part of the body, which are very frequent in real\nscenarios, decreases considerably. We have also tested our method using other\nperiodic motions such as running or jumping. Finally, we formally prove that\nour method is robust to small perturbations in the input data and does not\ndepend on the number of periods contained in the periodic motion sequence.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:52:00 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lamar-Leon", "Javier", ""], ["Gonzalez-Diaz", "Rocio", ""], ["Garcia-Reyes", "Edel", ""]]}, {"id": "1904.06213", "submitter": "David Jimenez-Cabello", "authors": "Artur Costa-Pazo, David Jimenez-Cabello, Esteban Vazquez-Fernandez,\n  Jose L. Alba-Castro and Roberto J. L\\'opez-Sastre", "title": "Generalized Presentation Attack Detection: a face anti-spoofing\n  evaluation proposal", "comments": "8 pages, to appear at International Conference on Biometrics (ICB19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past few years, Presentation Attack Detection (PAD) has become a\nfundamental part of facial recognition systems. Although much effort has been\ndevoted to anti-spoofing research, generalization in real scenarios remains a\nchallenge. In this paper we present a new open-source evaluation framework to\nstudy the generalization capacity of face PAD methods, coined here as\nface-GPAD. This framework facilitates the creation of new protocols focused on\nthe generalization problem establishing fair procedures of evaluation and\ncomparison between PAD solutions. We also introduce a large aggregated and\ncategorized dataset to address the problem of incompatibility between publicly\navailable datasets. Finally, we propose a benchmark adding two novel evaluation\nprotocols: one for measuring the effect introduced by the variations in face\nresolution, and the second for evaluating the influence of adversarial\noperating conditions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:57:54 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Costa-Pazo", "Artur", ""], ["Jimenez-Cabello", "David", ""], ["Vazquez-Fernandez", "Esteban", ""], ["Alba-Castro", "Jose L.", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1904.06236", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin, Stefan Klein, Sita M.A. Bierma-Zeinstra, J\\'er\\^ome\n  Thevenot, Esa Rahtu, Joyce van Meurs, Edwin H.G. Oei, Simo Saarakkala", "title": "Multimodal Machine Learning-based Knee Osteoarthritis Progression\n  Prediction from Plain Radiographs and Clinical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knee osteoarthritis (OA) is the most common musculoskeletal disease without a\ncure, and current treatment options are limited to symptomatic relief.\nPrediction of OA progression is a very challenging and timely issue, and it\ncould, if resolved, accelerate the disease modifying drug development and\nultimately help to prevent millions of total joint replacement surgeries\nperformed annually. Here, we present a multi-modal machine learning-based OA\nprogression prediction model that utilizes raw radiographic data, clinical\nexamination results and previous medical history of the patient. We validated\nthis approach on an independent test set of 3,918 knee images from 2,129\nsubjects. Our method yielded area under the ROC curve (AUC) of 0.79 (0.78-0.81)\nand Average Precision (AP) of 0.68 (0.66-0.70). In contrast, a reference\napproach, based on logistic regression, yielded AUC of 0.75 (0.74-0.77) and AP\nof 0.62 (0.60-0.64). The proposed method could significantly improve the\nsubject selection process for OA drug-development trials and help the\ndevelopment of personalized therapeutic plans.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 13:57:50 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 13:56:21 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Klein", "Stefan", ""], ["Bierma-Zeinstra", "Sita M. A.", ""], ["Thevenot", "J\u00e9r\u00f4me", ""], ["Rahtu", "Esa", ""], ["van Meurs", "Joyce", ""], ["Oei", "Edwin H. G.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1904.06250", "submitter": "Jiaqi Guan", "authors": "Jiaqi Guan, Ye Yuan, Kris M. Kitani, Nicholas Rhinehart", "title": "Generative Hybrid Representations for Activity Forecasting with\n  No-Regret Learning", "comments": "Oral presentation at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically reasoning about future human behaviors is a difficult problem\nbut has significant practical applications to assistive systems. Part of this\ndifficulty stems from learning systems' inability to represent all kinds of\nbehaviors. Some behaviors, such as motion, are best described with continuous\nrepresentations, whereas others, such as picking up a cup, are best described\nwith discrete representations. Furthermore, human behavior is generally not\nfixed: people can change their habits and routines. This suggests these systems\nmust be able to learn and adapt continuously. In this work, we develop an\nefficient deep generative model to jointly forecast a person's future discrete\nactions and continuous motions. On a large-scale egocentric dataset,\nEPIC-KITCHENS, we observe our method generates high-quality and diverse samples\nwhile exhibiting better generalization than related generative models. Finally,\nwe propose a variant to continually learn our model from streaming data,\nobserve its practical effectiveness, and theoretically justify its learning\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:22:37 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 18:27:39 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Guan", "Jiaqi", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris M.", ""], ["Rhinehart", "Nicholas", ""]]}, {"id": "1904.06252", "submitter": "Jingcai Guo", "authors": "Jingcai Guo, Shiheng Ma, Song Guo", "title": "MAANet: Multi-view Aware Attention Networks for Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most recent years, deep convolutional neural networks (DCNNs) based image\nsuper-resolution (SR) has gained increasing attention in multimedia and\ncomputer vision communities, focusing on restoring the high-resolution (HR)\nimage from a low-resolution (LR) image. However, one nonnegligible flaw of\nDCNNs based methods is that most of them are not able to restore\nhigh-resolution images containing sufficient high-frequency information from\nlow-resolution images with low-frequency information redundancy. Worse still,\nas the depth of DCNNs increases, the training easily encounters the problem of\nvanishing gradients, which makes the training more difficult. These problems\nhinder the effectiveness of DCNNs in image SR task. To solve these problems, we\npropose the Multi-view Aware Attention Networks (MAANet) for image SR task.\nSpecifically, we propose the local aware (LA) and global aware (GA) attention\nto deal with LR features in unequal manners, which can highlight the\nhigh-frequency components and discriminate each feature from LR images in the\nlocal and the global views, respectively. Furthermore, we propose the local\nattentive residual-dense (LARD) block, which combines the LA attention with\nmultiple residual and dense connections, to fit a deeper yet easy to train\narchitecture. The experimental results show that our proposed approach can\nachieve remarkable performance compared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:32:10 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Guo", "Jingcai", ""], ["Ma", "Shiheng", ""], ["Guo", "Song", ""]]}, {"id": "1904.06264", "submitter": "Francesco Tonolini", "authors": "Francesco Tonolini, Jack Radford, Alex Turpin, Daniele Faccio,\n  Roderick Murray-Smith", "title": "Variational Inference for Computational Imaging Inverse Problems", "comments": "29+15 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods for computational imaging require uncertainty\nestimation to be reliable in real settings. While Bayesian models offer a\ncomputationally tractable way of recovering uncertainty, they need large data\nvolumes to be trained, which in imaging applications implicates prohibitively\nexpensive collections with specific imaging instruments. This paper introduces\na novel framework to train variational inference for inverse problems\nexploiting in combination few experimentally collected data, domain expertise\nand existing image data sets. In such a way, Bayesian machine learning models\ncan solve imaging inverse problems with minimal data collection efforts.\nExtensive simulated experiments show the advantages of the proposed framework.\nThe approach is then applied to two real experimental optics settings:\nholographic image reconstruction and imaging through highly scattering media.\nIn both settings, state of the art reconstructions are achieved with little\ncollection of training data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:10:57 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 12:43:52 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 15:18:09 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Tonolini", "Francesco", ""], ["Radford", "Jack", ""], ["Turpin", "Alex", ""], ["Faccio", "Daniele", ""], ["Murray-Smith", "Roderick", ""]]}, {"id": "1904.06268", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Xin Wang, Joseph E. Gonzalez, Tom Goldstein, Larry S. Davis", "title": "ACE: Adapting to Changing Environments for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks exhibit exceptional accuracy when they are trained and\ntested on the same data distributions. However, neural classifiers are often\nextremely brittle when confronted with domain shift---changes in the input\ndistribution that occur over time. We present ACE, a framework for semantic\nsegmentation that dynamically adapts to changing environments over the time. By\naligning the distribution of labeled training data from the original source\ndomain with the distribution of incoming data in a shifted domain, ACE\nsynthesizes labeled training data for environments as it sees them. This\nstylized data is then used to update a segmentation model so that it performs\nwell in new environments. To avoid forgetting knowledge from past environments,\nwe introduce a memory that stores feature statistics from previously seen\ndomains. These statistics can be used to replay images in any of the previously\nobserved domains, thus preventing catastrophic forgetting. In addition to\nstandard batch training using stochastic gradient decent (SGD), we also\nexperiment with fast adaptation methods based on adaptive meta-learning.\nExtensive experiments are conducted on two datasets from SYNTHIA, the results\ndemonstrate the effectiveness of the proposed approach when adapting to a\nnumber of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:15:15 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wu", "Zuxuan", ""], ["Wang", "Xin", ""], ["Gonzalez", "Joseph E.", ""], ["Goldstein", "Tom", ""], ["Davis", "Larry S.", ""]]}, {"id": "1904.06281", "submitter": "Chen Chen", "authors": "Bin Sun, Chen Chen, Yingying Zhu, Jianmin Jiang", "title": "GeoCapsNet: Aerial to Ground view Image Geo-localization using Capsule\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of cross-view image geo-localization aims to determine the\ngeo-location (GPS coordinates) of a query ground-view image by matching it with\nthe GPS-tagged aerial (satellite) images in a reference dataset. Due to the\ndramatic changes of viewpoint, matching the cross-view images is challenging.\nIn this paper, we propose the GeoCapsNet based on the capsule network for\nground-to-aerial image geo-localization. The network first extracts features\nfrom both ground-view and aerial images via standard convolution layers and the\ncapsule layers further encode the features to model the spatial feature\nhierarchies and enhance the representation power. Moreover, we introduce a\nsimple and effective weighted soft-margin triplet loss with online batch hard\nsample mining, which can greatly improve image retrieval accuracy. Experimental\nresults show that our GeoCapsNet significantly outperforms the state-of-the-art\napproaches on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:34:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Sun", "Bin", ""], ["Chen", "Chen", ""], ["Zhu", "Yingying", ""], ["Jiang", "Jianmin", ""]]}, {"id": "1904.06329", "submitter": "Sheng-Yong Niu", "authors": "Sheng-Yong Niu, Lun-Zhang Guo, Yue Li, Tzung-Dau Wang, Yu Tsao,\n  Tzu-Ming Liu", "title": "Boundary-Preserved Deep Denoising of the Stochastic Resonance Enhanced\n  Multiphoton Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the rapid growth of high-speed and deep-tissue imaging in biomedical\nresearch, it is urgent to find a robust and effective denoising method to\nretain morphological features for further texture analysis and segmentation.\nConventional denoising filters and models can easily suppress perturbative\nnoises in high contrast images. However, for low photon budget multi-photon\nimages, high detector gain will not only boost signals, but also bring huge\nbackground noises. In such stochastic resonance regime of imaging,\nsub-threshold signals may be detectable with the help of noises. Therefore, a\ndenoising filter that can smartly remove noises without sacrificing the\nimportant cellular features such as cell boundaries is highly desired. In this\npaper, we propose a convolutional neural network based autoencoder method,\nFully Convolutional Deep Denoising Autoencoder (DDAE), to improve the quality\nof Three-Photon Fluorescence (3PF) and Third Harmonic Generation (THG)\nmicroscopy images. The average of the acquired 200 images of a given location\nserved as the low-noise answer for DDAE training. Compared with other widely\nused denoising methods, our DDAE model shows better signal-to-noise ratio (26.6\nand 29.9 for 3PF and THG, respectively), structure similarity (0.86 and 0.87\nfor 3PF and THG, respectively), and preservation of nuclear or cellular\nboundaries.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:17:40 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 03:00:57 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Niu", "Sheng-Yong", ""], ["Guo", "Lun-Zhang", ""], ["Li", "Yue", ""], ["Wang", "Tzung-Dau", ""], ["Tsao", "Yu", ""], ["Liu", "Tzu-Ming", ""]]}, {"id": "1904.06345", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Jean Kossaifi and Georgios Tzimiropoulos and Maja\n  Pantic", "title": "Incremental multi-domain learning with network latent tensor\n  factorization", "comments": "AAAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prominence of deep learning, large amount of annotated data and\nincreasingly powerful hardware made it possible to reach remarkable performance\nfor supervised classification tasks, in many cases saturating the training\nsets. However the resulting models are specialized to a single very specific\ntask and domain. Adapting the learned classification to new domains is a hard\nproblem due to at least three reasons: (1) the new domains and the tasks might\nbe drastically different; (2) there might be very limited amount of annotated\ndata on the new domain and (3) full training of a new model for each new task\nis prohibitive in terms of computation and memory, due to the sheer number of\nparameters of deep CNNs. In this paper, we present a method to learn\nnew-domains and tasks incrementally, building on prior knowledge from already\nlearned tasks and without catastrophic forgetting. We do so by jointly\nparametrizing weights across layers using low-rank Tucker structure. The core\nis task agnostic while a set of task specific factors are learnt on each new\ndomain. We show that leveraging tensor structure enables better performance\nthan simply using matrix operations. Joint tensor modelling also naturally\nleverages correlations across different layers. Compared with previous methods\nwhich have focused on adapting each layer separately, our approach results in\nmore compact representations for each new task/domain. We apply the proposed\nmethod to the 10 datasets of the Visual Decathlon Challenge and show that our\nmethod offers on average about 7.5x reduction in number of parameters and\ncompetitive performance in terms of both classification accuracy and Decathlon\nscore.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:57:05 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 14:04:15 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bulat", "Adrian", ""], ["Kossaifi", "Jean", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.06346", "submitter": "Yuyin Zhou", "authors": "Yuyin Zhou, Zhe Li, Song Bai, Chong Wang, Xinlei Chen, Mei Han, Elliot\n  Fishman, Alan Yuille", "title": "Prior-aware Neural Network for Partially-Supervised Multi-Organ\n  Segmentation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate multi-organ abdominal CT segmentation is essential to many clinical\napplications such as computer-aided intervention. As data annotation requires\nmassive human labor from experienced radiologists, it is common that training\ndata are partially labeled, e.g., pancreas datasets only have the pancreas\nlabeled while leaving the rest marked as background. However, these background\nlabels can be misleading in multi-organ segmentation since the \"background\"\nusually contains some other organs of interest. To address the background\nambiguity in these partially-labeled datasets, we propose Prior-aware Neural\nNetwork (PaNN) via explicitly incorporating anatomical priors on abdominal\norgan sizes, guiding the training process with domain-specific knowledge. More\nspecifically, PaNN assumes that the average organ size distributions in the\nabdomen should approximate their empirical distributions, a prior statistics\nobtained from the fully-labeled dataset. As our training objective is difficult\nto be directly optimized using stochastic gradient descent [20], we propose to\nreformulate it in a min-max form and optimize it via the stochastic primal-dual\ngradient algorithm. PaNN achieves state-of-the-art performance on the\nMICCAI2015 challenge \"Multi-Atlas Labeling Beyond the Cranial Vault\", a\ncompetition on organ segmentation in the abdomen. We report an average Dice\nscore of 84.97%, surpassing the prior art by a large margin of 3.27%.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:57:40 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 06:23:42 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zhou", "Yuyin", ""], ["Li", "Zhe", ""], ["Bai", "Song", ""], ["Wang", "Chong", ""], ["Chen", "Xinlei", ""], ["Han", "Mei", ""], ["Fishman", "Elliot", ""], ["Yuille", "Alan", ""]]}, {"id": "1904.06347", "submitter": "Anand Bhattad", "authors": "Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li and D. A. Forsyth", "title": "Unrestricted Adversarial Examples via Semantic Manipulation", "comments": "Accepted to ICLR 2020. First two authors contributed equally. Code:\n  https://github.com/aisecure/Big-but-Invisible-Adversarial-Attack and\n  Openreview: https://openreview.net/forum?id=Sye_OgHFwH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, especially deep neural networks (DNNs), have been\nshown to be vulnerable against adversarial examples which are carefully crafted\nsamples with a small magnitude of the perturbation. Such adversarial\nperturbations are usually restricted by bounding their $\\mathcal{L}_p$ norm\nsuch that they are imperceptible, and thus many current defenses can exploit\nthis property to reduce their adversarial impact. In this paper, we instead\nintroduce \"unrestricted\" perturbations that manipulate semantically meaningful\nimage-based visual descriptors - color and texture - in order to generate\neffective and photorealistic adversarial examples. We show that these\nsemantically aware perturbations are effective against JPEG compression,\nfeature squeezing and adversarially trained model. We also show that the\nproposed methods can effectively be applied to both image classification and\nimage captioning tasks on complex datasets such as ImageNet and MSCOCO. In\naddition, we conduct comprehensive user studies to show that our generated\nsemantic adversarial examples are photorealistic to humans despite large\nmagnitude perturbations when compared to other attacks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:59:30 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 17:59:15 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Bhattad", "Anand", ""], ["Chong", "Min Jin", ""], ["Liang", "Kaizhao", ""], ["Li", "Bo", ""], ["Forsyth", "D. A.", ""]]}, {"id": "1904.06373", "submitter": "Kean Chen", "authors": "Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang, Lingyu Duan,\n  Zhibo Chen, Changwei He, Junni Zou", "title": "Towards Accurate One-Stage Object Detection with AP-Loss", "comments": "13 pages, 7 figures, 4 tables, main paper + supplementary material,\n  accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-stage object detectors are trained by optimizing classification-loss and\nlocalization-loss simultaneously, with the former suffering much from extreme\nforeground-background class imbalance issue due to the large number of anchors.\nThis paper alleviates this issue by proposing a novel framework to replace the\nclassification task in one-stage detectors with a ranking task, and adopting\nthe Average-Precision loss (AP-loss) for the ranking problem. Due to its\nnon-differentiability and non-convexity, the AP-loss cannot be optimized\ndirectly. For this purpose, we develop a novel optimization algorithm, which\nseamlessly combines the error-driven update scheme in perceptron learning and\nbackpropagation algorithm in deep networks. We verify good convergence property\nof the proposed algorithm theoretically and empirically. Experimental results\ndemonstrate notable performance improvement in state-of-the-art one-stage\ndetectors based on AP-loss over different kinds of classification-losses on\nvarious benchmarks, without changing the network architectures. Code is\navailable at https://github.com/cccorn/AP-loss.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 18:58:55 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 10:04:05 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 18:23:50 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Kean", ""], ["Li", "Jianguo", ""], ["Lin", "Weiyao", ""], ["See", "John", ""], ["Wang", "Ji", ""], ["Duan", "Lingyu", ""], ["Chen", "Zhibo", ""], ["He", "Changwei", ""], ["Zou", "Junni", ""]]}, {"id": "1904.06396", "submitter": "Valentin De Bortoli", "authors": "De Bortoli Valentin, Desolneux Agn\\`es, Galerne Bruno, Leclaire Arthur", "title": "Macrocanonical Models for Texture Synthesis", "comments": "Accepted to Scale Space and Variational Methods in Computer Vision\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider macrocanonical models for texture synthesis. In\nthese models samples are generated given an input texture image and a set of\nfeatures which should be matched in expectation. It is known that if the images\nare quantized, macrocanonical models are given by Gibbs measures, using the\nmaximum entropy principle. We study conditions under which this result extends\nto real-valued images. If these conditions hold, finding a macrocanonical model\namounts to minimizing a convex function and sampling from an associated Gibbs\nmeasure. We analyze an algorithm which alternates between sampling and\nminimizing. We present experiments with neural network features and study the\ndrawbacks and advantages of using this sampling scheme.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 20:08:39 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Valentin", "De Bortoli", ""], ["Agn\u00e8s", "Desolneux", ""], ["Bruno", "Galerne", ""], ["Arthur", "Leclaire", ""]]}, {"id": "1904.06397", "submitter": "Arno Solin", "authors": "Yuxin Hou and Juho Kannala and Arno Solin", "title": "Multi-View Stereo by Temporal Nonparametric Fusion", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel idea for depth estimation from multi-view image-pose\npairs, where the model has capability to leverage information from previous\nlatent-space encodings of the scene. This model uses pairs of images and poses,\nwhich are passed through an encoder--decoder model for disparity estimation.\nThe novelty lies in soft-constraining the bottleneck layer by a nonparametric\nGaussian process prior. We propose a pose-kernel structure that encourages\nsimilar poses to have resembling latent spaces. The flexibility of the Gaussian\nprocess (GP) prior provides adapting memory for fusing information from\nprevious views. We train the encoder--decoder and the GP hyperparameters\njointly end-to-end. In addition to a batch method, we derive a lightweight\nestimation scheme that circumvents standard pitfalls in scaling Gaussian\nprocess inference, and demonstrate how our scheme can run in real-time on smart\ndevices.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 20:13:23 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 09:59:21 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Hou", "Yuxin", ""], ["Kannala", "Juho", ""], ["Solin", "Arno", ""]]}, {"id": "1904.06399", "submitter": "Leonel Merino", "authors": "Leonel Merino, Mario Hess, Alexandre Bergel, Oscar Nierstrasz, Daniel\n  Weiskopf", "title": "PerfVis: Pervasive Visualization in Immersive AugmentedReality for\n  Performance Awareness", "comments": "ICPE'19 vision, 4 pages, 2 figure, conference", "journal-ref": null, "doi": "10.1145/3302541.3313104", "report-no": null, "categories": "cs.HC cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Developers are usually unaware of the impact of code changes to the\nperformance of software systems. Although developers can analyze the\nperformance of a system by executing, for instance, a performance test to\ncompare the performance of two consecutive versions of the system, changing\nfrom a programming task to a testing task would disrupt the development flow.\nIn this paper, we propose the use of a city visualization that dynamically\nprovides developers with a pervasive view of the continuous performance of a\nsystem. We use an immersive augmented reality device (Microsoft HoloLens) to\ndisplay our visualization and extend the integrated development environment on\na computer screen to use the physical space. We report on technical details of\nthe design and implementation of our visualization tool, and discuss early\nfeedback that we collected of its usability. Our investigation explores a new\nvisual metaphor to support the exploration and analysis of possibly very large\nand multidimensional performance data. Our initial result indicates that the\ncity metaphor can be adequate to analyze dynamic performance data on a large\nand non-trivial software system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:59:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Merino", "Leonel", ""], ["Hess", "Mario", ""], ["Bergel", "Alexandre", ""], ["Nierstrasz", "Oscar", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "1904.06400", "submitter": "Jianguo Chen", "authors": "Jianguo Chen, Kenli Li, Qingying Deng, Keqin Li, Philip S. Yu", "title": "Distributed Deep Learning Model for Intelligent Video Surveillance\n  Systems with Edge Computing", "comments": "IEEE Transactions on Industrial Informatics. 2019", "journal-ref": null, "doi": "10.1109/TII.2019.2909473", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Distributed Intelligent Video Surveillance (DIVS)\nsystem using Deep Learning (DL) algorithms and deploy it in an edge computing\nenvironment. We establish a multi-layer edge computing architecture and a\ndistributed DL training model for the DIVS system. The DIVS system can migrate\ncomputing workloads from the network center to network edges to reduce huge\nnetwork communication overhead and provide low-latency and accurate video\nanalysis solutions. We implement the proposed DIVS system and address the\nproblems of parallel training, model synchronization, and workload balancing.\nTask-level parallel and model-level parallel training methods are proposed to\nfurther accelerate the video analysis process. In addition, we propose a model\nparameter updating method to achieve model synchronization of the global DL\nmodel in a distributed EC environment. Moreover, a dynamic data migration\napproach is proposed to address the imbalance of workload and computational\npower of edge nodes. Experimental results showed that the EC architecture can\nprovide elastic and scalable computing power, and the proposed DIVS system can\nefficiently handle video surveillance and analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 20:17:05 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chen", "Jianguo", ""], ["Li", "Kenli", ""], ["Deng", "Qingying", ""], ["Li", "Keqin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1904.06428", "submitter": "Valentin De Bortoli", "authors": "De Bortoli Valentin, Desolneux Agn\\`es, Galerne Bruno, Leclaire Arthur", "title": "Patch redundancy in images: a statistical testing framework and some\n  applications", "comments": "Submitted to SIIMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a statistical framework in order to analyze the\nspatial redundancy in natural images. This notion of spatial redundancy must be\ndefined locally and thus we give some examples of functions (auto-similarity\nand template similarity) which, given one or two images, computes a similarity\nmeasurement between patches. Two patches are said to be similar if the\nsimilarity measurement is small enough. To derive a criterion for taking a\ndecision on the similarity between two patches we present an a contrario model.\nNamely, two patches are said to be similar if the associated similarity\nmeasurement is unlikely to happen in a background model. Choosing Gaussian\nrandom fields as background models we derive non-asymptotic expressions for the\nprobability distribution function of similarity measurements. We introduce a\nfast algorithm in order to assess redundancy in natural images and present\napplications in denoising, periodicity analysis and texture ranking.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 21:36:14 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Valentin", "De Bortoli", ""], ["Agn\u00e8s", "Desolneux", ""], ["Bruno", "Galerne", ""], ["Arthur", "Leclaire", ""]]}, {"id": "1904.06435", "submitter": "Akinori Mitani", "authors": "Akinori Mitani, Yun Liu, Abigail Huang, Greg S. Corrado, Lily Peng,\n  Dale R. Webster, Naama Hammel, Avinash V. Varadarajan", "title": "Detecting Anemia from Retinal Fundus Images", "comments": "31 pages, 5 figures, 3 tables", "journal-ref": "Nature Biomedical Engineering (2019)", "doi": "10.1038/s41551-019-0487-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its high prevalence, anemia is often undetected due to the\ninvasiveness and cost of screening and diagnostic tests. Though some\nnon-invasive approaches have been developed, they are less accurate than\ninvasive methods, resulting in an unmet need for more accurate non-invasive\nmethods. Here, we show that deep learning-based algorithms can detect anemia\nand quantify several related blood measurements using retinal fundus images\nboth in isolation and in combination with basic metadata such as patient\ndemographics. On a validation dataset of 11,388 patients from the UK Biobank,\nour algorithms achieved a mean absolute error of 0.63 g/dL (95% confidence\ninterval (CI) 0.62-0.64) in quantifying hemoglobin concentration and an area\nunder receiver operating characteristic curve (AUC) of 0.88 (95% CI 0.86-0.89)\nin detecting anemia. This work shows the potential of automated non-invasive\nanemia screening based on fundus images, particularly in diabetic patients, who\nmay have regular retinal imaging and are at increased risk of further morbidity\nand mortality from anemia.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 22:25:48 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Mitani", "Akinori", ""], ["Liu", "Yun", ""], ["Huang", "Abigail", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""], ["Hammel", "Naama", ""], ["Varadarajan", "Avinash V.", ""]]}, {"id": "1904.06447", "submitter": "Kyle Genova", "authors": "Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T.\n  Freeman, Thomas Funkhouser", "title": "Learning Shape Templates with Structured Implicit Functions", "comments": "12 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template 3D shapes are useful for many tasks in graphics and vision,\nincluding fitting observation data, analyzing shape collections, and\ntransferring shape attributes. Because of the variety of geometry and topology\nof real-world shapes, previous methods generally use a library of hand-made\ntemplates. In this paper, we investigate learning a general shape template from\ndata. To allow for widely varying geometry and topology, we choose an implicit\nsurface representation based on composition of local shape elements. While long\nknown to computer graphics, this representation has not yet been explored in\nthe context of machine learning for vision. We show that structured implicit\nfunctions are suitable for learning and allow a network to smoothly and\nsimultaneously fit multiple classes of shapes. The learned shape template\nsupports applications such as shape exploration, correspondence, abstraction,\ninterpolation, and semantic segmentation from an RGB image.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 23:15:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Genova", "Kyle", ""], ["Cole", "Forrester", ""], ["Vlasic", "Daniel", ""], ["Sarna", "Aaron", ""], ["Freeman", "William T.", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1904.06458", "submitter": "Kyle Olszewski", "authors": "Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li and Linjie\n  Luo", "title": "Transformable Bottleneck Networks", "comments": "Project Page: https://kyleolsz.github.io/TB-Networks/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to performing fine-grained 3D manipulation of\nimage content via a convolutional neural network, which we call the\nTransformable Bottleneck Network (TBN). It applies given spatial\ntransformations directly to a volumetric bottleneck within our\nencoder-bottleneck-decoder architecture. Multi-view supervision encourages the\nnetwork to learn to spatially disentangle the feature space within the\nbottleneck. The resulting spatial structure can be manipulated with arbitrary\nspatial transformations. We demonstrate the efficacy of TBNs for novel view\nsynthesis, achieving state-of-the-art results on a challenging benchmark. We\ndemonstrate that the bottlenecks produced by networks trained for this task\ncontain meaningful spatial structure that allows us to intuitively perform a\nvariety of image manipulations in 3D, well beyond the rigid transformations\nseen during training. These manipulations include non-uniform scaling,\nnon-rigid warping, and combining content from different images. Finally, we\nextract explicit 3D structure from the bottleneck, performing impressive 3D\nreconstruction from a single input image.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 00:56:29 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 01:58:49 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 05:36:40 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2019 00:24:18 GMT"}, {"version": "v5", "created": "Mon, 26 Aug 2019 05:40:19 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Olszewski", "Kyle", ""], ["Tulyakov", "Sergey", ""], ["Woodford", "Oliver", ""], ["Li", "Hao", ""], ["Luo", "Linjie", ""]]}, {"id": "1904.06487", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell and Kate\n  Saenko", "title": "Semi-supervised Domain Adaptation via Minimax Entropy", "comments": "accepted to ICCV2019. ICCV paper version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary domain adaptation methods are very effective at aligning feature\ndistributions of source and target domains without any target supervision.\nHowever, we show that these techniques perform poorly when even a few labeled\nexamples are available in the target. To address this semi-supervised domain\nadaptation (SSDA) setting, we propose a novel Minimax Entropy (MME) approach\nthat adversarially optimizes an adaptive few-shot model. Our base model\nconsists of a feature encoding network, followed by a classification layer that\ncomputes the features' similarity to estimated prototypes (representatives of\neach class). Adaptation is achieved by alternately maximizing the conditional\nentropy of unlabeled target data with respect to the classifier and minimizing\nit with respect to the feature encoder. We empirically demonstrate the\nsuperiority of our method over many baselines, including conventional feature\nalignment and few-shot methods, setting a new state of the art for SSDA.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 05:33:46 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 02:04:32 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 21:01:04 GMT"}, {"version": "v4", "created": "Tue, 23 Jul 2019 01:46:27 GMT"}, {"version": "v5", "created": "Sat, 14 Sep 2019 18:06:46 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Saito", "Kuniaki", ""], ["Kim", "Donghyun", ""], ["Sclaroff", "Stan", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1904.06490", "submitter": "Chen Chao", "authors": "Chao Chen, Zhihang Fu, Zhihong Chen, Zhaowei Cheng, Xinyu Jin,\n  Xian-Sheng Hua", "title": "Towards Self-similarity Consistency and Feature Discrimination for\n  Unsupervised Domain Adaptation", "comments": "This paper has been submitted to ACMMMM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in unsupervised domain adaptation mainly focus on learning\nshared representations by global distribution alignment without considering\nclass information across domains. The neglect of class information, however,\nmay lead to partial alignment (or even misalignment) and poor generalization\nperformance. For comprehensive alignment, we argue that the similarities across\ndifferent features in the source domain should be consistent with that of in\nthe target domain. Based on this assumption, we propose a new domain\ndiscrepancy metric, i.e., Self-similarity Consistency (SSC), to enforce the\nfeature structure being consistent across domains. The renowned correlation\nalignment (CORAL) is proven to be a special case, and a sub-optimal measure of\nour proposed SSC. Furthermore, we also propose to mitigate the side effect of\nthe partial alignment and misalignment by incorporating the discriminative\ninformation of the deep representations. Specifically, an embarrassingly simple\nand effective feature norm constraint is exploited to enlarge the discrepancy\nof inter-class samples. It relieves the requirements of strict alignment when\nperforming adaptation, therefore improving the adaptation performance\nsignificantly. Extensive experiments on visual domain adaptation tasks\ndemonstrate the effectiveness of our proposed SSC metric and feature\ndiscrimination approach.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 06:11:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Chen", "Chao", ""], ["Fu", "Zhihang", ""], ["Chen", "Zhihong", ""], ["Cheng", "Zhaowei", ""], ["Jin", "Xinyu", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1904.06493", "submitter": "Yue Wu", "authors": "Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li,\n  Yun Fu", "title": "Rethinking Classification and Localization for Object Detection", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two head structures (i.e. fully connected head and convolution head) have\nbeen widely used in R-CNN based detectors for classification and localization\ntasks. However, there is a lack of understanding of how does these two head\nstructures work for these two tasks. To address this issue, we perform a\nthorough analysis and find an interesting fact that the two head structures\nhave opposite preferences towards the two tasks. Specifically, the fully\nconnected head (fc-head) is more suitable for the classification task, while\nthe convolution head (conv-head) is more suitable for the localization task.\nFurthermore, we examine the output feature maps of both heads and find that\nfc-head has more spatial sensitivity than conv-head. Thus, fc-head has more\ncapability to distinguish a complete object from part of an object, but is not\nrobust to regress the whole object. Based upon these findings, we propose a\nDouble-Head method, which has a fully connected head focusing on classification\nand a convolution head for bounding box regression. Without bells and whistles,\nour method gains +3.5 and +2.8 AP on MS COCO dataset from Feature Pyramid\nNetwork (FPN) baselines with ResNet-50 and ResNet-101 backbones, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 06:41:56 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 16:04:07 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 21:26:52 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2020 21:11:47 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wu", "Yue", ""], ["Chen", "Yinpeng", ""], ["Yuan", "Lu", ""], ["Liu", "Zicheng", ""], ["Wang", "Lijuan", ""], ["Li", "Hongzhi", ""], ["Fu", "Yun", ""]]}, {"id": "1904.06504", "submitter": "Vladyslav Usenko", "authors": "Vladyslav Usenko, Nikolaus Demmel, David Schubert, J\\\"org St\\\"uckler,\n  Daniel Cremers", "title": "Visual-Inertial Mapping with Non-Linear Factor Recovery", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2019.2961227", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras and inertial measurement units are complementary sensors for\nego-motion estimation and environment mapping. Their combination makes\nvisual-inertial odometry (VIO) systems more accurate and robust. For globally\nconsistent mapping, however, combining visual and inertial information is not\nstraightforward. To estimate the motion and geometry with a set of images large\nbaselines are required. Because of that, most systems operate on keyframes that\nhave large time intervals between each other. Inertial data on the other hand\nquickly degrades with the duration of the intervals and after several seconds\nof integration, it typically contains only little useful information.\n  In this paper, we propose to extract relevant information for visual-inertial\nmapping from visual-inertial odometry using non-linear factor recovery. We\nreconstruct a set of non-linear factors that make an optimal approximation of\nthe information on the trajectory accumulated by VIO. To obtain a globally\nconsistent map we combine these factors with loop-closing constraints using\nbundle adjustment. The VIO factors make the roll and pitch angles of the global\nmap observable, and improve the robustness and the accuracy of the mapping. In\nexperiments on a public benchmark, we demonstrate superior performance of our\nmethod over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 08:15:00 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 16:19:36 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 19:01:09 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Usenko", "Vladyslav", ""], ["Demmel", "Nikolaus", ""], ["Schubert", "David", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1904.06505", "submitter": "Kede Ma", "authors": "Kede Ma, Wentao Liu, Tongliang Liu, Zhou Wang, Dacheng Tao", "title": "dipIQ: Blind Image Quality Assessment by Learning-to-Rank Discriminable\n  Image Pairs", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2708503", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective assessment of image quality is fundamentally important in many\nimage processing tasks. In this work, we focus on learning blind image quality\nassessment (BIQA) models which predict the quality of a digital image with no\naccess to its original pristine-quality counterpart as reference. One of the\nbiggest challenges in learning BIQA models is the conflict between the gigantic\nimage space (which is in the dimension of the number of image pixels) and the\nextremely limited reliable ground truth data for training. Such data are\ntypically collected via subjective testing, which is cumbersome, slow, and\nexpensive. Here we first show that a vast amount of reliable training data in\nthe form of quality-discriminable image pairs (DIP) can be obtained\nautomatically at low cost by exploiting large-scale databases with diverse\nimage content. We then learn an opinion-unaware BIQA (OU-BIQA, meaning that no\nsubjective opinions are used for training) model using RankNet, a pairwise\nlearning-to-rank (L2R) algorithm, from millions of DIPs, each associated with a\nperceptual uncertainty level, leading to a DIP inferred quality (dipIQ) index.\nExtensive experiments on four benchmark IQA databases demonstrate that dipIQ\noutperforms state-of-the-art OU-BIQA models. The robustness of dipIQ is also\nsignificantly improved as confirmed by the group MAximum Differentiation (gMAD)\ncompetition method. Furthermore, we extend the proposed framework by learning\nmodels with ListNet (a listwise L2R algorithm) on quality-discriminable image\nlists (DIL). The resulting DIL Inferred Quality (dilIQ) index achieves an\nadditional performance gain.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 08:25:54 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ma", "Kede", ""], ["Liu", "Wentao", ""], ["Liu", "Tongliang", ""], ["Wang", "Zhou", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06535", "submitter": "Bi Li", "authors": "Chengquan Zhang, Borong Liang, Zuming Huang, Mengyi En, Junyu Han,\n  Errui Ding, Xinghao Ding", "title": "Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes", "comments": "Accepted by CVPR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous scene text detection methods have progressed substantially over the\npast years. However, limited by the receptive field of CNNs and the simple\nrepresentations like rectangle bounding box or quadrangle adopted to describe\ntext, previous methods may fall short when dealing with more challenging text\ninstances, such as extremely long text and arbitrarily shaped text. To address\nthese two problems, we present a novel text detector namely LOMO, which\nlocalizes the text progressively for multiple times (or in other word, LOok\nMore than Once). LOMO consists of a direct regressor (DR), an iterative\nrefinement module (IRM) and a shape expression module (SEM). At first, text\nproposals in the form of quadrangle are generated by DR branch. Next, IRM\nprogressively perceives the entire long text by iterative refinement based on\nthe extracted feature blocks of preliminary proposals. Finally, a SEM is\nintroduced to reconstruct more precise representation of irregular text by\nconsidering the geometry properties of text instance, including text region,\ntext center line and border offsets. The state-of-the-art results on several\npublic benchmarks including ICDAR2017-RCTW, SCUT-CTW1500, Total-Text, ICDAR2015\nand ICDAR17-MLT confirm the striking robustness and effectiveness of LOMO.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 12:50:24 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhang", "Chengquan", ""], ["Liang", "Borong", ""], ["Huang", "Zuming", ""], ["En", "Mengyi", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Ding", "Xinghao", ""]]}, {"id": "1904.06539", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen,\n  Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu", "title": "HAKE: Human Activity Knowledge Engine", "comments": "Work in progress. Project website: http://hake-mvig.cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity understanding is crucial for building automatic intelligent\nsystem. With the help of deep learning, activity understanding has made huge\nprogress recently. But some challenges such as imbalanced data distribution,\naction ambiguity, complex visual patterns still remain. To address these and\npromote the activity understanding, we build a large-scale Human Activity\nKnowledge Engine (HAKE) based on the human body part states. Upon existing\nactivity datasets, we annotate the part states of all the active persons in all\nimages, thus establish the relationship between instance activity and body part\nstates. Furthermore, we propose a HAKE based part state recognition model with\na knowledge extractor named Activity2Vec and a corresponding part state based\nreasoning network. With HAKE, our method can alleviate the learning difficulty\nbrought by the long-tail data distribution, and bring in interpretability. Now\nour HAKE has more than 7 M+ part state annotations and is still under\nconstruction. We first validate our approach on a part of HAKE in this\npreliminary paper, where we show 7.2 mAP performance improvement on\nHuman-Object Interaction recognition, and 12.38 mAP improvement on the one-shot\nsubsets.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 12:56:17 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 17:18:11 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 16:00:12 GMT"}, {"version": "v4", "created": "Sat, 3 Aug 2019 07:47:43 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 12:47:41 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Li", "Yong-Lu", ""], ["Xu", "Liang", ""], ["Liu", "Xinpeng", ""], ["Huang", "Xijie", ""], ["Xu", "Yue", ""], ["Chen", "Mingyang", ""], ["Ma", "Ze", ""], ["Wang", "Shiyi", ""], ["Fang", "Hao-Shu", ""], ["Lu", "Cewu", ""]]}, {"id": "1904.06554", "submitter": "Shervan Fekri-Ershad", "authors": "Laleh Armi, Shervan Fekri-Ershad", "title": "Texture image analysis and texture classification methods - A review", "comments": "29 Pages, Keywords: Texture Image, Texture Analysis, Texture\n  classification, Feature extraction, Image processing, Local Binary Patterns,\n  Benchmark texture image datasets", "journal-ref": "International Online Journal of Image Processing and Pattern\n  Recognition Vol. 2, No.1, pp. 1-29, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tactile texture refers to the tangible feel of a surface and visual texture\nrefers to see the shape or contents of the image. In the image processing, the\ntexture can be defined as a function of spatial variation of the brightness\nintensity of the pixels. Texture is the main term used to define objects or\nconcepts of a given image. Texture analysis plays an important role in computer\nvision cases such as object recognition, surface defect detection, pattern\nrecognition, medical image analysis, etc. Since now many approaches have been\nproposed to describe texture images accurately. Texture analysis methods\nusually are classified into four categories: statistical methods, structural,\nmodel-based and transform-based methods. This paper discusses the various\nmethods used for texture or analysis in details. New researches shows the power\nof combinational methods for texture analysis, which can't be in specific\ncategory. This paper provides a review on well known combinational methods in a\nspecific section with details. This paper counts advantages and disadvantages\nof well-known texture image descriptors in the result part. Main focus in all\nof the survived methods is on discrimination performance, computational\ncomplexity and resistance to challenges such as noise, rotation, etc. A brief\nreview is also made on the common classifiers used for texture image\nclassification. Also, a survey on texture image benchmark datasets is included.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 14:25:02 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Armi", "Laleh", ""], ["Fekri-Ershad", "Shervan", ""]]}, {"id": "1904.06577", "submitter": "Jon Zubizarreta", "authors": "Jon Zubizarreta, Iker Aguinaga, J. M. M. Montiel", "title": "Direct Sparse Mapping", "comments": "Accepted for publication in IEEE Transactions on Robotics", "journal-ref": null, "doi": "10.1109/TRO.2020.2991614", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric bundle adjustment, PBA, accurately estimates geometry from video.\nHowever, current PBA systems have a temporary map that cannot manage scene\nreobservations. We present, DSM, a full monocular visual SLAM based on PBA. Its\npersistent map handles reobservations, yielding the most accurate results up to\ndate on EuRoC for a direct method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 17:50:28 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 11:18:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zubizarreta", "Jon", ""], ["Aguinaga", "Iker", ""], ["Montiel", "J. M. M.", ""]]}, {"id": "1904.06585", "submitter": "Tim Oblak", "authors": "Tim Oblak, Klemen Grm, Ale\\v{s} Jakli\\v{c}, Peter Peer, Vitomir\n  \\v{S}truc, Franc Solina", "title": "Recovery of Superquadrics from Range Images using Deep Learning: A\n  Preliminary Study", "comments": null, "journal-ref": "In 2019 International Work Conference on Bioinspired Intelligence\n  (IWOBI), pp. 45-52. IEEE, 2019", "doi": "10.1109/IWOBI47054.2019.9114452", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a longstanding goal in computer vision to describe the 3D\nphysical space in terms of parameterized volumetric models that would allow\nautonomous machines to understand and interact with their surroundings. Such\nmodels are typically motivated by human visual perception and aim to represents\nall elements of the physical word ranging from individual objects to complex\nscenes using a small set of parameters. One of the de facto stadards to\napproach this problem are superquadrics - volumetric models that define various\n3D shape primitives and can be fitted to actual 3D data (either in the form of\npoint clouds or range images). However, existing solutions to superquadric\nrecovery involve costly iterative fitting procedures, which limit the\napplicability of such techniques in practice. To alleviate this problem, we\nexplore in this paper the possibility to recover superquadrics from range\nimages without time consuming iterative parameter estimation techniques by\nusing contemporary deep-learning models, more specifically, convolutional\nneural networks (CNNs). We pose the superquadric recovery problem as a\nregression task and develop a CNN regressor that is able to estimate the\nparameters of a superquadric model from a given range image. We train the\nregressor on a large set of synthetic range images, each containing a single\n(unrotated) superquadric shape and evaluate the learned model in comparaitve\nexperiments with the current state-of-the-art. Additionally, we also present a\nqualitative analysis involving a dataset of real-world objects. The results of\nour experiments show that the proposed regressor not only outperforms the\nexisting state-of-the-art, but also ensures a 270x faster execution time.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 19:01:11 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 17:01:31 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 15:22:17 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Oblak", "Tim", ""], ["Grm", "Klemen", ""], ["Jakli\u010d", "Ale\u0161", ""], ["Peer", "Peter", ""], ["\u0160truc", "Vitomir", ""], ["Solina", "Franc", ""]]}, {"id": "1904.06587", "submitter": "Feihu Zhang", "authors": "Feihu Zhang, Victor Prisacariu, Ruigang Yang, Philip H.S. Torr", "title": "GA-Net: Guided Aggregation Net for End-to-end Stereo Matching", "comments": "CVPR 2019 (Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stereo matching task, matching cost aggregation is crucial in both\ntraditional methods and deep neural network models in order to accurately\nestimate disparities. We propose two novel neural net layers, aimed at\ncapturing local and the whole-image cost dependencies respectively. The first\nis a semi-global aggregation layer which is a differentiable approximation of\nthe semi-global matching, the second is the local guided aggregation layer\nwhich follows a traditional cost filtering strategy to refine thin structures.\nThese two layers can be used to replace the widely used 3D convolutional layer\nwhich is computationally costly and memory-consuming as it has cubic\ncomputational/memory complexity. In the experiments, we show that nets with a\ntwo-layer guided aggregation block easily outperform the state-of-the-art\nGC-Net which has nineteen 3D convolutional layers. We also train a deep guided\naggregation network (GA-Net) which gets better accuracies than state-of-the-art\nmethods on both Scene Flow dataset and KITTI benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 19:30:13 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhang", "Feihu", ""], ["Prisacariu", "Victor", ""], ["Yang", "Ruigang", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1904.06593", "submitter": "Guoliang Kang", "authors": "Guoliang Kang, Jun Li, Dacheng Tao", "title": "Shakeout: A New Approach to Regularized Deep Neural Network Training", "comments": "Appears at T-PAMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the success of deep neural networks in dealing\nwith a plenty of practical problems. Dropout has played an essential role in\nmany successful deep neural networks, by inducing regularization in the model\ntraining. In this paper, we present a new regularized training approach:\nShakeout. Instead of randomly discarding units as Dropout does at the training\nstage, Shakeout randomly chooses to enhance or reverse each unit's contribution\nto the next layer. This minor modification of Dropout has the statistical\ntrait: the regularizer induced by Shakeout adaptively combines $L_0$, $L_1$ and\n$L_2$ regularization terms. Our classification experiments with representative\ndeep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that\nShakeout deals with over-fitting effectively and outperforms Dropout. We\nempirically demonstrate that Shakeout leads to sparser weights under both\nunsupervised and supervised settings. Shakeout also leads to the grouping\neffect of the input units in a layer. Considering the weights in reflecting the\nimportance of connections, Shakeout is superior to Dropout, which is valuable\nfor the deep model compression. Moreover, we demonstrate that Shakeout can\neffectively reduce the instability of the training process of the deep\narchitecture.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 21:38:16 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kang", "Guoliang", ""], ["Li", "Jun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06611", "submitter": "John Collomosse", "authors": "John Collomosse, Tu Bui, Hailin Jin", "title": "LiveSketch: Query Perturbations for Guided Sketch-based Visual Search", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiveSketch is a novel algorithm for searching large image collections using\nhand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch\nsearch by creating visual suggestions that augment the query as it is drawn,\nmaking query specification an iterative rather than one-shot process that helps\ndisambiguate users' search intent. Our technical contributions are: a triplet\nconvnet architecture that incorporates an RNN based variational autoencoder to\nsearch for images using vector (stroke-based) queries; real-time clustering to\nidentify likely search intents (and so, targets within the search embedding);\nand the use of backpropagation from those targets to perturb the input stroke\nsequence, so suggesting alterations to the query in order to guide the search.\nWe show improvements in accuracy and time-to-task over contemporary baselines\nusing a 67M image corpus.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 00:33:15 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Collomosse", "John", ""], ["Bui", "Tu", ""], ["Jin", "Hailin", ""]]}, {"id": "1904.06624", "submitter": "Jie Cao", "authors": "Jie Cao, Huaibo Huang, Yi Li, Jingtuo Liu, Ran He, Zhenan Sun", "title": "Biphasic Learning of GANs for High-Resolution Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite that the performance of image-to-image translation has been\nsignificantly improved by recent progress in generative models, current methods\nstill suffer from severe degradation in training stability and sample quality\nwhen applied to the high-resolution situation. In this work, we present a novel\ntraining framework for GANs, namely biphasic learning, to achieve\nimage-to-image translation in multiple visual domains at $1024^2$ resolution.\nOur core idea is to design an adjustable objective function that varies across\ntraining phases. Within the biphasic learning framework, we propose a novel\ninherited adversarial loss to achieve the enhancement of model capacity and\nstabilize the training phase transition. Furthermore, we introduce a\nperceptual-level consistency loss through mutual information estimation and\nmaximization. To verify the superiority of the proposed method, we apply it to\na wide range of face-related synthesis tasks and conduct experiments on\nmultiple large-scale datasets. Through comprehensive quantitative analyses, we\ndemonstrate that our method significantly outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 04:16:04 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Cao", "Jie", ""], ["Huang", "Huaibo", ""], ["Li", "Yi", ""], ["Liu", "Jingtuo", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1904.06627", "submitter": "Xun Wang", "authors": "Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, Matthew R. Scott", "title": "Multi-Similarity Loss with General Pair Weighting for Deep Metric\n  Learning", "comments": "Accepted CVPR 2019, rewrite main method to be more clear", "journal-ref": null, "doi": null, "report-no": "13 pages, 4 figures, 7 tables, including supplementary materials", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of loss functions built on pair-based computation have been proposed\nin the literature which provide a myriad of solutions for deep metric learning.\nIn this paper, we provide a general weighting framework for understanding\nrecent pair-based loss functions. Our contributions are three-fold: (1) we\nestablish a General Pair Weighting (GPW) framework, which casts the sampling\nproblem of deep metric learning into a unified view of pair weighting through\ngradient analysis, providing a powerful tool for understanding recent\npair-based loss functions; (2) we show that with GPW, various existing\npair-based methods can be compared and discussed comprehensively, with clear\ndifferences and key limitations identified; (3) we propose a new loss called\nmulti-similarity loss (MS loss) under the GPW, which is implemented in two\niterative steps (i.e., mining and weighting). This allows it to fully consider\nthree similarities for pair weighting, providing a more principled approach for\ncollecting and weighting informative pairs. Finally, the proposed MS loss\nobtains new state-of-the-art performance on four image retrieval benchmarks,\nwhere it outperforms the most recent approaches, such as\nABE\\cite{Kim_2018_ECCV} and HTL by a large margin: 60.6% to 65.7% on CUB200,\nand 80.9% to 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is\navailable at https://github.com/MalongTech/research-ms-loss.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 04:46:25 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 08:31:22 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 03:54:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Wang", "Xun", ""], ["Han", "Xintong", ""], ["Huang", "Weilin", ""], ["Dong", "Dengke", ""], ["Scott", "Matthew R.", ""]]}, {"id": "1904.06633", "submitter": "Abhishek Joshi", "authors": "Abhishek Joshi, Vinay P. Namboodiri", "title": "Unsupervised Synthesis of Anomalies in Videos: Transforming the Normal", "comments": "Accepted in IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Abnormal activity recognition requires detection of occurrence of anomalous\nevents that suffer from a severe imbalance in data. In a video, normal is used\nto describe activities that conform to usual events while the irregular events\nwhich do not conform to the normal are referred to as abnormal. It is far more\ncommon to observe normal data than to obtain abnormal data in visual\nsurveillance. In this paper, we propose an approach where we can obtain\nabnormal data by transforming normal data. This is a challenging task that is\nsolved through a multi-stage pipeline approach. We utilize a number of\ntechniques from unsupervised segmentation in order to synthesize new samples of\ndata that are transformed from an existing set of normal examples. Further,\nthis synthesis approach has useful applications as a data augmentation\ntechnique. An incrementally trained Bayesian convolutional neural network (CNN)\nis used to carefully select the set of abnormal samples that can be added.\nFinally through this synthesis approach we obtain a comparable set of abnormal\nsamples that can be used for training the CNN for the classification of normal\nvs abnormal samples. We show that this method generalizes to multiple settings\nby evaluating it on two real world datasets and achieves improved performance\nover other probabilistic techniques that have been used in the past for this\ntask.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 05:49:43 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Joshi", "Abhishek", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1904.06635", "submitter": "Zhe Xin", "authors": "Zhe Xin, Yinghao Cai, Tao Lu, Xiaoxia Xing, Shaojun Cai, Jixiang\n  Zhang, Yiping Yang, Yanqing Wang", "title": "Localizing Discriminative Visual Landmarks for Place Recognition", "comments": "7 pages, 8 figures, ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of visual place recognition with perceptual changes.\nThe fundamental problem of visual place recognition is generating robust image\nrepresentations which are not only insensitive to environmental changes but\nalso distinguishable to different places. Taking advantage of the feature\nextraction ability of Convolutional Neural Networks (CNNs), we further\ninvestigate how to localize discriminative visual landmarks that positively\ncontribute to the similarity measurement, such as buildings and vegetations. In\nparticular, a Landmark Localization Network (LLN) is designed to indicate which\nregions of an image are used for discrimination. Detailed experiments are\nconducted on open source datasets with varied appearance and viewpoint changes.\nThe proposed approach achieves superior performance against state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 06:05:54 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xin", "Zhe", ""], ["Cai", "Yinghao", ""], ["Lu", "Tao", ""], ["Xing", "Xiaoxia", ""], ["Cai", "Shaojun", ""], ["Zhang", "Jixiang", ""], ["Yang", "Yiping", ""], ["Wang", "Yanqing", ""]]}, {"id": "1904.06656", "submitter": "Na Zhang", "authors": "Na Zhang, Xuefeng Guan, Jun Cao, Xinglei Wang, Huayi Wu", "title": "A Hybrid Traffic Speed Forecasting Approach Integrating Wavelet\n  Transform and Motif-based Graph Convolutional Recurrent Neural Network", "comments": "7 pages, IJCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic forecasting is crucial for urban traffic management and guidance.\nHowever, existing methods rarely exploit the time-frequency properties of\ntraffic speed observations, and often neglect the propagation of traffic flows\nfrom upstream to downstream road segments. In this paper, we propose a hybrid\napproach that learns the spatio-temporal dependency in traffic flows and\npredicts short-term traffic speeds on a road network. Specifically, we employ\nwavelet transform to decompose raw traffic data into several components with\ndifferent frequency sub-bands. A Motif-based Graph Convolutional Recurrent\nNeural Network (Motif-GCRNN) and Auto-Regressive Moving Average (ARMA) are used\nto train and predict low-frequency components and high-frequency components,\nrespectively. In the Motif-GCRNN framework, we integrate Graph Convolutional\nNetworks (GCNs) with local sub-graph structures - Motifs - to capture the\nspatial correlations among road segments, and apply Long Short-Term Memory\n(LSTM) to extract the short-term and periodic patterns in traffic speeds.\nExperiments on a traffic dataset collected in Chengdu, China, demonstrate that\nthe proposed hybrid method outperforms six state-of-art prediction methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 08:40:58 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhang", "Na", ""], ["Guan", "Xuefeng", ""], ["Cao", "Jun", ""], ["Wang", "Xinglei", ""], ["Wu", "Huayi", ""]]}, {"id": "1904.06658", "submitter": "Santosh Vipparthi Kumar", "authors": "Monu Verma, Jaspreet Kaur Bhui, Santosh Vipparthi, Girdhari Singh", "title": "EXPERTNet Exigent Features Preservative Network for Facial Expression\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions have essential cues to infer the humans state of mind,\nthat conveys adequate information to understand individuals actual feelings.\nThus, automatic facial expression recognition is an interesting and crucial\ntask to interpret the humans cognitive state through the machine. In this\npaper, we proposed an Exigent Features Preservative Network (EXPERTNet), to\ndescribe the features of the facial expressions. The EXPERTNet extracts only\npertinent features and neglect others by using exigent feature (ExFeat) block,\nmainly comprises of elective layer. Specifically, elective layer selects the\ndesired edge variation features from the previous layer outcomes, which are\ngenerated by applying different sized filters as 1 x 1, 3 x 3, 5 x 5 and 7 x 7.\nDifferent sized filters aid to elicits both micro and high-level features that\nenhance the learnability of neurons. ExFeat block preserves the spatial\nstructural information of the facial expression, which allows to discriminate\nbetween different classes of facial expressions. Visual representation of the\nproposed method over different facial expressions shows the learning capability\nof the neurons of different layers. Experimental and comparative analysis\nresults over four comprehensive datasets CK+, MMI DISFA and GEMEP-FERA, ensures\nthe better performance of the proposed network as compared to existing\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 08:45:42 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Verma", "Monu", ""], ["Bhui", "Jaspreet Kaur", ""], ["Vipparthi", "Santosh", ""], ["Singh", "Girdhari", ""]]}, {"id": "1904.06683", "submitter": "Hiya Roy", "authors": "Hiya Roy, Subhajit Chaudhury, Toshihiko Yamasaki, Danielle DeLatte,\n  Makiko Ohtake, Tatsuaki Hashimoto", "title": "Lunar surface image restoration using U-net based deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration is a technique that reconstructs a feasible estimate of the\noriginal image from the noisy observation. In this paper, we present a U-Net\nbased deep neural network model to restore the missing pixels on the lunar\nsurface image in a context-aware fashion, which is often known as image\ninpainting problem. We use the grayscale image of the lunar surface captured by\nMultiband Imager (MI) onboard Kaguya satellite for our experiments and the\nresults show that our method can reconstruct the lunar surface image with good\nvisual quality and improved PSNR values.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 12:10:43 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Roy", "Hiya", ""], ["Chaudhury", "Subhajit", ""], ["Yamasaki", "Toshihiko", ""], ["DeLatte", "Danielle", ""], ["Ohtake", "Makiko", ""], ["Hashimoto", "Tatsuaki", ""]]}, {"id": "1904.06689", "submitter": "Zengmao Wang PhD", "authors": "Bo Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang, Dacheng Tao", "title": "Robust and Discriminative Labeling for Multi-label Active Learning Based\n  on Maximum Correntropy Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label learning draws great interests in many real world applications.\nIt is a highly costly task to assign many labels by the oracle for one\ninstance. Meanwhile, it is also hard to build a good model without diagnosing\ndiscriminative labels. Can we reduce the label costs and improve the ability to\ntrain a good model for multi-label learning simultaneously?\n  Active learning addresses the less training samples problem by querying the\nmost valuable samples to achieve a better performance with little costs. In\nmulti-label active learning, some researches have been done for querying the\nrelevant labels with less training samples or querying all labels without\ndiagnosing the discriminative information. They all cannot effectively handle\nthe outlier labels for the measurement of uncertainty. Since Maximum\nCorrentropy Criterion (MCC) provides a robust analysis for outliers in many\nmachine learning and data mining algorithms, in this paper, we derive a robust\nmulti-label active learning algorithm based on MCC by merging uncertainty and\nrepresentativeness, and propose an efficient alternating optimization method to\nsolve it. With MCC, our method can eliminate the influence of outlier labels\nthat are not discriminative to measure the uncertainty. To make further\nimprovement on the ability of information measurement, we merge uncertainty and\nrepresentativeness with the prediction labels of unknown data. It can not only\nenhance the uncertainty but also improve the similarity measurement of\nmulti-label data with labels information. Experiments on benchmark multi-label\ndata sets have shown a superior performance than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 12:53:57 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Du", "Bo", ""], ["Wang", "Zengmao", ""], ["Zhang", "Lefei", ""], ["Zhang", "Liangpei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06699", "submitter": "Yi Wei", "authors": "Yi Wei, Shaohui Liu, Wang Zhao, Jiwen Lu, Jie Zhou", "title": "Conditional Single-view Shape Generation for Multi-view Stereo\n  Reconstruction", "comments": "14 pages, 13 figures, to appear in CVPR 2019. Code and data:\n  https://github.com/weiyithu/OptimizeMVS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new perspective towards image-based shape\ngeneration. Most existing deep learning based shape reconstruction methods\nemploy a single-view deterministic model which is sometimes insufficient to\ndetermine a single groundtruth shape because the back part is occluded. In this\nwork, we first introduce a conditional generative network to model the\nuncertainty for single-view reconstruction. Then, we formulate the task of\nmulti-view reconstruction as taking the intersection of the predicted shape\nspaces on each single image. We design new differentiable guidance including\nthe front constraint, the diversity constraint, and the consistency loss to\nenable effective single-view conditional generation and multi-view synthesis.\nExperimental results and ablation studies show that our proposed approach\noutperforms state-of-the-art methods on 3D reconstruction test error and\ndemonstrate its generalization ability on real world data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 14:16:32 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 10:48:52 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Wei", "Yi", ""], ["Liu", "Shaohui", ""], ["Zhao", "Wang", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "1904.06726", "submitter": "Ya-Liang Chang", "authors": "Ya-Liang Chang, Zhe Yu Liu, Winston Hsu", "title": "VORNet: Spatio-temporally Consistent Video Inpainting for Object Removal", "comments": "Accepted to CVPRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object removal is a challenging task in video processing that often\nrequires massive human efforts. Given the mask of the foreground object in each\nframe, the goal is to complete (inpaint) the object region and generate a video\nwithout the target object. While recently deep learning based methods have\nachieved great success on the image inpainting task, they often lead to\ninconsistent results between frames when applied to videos. In this work, we\npropose a novel learning-based Video Object Removal Network (VORNet) to solve\nthe video object removal task in a spatio-temporally consistent manner, by\ncombining the optical flow warping and image-based inpainting model.\nExperiments are done on our Synthesized Video Object Removal (SVOR) dataset\nbased on the YouTube-VOS video segmentation dataset, and both the objective and\nsubjective evaluation demonstrate that our VORNet generates more spatially and\ntemporally consistent videos compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 17:12:53 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Chang", "Ya-Liang", ""], ["Liu", "Zhe Yu", ""], ["Hsu", "Winston", ""]]}, {"id": "1904.06770", "submitter": "Chang-Ryeol Lee", "authors": "Chang-Ryeol Lee, Ju Hong Yoon, Min-Gyu Park, Kuk-Jin Yoon", "title": "Gyroscope-aided Relative Pose Estimation for Rolling Shutter Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rolling shutter camera has received great attention due to its low cost\nimaging capability, however, the estimation of relative pose between rolling\nshutter cameras still remains a difficult problem owing to its line-by-line\nimage capturing characteristics. To alleviate this problem, we exploit\ngyroscope measurements, angular velocity, along with image measurement to\ncompute the relative pose between rolling shutter cameras. The gyroscope\nmeasurements provide the information about instantaneous motion that causes the\nrolling shutter distortion. Having gyroscope measurements in one hand, we\nsimplify the relative pose estimation problem and find a minimal solution for\nthe problem based on the Grobner basis polynomial solver. The proposed method\nrequires only five points to compute relative pose between rolling shutter\ncameras, whereas previous methods require 20 or 44 corresponding points for\nlinear and uniform rolling shutter geometry models, respectively. Experimental\nresults on synthetic and real data verify the superiority of the proposed\nmethod over existing relative pose estimation methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 22:07:10 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lee", "Chang-Ryeol", ""], ["Yoon", "Ju Hong", ""], ["Park", "Min-Gyu", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1904.06775", "submitter": "Yung-Hsiang Lu", "authors": "Yung-Hsiang Lu, George K. Thiruvathukal, Ahmed S. Kaseb, Kent Gauen,\n  Damini Rijhwani, Ryan Dailey, Deeptanshu Malik, Yutong Huang, Sarah\n  Aghajanzadeh, Minghao Guo", "title": "See the World through Network Cameras", "comments": "This paper is accepted by IEEE Computer for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of network cameras have been deployed worldwide. Real-time data from\nmany network cameras can offer instant views of multiple locations with\napplications in public safety, transportation management, urban planning,\nagriculture, forestry, social sciences, atmospheric information, and more. This\npaper describes the real-time data available from worldwide network cameras and\npotential applications. Second, this paper outlines the CAM2 System available\nto users at https://www.cam2project.net/. This information includes strategies\nto discover network cameras and create the camera database, user interface, and\ncomputing platforms. Third, this paper describes many opportunities provided by\ndata from network cameras and challenges to be addressed.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 22:37:51 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lu", "Yung-Hsiang", ""], ["Thiruvathukal", "George K.", ""], ["Kaseb", "Ahmed S.", ""], ["Gauen", "Kent", ""], ["Rijhwani", "Damini", ""], ["Dailey", "Ryan", ""], ["Malik", "Deeptanshu", ""], ["Huang", "Yutong", ""], ["Aghajanzadeh", "Sarah", ""], ["Guo", "Minghao", ""]]}, {"id": "1904.06805", "submitter": "Seungkwan Lee", "authors": "Seungkwan Lee, Suha Kwak, and Minsu Cho", "title": "Universal Bounding Box Regression and Its Applications", "comments": "ACCV 2018 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounding-box regression is a popular technique to refine or predict\nlocalization boxes in recent object detection approaches. Typically,\nbounding-box regressors are trained to regress from either region proposals or\nfixed anchor boxes to nearby bounding boxes of a pre-defined target object\nclasses. This paper investigates whether the technique is generalizable to\nunseen classes and is transferable to other tasks beyond supervised object\ndetection. To this end, we propose a class-agnostic and anchor-free box\nregressor, dubbed Universal Bounding-Box Regressor (UBBR), which predicts a\nbounding box of the nearest object from any given box. Trained on a relatively\nsmall set of annotated images, UBBR successfully generalizes to unseen classes,\nand can be used to improve localization in many vision problems. We demonstrate\nits effectivenss on weakly supervised object detection and object discovery.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 01:21:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lee", "Seungkwan", ""], ["Kwak", "Suha", ""], ["Cho", "Minsu", ""]]}, {"id": "1904.06807", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, Yan Yan", "title": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance\n  for Cross-View Image Translation", "comments": "20 pages, 16 figures, accepted to CVPR 2019 as an oral paper", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view image translation is challenging because it involves images with\ndrastically different views and severe deformation. In this paper, we propose a\nnovel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that\nmakes it possible to generate images of natural scenes in arbitrary viewpoints,\nbased on an image of the scene and a novel semantic map. The proposed\nSelectionGAN explicitly utilizes the semantic information and consists of two\nstages. In the first stage, the condition image and the target semantic map are\nfed into a cycled semantic-guided generation network to produce initial coarse\nresults. In the second stage, we refine the initial results by using a\nmulti-channel attention selection mechanism. Moreover, uncertainty maps\nautomatically learned from attentions are used to guide the pixel loss for\nbetter network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top\ndatasets show that our model is able to generate significantly better results\nthan the state-of-the-art methods. The source code, data and trained models are\navailable at https://github.com/Ha0Tang/SelectionGAN.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:04:15 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 20:36:07 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Wang", "Yanzhi", ""], ["Corso", "Jason J.", ""], ["Yan", "Yan", ""]]}, {"id": "1904.06823", "submitter": "Feng Xiao", "authors": "Feng Xiao, Dapeng Zhang, Gang Kou, Lu Li", "title": "Learning Spatiotemporal Features of Ride-sourcing Services with Fusion\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To collectively forecast the demand for ride-sourcing services in all regions\nof a city, the deep learning approaches have been applied with commendable\nresults. However, the local statistical differences throughout the geographical\nlayout of the city make the spatial stationarity assumption of the convolution\ninvalid, which limits the performance of CNNs on the demand forecasting task.\nIn this paper, we propose a novel deep learning framework called LC-ST-FCN\n(locally connected spatiotemporal fully-convolutional neural network) to\naddress the unique challenges of the region-level demand forecasting problem\nwithin one end-to-end architecture (E2E). We first employ the 3D convolutional\nlayers to fuse the spatial and temporal information existed in the input and\nthen feed the spatiotemporal features extracted by the 3D convolutional layers\nto the subsequent 2D convolutional layers. Afterward, the prediction value of\neach region is obtained by the locally connected convolutional layers which\nrelax the parameter sharing scheme. We evaluate the proposed model on a real\ndataset from a ride-sourcing service platform (DiDiChuxing) and observe\nsignificant improvements compared with a bunch of baseline models. Besides, we\nalso illustrate the effectiveness of our proposed model by visualizing how\ndifferent types of convolutional layers transform their input and capture\nuseful features. The visualization results show that fully convolutional\narchitecture enables the model to better localize the related regions. And the\nlocally connected layers play an important role in dealing with the local\nstatistical differences and activating useful regions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:10:45 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 08:48:30 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Xiao", "Feng", ""], ["Zhang", "Dapeng", ""], ["Kou", "Gang", ""], ["Li", "Lu", ""]]}, {"id": "1904.06827", "submitter": "Senthil Purushwalkam", "authors": "Senthil Purushwalkam, Abhinav Gupta, Danny M. Kaufman, Bryan Russell", "title": "Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces", "comments": "Accepted for publication at the International Conference on Learning\n  Representations (ICLR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to model surface properties governing bounces in\neveryday scenes. Our model learns end-to-end, starting from sensor inputs, to\npredict post-bounce trajectories and infer two underlying physical properties\nthat govern bouncing - restitution and effective collision normals. Our model,\nBounce and Learn, comprises two modules -- a Physics Inference Module (PIM) and\na Visual Inference Module (VIM). VIM learns to infer physical parameters for\nlocations in a scene given a single still image, while PIM learns to model\nphysical interactions for the prediction task given physical parameters and\nobserved pre-collision 3D trajectories. To achieve our results, we introduce\nthe Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a\nfoam ball to probe surfaces of varying shapes and materials in everyday scenes\nincluding homes and offices. Our proposed model learns from our collected\ndataset of real-world bounces and is bootstrapped with additional information\nfrom simple physics simulations. We show on our newly collected dataset that\nour model out-performs baselines, including trajectory fitting with Newtonian\nphysics, in predicting post-bounce trajectories and inferring physical\nproperties of a scene.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:31:31 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Purushwalkam", "Senthil", ""], ["Gupta", "Abhinav", ""], ["Kaufman", "Danny M.", ""], ["Russell", "Bryan", ""]]}, {"id": "1904.06830", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt, Cusuh Ham, Charles C. Kemp and James Hays", "title": "ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping and manipulating objects is an important human skill. Since\nhand-object contact is fundamental to grasping, capturing it can lead to\nimportant insights. However, observing contact through external sensors is\nchallenging because of occlusion and the complexity of the human hand. We\npresent ContactDB, a novel dataset of contact maps for household objects that\ncaptures the rich hand-object contact that occurs during grasping, enabled by\nuse of a thermal camera. Participants in our study grasped 3D printed objects\nwith a post-grasp functional intent. ContactDB includes 3750 3D meshes of 50\nhousehold objects textured with contact maps and 375K frames of synchronized\nRGB-D+thermal images. To the best of our knowledge, this is the first\nlarge-scale dataset that records detailed contact maps for human grasps.\nAnalysis of this data shows the influence of functional intent and object size\non grasping, the tendency to touch/avoid 'active areas', and the high frequency\nof palm and proximal finger contact. Finally, we train state-of-the-art image\ntranslation and 3D convolution algorithms to predict diverse contact patterns\nfrom object shape. Data, code and models are available at\nhttps://contactdb.cc.gatech.edu.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:50:16 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Ham", "Cusuh", ""], ["Kemp", "Charles C.", ""], ["Hays", "James", ""]]}, {"id": "1904.06836", "submitter": "Peihua Li", "authors": "Qilong Wang and Jiangtao Xie and Wangmeng Zuo and Lei Zhang and Peihua\n  Li", "title": "Deep CNNs Meet Global Covariance Pooling: Better Representation and\n  Generalization", "comments": "Accepted to IEEE TPAMI. Code is at http://peihuali.org/MPN-COV/", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2974833", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with global average pooling in existing deep convolutional neural\nnetworks (CNNs), global covariance pooling can capture richer statistics of\ndeep features, having potential for improving representation and generalization\nabilities of deep CNNs. However, integration of global covariance pooling into\ndeep CNNs brings two challenges: (1) robust covariance estimation given deep\nfeatures of high dimension and small sample size; (2) appropriate usage of\ngeometry of covariances. To address these challenges, we propose a global\nMatrix Power Normalized COVariance (MPN-COV) Pooling. Our MPN-COV conforms to a\nrobust covariance estimator, very suitable for scenario of high dimension and\nsmall sample size. It can also be regarded as Power-Euclidean metric between\ncovariances, effectively exploiting their geometry. Furthermore, a global\nGaussian embedding network is proposed to incorporate first-order statistics\ninto MPN-COV. For fast training of MPN-COV networks, we implement an iterative\nmatrix square root normalization, avoiding GPU unfriendly eigen-decomposition\ninherent in MPN-COV. Additionally, progressive 1x1 convolutions and group\nconvolution are introduced to compress covariance representations. The proposed\nmethods are highly modular, readily plugged into existing deep CNNs. Extensive\nexperiments are conducted on large-scale object classification, scene\ncategorization, fine-grained visual recognition and texture classification,\nshowing our methods outperform the counterparts and obtain state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 04:30:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 02:49:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Qilong", ""], ["Xie", "Jiangtao", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""], ["Li", "Peihua", ""]]}, {"id": "1904.06841", "submitter": "Zhong Li", "authors": "Zhong Li, Jinwei Ye, Yu Ji, Hao Sheng, Jingyi Yu", "title": "PIV-Based 3D Fluid Flow Reconstruction Using Light Field Camera", "comments": "This submission need to be withdrawn due to an unresolved conflict\n  between authors. This article was submitted without consent of Jinwei Ye", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Imaging Velocimetry (PIV) estimates the flow of fluid by analyzing\nthe motion of injected particles. The problem is challenging as the particles\nlie at different depths but have similar appearance and tracking a large number\nof particles is particularly difficult. In this paper, we present a PIV\nsolution that uses densely sampled light field to reconstruct and track 3D\nparticles. We exploit the refocusing capability and focal symmetry constraint\nof the light field for reliable particle depth estimation. We further propose a\nnew motion-constrained optical flow estimation scheme by enforcing local motion\nrigidity and the Navier-Stoke constraint. Comprehensive experiments on\nsynthetic and real experiments show that using a single light field camera, our\ntechnique can recover dense and accurate 3D fluid flows in small to medium\nvolumes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:00:28 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 17:54:50 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Zhong", ""], ["Ye", "Jinwei", ""], ["Ji", "Yu", ""], ["Sheng", "Hao", ""], ["Yu", "Jingyi", ""]]}, {"id": "1904.06842", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Chen Gong, Xiaolin Huang, Tao Zhou, Jie Yang, and Dacheng\n  Tao", "title": "Robust Visual Tracking Revisited: From Correlation Filter to Template\n  Matching", "comments": "has been published on IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2018.2813161", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel matching based tracker by investigating the\nrelationship between template matching and the recent popular correlation\nfilter based trackers (CFTs). Compared to the correlation operation in CFTs, a\nsophisticated similarity metric termed \"mutual buddies similarity\" (MBS) is\nproposed to exploit the relationship of multiple reciprocal nearest neighbors\nfor target matching. By doing so, our tracker obtains powerful discriminative\nability on distinguishing target and background as demonstrated by both\nempirical and theoretical analyses. Besides, instead of utilizing single\ntemplate with the improper updating scheme in CFTs, we design a novel online\ntemplate updating strategy named \"memory filtering\" (MF), which aims to select\na certain amount of representative and reliable tracking results in history to\nconstruct the current stable and expressive template set. This scheme is\nbeneficial for the proposed tracker to comprehensively \"understand\" the target\nappearance variations, \"recall\" some stable results. Both qualitative and\nquantitative evaluations on two benchmarks suggest that the proposed tracking\nmethod performs favorably against some recently developed CFTs and other\ncompetitive trackers.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:06:59 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Liu", "Fanghui", ""], ["Gong", "Chen", ""], ["Huang", "Xiaolin", ""], ["Zhou", "Tao", ""], ["Yang", "Jie", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06859", "submitter": "Deep Chakraborty", "authors": "Debasmita Ghose, Shasvat Mukeshkumar Desai, Sneha Bhattacharya, Deep\n  Chakraborty, Madalina Fiterau, Tauhidur Rahman", "title": "Pedestrian Detection in Thermal Images using Saliency Maps", "comments": "Accepted at CVPR 2019 Workshop (PBVS), 10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal images are mainly used to detect the presence of people at night or\nin bad lighting conditions, but perform poorly at daytime. To solve this\nproblem, most state-of-the-art techniques employ a fusion network that uses\nfeatures from paired thermal and color images. Instead, we propose to augment\nthermal images with their saliency maps, to serve as an attention mechanism for\nthe pedestrian detector especially during daytime. We investigate how such an\napproach results in improved performance for pedestrian detection using only\nthermal images, eliminating the need for paired color images. For our\nexperiments, we train the Faster R-CNN for pedestrian detection and report the\nadded effect of saliency maps generated using static and deep methods (PiCA-Net\nand R3-Net). Our best performing model results in an absolute reduction of miss\nrate by 13.4% and 19.4% over the baseline in day and night images respectively.\nWe also annotate and release pixel level masks of pedestrians on a subset of\nthe KAIST Multispectral Pedestrian Detection dataset, which is a first publicly\navailable dataset for salient pedestrian detection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:42:44 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ghose", "Debasmita", ""], ["Desai", "Shasvat Mukeshkumar", ""], ["Bhattacharya", "Sneha", ""], ["Chakraborty", "Deep", ""], ["Fiterau", "Madalina", ""], ["Rahman", "Tauhidur", ""]]}, {"id": "1904.06861", "submitter": "Junlong Gao", "authors": "Junlong Gao, Shiqi Wang, Shanshe Wang, Siwei Ma, Wen Gao", "title": "Self-critical n-step Training for Image Captioning", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for image captioning are usually trained by cross entropy\nloss, which leads to exposure bias and the inconsistency between the optimizing\nfunction and evaluation metrics. Recently it has been shown that these two\nissues can be addressed by incorporating techniques from reinforcement\nlearning, where one of the popular techniques is the advantage actor-critic\nalgorithm that calculates per-token advantage by estimating state value with a\nparametrized estimator at the cost of introducing estimation bias. In this\npaper, we estimate state value without using a parametrized value estimator.\nWith the properties of image captioning, namely, the deterministic state\ntransition function and the sparse reward, state value is equivalent to its\npreceding state-action value, and we reformulate advantage function by simply\nreplacing the former with the latter. Moreover, the reformulated advantage is\nextended to n-step, which can generally increase the absolute value of the mean\nof reformulated advantage while lowering variance. Then two kinds of rollout\nare adopted to estimate state-action value, which we call self-critical n-step\ntraining. Empirically we find that our method can obtain better performance\ncompared to the state-of-the-art methods that use the sequence level advantage\nand parametrized estimator respectively on the widely used MSCOCO benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:47:23 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gao", "Junlong", ""], ["Wang", "Shiqi", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "1904.06882", "submitter": "Iaroslav Melekhov", "authors": "Zakaria Laskar, Iaroslav Melekhov, Hamed R. Tavakoli, Juha Ylioinas,\n  Juho Kannala", "title": "Geometric Image Correspondence Verification by Dense Pixel Matching", "comments": "The appendix has been updated by adding some clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of determining dense pixel correspondences\nbetween two images and its application to geometric correspondence verification\nin image retrieval. The main contribution is a geometric correspondence\nverification approach for re-ranking a shortlist of retrieved database images\nbased on their dense pair-wise matching with the query image at a pixel level.\nWe determine a set of cyclically consistent dense pixel matches between the\npair of images and evaluate local similarity of matched pixels using neural\nnetwork based image descriptors. Final re-ranking is based on a novel\nsimilarity function, which fuses the local similarity metric with a global\nsimilarity metric and a geometric consistency measure computed for the matched\npixels. For dense matching our approach utilizes a modified version of a\nrecently proposed dense geometric correspondence network (DGC-Net), which we\nalso improve by optimizing the architecture. The proposed model and similarity\nmetric compare favourably to the state-of-the-art image retrieval methods. In\naddition, we apply our method to the problem of long-term visual localization\ndemonstrating promising results and generalization across datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:25:36 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 15:45:28 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 13:50:30 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Laskar", "Zakaria", ""], ["Melekhov", "Iaroslav", ""], ["Tavakoli", "Hamed R.", ""], ["Ylioinas", "Juha", ""], ["Kannala", "Juho", ""]]}, {"id": "1904.06883", "submitter": "Shuai Chen", "authors": "Shuai Chen, Jinpeng Li, Chuanqi Yao, Wenbo Hou, Shuo Qin, Wenyao Jin,\n  Xu Tang", "title": "DuBox: No-Prior Box Objection Detection via Residual Dual Scale\n  Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional neural objection detection methods use multi-scale features that\nallow multiple detectors to perform detecting tasks independently and in\nparallel. At the same time, with the handling of the prior box, the algorithm's\nability to deal with scale invariance is enhanced. However, too many prior\nboxes and independent detectors will increase the computational redundancy of\nthe detection algorithm. In this study, we introduce Dubox, a new one-stage\napproach that detects the objects without prior box. Working with multi-scale\nfeatures, the designed dual scale residual unit makes dual scale detectors no\nlonger run independently. The second scale detector learns the residual of the\nfirst. Dubox has enhanced the capacity of heuristic-guided that can further\nenable the first scale detector to maximize the detection of small targets and\nthe second to detect objects that cannot be identified by the first one.\nBesides, for each scale detector, with the new classification-regression\nprogressive strapped loss makes our process not based on prior boxes.\nIntegrating these strategies, our detection algorithm has achieved excellent\nperformance in terms of speed and accuracy. Extensive experiments on the VOC,\nCOCO object detection benchmark have confirmed the effectiveness of this\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:32:08 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 05:37:49 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Chen", "Shuai", ""], ["Li", "Jinpeng", ""], ["Yao", "Chuanqi", ""], ["Hou", "Wenbo", ""], ["Qin", "Shuo", ""], ["Jin", "Wenyao", ""], ["Tang", "Xu", ""]]}, {"id": "1904.06890", "submitter": "Johannes Stegmaier", "authors": "Dennis Eschweiler and Johannes Stegmaier", "title": "Algorithms used for the Cell Segmentation Benchmark Competition at ISBI\n  2019 by RWTH-GE", "comments": "4 pages, algorithms used for the Cell Segmentation Benchmark\n  competition at IEEE International Symposium on Biomedical Imaging (ISBI) 2019\n  in Venice, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented algorithms for segmentation and tracking follow a 3-step\napproach where we detect, track and finally segment nuclei. In the\npreprocessing phase, we detect centroids of the cell nuclei using a\nconvolutional neural network (CNN) for the 2D images and a\nLaplacian-of-Gaussian Scale Space Maximum Projection approach for the 3D data\nsets. Tracking was performed in a backwards fashion on the predicted seed\npoints, i.e., starting at the last frame and sequentially connecting\ncorresponding objects until the first frame was reached. Correspondences were\nidentified by propagating detections of a frame t to its preceding frame t-1\nand by combining redundant detections using a hierarchical clustering approach.\nThe tracked centroids were then used as input to variants of the seeded\nwatershed algorithm to obtain the final segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:45:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Eschweiler", "Dennis", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "1904.06903", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Muchen Li, Wenxiu Sun", "title": "Learning Deformable Kernels for Image and Video Denoising", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the classical denoising methods restore clear results by selecting\nand averaging pixels in the noisy input. Instead of relying on hand-crafted\nselecting and averaging strategies, we propose to explicitly learn this process\nwith deep neural networks. Specifically, we propose deformable 2D kernels for\nimage denoising where the sampling locations and kernel weights are both\nlearned. The proposed kernel naturally adapts to image structures and could\neffectively reduce the oversmoothing artifacts. Furthermore, we develop 3D\ndeformable kernels for video denoising to more efficiently sample pixels across\nthe spatial-temporal space. Our method is able to solve the misalignment issues\nof large motion from dynamic scenes. For better training our video denoising\nmodel, we introduce the trilinear sampler and a new regularization term. We\ndemonstrate that the proposed method performs favorably against the\nstate-of-the-art image and video denoising approaches on both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 08:15:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Xiangyu", ""], ["Li", "Muchen", ""], ["Sun", "Wenxiu", ""]]}, {"id": "1904.06913", "submitter": "Yiftach Ginger", "authors": "Yiftach Ginger, Dov Danon, Hadar Averbuch-Elor, Daniel Cohen-Or", "title": "Implicit Pairs for Boosting Unpaired Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": "10.1016/j.visinf.2020.10.001", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image-to-image translation the goal is to learn a mapping from one image\ndomain to another. In the case of supervised approaches the mapping is learned\nfrom paired samples. However, collecting large sets of image pairs is often\neither prohibitively expensive or not possible. As a result, in recent years\nmore attention has been given to techniques that learn the mapping from\nunpaired sets.\n  In our work, we show that injecting implicit pairs into unpaired sets\nstrengthens the mapping between the two domains, improves the compatibility of\ntheir distributions, and leads to performance boosting of unsupervised\ntechniques by over 14% across several measurements.\n  The competence of the implicit pairs is further displayed with the use of\npseudo-pairs, i.e., paired samples which only approximate a real pair. We\ndemonstrate the effect of the approximated implicit samples on image-to-image\ntranslation problems, where such pseudo-pairs may be synthesized in one\ndirection, but not in the other. We further show that pseudo-pairs are\nsignificantly more effective as implicit pairs in an unpaired setting, than\ndirectly using them explicitly in a paired setting.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 08:52:25 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 13:10:09 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 13:54:32 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 17:37:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ginger", "Yiftach", ""], ["Danon", "Dov", ""], ["Averbuch-Elor", "Hadar", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1904.06925", "submitter": "Jianlong Wu", "authors": "Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin,\n  Hongbin Zha", "title": "Deep Comprehensive Correlation Mining for Image Clustering", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developed deep unsupervised methods allow us to jointly learn\nrepresentation and cluster unlabelled data. These deep clustering methods\nmainly focus on the correlation among samples, e.g., selecting high precision\npairs to gradually tune the feature representation, which neglects other useful\ncorrelations. In this paper, we propose a novel clustering framework, named\ndeep comprehensive correlation mining(DCCM), for exploring and taking full\nadvantage of various kinds of correlations behind the unlabeled data from three\naspects: 1) Instead of only using pair-wise information, pseudo-label\nsupervision is proposed to investigate category information and learn\ndiscriminative features. 2) The features' robustness to image transformation of\ninput space is fully explored, which benefits the network learning and\nsignificantly improves the performance. 3) The triplet mutual information among\nfeatures is presented for clustering problem to lift the recently discovered\ninstance-level deep mutual information to a triplet-level formation, which\nfurther helps to learn more discriminative features. Extensive experiments on\nseveral challenging datasets show that our method achieves good performance,\ne.g., attaining $62.3\\%$ clustering accuracy on CIFAR-10, which is $10.1\\%$\nhigher than the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 09:26:58 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 02:14:10 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 06:02:44 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wu", "Jianlong", ""], ["Long", "Keyu", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Li", "Cheng", ""], ["Lin", "Zhouchen", ""], ["Zha", "Hongbin", ""]]}, {"id": "1904.06964", "submitter": "Vassili Kovalev", "authors": "Vassili Kovalev and Dmitry Voynov", "title": "Influence of Control Parameters and the Size of Biomedical Image\n  Datasets on the Success of Adversarial Attacks", "comments": "10 pages, 3 figures, 1 table, 3746 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study dependence of the success rate of adversarial attacks\nto the Deep Neural Networks on the biomedical image type, control parameters,\nand image dataset size. With this work, we are going to contribute towards\naccumulation of experimental results on adversarial attacks for the community\ndealing with biomedical images. The white-box Projected Gradient Descent\nattacks were examined based on 8 classification tasks and 13 image datasets\ncontaining a total of 605,080 chest X-ray and 317,000 histology images of\nmalignant tumors. We concluded that: (1) An increase of the amplitude of\nperturbation in generating malicious adversarial images leads to a growth of\nthe fraction of successful attacks for the majority of image types examined in\nthis study. (2) Histology images tend to be less sensitive to the growth of\namplitude of adversarial perturbations. (3) Percentage of successful attacks is\ngrowing with an increase of the number of iterations of the algorithm of\ngenerating adversarial perturbations with an asymptotic stabilization. (4) It\nwas found that the success of attacks dropping dramatically when the original\nconfidence of predicting image class exceeds 0.95. (5) The expected dependence\nof the percentage of successful attacks on the size of image training set was\nnot confirmed.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 11:07:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kovalev", "Vassili", ""], ["Voynov", "Dmitry", ""]]}, {"id": "1904.06969", "submitter": "Nikolay Burlutskiy", "authors": "Nikolay Burlutskiy, Nicolas Pinchaud, Feng Gu, Daniel H\\\"agg, Mats\n  Andersson, Lars Bj\\\"ork, Kristian Eur\\'en, Cristina Svensson, Lena Kajland\n  Wil\\'en, Martin Hedlund", "title": "Segmenting Potentially Cancerous Areas in Prostate Biopsies using\n  Semi-Automatically Annotated Data", "comments": "Accepted as oral presentation at Medical Imaging with Deep Learning\n  (MIDL) 2019, July, London, England", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gleason grading specified in ISUP 2014 is the clinical standard in staging\nprostate cancer and the most important part of the treatment decision. However,\nthe grading is subjective and suffers from high intra and inter-user\nvariability. To improve the consistency and objectivity in the grading, we\nintroduced glandular tissue WithOut Basal cells (WOB) as the ground truth. The\npresence of basal cells is the most accepted biomarker for benign glandular\ntissue and the absence of basal cells is a strong indicator of acinar prostatic\nadenocarcinoma, the most common form of prostate cancer. Glandular tissue can\nobjectively be assessed as WOB or not WOB by using specific immunostaining for\nglandular tissue (Cytokeratin 8/18) and for basal cells (Cytokeratin 5/6 +\np63). Even more, WOB allowed us to develop a semi-automated data generation\npipeline to speed up the tremendously time consuming and expensive process of\nannotating whole slide images by pathologists. We generated 295 prostatectomy\nimages exhaustively annotated with WOB. Then we used our Deep Learning\nFramework, which achieved the $2^{nd}$ best reported score in Camelyon17\nChallenge, to train networks for segmenting WOB in needle biopsies. Evaluation\nof the model on 63 needle biopsies showed promising results which were improved\nfurther by finetuning the model on 118 biopsies annotated with WOB, achieving\nF1-score of 0.80 and Precision-Recall AUC of 0.89 at the pixel-level. Then we\ncompared the performance of the model against 17 biopsies annotated\nindependently by 3 pathologists using only H\\&E staining. The comparison\ndemonstrated that the model performed on a par with the pathologists. Finally,\nthe model detected and accurately outlined existing WOB areas in two biopsies\nincorrectly annotated as totally WOB-free biopsies by three pathologists and in\none biopsy by two pathologists.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 11:18:27 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Burlutskiy", "Nikolay", ""], ["Pinchaud", "Nicolas", ""], ["Gu", "Feng", ""], ["H\u00e4gg", "Daniel", ""], ["Andersson", "Mats", ""], ["Bj\u00f6rk", "Lars", ""], ["Eur\u00e9n", "Kristian", ""], ["Svensson", "Cristina", ""], ["Wil\u00e9n", "Lena Kajland", ""], ["Hedlund", "Martin", ""]]}, {"id": "1904.06993", "submitter": "Haoyang Ye", "authors": "Haoyang Ye, Yuying Chen and Ming Liu", "title": "Tightly Coupled 3D Lidar Inertial Odometry and Mapping", "comments": "Accepted by ICRA 2019", "journal-ref": null, "doi": "10.1109/ICRA.2019.8793511", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ego-motion estimation is a fundamental requirement for most mobile robotic\napplications. By sensor fusion, we can compensate the deficiencies of\nstand-alone sensors and provide more reliable estimations. We introduce a\ntightly coupled lidar-IMU fusion method in this paper. By jointly minimizing\nthe cost derived from lidar and IMU measurements, the lidar-IMU odometry (LIO)\ncan perform well with acceptable drift after long-term experiment, even in\nchallenging cases where the lidar measurements can be degraded. Besides, to\nobtain more reliable estimations of the lidar poses, a rotation-constrained\nrefinement algorithm (LIO-mapping) is proposed to further align the lidar poses\nwith the global map. The experiment results demonstrate that the proposed\nmethod can estimate the poses of the sensor pair at the IMU update rate with\nhigh precision, even under fast motion conditions or with insufficient\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:26:12 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Ye", "Haoyang", ""], ["Chen", "Yuying", ""], ["Liu", "Ming", ""]]}, {"id": "1904.06996", "submitter": "Zihan Ye", "authors": "Zihan Ye and Fan Lyu and Linyan Li and Qiming Fu and Jinchang Ren and\n  Fuyuan Hu", "title": "SR-GAN: Semantic Rectifying Generative Adversarial Network for Zero-shot\n  Learning", "comments": "ICME 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing Zero-Shot learning (ZSL) methods may suffer from the vague class\nattributes that are highly overlapped for different classes. Unlike these\nmethods that ignore the discrimination among classes, in this paper, we propose\nto classify unseen image by rectifying the semantic space guided by the visual\nspace. First, we pre-train a Semantic Rectifying Network (SRN) to rectify\nsemantic space with a semantic loss and a rectifying loss. Then, a Semantic\nRectifying Generative Adversarial Network (SR-GAN) is built to generate\nplausible visual feature of unseen class from both semantic feature and\nrectified semantic feature. To guarantee the effectiveness of rectified\nsemantic features and synthetic visual features, a pre-reconstruction and a\npost reconstruction networks are proposed, which keep the consistency between\nvisual feature and semantic feature. Experimental results demonstrate that our\napproach significantly outperforms the state-of-the-arts on four benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:30:09 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Ye", "Zihan", ""], ["Lyu", "Fan", ""], ["Li", "Linyan", ""], ["Fu", "Qiming", ""], ["Ren", "Jinchang", ""], ["Hu", "Fuyuan", ""]]}, {"id": "1904.07002", "submitter": "Panagiotis Tzirakis", "authors": "Panagiotis Tzirakis, Athanasios Papaioannou, Alexander Lattas, Michail\n  Tarasiou, Bj\\\"orn Schuller, Stefanos Zafeiriou", "title": "Synthesising 3D Facial Motion from \"In-the-Wild\" Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesising 3D facial motion from speech is a crucial problem manifesting in\na multitude of applications such as computer games and movies. Recently\nproposed methods tackle this problem in controlled conditions of speech. In\nthis paper, we introduce the first methodology for 3D facial motion synthesis\nfrom speech captured in arbitrary recording conditions (\"in-the-wild\") and\nindependent of the speaker. For our purposes, we captured 4D sequences of\npeople uttering 500 words, contained in the Lip Reading Words (LRW) a publicly\navailable large-scale in-the-wild dataset, and built a set of 3D blendshapes\nappropriate for speech. We correlate the 3D shape parameters of the speech\nblendshapes to the LRW audio samples by means of a novel time-warping\ntechnique, named Deep Canonical Attentional Warping (DCAW), that can\nsimultaneously learn hierarchical non-linear representations and a warping path\nin an end-to-end manner. We thoroughly evaluate our proposed methods, and show\nthe ability of a deep learning model to synthesise 3D facial motion in handling\ndifferent speakers and continuous speech signals in uncontrolled conditions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:42:43 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Tzirakis", "Panagiotis", ""], ["Papaioannou", "Athanasios", ""], ["Lattas", "Alexander", ""], ["Tarasiou", "Michail", ""], ["Schuller", "Bj\u00f6rn", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1904.07073", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Felix Zhou, Adam Bailey, Barbara Braden, James East, Xin\n  Lu and Jens Rittscher", "title": "A deep learning framework for quality assessment and restoration in\n  video endoscopy", "comments": "14 pages", "journal-ref": "Medical Image Analysis, 101900(2020)", "doi": "10.1016/j.media.2020.101900", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Endoscopy is a routine imaging technique used for both diagnosis and\nminimally invasive surgical treatment. Artifacts such as motion blur, bubbles,\nspecular reflections, floating objects and pixel saturation impede the visual\ninterpretation and the automated analysis of endoscopy videos. Given the\nwidespread use of endoscopy in different clinical applications, we contend that\nthe robust and reliable identification of such artifacts and the automated\nrestoration of corrupted video frames is a fundamental medical imaging problem.\nExisting state-of-the-art methods only deal with the detection and restoration\nof selected artifacts. However, typically endoscopy videos contain numerous\nartifacts which motivates to establish a comprehensive solution.\n  We propose a fully automatic framework that can: 1) detect and classify six\ndifferent primary artifacts, 2) provide a quality score for each frame and 3)\nrestore mildly corrupted frames. To detect different artifacts our framework\nexploits fast multi-scale, single stage convolutional neural network detector.\nWe introduce a quality metric to assess frame quality and predict image\nrestoration success. Generative adversarial networks with carefully chosen\nregularization are finally used to restore corrupted frames.\n  Our detector yields the highest mean average precision (mAP at 5% threshold)\nof 49.0 and the lowest computational time of 88 ms allowing for accurate\nreal-time processing. Our restoration models for blind deblurring, saturation\ncorrection and inpainting demonstrate significant improvements over previous\nmethods. On a set of 10 test videos we show that our approach preserves an\naverage of 68.7% which is 25% more frames than that retained from the raw\nvideos.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:26:38 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ali", "Sharib", ""], ["Zhou", "Felix", ""], ["Bailey", "Adam", ""], ["Braden", "Barbara", ""], ["East", "James", ""], ["Lu", "Xin", ""], ["Rittscher", "Jens", ""]]}, {"id": "1904.07077", "submitter": "Cunxi Yu", "authors": "Cunxi Yu and Zhiru Zhang", "title": "Painting on Placement: Forecasting Routing Congestion using Conditional\n  Generative Adversarial Nets", "comments": "6 pages, 9 figures, to appear at DAC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical design process commonly consumes hours to days for large designs,\nand routing is known as the most critical step. Demands for accurate routing\nquality prediction raise to a new level to accelerate hardware innovation with\nadvanced technology nodes. This work presents an approach that forecasts the\ndensity of all routing channels over the entire floorplan, with features\ncollected up to placement, using conditional GANs. Specifically, forecasting\nthe routing congestion is constructed as an image translation (colorization)\nproblem. The proposed approach is applied to a) placement exploration for\nminimum congestion, b) constrained placement exploration and c) forecasting\ncongestion in real-time during incremental placement, using eight designs\ntargeting a fixed FPGA architecture.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:35:14 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yu", "Cunxi", ""], ["Zhang", "Zhiru", ""]]}, {"id": "1904.07080", "submitter": "Li Yang", "authors": "Mai Xu, Li Yang, Xiaoming Tao, Yiping Duan and Zulin Wang", "title": "Saliency Prediction on Omnidirectional Images with Generative\n  Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When watching omnidirectional images (ODIs), subjects can access different\nviewports by moving their heads. Therefore, it is necessary to predict\nsubjects' head fixations on ODIs. Inspired by generative adversarial imitation\nlearning (GAIL), this paper proposes a novel approach to predict saliency of\nhead fixations on ODIs, named SalGAIL. First, we establish a dataset for\nattention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset\nis large-scale, which contains the head fixations of 30 subjects viewing 600\nODIs. Next, we mine our AOI dataset and determine three findings: (1) The\nconsistency of head fixations are consistent among subjects, and it grows\nalongside the increased subject number; (2) The head fixations exist with a\nfront center bias (FCB); and (3) The magnitude of head movement is similar\nacross subjects. According to these findings, our SalGAIL approach applies deep\nreinforcement learning (DRL) to predict the head fixations of one subject, in\nwhich GAIL learns the reward of DRL, rather than the traditional human-designed\nreward. Then, multi-stream DRL is developed to yield the head fixations of\ndifferent subjects, and the saliency map of an ODI is generated via convoluting\npredicted head fixations. Finally, experiments validate the effectiveness of\nour approach in predicting saliency maps of ODIs, significantly better than 10\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:36:40 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Mai", ""], ["Yang", "Li", ""], ["Tao", "Xiaoming", ""], ["Duan", "Yiping", ""], ["Wang", "Zulin", ""]]}, {"id": "1904.07087", "submitter": "Rui Wang", "authors": "Rui Wang, Stephen M. Pizer, Jan-Michael Frahm", "title": "Recurrent Neural Network for (Un-)supervised Learning of Monocular\n  VideoVisual Odometry and Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based, single-view depth estimation methods have recently shown\nhighly promising results. However, such methods ignore one of the most\nimportant features for determining depth in the human vision system, which is\nmotion. We propose a learning-based, multi-view dense depth map and odometry\nestimation method that uses Recurrent Neural Networks (RNN) and trains\nutilizing multi-view image reprojection and forward-backward flow-consistency\nlosses. Our model can be trained in a supervised or even unsupervised mode. It\nis designed for depth and visual odometry estimation from video where the input\nframes are temporally correlated. However, it also generalizes to single-view\ndepth estimation. Our method produces superior results to the state-of-the-art\napproaches for single-view and multi-view learning-based depth estimation on\nthe KITTI driving dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:48:43 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wang", "Rui", ""], ["Pizer", "Stephen M.", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1904.07092", "submitter": "Christian Joppi", "authors": "Marco Godi, Christian Joppi, Andrea Giachetti, Marco Cristani", "title": "SIMCO: SIMilarity-based object COunting", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SIMCO, the first agnostic multi-class object counting approach.\nSIMCO starts by detecting foreground objects through a novel Mask RCNN-based\narchitecture trained beforehand (just once) on a brand-new synthetic 2D shape\ndataset, InShape; the idea is to highlight every object resembling a primitive\n2D shape (circle, square, rectangle, etc.). Each object detected is described\nby a low-dimensional embedding, obtained from a novel similarity-based head\nbranch; this latter implements a triplet loss, encouraging similar objects\n(same 2D shape + color and scale) to map close. Subsequently, SIMCO uses this\nembedding for clustering, so that different types of objects can emerge and be\ncounted, making SIMCO the very first multi-class unsupervised counter.\nExperiments show that SIMCO provides state-of-the-art scores on counting\nbenchmarks and that it can also help in many challenging image understanding\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:52:31 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 08:16:53 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Godi", "Marco", ""], ["Joppi", "Christian", ""], ["Giachetti", "Andrea", ""], ["Cristani", "Marco", ""]]}, {"id": "1904.07099", "submitter": "Alasdair Newson", "authors": "Alasdair Newson, Andr\\'es Almansa, Yann Gousseau, Sa\\\"id Ladjal", "title": "Processsing Simple Geometric Attributes with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis is a core problem in modern deep learning, and many recent\narchitectures such as autoencoders and Generative Adversarial networks produce\nspectacular results on highly complex data, such as images of faces or\nlandscapes. While these results open up a wide range of new, advanced synthesis\napplications, there is also a severe lack of theoretical understanding of how\nthese networks work. This results in a wide range of practical problems, such\nas difficulties in training, the tendency to sample images with little or no\nvariability, and generalisation problems. In this paper, we propose to analyse\nthe ability of the simplest generative network, the autoencoder, to encode and\ndecode two simple geometric attributes : size and position. We believe that, in\norder to understand more complicated tasks, it is necessary to first understand\nhow these networks process simple attributes. For the first property, we\nanalyse the case of images of centred disks with variable radii. We explain how\nthe autoencoder projects these images to and from a latent space of smallest\npossible dimension, a scalar. In particular, we describe a closed-form solution\nto the decoding training problem in a network without biases, and show that\nduring training, the network indeed finds this solution. We then investigate\nthe best regularisation approaches which yield networks that generalise well.\nFor the second property, position, we look at the encoding and decoding of\nDirac delta functions, also known as `one-hot' vectors. We describe a\nhand-crafted filter that achieves encoding perfectly, and show that the network\nnaturally finds this filter during training. We also show experimentally that\nthe decoding can be achieved if the dataset is sampled in an appropriate\nmanner.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:01:27 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Newson", "Alasdair", ""], ["Almansa", "Andr\u00e9s", ""], ["Gousseau", "Yann", ""], ["Ladjal", "Sa\u00efd", ""]]}, {"id": "1904.07165", "submitter": "Fethiye Irmak Do\\u{g}an", "authors": "Fethiye Irmak Do\\u{g}an, Sinan Kalkan and Iolanda Leite", "title": "Learning to Generate Unambiguous Spatial Referring Expressions for\n  Real-World Environments", "comments": "International Conference on Intelligent Robots and Systems (IROS\n  2019), Demo 1: Finding the described object (https://youtu.be/BE6-F6chW0w),\n  Demo 2: Referring to the pointed object (https://youtu.be/nmmv6JUpy8M),\n  Supplementary Video (https://youtu.be/sFjBa_MHS98)", "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (2019) 4992-4999", "doi": "10.1109/IROS40897.2019.8968510", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Referring to objects in a natural and unambiguous manner is crucial for\neffective human-robot interaction. Previous research on learning-based\nreferring expressions has focused primarily on comprehension tasks, while\ngenerating referring expressions is still mostly limited to rule-based methods.\nIn this work, we propose a two-stage approach that relies on deep learning for\nestimating spatial relations to describe an object naturally and unambiguously\nwith a referring expression. We compare our method to the state of the art\nalgorithm in ambiguous environments (e.g., environments that include very\nsimilar objects with similar relationships). We show that our method generates\nreferring expressions that people find to be more accurate ($\\sim$30% better)\nand would prefer to use ($\\sim$32% more often).\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:25:49 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 14:48:16 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 12:24:02 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 09:46:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Do\u011fan", "Fethiye Irmak", ""], ["Kalkan", "Sinan", ""], ["Leite", "Iolanda", ""]]}, {"id": "1904.07172", "submitter": "Jan Eric Lenssen", "authors": "Jan Eric Lenssen, Christian Osendorfer, Jonathan Masci", "title": "Deep Iterative Surface Normal Estimation", "comments": "Presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end differentiable algorithm for robust and\ndetail-preserving surface normal estimation on unstructured point-clouds. We\nutilize graph neural networks to iteratively parameterize an adaptive\nanisotropic kernel that produces point weights for weighted least-squares plane\nfitting in local neighborhoods. The approach retains the interpretability and\nefficiency of traditional sequential plane fitting while benefiting from\nadaptation to data set statistics through deep learning. This results in a\nstate-of-the-art surface normal estimator that is robust to noise, outliers and\npoint density variation, preserves sharp features through anisotropic kernels\nand equivariance through a local quaternion-based spatial transformer. Contrary\nto previous deep learning methods, the proposed approach does not require any\nhand-crafted features or preprocessing. It improves on the state-of-the-art\nresults while being more than two orders of magnitude faster and more parameter\nefficient.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:40:38 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:51:28 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 13:01:36 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lenssen", "Jan Eric", ""], ["Osendorfer", "Christian", ""], ["Masci", "Jonathan", ""]]}, {"id": "1904.07190", "submitter": "Arun Mukundan", "authors": "Arun Mukundan, Giorgos Tolias, Ondrej Chum", "title": "Explicit Spatial Encoding for Deep Local Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a kernelized deep local-patch descriptor based on efficient match\nkernels of neural network activations. Response of each receptive field is\nencoded together with its spatial location using explicit feature maps. Two\nlocation parametrizations, Cartesian and polar, are used to provide robustness\nto a different types of canonical patch misalignment. Additionally, we analyze\nhow the conventional architecture, i.e. a fully connected layer attached after\nthe convolutional part, encodes responses in a spatially variant way. In\ncontrary, explicit spatial encoding is used in our descriptor, whose potential\napplications are not limited to local-patches. We evaluate the descriptor on\nstandard benchmarks. Both versions, encoding 32x32 or 64x64 patches,\nconsistently outperform all other methods on all benchmarks. The number of\nparameters of the model is independent of the input patch resolution.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:03:50 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Mukundan", "Arun", ""], ["Tolias", "Giorgos", ""], ["Chum", "Ondrej", ""]]}, {"id": "1904.07220", "submitter": "Goutam Bhat", "authors": "Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte", "title": "Learning Discriminative Model Prediction for Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current strive towards end-to-end trainable computer vision systems\nimposes major challenges for the task of visual tracking. In contrast to most\nother vision problems, tracking requires the learning of a robust\ntarget-specific appearance model online, during the inference stage. To be\nend-to-end trainable, the online learning of the target model thus needs to be\nembedded in the tracking architecture itself. Due to the imposed challenges,\nthe popular Siamese paradigm simply predicts a target feature template, while\nignoring the background appearance information during inference. Consequently,\nthe predicted model possesses limited target-background discriminability.\n  We develop an end-to-end tracking architecture, capable of fully exploiting\nboth target and background appearance information for target model prediction.\nOur architecture is derived from a discriminative learning loss by designing a\ndedicated optimization process that is capable of predicting a powerful model\nin only a few iterations. Furthermore, our approach is able to learn key\naspects of the discriminative loss itself. The proposed tracker sets a new\nstate-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on\nVOT2018, while running at over 40 FPS. The code and models are available at\nhttps://github.com/visionml/pytracking.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:57:09 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:31:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bhat", "Goutam", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "1904.07223", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, Jan\n  Kautz", "title": "Joint Discriminative and Generative Learning for Person\n  Re-identification", "comments": "CVPR 2019 (Oral). Add transfer learning results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) remains challenging due to significant\nintra-class variations across different cameras. Recently, there has been a\ngrowing interest in using generative models to augment training data and\nenhance the invariance to input changes. The generative pipelines in existing\nmethods, however, stay relatively separate from the discriminative re-id\nlearning stages. Accordingly, re-id models are often trained in a\nstraightforward manner on the generated data. In this paper, we seek to improve\nlearned re-id embeddings by better leveraging the generated data. To this end,\nwe propose a joint learning framework that couples re-id learning and data\ngeneration end-to-end. Our model involves a generative module that separately\nencodes each person into an appearance code and a structure code, and a\ndiscriminative module that shares the appearance encoder with the generative\nmodule. By switching the appearance or structure codes, the generative module\nis able to generate high-quality cross-id composed images, which are online fed\nback to the appearance encoder and used to improve the discriminative module.\nThe proposed joint learning framework renders significant improvement over the\nbaseline without using generated data, leading to the state-of-the-art\nperformance on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:59:43 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 10:33:00 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 06:36:47 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zheng", "Zhedong", ""], ["Yang", "Xiaodong", ""], ["Yu", "Zhiding", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""], ["Kautz", "Jan", ""]]}, {"id": "1904.07233", "submitter": "Shreetam Behera", "authors": "Shreetam Behera, Debi Prosad Dogra, Malay Kumar Bandyopadhyay and\n  Partha Pratim Roy", "title": "Estimation of Linear Motion in Dense Crowd Videos using Langevin Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd gatherings at social and cultural events are increasing in leaps and\nbounds with the increase in population. Surveillance through computer vision\nand expert decision making systems can help to understand the crowd phenomena\nat large gatherings. Understanding crowd phenomena can be helpful in early\nidentification of unwanted incidents and their prevention. Motion flow is one\nof the important crowd phenomena that can be instrumental in describing the\ncrowd behavior. Flows can be useful in understanding instabilities in the\ncrowd. However, extracting motion flows is a challenging task due to randomness\nin crowd movement and limitations of the sensing device. Moreover, low-level\nfeatures such as optical flow can be misleading if the randomness is high. In\nthis paper, we propose a new model based on Langevin equation to analyze the\nlinear dominant flows in videos of densely crowded scenarios. We assume a force\nmodel with three components, namely external force, confinement/drift force,\nand disturbance force. These forces are found to be sufficient to describe the\nlinear or near-linear motion in dense crowd videos. The method is significantly\nfaster as compared to existing popular crowd segmentation methods. The\nevaluation of the proposed model has been carried out on publicly available\ndatasets as well as using our dataset. It has been observed that the proposed\nmethod is able to estimate and segment the linear flows in the dense crowd with\nbetter accuracy as compared to state-of-the-art techniques with substantial\ndecrease in the computational overhead.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 09:17:16 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Behera", "Shreetam", ""], ["Dogra", "Debi Prosad", ""], ["Bandyopadhyay", "Malay Kumar", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1904.07235", "submitter": "Guillermo Gallego", "authors": "Guillermo Gallego, Mathias Gehrig, Davide Scaramuzza", "title": "Focus Is All You Need: Loss Functions For Event-based Vision", "comments": "29 pages, 19 figures, 4 tables", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Long Beach, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel vision sensors that output pixel-level brightness\nchanges (\"events\") instead of traditional video frames. These asynchronous\nsensors offer several advantages over traditional cameras, such as, high\ntemporal resolution, very high dynamic range, and no motion blur. To unlock the\npotential of such sensors, motion compensation methods have been recently\nproposed. We present a collection and taxonomy of twenty two objective\nfunctions to analyze event alignment in motion compensation approaches (Fig.\n1). We call them Focus Loss Functions since they have strong connections with\nfunctions used in traditional shape-from-focus applications. The proposed loss\nfunctions allow bringing mature computer vision tools to the realm of event\ncameras. We compare the accuracy and runtime performance of all loss functions\non a publicly available dataset, and conclude that the variance, the gradient\nand the Laplacian magnitudes are among the best loss functions. The\napplicability of the loss functions is shown on multiple tasks: rotational\nmotion, depth and optical flow estimation. The proposed focus loss functions\nallow to unlock the outstanding properties of event cameras.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:40:56 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Gallego", "Guillermo", ""], ["Gehrig", "Mathias", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.07282", "submitter": "Hongming Li", "authors": "Hongming Li, Mohamad Habes, David A. Wolk, Yong Fan", "title": "A deep learning model for early prediction of Alzheimer's disease\n  dementia based on hippocampal MRI", "comments": "Accepted for publication in Alzheimer's & Dementia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introduction: It is challenging at baseline to predict when and which\nindividuals who meet criteria for mild cognitive impairment (MCI) will\nultimately progress to Alzheimer's disease (AD) dementia. Methods: A deep\nlearning method is developed and validated based on MRI scans of 2146 subjects\n(803 for training and 1343 for validation) to predict MCI subjects' progression\nto AD dementia in a time-to-event analysis setting. Results: The deep learning\ntime-to-event model predicted individual subjects' progression to AD dementia\nwith a concordance index (C-index) of 0.762 on 439 ADNI testing MCI subjects\nwith follow-up duration from 6 to 78 months (quartiles: [24, 42, 54]) and a\nC-index of 0.781 on 40 AIBL testing MCI subjects with follow-up duration from\n18-54 months (quartiles: [18, 36,54]). The predicted progression risk also\nclustered individual subjects into subgroups with significant differences in\ntheir progression time to AD dementia (p<0.0002). Improved performance for\npredicting progression to AD dementia (C-index=0.864) was obtained when the\ndeep learning based progression risk was combined with baseline clinical\nmeasures. Conclusion: Our method provides a cost effective and accurate means\nfor prognosis and potentially to facilitate enrollment in clinical trials with\nindividuals likely to progress within a specific temporal period.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 18:37:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Li", "Hongming", ""], ["Habes", "Mohamad", ""], ["Wolk", "David A.", ""], ["Fan", "Yong", ""]]}, {"id": "1904.07290", "submitter": "Mingchen Gao", "authors": "Yan Shen, Mingchen Gao", "title": "Brain Tumor Segmentation on MRI with Missing Modalities", "comments": "Will appear in IPMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain Tumor Segmentation from magnetic resonance imaging (MRI) is a critical\ntechnique for early diagnosis. However, rather than having complete four\nmodalities as in BraTS dataset, it is common to have missing modalities in\nclinical scenarios. We design a brain tumor segmentation algorithm that is\nrobust to the absence of any modality. Our network includes a\nchannel-independent encoding path and a feature-fusion decoding path. We use\nself-supervised training through channel dropout and also propose a novel\ndomain adaptation method on feature maps to recover the information from the\nmissing channel. Our results demonstrate that the quality of the segmentation\ndepends on which modality is missing. Furthermore, we also discuss and\nvisualize the contribution of each modality to the segmentation results. Their\ncontributions are along well with the expert screening routine.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:00:24 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Shen", "Yan", ""], ["Gao", "Mingchen", ""]]}, {"id": "1904.07302", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Fran\\c{c}ois\n  Petitjean, Lhassane Idoumghar, Pierre-Alain Muller", "title": "Automatic alignment of surgical videos using kinematic data", "comments": "Accepted at AIME 2019", "journal-ref": null, "doi": "10.1007/978-3-030-21642-9_14", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past one hundred years, the classic teaching methodology of \"see\none, do one, teach one\" has governed the surgical education systems worldwide.\nWith the advent of Operation Room 2.0, recording video, kinematic and many\nother types of data during the surgery became an easy task, thus allowing\nartificial intelligence systems to be deployed and used in surgical and medical\npractice. Recently, surgical videos has been shown to provide a structure for\npeer coaching enabling novice trainees to learn from experienced surgeons by\nreplaying those videos. However, the high inter-operator variability in\nsurgical gesture duration and execution renders learning from comparing novice\nto expert surgical videos a very difficult task. In this paper, we propose a\nnovel technique to align multiple videos based on the alignment of their\ncorresponding kinematic multivariate time series data. By leveraging the\nDynamic Time Warping measure, our algorithm synchronizes a set of videos in\norder to show the same gesture being performed at different speed. We believe\nthat the proposed approach is a valuable addition to the existing learning\ntools for surgery.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:46:08 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 12:27:13 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Petitjean", "Fran\u00e7ois", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1904.07304", "submitter": "Zhen Zhao", "authors": "Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, and K. P.\n  Unnikrishnan", "title": "Fast Inference in Capsule Networks Using Accumulated Routing\n  Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for fast inference in Capsule Networks (CapsNets) by\ntaking advantage of a key insight regarding the routing coefficients that link\ncapsules between adjacent network layers. Since the routing coefficients are\nresponsible for assigning object parts to wholes, and an object whole generally\ncontains similar intra-class and dissimilar inter-class parts, the routing\ncoefficients tend to form a unique signature for each object class. For fast\ninference, a network is first trained in the usual manner using examples from\nthe training dataset. Afterward, the routing coefficients associated with the\ntraining examples are accumulated offline and used to create a set of \"master\"\nrouting coefficients. During inference, these master routing coefficients are\nused in place of the dynamically calculated routing coefficients. Our method\neffectively replaces the for-loop iterations in the dynamic routing procedure\nwith a single matrix multiply operation, providing a significant boost in\ninference speed. Compared with the dynamic routing procedure, fast inference\ndecreases the test accuracy for the MNIST, Background MNIST, Fashion MNIST, and\nRotated MNIST datasets by less than 0.5% and by approximately 5% for CIFAR10.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:44:52 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhao", "Zhen", ""], ["Kleinhans", "Ashley", ""], ["Sandhu", "Gursharan", ""], ["Patel", "Ishan", ""], ["Unnikrishnan", "K. P.", ""]]}, {"id": "1904.07305", "submitter": "Aruni RoyChowdhury", "authors": "Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung\n  Jin, Huaizu Jiang, Liangliang Cao and Erik Learned-Miller", "title": "Automatic adaptation of object detectors to new domains using\n  self-training", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the unsupervised adaptation of an existing object\ndetector to a new target domain. We assume that a large number of unlabeled\nvideos from this domain are readily available. We automatically obtain labels\non the target data by using high-confidence detections from the existing\ndetector, augmented with hard (misclassified) examples acquired by exploiting\ntemporal cues using a tracker. These automatically-obtained labels are then\nused for re-training the original model. A modified knowledge distillation loss\nis proposed, and we investigate several ways of assigning soft-labels to the\ntraining examples from the target domain. Our approach is empirically evaluated\non challenging face and pedestrian detection tasks: a face detector trained on\nWIDER-Face, which consists of high-quality images crawled from the web, is\nadapted to a large-scale surveillance data set; a pedestrian detector trained\non clear, daytime images from the BDD-100K driving data set is adapted to all\nother scenarios such as rainy, foggy, night-time. Our results demonstrate the\nusefulness of incorporating hard examples obtained from tracking, the advantage\nof using soft-labels via distillation loss versus hard-labels, and show\npromising performance as a simple method for unsupervised domain adaptation of\nobject detectors, with minimal dependence on hyper-parameters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:46:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["RoyChowdhury", "Aruni", ""], ["Chakrabarty", "Prithvijit", ""], ["Singh", "Ashish", ""], ["Jin", "SouYoung", ""], ["Jiang", "Huaizu", ""], ["Cao", "Liangliang", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1904.07312", "submitter": "Marnim Galib", "authors": "Reza Ghoddoosian, Marnim Galib, Vassilis Athitsos", "title": "A Realistic Dataset and Baseline Temporal Model for Early Drowsiness\n  Detection", "comments": "Computer Vision and Pattern Recognition Workshops (CVPRW 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drowsiness can put lives of many drivers and workers in danger. It is\nimportant to design practical and easy-to-deploy real-world systems to detect\nthe onset of drowsiness.In this paper, we address early drowsiness detection,\nwhich can provide early alerts and offer subjects ample time to react. We\npresent a large and public real-life dataset of 60 subjects, with video\nsegments labeled as alert, low vigilant, or drowsy. This dataset consists of\naround 30 hours of video, with contents ranging from subtle signs of drowsiness\nto more obvious ones. We also benchmark a temporal model for our dataset, which\nhas low computational and storage demands. The core of our proposed method is a\nHierarchical Multiscale Long Short-Term Memory (HM-LSTM) network, that is fed\nby detected blink features in sequence. Our experiments demonstrate the\nrelationship between the sequential blink features and drowsiness. In the\nexperimental results, our baseline method produces higher accuracy than human\njudgment.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:10:47 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Ghoddoosian", "Reza", ""], ["Galib", "Marnim", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1904.07318", "submitter": "David Schlangen", "authors": "David Schlangen", "title": "Natural Language Semantics With Pictures: Some Language & Vision\n  Datasets and Potential Uses for Computational Semantics", "comments": "Presented at the 13th International Conference on Computational\n  Semantics (IWCS 2019), Gothenburg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propelling, and propelled by, the \"deep learning revolution\", recent years\nhave seen the introduction of ever larger corpora of images annotated with\nnatural language expressions. We survey some of these corpora, taking a\nperspective that reverses the usual directionality, as it were, by viewing the\nimages as semantic annotation of the natural language expressions. We discuss\ndatasets that can be derived from the corpora, and tasks of potential interest\nfor computational semanticists that can be defined on those. In this, we make\nuse of relations provided by the corpora (namely, the link between expression\nand image, and that between two expressions linked to the same image) and\nrelations that we can add (similarity relations between expressions, or between\nimages). Specifically, we show that in this way we can create data that can be\nused to learn and evaluate lexical and compositional grounded semantics, and we\nshow that the \"linked to same image\" relation tracks a semantic implication\nrelation that is recognisable to annotators even in the absence of the linking\nimage as evidence. Finally, as an example of possible benefits of this\napproach, we show that an exemplar-model-based approach to implication beats a\n(simple) distributional space-based one on some derived datasets, while lending\nitself to explainability.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:15:46 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Schlangen", "David", ""]]}, {"id": "1904.07325", "submitter": "Krishnapriya K. S", "authors": "KS Krishnapriya, Kushal Vangara, Michael C. King, Vitor Albiero, and\n  Kevin Bowyer", "title": "Characterizing the Variability in Face Recognition Accuracy Relative to\n  Race", "comments": "Paper will appear in the BEFA workshop at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent news headlines have labeled face recognition technology as biased\nor racist. We report on a methodical investigation into differences in face\nrecognition accuracy between African-American and Caucasian image cohorts of\nthe MORPH dataset. We find that, for all four matchers considered, the impostor\nand the genuine distributions are statistically significantly different between\ncohorts. For a fixed decision threshold, the African-American image cohort has\na higher false match rate and a lower false non-match rate. ROC curves compare\nverification rates at the same false match rate, but the different cohorts\nachieve the same false match rate at different thresholds. This means that ROC\ncomparisons are not relevant to operational scenarios that use a fixed decision\nthreshold. We show that, for the ResNet matcher, the two cohorts have\napproximately equal separation of impostor and genuine distributions. Using\nICAO compliance as a standard of image quality, we find that the initial image\ncohorts have unequal rates of good quality images. The ICAO-compliant subsets\nof the original image cohorts show improved accuracy, with the main effect\nbeing to reducing the low-similarity tail of the genuine distributions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:46:29 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 03:32:15 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 04:19:44 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Krishnapriya", "KS", ""], ["Vangara", "Kushal", ""], ["King", "Michael C.", ""], ["Albiero", "Vitor", ""], ["Bowyer", "Kevin", ""]]}, {"id": "1904.07344", "submitter": "Xing Di", "authors": "Xing Di, Benjamin S. Riggan, Shuowen Hu, Nathaniel J. Short, Vishal M.\n  Patel", "title": "Polarimetric Thermal to Visible Face Verification via Self-Attention\n  Guided Synthesis", "comments": "This work is accepted at the 12th IAPR International Conference On\n  Biometrics (ICB 2019)", "journal-ref": null, "doi": "10.1109/ICB45273.2019.8987329", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric thermal to visible face verification entails matching two images\nthat contain significant domain differences. Several recent approaches have\nattempted to synthesize visible faces from thermal images for cross-modal\nmatching. In this paper, we take a different approach in which rather than\nfocusing only on synthesizing visible faces from thermal faces, we also propose\nto synthesize thermal faces from visible faces. Our intuition is based on the\nfact that thermal images also contain some discriminative information about the\nperson for verification. Deep features from a pre-trained Convolutional Neural\nNetwork (CNN) are extracted from the original as well as the synthesized\nimages. These features are then fused to generate a template which is then used\nfor verification. The proposed synthesis network is based on the self-attention\ngenerative adversarial network (SAGAN) which essentially allows efficient\nattention-guided image synthesis. Extensive experiments on the ARL polarimetric\nthermal face dataset demonstrate that the proposed method achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 21:58:40 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Di", "Xing", ""], ["Riggan", "Benjamin S.", ""], ["Hu", "Shuowen", ""], ["Short", "Nathaniel J.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1904.07349", "submitter": "Boyang Li", "authors": "Boyang Li, Changhao Chenli, Xiaowei Xu, Yiyu Shi, Taeho Jung", "title": "DLBC: A Deep Learning-Based Consensus in Blockchains for Deep Learning\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing artificial intelligence application, deep neural network\n(DNN) has become an emerging task. However, to train a good deep learning model\nwill suffer from enormous computation cost and energy consumption. Recently,\nblockchain has been widely used, and during its operation, a huge amount of\ncomputation resources are wasted for the Proof of Work (PoW) consensus. In this\npaper, we propose DLBC to exploit the computation power of miners for deep\nlearning training as proof of useful work instead of calculating hash values.\nit distinguishes itself from recent proof of useful work mechanisms by\naddressing various limitations of them. Specifically, DLBC handles multiple\ntasks, larger model and training datasets, and introduces a comprehensive\nranking mechanism that considers tasks difficulty(e.g., model complexity,\nnetwork burden, data size, queue length). We also applied DNN-watermark [1] to\nimprove the robustness. In Section V, the average overhead of digital signature\nis 1.25, 0.001, 0.002 and 0.98 seconds, respectively, and the average overhead\nof network is 3.77, 3.01, 0.37 and 0.41 seconds, respectively. Embedding a\nwatermark takes 3 epochs and removing a watermark takes 30 epochs. This penalty\nof removing watermark will prevent attackers from stealing, improving, and\nresubmitting DL models from honest miners.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 22:28:45 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 03:44:56 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Li", "Boyang", ""], ["Chenli", "Changhao", ""], ["Xu", "Xiaowei", ""], ["Shi", "Yiyu", ""], ["Jung", "Taeho", ""]]}, {"id": "1904.07361", "submitter": "Evgeny Abdulin", "authors": "Evgeniy Abdulin, Lee Friedman, Oleg Komogortsev", "title": "Custom Video-Oculography Device and Its Application to Fourth Purkinje\n  Image Detection during Saccades", "comments": "8 pages, 8 figures, appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We built a custom video-based eye-tracker that saves every video frame as a\nfull resolution image (MJPEG). Images can be processed offline for the\ndetection of ocular features, including the pupil and corneal reflection (First\nPurkinje Image, P1) position. A comparison of multiple algorithms for detection\nof pupil and corneal reflection can be performed. The system provides for\nhighly flexible stimulus creation, with mixing of graphic, image, and video\nstimuli. We can change cameras and infrared illuminators depending on the image\nqualities and frame rate desired. Using this system, we have detected the\nposition of the Fourth Purkinje image (P4) in the frames. We show that when we\nestimate gaze by calculating P1-P4, signal compares well with gaze estimated\nwith a DPI eye-tracker, which natively detects and tracks the P1 and P4.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 23:10:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Abdulin", "Evgeniy", ""], ["Friedman", "Lee", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "1904.07387", "submitter": "Po-Yu Kao", "authors": "Po-Yu Kao, Angela Zhang, Michael Goebel, Jefferson W. Chen, B.S.\n  Manjunath", "title": "Predicting Fluid Intelligence of Children using T1-weighted MR Images\n  and a StackNet", "comments": "8 pages, 2 figures, 3 tables, Accepted by MICCAI ABCD-NP Challenge\n  2019; Added NDA", "journal-ref": null, "doi": "10.1007/978-3-030-31901-4_2", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we utilize T1-weighted MR images and StackNet to predict fluid\nintelligence in adolescents. Our framework includes feature extraction, feature\nnormalization, feature denoising, feature selection, training a StackNet, and\npredicting fluid intelligence. The extracted feature is the distribution of\ndifferent brain tissues in different brain parcellation regions. The proposed\nStackNet consists of three layers and 11 models. Each layer uses the\npredictions from all previous layers including the input layer. The proposed\nStackNet is tested on a public benchmark Adolescent Brain Cognitive Development\nNeurocognitive Prediction Challenge 2019 and achieves a mean squared error of\n82.42 on the combined training and validation set with 10-fold\ncross-validation. In addition, the proposed StackNet also achieves a mean\nsquared error of 94.25 on the testing data. The source code is available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:04:20 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 02:31:27 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 02:25:31 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Kao", "Po-Yu", ""], ["Zhang", "Angela", ""], ["Goebel", "Michael", ""], ["Chen", "Jefferson W.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1904.07392", "submitter": "Golnaz Ghiasi", "authors": "Golnaz Ghiasi, Tsung-Yi Lin, Ruoming Pang, Quoc V. Le", "title": "NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object\n  Detection", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art convolutional architectures for object detection are\nmanually designed. Here we aim to learn a better architecture of feature\npyramid network for object detection. We adopt Neural Architecture Search and\ndiscover a new feature pyramid architecture in a novel scalable search space\ncovering all cross-scale connections. The discovered architecture, named\nNAS-FPN, consists of a combination of top-down and bottom-up connections to\nfuse features across scales. NAS-FPN, combined with various backbone models in\nthe RetinaNet framework, achieves better accuracy and latency tradeoff compared\nto state-of-the-art object detection models. NAS-FPN improves mobile detection\naccuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in\n[32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy\nwith less computation time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:32:33 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Ghiasi", "Golnaz", ""], ["Lin", "Tsung-Yi", ""], ["Pang", "Ruoming", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.07394", "submitter": "Quanquan Shao", "authors": "Quanquan Shao, Jie Hu", "title": "Combining RGB and Points to Predict Grasping Region for Robotic\n  Bin-Picking", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper focuses on a robotic picking tasks in cluttered scenario. Because\nof the diversity of objects and clutter by placing, it is much difficult to\nrecognize and estimate their pose before grasping. Here, we use U-net, a\nspecial Convolution Neural Networks (CNN), to combine RGB images and depth\ninformation to predict picking region without recognition and pose estimation.\nThe efficiency of diverse visual input of the network were compared, including\nRGB, RGB-D and RGB-Points. And we found the RGB-Points input could get a\nprecision of 95.74%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:47:27 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 09:06:01 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Shao", "Quanquan", ""], ["Hu", "Jie", ""]]}, {"id": "1904.07396", "submitter": "Saeed Anwar", "authors": "Saeed Anwar and Nick Barnes", "title": "Real Image Denoising with Feature Attention", "comments": "Accepted in ICCV (Oral), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks perform better on images containing\nspatially invariant noise (synthetic noise); however, their performance is\nlimited on real-noisy photographs and requires multiple stage network modeling.\nTo advance the practicability of denoising algorithms, this paper proposes a\nnovel single-stage blind real image denoising network (RIDNet) by employing a\nmodular architecture. We use a residual on the residual structure to ease the\nflow of low-frequency information and apply feature attention to exploit the\nchannel dependencies. Furthermore, the evaluation in terms of quantitative\nmetrics and visual quality on three synthetic and four real noisy datasets\nagainst 19 state-of-the-art algorithms demonstrate the superiority of our\nRIDNet.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:55:08 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 04:57:08 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Anwar", "Saeed", ""], ["Barnes", "Nick", ""]]}, {"id": "1904.07399", "submitter": "Xinyao Wang", "authors": "Xinyao Wang, Liefeng Bo, Li Fuxin", "title": "Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression", "comments": "[v2] Camera-ready version for ICCV 2019. [v3] Corrected AUC(fr10%) on\n  table 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap regression with a deep network has become one of the mainstream\napproaches to localize facial landmarks. However, the loss function for heatmap\nregression is rarely studied. In this paper, we analyze the ideal loss function\nproperties for heatmap regression in face alignment problems. Then we propose a\nnovel loss function, named Adaptive Wing loss, that is able to adapt its shape\nto different types of ground truth heatmap pixels. This adaptability penalizes\nloss more on foreground pixels while less on background pixels. To address the\nimbalance between foreground and background pixels, we also propose Weighted\nLoss Map, which assigns high weights on foreground and difficult background\npixels to help training process focus more on pixels that are crucial to\nlandmark localization. To further improve face alignment accuracy, we introduce\nboundary prediction and CoordConv with boundary coordinates. Extensive\nexperiments on different benchmarks, including COFW, 300W and WFLW, show our\napproach outperforms the state-of-the-art by a significant margin on various\nevaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap\nregression tasks. Code will be made publicly available at\nhttps://github.com/protossw512/AdaptiveWingLoss.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:00:24 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 23:12:06 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 05:30:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Wang", "Xinyao", ""], ["Bo", "Liefeng", ""], ["Fuxin", "Li", ""]]}, {"id": "1904.07402", "submitter": "Quanquan Shao", "authors": "Quanquan Shao, Jie Hu, Weiming Wang, Yi Fang, Wenhai Liu, Jin Qi, Jin\n  Ma", "title": "Suction Grasp Region Prediction using Self-supervised Learning for\n  Object Picking in Dense Clutter", "comments": "6 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper focuses on robotic picking tasks in cluttered scenario. Because of\nthe diversity of poses, types of stack and complicated background in bin\npicking situation, it is much difficult to recognize and estimate their pose\nbefore grasping them. Here, this paper combines Resnet with U-net structure, a\nspecial framework of Convolution Neural Networks (CNN), to predict picking\nregion without recognition and pose estimation. And it makes robotic picking\nsystem learn picking skills from scratch. At the same time, we train the\nnetwork end to end with online samples. In the end of this paper, several\nexperiments are conducted to demonstrate the performance of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:03:57 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 09:01:33 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Shao", "Quanquan", ""], ["Hu", "Jie", ""], ["Wang", "Weiming", ""], ["Fang", "Yi", ""], ["Liu", "Wenhai", ""], ["Qi", "Jin", ""], ["Ma", "Jin", ""]]}, {"id": "1904.07424", "submitter": "Huangyue Yu", "authors": "Huangyue Yu and Minjie Cai and Yunfei Liu and Feng Lu", "title": "What I See Is What You See: Joint Attention Learning for First and Third\n  Person Video Co-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, more and more videos are captured from the first-person\nviewpoint by wearable cameras. Such first-person video provides additional\ninformation besides the traditional third-person video, and thus has a wide\nrange of applications. However, techniques for analyzing the first-person video\ncan be fundamentally different from those for the third-person video, and it is\neven more difficult to explore the shared information from both viewpoints. In\nthis paper, we propose a novel method for first- and third-person video\nco-analysis. At the core of our method is the notion of \"joint attention\",\nindicating the learnable representation that corresponds to the shared\nattention regions in different viewpoints and thus links the two viewpoints. To\nthis end, we develop a multi-branch deep network with a triplet loss to extract\nthe joint attention from the first- and third-person videos via self-supervised\nlearning. We evaluate our method on the public dataset with cross-viewpoint\nvideo matching tasks. Our method outperforms the state-of-the-art both\nqualitatively and quantitatively. We also demonstrate how the learned joint\nattention can benefit various applications through a set of additional\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:09:50 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yu", "Huangyue", ""], ["Cai", "Minjie", ""], ["Liu", "Yunfei", ""], ["Lu", "Feng", ""]]}, {"id": "1904.07426", "submitter": "Jinghan Yao", "authors": "Jun Yu, Jinghan Yao, Jian Zhang, Zhou Yu, Dacheng Tao", "title": "Single Pixel Reconstruction for One-stage Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object instance segmentation is one of the most fundamental but challenging\ntasks in computer vision, and it requires the pixel-level image understanding.\nMost existing approaches address this problem by adding a mask prediction\nbranch to a two-stage object detector with the Region Proposal Network (RPN).\nAlthough producing good segmentation results, the efficiency of these two-stage\napproaches is far from satisfactory, restricting their applicability in\npractice. In this paper, we propose a one-stage framework, SPRNet, which\nperforms efficient instance segmentation by introducing a single pixel\nreconstruction (SPR) branch to off-the-shelf one-stage detectors. The added SPR\nbranch reconstructs the pixel-level mask from every single pixel in the\nconvolution feature map directly. Using the same ResNet-50 backbone, SPRNet\nachieves comparable mask AP to Mask R-CNN at a higher inference speed, and\ngains all-round improvements on box AP at every scale comparing with RetinaNet.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:11:13 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 01:35:22 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 07:39:51 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Yu", "Jun", ""], ["Yao", "Jinghan", ""], ["Zhang", "Jian", ""], ["Yu", "Zhou", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.07429", "submitter": "Mingxin Jin", "authors": "Mingxin Jin, Yongsheng Dong, Lintao Zheng, Lingfei Liang, Tianyu Wang,\n  Hongyan zhang", "title": "Shortest Paths in HSI Space for Color Texture Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color texture representation is an important step in the task of texture\nclassification. Shortest paths was used to extract color texture features from\nRGB and HSV color spaces. In this paper, we propose to use shortest paths in\nthe HSI space to build a texture representation for classification. In\nparticular, two undirected graphs are used to model the H channel and the S and\nI channels respectively in order to represent a color texture image. Moreover,\nthe shortest paths is constructed by using four pairs of pixels according to\ndifferent scales and directions of the texture image. Experimental results on\ncolored Brodatz and USPTex databases reveal that our proposed method is\neffective, and the highest classification accuracy rate is 96.93% in the\nBrodatz database.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:24:35 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Jin", "Mingxin", ""], ["Dong", "Yongsheng", ""], ["Zheng", "Lintao", ""], ["Liang", "Lingfei", ""], ["Wang", "Tianyu", ""], ["zhang", "Hongyan", ""]]}, {"id": "1904.07435", "submitter": "Agastya Kalra", "authors": "Agastya Kalra, Ben Peterson", "title": "Photofeeler-D3: A Neural Network with Voter Modeling for Dating Photo\n  Impression Prediction", "comments": "10 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In just a few years, online dating has become the dominant way that young\npeople meet to date, making the deceptively error-prone task of picking good\ndating profile photos vital to a generation's ability to form romantic\nconnections. Until now, artificial intelligence approaches to Dating Photo\nImpression Prediction (DPIP) have been very inaccurate, unadaptable to\nreal-world application, and have only taken into account a subject's physical\nattractiveness. To that effect, we propose Photofeeler-D3 - the first\nconvolutional neural network as accurate as 10 human votes for how smart,\ntrustworthy, and attractive the subject appears in highly variable dating\nphotos. Our \"attractive\" output is also applicable to Facial Beauty Prediction\n(FBP), making Photofeeler-D3 state-of-the-art for both DPIP and FBP. We achieve\nthis by leveraging Photofeeler's Dating Dataset (PDD) with over 1 million\nimages and tens of millions of votes, our novel technique of voter modeling,\nand cutting-edge computer vision techniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:44:08 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 18:00:48 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 17:38:08 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Kalra", "Agastya", ""], ["Peterson", "Ben", ""]]}, {"id": "1904.07442", "submitter": "Yupan Huang", "authors": "Yupan Huang, Qi Dai, Yutong Lu", "title": "Decoupling Localization and Classification in Single Shot Temporal\n  Action Detection", "comments": "ICME 2019. https://github.com/HYPJUDY/Decouple-SSAD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video temporal action detection aims to temporally localize and recognize the\naction in untrimmed videos. Existing one-stage approaches mostly focus on\nunifying two subtasks, i.e., localization of action proposals and\nclassification of each proposal through a fully shared backbone. However, such\ndesign of encapsulating all components of two subtasks in one single network\nmight restrict the training by ignoring the specialized characteristic of each\nsubtask. In this paper, we propose a novel Decoupled Single Shot temporal\nAction Detection (Decouple-SSAD) method to mitigate such problem by decoupling\nthe localization and classification in a one-stage scheme. Particularly, two\nseparate branches are designed in parallel to enable each component to own\nrepresentations privately for accurate localization or classification. Each\nbranch produces a set of action anchor layers by applying deconvolution to the\nfeature maps of the main stream. Each branch produces a set of feature maps by\napplying deconvolution to the feature maps of the main stream. High-level\nsemantic information from deeper layers is thus incorporated to enhance the\nfeature representations. We conduct extensive experiments on THUMOS14 dataset\nand demonstrate superior performance over state-of-the-art methods. Our code is\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:50:57 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Huang", "Yupan", ""], ["Dai", "Qi", ""], ["Lu", "Yutong", ""]]}, {"id": "1904.07451", "submitter": "Yash Goyal", "authors": "Yash Goyal and Ziyan Wu and Jan Ernst and Dhruv Batra and Devi Parikh\n  and Stefan Lee", "title": "Counterfactual Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a technique to produce counterfactual visual\nexplanations. Given a 'query' image $I$ for which a vision system predicts\nclass $c$, a counterfactual visual explanation identifies how $I$ could change\nsuch that the system would output a different specified class $c'$. To do this,\nwe select a 'distractor' image $I'$ that the system predicts as class $c'$ and\nidentify spatial regions in $I$ and $I'$ such that replacing the identified\nregion in $I$ with the identified region in $I'$ would push the system towards\nclassifying $I$ as $c'$. We apply our approach to multiple image classification\ndatasets generating qualitative results showcasing the interpretability and\ndiscriminativeness of our counterfactual explanations. To explore the\neffectiveness of our explanations in teaching humans, we present machine\nteaching experiments for the task of fine-grained bird classification. We find\nthat users trained to distinguish bird species fare better when given access to\ncounterfactual explanations in addition to training examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:16:11 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 16:49:55 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Goyal", "Yash", ""], ["Wu", "Ziyan", ""], ["Ernst", "Jan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1904.07454", "submitter": "Jorge Emmanuel Arce Garro", "authors": "Jorge Arce Garro, David Jim\\'enez L\\'opez", "title": "Point cloud registration: matching a maximal common subset on\n  pointclouds with noise (with 2D implementation)", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of determining whether 2 given point clouds in 2D,\nwith any distinct cardinality and any number of outliers, have subsets of the\nsame size that can be matched via a rigid motion. This problem is important,\nfor example, in the application of fingerprint matching with incomplete data.\nWe propose an algorithm that, under assumptions on the noise tolerance, allows\nto find corresponding subclouds of the maximum possible size. Our procedure\noptimizes a potential energy function to do so, which was first inspired in the\npotential energy interaction that occurs between point charges in\nelectrostatics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:23:30 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Garro", "Jorge Arce", ""], ["L\u00f3pez", "David Jim\u00e9nez", ""]]}, {"id": "1904.07457", "submitter": "Zezhou Cheng", "authors": "Zezhou Cheng, Matheus Gadelha, Subhransu Maji, Daniel Sheldon", "title": "A Bayesian Perspective on the Deep Image Prior", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep image prior was recently introduced as a prior for natural images.\nIt represents images as the output of a convolutional network with random\ninputs. For \"inference\", gradient descent is performed to adjust network\nparameters to make the output match observations. This approach yields good\nperformance on a range of image reconstruction tasks. We show that the deep\nimage prior is asymptotically equivalent to a stationary Gaussian process prior\nin the limit as the number of channels in each layer of the network goes to\ninfinity, and derive the corresponding kernel. This informs a Bayesian approach\nto inference. We show that by conducting posterior inference using stochastic\ngradient Langevin we avoid the need for early stopping, which is a drawback of\nthe current approach, and improve results for denoising and impainting tasks.\nWe illustrate these intuitions on a number of 1D and 2D signal reconstruction\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:39:29 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Cheng", "Zezhou", ""], ["Gadelha", "Matheus", ""], ["Maji", "Subhransu", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1904.07460", "submitter": "Qing Ping", "authors": "Qing Ping, Bing Wu, Wanying Ding, Jiangbo Yuan", "title": "Fashion-AttGAN: Attribute-Aware Fashion Editing with Multi-Objective GAN", "comments": "3 pages, 2 figures, accepted at FFSS-USAD workshop @ CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce attribute-aware fashion-editing, a novel task, to\nthe fashion domain. We re-define the overall objectives in AttGAN and propose\nthe Fashion-AttGAN model for this new task. A dataset is constructed for this\ntask with 14,221 and 22 attributes, which has been made publically available.\nExperimental results show the effectiveness of our Fashion-AttGAN on fashion\nediting over the original AttGAN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:51:59 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 17:05:15 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ping", "Qing", ""], ["Wu", "Bing", ""], ["Ding", "Wanying", ""], ["Yuan", "Jiangbo", ""]]}, {"id": "1904.07461", "submitter": "Siyu Chen", "authors": "Jingzhou Chen, Siyu Chen, Peilin Zhou, Yuntao Qian", "title": "Deep Neural Network Based Hyperspectral Pixel Classification With\n  Factorized Spectral-Spatial Feature Representation", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely used for hyperspectral pixel classification due\nto its ability of generating deep feature representation. However, how to\nconstruct an efficient and powerful network suitable for hyperspectral data is\nstill under exploration. In this paper, a novel neural network model is\ndesigned for taking full advantage of the spectral-spatial structure of\nhyperspectral data. Firstly, we extract pixel-based intrinsic features from\nrich yet redundant spectral bands by a subnetwork with supervised pre-training\nscheme. Secondly, in order to utilize the local spatial correlation among\npixels, we share the previous subnetwork as a spectral feature extractor for\neach pixel in a patch of image, after which the spectral features of all pixels\nin a patch are combined and feeded into the subsequent classification\nsubnetwork. Finally, the whole network is further fine-tuned to improve its\nclassification performance. Specially, the spectral-spatial factorization\nscheme is applied in our model architecture, making the network size and the\nnumber of parameters great less than the existing spectral-spatial deep\nnetworks for hyperspectral image classification. Experiments on the\nhyperspectral data sets show that, compared with some state-of-art deep\nlearning methods, our method achieves better classification results while\nhaving smaller network size and less parameters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:52:19 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Chen", "Jingzhou", ""], ["Chen", "Siyu", ""], ["Zhou", "Peilin", ""], ["Qian", "Yuntao", ""]]}, {"id": "1904.07470", "submitter": "Ying Da Wang", "authors": "Ying Da Wang, Ryan Armstrong, Peyman Mostaghimi", "title": "Super Resolution Convolutional Neural Network Models for Enhancing\n  Resolution of Rock Micro-CT Images", "comments": "24 pages", "journal-ref": null, "doi": "10.1016/j.petrol.2019.106261", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Image Super Resolution (SISR) techniques based on Super Resolution\nConvolutional Neural Networks (SRCNN) are applied to micro-computed tomography\n({\\mu}CT) images of sandstone and carbonate rocks. Digital rock imaging is\nlimited by the capability of the scanning device resulting in trade-offs\nbetween resolution and field of view, and super resolution methods tested in\nthis study aim to compensate for these limits. SRCNN models SR-Resnet, Enhanced\nDeep SR (EDSR), and Wide-Activation Deep SR (WDSR) are used on the Digital Rock\nSuper Resolution 1 (DRSRD1) Dataset of 4x downsampled images, comprising of\n2000 high resolution (800x800) raw micro-CT images of Bentheimer sandstone and\nEstaillades carbonate. The trained models are applied to the validation and\ntest data within the dataset and show a 3-5 dB rise in image quality compared\nto bicubic interpolation, with all tested models performing within a 0.1 dB\nrange. Difference maps indicate that edge sharpness is completely recovered in\nimages within the scope of the trained model, with only high frequency noise\nrelated detail loss. We find that aside from generation of high-resolution\nimages, a beneficial side effect of super resolution methods applied to\nsynthetically downgraded images is the removal of image noise while recovering\nedgewise sharpness which is beneficial for the segmentation process. The model\nis also tested against real low-resolution images of Bentheimer rock with image\naugmentation to account for natural noise and blur. The SRCNN method is shown\nto act as a preconditioner for image segmentation under these circumstances\nwhich naturally leads to further future development and training of models that\nsegment an image directly. Image restoration by SRCNN on the rock images is of\nsignificantly higher quality than traditional methods and suggests SRCNN\nmethods are a viable processing step in a digital rock workflow.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:24:49 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Da Wang", "Ying", ""], ["Armstrong", "Ryan", ""], ["Mostaghimi", "Peyman", ""]]}, {"id": "1904.07475", "submitter": "Yanhong Zeng", "authors": "Yanhong Zeng, Jianlong Fu, Hongyang Chao, Baining Guo", "title": "Learning Pyramid-Context Encoder Network for High-Quality Image\n  Inpainting", "comments": "Accepted as a CVPR 2019 poster paper; update SUPP;update Eq5;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality image inpainting requires filling missing regions in a damaged\nimage with plausible content. Existing works either fill the regions by copying\nimage patches or generating semantically-coherent patches from region context,\nwhile neglect the fact that both visual and semantic plausibility are\nhighly-demanded. In this paper, we propose a Pyramid-context ENcoder Network\n(PEN-Net) for image inpainting by deep generative models. The PEN-Net is built\nupon a U-Net structure, which can restore an image by encoding contextual\nsemantics from full resolution input, and decoding the learned semantic\nfeatures back into images. Specifically, we propose a pyramid-context encoder,\nwhich progressively learns region affinity by attention from a high-level\nsemantic feature map and transfers the learned attention to the previous\nlow-level feature map. As the missing content can be filled by attention\ntransfer from deep to shallow in a pyramid fashion, both visual and semantic\ncoherence for image inpainting can be ensured. We further propose a multi-scale\ndecoder with deeply-supervised pyramid losses and an adversarial loss. Such a\ndesign not only results in fast convergence in training, but more realistic\nresults in testing. Extensive experiments on various datasets show the superior\nperformance of the proposed network\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:51:37 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 04:55:00 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 02:18:05 GMT"}, {"version": "v4", "created": "Thu, 11 Jul 2019 03:02:28 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zeng", "Yanhong", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""], ["Guo", "Baining", ""]]}, {"id": "1904.07478", "submitter": "Rebecca Simpson", "authors": "Becks Simpson, Francis Dutil, Yoshua Bengio and Joseph Paul Cohen", "title": "GradMask: Reduce Overfitting by Regularizing Saliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With too few samples or too many model parameters, overfitting can inhibit\nthe ability to generalise predictions to new data. Within medical imaging, this\ncan occur when features are incorrectly assigned importance such as distinct\nhospital specific artifacts, leading to poor performance on a new dataset from\na different institution without those features, which is undesirable. Most\nregularization methods do not explicitly penalize the incorrect association of\nthese features to the target class and hence fail to address this issue. We\npropose a regularization method, GradMask, which penalizes saliency maps\ninferred from the classifier gradients when they are not consistent with the\nlesion segmentation. This prevents non-tumor related features to contribute to\nthe classification of unhealthy samples. We demonstrate that this method can\nimprove test accuracy between 1-3% compared to the baseline without GradMask,\nshowing that it has an impact on reducing overfitting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:57:50 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Simpson", "Becks", ""], ["Dutil", "Francis", ""], ["Bengio", "Yoshua", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1904.07482", "submitter": "Guangxiang Zhu", "authors": "Guangxiang Zhu, Jianhao Wang, Zhizhou Ren, Zichuan Lin, and Chongjie\n  Zhang", "title": "Object-Oriented Dynamics Learning through Multi-Level Abstraction", "comments": "Accepted to the Thirthy-Fourth AAAI Conference On Artificial\n  Intelligence (AAAI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-based approaches for learning action-conditioned dynamics has\ndemonstrated promise for generalization and interpretability. However, existing\napproaches suffer from structural limitations and optimization difficulties for\ncommon environments with multiple dynamic objects. In this paper, we present a\nnovel self-supervised learning framework, called Multi-level Abstraction\nObject-oriented Predictor (MAOP), which employs a three-level learning\narchitecture that enables efficient object-based dynamics learning from raw\nvisual observations. We also design a spatial-temporal relational reasoning\nmechanism for MAOP to support instance-level dynamics learning and handle\npartial observability. Our results show that MAOP significantly outperforms\nprevious methods in terms of sample efficiency and generalization over novel\nenvironments for learning environment models. We also demonstrate that learned\ndynamics models enable efficient planning in unseen environments, comparable to\ntrue environment models. In addition, MAOP learns semantically and visually\ninterpretable disentangled representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 06:01:17 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 03:46:55 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 10:29:10 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 06:05:28 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhu", "Guangxiang", ""], ["Wang", "Jianhao", ""], ["Ren", "Zhizhou", ""], ["Lin", "Zichuan", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1904.07483", "submitter": "Lan Ma", "authors": "Di Zhao, Lan Ma, Songnan Li, Dahai Yu", "title": "End-to-End Denoising of Dark Burst Images Using Recurrent Fully\n  Convolutional Networks", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When taking photos in dim-light environments, due to the small amount of\nlight entering, the shot images are usually extremely dark, with a great deal\nof noise, and the color cannot reflect real-world color. Under this condition,\nthe traditional methods used for single image denoising have always failed to\nbe effective. One common idea is to take multiple frames of the same scene to\nenhance the signal-to-noise ratio. This paper proposes a recurrent fully\nconvolutional network (RFCN) to process burst photos taken under extremely\nlow-light conditions, and to obtain denoised images with improved brightness.\nOur model maps raw burst images directly to sRGB outputs, either to produce a\nbest image or to generate a multi-frame denoised image sequence. This process\nhas proven to be capable of accomplishing the low-level task of denoising, as\nwell as the high-level task of color correction and enhancement, all of which\nis end-to-end processing through our network. Our method has achieved better\nresults than state-of-the-art methods. In addition, we have applied the model\ntrained by one type of camera without fine-tuning on photos captured by\ndifferent cameras and have obtained similar end-to-end enhancements.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 06:03:49 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhao", "Di", ""], ["Ma", "Lan", ""], ["Li", "Songnan", ""], ["Yu", "Dahai", ""]]}, {"id": "1904.07488", "submitter": "Dacheng Tao", "authors": "Erkun Yang, Cheng Deng, Chao Li, Wei Liu, Jie Li, Dacheng Tao", "title": "Shared Predictive Cross-Modal Deep Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With explosive growth of data volume and ever-increasing diversity of data\nmodalities, cross-modal similarity search, which conducts nearest neighbor\nsearch across different modalities, has been attracting increasing interest.\nThis paper presents a deep compact code learning solution for efficient\ncross-modal similarity search. Many recent studies have proven that\nquantization-based approaches perform generally better than hashing-based\napproaches on single-modal similarity search. In this paper, we propose a deep\nquantization approach, which is among the early attempts of leveraging deep\nneural networks into quantization-based cross-modal similarity search. Our\napproach, dubbed shared predictive deep quantization (SPDQ), explicitly\nformulates a shared subspace across different modalities and two private\nsubspaces for individual modalities, and representations in the shared subspace\nand the private subspaces are learned simultaneously by embedding them to a\nreproducing kernel Hilbert space, where the mean embedding of different\nmodality distributions can be explicitly compared. In addition, in the shared\nsubspace, a quantizer is learned to produce the semantics preserving compact\ncodes with the help of label alignment. Thanks to this novel network\narchitecture in cooperation with supervised quantization training, SPDQ can\npreserve intramodal and intermodal similarities as much as possible and greatly\nreduce quantization error. Experiments on two popular benchmarks corroborate\nthat our approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 06:29:02 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yang", "Erkun", ""], ["Deng", "Cheng", ""], ["Li", "Chao", ""], ["Liu", "Wei", ""], ["Li", "Jie", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.07496", "submitter": "Chong Peng", "authors": "Chong Peng, Qiang Cheng", "title": "Discriminative Ridge Machine: A Classifier for High-Dimensional Data or\n  Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a discriminative regression approach to supervised\nclassification in this paper. It estimates a representation model while\naccounting for discriminativeness between classes, thereby enabling accurate\nderivation of categorical information. This new type of regression models\nextends existing models such as ridge, lasso, and group lasso through\nexplicitly incorporating discriminative information. As a special case we focus\non a quadratic model that admits a closed-form analytical solution. The\ncorresponding classifier is called discriminative regression machine (DRM).\nThree iterative algorithms are further established for the DRM to enhance the\nefficiency and scalability for real applications. Our approach and the\nalgorithms are applicable to general types of data including images,\nhigh-dimensional data, and imbalanced data. We compare the DRM with currently\nstate-of-the-art classifiers. Our extensive experimental results show superior\nperformance of the DRM and confirm the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:07:01 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 17:54:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1904.07497", "submitter": "Chong Peng", "authors": "Chong Peng, Chenglizhao Chen, Zhao Kang, Jianbo Li, Qiang Cheng", "title": "RES-PCA: A Scalable Approach to Recovering Low-rank Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) has drawn significant attentions\ndue to its powerful capability in recovering low-rank matrices as well as\nsuccessful appplications in various real world problems. The current\nstate-of-the-art algorithms usually need to solve singular value decomposition\nof large matrices, which generally has at least a quadratic or even cubic\ncomplexity. This drawback has limited the application of RPCA in solving real\nworld problems. To combat this drawback, in this paper we propose a new type of\nRPCA method, RES-PCA, which is linearly efficient and scalable in both data\nsize and dimension. For comparison purpose, AltProj, an existing scalable\napproach to RPCA requires the precise knowlwdge of the true rank; otherwise, it\nmay fail to recover low-rank matrices. By contrast, our method works with or\nwithout knowing the true rank; even when both methods work, our method is\nfaster. Extensive experiments have been performed and testified to the\neffectiveness of proposed method quantitatively and in visual quality, which\nsuggests that our method is suitable to be employed as a light-weight, scalable\ncomponent for RPCA in any application pipelines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:07:44 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Peng", "Chong", ""], ["Chen", "Chenglizhao", ""], ["Kang", "Zhao", ""], ["Li", "Jianbo", ""], ["Cheng", "Qiang", ""]]}, {"id": "1904.07516", "submitter": "Siyu Chen", "authors": "Zhijian Luo, Siyu Chen, Yuntao Qian", "title": "A Deep Optimization Approach for Image Deconvolution", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In blind image deconvolution, priors are often leveraged to constrain the\nsolution space, so as to alleviate the under-determinacy. Priors which are\ntrained separately from the task of deconvolution tend to be instable, or\nineffective. We propose the Golf Optimizer, a novel but simple form of network\nthat learns deep priors from data with better propagation behavior. Like\nplaying golf, our method first estimates an aggressive propagation towards\noptimum using one network, and recurrently applies a residual CNN to learn the\ngradient of prior for delicate correction on restoration. Experiments show that\nour network achieves competitive performance on GoPro dataset, and our model is\nextremely lightweight compared with the state-of-art works.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:52:45 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Luo", "Zhijian", ""], ["Chen", "Siyu", ""], ["Qian", "Yuntao", ""]]}, {"id": "1904.07523", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Salman Khan, Nick Barnes", "title": "A Deep Journey into Super-resolution: A survey", "comments": "Accepted in ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks based super-resolution is a fast-growing field\nwith numerous practical applications. In this exposition, we extensively\ncompare 30+ state-of-the-art super-resolution Convolutional Neural Networks\n(CNNs) over three classical and three recently introduced challenging datasets\nto benchmark single image super-resolution. We introduce a taxonomy for\ndeep-learning based super-resolution networks that groups existing methods into\nnine categories including linear, residual, multi-branch, recursive,\nprogressive, attention-based and adversarial designs. We also provide\ncomparisons between the models in terms of network complexity, memory\nfootprint, model input and output, learning details, the type of network losses\nand important architectural differences (e.g., depth, skip-connections,\nfilters). The extensive evaluation performed, shows the consistent and rapid\ngrowth in the accuracy in the past few years along with a corresponding boost\nin model complexity and the availability of large-scale datasets. It is also\nobserved that the pioneering methods identified as the benchmark have been\nsignificantly outperformed by the current contenders. Despite the progress in\nrecent years, we identify several shortcomings of existing techniques and\nprovide future research directions towards the solution of these open problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:08:14 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 02:00:08 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 04:37:12 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Anwar", "Saeed", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""]]}, {"id": "1904.07528", "submitter": "Yikang Li", "authors": "Yikang Li, Chris Twigg, Yuting Ye, Lingling Tao, Xiaogang Wang", "title": "Disentangling Pose from Appearance in Monochrome Hand Images", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from the monocular 2D image is challenging due to the\nvariation in lighting, appearance, and background. While some success has been\nachieved using deep neural networks, they typically require collecting a large\ndataset that adequately samples all the axes of variation of hand images. It\nwould, therefore, be useful to find a representation of hand pose which is\nindependent of the image appearance~(like hand texture, lighting, background),\nso that we can synthesize unseen images by mixing pose-appearance combinations.\nIn this paper, we present a novel technique that disentangles the\nrepresentation of pose from a complementary appearance factor in 2D monochrome\nimages. We supervise this disentanglement process using a network that learns\nto generate images of hand using specified pose+appearance features. Unlike\nprevious work, we do not require image pairs with a matching pose; instead, we\nuse the pose annotations already available and introduce a novel use of cycle\nconsistency to ensure orthogonality between the factors. Experimental results\nshow that our self-disentanglement scheme successfully decomposes the hand\nimage into the pose and its complementary appearance features of comparable\nquality as the method using paired data. Additionally, training the model with\nextra synthesized images with unseen hand-appearance combinations by re-mixing\npose and appearance factors from different images can improve the 2D pose\nestimation performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:15:26 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Li", "Yikang", ""], ["Twigg", "Chris", ""], ["Ye", "Yuting", ""], ["Tao", "Lingling", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1904.07537", "submitter": "Stefan Milz", "authors": "Martin Simon, Karl Amende, Andrea Kraus, Jens Honer, Timo S\\\"amann,\n  Hauke Kaulbersch, Stefan Milz, Horst Michael Gross", "title": "Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of 3D objects is a fundamental problem in computer vision\nand has an enormous impact on autonomous cars, augmented/virtual reality and\nmany applications in robotics. In this work we present a novel fusion of neural\nnetwork based state-of-the-art 3D detector and visual semantic segmentation in\nthe context of autonomous driving. Additionally, we introduce\nScale-Rotation-Translation score (SRTs), a fast and highly parameterizable\nevaluation metric for comparison of object detections, which speeds up our\ninference time up to 20\\% and halves training time. On top, we apply\nstate-of-the-art online multi target feature tracking on the object\nmeasurements to further increase accuracy and robustness utilizing temporal\ninformation. Our experiments on KITTI show that we achieve same results as\nstate-of-the-art in all related categories, while maintaining the performance\nand accuracy trade-off and still run in real-time. Furthermore, our model is\nthe first one that fuses visual semantic with 3D object detection.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:49:06 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Simon", "Martin", ""], ["Amende", "Karl", ""], ["Kraus", "Andrea", ""], ["Honer", "Jens", ""], ["S\u00e4mann", "Timo", ""], ["Kaulbersch", "Hauke", ""], ["Milz", "Stefan", ""], ["Gross", "Horst Michael", ""]]}, {"id": "1904.07538", "submitter": "Naoya Fushishita", "authors": "Naoya Fushishita, Antonio Tejero-de-Pablos, Yusuke Mukuta, Tatsuya\n  Harada", "title": "Long-Term Human Video Generation of Multiple Futures Using Poses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future human behavior from an input human video is a useful task\nfor applications such as autonomous driving and robotics. While most previous\nworks predict a single future, multiple futures with different behavior can\npotentially occur. Moreover, if the predicted future is too short (e.g., less\nthan one second), it may not be fully usable by a human or other systems. In\nthis paper, we propose a novel method for future human pose prediction capable\nof predicting multiple long-term futures. This makes the predictions more\nsuitable for real applications. Also, from the input video and the predicted\nhuman behavior, we generate future videos. First, from an input human video, we\ngenerate sequences of future human poses (i.e., the image coordinates of their\nbody-joints) via adversarial learning. Adversarial learning suffers from mode\ncollapse, which makes it difficult to generate a variety of multiple poses. We\nsolve this problem by utilizing two additional inputs to the generator to make\nthe outputs diverse, namely, a latent code (to reflect various behaviors) and\nan attraction point (to reflect various trajectories). In addition, we generate\nlong-term future human poses using a novel approach based on unidimensional\nconvolutional neural networks. Last, we generate an output video based on the\ngenerated poses for visualization. We evaluate the generated future poses and\nvideos using three criteria (i.e., realism, diversity and accuracy), and show\nthat our proposed method outperforms other state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:50:44 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 02:30:14 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 09:39:30 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 16:00:22 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Fushishita", "Naoya", ""], ["Tejero-de-Pablos", "Antonio", ""], ["Mukuta", "Yusuke", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1904.07588", "submitter": "Yongsheng Dong", "authors": "Xuelong Li, Kang Liu, Yongsheng Dong, and Dacheng Tao", "title": "Patch alignment manifold matting", "comments": "13 pages, 7 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, July,\n  2018", "doi": "10.1109/TNNLS.2017.2727140", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is generally modeled as a space transform from the color space\nto the alpha space. By estimating the alpha factor of the model, the foreground\nof an image can be extracted. However, there is some dimensional information\nredundancy in the alpha space. It usually leads to the misjudgments of some\npixels near the boundary between the foreground and the background. In this\npaper, a manifold matting framework named Patch Alignment Manifold Matting is\nproposed for image matting. In particular, we first propose a part modeling of\ncolor space in the local image patch. We then perform whole alignment\noptimization for approximating the alpha results using subspace reconstructing\nerror. Furthermore, we utilize Nesterov's algorithm to solve the optimization\nproblem. Finally, we apply some manifold learning methods in the framework, and\nobtain several image matting methods, such as named ISOMAP matting and its\nderived Cascade ISOMAP matting. The experimental results reveal that the\nmanifold matting framework and its two examples are effective when compared\nwith several representative matting methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 10:52:24 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Li", "Xuelong", ""], ["Liu", "Kang", ""], ["Dong", "Yongsheng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.07595", "submitter": "Krzysztof Lis", "authors": "Krzysztof Lis, Krishna Nakka, Pascal Fua, Mathieu Salzmann", "title": "Detecting the Unexpected via Image Resynthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Classical semantic segmentation methods, including the recent deep learning\nones, assume that all classes observed at test time have been seen during\ntraining. In this paper, we tackle the more realistic scenario where unexpected\nobjects of unknown classes can appear at test time. The main trends in this\narea either leverage the notion of prediction uncertainty to flag the regions\nwith low confidence as unknown, or rely on autoencoders and highlight\npoorly-decoded regions. Having observed that, in both cases, the detected\nregions typically do not correspond to unexpected objects, in this paper, we\nintroduce a drastically different strategy: It relies on the intuition that the\nnetwork will produce spurious labels in regions depicting unexpected objects.\nTherefore, resynthesizing the image from the resulting semantic map will yield\nsignificant appearance differences with respect to the input image. In other\nwords, we translate the problem of detecting unknown classes to one of\nidentifying poorly-resynthesized image regions. We show that this outperforms\nboth uncertainty- and autoencoder-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 11:08:39 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 12:27:02 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lis", "Krzysztof", ""], ["Nakka", "Krishna", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1904.07601", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Bin Fan, Shiming Xiang and Chunhong Pan", "title": "Relation-Shape Convolutional Neural Network for Point Cloud Analysis", "comments": "Accepted to CVPR 2019 as an oral presentation. Project page at\n  https://yochengliu.github.io/Relation-Shape-CNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is very challenging, as the shape implied in irregular\npoints is difficult to capture. In this paper, we propose RS-CNN, namely,\nRelation-Shape Convolutional Neural Network, which extends regular grid CNN to\nirregular configuration for point cloud analysis. The key to RS-CNN is learning\nfrom relation, i.e., the geometric topology constraint among points.\nSpecifically, the convolutional weight for local point set is forced to learn a\nhigh-level relation expression from predefined geometric priors, between a\nsampled point from this point set and the others. In this way, an inductive\nlocal representation with explicit reasoning about the spatial layout of points\ncan be obtained, which leads to much shape awareness and robustness. With this\nconvolution as a basic operator, RS-CNN, a hierarchical architecture can be\ndeveloped to achieve contextual shape-aware learning for point cloud analysis.\nExtensive experiments on challenging benchmarks across three tasks verify\nRS-CNN achieves the state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 11:28:51 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 06:56:46 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 03:55:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1904.07615", "submitter": "Pedro Hermosilla Casajus", "authors": "Pedro Hermosilla, Tobias Ritschel, Timo Ropinski", "title": "Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning", "comments": "Proceedings of ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that denoising of 3D point clouds can be learned unsupervised,\ndirectly from noisy 3D point cloud data only. This is achieved by extending\nrecent ideas from learning of unsupervised image denoisers to unstructured 3D\npoint clouds. Unsupervised image denoisers operate under the assumption that a\nnoisy pixel observation is a random realization of a distribution around a\nclean pixel value, which allows appropriate learning on this distribution to\neventually converge to the correct value. Regrettably, this assumption is not\nvalid for unstructured points: 3D point clouds are subject to total noise, i.\ne., deviations in all coordinates, with no reliable pixel grid. Thus, an\nobservation can be the realization of an entire manifold of clean 3D points,\nwhich makes a na\\\"ive extension of unsupervised image denoisers to 3D point\nclouds impractical. Overcoming this, we introduce a spatial prior term, that\nsteers converges to the unique closest out of the many possible modes on a\nmanifold. Our results demonstrate unsupervised denoising performance similar to\nthat of supervised learning with clean data when given enough training examples\n- whereby we do not need any pairs of noisy and clean training data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:11:26 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 22:53:52 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Hermosilla", "Pedro", ""], ["Ritschel", "Tobias", ""], ["Ropinski", "Timo", ""]]}, {"id": "1904.07642", "submitter": "Huikai Wu", "authors": "Huikai Wu, Junge Zhang, Kaiqi Huang", "title": "SparseMask: Differentiable Connectivity Learning for Dense Image\n  Prediction", "comments": "Accepted by ICCV 2019. Code is available at\n  https://github.com/wuhuikai/SparseMask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we aim at automatically searching an efficient network\narchitecture for dense image prediction. Particularly, we follow the\nencoder-decoder style and focus on designing a connectivity structure for the\ndecoder. To achieve that, we design a densely connected network with learnable\nconnections, named Fully Dense Network, which contains a large set of possible\nfinal connectivity structures. We then employ gradient descent to search the\noptimal connectivity from the dense connections. The search process is guided\nby a novel loss function, which pushes the weight of each connection to be\nbinary and the connections to be sparse. The discovered connectivity achieves\ncompetitive results on two segmentation datasets, while runs more than three\ntimes faster and requires less than half parameters compared to the\nstate-of-the-art methods. An extensive experiment shows that the discovered\nconnectivity is compatible with various backbones and generalizes well to other\ndense image prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:13:53 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 10:03:53 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Wu", "Huikai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1904.07647", "submitter": "Sudhakar Kumawat", "authors": "Sudhakar Kumawat, Manisha Verma, Shanmuganathan Raman", "title": "LBVCNN: Local Binary Volume Convolutional Neural Network for Facial\n  Expression Recognition from Image Sequences", "comments": "Accepted in CVPRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing facial expressions is one of the central problems in computer\nvision. Temporal image sequences have useful spatio-temporal features for\nrecognizing expressions. In this paper, we propose a new 3D Convolution Neural\nNetwork (CNN) that can be trained end-to-end for facial expression recognition\non temporal image sequences without using facial landmarks. More specifically,\na novel 3D convolutional layer that we call Local Binary Volume (LBV) layer is\nproposed. The LBV layer, when used with our newly proposed LBVCNN network,\nachieve comparable results compared to state-of-the-art landmark-based or\nwithout landmark-based models on image sequences from CK+, Oulu-CASIA, and UNBC\nMcMaster shoulder pain datasets. Furthermore, our LBV layer reduces the number\nof trainable parameters by a significant amount when compared to a conventional\n3D convolutional layer. As a matter of fact, when compared to a 3x3x3\nconventional 3D convolutional layer, the LBV layer uses 27 times less trainable\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:19:59 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kumawat", "Sudhakar", ""], ["Verma", "Manisha", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1904.07659", "submitter": "Akanksha Paul", "authors": "Akanksha Paul, Narayanan C. Krishnan, Prateek Munjal", "title": "Semantically Aligned Bias Reducing Zero Shot Learning", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero shot learning (ZSL) aims to recognize unseen classes by exploiting\nsemantic relationships between seen and unseen classes. Two major problems\nfaced by ZSL algorithms are the hubness problem and the bias towards the seen\nclasses. Existing ZSL methods focus on only one of these problems in the\nconventional and generalized ZSL setting. In this work, we propose a novel\napproach, Semantically Aligned Bias Reducing (SABR) ZSL, which focuses on\nsolving both the problems. It overcomes the hubness problem by learning a\nlatent space that preserves the semantic relationship between the labels while\nencoding the discriminating information about the classes. Further, we also\npropose ways to reduce the bias of the seen classes through a simple\ncross-validation process in the inductive setting and a novel weak transfer\nconstraint in the transductive setting. Extensive experiments on three\nbenchmark datasets suggest that the proposed model significantly outperforms\nexisting state-of-the-art algorithms by ~1.5-9% in the conventional ZSL setting\nand by ~2-14% in the generalized ZSL for both the inductive and transductive\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:37:15 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Paul", "Akanksha", ""], ["Krishnan", "Narayanan C.", ""], ["Munjal", "Prateek", ""]]}, {"id": "1904.07714", "submitter": "Yung-Hsiang Lu", "authors": "Sergei Alyamkin, Matthew Ardi, Alexander C. Berg, Achille Brighton, Bo\n  Chen, Yiran Chen, Hsin-Pai Cheng, Zichen Fan, Chen Feng, Bo Fu, Kent Gauen,\n  Abhinav Goel, Alexander Goncharenko, Xuyang Guo, Soonhoi Ha, Andrew Howard,\n  Xiao Hu, Yuanjun Huang, Donghyun Kang, Jaeyoun Kim, Jong Gook Ko, Alexander\n  Kondratyev, Junhyeok Lee, Seungjae Lee, Suwoong Lee, Zichao Li, Zhiyu Liang,\n  Juzheng Liu, Xin Liu, Yang Lu, Yung-Hsiang Lu, Deeptanshu Malik, Hong Hanh\n  Nguyen, Eunbyung Park, Denis Repin, Liang Shen, Tao Sheng, Fei Sun, David\n  Svitov, George K. Thiruvathukal, Baiwu Zhang, Jingchi Zhang, Xiaopeng Zhang,\n  Shaojie Zhuo", "title": "Low-Power Computer Vision: Status, Challenges, Opportunities", "comments": "Preprint, Accepted by IEEE Journal on Emerging and Selected Topics in\n  Circuits and Systems. arXiv admin note: substantial text overlap with\n  arXiv:1810.01732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has achieved impressive progress in recent years. Meanwhile,\nmobile phones have become the primary computing platforms for millions of\npeople. In addition to mobile phones, many autonomous systems rely on visual\ndata for making decisions and some of these systems have limited energy (such\nas unmanned aerial vehicles also called drones and mobile robots). These\nsystems rely on batteries and energy efficiency is critical. This article\nserves two main purposes: (1) Examine the state-of-the-art for low-power\nsolutions to detect objects in images. Since 2015, the IEEE Annual\nInternational Low-Power Image Recognition Challenge (LPIRC) has been held to\nidentify the most energy-efficient computer vision solutions. This article\nsummarizes 2018 winners' solutions. (2) Suggest directions for research as well\nas opportunities for low-power computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:48:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Alyamkin", "Sergei", ""], ["Ardi", "Matthew", ""], ["Berg", "Alexander C.", ""], ["Brighton", "Achille", ""], ["Chen", "Bo", ""], ["Chen", "Yiran", ""], ["Cheng", "Hsin-Pai", ""], ["Fan", "Zichen", ""], ["Feng", "Chen", ""], ["Fu", "Bo", ""], ["Gauen", "Kent", ""], ["Goel", "Abhinav", ""], ["Goncharenko", "Alexander", ""], ["Guo", "Xuyang", ""], ["Ha", "Soonhoi", ""], ["Howard", "Andrew", ""], ["Hu", "Xiao", ""], ["Huang", "Yuanjun", ""], ["Kang", "Donghyun", ""], ["Kim", "Jaeyoun", ""], ["Ko", "Jong Gook", ""], ["Kondratyev", "Alexander", ""], ["Lee", "Junhyeok", ""], ["Lee", "Seungjae", ""], ["Lee", "Suwoong", ""], ["Li", "Zichao", ""], ["Liang", "Zhiyu", ""], ["Liu", "Juzheng", ""], ["Liu", "Xin", ""], ["Lu", "Yang", ""], ["Lu", "Yung-Hsiang", ""], ["Malik", "Deeptanshu", ""], ["Nguyen", "Hong Hanh", ""], ["Park", "Eunbyung", ""], ["Repin", "Denis", ""], ["Shen", "Liang", ""], ["Sheng", "Tao", ""], ["Sun", "Fei", ""], ["Svitov", "David", ""], ["Thiruvathukal", "George K.", ""], ["Zhang", "Baiwu", ""], ["Zhang", "Jingchi", ""], ["Zhang", "Xiaopeng", ""], ["Zhuo", "Shaojie", ""]]}, {"id": "1904.07734", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Andreas S. Tolias", "title": "Three scenarios for continual learning", "comments": "Extended version of work presented at the NeurIPS Continual Learning\n  workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard artificial neural networks suffer from the well-known issue of\ncatastrophic forgetting, making continual or lifelong learning difficult for\nmachine learning. In recent years, numerous methods have been proposed for\ncontinual learning, but due to differences in evaluation protocols it is\ndifficult to directly compare their performance. To enable more structured\ncomparisons, we describe three continual learning scenarios based on whether at\ntest time task identity is provided and--in case it is not--whether it must be\ninferred. Any sequence of well-defined tasks can be performed according to each\nscenario. Using the split and permuted MNIST task protocols, for each scenario\nwe carry out an extensive comparison of recently proposed continual learning\nmethods. We demonstrate substantial differences between the three scenarios in\nterms of difficulty and in terms of how efficient different methods are. In\nparticular, when task identity must be inferred (i.e., class incremental\nlearning), we find that regularization-based approaches (e.g., elastic weight\nconsolidation) fail and that replaying representations of previous experiences\nseems required for solving this scenario.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:22:36 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1904.07750", "submitter": "Ruohan Gao", "authors": "Ruohan Gao and Kristen Grauman", "title": "Co-Separating Sounds of Visual Objects", "comments": "ICCV 2019, Project page:\n  http://vision.cs.utexas.edu/projects/coseparation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how objects sound from video is challenging, since they often\nheavily overlap in a single audio channel. Current methods for visually-guided\naudio source separation sidestep the issue by training with artificially mixed\nvideo clips, but this puts unwieldy restrictions on training data collection\nand may even prevent learning the properties of \"true\" mixed sounds. We\nintroduce a co-separation training paradigm that permits learning object-level\nsounds from unlabeled multi-source videos. Our novel training objective\nrequires that the deep neural network's separated audio for similar-looking\nobjects be consistently identifiable, while simultaneously reproducing accurate\nvideo-level audio tracks for each source training pair. Our approach\ndisentangles sounds in realistic test videos, even in cases where an object was\nnot observed individually during training. We obtain state-of-the-art results\non visually-guided audio source separation and audio denoising for the MUSIC,\nAudioSet, and AV-Bench datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:07:50 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 21:18:03 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Gao", "Ruohan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1904.07768", "submitter": "Austin Lawson", "authors": "Yu-Min Chung and Austin Lawson", "title": "Persistence Curves: A canonical framework for summarizing persistence\n  diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams are one of the main tools in the field of Topological\nData Analysis (TDA). They contain fruitful information about the shape of data.\nThe use of machine learning algorithms on the space of persistence diagrams\nproves to be challenging as the space is complicated. For that reason,\ntransforming these diagrams in a way that is compatible with machine learning\nis an important topic currently researched in TDA. In this paper, our main\ncontribution consists of three components. First, we develop a general and\nunifying framework of vectorizing diagrams that we call the Persistence Curves\n(PCs), and show that several well-known summaries, such as Persistence\nLandscapes, fall under the PC framework. Second, we propose several new\nsummaries based on PC framework and provide a theoretical foundation for their\nstability analysis. Finally, we apply proposed PCs to two\napplications---texture classification and determining the parameters of a\ndiscrete dynamical system; their performances are competitive with other TDA\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:38:41 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 20:54:31 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 14:47:33 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Chung", "Yu-Min", ""], ["Lawson", "Austin", ""]]}, {"id": "1904.07772", "submitter": "Yifeng Fan", "authors": "Yifeng Fan, Zhizhen Zhao", "title": "Cryo-Electron Microscopy Image Analysis Using Multi-Frequency Vector\n  Diffusion Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (EM) single particle reconstruction is an entirely\ngeneral technique for 3D structure determination of macromolecular complexes.\nHowever, because the images are taken at low electron dose, it is extremely\nhard to visualize the individual particle with low contrast and high noise\nlevel. In this paper, we propose a novel approach called multi-frequency vector\ndiffusion maps (MFVDM) to improve the efficiency and accuracy of cryo-EM 2D\nimage classification and denoising. This framework incorporates different\nirreducible representations of the estimated alignment between similar images.\nIn addition, we propose a graph filtering scheme to denoise the images using\nthe eigenvalues and eigenvectors of the MFVDM matrices. Through both simulated\nand publicly available real data, we demonstrate that our proposed method is\nefficient and robust to noise compared with the state-of-the-art cryo-EM 2D\nclass averaging and image restoration algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:45:28 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Fan", "Yifeng", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "1904.07774", "submitter": "Basura Fernando", "authors": "Basura Fernando and Cheston Tan Yin Chet and Hakan Bilen", "title": "Weakly Supervised Gaussian Networks for Action Detection", "comments": "Accepted in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting temporal extents of human actions in videos is a challenging\ncomputer vision problem that requires detailed manual supervision including\nframe-level labels. This expensive annotation process limits deploying action\ndetectors to a limited number of categories. We propose a novel method, called\nWSGN, that learns to detect actions from \\emph{weak supervision}, using only\nvideo-level labels. WSGN learns to exploit both video-specific and dataset-wide\nstatistics to predict relevance of each frame to an action category. This\nstrategy leads to significant gains in action detection for two standard\nbenchmarks THUMOS14 and Charades. Our method obtains excellent results compared\nto state-of-the-art methods that uses similar features and loss functions on\nTHUMOS14 dataset. Similarly, our weakly supervised method is only 0.3% mAP\nbehind a state-of-the-art supervised method on challenging Charades dataset for\naction localization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:48:36 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 03:09:52 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 03:59:18 GMT"}, {"version": "v4", "created": "Mon, 6 Jan 2020 02:46:05 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Fernando", "Basura", ""], ["Chet", "Cheston Tan Yin", ""], ["Bilen", "Hakan", ""]]}, {"id": "1904.07776", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe,\n  Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, and Fran\\c{c}ois Piti\\'e", "title": "The ALOS Dataset for Advert Localization in Outdoor Scenes", "comments": "Published in Proc. Eleventh International Conference on Quality of\n  Multimedia Experience (QoMEX), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid increase in the number of online videos provides the marketing and\nadvertising agents ample opportunities to reach out to their audience. One of\nthe most widely used strategies is product placement, or embedded marketing,\nwherein new advertisements are integrated seamlessly into existing\nadvertisements in videos. Such strategies involve accurately localizing the\nposition of the advert in the image frame, either manually in the video editing\nphase, or by using machine learning frameworks. However, these machine learning\ntechniques and deep neural networks need a massive amount of data for training.\nIn this paper, we propose and release the first large-scale dataset of\nadvertisement billboards, captured in outdoor scenes. We also benchmark several\nstate-of-the-art semantic segmentation algorithms on our proposed dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:50:24 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Hossari", "Murhaf", ""], ["Nicholson", "Matthew", ""], ["McCabe", "Killian", ""], ["Nautiyal", "Atul", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Xu", "Wei", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1904.07793", "submitter": "Xiaosen Wang", "authors": "Xiaosen Wang, Kun He, Chuanbiao Song, Liwei Wang, John E. Hopcroft", "title": "AT-GAN: An Adversarial Generator Model for Non-constrained Adversarial\n  Examples", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid development of adversarial machine learning, most\nadversarial attack and defense researches mainly focus on the\nperturbation-based adversarial examples, which is constrained by the input\nimages. In comparison with existing works, we propose non-constrained\nadversarial examples, which are generated entirely from scratch without any\nconstraint on the input. Unlike perturbation-based attacks, or the so-called\nunrestricted adversarial attack which is still constrained by the input noise,\nwe aim to learn the distribution of adversarial examples to generate\nnon-constrained but semantically meaningful adversarial examples. Following\nthis spirit, we propose a novel attack framework called AT-GAN (Adversarial\nTransfer on Generative Adversarial Net). Specifically, we first develop a\nnormal GAN model to learn the distribution of benign data, and then transfer\nthe pre-trained GAN model to estimate the distribution of adversarial examples\nfor the target model. In this way, AT-GAN can learn the distribution of\nadversarial examples that is very close to the distribution of real data. To\nour knowledge, this is the first work of building an adversarial generator\nmodel that could produce adversarial examples directly from any input noise.\nExtensive experiments and visualizations show that the proposed AT-GAN can very\nefficiently generate diverse adversarial examples that are more realistic to\nhuman perception. In addition, AT-GAN yields higher attack success rates\nagainst adversarially trained models under white-box attack setting and\nexhibits moderate transferability against black-box models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:26:19 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 02:19:07 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 15:26:32 GMT"}, {"version": "v4", "created": "Fri, 7 Feb 2020 18:11:58 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Wang", "Xiaosen", ""], ["He", "Kun", ""], ["Song", "Chuanbiao", ""], ["Wang", "Liwei", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1904.07798", "submitter": "Jaewon Jung", "authors": "Jaewon Jung, Jongyoul Park", "title": "Visual Relationship Detection with Language prior and Softmax", "comments": "6 pages, 4 figures", "journal-ref": "Third IEEE International Conference on Image Processing,\n  Applications and Systems (IPAS 2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection is an intermediate image understanding task\nthat detects two objects and classifies a predicate that explains the\nrelationship between two objects in an image. The three components are\nlinguistically and visually correlated (e.g. \"wear\" is related to \"person\" and\n\"shirt\", while \"laptop\" is related to \"table\" and \"on\") thus, the solution\nspace is huge because there are many possible cases between them. Language and\nvisual modules are exploited and a sophisticated spatial vector is proposed.\nThe models in this work outperformed the state of arts without costly\nlinguistic knowledge distillation from a large text corpus and building complex\nloss functions. All experiments were only evaluated on Visual Relationship\nDetection and Visual Genome dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:29:52 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Jung", "Jaewon", ""], ["Park", "Jongyoul", ""]]}, {"id": "1904.07826", "submitter": "Jack Hessel", "authors": "Jack Hessel, Lillian Lee, David Mimno", "title": "Unsupervised Discovery of Multimodal Links in Multi-image,\n  Multi-sentence Documents", "comments": "Code and data available at\n  http://www.cs.cornell.edu/~jhessel/multiretrieval/multiretrieval.html", "journal-ref": "EMNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images and text co-occur constantly on the web, but explicit links between\nimages and sentences (or other intra-document textual units) are often not\npresent. We present algorithms that discover image-sentence relationships\nwithout relying on explicit multimodal annotation in training. We experiment on\nseven datasets of varying difficulty, ranging from documents consisting of\ngroups of images captioned post hoc by crowdworkers to naturally-occurring\nuser-generated multimodal documents. We find that a structured training\nobjective based on identifying whether collections of images and sentences\nco-occur in documents can suffice to predict links between specific sentences\nand specific images within the same document at test time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:07:20 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 20:43:53 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hessel", "Jack", ""], ["Lee", "Lillian", ""], ["Mimno", "David", ""]]}, {"id": "1904.07834", "submitter": "Jonathan De Matos", "authors": "Jonathan de Matos, Alceu de S. Britto Jr., Luiz E. S. Oliveira,\n  Alessandro L. Koerich", "title": "Double Transfer Learning for Breast Cancer Histopathologic Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a classification approach for breast cancer\nhistopathologic images (HI) that uses transfer learning to extract features\nfrom HI using an Inception-v3 CNN pre-trained with ImageNet dataset. We also\nuse transfer learning on training a support vector machine (SVM) classifier on\na tissue labeled colorectal cancer dataset aiming to filter the patches from a\nbreast cancer HI and remove the irrelevant ones. We show that removing\nirrelevant patches before training a second SVM classifier, improves the\naccuracy for classifying malign and benign tumors on breast cancer images. We\nare able to improve the classification accuracy in 3.7% using the feature\nextraction transfer learning and an additional 0.7% using the irrelevant patch\nelimination. The proposed approach outperforms the state-of-the-art in three\nout of the four magnification factors of the breast cancer dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:26:34 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["de Matos", "Jonathan", ""], ["Britto", "Alceu de S.", "Jr."], ["Oliveira", "Luiz E. S.", ""], ["Koerich", "Alessandro L.", ""]]}, {"id": "1904.07846", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet,\n  Andrew Zisserman", "title": "Temporal Cycle-Consistency Learning", "comments": "Accepted at CVPR 2019. Project webpage:\n  https://sites.google.com/view/temporal-cycle-consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a self-supervised representation learning method based on the\ntask of temporal alignment between videos. The method trains a network using\ntemporal cycle consistency (TCC), a differentiable cycle-consistency loss that\ncan be used to find correspondences across time in multiple videos. The\nresulting per-frame embeddings can be used to align videos by simply matching\nframes using the nearest-neighbors in the learned embedding space.\n  To evaluate the power of the embeddings, we densely label the Pouring and\nPenn Action video datasets for action phases. We show that (i) the learned\nembeddings enable few-shot classification of these action phases, significantly\nreducing the supervised training requirements; and (ii) TCC is complementary to\nother methods of self-supervised learning in videos, such as Shuffle and Learn\nand Time-Contrastive Networks. The embeddings are also used for a number of\napplications based on alignment (dense temporal correspondence) between video\npairs, including transfer of metadata of synchronized modalities between videos\n(sounds, temporal semantic labels), synchronized playback of multiple videos,\nand anomaly detection. Project webpage:\nhttps://sites.google.com/view/temporal-cycle-consistency .\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:49:50 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Aytar", "Yusuf", ""], ["Tompson", "Jonathan", ""], ["Sermanet", "Pierre", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1904.07848", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji,\n  Manmohan Chandraker", "title": "Active Adversarial Domain Adaptation", "comments": "WACV 2020 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an active learning approach for transferring representations\nacross domains. Our approach, active adversarial domain adaptation (AADA),\nexplores a duality between two related problems: adversarial domain alignment\nand importance sampling for adapting models across domains. The former uses a\ndomain discriminative model to align domains, while the latter utilizes it to\nweigh samples to account for distribution shifts. Specifically, our importance\nweight promotes samples with large uncertainty in classification and diversity\nfrom labeled examples, thus serves as a sample selection scheme for active\nlearning. We show that these two views can be unified in one framework for\ndomain adaptation and transfer learning when the source domain has many labeled\nexamples while the target domain does not. AADA provides significant\nimprovements over fine-tuning based approaches and other sampling methods when\nthe two domains are closely related. Results on challenging domain adaptation\ntasks, e.g., object detection, demonstrate that the advantage over baseline\napproaches is retained even after hundreds of examples being actively\nannotated.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:52:54 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 19:19:09 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Tsai", "Yi-Hsuan", ""], ["Sohn", "Kihyuk", ""], ["Liu", "Buyu", ""], ["Maji", "Subhransu", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1904.07850", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Dequan Wang, Philipp Kr\\\"ahenb\\\"uhl", "title": "Objects as Points", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection identifies objects as axis-aligned boxes in an image. Most\nsuccessful object detectors enumerate a nearly exhaustive list of potential\nobject locations and classify each. This is wasteful, inefficient, and requires\nadditional post-processing. In this paper, we take a different approach. We\nmodel an object as a single point --- the center point of its bounding box. Our\ndetector uses keypoint estimation to find center points and regresses to all\nother object properties, such as size, 3D location, orientation, and even pose.\nOur center point based approach, CenterNet, is end-to-end differentiable,\nsimpler, faster, and more accurate than corresponding bounding box based\ndetectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO\ndataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with\nmulti-scale testing at 1.4 FPS. We use the same approach to estimate 3D\nbounding box in the KITTI benchmark and human pose on the COCO keypoint\ndataset. Our method performs competitively with sophisticated multi-stage\nmethods and runs in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:54:26 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 16:20:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Zhou", "Xingyi", ""], ["Wang", "Dequan", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1904.07852", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Jean Kossaifi and Georgios Tzimiropoulos and Maja\n  Pantic", "title": "Matrix and tensor decompositions for training binary neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on improving the training of binary neural networks in which\nboth activations and weights are binary. While prior methods for neural network\nbinarization binarize each filter independently, we propose to instead\nparametrize the weight tensor of each layer using matrix or tensor\ndecomposition. The binarization process is then performed using this latent\nparametrization, via a quantization function (e.g. sign function) applied to\nthe reconstructed weights. A key feature of our method is that while the\nreconstruction is binarized, the computation in the latent factorized space is\ndone in the real domain. This has several advantages: (i) the latent\nfactorization enforces a coupling of the filters before binarization, which\nsignificantly improves the accuracy of the trained models. (ii) while at\ntraining time, the binary weights of each convolutional layer are parametrized\nusing real-valued matrix or tensor decomposition, during inference we simply\nuse the reconstructed (binary) weights. As a result, our method does not\nsacrifice any advantage of binary networks in terms of model compression and\nspeeding-up inference. As a further contribution, instead of computing the\nbinary weight scaling factors analytically, as in prior work, we propose to\nlearn them discriminatively via back-propagation. Finally, we show that our\napproach significantly outperforms existing methods when tested on the\nchallenging tasks of (a) human pose estimation (more than 4% improvements) and\n(b) ImageNet classification (up to 5% performance gains).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:57:27 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bulat", "Adrian", ""], ["Kossaifi", "Jean", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.07854", "submitter": "Avi Singh", "authors": "Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, Sergey\n  Levine", "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering", "comments": "Accepted to RSS 2019. 14 pages and 13 figures including references\n  and appendix. Website: https://sites.google.com/view/reward-learning-rl/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of deep neural network models and reinforcement learning\nalgorithms can make it possible to learn policies for robotic behaviors that\ndirectly read in raw sensory inputs, such as camera images, effectively\nsubsuming both estimation and control into one model. However, real-world\napplications of reinforcement learning must specify the goal of the task by\nmeans of a manually programmed reward function, which in practice requires\neither designing the very same perception pipeline that end-to-end\nreinforcement learning promises to avoid, or else instrumenting the environment\nwith additional sensors to determine if the task has been performed\nsuccessfully. In this paper, we propose an approach for removing the need for\nmanual engineering of reward specifications by enabling a robot to learn from a\nmodest number of examples of successful outcomes, followed by actively\nsolicited queries, where the robot shows the user a state and asks for a label\nto determine whether that state represents successful completion of the task.\nWhile requesting labels for every single state would amount to asking the user\nto manually provide the reward signal, our method requires labels for only a\ntiny fraction of the states seen during training, making it an efficient and\npractical approach for learning skills without manually engineered rewards. We\nevaluate our method on real-world robotic manipulation tasks where the\nobservations consist of images viewed by the robot's camera. In our\nexperiments, our method effectively learns to arrange objects, place books, and\ndrape cloth, directly from images and without any manually specified reward\nfunctions, and with only 1-4 hours of interaction with the real world.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:59:23 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 00:00:22 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Singh", "Avi", ""], ["Yang", "Larry", ""], ["Hartikainen", "Kristian", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1904.07900", "submitter": "Jonathan De Matos", "authors": "Jonathan de Matos, Alceu de Souza Britto Jr., Luiz E. S. Oliveira,\n  Alessandro L. Koerich", "title": "Histopathologic Image Processing: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathologic Images (HI) are the gold standard for evaluation of some\ntumors. However, the analysis of such images is challenging even for\nexperienced pathologists, resulting in problems of inter and intra observer.\nBesides that, the analysis is time and resource consuming. One of the ways to\naccelerate such an analysis is by using Computer Aided Diagnosis systems. In\nthis work we present a literature review about the computing techniques to\nprocess HI, including shallow and deep methods. We cover the most common tasks\nfor processing HI such as segmentation, feature extraction, unsupervised\nlearning and supervised learning. A dataset section show some datasets found\nduring the literature review. We also bring a study case of breast cancer\nclassification using a mix of deep and shallow machine learning methods. The\nproposed method obtained an accuracy of 91% in the best case, outperforming the\ncompared baseline of the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 18:04:24 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["de Matos", "Jonathan", ""], ["Britto", "Alceu de Souza", "Jr."], ["Oliveira", "Luiz E. S.", ""], ["Koerich", "Alessandro L.", ""]]}, {"id": "1904.07911", "submitter": "Yi Li", "authors": "Yi Li, Nuno Vasconcelos", "title": "REPAIR: Removing Representation Bias by Dataset Resampling", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning datasets can have biases for certain representations\nthat are leveraged by algorithms to achieve high performance without learning\nto solve the underlying task. This problem is referred to as \"representation\nbias\". The question of how to reduce the representation biases of a dataset is\ninvestigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure\nis proposed. This formulates bias minimization as an optimization problem,\nseeking a weight distribution that penalizes examples easy for a classifier\nbuilt on a given feature representation. Bias reduction is then equated to\nmaximizing the ratio between the classification loss on the reweighted dataset\nand the uncertainty of the ground-truth class labels. This is a minimax problem\nthat REPAIR solves by alternatingly updating classifier parameters and dataset\nresampling weights, using stochastic gradient descent. An experimental set-up\nis also introduced to measure the bias of any dataset for a given\nrepresentation, and the impact of this bias on the performance of recognition\nmodels. Experiments with synthetic and action recognition data show that\ndataset REPAIR can significantly reduce representation bias, and lead to\nimproved generalization of models trained on REPAIRed datasets. The tools used\nfor characterizing representation bias, and the proposed dataset REPAIR\nalgorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 18:35:40 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Yi", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1904.07916", "submitter": "Abdullah Hamdi", "authors": "Abdullah Hamdi, Bernard Ghanem", "title": "IAN: Combining Generative Adversarial Networks for Imaginative Face\n  Generation", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have gained momentum for their ability\nto model image distributions. They learn to emulate the training set and that\nenables sampling from that domain and using the knowledge learned for useful\napplications. Several methods proposed enhancing GANs, including regularizing\nthe loss with some feature matching. We seek to push GANs beyond the data in\nthe training and try to explore unseen territory in the image manifold. We\nfirst propose a new regularizer for GAN based on K-nearest neighbor (K-NN)\nselective feature matching to a target set Y in high-level feature space,\nduring the adversarial training of GAN on the base set X, and we call this\nnovel model K-GAN. We show that minimizing the added term follows from\ncross-entropy minimization between the distributions of GAN and the set Y.\nThen, We introduce a cascaded framework for GANs that try to address the task\nof imagining a new distribution that combines the base set X and target set Y\nby cascading sampling GANs with translation GANs, and we dub the cascade of\nsuch GANs as the Imaginative Adversarial Network (IAN). We conduct an objective\nand subjective evaluation for different IAN setups in the addressed task and\nshow some useful applications for these IANs, like manifold traversing and\ncreative face generation for characters' design in movies or video games.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 18:45:33 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Hamdi", "Abdullah", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.07933", "submitter": "Andr\\'es F. P\\'erez", "authors": "Andr\\'es F. P\\'erez, Valentina Sanguineti, Pietro Morerio, Vittorio\n  Murino", "title": "Audio-Visual Model Distillation Using Acoustic Images", "comments": "Accepted at WACV 2020; supplementary material at page 11; code\n  available at https://github.com/afperezm/acoustic-images-distillation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how to learn rich and robust feature\nrepresentations for audio classification from visual data and acoustic images,\na novel audio data modality. Former models learn audio representations from raw\nsignals or spectral data acquired by a single microphone, with remarkable\nresults in classification and retrieval. However, such representations are not\nso robust towards variable environmental sound conditions. We tackle this\ndrawback by exploiting a new multimodal labeled action recognition dataset\nacquired by a hybrid audio-visual sensor that provides RGB video, raw audio\nsignals, and spatialized acoustic data, also known as acoustic images, where\nthe visual and acoustic images are aligned in space and synchronized in time.\nUsing this richer information, we train audio deep learning models in a\nteacher-student fashion. In particular, we distill knowledge into audio\nnetworks from both visual and acoustic image teachers. Our experiments suggest\nthat the learned representations are more powerful and have better\ngeneralization capabilities than the features learned from models trained using\njust single-microphone audio data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 19:15:00 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 11:01:28 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["P\u00e9rez", "Andr\u00e9s F.", ""], ["Sanguineti", "Valentina", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1904.07934", "submitter": "David Acuna", "authors": "David Acuna, Amlan Kar, Sanja Fidler", "title": "Devil is in the Edges: Learning Semantic Boundaries from Noisy\n  Annotations", "comments": "Accepted as a CVPR 2019 oral paper (Project Page:\n  https://nv-tlabs.github.io/STEAL/)", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of semantic boundary prediction, which aims to identify\npixels that belong to object(class) boundaries. We notice that relevant\ndatasets consist of a significant level of label noise, reflecting the fact\nthat precise annotations are laborious to get and thus annotators trade-off\nquality with efficiency. We aim to learn sharp and precise semantic boundaries\nby explicitly reasoning about annotation noise during training. We propose a\nsimple new layer and loss that can be used with existing learning-based\nboundary detectors. Our layer/loss enforces the detector to predict a maximum\nresponse along the normal direction at an edge, while also regularizing its\ndirection. We further reason about true object boundaries during training using\na level set formulation, which allows the network to learn from misaligned\nlabels in an end-to-end fashion. Experiments show that we improve over the\nCASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in\nterms of AP, outperforming all current state-of-the-art methods including those\nthat deal with alignment. Furthermore, we show that our learned network can be\nused to significantly improve coarse segmentation labels, lending itself as an\nefficient way to label new data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 19:16:57 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 17:30:52 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Acuna", "David", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1904.07936", "submitter": "Joshua Dickey", "authors": "Joshua Dickey, Brett Borghetti, William Junek, Richard Martin", "title": "Beyond Correlation: A Path-Invariant Measure for Seismogram Similarity", "comments": null, "journal-ref": "Seismological Research Letters 2019", "doi": "10.1785/0220190090", "report-no": null, "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a popular technique for seismic signal processing, with\ntemplate matching, matched filters and subspace detectors being utilized for a\nwide variety of tasks, including both signal detection and source\ndiscrimination. Traditionally, these techniques rely on the cross-correlation\nfunction as the basis for measuring similarity. Unfortunately, seismogram\ncorrelation is dominated by path effects, essentially requiring a distinct\nwaveform template along each path of interest. To address this limitation, we\npropose a novel measure of seismogram similarity that is explicitly invariant\nto path. Using Earthscope's USArray experiment, a path-rich dataset of 207,291\nregional seismograms across 8,452 unique events is constructed, and then\nemployed via the batch-hard triplet loss function, to train a deep\nconvolutional neural network which maps raw seismograms to a low dimensional\nembedding space, where nearness on the space corresponds to nearness of source\nfunction, regardless of path or recording instrumentation. This path-agnostic\nembedding space forms a new representation for seismograms, characterized by\nrobust, source-specific features, which we show to be useful for performing\nboth pairwise event association as well as template-based source discrimination\nwith a single template.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 19:22:00 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 16:06:57 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 15:39:34 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2019 18:48:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Dickey", "Joshua", ""], ["Borghetti", "Brett", ""], ["Junek", "William", ""], ["Martin", "Richard", ""]]}, {"id": "1904.07950", "submitter": "Ziqiang Guan", "authors": "Ziqiang Guan and Ritesh Kumar and Yi Ren Fung and Yeahuay Wu and\n  Madalina Fiterau", "title": "A Comprehensive Study of Alzheimer's Disease Classification Using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of deep learning models have been developed for the task of\nAlzheimer's disease classification from brain MRI scans. Many of these models\nreport high performance, achieving three-class classification accuracy of up to\n95%. However, it is common for these studies to draw performance comparisons\nbetween models that are trained on different subsets of a dataset or use\nvarying imaging preprocessing techniques, making it difficult to objectively\nassess model performance. Furthermore, many of these works do not provide\ndetails such as hyperparameters, the specific MRI scans used, or their source\ncode, making it difficult to replicate their experiments. To address these\nconcerns, we present a comprehensive study of some of the deep learning methods\nand architectures on the full set of images available from ADNI. We find that,\n(1) classification using 3D models gives an improvement of 1% in our setup, at\nthe cost of significantly longer training time and more computation power, (2)\nwith our dataset, pre-training yields minimal ($<0.5\\%$) improvement in model\nperformance, (3) most popular convolutional neural network models yield similar\nperformance when compared to each other. Lastly, we briefly compare the effects\nof two image preprocessing programs: FreeSurfer and Clinica, and find that the\nspatially normalized and segmented outputs from Clinica increased the accuracy\nof model prediction from 63% to 89% when compared to FreeSurfer images.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:00:15 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Guan", "Ziqiang", ""], ["Kumar", "Ritesh", ""], ["Fung", "Yi Ren", ""], ["Wu", "Yeahuay", ""], ["Fiterau", "Madalina", ""]]}, {"id": "1904.07967", "submitter": "Mubariz Zaffar", "authors": "Mubariz Zaffar, Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Kostas\n  Alexis, Klaus McDonald-Maier", "title": "Are State-of-the-art Visual Place Recognition Techniques any Good for\n  Aerial Robotics?", "comments": "IEEE ICRA 2019 Workshop on Aerial Robotics 8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) has seen significant advances at the frontiers\nof matching performance and computational superiority over the past few years.\nHowever, these evaluations are performed for ground-based mobile platforms and\ncannot be generalized to aerial platforms. The degree of viewpoint variation\nexperienced by aerial robots is complex, with their processing power and\non-board memory limited by payload size and battery ratings. Therefore, in this\npaper, we collect $8$ state-of-the-art VPR techniques that have been previously\nevaluated for ground-based platforms and compare them on $2$ recently proposed\naerial place recognition datasets with three prime focuses: a) Matching\nperformance b) Processing power consumption c) Projected memory requirements.\nThis gives a birds-eye view of the applicability of contemporary VPR research\nto aerial robotics and lays down the the nature of challenges for aerial-VPR.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:34:39 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 18:21:04 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Zaffar", "Mubariz", ""], ["Khaliq", "Ahmad", ""], ["Ehsan", "Shoaib", ""], ["Milford", "Michael", ""], ["Alexis", "Kostas", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1904.07969", "submitter": "Lana Sinapayen", "authors": "Lana Sinapayen, Atsushi Noda", "title": "DNN Architecture for High Performance Prediction on Natural Videos Loses\n  Submodule's Ability to Learn Discrete-World Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is cognition a collection of loosely connected functions tuned to different\ntasks, or can there be a general learning algorithm? If such an hypothetical\ngeneral algorithm did exist, tuned to our world, could it adapt seamlessly to a\nworld with different laws of nature? We consider the theory that predictive\ncoding is such a general rule, and falsify it for one specific neural\narchitecture known for high-performance predictions on natural videos and\nreplication of human visual illusions: PredNet. Our results show that PredNet's\nhigh performance generalizes without retraining on a completely different\nnatural video dataset. Yet PredNet cannot be trained to reach even mediocre\naccuracy on an artificial video dataset created with the rules of the Game of\nLife (GoL). We also find that a submodule of PredNet, a Convolutional Neural\nNetwork trained alone, reaches perfect accuracy on the GoL while being mediocre\nfor natural videos, showing that PredNet's architecture itself is responsible\nfor both the high performance on natural videos and the loss of performance on\nthe GoL. Just as humans cannot predict the dynamics of the GoL, our results\nsuggest that there might be a trade-off between high performance on sensory\ninputs with different sets of rules.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:35:09 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Sinapayen", "Lana", ""], ["Noda", "Atsushi", ""]]}, {"id": "1904.07979", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Atul Nautiyal, Yee Hui Lee, and Stefan Winkler", "title": "CloudSegNet: A Deep Network for Nychthemeron Cloud Image Segmentation", "comments": "Published in IEEE Geoscience and Remote Sensing Letters, 2019", "journal-ref": null, "doi": "10.1109/LGRS.2019.2912140", "report-no": null, "categories": "physics.ao-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze clouds in the earth's atmosphere using ground-based sky cameras.\nAn accurate segmentation of clouds in the captured sky/cloud image is\ndifficult, owing to the fuzzy boundaries of clouds. Several techniques have\nbeen proposed that use color as the discriminatory feature for cloud detection.\nIn the existing literature, however, analysis of daytime and nighttime images\nis considered separately, mainly because of differences in image\ncharacteristics and applications. In this paper, we propose a light-weight\ndeep-learning architecture called CloudSegNet. It is the first that integrates\ndaytime and nighttime (also known as nychthemeron) image segmentation in a\nsingle framework, and achieves state-of-the-art results on public databases.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:49:20 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Nautiyal", "Atul", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1904.08008", "submitter": "Fan Yang", "authors": "Fan Yang, Heng Fan, Peng Chu, Erik Blasch, Haibin Ling", "title": "Clustered Object Detection in Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects in aerial images is challenging for at least two reasons:\n(1) target objects like pedestrians are very small in pixels, making them\nhardly distinguished from surrounding background; and (2) targets are in\ngeneral sparsely and non-uniformly distributed, making the detection very\ninefficient. In this paper, we address both issues inspired by observing that\nthese targets are often clustered. In particular, we propose a Clustered\nDetection (ClusDet) network that unifies object clustering and detection in an\nend-to-end framework. The key components in ClusDet include a cluster proposal\nsub-network (CPNet), a scale estimation sub-network (ScaleNet), and a dedicated\ndetection network (DetecNet). Given an input image, CPNet produces object\ncluster regions and ScaleNet estimates object scales for these regions. Then,\neach scale-normalized cluster region is fed into DetecNet for object detection.\nClusDet has several advantages over previous solutions: (1) it greatly reduces\nthe number of chips for final object detection and hence achieves high running\ntime efficiency, (2) the cluster-based scale estimation is more accurate than\npreviously used single-object based ones, hence effectively improves the\ndetection for small objects, and (3) the final DetecNet is dedicated for\nclustered regions and implicitly models the prior context information so as to\nboost detection accuracy. The proposed method is tested on three popular aerial\nimage datasets including VisDrone, UAVDT and DOTA. In all experiments, ClusDet\nachieves promising performance in comparison with state-of-the-art detectors.\nCode will be available in \\url{https://github.com/fyangneil}.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:01:53 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 00:41:48 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 01:43:34 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Yang", "Fan", ""], ["Fan", "Heng", ""], ["Chu", "Peng", ""], ["Blasch", "Erik", ""], ["Ling", "Haibin", ""]]}, {"id": "1904.08017", "submitter": "Artem Komarichev", "authors": "Artem Komarichev, Zichun Zhong, Jing Hua", "title": "A-CNN: Annularly Convolutional Neural Networks on Point Clouds", "comments": "17 pages, 14 figures. To appear, Proceedings of the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR), June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the geometric and semantic properties of 3D point clouds through\nthe deep networks is still challenging due to the irregularity and sparsity of\nsamplings of their geometric structures. This paper presents a new method to\ndefine and compute convolution directly on 3D point clouds by the proposed\nannular convolution. This new convolution operator can better capture the local\nneighborhood geometry of each point by specifying the (regular and dilated)\nring-shaped structures and directions in the computation. It can adapt to the\ngeometric variability and scalability at the signal processing level. We apply\nit to the developed hierarchical neural networks for object classification,\npart segmentation, and semantic segmentation in large-scale scenes. The\nextensive experiments and comparisons demonstrate that our approach outperforms\nthe state-of-the-art methods on a variety of standard benchmark datasets (e.g.,\nModelNet10, ModelNet40, ShapeNet-part, S3DIS, and ScanNet).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:34:55 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Komarichev", "Artem", ""], ["Zhong", "Zichun", ""], ["Hua", "Jing", ""]]}, {"id": "1904.08034", "submitter": "Brenden Lake", "authors": "Brenden M. Lake and Steven T. Piantadosi", "title": "People infer recursive visual concepts from just a few examples", "comments": "In press at \"Computational Brain & Behavior\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has made major advances in categorizing objects in images,\nyet the best algorithms miss important aspects of how people learn and think\nabout categories. People can learn richer concepts from fewer examples,\nincluding causal models that explain how members of a category are formed.\nHere, we explore the limits of this human ability to infer causal \"programs\" --\nlatent generating processes with nontrivial algorithmic properties -- from one,\ntwo, or three visual examples. People were asked to extrapolate the programs in\nseveral ways, for both classifying and generating new examples. As a theory of\nthese inductive abilities, we present a Bayesian program learning model that\nsearches the space of programs for the best explanation of the observations.\nAlthough variable, people's judgments are broadly consistent with the model and\ninconsistent with several alternatives, including a pre-trained deep neural\nnetwork for object recognition, indicating that people can learn and reason\nwith rich algorithmic abstractions from sparse input data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 00:45:05 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 15:26:40 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lake", "Brenden M.", ""], ["Piantadosi", "Steven T.", ""]]}, {"id": "1904.08056", "submitter": "Lei Liu", "authors": "Lei Liu, Jie Jiang, Wenjing Jia, Saeed Amirgholipour, Michelle\n  Zeibots, Xiangjian He", "title": "DENet: A Universal Network for Counting Crowd with Varying Densities and\n  Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting people or objects with significantly varying scales and densities\nhas attracted much interest from the research community and yet it remains an\nopen problem. In this paper, we propose a simple but an efficient and effective\nnetwork, named DENet, which is composed of two components, i.e., a detection\nnetwork (DNet) and an encoder-decoder estimation network (ENet). We first run\nDNet on an input image to detect and count individuals who can be segmented\nclearly. Then, ENet is utilized to estimate the density maps of the remaining\nareas, where the numbers of individuals cannot be detected. We propose a\nmodified Xception as an encoder for feature extraction and a combination of\ndilated convolution and transposed convolution as a decoder. In the\nShanghaiTech Part A, UCF and WorldExpo'10 datasets, our DENet achieves lower\nMean Absolute Error (MAE) than those of the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 02:34:04 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Liu", "Lei", ""], ["Jiang", "Jie", ""], ["Jia", "Wenjing", ""], ["Amirgholipour", "Saeed", ""], ["Zeibots", "Michelle", ""], ["He", "Xiangjian", ""]]}, {"id": "1904.08060", "submitter": "Pengfei Xiong", "authors": "Xin Hong and Pengfei Xiong and Renhe Ji and Haoqiang Fan", "title": "Deep Fusion Network for Image Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image completion usually fails to harmonically blend the restored image\ninto existing content, especially in the boundary area. This paper handles with\nthis problem from a new perspective of creating a smooth transition and\nproposes a concise Deep Fusion Network (DFNet). Firstly, a fusion block is\nintroduced to generate a flexible alpha composition map for combining known and\nunknown regions. The fusion block not only provides a smooth fusion between\nrestored and existing content, but also provides an attention map to make\nnetwork focus more on the unknown pixels. In this way, it builds a bridge for\nstructural and texture information, so that information can be naturally\npropagated from known region into completion. Furthermore, fusion blocks are\nembedded into several decoder layers of the network. Accompanied by the\nadjustable loss constraints on each layer, more accurate structure information\nare achieved. We qualitatively and quantitatively compare our method with other\nstate-of-the-art methods on Places2 and CelebA datasets. The results show the\nsuperior performance of DFNet, especially in the aspects of harmonious texture\ntransition, texture detail and semantic structural consistency. Our source code\nwill be avaiable at: \\url{https://github.com/hughplay/DFNet}\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 02:48:02 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Hong", "Xin", ""], ["Xiong", "Pengfei", ""], ["Ji", "Renhe", ""], ["Fan", "Haoqiang", ""]]}, {"id": "1904.08064", "submitter": "Feng Li", "authors": "Xixi Li, Yanfei Kang, Feng Li", "title": "Forecasting with time series imaging", "comments": null, "journal-ref": "Expert Systems with Applications (2020)", "doi": "10.1016/j.eswa.2020.113680", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based time series representations have attracted substantial\nattention in a wide range of time series analysis methods. Recently, the use of\ntime series features for forecast model averaging has been an emerging research\nfocus in the forecasting community. Nonetheless, most of the existing\napproaches depend on the manual choice of an appropriate set of features.\nExploiting machine learning methods to extract features from time series\nautomatically becomes crucial in state-of-the-art time series analysis. In this\npaper, we introduce an automated approach to extract time series features based\non time series imaging. We first transform time series into recurrence plots,\nfrom which local features can be extracted using computer vision algorithms.\nThe extracted features are used for forecast model averaging. Our experiments\nshow that forecasting based on automatically extracted features, with less\nhuman intervention and a more comprehensive view of the raw time series data,\nyields highly comparable performances with the best methods in the largest\nforecasting competition dataset (M4) and outperforms the top methods in the\nTourism forecasting competition dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:18:45 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 03:33:57 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 03:13:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Xixi", ""], ["Kang", "Yanfei", ""], ["Li", "Feng", ""]]}, {"id": "1904.08066", "submitter": "Roghayeh Barmaki", "authors": "Zhang Guo, Kevin Yu, Rebecca Pearlman, Nassir Navab, and Roghayeh\n  Barmaki", "title": "Collaboration Analysis Using Deep Learning", "comments": "6 pages, 4 Figures and a Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the collaborative learning process is one of the growing\nfields of education research, which has many different analytic solutions. In\nthis paper, we provided a new solution to improve automated collaborative\nlearning analyses using deep neural networks. Instead of using self-reported\nquestionnaires, which are subject to bias and noise, we automatically extract\ngroup-working information by object recognition results using Mask R-CNN\nmethod. This process is based on detecting the people and other objects from\npictures and video clips of the collaborative learning process, then evaluate\nthe mobile learning performance using the collaborative indicators. We tested\nour approach to automatically evaluate the group-work collaboration in a\ncontrolled study of thirty-three dyads while performing an anatomy body\npainting intervention. The results indicate that our approach recognizes the\ndifferences of collaborations among teams of treatment and control groups in\nthe case study. This work introduces new methods for automated quality\nprediction of collaborations among human-human interactions using computer\nvision techniques.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:23:17 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Guo", "Zhang", ""], ["Yu", "Kevin", ""], ["Pearlman", "Rebecca", ""], ["Navab", "Nassir", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "1904.08080", "submitter": "Ahmed Abbas", "authors": "Ahmed Abbas, Paul Swoboda (Max Planck Institute for Informatics,\n  Saarbr\\\"ucken)", "title": "Bottleneck potentials in Markov Random Fields", "comments": "Published in ICCV 2019 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider general discrete Markov Random Fields(MRFs) with additional\nbottleneck potentials which penalize the maximum (instead of the sum) over\nlocal potential value taken by the MRF-assignment. Bottleneck potentials or\nanalogous constructions have been considered in (i) combinatorial optimization\n(e.g. bottleneck shortest path problem, the minimum bottleneck spanning tree\nproblem, bottleneck function minimization in greedoids), (ii) inverse problems\nwith $L_{\\infty}$-norm regularization, and (iii) valued constraint satisfaction\non the $(\\min,\\max)$-pre-semirings. Bottleneck potentials for general discrete\nMRFs are a natural generalization of the above direction of modeling work to\nMaximum-A-Posteriori (MAP) inference in MRFs. To this end, we propose MRFs\nwhose objective consists of two parts: terms that factorize according to (i)\n$(\\min,+)$, i.e. potentials as in plain MRFs, and (ii) $(\\min,\\max)$, i.e.\nbottleneck potentials. To solve the ensuing inference problem, we propose\nhigh-quality relaxations and efficient algorithms for solving them. We\nempirically show efficacy of our approach on large scale seismic horizon\ntracking problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 04:43:38 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 19:45:59 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Abbas", "Ahmed", "", "Max Planck Institute for Informatics,\n  Saarbr\u00fccken"], ["Swoboda", "Paul", "", "Max Planck Institute for Informatics,\n  Saarbr\u00fccken"]]}, {"id": "1904.08084", "submitter": "Sheryl Brahnam", "authors": "L. Nanni, S. Brahnam, S. Ghidoni, and G. Maguolo", "title": "General Purpose (GenP) Bioimage Ensemble of Handcrafted and Learned\n  Features with Data Augmentation", "comments": "27 pages, 1 figure, 5 tables, manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioimage classification plays a crucial role in many biological problems. In\nthis work, we present a new General Purpose (GenP) ensemble that boosts\nperformance by combining local features, dense sampling features, and deep\nlearning approaches. First, we introduce three new methods for data\naugmentation based on PCA/DCT; second, we show that different data augmentation\napproaches can boost the performance of an ensemble of CNNs; and, finally, we\npropose a set of handcrafted/learned descriptors that are highly generalizable.\nEach handcrafted descriptor is used to train a different Support Vector Machine\n(SVM), and the different SVMs are combined with the ensemble of CNNs. Our\nmethod is evaluated on a diverse set of bioimage classification problems.\nResults demonstrate that the proposed GenP bioimage ensemble obtains\nstate-of-the-art performance without any ad-hoc dataset tuning of parameters\n(thus avoiding the risk of overfitting/overtraining).\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 05:11:55 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:02:29 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 05:51:58 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 05:31:12 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nanni", "L.", ""], ["Brahnam", "S.", ""], ["Ghidoni", "S.", ""], ["Maguolo", "G.", ""]]}, {"id": "1904.08089", "submitter": "Yuxian Qiu", "authors": "Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo and\n  Yuhao Zhu", "title": "Adversarial Defense Through Network Profiling Based Path Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have started decomposing deep neural network models\naccording to their semantics or functions. Recent work has shown the\neffectiveness of decomposed functional blocks for defending adversarial\nattacks, which add small input perturbation to the input image to fool the DNN\nmodels. This work proposes a profiling-based method to decompose the DNN models\nto different functional blocks, which lead to the effective path as a new\napproach to exploring DNNs' internal organization. Specifically, the per-image\neffective path can be aggregated to the class-level effective path, through\nwhich we observe that adversarial images activate effective path different from\nnormal images. We propose an effective path similarity-based method to detect\nadversarial images with an interpretable model, which achieve better accuracy\nand broader applicability than the state-of-the-art technique.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 05:51:24 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 04:40:40 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Qiu", "Yuxian", ""], ["Leng", "Jingwen", ""], ["Guo", "Cong", ""], ["Chen", "Quan", ""], ["Li", "Chao", ""], ["Guo", "Minyi", ""], ["Zhu", "Yuhao", ""]]}, {"id": "1904.08095", "submitter": "Vinoj Yasanga Jayasundara Magalle Hewa Mr.", "authors": "Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Jathushan\n  Rajasegaran, Suranga Seneviratne, Ranga Rodrigo", "title": "TextCaps : Handwritten Character Recognition with Very Small Datasets", "comments": null, "journal-ref": "In 2019 IEEE Winter Conference on Applications of Computer Vision\n  (WACV) (pp. 254-262). IEEE 2019", "doi": "10.1109/WACV.2019.00033", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many localized languages struggle to reap the benefits of recent advancements\nin character recognition systems due to the lack of substantial amount of\nlabeled training data. This is due to the difficulty in generating large\namounts of labeled data for such languages and inability of deep learning\ntechniques to properly learn from small number of training samples. We solve\nthis problem by introducing a technique of generating new training samples from\nthe existing samples, with realistic augmentations which reflect actual\nvariations that are present in human hand writing, by adding random controlled\nnoise to their corresponding instantiation parameters. Our results with a mere\n200 training samples per class surpass existing character recognition results\nin the EMNIST-letter dataset while achieving the existing results in the three\ndatasets: EMNIST-balanced, EMNIST-digits, and MNIST. We also develop a strategy\nto effectively use a combination of loss functions to improve reconstructions.\nOur system is useful in character recognition for localized languages that lack\nmuch labeled training data and even in other related more general contexts such\nas object recognition.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:09:59 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Jayasundara", "Vinoj", ""], ["Jayasekara", "Sandaru", ""], ["Jayasekara", "Hirunima", ""], ["Rajasegaran", "Jathushan", ""], ["Seneviratne", "Suranga", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "1904.08098", "submitter": "Dacheng Tao", "authors": "Qiang Li, Bo Xie, Jane You, Wei Bian, Dacheng Tao", "title": "Correlated Logistic Model With Elastic Net Regularization for Multilabel\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present correlated logistic (CorrLog) model for multilabel\nimage classification. CorrLog extends conventional logistic regression model\ninto multilabel cases, via explicitly modeling the pairwise correlation between\nlabels. In addition, we propose to learn the model parameters of CorrLog with\nelastic net regularization, which helps exploit the sparsity in feature\nselection and label correlations and thus further boost the performance of\nmultilabel classification. CorrLog can be efficiently learned, though\napproximately, by regularized maximum pseudo likelihood estimation, and it\nenjoys a satisfying generalization bound that is independent of the number of\nlabels. CorrLog performs competitively for multilabel image classification on\nbenchmark data sets MULAN scene, MIT outdoor scene, PASCAL VOC 2007, and PASCAL\nVOC 2012, compared with the state-of-the-art multilabel classification\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:16:59 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Qiang", ""], ["Xie", "Bo", ""], ["You", "Jane", ""], ["Bian", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.08103", "submitter": "Qingshan Xu", "authors": "Qingshan Xu and Wenbing Tao", "title": "Multi-Scale Geometric Consistency Guided Multi-View Stereo", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient multi-scale geometric consistency\nguided multi-view stereo method for accurate and complete depth map estimation.\nWe first present our basic multi-view stereo method with Adaptive Checkerboard\nsampling and Multi-Hypothesis joint view selection (ACMH). It leverages\nstructured region information to sample better candidate hypotheses for\npropagation and infer the aggregation view subset at each pixel. For the depth\nestimation of low-textured areas, we further propose to combine ACMH with\nmulti-scale geometric consistency guidance (ACMM) to obtain the reliable depth\nestimates for low-textured areas at coarser scales and guarantee that they can\nbe propagated to finer scales. To correct the erroneous estimates propagated\nfrom the coarser scales, we present a novel detail restorer. Experiments on\nextensive datasets show our method achieves state-of-the-art performance,\nrecovering the depth estimation not only in low-textured areas but also in\ndetails.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:36:44 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Xu", "Qingshan", ""], ["Tao", "Wenbing", ""]]}, {"id": "1904.08105", "submitter": "Matthias Ochs", "authors": "Robin Kreuzig, Matthias Ochs, Rudolf Mester", "title": "DistanceNet: Estimating Traveled Distance from Monocular Images using a\n  Recurrent Convolutional Neural Network", "comments": "Paper is accepted at CVPR 2019 - Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical monocular vSLAM/VO methods suffer from the scale ambiguity problem.\nHybrid approaches solve this problem by adding deep learning methods, for\nexample by using depth maps which are predicted by a CNN. We suggest that it is\nbetter to base scale estimation on estimating the traveled distance for a set\nof subsequent images. In this paper, we propose a novel end-to-end many-to-one\ntraveled distance estimator. By using a deep recurrent convolutional neural\nnetwork (RCNN), the traveled distance between the first and last image of a set\nof consecutive frames is estimated by our DistanceNet. Geometric features are\nlearned in the CNN part of our model, which are subsequently used by the RNN to\nlearn dynamics and temporal information. Moreover, we exploit the natural order\nof distances by using ordinal regression to predict the distance. The\nevaluation on the KITTI dataset shows that our approach outperforms current\nstate-of-the-art deep learning pose estimators and classical mono vSLAM/VO\nmethods in terms of distance prediction. Thus, our DistanceNet can be used as a\ncomponent to solve the scale problem and help improve current and future\nclassical mono vSLAM/VO methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:38:00 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Kreuzig", "Robin", ""], ["Ochs", "Matthias", ""], ["Mester", "Rudolf", ""]]}, {"id": "1904.08118", "submitter": "Jingwen He", "authors": "Jingwen He, Chao Dong, Yu Qiao", "title": "Modulating Image Restoration with Continual Levels via Adaptive Feature\n  Modification Layers", "comments": "Accepted by CVPR 2019 (oral); code is available:\n  https://github.com/hejingwenhejingwen/AdaFM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image restoration tasks, like denoising and super resolution, continual\nmodulation of restoration levels is of great importance for real-world\napplications, but has failed most of existing deep learning based image\nrestoration methods. Learning from discrete and fixed restoration levels, deep\nmodels cannot be easily generalized to data of continuous and unseen levels.\nThis topic is rarely touched in literature, due to the difficulty of modulating\nwell-trained models with certain hyper-parameters. We make a step forward by\nproposing a unified CNN framework that consists of few additional parameters\nthan a single-level model yet could handle arbitrary restoration levels between\na start and an end level. The additional module, namely AdaFM layer, performs\nchannel-wise feature modification, and can adapt a model to another restoration\nlevel with high accuracy. By simply tweaking an interpolation coefficient, the\nintermediate model - AdaFM-Net could generate smooth and continuous restoration\neffects without artifacts. Extensive experiments on three image restoration\ntasks demonstrate the effectiveness of both model training and modulation\ntesting. Besides, we carefully investigate the properties of AdaFM layers,\nproviding a detailed guidance on the usage of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:10:22 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 08:03:29 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 02:55:31 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["He", "Jingwen", ""], ["Dong", "Chao", ""], ["Qiao", "Yu", ""]]}, {"id": "1904.08128", "submitter": "Fabian Isensee", "authors": "Fabian Isensee, Paul F. J\\\"ager, Simon A. A. Kohl, Jens Petersen,\n  Klaus H. Maier-Hein", "title": "Automated Design of Deep Learning Methods for Biomedical Image\n  Segmentation", "comments": "* Fabian Isensee and Paul F. J\\\"ager share the first authorship", "journal-ref": "Nature Methods (2020)", "doi": "10.1038/s41592-020-01008-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical imaging is a driver of scientific discovery and core component of\nmedical care, currently stimulated by the field of deep learning. While\nsemantic segmentation algorithms enable 3D image analysis and quantification in\nmany applications, the design of respective specialised solutions is\nnon-trivial and highly dependent on dataset properties and hardware conditions.\nWe propose nnU-Net, a deep learning framework that condenses the current domain\nknowledge and autonomously takes the key decisions required to transfer a basic\narchitecture to different datasets and segmentation tasks. Without manual\ntuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public\ninternational competitions and sets a new state of the art in the majority of\nthe 49 tasks. The results demonstrate a vast hidden potential in the systematic\nadaptation of deep learning methods to different datasets. We make nnU-Net\npublicly available as an open-source tool that can effectively be used\nout-of-the-box, rendering state of the art segmentation accessible to\nnon-experts and catalyzing scientific progress as a framework for automated\nmethod design.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:30:17 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 13:32:30 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Isensee", "Fabian", ""], ["J\u00e4ger", "Paul F.", ""], ["Kohl", "Simon A. A.", ""], ["Petersen", "Jens", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1904.08141", "submitter": "Shuangjie Xu", "authors": "Shuangjie Xu, Daizong Liu, Linchao Bao, Wei Liu and Pan Zhou", "title": "MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation", "comments": "accepted to CVPR 2019 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semi-supervised video object segmentation (VOS),\nwhere the masks of objects of interests are given in the first frame of an\ninput video. To deal with challenging cases where objects are occluded or\nmissing, previous work relies on greedy data association strategies that make\ndecisions for each frame individually. In this paper, we propose a novel\napproach to defer the decision making for a target object in each frame, until\na global view can be established with the entire video being taken into\nconsideration. Our approach is in the same spirit as Multiple Hypotheses\nTracking (MHT) methods, making several critical adaptations for the VOS\nproblem. We employ the bounding box (bbox) hypothesis for tracking tree\nformation, and the multiple hypotheses are spawned by propagating the preceding\nbbox into the detected bbox proposals within a gated region starting from the\ninitial object mask in the first frame. The gated region is determined by a\ngating scheme which takes into account a more comprehensive motion model rather\nthan the simple Kalman filtering model in traditional MHT. To further design\nmore customized algorithms tailored for VOS, we develop a novel mask\npropagation score instead of the appearance similarity score that could be\nbrittle due to large deformations. The mask propagation score, together with\nthe motion score, determines the affinity between the hypotheses during tree\npruning. Finally, a novel mask merging strategy is employed to handle mask\nconflicts between objects. Extensive experiments on challenging datasets\ndemonstrate the effectiveness of the proposed method, especially in the case of\nobject missing.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:52:03 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Xu", "Shuangjie", ""], ["Liu", "Daizong", ""], ["Bao", "Linchao", ""], ["Liu", "Wei", ""], ["Zhou", "Pan", ""]]}, {"id": "1904.08155", "submitter": "Dominique Vaufreydaz", "authors": "Justin Le Louedec (PERVASIVE), Thomas Guntz (PERVASIVE), James Crowley\n  (PERVASIVE), Dominique Vaufreydaz (PERVASIVE)", "title": "Deep learning investigation for chess player attention prediction using\n  eye-tracking and game data", "comments": null, "journal-ref": "ACM Symposium On Eye Tracking Research & Applications (ETRA 2019),\n  Jun 2019, Denver, United States.", "doi": "10.1145/3314111.3319827", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reports on an investigation of the use of convolutional neural\nnetworks to predict the visual attention of chess players. The visual attention\nmodel described in this article has been created to generate saliency maps that\ncapture hierarchical and spatial features of chessboard, in order to predict\nthe probability fixation for individual pixels Using a skip-layer architecture\nof an autoencoder, with a unified decoder, we are able to use multiscale\nfeatures to predict saliency of part of the board at different scales, showing\nmultiple relations between pieces. We have used scan path and fixation data\nfrom players engaged in solving chess problems, to compute 6600 saliency maps\nassociated to the corresponding chess piece configurations. This corpus is\ncompleted with synthetically generated data from actual games gathered from an\nonline chess platform. Experiments realized using both scan-paths from chess\nplayers and the CAT2000 saliency dataset of natural images, highlights several\nresults. Deep features, pretrained on natural images, were found to be helpful\nin training visual attention prediction for chess. The proposed neural network\narchitecture is able to generate meaningful saliency maps on unseen chess\nconfigurations with good scores on standard metrics. This work provides a\nbaseline for future work on visual attention prediction in similar contexts.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:31:37 GMT"}], "update_date": "2019-04-21", "authors_parsed": [["Louedec", "Justin Le", "", "PERVASIVE"], ["Guntz", "Thomas", "", "PERVASIVE"], ["Crowley", "James", "", "PERVASIVE"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"]]}, {"id": "1904.08159", "submitter": "Daniel Koguciuk M.Sc.Eng.", "authors": "Daniel Koguciuk and {\\L}ukasz Chechli\\'nski and Tarek El-Gaaly", "title": "3D Object Recognition with Ensemble Learning --- A Study of Point\n  Cloud-Based Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present an analysis of model-based ensemble learning for 3D\npoint-cloud object classification and detection. An ensemble of multiple model\ninstances is known to outperform a single model instance, but there is little\nstudy of the topic of ensemble learning for 3D point clouds. First, an ensemble\nof multiple model instances trained on the same part of the\n$\\textit{ModelNet40}$ dataset was tested for seven deep learning, point\ncloud-based classification algorithms: $\\textit{PointNet}$,\n$\\textit{PointNet++}$, $\\textit{SO-Net}$, $\\textit{KCNet}$,\n$\\textit{DeepSets}$, $\\textit{DGCNN}$, and $\\textit{PointCNN}$. Second, the\nensemble of different architectures was tested. Results of our experiments show\nthat the tested ensemble learning methods improve over state-of-the-art on the\n$\\textit{ModelNet40}$ dataset, from $92.65\\%$ to $93.64\\%$ for the ensemble of\nsingle architecture instances, $94.03\\%$ for two different architectures, and\n$94.15\\%$ for five different architectures. We show that the ensemble of two\nmodels with different architectures can be as effective as the ensemble of 10\nmodels with the same architecture. Third, a study on classic bagging i.e. with\ndifferent subsets used for training multiple model instances) was tested and\nsources of ensemble accuracy growth were investigated for best-performing\narchitecture, i.e. $\\textit{SO-Net}$. We also investigate the ensemble learning\nof $\\textit{Frustum PointNet}$ approach in the task of 3D object detection,\nincreasing the average precision of 3D box detection on the $\\textit{KITTI}$\ndataset from $63.1\\%$ to $66.5\\%$ using only three model instances. We measure\nthe inference time of all 3D classification architectures on a $\\textit{Nvidia\nJetson TX2}$, a common embedded computer for mobile robots, to allude to the\nuse of these models in real-life applications.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:51:12 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 22:46:23 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Koguciuk", "Daniel", ""], ["Chechli\u0144ski", "\u0141ukasz", ""], ["El-Gaaly", "Tarek", ""]]}, {"id": "1904.08170", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, Zhibo Chen", "title": "CaseNet: Content-Adaptive Scale Interaction Networks for Scene Parsing", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects at different spatial positions in an image exhibit different scales.\nAdaptive receptive fields are expected to capture suitable ranges of context\nfor accurate pixel level semantic prediction. Recently, atrous convolution with\ndifferent dilation rates has been used to generate features of multi-scales\nthrough several branches which are then fused for prediction. However, there is\na lack of explicit interaction among the branches of different scales to\nadaptively make full use of the contexts. In this paper, we propose a\nContent-Adaptive Scale Interaction Network (CASINet) to exploit the multi-scale\nfeatures for scene parsing. We build CASINet based on the classic Atrous\nSpatial Pyramid Pooling (ASPP) module, followed by a proposed contextual scale\ninteraction (CSI) module, and a scale adaptation (SA) module. Specifically, in\nthe CSI module, for each spatial position of some scale, instead of being\nlimited by a fixed set of convolutional filters that are shared across\ndifferent spatial positions for feature learning, we promote the adaptivity of\nthe convolutional filters to spatial positions. We achieve this by the context\ninteraction among the features of different scales. The SA module explicitly\nand softly selects the suitable scale for each spatial position and each\nchannel. Ablation studies demonstrate the effectiveness of the proposed\nmodules. We achieve state-of-the-art performance on three scene parsing\nbenchmarks Cityscapes, ADE20K and LIP.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 10:33:03 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 05:14:22 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 08:13:39 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Zhang", "Zhizheng", ""], ["Chen", "Zhibo", ""]]}, {"id": "1904.08177", "submitter": "Jia Li", "authors": "Jia Li, Xing Wei, Guoqiang Yang, Xiao Sun, Changliang Li", "title": "Downhole Track Detection via Multiscale Conditional Generative\n  Adversarial Nets", "comments": "arXiv admin note: text overlap with arXiv:1711.11585 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent mine disasters cause a large number of casualties and property\nlosses. Autonomous driving is a fundamental measure for solving this problem,\nand track detection is one of the key technologies for computer vision to\nachieve downhole automatic driving. The track detection result based on the\ntraditional convolutional neural network (CNN) algorithm lacks the detailed and\nunique description of the object and relies too much on visual postprocessing\ntechnology. Therefore, this paper proposes a track detection algorithm based on\na multiscale conditional generative adversarial network (CGAN). The generator\nis decomposed into global and local parts using a multigranularity structure in\nthe generator network. A multiscale shared convolution structure is adopted in\nthe discriminator network to further supervise training the generator. Finally,\nthe Monte Carlo search technique is introduced to search the intermediate state\nof the generator, and the result is sent to the discriminator for comparison.\nCompared with the existing work, our model achieved 82.43\\% pixel accuracy and\nan average intersection-over-union (IOU) of 0.6218, and the detection of the\ntrack reached 95.01\\% accuracy in the downhole roadway scene test set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 10:50:37 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Jia", ""], ["Wei", "Xing", ""], ["Yang", "Guoqiang", ""], ["Sun", "Xiao", ""], ["Li", "Changliang", ""]]}, {"id": "1904.08189", "submitter": "Kaiwen Duan", "authors": "Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi\n  Tian", "title": "CenterNet: Keypoint Triplets for Object Detection", "comments": "10 pages (including 2 pages of References), 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, keypoint-based approaches often suffer a large number of\nincorrect object bounding boxes, arguably due to the lack of an additional look\ninto the cropped regions. This paper presents an efficient solution which\nexplores the visual patterns within each cropped region with minimal costs. We\nbuild our framework upon a representative one-stage keypoint-based detector\nnamed CornerNet. Our approach, named CenterNet, detects each object as a\ntriplet, rather than a pair, of keypoints, which improves both precision and\nrecall. Accordingly, we design two customized modules named cascade corner\npooling and center pooling, which play the roles of enriching information\ncollected by both top-left and bottom-right corners and providing more\nrecognizable information at the central regions, respectively. On the MS-COCO\ndataset, CenterNet achieves an AP of 47.0%, which outperforms all existing\none-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed,\nCenterNet demonstrates quite comparable performance to the top-ranked two-stage\ndetectors. Code is available at https://github.com/Duankaiwen/CenterNet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 11:20:01 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 09:48:49 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 02:23:52 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Duan", "Kaiwen", ""], ["Bai", "Song", ""], ["Xie", "Lingxi", ""], ["Qi", "Honggang", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "1904.08208", "submitter": "Rodrigo Caye Daudt", "authors": "Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau", "title": "Guided Anisotropic Diffusion and Iterative Learning for Weakly\n  Supervised Change Detection", "comments": "Accepted at CVPR 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Large scale datasets created from user labels or openly available data have\nbecome crucial to provide training data for large scale learning algorithms.\nWhile these datasets are easier to acquire, the data are frequently noisy and\nunreliable, which is motivating research on weakly supervised learning\ntechniques. In this paper we propose an iterative learning method that extracts\nthe useful information from a large scale change detection dataset generated\nfrom open vector data to train a fully convolutional network which surpasses\nthe performance obtained by naive supervised learning. We also propose the\nguided anisotropic diffusion algorithm, which improves semantic segmentation\nresults using the input images as guides to perform edge preserving filtering,\nand is used in conjunction with the iterative training method to improve\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 11:53:52 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Daudt", "Rodrigo Caye", ""], ["Saux", "Bertrand Le", ""], ["Boulch", "Alexandre", ""], ["Gousseau", "Yann", ""]]}, {"id": "1904.08241", "submitter": "David Jimenez-Cabello", "authors": "Daniel P\\'erez-Cabo, David Jim\\'enez-Cabello, Artur Costa-Pazo,\n  Roberto J. L\\'opez-Sastre", "title": "Deep Anomaly Detection for Generalized Face Anti-Spoofing", "comments": "To appear at CVPR19 (workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition has achieved unprecedented results, surpassing human\ncapabilities in certain scenarios. However, these automatic solutions are not\nready for production because they can be easily fooled by simple identity\nimpersonation attacks. And although much effort has been devoted to develop\nface anti-spoofing models, their generalization capacity still remains a\nchallenge in real scenarios. In this paper, we introduce a novel approach that\nreformulates the Generalized Presentation Attack Detection (GPAD) problem from\nan anomaly detection perspective. Technically, a deep metric learning model is\nproposed, where a triplet focal loss is used as a regularization for a novel\nloss coined \"metric-softmax\", which is in charge of guiding the learning\nprocess towards more discriminative feature representations in an embedding\nspace. Finally, we demonstrate the benefits of our deep anomaly detection\narchitecture, by introducing a few-shot a posteriori probability estimation\nthat does not need any classifier to be trained on the learned features. We\nconduct extensive experiments using the GRAD-GPAD framework that provides the\nlargest aggregated dataset for face GPAD. Results confirm that our approach is\nable to outperform all the state-of-the-art methods by a considerable margin.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:52:21 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["P\u00e9rez-Cabo", "Daniel", ""], ["Jim\u00e9nez-Cabello", "David", ""], ["Costa-Pazo", "Artur", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1904.08242", "submitter": "Qing Li", "authors": "Qing Li, Shaoyang Chen, Cheng Wang, Xin Li, Chenglu Wen, Ming Cheng,\n  Jonathan Li", "title": "LO-Net: Deep Real-time Lidar Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep convolutional network pipeline, LO-Net, for real-time\nlidar odometry estimation. Unlike most existing lidar odometry (LO) estimations\nthat go through individually designed feature selection, feature matching, and\npose estimation pipeline, LO-Net can be trained in an end-to-end manner. With a\nnew mask-weighted geometric constraint loss, LO-Net can effectively learn\nfeature representation for LO estimation, and can implicitly exploit the\nsequential dependencies and dynamics in the data. We also design a scan-to-map\nmodule, which uses the geometric and semantic information learned in LO-Net, to\nimprove the estimation accuracy. Experiments on benchmark datasets demonstrate\nthat LO-Net outperforms existing learning based approaches and has similar\naccuracy with the state-of-the-art geometry-based approach, LOAM.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:53:30 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 08:35:14 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Li", "Qing", ""], ["Chen", "Shaoyang", ""], ["Wang", "Cheng", ""], ["Li", "Xin", ""], ["Wen", "Chenglu", ""], ["Cheng", "Ming", ""], ["Li", "Jonathan", ""]]}, {"id": "1904.08245", "submitter": "Daniel Gehrig", "authors": "Daniel Gehrig and Antonio Loquercio and Konstantinos G. Derpanis and\n  Davide Scaramuzza", "title": "End-to-End Learning of Representations for Asynchronous Event-Based Data", "comments": "To appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are vision sensors that record asynchronous streams of\nper-pixel brightness changes, referred to as \"events\". They have appealing\nadvantages over frame-based cameras for computer vision, including high\ntemporal resolution, high dynamic range, and no motion blur. Due to the sparse,\nnon-uniform spatiotemporal layout of the event signal, pattern recognition\nalgorithms typically aggregate events into a grid-based representation and\nsubsequently process it by a standard vision pipeline, e.g., Convolutional\nNeural Network (CNN). In this work, we introduce a general framework to convert\nevent streams into grid-based representations through a sequence of\ndifferentiable operations. Our framework comes with two main advantages: (i)\nallows learning the input event representation together with the task dedicated\nnetwork in an end to end manner, and (ii) lays out a taxonomy that unifies the\nmajority of extant event representations in the literature and identifies novel\nones. Empirically, we show that our approach to learning the event\nrepresentation end-to-end yields an improvement of approximately 12% on optical\nflow estimation and object recognition over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:54:37 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 07:25:17 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 12:47:23 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2019 08:01:19 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Gehrig", "Daniel", ""], ["Loquercio", "Antonio", ""], ["Derpanis", "Konstantinos G.", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.08254", "submitter": "Changhee Han", "authors": "Leonardo Rundo, Changhee Han, Yudai Nagano, Jin Zhang, Ryuichiro\n  Hataya, Carmelo Militello, Andrea Tangherloni, Marco S. Nobile, Claudio\n  Ferretti, Daniela Besozzi, Maria Carla Gilardi, Salvatore Vitabile, Giancarlo\n  Mauri, Hideki Nakayama, Paolo Cazzaniga", "title": "USE-Net: incorporating Squeeze-and-Excitation blocks into U-Net for\n  prostate zonal segmentation of multi-institutional MRI datasets", "comments": "44 pages, 6 figures, Accepted to Neurocomputing, Co-first authors:\n  Leonardo Rundo and Changhee Han", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is the most common malignant tumors in men but prostate\nMagnetic Resonance Imaging (MRI) analysis remains challenging. Besides whole\nprostate gland segmentation, the capability to differentiate between the blurry\nboundary of the Central Gland (CG) and Peripheral Zone (PZ) can lead to\ndifferential diagnosis, since tumor's frequency and severity differ in these\nregions. To tackle the prostate zonal segmentation task, we propose a novel\nConvolutional Neural Network (CNN), called USE-Net, which incorporates\nSqueeze-and-Excitation (SE) blocks into U-Net. Especially, the SE blocks are\nadded after every Encoder (Enc USE-Net) or Encoder-Decoder block (Enc-Dec\nUSE-Net). This study evaluates the generalization ability of CNN-based\narchitectures on three T2-weighted MRI datasets, each one consisting of a\ndifferent number of patients and heterogeneous image characteristics, collected\nby different institutions. The following mixed scheme is used for\ntraining/testing: (i) training on either each individual dataset or multiple\nprostate MRI datasets and (ii) testing on all three datasets with all possible\ntraining/testing combinations. USE-Net is compared against three\nstate-of-the-art CNN-based architectures (i.e., U-Net, pix2pix, and Mixed-Scale\nDense Network), along with a semi-automatic continuous max-flow model. The\nresults show that training on the union of the datasets generally outperforms\ntraining on each dataset separately, allowing for both intra-/cross-dataset\ngeneralization. Enc USE-Net shows good overall generalization under any\ntraining condition, while Enc-Dec USE-Net remarkably outperforms the other\nmethods when trained on all datasets. These findings reveal that the SE blocks'\nadaptive feature recalibration provides excellent cross-dataset generalization\nwhen testing is performed on samples of the datasets used during training.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 13:02:30 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 04:10:46 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Rundo", "Leonardo", ""], ["Han", "Changhee", ""], ["Nagano", "Yudai", ""], ["Zhang", "Jin", ""], ["Hataya", "Ryuichiro", ""], ["Militello", "Carmelo", ""], ["Tangherloni", "Andrea", ""], ["Nobile", "Marco S.", ""], ["Ferretti", "Claudio", ""], ["Besozzi", "Daniela", ""], ["Gilardi", "Maria Carla", ""], ["Vitabile", "Salvatore", ""], ["Mauri", "Giancarlo", ""], ["Nakayama", "Hideki", ""], ["Cazzaniga", "Paolo", ""]]}, {"id": "1904.08265", "submitter": "Li Yuan", "authors": "Li Yuan, Francis EH Tay, Ping Li, Li Zhou, Jiashi Feng", "title": "Cycle-SUM: Cycle-consistent Adversarial LSTM Networks for Unsupervised\n  Video Summarization", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel unsupervised video summarization model that\nrequires no manual annotation. The proposed model termed Cycle-SUM adopts a new\ncycle-consistent adversarial LSTM architecture that can effectively maximize\nthe information preserving and compactness of the summary video. It consists of\na frame selector and a cycle-consistent learning based evaluator. The selector\nis a bi-direction LSTM network that learns video representations that embed the\nlong-range relationships among video frames. The evaluator defines a learnable\ninformation preserving metric between original video and summary video and\n\"supervises\" the selector to identify the most informative frames to form the\nsummary video. In particular, the evaluator is composed of two generative\nadversarial networks (GANs), in which the forward GAN is learned to reconstruct\noriginal video from summary video while the backward GAN learns to invert the\nprocessing. The consistency between the output of such cycle learning is\nadopted as the information preserving metric for video summarization. We\ndemonstrate the close relation between mutual information maximization and such\ncycle learning procedure. Experiments on two video summarization benchmark\ndatasets validate the state-of-the-art performance and superiority of the\nCycle-SUM model over previous baselines.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 13:30:49 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Yuan", "Li", ""], ["Tay", "Francis EH", ""], ["Li", "Ping", ""], ["Zhou", "Li", ""], ["Feng", "Jiashi", ""]]}, {"id": "1904.08269", "submitter": "Yaoming Cai", "authors": "Yaoming Cai, Xiaobo Liu, and Zhihua Cai", "title": "BS-Nets: An End-to-End Framework For Band Selection of Hyperspectral\n  Image", "comments": "The paper has been submitted to IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2019.2951433", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) consists of hundreds of continuous narrow bands\nwith high spectral correlation, which would lead to the so-called Hughes\nphenomenon and the high computational cost in processing. Band selection has\nbeen proven effective in avoiding such problems by removing the redundant\nbands. However, many of existing band selection methods separately estimate the\nsignificance for every single band and cannot fully consider the nonlinear and\nglobal interaction between spectral bands. In this paper, by assuming that a\ncomplete HSI can be reconstructed from its few informative bands, we propose a\ngeneral band selection framework, Band Selection Network (termed as BS-Net).\nThe framework consists of a band attention module (BAM), which aims to\nexplicitly model the nonlinear inter-dependencies between spectral bands, and a\nreconstruction network (RecNet), which is used to restore the original HSI cube\nfrom the learned informative bands, resulting in a flexible architecture. The\nresulting framework is end-to-end trainable, making it easier to train from\nscratch and to combine with existing networks. We implement two BS-Nets\nrespectively using fully connected networks (BS-Net-FC) and convolutional\nneural networks (BS-Net-Conv), and compare the results with many existing band\nselection approaches for three real hyperspectral images, demonstrating that\nthe proposed BS-Nets can accurately select informative band subset with less\nredundancy and achieve significantly better classification performance with an\nacceptable time cost.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 13:41:38 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Cai", "Yaoming", ""], ["Liu", "Xiaobo", ""], ["Cai", "Zhihua", ""]]}, {"id": "1904.08279", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad, Jan Hendrik Metzen, Arnold Smeulders and Zeynep Akata", "title": "Interpreting Adversarial Examples with Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep computer vision systems being vulnerable to imperceptible and carefully\ncrafted noise have raised questions regarding the robustness of their\ndecisions. We take a step back and approach this problem from an orthogonal\ndirection. We propose to enable black-box neural networks to justify their\nreasoning both for clean and for adversarial examples by leveraging attributes,\ni.e. visually discriminative properties of objects. We rank attributes based on\ntheir class relevance, i.e. how the classification decision changes when the\ninput is visually slightly perturbed, as well as image relevance, i.e. how well\nthe attributes can be localized on both clean and perturbed images. We present\ncomprehensive experiments for attribute prediction, adversarial example\ngeneration, adversarially robust learning, and their qualitative and\nquantitative analysis using predicted attributes on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 14:04:17 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Metzen", "Jan Hendrik", ""], ["Smeulders", "Arnold", ""], ["Akata", "Zeynep", ""]]}, {"id": "1904.08298", "submitter": "Henri Rebecq", "authors": "Henri Rebecq, Ren\\'e Ranftl, Vladlen Koltun, Davide Scaramuzza", "title": "Events-to-Video: Bringing Modern Computer Vision to Event Cameras", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Long Beach, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel sensors that report brightness changes in the form of\nasynchronous \"events\" instead of intensity frames. They have significant\nadvantages over conventional cameras: high temporal resolution, high dynamic\nrange, and no motion blur. Since the output of event cameras is fundamentally\ndifferent from conventional cameras, it is commonly accepted that they require\nthe development of specialized algorithms to accommodate the particular nature\nof events. In this work, we take a different view and propose to apply\nexisting, mature computer vision techniques to videos reconstructed from event\ndata. We propose a novel recurrent network to reconstruct videos from a stream\nof events, and train it on a large amount of simulated event data. Our\nexperiments show that our approach surpasses state-of-the-art reconstruction\nmethods by a large margin (> 20%) in terms of image quality. We further apply\noff-the-shelf computer vision algorithms to videos reconstructed from event\ndata on tasks such as object classification and visual-inertial odometry, and\nshow that this strategy consistently outperforms algorithms that were\nspecifically designed for event data. We believe that our approach opens the\ndoor to bringing the outstanding properties of event cameras to an entirely new\nrange of tasks. A video of the experiments is available at\nhttps://youtu.be/IdYrC4cUO0I\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 14:54:49 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rebecq", "Henri", ""], ["Ranftl", "Ren\u00e9", ""], ["Koltun", "Vladlen", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.08324", "submitter": "Yanze Wu", "authors": "Yanze Wu, Qiang Sun, Jianqi Ma, Bin Li, Yanwei Fu, Yao Peng, Xiangyang\n  Xue", "title": "Question Guided Modular Routing Networks for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the task of Visual Question Answering (VQA), which is\ntopical in Multimedia community recently. Particularly, we explore two critical\nresearch problems existed in VQA: (1) efficiently fusing the visual and textual\nmodalities; (2) enabling the visual reasoning ability of VQA models in\nanswering complex questions. To address these challenging problems, a novel\nQuestion Guided Modular Routing Networks (QGMRN) has been proposed in this\npaper. Particularly, The QGMRN is composed of visual, textual and routing\nnetwork. The visual and textual network serve as the backbones for the generic\nfeature extractors of visual and textual modalities. QGMRN can fuse the visual\nand textual modalities at multiple semantic levels. Typically, the visual\nreasoning is facilitated by the routing network in a discrete and stochastic\nway by using Gumbel-Softmax trick for module selection. When the input reaches\na certain modular layer, routing network newly proposed in this paper,\ndynamically selects a portion of modules from that layer to process the input\ndepending on the question features generated by the textual network. It can\nalso learn to reason by routing between the generic modules without additional\nsupervision information or expert knowledge. Benefiting from the dynamic\nrouting mechanism, QGMRN can outperform the previous classical VQA methods by a\nlarge margin and achieve the competitive results against the state-of-the-art\nmethods. Furthermore, attention mechanism is integrated into our QGMRN model\nand thus can further boost the model performance. Empirically, extensive\nexperiments on the CLEVR and CLEVR-Humans datasets validate the effectiveness\nof our proposed model, and the state-of-the-art performance has been achieved.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:45:13 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 03:52:08 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 17:21:28 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Wu", "Yanze", ""], ["Sun", "Qiang", ""], ["Ma", "Jianqi", ""], ["Li", "Bin", ""], ["Fu", "Yanwei", ""], ["Peng", "Yao", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1904.08364", "submitter": "Zecheng Xie", "authors": "Zecheng Xie, Yaoxiong Huang, Yuanzhi Zhu, Lianwen Jin, Yuliang Liu,\n  Lele Xie", "title": "Aggregation Cross-Entropy for Sequence Recognition", "comments": "10 pages, 6 figures, Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method, aggregation cross-entropy (ACE),\nfor sequence recognition from a brand new perspective. The ACE loss function\nexhibits competitive performance to CTC and the attention mechanism, with much\nquicker implementation (as it involves only four fundamental formulas), faster\ninference\\back-propagation (approximately O(1) in parallel), less storage\nrequirement (no parameter and negligible runtime memory), and convenient\nemployment (by replacing CTC with ACE). Furthermore, the proposed ACE loss\nfunction exhibits two noteworthy properties: (1) it can be directly applied for\n2D prediction by flattening the 2D prediction into 1D prediction as the input\nand (2) it requires only characters and their numbers in the sequence\nannotation for supervision, which allows it to advance beyond sequence\nrecognition, e.g., counting problem. The code is publicly available at\nhttps://github.com/summerlvsong/Aggregation-Cross-Entropy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:03:48 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 03:32:38 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Xie", "Zecheng", ""], ["Huang", "Yaoxiong", ""], ["Zhu", "Yuanzhi", ""], ["Jin", "Lianwen", ""], ["Liu", "Yuliang", ""], ["Xie", "Lele", ""]]}, {"id": "1904.08366", "submitter": "Tao Hu", "authors": "Tao Hu, Zhizhong Han, Abhinav Shrivastava, Matthias Zwicker", "title": "Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape\n  Completion", "comments": "ICCV 2019 workshop on Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for 3D shape completion by synthesizing\nmulti-view depth maps. While previous work for shape completion relies on\nvolumetric representations, meshes, or point clouds, we propose to use\nmulti-view depth maps from a set of fixed viewing angles as our shape\nrepresentation. This allows us to be free of the limitations of memory for\nvolumetric representations and point clouds by casting shape completion into an\nimage-to-image translation problem. Specifically, we render depth maps of the\nincomplete shape from a fixed set of viewpoints, and perform depth map\ncompletion in each view. Different from image-to-image translation network that\ncompletes each view separately, our novel network, multi-view completion net\n(MVCN), leverages information from all views of a 3D shape to help the\ncompletion of each single view. This enables MVCN to leverage more information\nfrom different depth views to achieve high accuracy in single depth view\ncompletion and keep the consistency among the completed depth images in\ndifferent views. Benefited by the multi-view representation and the novel\nnetwork structure, MVCN significantly improves the accuracy of 3D shape\ncompletion in large-scale benchmarks compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:07:47 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 01:08:03 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 19:55:19 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 22:25:37 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Hu", "Tao", ""], ["Han", "Zhizhong", ""], ["Shrivastava", "Abhinav", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1904.08377", "submitter": "Yuying Chen", "authors": "Yuying Chen, Congcong Liu, Lei Tai, Ming Liu, Bertram E. Shi", "title": "Gaze Training by Modulated Dropout Improves Imitation Learning", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning by behavioral cloning is a prevalent method that has\nachieved some success in vision-based autonomous driving. The basic idea behind\nbehavioral cloning is to have the neural network learn from observing a human\nexpert's behavior. Typically, a convolutional neural network learns to predict\nthe steering commands from raw driver-view images by mimicking the behaviors of\nhuman drivers. However, there are other cues, such as gaze behavior, available\nfrom human drivers that have yet to be exploited. Previous researches have\nshown that novice human learners can benefit from observing experts' gaze\npatterns. We present here that deep neural networks can also profit from this.\nWe propose a method, gaze-modulated dropout, for integrating this gaze\ninformation into a deep driving network implicitly rather than as an additional\ninput. Our experimental results demonstrate that gaze-modulated dropout\nenhances the generalization capability of the network to unseen scenes.\nPrediction error in steering commands is reduced by 23.5% compared to uniform\ndropout. Running closed loop in the simulator, the gaze-modulated dropout net\nincreased the average distance travelled between infractions by 58.5%.\nConsistent with these results, the gaze-modulated dropout net shows lower model\nuncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:25:37 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 11:36:33 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1904.08379", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf, Yaniv Taigman", "title": "Vid2Game: Controllable Characters Extracted from Real-World Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a video of a person performing a certain activity, from which we\nextract a controllable model. The model generates novel image sequences of that\nperson, according to arbitrary user-defined control signals, typically marking\nthe displacement of the moving body. The generated video can have an arbitrary\nbackground, and effectively capture both the dynamics and appearance of the\nperson.\n  The method is based on two networks. The first network maps a current pose,\nand a single-instance control signal to the next pose. The second network maps\nthe current pose, the new pose, and a given background, to an output frame.\nBoth networks include multiple novelties that enable high-quality performance.\nThis is demonstrated on multiple characters extracted from various videos of\ndancers and athletes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:26:14 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""]]}, {"id": "1904.08396", "submitter": "S\\'ebastien Lablanche", "authors": "Sebastien Lablanche, Gerard Lablanche", "title": "Process of image super-resolution", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explain a process of super-resolution reconstruction\nallowing to increase the resolution of an image.The need for high-resolution\ndigital images exists in diverse domains, for example the medical and spatial\ndomains. The obtaining of high-resolution digital images can be made at the\ntime of the shooting, but it is often synonymic of important costs because of\nthe necessary material to avoid such costs, it is known how to use methods of\nsuper-resolution reconstruction, consisting from one or several low resolution\nimages to obtain a high-resolution image. The american patent US 9208537\ndescribes such an algorithm. A zone of one low-resolution image is isolated and\ncategorized according to the information contained in pixels forming the\nborders of the zone. The category of it zone determines the type of\ninterpolation used to add pixels in aforementioned zone, to increase the\nneatness of the images. It is also known how to reconstruct a low-resolution\nimage there high-resolution image by using a model of super-resolution\nreconstruction whose learning is based on networks of neurons and on image or a\npicture library. The demand of chinese patent CN 107563965 and the scientist\npublication \"Pixel Recursive Super Resolution\", R. Dahl, M. Norouzi, J. Shlens\npropose such methods. The aim of this paper is to demonstrate that it is\npossible to reconstruct coherent human faces from very degraded pixelated\nimages with a very fast algorithm, more faster than compressed sensing (CS),\neasier to compute and without deep learning, so without important technology\nresources, i.e. a large database of thousands training images (see\narXiv:2003.13063).\n  This technological breakthrough has been patented in 2018 with the demand of\nFrench patent FR 1855485 (https://patents.google.com/patent/FR3082980A1, see\nthe HAL reference https://hal.archives-ouvertes.fr/hal-01875898v1).\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:52:52 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 17:43:33 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 12:07:40 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 19:22:51 GMT"}, {"version": "v5", "created": "Tue, 17 Mar 2020 10:46:26 GMT"}, {"version": "v6", "created": "Tue, 8 Dec 2020 07:28:32 GMT"}, {"version": "v7", "created": "Fri, 11 Dec 2020 08:46:41 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Lablanche", "Sebastien", ""], ["Lablanche", "Gerard", ""]]}, {"id": "1904.08405", "submitter": "Guillermo Gallego", "authors": "Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi,\n  Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Joerg Conradt,\n  Kostas Daniilidis, Davide Scaramuzza", "title": "Event-based Vision: A Survey", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3008413", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired sensors that differ from conventional frame\ncameras: Instead of capturing images at a fixed rate, they asynchronously\nmeasure per-pixel brightness changes, and output a stream of events that encode\nthe time, location and sign of the brightness changes. Event cameras offer\nattractive properties compared to traditional cameras: high temporal resolution\n(in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low\npower consumption, and high pixel bandwidth (on the order of kHz) resulting in\nreduced motion blur. Hence, event cameras have a large potential for robotics\nand computer vision in challenging scenarios for traditional cameras, such as\nlow-latency, high speed, and high dynamic range. However, novel methods are\nrequired to process the unconventional output of these sensors in order to\nunlock their potential. This paper provides a comprehensive overview of the\nemerging field of event-based vision, with a focus on the applications and the\nalgorithms developed to unlock the outstanding properties of event cameras. We\npresent event cameras from their working principle, the actual sensors that are\navailable and the tasks that they have been used for, from low-level vision\n(feature detection and tracking, optic flow, etc.) to high-level vision\n(reconstruction, segmentation, recognition). We also discuss the techniques\ndeveloped to process events, including learning-based techniques, as well as\nspecialized processors for these novel sensors, such as spiking neural\nnetworks. Additionally, we highlight the challenges that remain to be tackled\nand the opportunities that lie ahead in the search for a more efficient,\nbio-inspired way for machines to perceive and interact with the world.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:59:34 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:52:38 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 10:55:56 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gallego", "Guillermo", ""], ["Delbruck", "Tobi", ""], ["Orchard", "Garrick", ""], ["Bartolozzi", "Chiara", ""], ["Taba", "Brian", ""], ["Censi", "Andrea", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew", ""], ["Conradt", "Joerg", ""], ["Daniilidis", "Kostas", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.08408", "submitter": "Benjamin Deguerre", "authors": "Benjamin Deguerre, Cl\\'ement Chatelain, Gilles Gasso", "title": "Fast object detection in compressed JPEG Images", "comments": null, "journal-ref": null, "doi": "10.1109/ITSC.2019.8916937", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in still images has drawn a lot of attention over past few\nyears, and with the advent of Deep Learning impressive performances have been\nachieved with numerous industrial applications. Most of these deep learning\nmodels rely on RGB images to localize and identify objects in the image.\nHowever in some application scenarii, images are compressed either for storage\nsavings or fast transmission. Therefore a time consuming image decompression\nstep is compulsory in order to apply the aforementioned deep models. To\nalleviate this drawback, we propose a fast deep architecture for object\ndetection in JPEG images, one of the most widespread compression format. We\ntrain a neural network to detect objects based on the blockwise DCT (discrete\ncosine transform) coefficients {issued from} the JPEG compression algorithm. We\nmodify the well-known Single Shot multibox Detector (SSD) by replacing its\nfirst layers with one convolutional layer dedicated to process the DCT inputs.\nExperimental evaluations on PASCAL VOC and industrial dataset comprising images\nof road traffic surveillance show that the model is about $2\\times$ faster than\nregular SSD with promising detection performances. To the best of our\nknowledge, this paper is the first to address detection in compressed JPEG\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 22:10:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Deguerre", "Benjamin", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Gasso", "Gilles", ""]]}, {"id": "1904.08410", "submitter": "Reiichiro Nakano", "authors": "Reiichiro Nakano", "title": "Neural Painters: A learned differentiable constraint for generating\n  brushstroke paintings", "comments": "Added more references and acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore neural painters, a generative model for brushstrokes learned from\na real non-differentiable and non-deterministic painting program. We show that\nwhen training an agent to \"paint\" images using brushstrokes, using a\ndifferentiable neural painter leads to much faster convergence. We propose a\nmethod for encouraging this agent to follow human-like strokes when\nreconstructing digits. We also explore the use of a neural painter as a\ndifferentiable image parameterization. By directly optimizing brushstrokes to\nactivate neurons in a pre-trained convolutional network, we can directly\nvisualize ImageNet categories and generate \"ideal\" paintings of each class.\nFinally, we present a new concept called intrinsic style transfer. By\nminimizing only the content loss from neural style transfer, we allow the\nartistic medium, in this case, brushstrokes, to naturally dictate the resulting\nstyle.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:03:49 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 17:14:53 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nakano", "Reiichiro", ""]]}, {"id": "1904.08412", "submitter": "Yongsheng Dong", "authors": "Xuelong Li, Quanmao Lu, Yongsheng Dong, and Dacheng Tao", "title": "SCE: A manifold regularized set-covering method for data partitioning", "comments": "14 pages, 10 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, May,\n  2018", "doi": "10.1109/TNNLS.2017.2682179", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis plays a very important role in data analysis. In these\nyears, cluster ensemble, as a cluster analysis tool, has drawn much attention\nfor its robustness, stability, and accuracy. Many efforts have been done to\ncombine different initial clustering results into a single clustering solution\nwith better performance. However, they neglect the structure information of the\nraw data in performing the cluster ensemble. In this paper, we propose a\nStructural Cluster Ensemble (SCE) algorithm for data partitioning formulated as\na set-covering problem. In particular, we construct a Laplacian regularized\nobjective function to capture the structure information among clusters.\nMoreover, considering the importance of the discriminative information\nunderlying in the initial clustering results, we add a discriminative\nconstraint into our proposed objective function. Finally, we verify the\nperformance of the SCE algorithm on both synthetic and real data sets. The\nexperimental results show the effectiveness of our proposed method SCE\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 04:43:17 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Xuelong", ""], ["Lu", "Quanmao", ""], ["Dong", "Yongsheng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.08414", "submitter": "Andreas Danzer", "authors": "Andreas Danzer, Thomas Griebel, Martin Bach, and Klaus Dietmayer", "title": "2D Car Detection in Radar Data with PointNets", "comments": null, "journal-ref": "IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 61-66", "doi": "10.1109/ITSC.2019.8917000", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many automated driving functions, a highly accurate perception of the\nvehicle environment is a crucial prerequisite. Modern high-resolution radar\nsensors generate multiple radar targets per object, which makes these sensors\nparticularly suitable for the 2D object detection task. This work presents an\napproach to detect 2D objects solely depending on sparse radar data using\nPointNets. In literature, only methods are presented so far which perform\neither object classification or bounding box estimation for objects. In\ncontrast, this method facilitates a classification together with a bounding box\nestimation of objects using a single radar sensor. To this end, PointNets are\nadjusted for radar data performing 2D object classification with segmentation,\nand 2D bounding box regression in order to estimate an amodal 2D bounding box.\nThe algorithm is evaluated using an automatically created dataset which consist\nof various realistic driving maneuvers. The results show the great potential of\nobject detection in high-resolution radar data using PointNets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:27:56 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 11:39:44 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 10:54:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Danzer", "Andreas", ""], ["Griebel", "Thomas", ""], ["Bach", "Martin", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1904.08421", "submitter": "Lambert Schomaker", "authors": "Lambert Schomaker", "title": "A large-scale field test on word-image classification in large\n  historical document collections using a traditional and two deep-learning\n  methods", "comments": "Field test of a large operational image search system, comparing BOVW\n  and end-to-end CNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes a practical field test on word-image\nclassification in a very large collection of more than 300 diverse handwritten\nhistorical manuscripts, with 1.6 million unique labeled images and more than 11\nmillion images used in testing. Results indicate that several deep-learning\ntests completely failed (mean accuracy 83%). In the tests with more than 1000\noutput units (lexical words) in one-hot encoding for classification,\nperformance steeply drops to almost zero percent accuracy, even with a modest\nsize of the pre-final (i.e., penultimate) layer (150 units). A traditional\nfeature method (BOVW) displays a consistent performance over numbers of classes\nand numbers of training examples (mean accuracy 87%). Additional tests using\nnearest mean on the output of the pre-final layer of an Inception V3 network,\nfor each book, only yielded mediocre results (mean accuracy 49\\%), but was not\nsensitive to high numbers of classes. Notably, this experiment was only\npossible on the basis of labels that were harvested on the basis of a\ntraditional method which already works starting from a single labeled image per\nclass. It is expected that the performance of the failed deep learning tests\ncan be repaired, but only on the basis of human handcrafting (sic) of network\narchitecture and hyperparameters. When the failed problematic books are not\nconsidered, end-to-end CNN training yields about 95% accuracy. This average is\ndominated by a large subset of Chinese characters, performances for other\nscript styles being lower.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 16:03:14 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Schomaker", "Lambert", ""]]}, {"id": "1904.08444", "submitter": "Ji Lin", "authors": "Ji Lin, Chuang Gan, Song Han", "title": "Defensive Quantization: When Efficiency Meets Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization is becoming an industry standard to efficiently\ndeploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and\nFPGAs. However, we observe that the conventional quantization approaches are\nvulnerable to adversarial attacks. This paper aims to raise people's awareness\nabout the security of the quantized models, and we designed a novel\nquantization methodology to jointly optimize the efficiency and robustness of\ndeep learning models. We first conduct an empirical study to show that vanilla\nquantization suffers more from adversarial attacks. We observe that the\ninferior robustness comes from the error amplification effect, where the\nquantization operation further enlarges the distance caused by amplified noise.\nThen we propose a novel Defensive Quantization (DQ) method by controlling the\nLipschitz constant of the network during quantization, such that the magnitude\nof the adversarial noise remains non-expansive during inference. Extensive\nexperiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization\nmethod can defend neural networks against adversarial examples, and even\nachieves superior robustness than their full-precision counterparts while\nmaintaining the same hardware efficiency as vanilla quantization approaches. As\na by-product, DQ can also improve the accuracy of quantized models without\nadversarial attack.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 18:23:24 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lin", "Ji", ""], ["Gan", "Chuang", ""], ["Han", "Song", ""]]}, {"id": "1904.08462", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "Zhenyu Zhang, St\\'ephane Lathuili\\`ere, Andrea Pilzer, Nicu Sebe,\n  Elisa Ricci, Jian Yang", "title": "Online Adaptation through Meta-Learning for Stereo Depth Estimation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of online adaptation for stereo depth\nestimation, that consists in continuously adapting a deep network to a target\nvideo recordedin an environment different from that of the source training set.\nTo address this problem, we propose a novel Online Meta-Learning model with\nAdaption (OMLA). Our proposal is based on two main contributions. First, to\nreducethe domain-shift between source and target feature distributions we\nintroduce an online feature alignment procedurederived from Batch\nNormalization. Second, we devise a meta-learning approach that exploits feature\nalignment forfaster convergence in an online learning setting. Additionally, we\npropose a meta-pre-training algorithm in order toobtain initial network weights\non the source dataset whichfacilitate adaptation on future data streams.\nExperimentally, we show that both OMLA and meta-pre-training helpthe model to\nadapt faster to a new environment. Our proposal is evaluated on the\nwellestablished KITTI dataset,where we show that our online method is\ncompetitive withstate of the art algorithms trained in a batch setting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:24:15 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Pilzer", "Andrea", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""], ["Yang", "Jian", ""]]}, {"id": "1904.08465", "submitter": "Zhenlin Xu", "authors": "Zhenlin Xu and Marc Niethammer", "title": "DeepAtlas: Joint Semi-Supervised Learning of Image Registration and\n  Segmentation", "comments": "To appear in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are state-of-the-art for semantic\nimage segmentation, but typically require many labeled training samples.\nObtaining 3D segmentations of medical images for supervised training is\ndifficult and labor intensive. Motivated by classical approaches for joint\nsegmentation and registration we therefore propose a deep learning framework\nthat jointly learns networks for image registration and image segmentation. In\ncontrast to previous work on deep unsupervised image registration, which showed\nthe benefit of weak supervision via image segmentations, our approach can use\nexisting segmentations when available and computes them via the segmentation\nnetwork otherwise, thereby providing the same registration benefit. Conversely,\nsegmentation network training benefits from the registration, which essentially\nprovides a realistic form of data augmentation. Experiments on knee and brain\n3D magnetic resonance (MR) images show that our approach achieves large\nsimultaneous improvements of segmentation and registration accuracy (over\nindependently trained networks) and allows training high-quality models with\nvery limited training data. Specifically, in a one-shot-scenario (with only one\nmanually labeled image) our approach increases Dice scores (%) over an\nunsupervised registration network by 2.7 and 1.8 on the knee and brain images\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:33:37 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 03:25:03 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Xu", "Zhenlin", ""], ["Niethammer", "Marc", ""]]}, {"id": "1904.08475", "submitter": "Moab Arar", "authors": "Moab Arar, Dov Danon, Daniel Cohen-Or, Ariel Shamir", "title": "Image Resizing by Reconstruction from Deep Features", "comments": "13 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image resizing methods usually work in pixel space and use\nvarious saliency measures. The challenge is to adjust the image shape while\ntrying to preserve important content. In this paper we perform image resizing\nin feature space where the deep layers of a neural network contain rich\nimportant semantic information. We directly adjust the image feature maps,\nextracted from a pre-trained classification network, and reconstruct the\nresized image using a neural-network based optimization. This novel approach\nleverages the hierarchical encoding of the network, and in particular, the\nhigh-level discriminative power of its deeper layers, that recognizes semantic\nobjects and regions and allows maintaining their aspect ratio. Our use of\nreconstruction from deep features diminishes the artifacts introduced by\nimage-space resizing operators. We evaluate our method on benchmarks, compare\nto alternative approaches, and demonstrate its strength on challenging images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:56:23 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 04:30:28 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Arar", "Moab", ""], ["Danon", "Dov", ""], ["Cohen-Or", "Daniel", ""], ["Shamir", "Ariel", ""]]}, {"id": "1904.08479", "submitter": "Yaoyao Liu", "authors": "Yaoyao Liu, Bernt Schiele, Qianru Sun", "title": "An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot learning aims to train efficient predictive models with a few\nexamples. The lack of training data leads to poor models that perform\nhigh-variance or low-confidence predictions. In this paper, we propose to\nmeta-learn the ensemble of epoch-wise empirical Bayes models (E3BM) to achieve\nrobust predictions. \"Epoch-wise\" means that each training epoch has a Bayes\nmodel whose parameters are specifically learned and deployed. \"Empirical\" means\nthat the hyperparameters, e.g., used for learning and ensembling the epoch-wise\nmodels, are generated by hyperprior learners conditional on task-specific data.\nWe introduce four kinds of hyperprior learners by considering inductive vs.\ntransductive, and epoch-dependent vs. epoch-independent, in the paradigm of\nmeta-learning. We conduct extensive experiments for five-class few-shot tasks\non three challenging benchmarks: miniImageNet, tieredImageNet, and FC100, and\nachieve top performance using the epoch-dependent transductive hyperprior\nlearner, which captures the richest information. Our ablation study shows that\nboth \"epoch-wise ensemble\" and \"empirical\" encourage high efficiency and\nrobustness in the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:02:24 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 15:04:44 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 11:51:59 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2019 14:06:29 GMT"}, {"version": "v5", "created": "Mon, 16 Mar 2020 07:54:13 GMT"}, {"version": "v6", "created": "Fri, 17 Jul 2020 09:31:15 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Yaoyao", ""], ["Schiele", "Bernt", ""], ["Sun", "Qianru", ""]]}, {"id": "1904.08482", "submitter": "Junsik Kim", "authors": "Junsik Kim, Tae-Hyun Oh, Seokju Lee, Fei Pan, In So Kweon", "title": "Variational Prototyping-Encoder: One-Shot Learning with Prototypical\n  Images", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In daily life, graphic symbols, such as traffic signs and brand logos, are\nubiquitously utilized around us due to its intuitive expression beyond language\nboundary. We tackle an open-set graphic symbol recognition problem by one-shot\nclassification with prototypical images as a single training example for each\nnovel class. We take an approach to learn a generalizable embedding space for\nnovel tasks. We propose a new approach called variational prototyping-encoder\n(VPE) that learns the image translation task from real-world input images to\ntheir corresponding prototypical images as a meta-task. As a result, VPE learns\nimage similarity as well as prototypical concepts which differs from widely\nused metric learning based approaches. Our experiments with diverse datasets\ndemonstrate that the proposed VPE performs favorably against competing metric\nlearning based one-shot methods. Also, our qualitative analyses show that our\nmeta-task induces an effective embedding space suitable for unseen data\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:26:09 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Kim", "Junsik", ""], ["Oh", "Tae-Hyun", ""], ["Lee", "Seokju", ""], ["Pan", "Fei", ""], ["Kweon", "In So", ""]]}, {"id": "1904.08483", "submitter": "Zhenzhou Wang", "authors": "Zhenzhou Wang", "title": "Deep learning for image segmentation: veritable or overhyped?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved great success as a powerful classification tool\nand also made great progress in sematic segmentation. As a result, many\nresearchers also believe that deep learning is the most powerful tool for pixel\nlevel image segmentation. Could deep learning achieve the same pixel level\naccuracy as traditional image segmentation techniques by mapping the features\nof the object into a non-linear function? This paper gives a short survey of\nthe accuracies achieved by deep learning so far in image classification and\nimage segmentation. Compared to the high accuracies achieved by deep learning\nin classifying limited categories in international vision challenges, the image\nsegmentation accuracies achieved by deep learning in the same challenges are\nonly about eighty percent. On the contrary, the image segmentation accuracies\nachieved in international biomedical challenges are close to ninty five\npercent. Why the difference is so big? Since the accuracies of the competitors\nmethods are only evaluated based on their submitted results instead of\nreproducing the results by submitting the source codes or the software, are the\nachieved accuracies verifiable or overhyped? We are going to find it out by\nanalyzing the working principle of deep learning. Finally, we compared the\naccuracies of state of the art deep learning methods with a threshold selection\nmethod quantitatively. Experimental results showed that the threshold selection\nmethod could achieve significantly higher accuracy than deep learning methods\nin image segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:43:40 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 09:06:50 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 07:33:26 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wang", "Zhenzhou", ""]]}, {"id": "1904.08484", "submitter": "Robin Yancey", "authors": "Robin Elizabeth Yancey, Norman Matloff, Paul Thompson", "title": "Multi-linear Faster RCNN with ELA for Image Tampering Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With technological advances leading to an increase in mechanisms for image\ntampering, fraud detection methods must continue to be upgraded to match their\nsophistication. One problem with current methods is that they require prior\nknowledge of the method of forgery in order to determine which features to\nextract from the image to localize the region of interest. When a machine\nlearning algorithm is used to learn different types of tampering from a large\nset of various image types, with a large enough database we can easily classify\nwhich images are tampered (by training on the entire image feature map for each\nimage). However, we still are left with the question of which features to train\non, and how to localize the manipulation. To solve this, object detection\nnetworks such as Faster R-CNN, which combine an RPN (Region Proposal Network)\nwith a CNN, have recently been adapted to fraud detection by utilizing their\nability to propose bounding boxes for objects of interest to localize the\ntampering artifacts. By making use of the computational powers of today's GPUs\nthis method also achieves a fast run-time and higher accuracy than the top\ncurrent methods such as noise analysis, ELA (Error Level Analysis), or CFA\n(Color Filter Array). In this work, a multi-linear Faster RCNN network will be\napplied similarly but with the second stream having an input of the ELA JPEG\ncompression level mask. This is shown to provide even higher accuracy by adding\ntraining features from the segmented image map to the network.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:39:17 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 17:37:44 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Yancey", "Robin Elizabeth", ""], ["Matloff", "Norman", ""], ["Thompson", "Paul", ""]]}, {"id": "1904.08486", "submitter": "Martin Mundt", "authors": "Martin Mundt, Sagnik Majumder, Sreenivas Murali, Panagiotis Panetsos,\n  Visvanathan Ramesh", "title": "Meta-learning Convolutional Neural Architectures for Multi-target\n  Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset", "comments": "Accepted for publication at CVPR 2019. Version includes supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of defects in concrete infrastructure, especially in bridges, is\na costly and time consuming crucial first step in the assessment of the\nstructural integrity. Large variation in appearance of the concrete material,\nchanging illumination and weather conditions, a variety of possible surface\nmarkings as well as the possibility for different types of defects to overlap,\nmake it a challenging real-world task. In this work we introduce the novel\nCOncrete DEfect BRidge IMage dataset (CODEBRIM) for multi-target classification\nof five commonly appearing concrete defects. We investigate and compare two\nreinforcement learning based meta-learning approaches, MetaQNN and efficient\nneural architecture search, to find suitable convolutional neural network\narchitectures for this challenging multi-class multi-target task. We show that\nlearned architectures have fewer overall parameters in addition to yielding\nbetter multi-target accuracy in comparison to popular neural architectures from\nthe literature evaluated in the context of our application.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:08:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Mundt", "Martin", ""], ["Majumder", "Sagnik", ""], ["Murali", "Sreenivas", ""], ["Panetsos", "Panagiotis", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1904.08487", "submitter": "Zihao Liu", "authors": "Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie\n  Wen, Meiping Huang, Haiyun Yuan, Jian Zhuang", "title": "Machine Vision Guided 3D Medical Image Compression for Efficient\n  Transmission and Accurate Segmentation in the Clouds", "comments": "IEEE Computer Society Conference on Computer Vision and Pattern\n  Recognition(CVPR), Long Beach, CA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud based medical image analysis has become popular recently due to the\nhigh computation complexities of various deep neural network (DNN) based\nframeworks and the increasingly large volume of medical images that need to be\nprocessed. It has been demonstrated that for medical images the transmission\nfrom local to clouds is much more expensive than the computation in the clouds\nitself. Towards this, 3D image compression techniques have been widely applied\nto reduce the data traffic. However, most of the existing image compression\ntechniques are developed around human vision, i.e., they are designed to\nminimize distortions that can be perceived by human eyes. In this paper we will\nuse deep learning based medical image segmentation as a vehicle and demonstrate\nthat interestingly, machine and human view the compression quality differently.\nMedical images compressed with good quality w.r.t. human vision may result in\ninferior segmentation accuracy. We then design a machine vision oriented 3D\nimage compression framework tailored for segmentation using DNNs. Our method\nautomatically extracts and retains image features that are most important to\nthe segmentation. Comprehensive experiments on widely adopted segmentation\nframeworks with HVSMR 2016 challenge dataset show that our method can achieve\nsignificantly higher segmentation accuracy at the same compression rate, or\nmuch better compression rate under the same segmentation accuracy, when\ncompared with the existing JPEG 2000 method. To the best of the authors'\nknowledge, this is the first machine vision guided medical image compression\nframework for segmentation in the clouds.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:34:25 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Zihao", ""], ["Xu", "Xiaowei", ""], ["Liu", "Tao", ""], ["Liu", "Qi", ""], ["Wang", "Yanzhi", ""], ["Shi", "Yiyu", ""], ["Wen", "Wujie", ""], ["Huang", "Meiping", ""], ["Yuan", "Haiyun", ""], ["Zhuang", "Jian", ""]]}, {"id": "1904.08489", "submitter": "Ameya Joshi", "authors": "Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, Chinmay Hegde", "title": "Semantic Adversarial Attacks: Parametric Transformations That Fool Deep\n  Classifiers", "comments": "Accepted to International Conference on Computer Vision, (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to exhibit an intriguing vulnerability\nto adversarial input images corrupted with imperceptible perturbations.\nHowever, the majority of adversarial attacks assume global, fine-grained\ncontrol over the image pixel space. In this paper, we consider a different\nsetting: what happens if the adversary could only alter specific attributes of\nthe input image? These would generate inputs that might be perceptibly\ndifferent, but still natural-looking and enough to fool a classifier. We\npropose a novel approach to generate such `semantic' adversarial examples by\noptimizing a particular adversarial loss over the range-space of a parametric\nconditional generative model. We demonstrate implementations of our attacks on\nbinary classifiers trained on face images, and show that such natural-looking\nsemantic adversarial examples exist. We evaluate the effectiveness of our\nattack on synthetic and real data, and present detailed comparisons with\nexisting attack methods. We supplement our empirical results with theoretical\nbounds that demonstrate the existence of such parametric adversarial examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:39:17 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 19:56:17 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Joshi", "Ameya", ""], ["Mukherjee", "Amitangshu", ""], ["Sarkar", "Soumik", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1904.08492", "submitter": "Sumanth Chennupati", "authors": "Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani and Samir A\n  Rawashdeh", "title": "MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy\n  for Multi-Task Learning", "comments": "Accepted for CVPR 2019 Workshop on Autonomous Driving (WAD). Demo\n  Video can be accessed at https://youtu.be/E378PzLq7lQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is commonly used in autonomous driving for solving\nvarious visual perception tasks. It offers significant benefits in terms of\nboth performance and computational complexity. Current work on multi-task\nlearning networks focus on processing a single input image and there is no\nknown implementation of multi-task learning handling a sequence of images. In\nthis work, we propose a multi-stream multi-task network to take advantage of\nusing feature representations from preceding frames in a video sequence for\njoint learning of segmentation, depth, and motion. The weights of the current\nand previous encoder are shared so that features computed in the previous frame\ncan be leveraged without additional computation. In addition, we propose to use\nthe geometric mean of task losses as a better alternative to the weighted\naverage of task losses. The proposed loss function facilitates better handling\nof the difference in convergence rates of different tasks. Experimental results\non KITTI, Cityscapes and SYNTHIA datasets demonstrate that the proposed\nstrategies outperform various existing multi-task learning solutions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:25:59 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 13:21:27 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chennupati", "Sumanth", ""], ["Sistu", "Ganesh", ""], ["Yogamani", "Senthil", ""], ["Rawashdeh", "Samir A", ""]]}, {"id": "1904.08493", "submitter": "Shiva Azimi", "authors": "Shiva Azimi, Brejesh lall, and Tapan K. Gandhi", "title": "Performance Evalution of 3D Keypoint Detectors and Descriptors for\n  Plants Health Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plant Phenomics based on imaging based techniques can be used to monitor the\nhealth and the diseases of plants and crops. The use of 3D data for plant\nphenomics is a recent phenomenon. However, since 3D point cloud contains more\ninformation than plant images, in this paper, we compare the performance of\ndifferent keypoint detectors and local feature descriptors combinations for the\nplant growth stage and it's growth condition classification based on 3D point\nclouds of the plants. We have also implemented a modified form of 3D SIFT\ndescriptor, that is invariant to rotation and is computationally less intense\nthan most of the 3D SIFT descriptors reported in the existing literature. The\nperformance is evaluated in terms of the classification accuracy and the\nresults are presented in terms of accuracy tables. We find the ISS-SHOT and the\nSIFT-SIFT combinations consistently perform better and Fisher Vector (FV) is a\nbetter encoder than Vector of Linearly Aggregated (VLAD) for such applications.\nIt can serve as a better modality.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:58:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Azimi", "Shiva", ""], ["lall", "Brejesh", ""], ["Gandhi", "Tapan K.", ""]]}, {"id": "1904.08494", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Frederic Jurie, Gaurav Sharma", "title": "Learning 2D to 3D Lifting for Object Detection in 3D for Autonomous\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of 3D object detection from 2D monocular images in\nautonomous driving scenarios. We propose to lift the 2D images to 3D\nrepresentations using learned neural networks and leverage existing networks\nworking directly on 3D data to perform 3D object detection and localization. We\nshow that, with carefully designed training mechanism and automatically\nselected minimally noisy data, such a method is not only feasible, but gives\nhigher results than many methods working on actual 3D inputs acquired from\nphysical sensors. On the challenging KITTI benchmark, we show that our 2D to 3D\nlifted method outperforms many recent competitive 3D networks while\nsignificantly outperforming previous state-of-the-art for 3D detection from\nmonocular images. We also show that a late fusion of the output of the network\ntrained on generated 3D images, with that trained on real 3D images, improves\nperformance. We find the results very interesting and argue that such a method\ncould serve as a highly reliable backup in case of malfunction of expensive 3D\nsensors, if not potentially making them redundant, at least in the case of low\nhuman injury risk autonomous navigation scenarios like warehouse automation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:59:40 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 07:02:34 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Jurie", "Frederic", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1904.08496", "submitter": "Loc Tran H", "authors": "Loc Hoang Tran, Linh Hoang Tran", "title": "Tensor Sparse PCA and Face Recognition: A Novel Approach", "comments": "It has some errors in the experimental section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is the important field in machine learning and pattern\nrecognition research area. It has a lot of applications in military, finance,\npublic security, to name a few. In this paper, the combination of the tensor\nsparse PCA with the nearest-neighbor method (and with the kernel ridge\nregression method) will be proposed and applied to the face dataset.\nExperimental results show that the combination of the tensor sparse PCA with\nany classification system does not always reach the best accuracy performance\nmeasures. However, the accuracy of the combination of the sparse PCA method and\none specific classification system is always better than the accuracy of the\ncombination of the PCA method and one specific classification system and is\nalways better than the accuracy of the classification system itself.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:43:57 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 08:22:03 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 03:14:49 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 08:08:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Tran", "Loc Hoang", ""], ["Tran", "Linh Hoang", ""]]}, {"id": "1904.08497", "submitter": "Pedro Ribeiro Mendes J\\'unior", "authors": "Pedro Ribeiro Mendes J\\'unior, Luca Bondi, Paolo Bestagini, Stefano\n  Tubaro, Anderson Rocha", "title": "An In-Depth Study on Open-Set Camera Model Identification", "comments": "Published through IEEE Access journal", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2921436", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera model identification refers to the problem of linking a picture to the\ncamera model used to shoot it. As this might be an enabling factor in different\nforensic applications to single out possible suspects (e.g., detecting the\nauthor of child abuse or terrorist propaganda material), many accurate camera\nmodel attribution methods have been developed in the literature. One of their\nmain drawbacks, however, is the typical closed-set assumption of the problem.\nThis means that an investigated photograph is always assigned to one camera\nmodel within a set of known ones present during investigation, i.e., training\ntime, and the fact that the picture can come from a completely unrelated camera\nmodel during actual testing is usually ignored. Under realistic conditions, it\nis not possible to assume that every picture under analysis belongs to one of\nthe available camera models. To deal with this issue, in this paper, we present\nthe first in-depth study on the possibility of solving the camera model\nidentification problem in open-set scenarios. Given a photograph, we aim at\ndetecting whether it comes from one of the known camera models of interest or\nfrom an unknown one. We compare different feature extraction algorithms and\nclassifiers specially targeting open-set recognition. We also evaluate possible\nopen-set training protocols that can be applied along with any open-set\nclassifier, observing that a simple of those alternatives obtains best results.\nThorough testing on independent datasets shows that it is possible to leverage\na recently proposed convolutional neural network as feature extractor paired\nwith a properly trained open-set classifier aiming at solving the open-set\ncamera model attribution problem even to small-scale image patches, improving\nover state-of-the-art available solutions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:48:55 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 19:53:21 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["J\u00fanior", "Pedro Ribeiro Mendes", ""], ["Bondi", "Luca", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""], ["Rocha", "Anderson", ""]]}, {"id": "1904.08499", "submitter": "Huibing Wang", "authors": "Huibing Wang, Jinjia Peng and Xianping Fu", "title": "Co-regularized Multi-view Sparse Reconstruction Embedding for Dimension\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of information technology, we have witnessed an age of\ndata explosion which produces a large variety of data filled with redundant\ninformation. Because dimension reduction is an essential tool which embeds\nhigh-dimensional data into a lower-dimensional subspace to avoid redundant\ninformation, it has attracted interests from researchers all over the world.\nHowever, facing with features from multiple views, it's difficult for most\ndimension reduction methods to fully comprehended multi-view features and\nintegrate compatible and complementary information from these features to\nconstruct low-dimensional subspace directly. Furthermore, most multi-view\ndimension reduction methods cannot handle features from nonlinear spaces with\nhigh dimensions. Therefore, how to construct a multi-view dimension reduction\nmethods which can deal with multi-view features from high-dimensional nonlinear\nspace is of vital importance but challenging. In order to address this problem,\nwe proposed a novel method named Co-regularized Multi-view Sparse\nReconstruction Embedding (CMSRE) in this paper. By exploiting correlations of\nsparse reconstruction from multiple views, CMSRE is able to learn local sparse\nstructures of nonlinear manifolds from multiple views and constructs\nsignificative low-dimensional representations for them. Due to the proposed\nco-regularized scheme, correlations of sparse reconstructions from multiple\nviews are preserved by CMSRE as much as possible. Furthermore, sparse\nrepresentation produces more meaningful correlations between features from each\nsingle view, which helps CMSRE to gain better performances. Various evaluations\nbased on the applications of document classification, face recognition and\nimage retrieval can demonstrate the effectiveness of the proposed approach on\nmulti-view dimension reduction.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 05:16:55 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Fu", "Xianping", ""]]}, {"id": "1904.08500", "submitter": "Jingfan Wang", "authors": "Jingfan Wang, Lyne P. Tchapmi, Arvind P. Ravikumara, Mike McGuire,\n  Clay S. Bell, Daniel Zimmerle, Silvio Savarese, Adam R. Brandt", "title": "Machine Vision for Natural Gas Methane Emissions Detection Using an\n  Infrared Camera", "comments": "This paper was submitted to Applied Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is crucial to reduce natural gas methane emissions, which can potentially\noffset the climate benefits of replacing coal with gas. Optical gas imaging\n(OGI) is a widely-used method to detect methane leaks, but is labor-intensive\nand cannot provide leak detection results without operators' judgment. In this\npaper, we develop a computer vision approach to OGI-based leak detection using\nconvolutional neural networks (CNN) trained on methane leak images to enable\nautomatic detection. First, we collect ~1 M frames of labeled video of methane\nleaks from different leaking equipment for building CNN model, covering a wide\nrange of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m).\nSecond, we examine different background subtraction methods to extract the\nmethane plume in the foreground. Third, we then test three CNN model variants,\ncollectively called GasNet, to detect plumes in videos taken at other pieces of\nleaking equipment. We assess the ability of GasNet to perform leak detection by\ncomparing it to a baseline method that uses optical-flow based change detection\nalgorithm. We explore the sensitivity of results to the CNN structure, with a\nmoderate-complexity variant performing best across distances. We find that the\ndetection accuracy can reach as high as 99%, the overall detection accuracy can\nexceed 95% for a case across all leak sizes and imaging distances. Binary\ndetection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely\n(~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater\nthan 94% accuracy across all leak sizes. At farthest distances (~13-16 m),\nperformance degrades rapidly, but it can achieve above 95% accuracy to detect\nlarge leaks (>950 gCH4/h). The GasNet-based computer vision approach could be\ndeployed in OGI surveys to allow automatic vigilance of methane leak detection\nwith high detection accuracy in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 05:38:59 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wang", "Jingfan", ""], ["Tchapmi", "Lyne P.", ""], ["Ravikumara", "Arvind P.", ""], ["McGuire", "Mike", ""], ["Bell", "Clay S.", ""], ["Zimmerle", "Daniel", ""], ["Savarese", "Silvio", ""], ["Brandt", "Adam R.", ""]]}, {"id": "1904.08501", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Mohamed Bahaj", "title": "New method for shape recognition based on dynamic programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method for shape recognition based on dynamic\nprogramming. First, each contour of shape is represented by a set of points.\nAfter alignment and matching between two shapes, the outline of the shape is\ndivided into parts according to N angular and M radial sectors , Each Sector\ncontains a portion of the contour; this portion is divided at the inflexion\npoints into convex and concave sections, and the information about sections are\nextracted in order to provide a semantic content to the outline shape, then\nthis information are coded and transformed into a string of symbols. Finally we\nfind the best alignment of two complete strings and compute the optimal cost of\nsimilarity. The algorithm has been tested on a large set of shape databases and\nreal images (MPEG-7, natural silhouette database).\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 11:37:53 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Bahaj", "Mohamed", ""]]}, {"id": "1904.08502", "submitter": "Davis Wertheimer", "authors": "Davis Wertheimer and Bharath Hariharan", "title": "Few-Shot Learning with Localization in Realistic Settings", "comments": "Appearing in CVPR 2019; added references in covariance pooling\n  sections, added link to code in supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recognition methods typically require large,\nartificially-balanced training classes, while few-shot learning methods are\ntested on artificially small ones. In contrast to both extremes, real world\nrecognition problems exhibit heavy-tailed class distributions, with cluttered\nscenes and a mix of coarse and fine-grained class distinctions. We show that\nprior methods designed for few-shot learning do not work out of the box in\nthese challenging conditions, based on a new \"meta-iNat\" benchmark. We\nintroduce three parameter-free improvements: (a) better training procedures\nbased on adapting cross-validation to meta-learning, (b) novel architectures\nthat localize objects using limited bounding box annotations before\nclassification, and (c) simple parameter-free expansions of the feature space\nbased on bilinear pooling. Together, these improvements double the accuracy of\nstate-of-the-art models on meta-iNat while generalizing to prior benchmarks,\ncomplex neural architectures, and settings with substantial domain shift.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:20:38 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 18:12:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Wertheimer", "Davis", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1904.08503", "submitter": "Assaf Arbelle", "authors": "Assaf Arbelle, Eliav Elul and Tammy Riklin Raviv", "title": "QANet -- Quality Assurance Network for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Deep Learning framework, which quantitatively estimates\nimage segmentation quality without the need for human inspection or labeling.\nWe refer to this method as a Quality Assurance Network -- QANet. Specifically,\ngiven an image and a `proposed' corresponding segmentation, obtained by any\nmethod including manual annotation, the QANet solves a regression problem in\norder to estimate a predefined quality measure with respect to the unknown\nground truth. The QANet is by no means yet another segmentation method.\nInstead, it performs a multi-level, multi-feature comparison of an\nimage-segmentation pair based on a unique network architecture, called the\nRibCage.\n  To demonstrate the strength of the QANet, we addressed the evaluation of\ninstance segmentation using two different datasets from different domains,\nnamely, high throughput live cell microscopy images from the Cell Segmentation\nBenchmark and natural images of plants from the Leaf Segmentation Challenge.\nWhile synthesized segmentations were used to train the QANet, it was tested on\nsegmentations obtained by publicly available methods that participated in the\ndifferent challenges. We show that the QANet accurately estimates the scores of\nthe evaluated segmentations with respect to the hidden ground truth, as\npublished by the challenges' organizers.\n  The code is available at: TBD.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:38:57 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 12:57:36 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 08:39:52 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 08:43:24 GMT"}, {"version": "v5", "created": "Tue, 5 Nov 2019 19:09:56 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Arbelle", "Assaf", ""], ["Elul", "Eliav", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1904.08504", "submitter": "Takashi Matsubara", "authors": "Kenta Hama, Takashi Matsubara, Kuniaki Uehara, Jianfei Cai", "title": "Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval\n  Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide development of black-box machine learning algorithms,\nparticularly deep neural network (DNN), the practical demand for the\nreliability assessment is rapidly rising. On the basis of the concept that\n`Bayesian deep learning knows what it does not know,' the uncertainty of DNN\noutputs has been investigated as a reliability measure for the classification\nand regression tasks. However, in the image-caption retrieval task, well-known\nsamples are not always easy-to-retrieve samples. This study investigates two\naspects of image-caption embedding-and-retrieval systems. On one hand, we\nquantify feature uncertainty by considering image-caption embedding as a\nregression task, and use it for model averaging, which can improve the\nretrieval performance. On the other hand, we further quantify posterior\nuncertainty by considering the retrieval as a classification task, and use it\nas a reliability measure, which can greatly improve the retrieval performance\nby rejecting uncertain queries. The consistent performance of two uncertainty\nmeasures is observed with different datasets (MS COCO and Flickr30k), different\ndeep learning architectures (dropout and batch normalization), and different\nsimilarity functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:19:09 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hama", "Kenta", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""], ["Cai", "Jianfei", ""]]}, {"id": "1904.08505", "submitter": "Clebeson Santos Msc", "authors": "Clebeson Canuto dos Santos, Jorge Leonid Aching Samatelo, Raquel\n  Frizera Vassallo", "title": "Dynamic Gesture Recognition by Using CNNs and Star RGB: a Temporal\n  Information Condensation", "comments": "19 pages, 12 figures, submitted to Neurocomputing Journal", "journal-ref": null, "doi": "10.1016/j.neucom.2020.03.038", "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the advance of technologies, machines are increasingly present in\npeople's daily lives. Thus, there has been more and more effort to develop\ninterfaces, such as dynamic gestures, that provide an intuitive way of\ninteraction. Currently, the most common trend is to use multimodal data, as\ndepth and skeleton information, to enable dynamic gesture recognition. However,\nusing only color information would be more interesting, since RGB cameras are\nusually available in almost every public place, and could be used for gesture\nrecognition without the need of installing other equipment. The main problem\nwith such approach is the difficulty of representing spatio-temporal\ninformation using just color. With this in mind, we propose a technique capable\nof condensing a dynamic gesture, shown in a video, in just one RGB image. We\ncall this technique star RGB. This image is then passed to a classifier formed\nby two Resnet CNNs, a soft-attention ensemble, and a fully connected layer,\nwhich indicates the class of the gesture present in the input video.\nExperiments were carried out using both Montalbano and GRIT datasets. For\nMontalbano dataset, the proposed approach achieved an accuracy of 94.58%. Such\nresult reaches the state-of-the-art when considering this dataset and only\ncolor information. Regarding the GRIT dataset, our proposal achieves more than\n98% of accuracy, recall, precision, and F1-score, outperforming the reference\napproach by more than 6%.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 00:39:32 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 15:57:22 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Santos", "Clebeson Canuto dos", ""], ["Samatelo", "Jorge Leonid Aching", ""], ["Vassallo", "Raquel Frizera", ""]]}, {"id": "1904.08506", "submitter": "Ehsan Taghavi", "authors": "Ehsan Nezhadarya, Ehsan Taghavi, Ryan Razani, Bingbing Liu and Jun Luo", "title": "Adaptive Hierarchical Down-Sampling for Point Cloud Classification", "comments": null, "journal-ref": "2020 Conference on Computer Vision and Pattern Recognition", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several convolution-like operators have recently been proposed for\nextracting features out of point clouds, down-sampling an unordered point cloud\nin a deep neural network has not been rigorously studied. Existing methods\ndown-sample the points regardless of their importance for the output. As a\nresult, some important points in the point cloud may be removed, while less\nvaluable points may be passed to the next layers. In contrast, adaptive\ndown-sampling methods sample the points by taking into account the importance\nof each point, which varies based on the application, task and training data.\nIn this paper, we propose a permutation-invariant learning-based adaptive\ndown-sampling layer, called Critical Points Layer (CPL), which reduces the\nnumber of points in an unordered point cloud while retaining the important\npoints. Unlike most graph-based point cloud down-sampling methods that use\n$k$-NN search algorithm to find the neighbouring points, CPL is a global\ndown-sampling method, rendering it computationally very efficient. The proposed\nlayer can be used along with any graph-based point cloud convolution layer to\nform a convolutional neural network, dubbed CP-Net in this paper. We introduce\na CP-Net for $3$D object classification that achieves the best accuracy for the\nModelNet$40$ dataset among point cloud-based methods, which validates the\neffectiveness of the CPL.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:10:13 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 20:50:01 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Nezhadarya", "Ehsan", ""], ["Taghavi", "Ehsan", ""], ["Razani", "Ryan", ""], ["Liu", "Bingbing", ""], ["Luo", "Jun", ""]]}, {"id": "1904.08516", "submitter": "Guanxiong Liu", "authors": "Guanxiong Liu, Issa Khalil, Abdallah Khreishah", "title": "ZK-GanDef: A GAN based Zero Knowledge Adversarial Training Defense for\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Network classifiers have been used successfully in a wide range of\napplications. However, their underlying assumption of attack free environment\nhas been defied by adversarial examples. Researchers tried to develop defenses;\nhowever, existing approaches are still far from providing effective solutions\nto this evolving problem. In this paper, we design a generative adversarial net\n(GAN) based zero knowledge adversarial training defense, dubbed ZK-GanDef,\nwhich does not consume adversarial examples during training. Therefore,\nZK-GanDef is not only efficient in training but also adaptive to new\nadversarial examples. This advantage comes at the cost of small degradation in\ntest accuracy compared to full knowledge approaches. Our experiments show that\nZK-GanDef enhances test accuracy on adversarial examples by up-to 49.17%\ncompared to zero knowledge approaches. More importantly, its test accuracy is\nclose to that of the state-of-the-art full knowledge approaches (maximum\ndegradation of 8.46%), while taking much less training time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:52:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Guanxiong", ""], ["Khalil", "Issa", ""], ["Khreishah", "Abdallah", ""]]}, {"id": "1904.08518", "submitter": "Josep R. Casas", "authors": "Xiao Lin, Josep R. Casas and Montse Pard\\`as", "title": "Graph based Dynamic Segmentation of Generic Objects in 3D", "comments": "CVPR 2016 workshops - SUNw: Scene Understanding Workshop 2016 See\n  workshop listing in http://cvpr2016.thecvf.com/program/workshops and paper\n  listing in http://sunw.csail.mit.edu/2016/posters.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D segmentation method for RBGD stream data to deal with\n3D object segmentation task in a generic scenario with frequent object\ninteractions. It mainly contributes in two aspects, while being generic and not\nrequiring initialization: firstly, a novel tree structure representation for\nthe point cloud of the scene is proposed. Then, a dynamic manangement mechanism\nfor connected component splits and merges exploits the tree structure\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 22:01:59 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lin", "Xiao", ""], ["Casas", "Josep R.", ""], ["Pard\u00e0s", "Montse", ""]]}, {"id": "1904.08534", "submitter": "Joseph Paul Cohen", "authors": "Hadrien Bertrand, Mohammad Hashir and Joseph Paul Cohen", "title": "Do Lateral Views Help Automated Chest X-ray Predictions?", "comments": "3 pages and 1 figure. Under review as extended abstract at MIDL 2019\n  [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/ryeLXFe494", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most convolutional neural networks in chest radiology use only the frontal\nposteroanterior (PA) view to make a prediction. However the lateral view is\nknown to help the diagnosis of certain diseases and conditions. The recently\nreleased PadChest dataset contains paired PA and lateral views, allowing us to\nstudy for which diseases and conditions the performance of a neural network\nimproves when provided a lateral x-ray view as opposed to a frontal\nposteroanterior (PA) view. Using a simple DenseNet model, we find that using\nthe lateral view increases the AUC of 8 of the 56 labels in our data and\nachieves the same performance as the PA view for 21 of the labels. We find that\nusing the PA and lateral views jointly doesn't trivially lead to an increase in\nperformance but suggest further investigation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:26:21 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:56:18 GMT"}], "update_date": "2019-07-27", "authors_parsed": [["Bertrand", "Hadrien", ""], ["Hashir", "Mohammad", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1904.08537", "submitter": "Matthew Purri", "authors": "Matthew Purri, Jia Xue, Kristin Dana, Matthew Leotta, Dan Lipsa,\n  Zhixin Li, Bo Xu, Jie Shan", "title": "Material Segmentation of Multi-View Satellite Imagery", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material recognition methods use image context and local cues for pixel-wise\nclassification. In many cases only a single image is available to make a\nmaterial prediction. Image sequences, routinely acquired in applications such\nas mutliview stereo, can provide a sampling of the underlying reflectance\nfunctions that reveal pixel-level material attributes. We investigate\nmulti-view material segmentation using two datasets generated for building\nmaterial segmentation and scene material segmentation from the SpaceNet\nChallenge satellite image dataset. In this paper, we explore the impact of\nmulti-angle reflectance information by introducing the \\textit{reflectance\nresidual encoding}, which captures both the multi-angle and multispectral\ninformation present in our datasets. The residuals are computed by differencing\nthe sparse-sampled reflectance function with a dictionary of pre-defined\ndense-sampled reflectance functions. Our proposed reflectance residual features\nimproves material segmentation performance when integrated into pixel-wise and\nsemantic segmentation architectures. At test time, predictions from individual\nsegmentations are combined through softmax fusion and refined by building\nsegment voting. We demonstrate robust and accurate pixelwise segmentation\nresults using the proposed material segmentation pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:52:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Purri", "Matthew", ""], ["Xue", "Jia", ""], ["Dana", "Kristin", ""], ["Leotta", "Matthew", ""], ["Lipsa", "Dan", ""], ["Li", "Zhixin", ""], ["Xu", "Bo", ""], ["Shan", "Jie", ""]]}, {"id": "1904.08542", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Aakansha Mishra, Ashish Mishra and Piyush Rai", "title": "Generative Model for Zero-Shot Sketch-Based Image Retrieval", "comments": "Accepted at CVPR-Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model for Sketch-Based Image Retrieval (SBIR)\nwhere, at retrieval time, we are given sketches from novel classes, that were\nnot present at training time. Existing SBIR methods, most of which rely on\nlearning class-wise correspondences between sketches and images, typically work\nwell only for previously seen sketch classes, and result in poor retrieval\nperformance on novel classes. To address this, we propose a generative model\nthat learns to generate images, conditioned on a given novel class sketch. This\nenables us to reduce the SBIR problem to a standard image-to-image search\nproblem. Our model is based on an inverse auto-regressive flow based\nvariational autoencoder, with a feedback mechanism to ensure robust image\ngeneration. We evaluate our model on two very challenging datasets, Sketchy,\nand TU Berlin, with novel train-test split. The proposed approach significantly\noutperforms various baselines on both the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:11:04 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Mishra", "Aakansha", ""], ["Mishra", "Ashish", ""], ["Rai", "Piyush", ""]]}, {"id": "1904.08573", "submitter": "Jiaxi He", "authors": "Jiaxi He and Frank Z. Xing and Ran Yang and Cishen Zhang", "title": "Fast Single Image Dehazing via Multilevel Wavelet Transform based\n  Optimization", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of images captured in outdoor environments can be affected by\npoor weather conditions such as fog, dust, and atmospheric scattering of other\nparticles. This problem can bring extra challenges to high-level computer\nvision tasks like image segmentation and object detection. However, previous\nstudies on image dehazing suffer from a huge computational workload and\ncorruption of the original image, such as over-saturation and halos. In this\npaper, we present a novel image dehazing approach based on the optical model\nfor haze images and regularized optimization. Specifically, we convert the\nnon-convex, bilinear problem concerning the unknown haze-free image and light\ntransmission distribution to a convex, linear optimization problem by\nestimating the atmosphere light constant. Our method is further accelerated by\nintroducing a multilevel Haar wavelet transform. The optimization, instead, is\napplied to the low frequency sub-band decomposition of the original image. This\ndimension reduction significantly improves the processing speed of our method\nand exhibits the potential for real-time applications. Experimental results\nshow that our approach outperforms state-of-the-art dehazing algorithms in\nterms of both image reconstruction quality and computational efficiency. For\nimplementation details, source code can be publicly accessed via\nhttp://github.com/JiaxiHe/Image-and-Video-Dehazing.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:38:07 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["He", "Jiaxi", ""], ["Xing", "Frank Z.", ""], ["Yang", "Ran", ""], ["Zhang", "Cishen", ""]]}, {"id": "1904.08582", "submitter": "Rui Fan", "authors": "Rui Fan, Mohammud Junaid Bocus, Yilong Zhu, Jianhao Jiao, Li Wang,\n  Fulong Ma, Shanshan Cheng, Ming Liu", "title": "Road Crack Detection Using Deep Convolutional Neural Network and\n  Adaptive Thresholding", "comments": "6 pages, 8 figures, 2019 IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crack is one of the most common road distresses which may pose road safety\nhazards. Generally, crack detection is performed by either certified inspectors\nor structural engineers. This task is, however, time-consuming, subjective and\nlabor-intensive. In this paper, we propose a novel road crack detection\nalgorithm based on deep learning and adaptive image segmentation. Firstly, a\ndeep convolutional neural network is trained to determine whether an image\ncontains cracks or not. The images containing cracks are then smoothed using\nbilateral filtering, which greatly minimizes the number of noisy pixels.\nFinally, we utilize an adaptive thresholding method to extract the cracks from\nroad surface. The experimental results illustrate that our network can classify\nimages with an accuracy of 99.92%, and the cracks can be successfully extracted\nfrom the images using our proposed thresholding algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 03:38:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Fan", "Rui", ""], ["Bocus", "Mohammud Junaid", ""], ["Zhu", "Yilong", ""], ["Jiao", "Jianhao", ""], ["Wang", "Li", ""], ["Ma", "Fulong", ""], ["Cheng", "Shanshan", ""], ["Liu", "Ming", ""]]}, {"id": "1904.08601", "submitter": "Julie Chang", "authors": "Julie Chang, Gordon Wetzstein", "title": "Deep Optics for Monocular Depth Estimation and 3D Object Detection", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation and 3D object detection are critical for scene understanding\nbut remain challenging to perform with a single image due to the loss of 3D\ninformation during image capture. Recent models using deep neural networks have\nimproved monocular depth estimation performance, but there is still difficulty\nin predicting absolute depth and generalizing outside a standard dataset. Here\nwe introduce the paradigm of deep optics, i.e. end-to-end design of optics and\nimage processing, to the monocular depth estimation problem, using coded\ndefocus blur as an additional depth cue to be decoded by a neural network. We\nevaluate several optical coding strategies along with an end-to-end\noptimization scheme for depth estimation on three datasets, including NYU Depth\nv2 and KITTI. We find an optimized freeform lens design yields the best\nresults, but chromatic aberration from a singlet lens offers significantly\nimproved performance as well. We build a physical prototype and validate that\nchromatic aberrations improve depth estimation on real-world results. In\naddition, we train object detection networks on the KITTI dataset and show that\nthe lens optimized for depth estimation also results in improved 3D object\ndetection performance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 06:25:30 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Chang", "Julie", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1904.08607", "submitter": "Junyeong Kim", "authors": "Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, Chang D. Yoo", "title": "Progressive Attention Memory Network for Movie Story Question Answering", "comments": "CVPR 2019, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the progressive attention memory network (PAMN) for movie\nstory question answering (QA). Movie story QA is challenging compared to VQA in\ntwo aspects: (1) pinpointing the temporal parts relevant to answer the question\nis difficult as the movies are typically longer than an hour, (2) it has both\nvideo and subtitle where different questions require different modality to\ninfer the answer. To overcome these challenges, PAMN involves three main\nfeatures: (1) progressive attention mechanism that utilizes cues from both\nquestion and answer to progressively prune out irrelevant temporal parts in\nmemory, (2) dynamic modality fusion that adaptively determines the contribution\nof each modality for answering the current question, and (3) belief correction\nanswering scheme that successively corrects the prediction score on each\ncandidate answer. Experiments on publicly available benchmark datasets, MovieQA\nand TVQA, demonstrate that each feature contributes to our movie story QA\narchitecture, PAMN, and improves performance to achieve the state-of-the-art\nresult. Qualitative analysis by visualizing the inference mechanism of PAMN is\nalso provided.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 06:52:17 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Kim", "Junyeong", ""], ["Ma", "Minuk", ""], ["Kim", "Kyungsu", ""], ["Kim", "Sungjin", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1904.08608", "submitter": "Xu Yang", "authors": "Xu Yang, Hanwang Zhang, Jianfei Cai", "title": "Learning to Collocate Neural Modules for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We do not speak word by word from scratch; our brain quickly structures a\npattern like \\textsc{sth do sth at someplace} and then fill in the detailed\ndescriptions. To render existing encoder-decoder image captioners such\nhuman-like reasoning, we propose a novel framework: learning to Collocate\nNeural Modules (CNM), to generate the `inner pattern' connecting visual encoder\nand language decoder. Unlike the widely-used neural module networks in visual\nQ\\&A, where the language (ie, question) is fully observable, CNM for captioning\nis more challenging as the language is being generated and thus is partially\nobservable. To this end, we make the following technical contributions for CNM\ntraining: 1) compact module design --- one for function words and three for\nvisual content words (eg, noun, adjective, and verb), 2) soft module fusion and\nmulti-step module execution, robustifying the visual reasoning in partial\nobservation, 3) a linguistic loss for module controller being faithful to\npart-of-speech collocations (eg, adjective is before noun). Extensive\nexperiments on the challenging MS-COCO image captioning benchmark validate the\neffectiveness of our CNM image captioner. In particular, CNM achieves a new\nstate-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40\non the official server. CNM is also robust to few training samples, eg, by\ntraining only one sentence per image, CNM can halve the performance loss\ncompared to a strong baseline.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:03:19 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yang", "Xu", ""], ["Zhang", "Hanwang", ""], ["Cai", "Jianfei", ""]]}, {"id": "1904.08610", "submitter": "Jan Egger", "authors": "Daniel Wild, Maximilian Weber, Jan Egger", "title": "Client/Server Based Online Environment for Manual Segmentation of\n  Medical Images", "comments": "8 pages", "journal-ref": "Proceedings of CESCG 2019: The 23rd Central European Seminar on\n  Computer Graphics", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a key step in analyzing and processing medical images. Due to\nthe low fault tolerance in medical imaging, manual segmentation remains the de\nfacto standard in this domain. Besides, efforts to automate the segmentation\nprocess often rely on large amounts of manually labeled data. While existing\nsoftware supporting manual segmentation is rich in features and delivers\naccurate results, the necessary time to set it up and get comfortable using it\ncan pose a hurdle for the collection of large datasets. This work introduces a\nclient/server based online environment, referred to as Studierfenster\n(studierfenster.at), that can be used to perform manual segmentations directly\nin a web browser. The aim of providing this functionality in the form of a web\napplication is to ease the collection of ground truth segmentation datasets.\nProviding a tool that is quickly accessible and usable on a broad range of\ndevices, offers the potential to accelerate this process. The manual\nsegmentation workflow of Studierfenster consists of dragging and dropping the\ninput file into the browser window and slice-by-slice outlining the object\nunder consideration. The final segmentation can then be exported as a file\nstoring its contours and as a binary segmentation mask. In order to evaluate\nthe usability of Studierfenster, a user study was performed. The user study\nresulted in a mean of 6.3 out of 7.0 possible points given by users, when asked\nabout their overall impression of the tool. The evaluation also provides\ninsights into the results achievable with the tool in practice, by presenting\ntwo ground truth segmentations performed by physicians.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:17:05 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wild", "Daniel", ""], ["Weber", "Maximilian", ""], ["Egger", "Jan", ""]]}, {"id": "1904.08613", "submitter": "Kazi Nazmul Haque", "authors": "Kazi Nazmul Haque, Siddique Latif, and Rajib Rana", "title": "Disentangled Representation Learning with Information Maximizing\n  Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representation from any unlabelled data is a\nnon-trivial problem. In this paper we propose Information Maximising\nAutoencoder (InfoAE) where the encoder learns powerful disentangled\nrepresentation through maximizing the mutual information between the\nrepresentation and given information in an unsupervised fashion. We have\nevaluated our model on MNIST dataset and achieved 98.9 ($\\pm .1$) $\\%$ test\naccuracy while using complete unsupervised training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:25:19 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Haque", "Kazi Nazmul", ""], ["Latif", "Siddique", ""], ["Rana", "Rajib", ""]]}, {"id": "1904.08630", "submitter": "Andreas Robinson", "authors": "Andreas Robinson, Felix J\\\"aremo Lawin, Martin Danelljan, Fahad\n  Shahbaz Khan, Michael Felsberg", "title": "Discriminative Online Learning for Fast Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the highly challenging problem of video object segmentation. Given\nonly the initial mask, the task is to segment the target in the subsequent\nframes. In order to effectively handle appearance changes and similar\nbackground objects, a robust representation of the target is required. Previous\napproaches either rely on fine-tuning a segmentation network on the first\nframe, or employ generative appearance models. Although partially successful,\nthese methods often suffer from impractically low frame rates or unsatisfactory\nrobustness.\n  We propose a novel approach, based on a dedicated target appearance model\nthat is exclusively learned online to discriminate between the target and\nbackground image regions. Importantly, we design a specialized loss and\ncustomized optimization techniques to enable highly efficient online training.\nOur light-weight target model is integrated into a carefully designed\nsegmentation network, trained offline to enhance the predictions generated by\nthe target model. Extensive experiments are performed on three datasets. Our\napproach achieves an overall score of over 70 on YouTube-VOS, while operating\nat 25 frames per second.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:11:07 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Robinson", "Andreas", ""], ["Lawin", "Felix J\u00e4remo", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1904.08631", "submitter": "Junbao Zhuo", "authors": "Junbao Zhuo, Shuhui Wang, Shuhao Cui and Qingming Huang", "title": "Unsupervised Open Domain Recognition by Semantic Discrepancy\n  Minimization", "comments": "Accepted to CVPR 2019, 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the unsupervised open domain recognition (UODR) problem, where\ncategories in labeled source domain S is only a subset of those in unlabeled\ntarget domain T. The task is to correctly classify all samples in T including\nknown and unknown categories. UODR is challenging due to the domain\ndiscrepancy, which becomes even harder to bridge when a large number of unknown\ncategories exist in T. Moreover, the classification rules propagated by graph\nCNN (GCN) may be distracted by unknown categories and lack generalization\ncapability. To measure the domain discrepancy for asymmetric label space\nbetween S and T, we propose Semantic-Guided Matching Discrepancy (SGMD), which\nfirst employs instance matching between S and T, and then the discrepancy is\nmeasured by a weighted feature distance between matched instances. We further\ndesign a limited balance constraint to achieve a more balanced classification\noutput on known and unknown categories. We develop Unsupervised Open Domain\nTransfer Network (UODTN), which learns both the backbone classification network\nand GCN jointly by reducing the SGMD, enforcing the limited balance constraint\nand minimizing the classification loss on S. UODTN better preserves the\nsemantic structure and enforces the consistency between the learned domain\ninvariant visual features and the semantic embeddings. Experimental results\nshow superiority of our method on recognizing images of both known and unknown\ncategories.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:13:54 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Zhuo", "Junbao", ""], ["Wang", "Shuhui", ""], ["Cui", "Shuhao", ""], ["Huang", "Qingming", ""]]}, {"id": "1904.08632", "submitter": "Ke Gu", "authors": "Ke Gu, Dacheng Tao, Junfei Qiao, Weisi Lin", "title": "Learning a No-Reference Quality Assessment Model of Enhanced Images With\n  Big Data", "comments": "12 pages, 45 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate into the problem of image quality assessment\n(IQA) and enhancement via machine learning. This issue has long attracted a\nwide range of attention in computational intelligence and image processing\ncommunities, since, for many practical applications, e.g. object detection and\nrecognition, raw images are usually needed to be appropriately enhanced to\nraise the visual quality (e.g. visibility and contrast). In fact, proper\nenhancement can noticeably improve the quality of input images, even better\nthan originally captured images which are generally thought to be of the best\nquality. In this work, we present two most important contributions. The first\ncontribution is to develop a new no-reference (NR) IQA model. Given an image,\nour quality measure first extracts 17 features through analysis of contrast,\nsharpness, brightness and more, and then yields a measre of visual quality\nusing a regression module, which is learned with big-data training samples that\nare much bigger than the size of relevant image datasets. Results of\nexperiments on nine datasets validate the superiority and efficiency of our\nblind metric compared with typical state-of-the-art full-, reduced- and\nno-reference IQA methods. The second contribution is that a robust image\nenhancement framework is established based on quality optimization. For an\ninput image, by the guidance of the proposed NR-IQA measure, we conduct\nhistogram modification to successively rectify image brightness and contrast to\na proper level. Thorough tests demonstrate that our framework can well enhance\nnatural images, low-contrast images, low-light images and dehazed images. The\nsource code will be released at\nhttps://sites.google.com/site/guke198701/publications.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:14:24 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Gu", "Ke", ""], ["Tao", "Dacheng", ""], ["Qiao", "Junfei", ""], ["Lin", "Weisi", ""]]}, {"id": "1904.08634", "submitter": "Toby Perrett", "authors": "Toby Perrett and Dima Damen", "title": "DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain alignment in convolutional networks aims to learn the degree of\nlayer-specific feature alignment beneficial to the joint learning of source and\ntarget datasets. While increasingly popular in convolutional networks, there\nhave been no previous attempts to achieve domain alignment in recurrent\nnetworks. Similar to spatial features, both source and target domains are\nlikely to exhibit temporal dependencies that can be jointly learnt and aligned.\n  In this paper we introduce Dual-Domain LSTM (DDLSTM), an architecture that is\nable to learn temporal dependencies from two domains concurrently. It performs\ncross-contaminated batch normalisation on both input-to-hidden and\nhidden-to-hidden weights, and learns the parameters for cross-contamination,\nfor both single-layer and multi-layer LSTM architectures. We evaluate DDLSTM on\nframe-level action recognition using three datasets, taking a pair at a time,\nand report an average increase in accuracy of 3.5%. The proposed DDLSTM\narchitecture outperforms standard, fine-tuned, and batch-normalised LSTMs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:17:35 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Perrett", "Toby", ""], ["Damen", "Dima", ""]]}, {"id": "1904.08643", "submitter": "Victor Kitov", "authors": "Victor Kitov", "title": "Real-Time Style Transfer With Strength Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is a problem of rendering a content image in the style of\nanother style image. A natural and common practical task in applications of\nstyle transfer is to adjust the strength of stylization. Algorithm of Gatys et\nal. (2016) provides this ability by changing the weighting factors of content\nand style losses but is computationally inefficient. Real-time style transfer\nintroduced by Johnson et al. (2016) enables fast stylization of any image by\npassing it through a pre-trained transformer network. Although fast, this\narchitecture is not able to continuously adjust style strength. We propose an\nextension to real-time style transfer that allows direct control of style\nstrength at inference, still requiring only a single transformer network. We\nconduct qualitative and quantitative experiments that demonstrate that the\nproposed method is capable of smooth stylization strength control and removes\ncertain stylization artifacts appearing in the original real-time style\ntransfer method. Comparisons with alternative real-time style transfer\nalgorithms, capable of adjusting stylization strength, show that our method\nreproduces style with more details.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:58:49 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Kitov", "Victor", ""]]}, {"id": "1904.08645", "submitter": "Thiemo Alldieck", "authors": "Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, Marcus Magnor", "title": "Tex2Shape: Detailed Full Human Body Geometry From a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective method to infer detailed full human body\nshape from only a single photograph. Our model can infer full-body shape\nincluding face, hair, and clothing including wrinkles at interactive\nframe-rates. Results feature details even on parts that are occluded in the\ninput image. Our main idea is to turn shape regression into an aligned\nimage-to-image translation problem. The input to our method is a partial\ntexture map of the visible region obtained from off-the-shelf methods. From a\npartial texture, we estimate detailed normal and vector displacement maps,\nwhich can be applied to a low-resolution smooth body model to add detail and\nclothing. Despite being trained purely with synthetic data, our model\ngeneralizes well to real-world photographs. Numerous results demonstrate the\nversatility and robustness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 09:10:20 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 21:17:04 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Alldieck", "Thiemo", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""], ["Magnor", "Marcus", ""]]}, {"id": "1904.08653", "submitter": "Wiebe Van Ranst", "authors": "Simen Thys and Wiebe Van Ranst and Toon Goedem\\'e", "title": "Fooling automated surveillance cameras: adversarial patches to attack\n  person detection", "comments": "Accepted for CVPR Workshop: CV-COPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks on machine learning models have seen increasing interest\nin the past years. By making only subtle changes to the input of a\nconvolutional neural network, the output of the network can be swayed to output\na completely different result. The first attacks did this by changing pixel\nvalues of an input image slightly to fool a classifier to output the wrong\nclass. Other approaches have tried to learn \"patches\" that can be applied to an\nobject to fool detectors and classifiers. Some of these approaches have also\nshown that these attacks are feasible in the real-world, i.e. by modifying an\nobject and filming it with a video camera. However, all of these approaches\ntarget classes that contain almost no intra-class variety (e.g. stop signs).\nThe known structure of the object is then used to generate an adversarial patch\non top of it.\n  In this paper, we present an approach to generate adversarial patches to\ntargets with lots of intra-class variety, namely persons. The goal is to\ngenerate a patch that is able successfully hide a person from a person\ndetector. An attack that could for instance be used maliciously to circumvent\nsurveillance systems, intruders can sneak around undetected by holding a small\ncardboard plate in front of their body aimed towards the surveillance camera.\n  From our results we can see that our system is able significantly lower the\naccuracy of a person detector. Our approach also functions well in real-life\nscenarios where the patch is filmed by a camera. To the best of our knowledge\nwe are the first to attempt this kind of attack on targets with a high level of\nintra-class variety like persons.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 09:46:03 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Thys", "Simen", ""], ["Van Ranst", "Wiebe", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "1904.08655", "submitter": "Julia Rackerseder", "authors": "Julia Rackerseder, R\\\"udiger G\\\"obl, Nassir Navab, Christoph\n  Hennersperger", "title": "Fully Automatic Segmentation of 3D Brain Ultrasound: Learning from\n  Coarse Annotations", "comments": "* Julia Rackerseder abd R\\\"udiger G\\\"obl contributed equally to this\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-operative ultrasound is an increasingly important imaging modality in\nneurosurgery. However, manual interaction with imaging data during the\nprocedures, for example to select landmarks or perform segmentation, is\ndifficult and can be time consuming. Yet, as registration to other imaging\nmodalities is required in most cases, some annotation is necessary. We propose\na segmentation method based on DeepVNet and specifically evaluate the\nintegration of pre-training with simulated ultrasound sweeps to improve\nautomatic segmentation and enable a fully automatic initialization of\nregistration. In this view, we show that despite training on coarse and\nincomplete semi-automatic annotations, our approach is able to capture the\ndesired superficial structures such as \\textit{sulci}, the \\textit{cerebellar\ntentorium}, and the \\textit{falx cerebri}. We perform a five-fold\ncross-validation on the publicly available RESECT dataset. Trained on the\ndataset alone, we report a Dice and Jaccard coefficient of $0.45 \\pm 0.09$ and\n$0.30 \\pm 0.07$ respectively, as well as an average distance of $0.78 \\pm\n0.36~mm$. With the suggested pre-training, we computed a Dice and Jaccard\ncoefficient of $0.47 \\pm 0.10$ and $0.31 \\pm 0.08$, and an average distance of\n$0.71 \\pm 0.38~mm$. The qualitative evaluation suggest that with pre-training\nthe network can learn to generalize better and provide refined and more\ncomplete segmentations in comparison to incomplete annotations provided as\ninput.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 09:51:06 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Rackerseder", "Julia", ""], ["G\u00f6bl", "R\u00fcdiger", ""], ["Navab", "Nassir", ""], ["Hennersperger", "Christoph", ""]]}, {"id": "1904.08668", "submitter": "Federico Magliani", "authors": "Federico Magliani and Kevin McGuinness and Eva Mohedano and Andrea\n  Prati", "title": "An Efficient Approximate kNN Graph Method for Diffusion on Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of the diffusion in many computer vision and artificial\nintelligence projects has been shown to give excellent improvements in\nperformance. One of the main bottlenecks of this technique is the quadratic\ngrowth of the kNN graph size due to the high-quantity of new connections\nbetween nodes in the graph, resulting in long computation times. Several\nstrategies have been proposed to address this, but none are effective and\nefficient. Our novel technique, based on LSH projections, obtains the same\nperformance as the exact kNN graph after diffusion, but in less time\n(approximately 18 times faster on a dataset of a hundred thousand images). The\nproposed method was validated and compared with other state-of-the-art on\nseveral public image datasets, including Oxford5k, Paris6k, and Oxford105k.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 10:15:41 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Magliani", "Federico", ""], ["McGuinness", "Kevin", ""], ["Mohedano", "Eva", ""], ["Prati", "Andrea", ""]]}, {"id": "1904.08671", "submitter": "Dayong Tian", "authors": "Dayong Tian and Dacheng Tao", "title": "Coupled Learning for Facial Deblur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blur in facial images significantly impedes the efficiency of recognition\napproaches. However, most existing blind deconvolution methods cannot generate\nsatisfactory results due to their dependence on strong edges, which are\nsufficient in natural images but not in facial images. In this paper, we\nrepresent point spread functions (PSFs) by the linear combination of a set of\npre-defined orthogonal PSFs, and similarly, an estimated intrinsic (EI) sharp\nface image is represented by the linear combination of a set of pre-defined\northogonal face images. In doing so, PSF and EI estimation is simplified to\ndiscovering two sets of linear combination coefficients, which are\nsimultaneously found by our proposed coupled learning algorithm. To make our\nmethod robust to different types of blurry face images, we generate several\ncandidate PSFs and EIs for a test image, and then, a non-blind deconvolution\nmethod is adopted to generate more EIs by those candidate PSFs. Finally, we\ndeploy a blind image quality assessment metric to automatically select the\noptimal EI. Thorough experiments on the facial recognition technology database,\nextended Yale face database B, CMU pose, illumination, and expression (PIE)\ndatabase, and face recognition grand challenge database version 2.0 demonstrate\nthat the proposed approach effectively restores intrinsic sharp face images\nand, consequently, improves the performance of face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 10:24:15 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Tian", "Dayong", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.08685", "submitter": "Dayong Tian", "authors": "Dayong Tian and Dacheng Tao", "title": "Global Hashing System for Fast Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods have been widely investigated for fast approximate nearest\nneighbor searching in large data sets. Most existing methods use binary vectors\nin lower dimensional spaces to represent data points that are usually real\nvectors of higher dimensionality. We divide the hashing process into two steps.\nData points are first embedded in a low-dimensional space, and the global\npositioning system method is subsequently introduced but modified for binary\nembedding. We devise dataindependent and data-dependent methods to distribute\nthe satellites at appropriate locations. Our methods are based on finding the\ntradeoff between the information losses in these two steps. Experiments show\nthat our data-dependent method outperforms other methods in different-sized\ndata sets from 100k to 10M. By incorporating the orthogonality of the code\nmatrix, both our data-independent and data-dependent methods are particularly\nimpressive in experiments on longer bits.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:02:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Tian", "Dayong", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.08688", "submitter": "Vassili Kovalev", "authors": "Vassili Kovalev and Siarhei Kazlouski", "title": "Examining the Capability of GANs to Replace Real Biomedical Images in\n  Classification Models Training", "comments": "10 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the possibility of generating artificial biomedical\nimages that can be used as a substitute for real image datasets in applied\nmachine learning tasks. We are focusing on generation of realistic chest X-ray\nimages as well as on the lymph node histology images using the two recent GAN\narchitectures including DCGAN and PGGAN. The possibility of the use of\nartificial images instead of real ones for training machine learning models was\nexamined by benchmark classification tasks being solved using conventional and\ndeep learning methods. In particular, a comparison was made by replacing real\nimages with synthetic ones at the model training stage and comparing the\nprediction results with the ones obtained while training on the real image\ndata. It was found that the drop of classification accuracy caused by such\ntraining data substitution ranged between 2.2% and 3.5% for deep learning\nmodels and between 5.5% and 13.25% for conventional methods such as LBP +\nRandom Forests.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:05:51 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Kovalev", "Vassili", ""], ["Kazlouski", "Siarhei", ""]]}, {"id": "1904.08703", "submitter": "Sanath Narayan", "authors": "Devraj Mandal, Sanath Narayan, Saikumar Dwivedi, Vikram Gupta, Shuaib\n  Ahmed, Fahad Shahbaz Khan and Ling Shao", "title": "Out-of-Distribution Detection for Generalized Zero-Shot Action\n  Recognition", "comments": "10 pages, 3 figures, 6 Tables. To appear in the proceedings of CVPR\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot action recognition is a challenging problem, where the\ntask is to recognize new action categories that are unavailable during the\ntraining stage, in addition to the seen action categories. Existing approaches\nsuffer from the inherent bias of the learned classifier towards the seen action\ncategories. As a consequence, unseen category samples are incorrectly\nclassified as belonging to one of the seen action categories. In this paper, we\nset out to tackle this issue by arguing for a separate treatment of seen and\nunseen action categories in generalized zero-shot action recognition. We\nintroduce an out-of-distribution detector that determines whether the video\nfeatures belong to a seen or unseen action category. To train our\nout-of-distribution detector, video features for unseen action categories are\nsynthesized using generative adversarial networks trained on seen action\ncategory features. To the best of our knowledge, we are the first to propose an\nout-of-distribution detector based GZSL framework for action recognition in\nvideos. Experiments are performed on three action recognition datasets: Olympic\nSports, HMDB51 and UCF101. For generalized zero-shot action recognition, our\nproposed approach outperforms the baseline (f-CLSWGAN) with absolute gains (in\nclassification accuracy) of 7.0%, 3.4%, and 4.9%, respectively, on these\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:37:23 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 11:36:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mandal", "Devraj", ""], ["Narayan", "Sanath", ""], ["Dwivedi", "Saikumar", ""], ["Gupta", "Vikram", ""], ["Ahmed", "Shuaib", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "1904.08709", "submitter": "Simone Paolo Ponzetto", "authors": "Lydia Weiland, Ioana Hulpus, Simone Paolo Ponzetto, Wolfgang\n  Effelsberg, Laura Dietz", "title": "Knowledge-rich Image Gist Understanding Beyond Literal Meaning", "comments": null, "journal-ref": "Data & Knowledge Engineering, Volume 117, September 2018, Pages\n  114-132", "doi": "10.1016/j.datak.2018.07.006", "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of understanding the message (gist) conveyed by\nimages and their captions as found, for instance, on websites or news articles.\nTo this end, we propose a methodology to capture the meaning of image-caption\npairs on the basis of large amounts of machine-readable knowledge that has\npreviously been shown to be highly effective for text understanding. Our method\nidentifies the connotation of objects beyond their denotation: where most\napproaches to image understanding focus on the denotation of objects, i.e.,\ntheir literal meaning, our work addresses the identification of connotations,\ni.e., iconic meanings of objects, to understand the message of images. We view\nimage understanding as the task of representing an image-caption pair on the\nbasis of a wide-coverage vocabulary of concepts such as the one provided by\nWikipedia, and cast gist detection as a concept-ranking problem with\nimage-caption pairs as queries. To enable a thorough investigation of the\nproblem of gist understanding, we produce a gold standard of over 300\nimage-caption pairs and over 8,000 gist annotations covering a wide variety of\ntopics at different levels of abstraction. We use this dataset to\nexperimentally benchmark the contribution of signals from heterogeneous\nsources, namely image and text. The best result with a Mean Average Precision\n(MAP) of 0.69 indicate that by combining both dimensions we are able to better\nunderstand the meaning of our image-caption pairs than when using language or\nvision information alone. We test the robustness of our gist detection approach\nwhen receiving automatically generated input, i.e., using automatically\ngenerated image tags or generated captions, and prove the feasibility of an\nend-to-end automated process.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:50:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Weiland", "Lydia", ""], ["Hulpus", "Ioana", ""], ["Ponzetto", "Simone Paolo", ""], ["Effelsberg", "Wolfgang", ""], ["Dietz", "Laura", ""]]}, {"id": "1904.08720", "submitter": "Tuan N.A. Hoang", "authors": "Thanh-Toan Do and Toan Tran and Ian Reid and Vijay Kumar and Tuan\n  Hoang and Gustavo Carneiro", "title": "A Theoretically Sound Upper Bound on the Triplet Loss for Improving the\n  Efficiency of Deep Distance Metric Learning", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that substantially improves the efficiency of deep\ndistance metric learning based on the optimization of the triplet loss\nfunction. One epoch of such training process based on a naive optimization of\nthe triplet loss function has a run-time complexity O(N^3), where N is the\nnumber of training samples. Such optimization scales poorly, and the most\ncommon approach proposed to address this high complexity issue is based on\nsub-sampling the set of triplets needed for the training process. Another\napproach explored in the field relies on an ad-hoc linearization (in terms of\nN) of the triplet loss that introduces class centroids, which must be optimized\nusing the whole training set for each mini-batch - this means that a naive\nimplementation of this approach has run-time complexity O(N^2). This complexity\nissue is usually mitigated with poor, but computationally cheap, approximate\ncentroid optimization methods. In this paper, we first propose a solid theory\non the linearization of the triplet loss with the use of class centroids, where\nthe main conclusion is that our new linear loss represents a tight upper-bound\nto the triplet loss. Furthermore, based on the theory above, we propose a\ntraining algorithm that no longer requires the centroid optimization step,\nwhich means that our approach is the first in the field with a guaranteed\nlinear run-time complexity. We show that the training of deep distance metric\nlearning methods using the proposed upper-bound is substantially faster than\ntriplet-based methods, while producing competitive retrieval accuracy results\non benchmark datasets (CUB-200-2011 and CAR196).\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 12:14:50 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Tran", "Toan", ""], ["Reid", "Ian", ""], ["Kumar", "Vijay", ""], ["Hoang", "Tuan", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1904.08739", "submitter": "Zhe Wu", "authors": "Zhe Wu, Li Su, Qingming Huang", "title": "Cascaded Partial Decoder for Fast and Accurate Salient Object Detection", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing state-of-the-art salient object detection networks rely on\naggregating multi-level features of pre-trained convolutional neural networks\n(CNNs). Compared to high-level features, low-level features contribute less to\nperformance but cost more computations because of their larger spatial\nresolutions. In this paper, we propose a novel Cascaded Partial Decoder (CPD)\nframework for fast and accurate salient object detection. On the one hand, the\nframework constructs partial decoder which discards larger resolution features\nof shallower layers for acceleration. On the other hand, we observe that\nintegrating features of deeper layers obtain relatively precise saliency map.\nTherefore we directly utilize generated saliency map to refine the features of\nbackbone network. This strategy efficiently suppresses distractors in the\nfeatures and significantly improves their representation ability. Experiments\nconducted on five benchmark datasets exhibit that the proposed model not only\nachieves state-of-the-art performance but also runs much faster than existing\nmodels. Besides, the proposed framework is further applied to improve existing\nmulti-level feature aggregation models and significantly improve their\nefficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 12:53:30 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wu", "Zhe", ""], ["Su", "Li", ""], ["Huang", "Qingming", ""]]}, {"id": "1904.08743", "submitter": "Christoph Sch\\\"oller", "authors": "Christoph Sch\\\"oller, Maximilian Schnettler, Annkathrin Kr\\\"ammer,\n  Gereon Hinz, Maida Bakovic, M\\\"uge G\\\"uzet, Alois Knoll", "title": "Targetless Rotational Auto-Calibration of Radar and Camera for\n  Intelligent Transportation Systems", "comments": "Accepted at the IEEE Intelligent Transportation Systems Conference\n  (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most intelligent transportation systems use a combination of radar sensors\nand cameras for robust vehicle perception. The calibration of these\nheterogeneous sensor types in an automatic fashion during system operation is\nchallenging due to differing physical measurement principles and the high\nsparsity of traffic radars. We propose - to the best of our knowledge - the\nfirst data-driven method for automatic rotational radar-camera calibration\nwithout dedicated calibration targets. Our approach is based on a coarse and a\nfine convolutional neural network. We employ a boosting-inspired training\nalgorithm, where we train the fine network on the residual error of the coarse\nnetwork. Due to the unavailability of public datasets combining radar and\ncamera measurements, we recorded our own real-world data. We demonstrate that\nour method is able to reach precise and robust sensor registration and show its\ngeneralization capabilities to different sensor alignments and perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:02:34 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 16:47:25 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Sch\u00f6ller", "Christoph", ""], ["Schnettler", "Maximilian", ""], ["Kr\u00e4mmer", "Annkathrin", ""], ["Hinz", "Gereon", ""], ["Bakovic", "Maida", ""], ["G\u00fczet", "M\u00fcge", ""], ["Knoll", "Alois", ""]]}, {"id": "1904.08755", "submitter": "Chris Choy", "authors": "Christopher Choy, JunYoung Gwak, Silvio Savarese", "title": "4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks", "comments": "CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotics and VR/AR applications, 3D-videos are readily-available\nsources of input (a continuous sequence of depth images, or LIDAR scans).\nHowever, those 3D-videos are processed frame-by-frame either through 2D\nconvnets or 3D perception algorithms. In this work, we propose 4-dimensional\nconvolutional neural networks for spatio-temporal perception that can directly\nprocess such 3D-videos using high-dimensional convolutions. For this, we adopt\nsparse tensors and propose the generalized sparse convolution that encompasses\nall discrete convolutions. To implement the generalized sparse convolution, we\ncreate an open-source auto-differentiation library for sparse tensors that\nprovides extensive functions for high-dimensional convolutional neural\nnetworks. We create 4D spatio-temporal convolutional neural networks using the\nlibrary and validate them on various 3D semantic segmentation benchmarks and\nproposed 4D datasets for 3D-video perception. To overcome challenges in the 4D\nspace, we propose the hybrid kernel, a special case of the generalized sparse\nconvolution, and the trilateral-stationary conditional random field that\nenforces spatio-temporal consistency in the 7D space-time-chroma space.\nExperimentally, we show that convolutional neural networks with only\ngeneralized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by\na large margin. Also, we show that on 3D-videos, 4D spatio-temporal\nconvolutional neural networks are robust to noise, outperform 3D convolutional\nneural networks and are faster than the 3D counterpart in some cases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:19:50 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 02:17:59 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 03:26:42 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 23:00:57 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Choy", "Christopher", ""], ["Gwak", "JunYoung", ""], ["Savarese", "Silvio", ""]]}, {"id": "1904.08760", "submitter": "Amjad Rehman Dr", "authors": "Amjad Rehman, Majid Harouni, Tanzila Saba", "title": "Cursive Multilingual Characters Recognition Based on Hard Geometric\n  Features", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cursive nature of multilingual characters segmentation and recognition of\nArabic, Persian, Urdu languages have attracted researchers from academia and\nindustry. However, despite several decades of research, still multilingual\ncharacters classification accuracy is not up to the mark. This paper presents\nan automated approach for multilingual characters segmentation and recognition.\nThe proposed methodology explores character based on their geometric features.\nHowever, due to uncertainty and without dictionary support few characters are\nover-divided. To expand the productivity of the proposed methodology a BPN is\nprepared with countless division focuses for cursive multilingual characters.\nPrepared BPN separates off base portioned indicates effectively with rapid\nupgrade character acknowledgment precision. For reasonable examination, only\nbenchmark dataset is utilized.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 08:26:38 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Rehman", "Amjad", ""], ["Harouni", "Majid", ""], ["Saba", "Tanzila", ""]]}, {"id": "1904.08764", "submitter": "Jaakko Sahlsten", "authors": "Jaakko Sahlsten, Joel Jaskari, Jyri Kivinen, Lauri Turunen, Esa\n  Jaanio, Kustaa Hietala and Kimmo Kaski", "title": "Deep Learning Fundus Image Analysis for Diabetic Retinopathy and Macular\n  Edema Grading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetes is a globally prevalent disease that can cause visible microvascular\ncomplications such as diabetic retinopathy and macular edema in the human eye\nretina, the images of which are today used for manual disease screening. This\nlabor-intensive task could greatly benefit from automatic detection using deep\nlearning technique. Here we present a deep learning system that identifies\nreferable diabetic retinopathy comparably or better than presented in the\nprevious studies, although we use only a small fraction of images (<1/4) in\ntraining but are aided with higher image resolutions. We also provide novel\nresults for five different screening and clinical grading systems for diabetic\nretinopathy and macular edema classification, including results for accurately\nclassifying images according to clinical five-grade diabetic retinopathy and\nfour-grade diabetic macular edema scales. These results suggest, that a deep\nlearning system could increase the cost-effectiveness of screening while\nattaining higher than recommended performance, and that the system could be\napplied in clinical examinations requiring finer grading.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:00:40 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Sahlsten", "Jaakko", ""], ["Jaskari", "Joel", ""], ["Kivinen", "Jyri", ""], ["Turunen", "Lauri", ""], ["Jaanio", "Esa", ""], ["Hietala", "Kustaa", ""], ["Kaski", "Kimmo", ""]]}, {"id": "1904.08771", "submitter": "Fabian Eitel", "authors": "Fabian Eitel, Emily Soehler, Judith Bellmann-Strobl, Alexander U.\n  Brandt, Klemens Ruprecht, Ren\\'e M. Giess, Joseph Kuchling, Susanna Asseyer,\n  Martin Weygandt, John-Dylan Haynes, Michael Scheel, Friedemann Paul, Kerstin\n  Ritter", "title": "Uncovering convolutional neural network decisions for diagnosing\n  multiple sclerosis on conventional MRI using layer-wise relevance propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning-based imaging diagnostics has recently reached or even\nsuperseded the level of clinical experts in several clinical domains. However,\nclassification decisions of a trained machine learning system are typically\nnon-transparent, a major hindrance for clinical integration, error tracking or\nknowledge discovery. In this study, we present a transparent deep learning\nframework relying on convolutional neural networks (CNNs) and layer-wise\nrelevance propagation (LRP) for diagnosing multiple sclerosis (MS). MS is\ncommonly diagnosed utilizing a combination of clinical presentation and\nconventional magnetic resonance imaging (MRI), specifically the occurrence and\npresentation of white matter lesions in T2-weighted images. We hypothesized\nthat using LRP in a naive predictive model would enable us to uncover relevant\nimage features that a trained CNN uses for decision-making. Since imaging\nmarkers in MS are well-established this would enable us to validate the\nrespective CNN model. First, we pre-trained a CNN on MRI data from the\nAlzheimer's Disease Neuroimaging Initiative (n = 921), afterwards specializing\nthe CNN to discriminate between MS patients and healthy controls (n = 147).\nUsing LRP, we then produced a heatmap for each subject in the holdout set\ndepicting the voxel-wise relevance for a particular classification decision.\nThe resulting CNN model resulted in a balanced accuracy of 87.04% and an area\nunder the curve of 96.08% in a receiver operating characteristic curve. The\nsubsequent LRP visualization revealed that the CNN model focuses indeed on\nindividual lesions, but also incorporates additional information such as lesion\nlocation, non-lesional white matter or gray matter areas such as the thalamus,\nwhich are established conventional and advanced MRI markers in MS. We conclude\nthat LRP and the proposed framework have the capability to make diagnostic\ndecisions of...\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:37:07 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Eitel", "Fabian", ""], ["Soehler", "Emily", ""], ["Bellmann-Strobl", "Judith", ""], ["Brandt", "Alexander U.", ""], ["Ruprecht", "Klemens", ""], ["Giess", "Ren\u00e9 M.", ""], ["Kuchling", "Joseph", ""], ["Asseyer", "Susanna", ""], ["Weygandt", "Martin", ""], ["Haynes", "John-Dylan", ""], ["Scheel", "Michael", ""], ["Paul", "Friedemann", ""], ["Ritter", "Kerstin", ""]]}, {"id": "1904.08773", "submitter": "Liu Qing", "authors": "Qing Liu and Xiaopeng Hong and Wei Ke and Zailiang Chen and Beiji Zou", "title": "DDNet: Cartesian-polar Dual-domain Network for the Joint Optic Disc and\n  Cup Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing joint optic disc and cup segmentation approaches are developed\neither in Cartesian or polar coordinate system. However, due to the subtle\noptic cup, the contextual information exploited from the single domain even by\nthe prevailing CNNs is still insufficient. In this paper, we propose a novel\nsegmentation approach, named Cartesian-polar dual-domain network (DDNet), which\nfor the first time considers the complementary of the Cartesian domain and the\npolar domain. We propose a two-branch of domain feature encoder and learn\ntranslation equivariant representations on rectilinear grid from Cartesian\ndomain and rotation equivariant representations on polar grid from polar domain\nparallelly. To fuse the features on two different grids, we propose a\ndual-domain fusion module. This module builds the correspondence between two\ngrids by the differentiable polar transform layer and learns the feature\nimportance across two domains in element-wise to enhance the expressive\ncapability. Finally, the decoder aggregates the fused features from low-level\nto high-level and makes dense predictions. We validate the state-of-the-art\nsegmentation performances of our DDNet on the public dataset ORIGA. According\nto the segmentation masks, we estimate the commonly used clinical measure for\nglaucoma, i.e., the vertical cup-to-disc ratio. The low cup-to-disc ratio\nestimation error demonstrates the potential application in glaucoma screening.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:38:55 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Qing", ""], ["Hong", "Xiaopeng", ""], ["Ke", "Wei", ""], ["Chen", "Zailiang", ""], ["Zou", "Beiji", ""]]}, {"id": "1904.08796", "submitter": "Eric Eaton", "authors": "Julia E. Reid, Eric Eaton", "title": "Artificial Intelligence for Pediatric Ophthalmology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 01:47:47 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Reid", "Julia E.", ""], ["Eaton", "Eric", ""]]}, {"id": "1904.08801", "submitter": "Matthias M\\\"uller", "authors": "Matthias M\\\"uller, Guohao Li, Vincent Casser, Neil Smith, Dominik L.\n  Michels, Bernard Ghanem", "title": "Learning a Controller Fusion Network by Online Trajectory Filtering for\n  Vision-based UAV Racing", "comments": "Accepted at CVPRW'19: UAVision 2019. First two authors contributed\n  equally. Based on the initial work of arXiv:1803.01129 which was eventually\n  split into two separate projects", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous UAV racing has recently emerged as an interesting research\nproblem. The dream is to beat humans in this new fast-paced sport. A common\napproach is to learn an end-to-end policy that directly predicts controls from\nraw images by imitating an expert. However, such a policy is limited by the\nexpert it imitates and scaling to other environments and vehicle dynamics is\ndifficult. One approach to overcome the drawbacks of an end-to-end policy is to\ntrain a network only on the perception task and handle control with a PID or\nMPC controller. However, a single controller must be extensively tuned and\ncannot usually cover the whole state space. In this paper, we propose learning\nan optimized controller using a DNN that fuses multiple controllers. The\nnetwork learns a robust controller with online trajectory filtering, which\nsuppresses noisy trajectories and imperfections of individual controllers. The\nresult is a network that is able to learn a good fusion of filtered\ntrajectories from different controllers leading to significant improvements in\noverall performance. We compare our trained network to controllers it has\nlearned from, end-to-end baselines and human pilots in a realistic simulation;\nour network beats all baselines in extensive experiments and approaches the\nperformance of a professional human pilot. A video summarizing this work is\navailable at https://youtu.be/hGKlE5X9Z5U\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:23:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["M\u00fcller", "Matthias", ""], ["Li", "Guohao", ""], ["Casser", "Vincent", ""], ["Smith", "Neil", ""], ["Michels", "Dominik L.", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.08816", "submitter": "Dong Liu", "authors": "Dong Liu, Haochen Zhang, Zhiwei Xiong", "title": "On The Classification-Distortion-Perception Tradeoff", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32, 2019,\n  https://papers.nips.cc/paper/8404-on-the-classification-distortion-perception-tradeoff", "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal degradation is ubiquitous and computational restoration of degraded\nsignal has been investigated for many years. Recently, it is reported that the\ncapability of signal restoration is fundamentally limited by the\nperception-distortion tradeoff, i.e. the distortion and the perceptual\ndifference between the restored signal and the ideal `original' signal cannot\nbe made both minimal simultaneously. Distortion corresponds to signal fidelity\nand perceptual difference corresponds to perceptual naturalness, both of which\nare important metrics in practice. Besides, there is another dimension worthy\nof consideration, namely the semantic quality or the utility for recognition\npurpose, of the restored signal. In this paper, we extend the previous\nperception-distortion tradeoff to the case of\nclassification-distortion-perception (CDP) tradeoff, where we introduced the\nclassification error rate of the restored signal in addition to distortion and\nperceptual difference. Two versions of the CDP tradeoff are considered, one\nusing a predefined classifier and the other dealing with the optimal classifier\nfor the restored signal. For both versions, we can rigorously prove the\nexistence of the CDP tradeoff, i.e. the distortion, perceptual difference, and\nclassification error rate cannot be made all minimal simultaneously. Our\nfindings can be useful especially for computer vision researches where some\nlow-level vision tasks (signal restoration) serve for high-level vision tasks\n(visual understanding).\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:43:29 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Dong", ""], ["Zhang", "Haochen", ""], ["Xiong", "Zhiwei", ""]]}, {"id": "1904.08818", "submitter": "Sandra Avila", "authors": "Alceu Bissoto and Michel Fornaciali and Eduardo Valle and Sandra Avila", "title": "(De)Constructing Bias on Skin Lesion Datasets", "comments": "9 pages, 6 figures. Paper accepted at 2019 ISIC Skin Image Anaylsis\n  Workshop @ IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is the deadliest form of skin cancer. Automated skin lesion analysis\nplays an important role for early detection. Nowadays, the ISIC Archive and the\nAtlas of Dermoscopy dataset are the most employed skin lesion sources to\nbenchmark deep-learning based tools. However, all datasets contain biases,\noften unintentional, due to how they were acquired and annotated. Those biases\ndistort the performance of machine-learning models, creating spurious\ncorrelations that the models can unfairly exploit, or, contrarily destroying\ncogent correlations that the models could learn. In this paper, we propose a\nset of experiments that reveal both types of biases, positive and negative, in\nexisting skin lesion datasets. Our results show that models can correctly\nclassify skin lesion images without clinically-meaningful information:\ndisturbingly, the machine-learning model learned over images where no\ninformation about the lesion remains, presents an accuracy above the AI\nbenchmark curated with dermatologists' performances. That strongly suggests\nspurious correlations guiding the models. We fed models with additional\nclinically meaningful information, which failed to improve the results even\nslightly, suggesting the destruction of cogent correlations. Our main findings\nraise awareness of the limitations of models trained and evaluated in small\ndatasets such as the ones we evaluated, and may suggest future guidelines for\nmodels intended for real-world deployment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:49:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Bissoto", "Alceu", ""], ["Fornaciali", "Michel", ""], ["Valle", "Eduardo", ""], ["Avila", "Sandra", ""]]}, {"id": "1904.08825", "submitter": "Ronnachai Jaroensri", "authors": "Ronnachai Jaroensri, Camille Biscarrat, Miika Aittala, Fr\\'edo Durand", "title": "Generating Training Data for Denoising Real RGB Images via Camera\n  Pipeline Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image reconstruction techniques such as denoising often need to be applied to\nthe RGB output of cameras and cellphones. Unfortunately, the commonly used\nadditive white noise (AWGN) models do not accurately reproduce the noise and\nthe degradation encountered on these inputs. This is particularly important for\nlearning-based techniques, because the mismatch between training and real world\ndata will hurt their generalization. This paper aims to accurately simulate the\ndegradation and noise transformation performed by camera pipelines. This allows\nus to generate realistic degradation in RGB images that can be used to train\nmachine learning models. We use our simulation to study the importance of noise\nmodeling for learning-based denoising. Our study shows that a realistic noise\nmodel is required for learning to denoise real JPEG images. A neural network\ntrained on realistic noise outperforms the one trained with AWGN by 3 dB. An\nablation study of our pipeline shows that simulating denoising and demosaicking\nis important to this improvement and that realistic demosaicking algorithms,\nwhich have been rarely considered, is needed. We believe this simulation will\nalso be useful for other image reconstruction tasks, and we will distribute our\ncode publicly.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:04:48 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Jaroensri", "Ronnachai", ""], ["Biscarrat", "Camille", ""], ["Aittala", "Miika", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "1904.08864", "submitter": "Haoyi Liang", "authors": "Haoyi Liang and Aijaz Naik and Cedric L. Williams and Jaideep Kapur\n  and Daniel S. Weller", "title": "Enhanced Center Coding for Cell Detection with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell imaging and analysis are fundamental to biomedical research because\ncells are the basic functional units of life. Among different cell-related\nanalysis, cell counting and detection are widely used. In this paper, we focus\non one common step of learning-based cell counting approaches: coding the raw\ndot labels into more suitable maps for learning. Two criteria of coding raw dot\nlabels are discussed, and a new coding scheme is proposed in this paper. The\ntwo criteria measure how easy it is to train the model with a coding scheme,\nand how robust the recovered raw dot labels are when predicting. The most\ncompelling advantage of the proposed coding scheme is the ability to\ndistinguish neighboring cells in crowded regions. Cell counting and detection\nexperiments are conducted for five coding schemes on four types of cells and\ntwo network architectures. The proposed coding scheme improves the counting\naccuracy versus the widely-used Gaussian and rectangle kernels up to 12%, and\nalso improves the detection accuracy versus the common proximity coding up to\n14%.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:18:01 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liang", "Haoyi", ""], ["Naik", "Aijaz", ""], ["Williams", "Cedric L.", ""], ["Kapur", "Jaideep", ""], ["Weller", "Daniel S.", ""]]}, {"id": "1904.08868", "submitter": "Hina Afridi", "authors": "Abdullah J. Alzahrani, Hina Afridi", "title": "Salient Object Detection: A Distinctive Feature Integration Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for salient object detection in different images.\nOur method integrates spatial features for efficient and robust representation\nto capture meaningful information about the salient objects. We then train a\nconditional random field (CRF) using the integrated features. The trained CRF\nmodel is then used to detect salient objects during the online testing stage.\nWe perform experiments on two standard datasets and compare the performance of\nour method with different reference methods. Our experiments show that our\nmethod outperforms the compared methods in terms of precision, recall, and\nF-Measure.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:23:45 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Alzahrani", "Abdullah J.", ""], ["Afridi", "Hina", ""]]}, {"id": "1904.08879", "submitter": "Xin Fu", "authors": "Jia Yan, Jie Li, Xin Fu", "title": "No-Reference Quality Assessment of Contrast-Distorted Images using\n  Contrast Enhancement", "comments": "Draft version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No-reference image quality assessment (NR-IQA) aims to measure the image\nquality without reference image. However, contrast distortion has been\noverlooked in the current research of NR-IQA. In this paper, we propose a very\nsimple but effective metric for predicting quality of contrast-altered images\nbased on the fact that a high-contrast image is often more similar to its\ncontrast enhanced image. Specifically, we first generate an enhanced image\nthrough histogram equalization. We then calculate the similarity of the\noriginal image and the enhanced one by using structural-similarity index (SSIM)\nas the first feature. Further, we calculate the histogram based entropy and\ncross entropy between the original image and the enhanced one respectively, to\ngain a sum of 4 features. Finally, we learn a regression module to fuse the\naforementioned 5 features for inferring the quality score. Experiments on four\npublicly available databases validate the superiority and efficiency of the\nproposed technique.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:36:43 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yan", "Jia", ""], ["Li", "Jie", ""], ["Fu", "Xin", ""]]}, {"id": "1904.08889", "submitter": "Hugues Thomas", "authors": "Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz\n  Marcotegui, Fran\\c{c}ois Goulette and Leonidas J. Guibas", "title": "KPConv: Flexible and Deformable Convolution for Point Clouds", "comments": "Camera-ready, accepted to ICCV 2019; project website:\n  https://github.com/HuguesTHOMAS/KPConv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Kernel Point Convolution (KPConv), a new design of point\nconvolution, i.e. that operates on point clouds without any intermediate\nrepresentation. The convolution weights of KPConv are located in Euclidean\nspace by kernel points, and applied to the input points close to them. Its\ncapacity to use any number of kernel points gives KPConv more flexibility than\nfixed grid convolutions. Furthermore, these locations are continuous in space\nand can be learned by the network. Therefore, KPConv can be extended to\ndeformable convolutions that learn to adapt kernel points to local geometry.\nThanks to a regular subsampling strategy, KPConv is also efficient and robust\nto varying densities. Whether they use deformable KPConv for complex tasks, or\nrigid KPconv for simpler tasks, our networks outperform state-of-the-art\nclassification and segmentation approaches on several datasets. We also offer\nablation studies and visualizations to provide understanding of what has been\nlearned by KPConv and to validate the descriptive power of deformable KPConv.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:55:44 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 14:45:58 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Thomas", "Hugues", ""], ["Qi", "Charles R.", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Marcotegui", "Beatriz", ""], ["Goulette", "Fran\u00e7ois", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1904.08900", "submitter": "Hei Law", "authors": "Hei Law, Yun Teng, Olga Russakovsky, Jia Deng", "title": "CornerNet-Lite: Efficient Keypoint Based Object Detection", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint-based methods are a relatively new paradigm in object detection,\neliminating the need for anchor boxes and offering a simplified detection\nframework. Keypoint-based CornerNet achieves state of the art accuracy among\nsingle-stage detectors. However, this accuracy comes at high processing cost.\nIn this work, we tackle the problem of efficient keypoint-based object\ndetection and introduce CornerNet-Lite. CornerNet-Lite is a combination of two\nefficient variants of CornerNet: CornerNet-Saccade, which uses an attention\nmechanism to eliminate the need for exhaustively processing all pixels of the\nimage, and CornerNet-Squeeze, which introduces a new compact backbone\narchitecture. Together these two variants address the two critical use cases in\nefficient object detection: improving efficiency without sacrificing accuracy,\nand improving accuracy at real-time efficiency. CornerNet-Saccade is suitable\nfor offline processing, improving the efficiency of CornerNet by 6.0x and the\nAP by 1.0% on COCO. CornerNet-Squeeze is suitable for real-time detection,\nimproving both the efficiency and accuracy of the popular real-time detector\nYOLOv3 (34.4% AP at 30ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for\nYOLOv3 on COCO). Together these contributions for the first time reveal the\npotential of keypoint-based detection to be useful for applications requiring\nprocessing efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:21:15 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 23:22:32 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Law", "Hei", ""], ["Teng", "Yun", ""], ["Russakovsky", "Olga", ""], ["Deng", "Jia", ""]]}, {"id": "1904.08910", "submitter": "Sandra Avila", "authors": "Akari Ishikawa and Edson Bollis and Sandra Avila", "title": "Combating the Elsagate phenomenon: Deep learning architectures for\n  disturbing cartoons", "comments": "6 pages, 5 figures, 2 tables. Paper accepted at 7th IAPR/IEEE\n  International Workshop on Biometrics and Forensics (IWBF)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching cartoons can be useful for children's intellectual, social and\nemotional development. However, the most popular video sharing platform today\nprovides many videos with Elsagate content. Elsagate is a phenomenon that\ndepicts childhood characters in disturbing circumstances (e.g., gore, toilet\nhumor, drinking urine, stealing). Even with this threat easily available for\nchildren, there is no work in the literature addressing the problem. As the\nfirst to explore disturbing content in cartoons, we proceed from the most\nrecent pornography detection literature applying deep convolutional neural\nnetworks combined with static and motion information of the video. Our solution\nis compatible with mobile platforms and achieved 92.6% of accuracy. Our goal is\nnot only to introduce the first solution but also to bring up the discussion\naround Elsagate.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:43:42 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Ishikawa", "Akari", ""], ["Bollis", "Edson", ""], ["Avila", "Sandra", ""]]}, {"id": "1904.08913", "submitter": "Wei-Chiu Ma", "authors": "Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, Raquel Urtasun", "title": "Deep Rigid Instance Scene Flow", "comments": "CVPR 2019. Rank 1st on KITTI scene flow benchmark. 800 times faster\n  than prior art", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of scene flow estimation in the context\nof self-driving. We leverage deep learning techniques as well as strong priors\nas in our application domain the motion of the scene can be composed by the\nmotion of the robot and the 3D motion of the actors in the scene. We formulate\nthe problem as energy minimization in a deep structured model, which can be\nsolved efficiently in the GPU by unrolling a Gaussian-Newton solver. Our\nexperiments in the challenging KITTI scene flow dataset show that we outperform\nthe state-of-the-art by a very large margin, while being 800 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:48:26 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Ma", "Wei-Chiu", ""], ["Wang", "Shenlong", ""], ["Hu", "Rui", ""], ["Xiong", "Yuwen", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1904.08916", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "Early Detection of Injuries in MLB Pitchers from Video", "comments": "CVPR Workshop on Computer Vision in Sports 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Injuries are a major cost in sports. Teams spend millions of dollars every\nyear on players who are hurt and unable to play, resulting in lost games,\ndecreased fan interest and additional wages for replacement players. Modern\nconvolutional neural networks have been successfully applied to many video\nrecognition tasks. In this paper, we introduce the problem of injury\ndetection/prediction in MLB pitchers and experimentally evaluate the ability of\nsuch convolutional models to detect and predict injuries in pitches only from\nvideo data. We conduct experiments on a large dataset of TV broadcast MLB\nvideos of 20 different pitchers who were injured during the 2017 season. We\nexperimentally evaluate the model's performance on each individual pitcher, how\nwell it generalizes to new pitchers, how it performs for various injuries, and\nhow early it can predict or detect an injury.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:52:01 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1904.08918", "submitter": "Kevis-Kokitsi Maninis", "authors": "Kevis-Kokitsi Maninis, Ilija Radosavovic, Iasonas Kokkinos", "title": "Attentive Single-Tasking of Multiple Tasks", "comments": "CVPR 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address task interference in universal networks by\nconsidering that a network is trained on multiple tasks, but performs one task\nat a time, an approach we refer to as \"single-tasking multiple tasks\". The\nnetwork thus modifies its behaviour through task-dependent feature adaptation,\nor task attention. This gives the network the ability to accentuate the\nfeatures that are adapted to a task, while shunning irrelevant ones. We further\nreduce task interference by forcing the task gradients to be statistically\nindistinguishable through adversarial training, ensuring that the common\nbackbone architecture serving all tasks is not dominated by any of the\ntask-specific gradients. Results in three multi-task dense labelling problems\nconsistently show: (i) a large reduction in the number of parameters while\npreserving, or even improving performance and (ii) a smooth trade-off between\ncomputation and multi-task accuracy. We provide our system's code and\npre-trained models at http://vision.ee.ethz.ch/~kmaninis/astmt/.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:54:30 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Radosavovic", "Ilija", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1904.08920", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen,\n  Dhruv Batra, Devi Parikh, Marcus Rohrbach", "title": "Towards VQA Models That Can Read", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that a dominant class of questions asked by visually\nimpaired users on images of their surroundings involves reading text in the\nimage. But today's VQA models can not read! Our paper takes a first step\ntowards addressing this problem. First, we introduce a new \"TextVQA\" dataset to\nfacilitate progress on this important problem. Existing datasets either have a\nsmall proportion of questions about text (e.g., the VQA dataset) or are too\nsmall (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408\nimages that require reasoning about text to answer. Second, we introduce a\nnovel model architecture that reads text in the image, reasons about it in the\ncontext of the image and the question, and predicts an answer which might be a\ndeduction based on the text and the image or composed of the strings found in\nthe image. Consequently, we call our approach Look, Read, Reason & Answer\n(LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on\nour TextVQA dataset. We find that the gap between human performance and machine\nperformance is significantly larger on TextVQA than on VQA 2.0, suggesting that\nTextVQA is well-suited to benchmark progress along directions complementary to\nVQA 2.0.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:55:37 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 23:28:48 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Singh", "Amanpreet", ""], ["Natarajan", "Vivek", ""], ["Shah", "Meet", ""], ["Jiang", "Yu", ""], ["Chen", "Xinlei", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1904.08921", "submitter": "Dmitriy Smirnov", "authors": "Dmitriy Smirnov, Matthew Fisher, Vladimir G. Kim, Richard Zhang,\n  Justin Solomon", "title": "Deep Parametric Shape Predictions using Distance Fields", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in graphics and vision demand machinery for converting shapes into\nconsistent representations with sparse sets of parameters; these\nrepresentations facilitate rendering, editing, and storage. When the source\ndata is noisy or ambiguous, however, artists and engineers often manually\nconstruct such representations, a tedious and potentially time-consuming\nprocess. While advances in deep learning have been successfully applied to\nnoisy geometric data, the task of generating parametric shapes has so far been\ndifficult for these methods. Hence, we propose a new framework for predicting\nparametric shape primitives using deep learning. We use distance fields to\ntransition between shape parameters like control points and input data on a\npixel grid. We demonstrate efficacy on 2D and 3D tasks, including font\nvectorization and surface abstraction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:55:57 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 15:07:10 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Smirnov", "Dmitriy", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Zhang", "Richard", ""], ["Solomon", "Justin", ""]]}, {"id": "1904.08928", "submitter": "Shuai Shao", "authors": "Yan-Jiang Wang and Shuai Shao and Rui Xu and Werifeng Liu and Bao-Di\n  Liu", "title": "Class specific or shared? A cascaded dictionary learning framework for\n  image classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2020.107697", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning methods can be split into: i) class specific dictionary\nlearning ii) class shared dictionary learning. The difference between the two\ncategories is how to use discriminative information. With the first category,\nsamples of different classes are mapped into different subspaces, which leads\nto some redundancy with the class specific base vectors. While for the second\ncategory, the samples in each specific class can not be described accurately.\nIn this paper, we first propose a novel class shared dictionary learning method\nnamed label embedded dictionary learning (LEDL). It is the improvement based on\nLCKSVD, which is easier to find out the optimal solution. Then we propose a\nnovel framework named cascaded dictionary learning framework (CDLF) to combine\nthe specific dictionary learning with shared dictionary learning to describe\nthe feature to boost the performance of classification sufficiently. Extensive\nexperimental results on six benchmark datasets illustrate that our methods are\ncapable of achieving superior performance compared to several state-of-art\nclassification algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:03:36 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:59:55 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wang", "Yan-Jiang", ""], ["Shao", "Shuai", ""], ["Xu", "Rui", ""], ["Liu", "Werifeng", ""], ["Liu", "Bao-Di", ""]]}, {"id": "1904.08939", "submitter": "Anh Nguyen", "authors": "Anh Nguyen and Jason Yosinski and Jeff Clune", "title": "Understanding Neural Networks via Feature Visualization: A survey", "comments": "A book chapter in an Interpretable ML book\n  (http://www.interpretable-ml.org/book/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neuroscience method to understanding the brain is to find and study the\npreferred stimuli that highly activate an individual cell or groups of cells.\nRecent advances in machine learning enable a family of methods to synthesize\npreferred stimuli that cause a neuron in an artificial or biological brain to\nfire strongly. Those methods are known as Activation Maximization (AM) or\nFeature Visualization via Optimization. In this chapter, we (1) review existing\nAM techniques in the literature; (2) discuss a probabilistic interpretation for\nAM; and (3) review the applications of AM in debugging and explaining networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:46:26 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1904.08959", "submitter": "Xuan Si", "authors": "Xingjian Du, Xuan Shi, Risheng Huang", "title": "RepGN:Object Detection with Relational Proposal Graph Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Region based object detectors achieve the state-of-the-art performance, but\nfew consider to model the relation of proposals. In this paper, we explore the\nidea of modeling the relationships among the proposals for object detection\nfrom the graph learning perspective. Specifically, we present relational\nproposal graph network (RepGN) which is defined on object proposals and the\nsemantic and spatial relation modeled as the edge. By integrating our RepGN\nmodule into object detectors, the relation and context constraints will be\nintroduced to the feature extraction of regions and bounding boxes regression\nand classification. Besides, we propose a novel graph-cut based pooling layer\nfor hierarchical coarsening of the graph, which empowers the RepGN module to\nexploit the inter-regional correlation and scene description in a hierarchical\nmanner. We perform extensive experiments on COCO object detection dataset and\nshow promising results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 18:07:20 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Du", "Xingjian", ""], ["Shi", "Xuan", ""], ["Huang", "Risheng", ""]]}, {"id": "1904.08963", "submitter": "Zhipeng Ding", "authors": "Zhipeng Ding, Xu Han, Marc Niethammer", "title": "VoteNet: A Deep Learning Label Fusion Method for Multi-Atlas\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) approaches are state-of-the-art for many medical image\nsegmentation tasks. They offer a number of advantages: they can be trained for\nspecific tasks, computations are fast at test time, and segmentation quality is\ntypically high. In contrast, previously popular multi-atlas segmentation (MAS)\nmethods are relatively slow (as they rely on costly registrations) and even\nthough sophisticated label fusion strategies have been proposed, DL approaches\ngenerally outperform MAS. In this work, we propose a DL-based label fusion\nstrategy (VoteNet) which locally selects a set of reliable atlases whose labels\nare then fused via plurality voting. Experiments on 3D brain MRI data show that\nby selecting a good initial atlas set MAS with VoteNet significantly\noutperforms a number of other label fusion strategies as well as a direct DL\nsegmentation approach. We also provide an experimental analysis of the upper\nperformance bound achievable by our method. While unlikely achievable in\npractice, this bound suggests room for further performance improvements.\nLastly, to address the runtime disadvantage of standard MAS, all our results\nmake use of a fast DL registration approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 18:16:18 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 02:46:44 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Ding", "Zhipeng", ""], ["Han", "Xu", ""], ["Niethammer", "Marc", ""]]}, {"id": "1904.08980", "submitter": "Felipe Codevilla", "authors": "Felipe Codevilla, Eder Santana, Antonio M. L\\'opez and Adrien Gaidon", "title": "Exploring the Limitations of Behavior Cloning for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving requires reacting to a wide variety of complex environment conditions\nand agent behaviors. Explicitly modeling each possible scenario is unrealistic.\nIn contrast, imitation learning can, in theory, leverage data from large fleets\nof human-driven cars. Behavior cloning in particular has been successfully used\nto learn simple visuomotor policies end-to-end, but scaling to the full\nspectrum of driving behaviors remains an unsolved problem. In this paper, we\npropose a new benchmark to experimentally investigate the scalability and\nlimitations of behavior cloning. We show that behavior cloning leads to\nstate-of-the-art results, including in unseen environments, executing complex\nlateral and longitudinal maneuvers without these reactions being explicitly\nprogrammed. However, we confirm well-known limitations (due to dataset bias and\noverfitting), new generalization issues (due to dynamic objects and the lack of\na causal model), and training instability requiring further research before\nbehavior cloning can graduate to real-world driving. The code of the studied\nbehavior cloning approaches can be found at\nhttps://github.com/felipecode/coiltraine .\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 19:29:56 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Codevilla", "Felipe", ""], ["Santana", "Eder", ""], ["L\u00f3pez", "Antonio M.", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1904.09007", "submitter": "Nico Engel", "authors": "Nico Engel, Stefan Hoermann, Markus Horn, Vasileios Belagiannis, Klaus\n  Dietmayer", "title": "DeepLocalization: Landmark-based Self-Localization with Deep Neural\n  Networks", "comments": "Accepted for publication by the IEEE Intelligent Transportation\n  Systems Conference (ITSC 2019), Auckland, New Zealand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of vehicle self-localization from multi-modal sensor\ninformation and a reference map. The map is generated off-line by extracting\nlandmarks from the vehicle's field of view, while the measurements are\ncollected similarly on the fly. Our goal is to determine the autonomous\nvehicle's pose from the landmark measurements and map landmarks. To learn this\nmapping, we propose DeepLocalization, a deep neural network that regresses the\nvehicle's translation and rotation parameters from unordered and dynamic input\nlandmarks. The proposed network architecture is robust to changes of the\ndynamic environment and can cope with a small number of extracted landmarks.\nDuring the training process we rely on synthetically generated ground-truth. In\nour experiments, we evaluate two inference approaches in real-world scenarios.\nWe show that DeepLocalization can be combined with regular GPS signals and\nfiltering algorithms such as the extended Kalman filter. Our approach achieves\nstate-of-the-art accuracy and is about ten times faster than the related work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 20:41:10 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:24:45 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Engel", "Nico", ""], ["Hoermann", "Stefan", ""], ["Horn", "Markus", ""], ["Belagiannis", "Vasileios", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1904.09013", "submitter": "Andrew Rouditchenko", "authors": "Andrew Rouditchenko, Hang Zhao, Chuang Gan, Josh McDermott, Antonio\n  Torralba", "title": "Self-Supervised Audio-Visual Co-Segmentation", "comments": "Accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting objects in images and separating sound sources in audio are\nchallenging tasks, in part because traditional approaches require large amounts\nof labeled data. In this paper we develop a neural network model for visual\nobject segmentation and sound source separation that learns from natural videos\nthrough self-supervision. The model is an extension of recently proposed work\nthat maps image pixels to sounds. Here, we introduce a learning approach to\ndisentangle concepts in the neural networks, and assign semantic categories to\nnetwork feature channels to enable independent image segmentation and sound\nsource separation after audio-visual training on videos. Our evaluations show\nthat the disentangled model outperforms several baselines in semantic\nsegmentation and sound source separation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 21:11:03 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Rouditchenko", "Andrew", ""], ["Zhao", "Hang", ""], ["Gan", "Chuang", ""], ["McDermott", "Josh", ""], ["Torralba", "Antonio", ""]]}, {"id": "1904.09021", "submitter": "Saeed Arabi", "authors": "Saeed Arabi, Arya Haghighat, Anuj Sharma", "title": "A deep learning based solution for construction equipment detection:\n  from development to deployment", "comments": "17 pages, 16 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing researchers and engineering professionals with a\npractical and comprehensive deep learning based solution to detect construction\nequipment from the very first step of its development to the last one which is\ndeployment. This paper focuses on the last step of deployment. The first phase\nof solution development, involved data preparation, model selection, model\ntraining, and model evaluation. The second phase of the study comprises of\nmodel optimization, application specific embedded system selection, and\neconomic analysis. Several embedded systems were proposed and compared. The\nreview of the results confirms superior real-time performance of the solutions\nwith a consistent above 90% rate of accuracy. The current study validates the\npracticality of deep learning based object detection solutions for construction\nscenarios. Moreover, the detailed knowledge, presented in this study, can be\nemployed for several purposes such as, safety monitoring, productivity\nassessments, and managerial decisions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 21:37:04 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Arabi", "Saeed", ""], ["Haghighat", "Arya", ""], ["Sharma", "Anuj", ""]]}, {"id": "1904.09028", "submitter": "Liqian Ma", "authors": "Liqian Ma, Qianru Sun, Bernt Schiele, Luc Van Gool", "title": "A Novel BiLevel Paradigm for Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image-to-image (I2I) translation is a pixel-level mapping that requires a\nlarge number of paired training data and often suffers from the problems of\nhigh diversity and strong category bias in image scenes. In order to tackle\nthese problems, we propose a novel BiLevel (BiL) learning paradigm that\nalternates the learning of two models, respectively at an instance-specific\n(IS) and a general-purpose (GP) level. In each scene, the IS model learns to\nmaintain the specific scene attributes. It is initialized by the GP model that\nlearns from all the scenes to obtain the generalizable translation knowledge.\nThis GP initialization gives the IS model an efficient starting point, thus\nenabling its fast adaptation to the new scene with scarce training data. We\nconduct extensive I2I translation experiments on human face and street view\ndatasets. Quantitative results validate that our approach can significantly\nboost the performance of classical I2I translation models, such as PG2 and\nPix2Pix. Our visualization results show both higher image quality and more\nappropriate instance-specific details, e.g., the translated image of a person\nlooks more like that person in terms of identity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:32:51 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Ma", "Liqian", ""], ["Sun", "Qianru", ""], ["Schiele", "Bernt", ""], ["Van Gool", "Luc", ""]]}, {"id": "1904.09030", "submitter": "Ahmad Wasfi Bitar", "authors": "Ahmad W. Bitar, Jean-Philippe Ovarlez, Loong-Fah Cheong, Ali Chehab", "title": "Automatic Target Detection for Sparse Hyperspectral Images", "comments": "Accepted for publication in the book \"Hyperspectral Image Analysis -\n  Advances in Signal Processing and Machine Learning\". arXiv admin note: text\n  overlap with arXiv:1711.08970, arXiv:1808.06490", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel target detector for hyperspectral imagery is developed.\nThe detector is independent on the unknown covariance matrix, behaves well in\nlarge dimensions, distributional free, invariant to atmospheric effects, and\ndoes not require a background dictionary to be constructed. Based on a\nmodification of the robust principal component analysis (RPCA), a given\nhyperspectral image (HSI) is regarded as being made up of the sum of a low-rank\nbackground HSI and a sparse target HSI that contains the targets based on a\npre-learned target dictionary specified by the user. The sparse component is\ndirectly used for the detection, that is, the targets are simply detected at\nthe non-zero entries of the sparse target HSI. Hence, a novel target detector\nis developed, which is simply a sparse HSI generated automatically from the\noriginal HSI, but containing only the targets with the background is\nsuppressed. The detector is evaluated on real experiments, and the results of\nwhich demonstrate its effectiveness for hyperspectral target detection\nespecially when the targets are well matched to the surroundings.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 12:00:54 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 14:27:55 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 15:21:11 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Bitar", "Ahmad W.", ""], ["Ovarlez", "Jean-Philippe", ""], ["Cheong", "Loong-Fah", ""], ["Chehab", "Ali", ""]]}, {"id": "1904.09035", "submitter": "Bin Wang", "authors": "Bin Wang, Yanan Sun, Bing Xue and Mengjie Zhang", "title": "Evolving Deep Neural Networks by Multi-objective Particle Swarm\n  Optimization for Image Classification", "comments": "conditionally accepted by gecco2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have become deeper in\norder to achieve better classification accuracy in image classification.\nHowever, it is difficult to deploy the state-of-the-art deep CNNs for\nindustrial use due to the difficulty of manually fine-tuning the\nhyperparameters and the trade-off between classification accuracy and\ncomputational cost. This paper proposes a novel multi-objective optimization\nmethod for evolving state-of-the-art deep CNNs in real-life applications, which\nautomatically evolves the non-dominant solutions at the Pareto front. Three\nmajor contributions are made: Firstly, a new encoding strategy is designed to\nencode one of the best state-of-the-art CNNs; With the classification accuracy\nand the number of floating point operations as the two objectives, a\nmulti-objective particle swarm optimization method is developed to evolve the\nnon-dominant solutions; Last but not least, a new infrastructure is designed to\nboost the experiments by concurrently running the experiments on multiple GPUs\nacross multiple machines, and a Python library is developed and released to\nmanage the infrastructure. The experimental results demonstrate that the\nnon-dominant solutions found by the proposed algorithm form a clear Pareto\nfront, and the proposed infrastructure is able to almost linearly reduce the\nrunning time.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 02:55:14 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 04:07:17 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1904.09037", "submitter": "Chu Wang", "authors": "Chu Wang and Lei Tang and Yang Lu and Shujun Bian and Hirohisa Fujita\n  and Da Zhang and Zuohua Zhang and Yongning Wu", "title": "ProductNet: a Collection of High-Quality Datasets for Product\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ProductNet is a collection of high-quality product datasets for better\nproduct understanding. Motivated by ImageNet, ProductNet aims at supporting\nproduct representation learning by curating product datasets of high quality\nwith properly chosen taxonomy. In this paper, the two goals of building\nhigh-quality product datasets and learning product representation support each\nother in an iterative fashion: the product embedding is obtained via a\nmulti-modal deep neural network (master model) designed to leverage product\nimage and catalog information; and in return, the embedding is utilized via\nactive learning (local model) to vastly accelerate the annotation process. For\nthe labeled data, the proposed master model yields high categorization accuracy\n(94.7% top-1 accuracy for 1240 classes), which can be used as search indices,\npartition keys, and input features for machine learning models. The product\nembedding, as well as the fined-tuned master model for a specific business\ntask, can also be used for various transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 23:17:07 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wang", "Chu", ""], ["Tang", "Lei", ""], ["Lu", "Yang", ""], ["Bian", "Shujun", ""], ["Fujita", "Hirohisa", ""], ["Zhang", "Da", ""], ["Zhang", "Zuohua", ""], ["Wu", "Yongning", ""]]}, {"id": "1904.09039", "submitter": "Yi Tian Xu", "authors": "Yi Tian Xu, Yaqiao Li, David Meger", "title": "Human Motion Prediction via Pattern Completion in Latent Representation\n  Space", "comments": "Accepted in the 16th Conference on Computer and Robot Vision (CRV\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by ideas in cognitive science, we propose a novel and general\napproach to solve human motion understanding via pattern completion on a\nlearned latent representation space. Our model outperforms current\nstate-of-the-art methods in human motion prediction across a number of tasks,\nwith no customization. To construct a latent representation for time-series of\nvarious lengths, we propose a new and generic autoencoder based on\nsequence-to-sequence learning. While traditional inference strategies find a\ncorrelation between an input and an output, we use pattern completion, which\nviews the input as a partial pattern and to predict the best corresponding\ncomplete pattern. Our results demonstrate that this approach has advantages\nwhen combined with our autoencoder in solving human motion prediction, motion\ngeneration and action classification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 23:36:19 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Xu", "Yi Tian", ""], ["Li", "Yaqiao", ""], ["Meger", "David", ""]]}, {"id": "1904.09048", "submitter": "Michael Weber", "authors": "Michael Weber and Michael F\\\"urst and J. Marius Z\\\"ollner", "title": "Automated Focal Loss for Image based Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art object detection algorithms still suffer the problem\nof imbalanced distribution of training data over object classes and background.\nRecent work introduced a new loss function called focal loss to mitigate this\nproblem, but at the cost of an additional hyperparameter. Manually tuning this\nhyperparameter for each training task is highly time-consuming.\n  With automated focal loss we introduce a new loss function which substitutes\nthis hyperparameter by a parameter that is automatically adapted during the\ntraining progress and controls the amount of focusing on hard training\nexamples. We show on the COCO benchmark that this leads to an up to 30% faster\ntraining convergence. We further introduced a focal regression loss which on\nthe more challenging task of 3D vehicle detection outperforms other loss\nfunctions by up to 1.8 AOS and can be used as a value range independent metric\nfor regression.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 01:24:52 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Weber", "Michael", ""], ["F\u00fcrst", "Michael", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "1904.09058", "submitter": "Jangho Kim", "authors": "Jangho Kim, Minsung Hyun, Inseop Chung, Nojun Kwak", "title": "Feature Fusion for Online Mutual Knowledge Distillation", "comments": "International Conference on Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning framework named Feature Fusion Learning (FFL) that\nefficiently trains a powerful classifier through a fusion module which combines\nthe feature maps generated from parallel neural networks. Specifically, we\ntrain a number of parallel neural networks as sub-networks, then we combine the\nfeature maps from each sub-network using a fusion module to create a more\nmeaningful feature map. The fused feature map is passed into the fused\nclassifier for overall classification. Unlike existing feature fusion methods,\nin our framework, an ensemble of sub-network classifiers transfers its\nknowledge to the fused classifier and then the fused classifier delivers its\nknowledge back to each sub-network, mutually teaching one another in an\nonline-knowledge distillation manner. This mutually teaching system not only\nimproves the performance of the fused classifier but also obtains performance\ngain in each sub-network. Moreover, our model is more beneficial because\ndifferent types of network can be used for each sub-network. We have performed\na variety of experiments on multiple datasets such as CIFAR-10, CIFAR-100 and\nImageNet and proved that our method is more effective than other alternative\nmethods in terms of performance of both sub-networks and the fused classifier.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 03:18:56 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 11:51:14 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Kim", "Jangho", ""], ["Hyun", "Minsung", ""], ["Chung", "Inseop", ""], ["Kwak", "Nojun", ""]]}, {"id": "1904.09059", "submitter": "Tzofi Klinghoffer", "authors": "Peter Morales, Tzofi Klinghoffer, Seung Jae Lee", "title": "Feature Forwarding for Efficient Single Image Dehazing", "comments": "Accepted to the NTIRE 2019 CVPR Workshop. Paper number 77. 8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze degrades content and obscures information of images, which can\nnegatively impact vision-based decision-making in real-time systems. In this\npaper, we propose an efficient fully convolutional neural network (CNN) image\ndehazing method designed to run on edge graphical processing units (GPUs). We\nutilize three variants of our architecture to explore the dependency of dehazed\nimage quality on parameter count and model design. The first two variants\npresented, a small and big version, make use of a single efficient\nencoder-decoder convolutional feature extractor. The final variant utilizes a\npair of encoder-decoders for atmospheric light and transmission map estimation.\nEach variant ends with an image refinement pyramid pooling network to form the\nfinal dehazed image. For the big variant of the single-encoder network, we\ndemonstrate state-of-the-art performance on the NYU Depth dataset. For the\nsmall variant, we maintain competitive performance on the super-resolution\nO/I-HAZE datasets without the need for image cropping. Finally, we examine some\nchallenges presented by the Dense-Haze dataset when leveraging CNN\narchitectures for dehazing of dense haze imagery and examine the impact of loss\nfunction selection on image quality. Benchmarks are included to show the\nfeasibility of introducing this approach into real-time systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 03:20:52 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:45:50 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Morales", "Peter", ""], ["Klinghoffer", "Tzofi", ""], ["Lee", "Seung Jae", ""]]}, {"id": "1904.09073", "submitter": "Karan Sikka", "authors": "Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, Ajay\n  Divakaran", "title": "Integrating Text and Image: Determining Multimodal Document Intent in\n  Instagram Posts", "comments": "Accepted at EMNLP'2019; Added dataset link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing author intent from multimodal data like Instagram posts requires\nmodeling a complex relationship between text and image. For example, a caption\nmight evoke an ironic contrast with the image, so neither caption nor image is\na mere transcript of the other. Instead they combine -- via what has been\ncalled meaning multiplication -- to create a new meaning that has a more\ncomplex relation to the literal meanings of text and image. Here we introduce a\nmultimodal dataset of 1299 Instagram posts labeled for three orthogonal\ntaxonomies: the authorial intent behind the image-caption pair, the contextual\nrelationship between the literal meanings of the image and caption, and the\nsemiotic relationship between the signified meanings of the image and caption.\nWe build a baseline deep multimodal classifier to validate the taxonomy,\nshowing that employing both text and image improves intent detection by 9.6%\ncompared to using only the image modality, demonstrating the commonality of\nnon-intersective meaning multiplication. The gain with multimodality is\ngreatest when the image and caption diverge semiotically. Our dataset offers a\nnew resource for the study of the rich meanings that result from pairing text\nand image.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:28:17 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 02:55:48 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 14:06:58 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kruk", "Julia", ""], ["Lubin", "Jonah", ""], ["Sikka", "Karan", ""], ["Lin", "Xiao", ""], ["Jurafsky", "Dan", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1904.09075", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Theus Aspiras, Tarek M. Taha, Vijayan K. Asari, TJ\n  Bowen, Dave Billiter, and Simon Arkell", "title": "Advanced Deep Convolutional Neural Network Approaches for Digital\n  Pathology Image Analysis: a comprehensive evaluation with different use cases", "comments": "25 pages, 28 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) approaches have been providing state-of-the-art\nperformance in different modalities in the field of medical imagining including\nDigital Pathology Image Analysis (DPIA). Out of many different DL approaches,\nDeep Convolutional Neural Network (DCNN) technique provides superior\nperformance for classification, segmentation, and detection tasks. Most of the\ntask in DPIA problems are somehow possible to solve with classification,\nsegmentation, and detection approaches. In addition, sometimes pre and\npost-processing methods are applied for solving some specific type of problems.\nRecently, different DCNN models including Inception residual recurrent CNN\n(IRRCNN), Densely Connected Recurrent Convolution Network (DCRCN), Recurrent\nResidual U-Net (R2U-Net), and R2U-Net based regression model (UD-Net) have\nproposed and provide state-of-the-art performance for different computer vision\nand medical image analysis tasks. However, these advanced DCNN models have not\nbeen explored for solving different problems related to DPIA. In this study, we\nhave applied these DCNN techniques for solving different DPIA problems and\nevaluated on different publicly available benchmark datasets for seven\ndifferent tasks in digital pathology including lymphoma classification,\nInvasive Ductal Carcinoma (IDC) detection, nuclei segmentation, epithelium\nsegmentation, tubule segmentation, lymphocyte detection, and mitosis detection.\nThe experimental results are evaluated with different performance metrics such\nas sensitivity, specificity, accuracy, F1-score, Receiver Operating\nCharacteristics (ROC) curve, dice coefficient (DC), and Means Squired Errors\n(MSE). The results demonstrate superior performance for classification,\nsegmentation, and detection tasks compared to existing machine learning and\nDCNN based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:37:41 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Aspiras", "Theus", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""], ["Bowen", "TJ", ""], ["Billiter", "Dave", ""], ["Arkell", "Simon", ""]]}, {"id": "1904.09085", "submitter": "Bichen Wu", "authors": "Bernie Wang, Virginia Wu, Bichen Wu, Kurt Keutzer", "title": "LATTE: Accelerating LiDAR Point Cloud Annotation via Sensor Fusion,\n  One-Click Annotation, and Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR (Light Detection And Ranging) is an essential and widely adopted sensor\nfor autonomous vehicles, particularly for those vehicles operating at higher\nlevels (L4-L5) of autonomy. Recent work has demonstrated the promise of\ndeep-learning approaches for LiDAR-based detection. However, deep-learning\nalgorithms are extremely data hungry, requiring large amounts of labeled\npoint-cloud data for training and evaluation. Annotating LiDAR point cloud data\nis challenging due to the following issues: 1) A LiDAR point cloud is usually\nsparse and has low resolution, making it difficult for human annotators to\nrecognize objects. 2) Compared to annotation on 2D images, the operation of\ndrawing 3D bounding boxes or even point-wise labels on LiDAR point clouds is\nmore complex and time-consuming. 3) LiDAR data are usually collected in\nsequences, so consecutive frames are highly correlated, leading to repeated\nannotations. To tackle these challenges, we propose LATTE, an open-sourced\nannotation tool for LiDAR point clouds. LATTE features the following\ninnovations: 1) Sensor fusion: We utilize image-based detection algorithms to\nautomatically pre-label a calibrated image, and transfer the labels to the\npoint cloud. 2) One-click annotation: Instead of drawing 3D bounding boxes or\npoint-wise labels, we simplify the annotation to just one click on the target\nobject, and automatically generate the bounding box for the target. 3)\nTracking: we integrate tracking into sequence annotation such that we can\ntransfer labels from one frame to subsequent ones and therefore significantly\nreduce repeated labeling. Experiments show the proposed features accelerate the\nannotation speed by 6.2x and significantly improve label quality with 23.6% and\n2.2% higher instance-level precision and recall, and 2.0% higher bounding box\nIoU. LATTE is open-sourced at https://github.com/bernwang/latte.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 05:53:14 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wang", "Bernie", ""], ["Wu", "Virginia", ""], ["Wu", "Bichen", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1904.09092", "submitter": "Qi Wang", "authors": "Qi Wang, Junyu Gao, Xuelong Li", "title": "Weakly Supervised Adversarial Domain Adaptation for Semantic\n  Segmentation in Urban Scenes", "comments": "To appear at TIP", "journal-ref": null, "doi": "10.1109/TIP.2019.2910667", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation, a pixel-level vision task, is developed rapidly by\nusing convolutional neural networks (CNNs). Training CNNs requires a large\namount of labeled data, but manually annotating data is difficult. For\nemancipating manpower, in recent years, some synthetic datasets are released.\nHowever, they are still different from real scenes, which causes that training\na model on the synthetic data (source domain) cannot achieve a good performance\non real urban scenes (target domain). In this paper, we propose a weakly\nsupervised adversarial domain adaptation to improve the segmentation\nperformance from synthetic data to real scenes, which consists of three deep\nneural networks. To be specific, a detection and segmentation (\"DS\" for short)\nmodel focuses on detecting objects and predicting segmentation map; a\npixel-level domain classifier (\"PDC\" for short) tries to distinguish the image\nfeatures from which domains; an object-level domain classifier (\"ODC\" for\nshort) discriminates the objects from which domains and predicts the objects\nclasses. PDC and ODC are treated as the discriminators, and DS is considered as\nthe generator. By adversarial learning, DS is supposed to learn\ndomain-invariant features. In experiments, our proposed method yields the new\nrecord of mIoU metric in the same problem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 06:30:36 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wang", "Qi", ""], ["Gao", "Junyu", ""], ["Li", "Xuelong", ""]]}, {"id": "1904.09099", "submitter": "Mostafa El-Khamy", "authors": "Xianzhi Du, Mostafa El-Khamy, Jungwon Lee", "title": "AMNet: Deep Atrous Multiscale Stereo Disparity Estimation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new deep learning architecture for stereo disparity\nestimation is proposed. The proposed atrous multiscale network (AMNet) adopts\nan efficient feature extractor with depthwise-separable convolutions and an\nextended cost volume that deploys novel stereo matching costs on the deep\nfeatures. A stacked atrous multiscale network is proposed to aggregate rich\nmultiscale contextual information from the cost volume which allows for\nestimating the disparity with high accuracy at multiple scales. AMNet can be\nfurther modified to be a foreground-background aware network, FBA-AMNet, which\nis capable of discriminating between the foreground and the background objects\nin the scene at multiple scales. An iterative multitask learning method is\nproposed to train FBA-AMNet end-to-end. The proposed disparity estimation\nnetworks, AMNet and FBA-AMNet, show accurate disparity estimates and advance\nthe state of the art on the challenging Middlebury, KITTI 2012, KITTI 2015, and\nSceneflow stereo disparity estimation benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:05:59 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Du", "Xianzhi", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1904.09105", "submitter": "Yiwen Guo", "authors": "Yiwen Guo, Ming Lu, Wangmeng Zuo, Changshui Zhang, Yurong Chen", "title": "Deep Likelihood Network for Image Restoration with Multiple Degradation\n  Levels", "comments": "Accepted by IEEE Transactions on Image Processing; 13 pages, 6\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been proven effective in a variety of\nimage restoration tasks. Most state-of-the-art solutions, however, are trained\nusing images with a single particular degradation level, and their performance\ndeteriorates drastically when applied to other degradation settings. In this\npaper, we propose deep likelihood network (DL-Net), aiming at generalizing\noff-the-shelf image restoration networks to succeed over a spectrum of\ndegradation levels. We slightly modify an off-the-shelf network by appending a\nsimple recursive module, which is derived from a fidelity term, for\ndisentangling the computation for multiple degradation levels. Extensive\nexperimental results on image inpainting, interpolation, and super-resolution\nshow the effectiveness of our DL-Net.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:45:28 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 06:25:48 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 01:39:20 GMT"}, {"version": "v4", "created": "Sun, 10 Jan 2021 02:07:26 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Guo", "Yiwen", ""], ["Lu", "Ming", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Changshui", ""], ["Chen", "Yurong", ""]]}, {"id": "1904.09106", "submitter": "Junxuan Chen", "authors": "Wenjia Wang, Junxuan Chen, Jie Zhao, Ying Chi, Xuansong Xie, Li Zhang,\n  Xiansheng Hua", "title": "Automated Segmentation of Pulmonary Lobes using Coordination-Guided Deep\n  Neural Networks", "comments": "ISBI 2019 (Oral)", "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019)", "doi": "10.1109/ISBI.2019.8759492", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of pulmonary lobes is of great importance in disease\ndiagnosis and treatment. A few lung diseases have regional disorders at lobar\nlevel. Thus, an accurate segmentation of pulmonary lobes is necessary. In this\nwork, we propose an automated segmentation of pulmonary lobes using\ncoordination-guided deep neural networks from chest CT images. We first employ\nan automated lung segmentation to extract the lung area from CT image, then\nexploit volumetric convolutional neural network (V-net) for segmenting the\npulmonary lobes. To reduce the misclassification of different lobes, we\ntherefore adopt coordination-guided convolutional layers (CoordConvs) that\ngenerate additional feature maps of the positional information of pulmonary\nlobes. The proposed model is trained and evaluated on a few publicly available\ndatasets and has achieved the state-of-the-art accuracy with a mean Dice\ncoefficient index of 0.947 $\\pm$ 0.044.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:49:03 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Wang", "Wenjia", ""], ["Chen", "Junxuan", ""], ["Zhao", "Jie", ""], ["Chi", "Ying", ""], ["Xie", "Xuansong", ""], ["Zhang", "Li", ""], ["Hua", "Xiansheng", ""]]}, {"id": "1904.09115", "submitter": "Di Hu", "authors": "Di Hu, Dong Wang, Xuelong Li, Feiping Nie, Qi Wang", "title": "Listen to the Image", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-to-auditory sensory substitution devices can assist the blind in\nsensing the visual environment by translating the visual information into a\nsound pattern. To improve the translation quality, the task performances of the\nblind are usually employed to evaluate different encoding schemes. In contrast\nto the toilsome human-based assessment, we argue that machine model can be also\ndeveloped for evaluation, and more efficient. To this end, we firstly propose\ntwo distinct cross-modal perception model w.r.t. the late-blind and\ncongenitally-blind cases, which aim to generate concrete visual contents based\non the translated sound. To validate the functionality of proposed models, two\nnovel optimization strategies w.r.t. the primary encoding scheme are presented.\nFurther, we conduct sets of human-based experiments to evaluate and compare\nthem with the conducted machine-based assessments in the cross-modal generation\ntask. Their highly consistent results w.r.t. different encoding schemes\nindicate that using machine model to accelerate optimization evaluation and\nreduce experimental cost is feasible to some extent, which could dramatically\npromote the upgrading of encoding scheme then help the blind to improve their\nvisual perception ability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:13:34 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hu", "Di", ""], ["Wang", "Dong", ""], ["Li", "Xuelong", ""], ["Nie", "Feiping", ""], ["Wang", "Qi", ""]]}, {"id": "1904.09117", "submitter": "Pengpeng Liu", "authors": "Pengpeng Liu, Michael Lyu, Irwin King, Jia Xu", "title": "SelFlow: Self-Supervised Learning of Optical Flow", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised learning approach for optical flow. Our method\ndistills reliable flow estimations from non-occluded pixels, and uses these\npredictions as ground truth to learn optical flow for hallucinated occlusions.\nWe further design a simple CNN to utilize temporal information from multiple\nframes for better flow estimation. These two principles lead to an approach\nthat yields the best performance for unsupervised optical flow learning on the\nchallenging benchmarks including MPI Sintel, KITTI 2012 and 2015. More notably,\nour self-supervised pre-trained model provides an excellent initialization for\nsupervised fine-tuning. Our fine-tuned models achieve state-of-the-art results\non all three datasets. At the time of writing, we achieve EPE=4.26 on the\nSintel benchmark, outperforming all submitted methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:21:16 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Liu", "Pengpeng", ""], ["Lyu", "Michael", ""], ["King", "Irwin", ""], ["Xu", "Jia", ""]]}, {"id": "1904.09120", "submitter": "Yunze Man", "authors": "Yunze Man, Yangsibo Huang, Junyi Feng, Xi Li, Fei Wu", "title": "Deep Q Learning Driven CT Pancreas Segmentation with Geometry-Aware\n  U-Net", "comments": "in IEEE Transactions on Medical Imaging (2019)", "journal-ref": null, "doi": "10.1109/TMI.2019.2911588", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of pancreas is important for medical image analysis, yet it\nfaces great challenges of class imbalance, background distractions and\nnon-rigid geometrical features. To address these difficulties, we introduce a\nDeep Q Network(DQN) driven approach with deformable U-Net to accurately segment\nthe pancreas by explicitly interacting with contextual information and extract\nanisotropic features from pancreas. The DQN based model learns a\ncontext-adaptive localization policy to produce a visually tightened and\nprecise localization bounding box of the pancreas. Furthermore, deformable\nU-Net captures geometry-aware information of pancreas by learning geometrically\ndeformable filters for feature extraction. Experiments on NIH dataset validate\nthe effectiveness of the proposed framework in pancreas segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:36:21 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Man", "Yunze", ""], ["Huang", "Yangsibo", ""], ["Feng", "Junyi", ""], ["Li", "Xi", ""], ["Wu", "Fei", ""]]}, {"id": "1904.09140", "submitter": "Dennis Ludl", "authors": "Dennis Ludl, Thomas Gulde and Crist\\'obal Curio", "title": "Simple yet efficient real-time pose-based action recognition", "comments": "Submitted to IEEE Intelligent Transportation Systems Conference\n  (ITSC) 2019. Code will be available soon at\n  https://github.com/noboevbo/ehpi_action_recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human actions is a core challenge for autonomous systems as they\ndirectly share the same space with humans. Systems must be able to recognize\nand assess human actions in real-time. In order to train corresponding\ndata-driven algorithms, a significant amount of annotated training data is\nrequired. We demonstrated a pipeline to detect humans, estimate their pose,\ntrack them over time and recognize their actions in real-time with standard\nmonocular camera sensors. For action recognition, we encode the human pose into\na new data format called Encoded Human Pose Image (EHPI) that can then be\nclassified using standard methods from the computer vision community. With this\nsimple procedure we achieve competitive state-of-the-art performance in\npose-based action detection and can ensure real-time performance. In addition,\nwe show a use case in the context of autonomous driving to demonstrate how such\na system can be trained to recognize human actions using simulation data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 10:36:28 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Ludl", "Dennis", ""], ["Gulde", "Thomas", ""], ["Curio", "Crist\u00f3bal", ""]]}, {"id": "1904.09146", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, and\n  Ruigang Yang", "title": "Salient Object Detection in the Deep Learning Era: An In-Depth Survey", "comments": "Published on IEEE TPAMI. All the saliency prediction maps, our\n  constructed dataset with annotations, and codes for evaluation are publicly\n  available at \\url{https://github.com/wenguanwang/SODsurvey}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential problem in computer vision, salient object detection (SOD)\nhas attracted an increasing amount of research attention over the years. Recent\nadvances in SOD are predominantly led by deep learning-based solutions (named\ndeep SOD). To enable in-depth understanding of deep SOD, in this paper, we\nprovide a comprehensive survey covering various aspects, ranging from algorithm\ntaxonomy to unsolved issues. In particular, we first review deep SOD algorithms\nfrom different perspectives, including network architecture, level of\nsupervision, learning paradigm, and object-/instance-level detection. Following\nthat, we summarize and analyze existing SOD datasets and evaluation metrics.\nThen, we benchmark a large group of representative SOD models, and provide\ndetailed analyses of the comparison results. Moreover, we study the performance\nof SOD algorithms under different attribute settings, which has not been\nthoroughly explored previously, by constructing a novel SOD dataset with rich\nattribute annotations covering various salient object types, challenging\nfactors, and scene categories. We further analyze, for the first time in the\nfield, the robustness of SOD models to random input perturbations and\nadversarial attacks. We also look into the generalization and difficulty of\nexisting SOD datasets. Finally, we discuss several open issues of SOD and\noutline future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 11:12:54 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 13:19:13 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 10:25:00 GMT"}, {"version": "v4", "created": "Sun, 19 Jul 2020 11:07:03 GMT"}, {"version": "v5", "created": "Fri, 8 Jan 2021 22:58:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Wenguan", ""], ["Lai", "Qiuxia", ""], ["Fu", "Huazhu", ""], ["Shen", "Jianbing", ""], ["Ling", "Haibin", ""], ["Yang", "Ruigang", ""]]}, {"id": "1904.09149", "submitter": "Baoyun Peng", "authors": "Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang,\n  Junjie Yan, Xiaolin Hu", "title": "Knowledge Distillation via Route Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distillation-based learning boosts the performance of the miniaturized neural\nnetwork based on the hypothesis that the representation of a teacher model can\nbe used as structured and relatively weak supervision, and thus would be easily\nlearned by a miniaturized model. However, we find that the representation of a\nconverged heavy model is still a strong constraint for training a small student\nmodel, which leads to a high lower bound of congruence loss. In this work,\ninspired by curriculum learning we consider the knowledge distillation from the\nperspective of curriculum learning by routing. Instead of supervising the\nstudent model with a converged teacher model, we supervised it with some anchor\npoints selected from the route in parameter space that the teacher model passed\nby, as we called route constrained optimization (RCO). We experimentally\ndemonstrate this simple operation greatly reduces the lower bound of congruence\nloss for knowledge distillation, hint and mimicking learning. On close-set\nclassification tasks like CIFAR100 and ImageNet, RCO improves knowledge\ndistillation by 2.14% and 1.5% respectively. For the sake of evaluating the\ngeneralization, we also test RCO on the open-set face recognition task\nMegaFace.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 11:24:20 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Jin", "Xiao", ""], ["Peng", "Baoyun", ""], ["Wu", "Yichao", ""], ["Liu", "Yu", ""], ["Liu", "Jiaheng", ""], ["Liang", "Ding", ""], ["Yan", "Junjie", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1904.09150", "submitter": "Thomas Deselaers", "authors": "R. Reeve Ingle, Yasuhisa Fujii, Thomas Deselaers, Jonathan Baccash,\n  Ashok C. Popat", "title": "A Scalable Handwritten Text Recognition System", "comments": "ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many studies on (Offline) Handwritten Text Recognition (HTR) systems have\nfocused on building state-of-the-art models for line recognition on small\ncorpora. However, adding HTR capability to a large scale multilingual OCR\nsystem poses new challenges. This paper addresses three problems in building\nsuch systems: data, efficiency, and integration. Firstly, one of the biggest\nchallenges is obtaining sufficient amounts of high quality training data. We\naddress the problem by using online handwriting data collected for a large\nscale production online handwriting recognition system. We describe our image\ndata generation pipeline and study how online data can be used to build HTR\nmodels. We show that the data improve the models significantly under the\ncondition where only a small number of real images is available, which is\nusually the case for HTR models. It enables us to support a new script at\nsubstantially lower cost. Secondly, we propose a line recognition model based\non neural networks without recurrent connections. The model achieves a\ncomparable accuracy with LSTM-based models while allowing for better\nparallelism in training and inference. Finally, we present a simple way to\nintegrate HTR models into an OCR system. These constitute a solution to bring\nHTR capability into a large scale OCR system.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 11:35:27 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 23:25:00 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Ingle", "R. Reeve", ""], ["Fujii", "Yasuhisa", ""], ["Deselaers", "Thomas", ""], ["Baccash", "Jonathan", ""], ["Popat", "Ashok C.", ""]]}, {"id": "1904.09154", "submitter": "J\\'er\\'emy Anger", "authors": "J\\'er\\'emy Anger, Mauricio Delbracio, Gabriele Facciolo", "title": "Efficient Blind Deblurring under High Noise Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The goal of blind image deblurring is to recover a sharp image from a motion\nblurred one without knowing the camera motion. Current state-of-the-art methods\nhave a remarkably good performance on images with no noise or very low noise\nlevels. However, the noiseless assumption is not realistic considering that low\nlight conditions are the main reason for the presence of motion blur due to\nrequiring longer exposure times. In fact, motion blur and high to moderate\nnoise often appear together. Most works approach this problem by first\nestimating the blur kernel $k$ and then deconvolving the noisy blurred image.\nIn this work, we first show that current state-of-the-art kernel estimation\nmethods based on the $\\ell_0$ gradient prior can be adapted to handle high\nnoise levels while keeping their efficiency. Then, we show that a fast\nnon-blind deconvolution method can be significantly improved by first denoising\nthe blurry image. The proposed approach yields results that are equivalent to\nthose obtained with much more computationally demanding methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 11:49:21 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 14:02:24 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Anger", "J\u00e9r\u00e9my", ""], ["Delbracio", "Mauricio", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "1904.09159", "submitter": "J\\'er\\'emy Anger", "authors": "J\\'er\\'emy Anger, Carlo de Franchis, Gabriele Facciolo", "title": "Assessing the Sharpness of Satellite Images: Study of the PlanetScope\n  Constellation", "comments": "Accepted at IGARSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  New micro-satellite constellations enable unprecedented systematic monitoring\napplications thanks to their wide coverage and short revisit capabilities.\nHowever, the large volumes of images that they produce have uneven qualities,\ncreating the need for automatic quality assessment methods. In this work, we\nquantify the sharpness of images from the PlanetScope constellation by\nestimating the blur kernel from each image. Once the kernel has been estimated,\nit is possible to compute an absolute measure of sharpness which allows to\ndiscard low quality images and deconvolve blurry images before any further\nprocessing. The method is fully blind and automatic, and since it does not\nrequire the knowledge of any satellite specifications it can be ported to other\nconstellations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 12:01:50 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Anger", "J\u00e9r\u00e9my", ""], ["de Franchis", "Carlo", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "1904.09169", "submitter": "Mohamed Attia Mr", "authors": "Mohamed Attia, Mohammed Hossny, Saeid Nahavandi, Anousha Yazdabadi,\n  and Hamed Asadi", "title": "Realistic Hair Simulation Using Image Blending", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this presented work, we propose a realistic hair simulator using image\nblending for dermoscopic images. This hair simulator can be used for\nbenchmarking and validation of the hair removal methods and in data\naugmentation for improving computer aided diagnostic tools. We adopted one of\nthe popular implementation of image blending to superimpose realistic hair\nmasks to hair lesion. This method was able to produce realistic hair masks\naccording to a predefined mask for hair. Thus, the produced hair images and\nmasks can be used as ground truth for hair segmentation and removal methods by\ninpainting hair according to a pre-defined hair masks on hairfree areas. Also,\nwe achieved a realism score equals to 1.65 in comparison to 1.59 for the\nstate-of-the-art hair simulator.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 12:40:12 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Attia", "Mohamed", ""], ["Hossny", "Mohammed", ""], ["Nahavandi", "Saeid", ""], ["Yazdabadi", "Anousha", ""], ["Asadi", "Hamed", ""]]}, {"id": "1904.09172", "submitter": "Rui Yao", "authors": "Rui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, and Yong Zhou", "title": "Video Object Segmentation and Tracking: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation and object tracking are fundamental research area in the\ncomputer vision community. These two topics are diffcult to handle some common\nchallenges, such as occlusion, deformation, motion blur, and scale variation.\nThe former contains heterogeneous object, interacting object, edge ambiguity,\nand shape complexity. And the latter suffers from difficulties in handling fast\nmotion, out-of-view, and real-time processing. Combining the two problems of\nvideo object segmentation and tracking (VOST) can overcome their respective\ndifficulties and improve their performance. VOST can be widely applied to many\npractical applications such as video summarization, high definition video\ncompression, human computer interaction, and autonomous vehicles. This article\naims to provide a comprehensive review of the state-of-the-art tracking\nmethods, and classify these methods into different categories, and identify new\ntrends. First, we provide a hierarchical categorization existing approaches,\nincluding unsupervised VOS, semi-supervised VOS, interactive VOS, weakly\nsupervised VOS, and segmentation-based tracking methods. Second, we provide a\ndetailed discussion and overview of the technical characteristics of the\ndifferent methods. Third, we summarize the characteristics of the related video\ndataset, and provide a variety of evaluation metrics. Finally, we point out a\nset of interesting future works and draw our own conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 12:49:22 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 01:41:15 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 05:36:58 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Yao", "Rui", ""], ["Lin", "Guosheng", ""], ["Xia", "Shixiong", ""], ["Zhao", "Jiaqi", ""], ["Zhou", "Yong", ""]]}, {"id": "1904.09201", "submitter": "Shichao Li", "authors": "Shichao Li and Kwang-Ting Cheng", "title": "Visualizing the decision-making process in deep neural decision forest", "comments": "Accepted by CVPR 2019 workshops on explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural decision forest (NDF) achieved remarkable performance on various\nvision tasks via combining decision tree and deep representation learning. In\nthis work, we first trace the decision-making process of this model and\nvisualize saliency maps to understand which portion of the input influence it\nmore for both classification and regression problems. We then apply NDF on a\nmulti-task coordinate regression problem and demonstrate the distribution of\nrouting probabilities, which is vital for interpreting NDF yet not shown for\nregression problems. The pre-trained model and code for visualization will be\navailable at https://github.com/Nicholasli1995/VisualizingNDF\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:10:03 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Li", "Shichao", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "1904.09229", "submitter": "Youbao Tang", "authors": "Youbao Tang, Yuxing Tang, Jing Xiao, Ronald M. Summers", "title": "XLSor: A Robust and Accurate Lung Segmentor on Chest X-Rays Using\n  Criss-Cross Attention and Customized Radiorealistic Abnormalities Generation", "comments": "Accepted by MIDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for lung segmentation in chest X-rays.\nIt consists of two key contributions, a criss-cross attention based\nsegmentation network and radiorealistic chest X-ray image synthesis (i.e. a\nsynthesized radiograph that appears anatomically realistic) for data\naugmentation. The criss-cross attention modules capture rich global contextual\ninformation in both horizontal and vertical directions for all the pixels thus\nfacilitating accurate lung segmentation. To reduce the manual annotation burden\nand to train a robust lung segmentor that can be adapted to pathological lungs\nwith hazy lung boundaries, an image-to-image translation module is employed to\nsynthesize radiorealistic abnormal CXRs from the source of normal ones for data\naugmentation. The lung masks of synthetic abnormal CXRs are propagated from the\nsegmentation results of their normal counterparts, and then serve as pseudo\nmasks for robust segmentor training. In addition, we annotate 100 CXRs with\nlung masks on a more challenging NIH Chest X-ray dataset containing both\nposterioranterior and anteroposterior views for evaluation. Extensive\nexperiments validate the robustness and effectiveness of the proposed\nframework. The code and data can be found from\nhttps://github.com/rsummers11/CADLab/tree/master/Lung_Segmentation_XLSor .\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:22:26 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Tang", "Youbao", ""], ["Tang", "Yuxing", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1904.09261", "submitter": "Wei-Lin Hsiao", "authors": "Wei-Lin Hsiao, Isay Katsman, Chao-Yuan Wu, Devi Parikh, Kristen\n  Grauman", "title": "Fashion++: Minimal Edits for Outfit Improvement", "comments": "accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an outfit, what small changes would most improve its fashionability?\nThis question presents an intriguing new vision challenge. We introduce\nFashion++, an approach that proposes minimal adjustments to a full-body\nclothing outfit that will have maximal impact on its fashionability. Our model\nconsists of a deep image generation neural network that learns to synthesize\nclothing conditioned on learned per-garment encodings. The latent encodings are\nexplicitly factorized according to shape and texture, thereby allowing direct\nedits for both fit/presentation and color/patterns/material, respectively. We\nshow how to bootstrap Web photos to automatically train a fashionability model,\nand develop an activation maximization-style approach to transform the input\nimage into its more fashionable self. The edits suggested range from swapping\nin a new garment to tweaking its color, how it is worn (e.g., rolling up\nsleeves), or its fit (e.g., making pants baggier). Experiments demonstrate that\nFashion++ provides successful edits, both according to automated metrics and\nhuman opinion. Project page is at\nhttp://vision.cs.utexas.edu/projects/FashionPlus.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 16:43:10 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 21:41:31 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 16:15:52 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hsiao", "Wei-Lin", ""], ["Katsman", "Isay", ""], ["Wu", "Chao-Yuan", ""], ["Parikh", "Devi", ""], ["Grauman", "Kristen", ""]]}, {"id": "1904.09288", "submitter": "Xitong Yang", "authors": "Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao, Larry Davis and\n  Jan Kautz", "title": "STEP: Spatio-Temporal Progressive Learning for Video Action Detection", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Spatio-TEmporal Progressive (STEP) action\ndetector---a progressive learning framework for spatio-temporal action\ndetection in videos. Starting from a handful of coarse-scale proposal cuboids,\nour approach progressively refines the proposals towards actions over a few\nsteps. In this way, high-quality proposals (i.e., adhere to action movements)\ncan be gradually obtained at later steps by leveraging the regression outputs\nfrom previous steps. At each step, we adaptively extend the proposals in time\nto incorporate more related temporal context. Compared to the prior work that\nperforms action detection in one run, our progressive learning framework is\nable to naturally handle the spatial displacement within action tubes and\ntherefore provides a more effective way for spatio-temporal modeling. We\nextensively evaluate our approach on UCF101 and AVA, and demonstrate superior\ndetection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two\ndatasets with 3 progressive steps and using respectively only 11 and 34 initial\nproposals.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 17:59:39 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Yang", "Xitong", ""], ["Yang", "Xiaodong", ""], ["Liu", "Ming-Yu", ""], ["Xiao", "Fanyi", ""], ["Davis", "Larry", ""], ["Kautz", "Jan", ""]]}, {"id": "1904.09290", "submitter": "Peng Zhang", "authors": "Peng Zhang, Fuhao Zou, Zhiwen Wu, Nengli Dai, Skarpness Mark, Michael\n  Fu, Juan Zhao, Kai Li", "title": "FeatherNets: Convolutional Neural Networks as Light as Feather for Face\n  Anti-spoofing", "comments": "10 pages;6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face Anti-spoofing gains increased attentions recently in both academic and\nindustrial fields. With the emergence of various CNN based solutions, the\nmulti-modal(RGB, depth and IR) methods based CNN showed better performance than\nsingle modal classifiers. However, there is a need for improving the\nperformance and reducing the complexity. Therefore, an extreme light network\narchitecture(FeatherNet A/B) is proposed with a streaming module which fixes\nthe weakness of Global Average Pooling and uses less parameters. Our single\nFeatherNet trained by depth image only, provides a higher baseline with 0.00168\nACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure\nwith ``ensemble + cascade'' structure is presented to satisfy the performance\npreferred use cases. Meanwhile, the MMFD dataset is collected to provide more\nattacks and diversity to gain better generalization. We use the fusion method\nin the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the\nresult of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and\n0.9814(TPR@FPR=10e-4).\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:04:36 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Peng", ""], ["Zou", "Fuhao", ""], ["Wu", "Zhiwen", ""], ["Dai", "Nengli", ""], ["Mark", "Skarpness", ""], ["Fu", "Michael", ""], ["Zhao", "Juan", ""], ["Li", "Kai", ""]]}, {"id": "1904.09317", "submitter": "Christopher Kanan", "authors": "Kushal Kafle, Robik Shrestha, Christopher Kanan", "title": "Challenges and Prospects in Vision and Language Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language grounded image understanding tasks have often been proposed as a\nmethod for evaluating progress in artificial intelligence. Ideally, these tasks\nshould test a plethora of capabilities that integrate computer vision,\nreasoning, and natural language understanding. However, rather than behaving as\nvisual Turing tests, recent studies have demonstrated state-of-the-art systems\nare achieving good performance through flaws in datasets and evaluation\nprocedures. We review the current state of affairs and outline a path forward.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:04:12 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 22:10:33 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kafle", "Kushal", ""], ["Shrestha", "Robik", ""], ["Kanan", "Christopher", ""]]}, {"id": "1904.09320", "submitter": "Ruotian Luo", "authors": "Ruotian Luo, Ning Zhang, Bohyung Han, Linjie Yang", "title": "Context-Aware Zero-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel problem setting in zero-shot learning, zero-shot object\nrecognition and detection in the context. Contrary to the traditional zero-shot\nlearning methods, which simply infers unseen categories by transferring\nknowledge from the objects belonging to semantically similar seen categories,\nwe aim to understand the identity of the novel objects in an image surrounded\nby the known objects using the inter-object relation prior. Specifically, we\nleverage the visual context and the geometric relationships between all pairs\nof objects in a single image, and capture the information useful to infer\nunseen categories. We integrate our context-aware zero-shot learning framework\ninto the traditional zero-shot learning techniques seamlessly using a\nConditional Random Field (CRF). The proposed algorithm is evaluated on both\nzero-shot region classification and zero-shot detection tasks. The results on\nVisual Genome (VG) dataset show that our model significantly boosts performance\nwith the additional visual context compared to traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:28:48 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 03:47:58 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 16:45:01 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Luo", "Ruotian", ""], ["Zhang", "Ning", ""], ["Han", "Bohyung", ""], ["Yang", "Linjie", ""]]}, {"id": "1904.09348", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Sharath Nittur Sridhar and Sairam Sundaresan and\n  Hanlin Tang", "title": "Compact Scene Graphs for Layout Composition and Patch Retrieval", "comments": "To appear in CVPRW 2019 (CEFRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured representations such as scene graphs serve as an efficient and\ncompact representation that can be used for downstream rendering or retrieval\ntasks. However, existing efforts to generate realistic images from scene graphs\nperform poorly on scene composition for cluttered or complex scenes. We propose\ntwo contributions to improve the scene composition. First, we enhance the scene\ngraph representation with heuristic-based relations, which add minimal storage\noverhead. Second, we use extreme points representation to supervise the\nlearning of the scene composition network. These methods achieve significantly\nhigher performance over existing work (69.0% vs 51.2% in relation score\nmetric). We additionally demonstrate how scene graphs can be used to retrieve\npose-constrained image patches that are semantically similar to the source\nquery. Improving structured scene graph representations for rendering or\nretrieval is an important step towards realistic image generation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 21:21:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Tripathi", "Subarna", ""], ["Sridhar", "Sharath Nittur", ""], ["Sundaresan", "Sairam", ""], ["Tang", "Hanlin", ""]]}, {"id": "1904.09390", "submitter": "Tae Hyung Kim", "authors": "Tae Hyung Kim, Pratyush Garg, Justin P. Haldar", "title": "LORAKI: Autocalibrated Recurrent Neural Networks for Autoregressive MRI\n  Reconstruction in k-Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate a new MRI reconstruction method named LORAKI that\ntrains an autocalibrated scan-specific recurrent neural network (RNN) to\nrecover missing k-space data. Methods like GRAPPA, SPIRiT, and AC-LORAKS assume\nthat k-space data has shift-invariant autoregressive structure, and that the\nscan-specific autoregression relationships needed to recover missing samples\ncan be learned from fully-sampled autocalibration (ACS) data. Recently, the\nstructure of the linear GRAPPA method has been translated into a nonlinear deep\nlearning method named RAKI. RAKI uses ACS data to train an artificial neural\nnetwork to interpolate missing k-space samples, and often outperforms GRAPPA.\nIn this work, we apply a similar principle to translate the linear AC-LORAKS\nmethod (simultaneously incorporating support, phase, and parallel imaging\nconstraints) into a nonlinear deep learning method named LORAKI. Since\nAC-LORAKS is iterative and convolutional, LORAKI takes the form of a\nconvolutional RNN. This new architecture admits a wide range of sampling\npatterns, and even calibrationless patterns are possible if synthetic ACS data\nis generated. The performance of LORAKI was evaluated with retrospectively\nundersampled brain datasets, with comparisons against other related\nreconstruction methods. Results suggest that LORAKI can provide improved\nreconstruction compared to other scan-specific autocalibrated reconstruction\nmethods like GRAPPA, RAKI, and AC-LORAKS. LORAKI offers a new deep-learning\napproach to MRI reconstruction based on RNNs in k-space, and enables improved\nimage quality and enhanced sampling flexibility.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 03:02:34 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 01:16:06 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Kim", "Tae Hyung", ""], ["Garg", "Pratyush", ""], ["Haldar", "Justin P.", ""]]}, {"id": "1904.09405", "submitter": "Qingqing Wang", "authors": "Qingqing Wang, Wenjing Jia, Xiangjian He, Yue Lu, Michael Blumenstein,\n  Ye Huang", "title": "FACLSTM: ConvLSTM with Focused Attention for Scene Text Recognition", "comments": "Accepted by Science China Information Science", "journal-ref": null, "doi": "10.1007/s11432-019-2713-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has recently been widely treated as a\nsequence-to-sequence prediction problem, where traditional fully-connected-LSTM\n(FC-LSTM) has played a critical role. Due to the limitation of FC-LSTM,\nexisting methods have to convert 2-D feature maps into 1-D sequential feature\nvectors, resulting in severe damages of the valuable spatial and structural\ninformation of text images. In this paper, we argue that scene text recognition\nis essentially a spatiotemporal prediction problem for its 2-D image inputs,\nand propose a convolution LSTM (ConvLSTM)-based scene text recognizer, namely,\nFACLSTM, i.e., Focused Attention ConvLSTM, where the spatial correlation of\npixels is fully leveraged when performing sequential prediction with LSTM.\nParticularly, the attention mechanism is properly incorporated into an\nefficient ConvLSTM structure via the convolutional operations and additional\ncharacter center masks are generated to help focus attention on right feature\nareas. The experimental results on benchmark datasets IIIT5K, SVT and CUTE\ndemonstrate that our proposed FACLSTM performs competitively on the regular,\nlow-resolution and noisy text images, and outperforms the state-of-the-art\napproaches on the curved text with large margins.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 05:44:37 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 06:59:13 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wang", "Qingqing", ""], ["Jia", "Wenjing", ""], ["He", "Xiangjian", ""], ["Lu", "Yue", ""], ["Blumenstein", "Michael", ""], ["Huang", "Ye", ""]]}, {"id": "1904.09409", "submitter": "QianRu Wei", "authors": "QianRu Wei, DaZheng Feng, WeiXing Zheng", "title": "Funnel Transform for Straight Line Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the classical approaches to straight line detection only deal with a\nbinary edge image and need to use 2D interpolation operation. This paper\nproposes a new transform method figuratively named as funnel transform which\ncan efficiently and rapidly detect straight lines. The funnel transform\nconsists of three 1D Fourier transforms and one nonlinear variable-metric\ntransform (NVMT). It only needs to exploit 1D interpolation operation for\nachieving its NVMT, and can directly handle grayscale images by using its\nhigh-pass filter property, which significantly improves the performance of the\nclosely-related approaches. Based on the slope-intercept line equation, the\nfunnel transform can more uniformly turn the straight lines formed by\nridge-typical and step-typical edges into the local maximum points (peaks). The\nparameters of each line can be uniquely extracted from its corresponding peak\ncoordinates. Additionally, each peak can be theoretically specified by a 2D\ndelta function, which makes the peaks and lines more easily identified and\ndetected, respectively. Theoretical analysis and experimental results\ndemonstrate that the funnel transform has advantages including smaller\ncomputational complexity, lower hardware cost, higher detection probability,\ngreater location precision, better parallelization properties, stronger\nanti-occlusion and noise robustness.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 07:30:11 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Wei", "QianRu", ""], ["Feng", "DaZheng", ""], ["Zheng", "WeiXing", ""]]}, {"id": "1904.09410", "submitter": "Santosh Vipparthi Kumar", "authors": "Monu Verma, Santosh Kumar Vipparthi, Girdhari Singh, Subrahmanyam\n  Murala", "title": "LEARNet Dynamic Imaging Network for Micro Expression Recognition", "comments": "Dynamic imaging, accretion, lateral, micro expression recognition", "journal-ref": null, "doi": "10.1109/TIP.2019.2912358", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike prevalent facial expressions, micro expressions have subtle,\ninvoluntary muscle movements which are short-lived in nature. These minute\nmuscle movements reflect true emotions of a person. Due to the short duration\nand low intensity, these micro-expressions are very difficult to perceive and\ninterpret correctly. In this paper, we propose the dynamic representation of\nmicro-expressions to preserve facial movement information of a video in a\nsingle frame. We also propose a Lateral Accretive Hybrid Network (LEARNet) to\ncapture micro-level features of an expression in the facial region. The LEARNet\nrefines the salient expression features in accretive manner by incorporating\naccretion layers (AL) in the network. The response of the AL holds the hybrid\nfeature maps generated by prior laterally connected convolution layers.\nMoreover, LEARNet architecture incorporates the cross decoupled relationship\nbetween convolution layers which helps in preserving the tiny but influential\nfacial muscle change information. The visual responses of the proposed LEARNet\ndepict the effectiveness of the system by preserving both high- and micro-level\nedge features of facial expression. The effectiveness of the proposed LEARNet\nis evaluated on four benchmark datasets: CASME-I, CASME-II, CAS(ME)^2 and SMIC.\nThe experimental results after investigation show a significant improvement of\n4.03%, 1.90%, 1.79% and 2.82% as compared with ResNet on CASME-I, CASME-II,\nCAS(ME)^2 and SMIC datasets respectively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 07:32:39 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Verma", "Monu", ""], ["Vipparthi", "Santosh Kumar", ""], ["Singh", "Girdhari", ""], ["Murala", "Subrahmanyam", ""]]}, {"id": "1904.09412", "submitter": "Hehe Fan", "authors": "Hehe Fan, Linchao Zhu, Yi Yang", "title": "Cubic LSTMs for Video Prediction", "comments": "Accepted to AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future frames in videos has become a promising direction of\nresearch for both computer vision and robot learning communities. The core of\nthis problem involves moving object capture and future motion prediction. While\nobject capture specifies which objects are moving in videos, motion prediction\ndescribes their future dynamics. Motivated by this analysis, we propose a Cubic\nLong Short-Term Memory (CubicLSTM) unit for video prediction. CubicLSTM\nconsists of three branches, i.e., a spatial branch for capturing moving\nobjects, a temporal branch for processing motions, and an output branch for\ncombining the first two branches to generate predicted frames. Stacking\nmultiple CubicLSTM units along the spatial branch and output branch, and then\nevolving along the temporal branch can form a cubic recurrent neural network\n(CubicRNN). Experiment shows that CubicRNN produces more accurate video\npredictions than prior methods on both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 07:45:08 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fan", "Hehe", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1904.09421", "submitter": "Aihong Yuan", "authors": "Xuelong Li, Aihong Yuan, Xiaoqiang Lu", "title": "Multi-modal gated recurrent units for image description", "comments": "25 pages, 7 figures, 6 tables, magazine", "journal-ref": "Multi-modal gated recurrent units for image description.\n  Multimedia Tools Appl. 77(22): 29847-29869 (2018)", "doi": "10.1007/s11042-018-5856-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a natural language sentence to describe the content of an image is a\nchallenging but very important task. It is challenging because a description\nmust not only capture objects contained in the image and the relationships\namong them, but also be relevant and grammatically correct. In this paper a\nmulti-modal embedding model based on gated recurrent units (GRU) which can\ngenerate variable-length description for a given image. In the training step,\nwe apply the convolutional neural network (CNN) to extract the image feature.\nThen the feature is imported into the multi-modal GRU as well as the\ncorresponding sentence representations. The multi-modal GRU learns the\ninter-modal relations between image and sentence. And in the testing step, when\nan image is imported to our multi-modal GRU model, a sentence which describes\nthe image content is generated. The experimental results demonstrate that our\nmulti-modal GRU model obtains the state-of-the-art performance on Flickr8K,\nFlickr30K and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 08:58:33 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Xuelong", ""], ["Yuan", "Aihong", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1904.09459", "submitter": "Gang Liu", "authors": "Gang Liu, Yu Yu, Kenneth A. Funes Mora, Jean-Marc Odobez", "title": "A Differential Approach for Gaze Estimation", "comments": "Extension to our paper A differential approach for gaze estimation\n  with calibration (BMVC 2018) Submitted to PAMI on Aug. 7th, 2018 Accepted by\n  PAMI short on Dec. 2019, in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2957373", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive gaze estimation methods usually regress gaze directions directly\nfrom a single face or eye image. However, due to important variabilities in eye\nshapes and inner eye structures amongst individuals, universal models obtain\nlimited accuracies and their output usually exhibit high variance as well as\nbiases which are subject dependent. Therefore, increasing accuracy is usually\ndone through calibration, allowing gaze predictions for a subject to be mapped\nto his/her actual gaze. In this paper, we introduce a novel image differential\nmethod for gaze estimation. We propose to directly train a differential\nconvolutional neural network to predict the gaze differences between two eye\ninput images of the same subject. Then, given a set of subject specific\ncalibration images, we can use the inferred differences to predict the gaze\ndirection of a novel eye sample. The assumption is that by allowing the\ncomparison between two eye images, annoyance factors (alignment, eyelid\nclosing, illumination perturbations) which usually plague single image\nprediction methods can be much reduced, allowing better prediction altogether.\nExperiments on 3 public datasets validate our approach which constantly\noutperforms state-of-the-art methods even when using only one calibration\nsample or when the latter methods are followed by subject specific gaze\nadaptation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 15:17:45 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 01:54:48 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 11:52:11 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Liu", "Gang", ""], ["Yu", "Yu", ""], ["Mora", "Kenneth A. Funes", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1904.09460", "submitter": "Yimin Chen", "authors": "Yi Li, Zhanghui Kuang, Yimin Chen, Wayne Zhang", "title": "Data-Driven Neuron Allocation for Scale Aggregation Networks", "comments": "11 pages,", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful visual recognition networks benefit from aggregating information\nspanning from a wide range of scales. Previous research has investigated\ninformation fusion of connected layers or multiple branches in a block, seeking\nto strengthen the power of multi-scale representations. Despite their great\nsuccesses, existing practices often allocate the neurons for each scale\nmanually, and keep the same ratio in all aggregation blocks of an entire\nnetwork, rendering suboptimal performance. In this paper, we propose to learn\nthe neuron allocation for aggregating multi-scale information in different\nbuilding blocks of a deep network. The most informative output neurons in each\nblock are preserved while others are discarded, and thus neurons for multiple\nscales are competitively and adaptively allocated. Our scale aggregation\nnetwork (ScaleNet) is constructed by repeating a scale aggregation (SA) block\nthat concatenates feature maps at a wide range of scales. Feature maps for each\nscale are generated by a stack of downsampling, convolution and upsampling\noperations. The data-driven neuron allocation and SA block achieve strong\nrepresentational power at the cost of considerably low computational\ncomplexity. The proposed ScaleNet, by replacing all 3x3 convolutions in ResNet\nwith our SA blocks, achieves better performance than ResNet and its outstanding\nvariants like ResNeXt and SE-ResNet, in the same computational complexity. On\nImageNet classification, ScaleNets absolutely reduce the top-1 error rate of\nResNets by 1.12 (101 layers) and 1.82 (50 layers). On COCO object detection,\nScaleNets absolutely improve the mmAP with backbone of ResNets by 3.6 (101\nlayers) and 4.6 (50 layers) on Faster RCNN, respectively. Code and models are\nreleased at https://github.com/Eli-YiLi/ScaleNet.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 15:18:20 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Yi", ""], ["Kuang", "Zhanghui", ""], ["Chen", "Yimin", ""], ["Zhang", "Wayne", ""]]}, {"id": "1904.09464", "submitter": "Huijiao Wang", "authors": "Huijiao Wang, Li Wang, Xulei Yang, Lei Yu, and Haijian Zhang", "title": "Facial Feature Embedded CycleGAN for VIS-NIR Translation", "comments": "reference [9] corrected; the organization of co-author Xulei Yang\n  corrected;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VIS-NIR face recognition remains a challenging task due to the distinction\nbetween spectral components of two modalities and insufficient paired training\ndata. Inspired by the CycleGAN, this paper presents a method aiming to\ntranslate VIS face images into fake NIR images whose distributions are intended\nto approximate those of true NIR images, which is achieved by proposing a new\nfacial feature embedded CycleGAN. Firstly, to learn the particular feature of\nNIR domain while preserving common facial representation between VIS and NIR\ndomains, we employ a general facial feature extractor (FFE) to replace the\nencoder in the original generator of CycleGAN. For implementing the facial\nfeature extractor, herein the MobileFaceNet is pretrained on a VIS face\ndatabase, and is able to extract effective features. Secondly, the\ndomain-invariant feature learning is enhanced by considering a new pixel\nconsistency loss. Lastly, we establish a new WHU VIS-NIR database which varies\nin face rotation and expressions to enrich the training data. Experimental\nresults on the Oulu-CASIA NIR-VIS database and the WHU VIS-NIR database show\nthat the proposed FFE-based CycleGAN (FFE-CycleGAN) outperforms\nstate-of-the-art VIS-NIR face recognition methods and achieves 96.5\\% accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 15:45:29 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 11:56:51 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Wang", "Huijiao", ""], ["Wang", "Li", ""], ["Yang", "Xulei", ""], ["Yu", "Lei", ""], ["Zhang", "Haijian", ""]]}, {"id": "1904.09471", "submitter": "Haoran Wang", "authors": "Zhong Ji, Haoran Wang, Jungong Han, Yanwei Pang", "title": "Saliency-Guided Attention Network for Image-Sentence Matching", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the task of matching image and sentence, where learning\nappropriate representations across the multi-modal data appears to be the main\nchallenge. Unlike previous approaches that predominantly deploy symmetrical\narchitecture to represent both modalities, we propose Saliency-guided Attention\nNetwork (SAN) that asymmetrically employs visual and textual attention modules\nto learn the fine-grained correlation intertwined between vision and language.\nThe proposed SAN mainly includes three components: saliency detector,\nSaliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual\nAttention (STA) module. Concretely, the saliency detector provides the visual\nsaliency information as the guidance for the two attention modules. SVA is\ndesigned to leverage the advantage of the saliency information to improve\ndiscrimination of visual representations. By fusing the visual information from\nSVA and textual information as a multi-modal guidance, STA learns\ndiscriminative textual representations that are highly sensitive to visual\nclues. Extensive experiments demonstrate SAN can substantially improve the\nstate-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 17:27:55 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 04:54:27 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 06:09:34 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 10:19:50 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ji", "Zhong", ""], ["Wang", "Haoran", ""], ["Han", "Jungong", ""], ["Pang", "Yanwei", ""]]}, {"id": "1904.09472", "submitter": "Farshid Rayhan", "authors": "Farshid Rayhan, Aphrodite Galata, Timothy F. Cootes", "title": "ChoiceNet: CNN learning through choice of multiple feature map\n  representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new architecture called ChoiceNet where each layer of the\nnetwork is highly connected with skip connections and channelwise\nconcatenations. This enables the network to alleviate the problem of vanishing\ngradients, reduces the number of parameters without sacrificing performance,\nand encourages feature reuse. We evaluate our proposed architecture on three\nbenchmark datasets for object recognition tasks (ImageNet, CIFAR- 10,\nCIFAR-100, SVHN) and on a semantic segmentation dataset (CamVid).\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 17:37:46 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 12:50:32 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 22:57:17 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Rayhan", "Farshid", ""], ["Galata", "Aphrodite", ""], ["Cootes", "Timothy F.", ""]]}, {"id": "1904.09503", "submitter": "Jianyu Chen", "authors": "Jianyu Chen, Bodi Yuan, Masayoshi Tomizuka", "title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban autonomous driving decision making is challenging due to complex road\ngeometry and multi-agent interactions. Current decision making methods are\nmostly manually designing the driving policy, which might result in sub-optimal\nsolutions and is expensive to develop, generalize and maintain at scale. On the\nother hand, with reinforcement learning (RL), a policy can be learned and\nimproved automatically without any manual designs. However, current RL methods\ngenerally do not work well on complex urban scenarios. In this paper, we\npropose a framework to enable model-free deep reinforcement learning in\nchallenging urban autonomous driving scenarios. We design a specific input\nrepresentation and use visual encoding to capture the low-dimensional latent\nstates. Several state-of-the-art model-free deep RL algorithms are implemented\ninto our framework, with several tricks to improve their performance. We\nevaluate our method in a challenging roundabout task with dense surrounding\nvehicles in a high-definition driving simulator. The result shows that our\nmethod can solve the task well and is significantly better than the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 22:02:45 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 21:11:58 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chen", "Jianyu", ""], ["Yuan", "Bodi", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1904.09507", "submitter": "Javad Amirian", "authors": "Javad Amirian, Jean-Bernard Hayet, Julien Pettre", "title": "Social Ways: Learning Multi-Modal Distributions of Pedestrian\n  Trajectories with GANs", "comments": "Accepted at CVPR Workshops 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for predicting the motion of pedestrians\ninteracting with others. It uses a Generative Adversarial Network (GAN) to\nsample plausible predictions for any agent in the scene. As GANs are very\nsusceptible to mode collapsing and dropping, we show that the recently proposed\nInfo-GAN allows dramatic improvements in multi-modal pedestrian trajectory\nprediction to avoid these issues. We also left out L2-loss in training the\ngenerator, unlike some previous works, because it causes serious mode\ncollapsing though faster convergence.\n  We show through experiments on real and synthetic data that the proposed\nmethod leads to generate more diverse samples and to preserve the modes of the\npredictive distribution. In particular, to prove this claim, we have designed a\ntoy example dataset of trajectories that can be used to assess the performance\nof different methods in preserving the predictive distribution modes.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 22:11:56 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:25:07 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Amirian", "Javad", ""], ["Hayet", "Jean-Bernard", ""], ["Pettre", "Julien", ""]]}, {"id": "1904.09523", "submitter": "Ning Zhu", "authors": "Ning Zhu", "title": "Neural Architecture Search for Deep Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the widespread popularity of electronic devices, the emergence of\nbiometric technology has brought significant convenience to user authentication\ncompared with the traditional password and mode unlocking. Among many\nbiological characteristics, the face is a universal and irreplaceable feature\nthat does not need too much cooperation and can significantly improve the\nuser's experience at the same time. Face recognition is one of the main\nfunctions of electronic equipment propaganda. Hence it's virtually worth\nresearching in computer vision. Previous work in this field has focused on two\ndirections: converting loss function to improve recognition accuracy in\ntraditional deep convolution neural networks (Resnet); combining the latest\nloss function with the lightweight system (MobileNet) to reduce network size at\nthe minimal expense of accuracy. But none of these has changed the network\nstructure. With the development of AutoML, neural architecture search (NAS) has\nshown excellent performance in the benchmark of image classification. In this\npaper, we integrate NAS technology into face recognition to customize a more\nsuitable network. We quote the framework of neural architecture search which\ntrains child and controller network alternately. At the same time, we mutate\nNAS by incorporating evaluation latency into rewards of reinforcement learning\nand utilize policy gradient algorithm to search the architecture automatically\nwith the most classical cross-entropy loss. The network architectures we\nsearched out have got state-of-the-art accuracy in the large-scale face\ndataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with\nrelatively small network size. To the best of our knowledge, this proposal is\nthe first attempt to use NAS to solve the problem of Deep Face Recognition and\nachieve the best results in this domain.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 01:05:41 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 09:54:47 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Zhu", "Ning", ""]]}, {"id": "1904.09524", "submitter": "Marc Niethammer", "authors": "Marc Niethammer and Roland Kwitt and Francois-Xavier Vialard", "title": "Metric Learning for Image Registration", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a key technique in medical image analysis to estimate\ndeformations between image pairs. A good deformation model is important for\nhigh-quality estimates. However, most existing approaches use ad-hoc\ndeformation models chosen for mathematical convenience rather than to capture\nobserved data variation. Recent deep learning approaches learn deformation\nmodels directly from data. However, they provide limited control over the\nspatial regularity of transformations. Instead of learning the entire\nregistration approach, we learn a spatially-adaptive regularizer within a\nregistration model. This allows controlling the desired level of regularity and\npreserving structural properties of a registration model. For example,\ndiffeomorphic transformations can be attained. Our approach is a radical\ndeparture from existing deep learning approaches to image registration by\nembedding a deep learning model in an optimization-based registration algorithm\nto parameterize and data-adapt the registration model itself.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 01:07:44 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Niethammer", "Marc", ""], ["Kwitt", "Roland", ""], ["Vialard", "Francois-Xavier", ""]]}, {"id": "1904.09527", "submitter": "Kamyar Nazeri", "authors": "Harrish Thasarathan, Kamyar Nazeri, Mehran Ebrahimi", "title": "Automatic Temporally Coherent Video Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Greyscale image colorization for applications in image restoration has seen\nsignificant improvements in recent years. Many of these techniques that use\nlearning-based methods struggle to effectively colorize sparse inputs. With the\nconsistent growth of the anime industry, the ability to colorize sparse input\nsuch as line art can reduce significant cost and redundant work for production\nstudios by eliminating the in-between frame colorization process. Simply using\nexisting methods yields inconsistent colors between related frames resulting in\na flicker effect in the final video. In order to successfully automate key\nareas of large-scale anime production, the colorization of line arts must be\ntemporally consistent between frames. This paper proposes a method to colorize\nline art frames in an adversarial setting, to create temporally coherent video\nof large anime by improving existing image to image translation methods. We\nshow that by adding an extra condition to the generator and discriminator, we\ncan effectively create temporally consistent video sequences from anime line\narts. Code and models available at: https://github.com/Harry-Thasarathan/TCVC\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 01:50:22 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Thasarathan", "Harrish", ""], ["Nazeri", "Kamyar", ""], ["Ebrahimi", "Mehran", ""]]}, {"id": "1904.09544", "submitter": "Aihong Yuan", "authors": "Aihong Yuan, Xuelong Li, Xiaoqiang Lu", "title": "3G structure for image caption generation", "comments": "35 pages, 7 figures, magazine", "journal-ref": "Neurocomputing 330: 17-28 (2019)", "doi": "10.1016/j.neucom.2018.10.059", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a big challenge of computer vision to make machine automatically\ndescribe the content of an image with a natural language sentence. Previous\nworks have made great progress on this task, but they only use the global or\nlocal image feature, which may lose some important subtle or global information\nof an image. In this paper, we propose a model with 3-gated model which fuses\nthe global and local image features together for the task of image caption\ngeneration. The model mainly has three gated structures. 1) Gate for the global\nimage feature, which can adaptively decide when and how much the global image\nfeature should be imported into the sentence generator. 2) The gated recurrent\nneural network (RNN) is used as the sentence generator. 3) The gated feedback\nmethod for stacking RNN is employed to increase the capability of nonlinearity\nfitting. More specially, the global and local image features are combined\ntogether in this paper, which makes full use of the image information. The\nglobal image feature is controlled by the first gate and the local image\nfeature is selected by the attention mechanism. With the latter two gates, the\nrelationship between image and text can be well explored, which improves the\nperformance of the language part as well as the multi-modal embedding part.\nExperimental results show that our proposed method outperforms the\nstate-of-the-art for image caption generation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 05:43:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yuan", "Aihong", ""], ["Li", "Xuelong", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1904.09546", "submitter": "Jathushan Rajasegaran", "authors": "Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima\n  Jayasekara, Suranga Seneviratne, Ranga Rodrigo", "title": "DeepCaps: Going Deeper with Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Network is a promising concept in deep learning, yet its true\npotential is not fully realized thus far, providing sub-par performance on\nseveral key benchmark datasets with complex data. Drawing intuition from the\nsuccess achieved by Convolutional Neural Networks (CNNs) by going deeper, we\nintroduce DeepCaps1, a deep capsule network architecture which uses a novel 3D\nconvolution based dynamic routing algorithm. With DeepCaps, we surpass the\nstate-of-the-art results in the capsule network domain on CIFAR10, SVHN and\nFashion MNIST, while achieving a 68% reduction in the number of parameters.\nFurther, we propose a class-independent decoder network, which strengthens the\nuse of reconstruction loss as a regularization term. This leads to an\ninteresting property of the decoder, which allows us to identify and control\nthe physical attributes of the images represented by the instantiation\nparameters.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 06:31:30 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Rajasegaran", "Jathushan", ""], ["Jayasundara", "Vinoj", ""], ["Jayasekara", "Sandaru", ""], ["Jayasekara", "Hirunima", ""], ["Seneviratne", "Suranga", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "1904.09568", "submitter": "Xiang Gao", "authors": "Xiang Gao and Shuhan Shen and Lingjie Zhu and Tianxin Shi and Zhiheng\n  Wang and Zhanyi Hu", "title": "Complete Scene Reconstruction by Merging Images and Laser Scans", "comments": "This manuscript has been accepted by IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image based modeling and laser scanning are two commonly used approaches in\nlarge-scale architectural scene reconstruction nowadays. In order to generate a\ncomplete scene reconstruction, an effective way is to completely cover the\nscene using ground and aerial images, supplemented by laser scanning on certain\nregions with low texture and complicated structure. Thus, the key issue is to\naccurately calibrate cameras and register laser scans in a unified framework.\nTo this end, we proposed a three-step pipeline for complete scene\nreconstruction by merging images and laser scans. First, images are captured\naround the architecture in a multi-view and multi-scale way and are feed into a\nstructure-from-motion (SfM) pipeline to generate SfM points. Then, based on the\nSfM result, the laser scanning locations are automatically planned by\nconsidering textural richness, structural complexity of the scene and spatial\nlayout of the laser scans. Finally, the images and laser scans are accurately\nmerged in a coarse-to-fine manner. Experimental evaluations on two ancient\nChinese architecture datasets demonstrate the effectiveness of our proposed\ncomplete scene reconstruction pipeline.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 08:56:07 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 06:50:56 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 02:56:24 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 08:29:27 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Gao", "Xiang", ""], ["Shen", "Shuhan", ""], ["Zhu", "Lingjie", ""], ["Shi", "Tianxin", ""], ["Wang", "Zhiheng", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1904.09569", "submitter": "Qibin Hou", "authors": "Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, Jianmin\n  Jiang", "title": "A Simple Pooling-Based Design for Real-Time Salient Object Detection", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the problem of salient object detection by investigating how to\nexpand the role of pooling in convolutional neural networks. Based on the\nU-shape architecture, we first build a global guidance module (GGM) upon the\nbottom-up pathway, aiming at providing layers at different feature levels the\nlocation information of potential salient objects. We further design a feature\naggregation module (FAM) to make the coarse-level semantic information well\nfused with the fine-level features from the top-down pathway. By adding FAMs\nafter the fusion operations in the top-down pathway, coarse-level features from\nthe GGM can be seamlessly merged with features at various scales. These two\npooling-based modules allow the high-level semantic features to be\nprogressively refined, yielding detail enriched saliency maps. Experiment\nresults show that our proposed approach can more accurately locate the salient\nobjects with sharpened details and hence substantially improve the performance\ncompared to the previous state-of-the-arts. Our approach is fast as well and\ncan run at a speed of more than 30 FPS when processing a $300 \\times 400$\nimage. Code can be found at http://mmcheng.net/poolnet/.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 09:22:25 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Liu", "Jiang-Jiang", ""], ["Hou", "Qibin", ""], ["Cheng", "Ming-Ming", ""], ["Feng", "Jiashi", ""], ["Jiang", "Jianmin", ""]]}, {"id": "1904.09571", "submitter": "Wayne Wu", "authors": "Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy", "title": "TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation", "comments": "Accepted to CVPR 2019. Project page:\n  https://wywu.github.io/projects/TGaGa/TGaGa.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation aims at learning a mapping between\ntwo visual domains. However, learning a translation across large geometry\nvariations always ends up with failure. In this work, we present a novel\ndisentangle-and-translate framework to tackle the complex objects\nimage-to-image translation task. Instead of learning the mapping on the image\nspace directly, we disentangle image space into a Cartesian product of the\nappearance and the geometry latent spaces. Specifically, we first introduce a\ngeometry prior loss and a conditional VAE loss to encourage the network to\nlearn independent but complementary representations. The translation is then\nbuilt on appearance and geometry space separately. Extensive experiments\ndemonstrate the superior performance of our method to other state-of-the-art\napproaches, especially in the challenging near-rigid and non-rigid objects\ntranslation tasks. In addition, by taking different exemplars as the appearance\nreferences, our method also supports multimodal translation. Project page:\nhttps://wywu.github.io/projects/TGaGa/TGaGa.html\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 09:42:10 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Wu", "Wayne", ""], ["Cao", "Kaidi", ""], ["Li", "Cheng", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "1904.09601", "submitter": "Chaofan Tao", "authors": "Chaofan Tao, Fengmao Lv, Lixin Duan, Min Wu", "title": "MiniMax Entropy Network: Learning Category-Invariant Features for Domain\n  Adaptation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to effectively learn from unlabeled data from the target domain is\ncrucial for domain adaptation, as it helps reduce the large performance gap due\nto domain shift or distribution change. In this paper, we propose an\neasy-to-implement method dubbed MiniMax Entropy Networks (MMEN) based on\nadversarial learning. Unlike most existing approaches which employ a generator\nto deal with domain difference, MMEN focuses on learning the categorical\ninformation from unlabeled target samples with the help of labeled source\nsamples. Specifically, we set an unfair multi-class classifier named\ncategorical discriminator, which classifies source samples accurately but be\nconfused about the categories of target samples. The generator learns a common\nsubspace that aligns the unlabeled samples based on the target pseudo-labels.\nFor MMEN, we also provide theoretical explanations to show that the learning of\nfeature alignment reduces domain mismatch at the category level. Experimental\nresults on various benchmark datasets demonstrate the effectiveness of our\nmethod over existing state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 13:39:29 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 15:15:06 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Tao", "Chaofan", ""], ["Lv", "Fengmao", ""], ["Duan", "Lixin", ""], ["Wu", "Min", ""]]}, {"id": "1904.09609", "submitter": "Ranjan Maitra", "authors": "Nicholas S. Berry and Ranjan Maitra", "title": "TiK-means: $K$-means clustering for skewed groups", "comments": "15 pages, 6 figures, to appear in Statistical Analysis and Data\n  Mining - The ASA Data Science Journal", "journal-ref": "Statistical Analysis and Data Mining -- The ASA Data Science\n  Journal, 2019, volume 12, number 3, pages 223-233", "doi": "10.1002/sam11416", "report-no": null, "categories": "stat.ML astro-ph.HE cs.CV cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$-means algorithm is extended to allow for partitioning of skewed\ngroups. Our algorithm is called TiK-Means and contributes a $K$-means type\nalgorithm that assigns observations to groups while estimating their\nskewness-transformation parameters. The resulting groups and transformation\nreveal general-structured clusters that can be explained by inverting the\nestimated transformation. Further, a modification of the jump statistic chooses\nthe number of groups. Our algorithm is evaluated on simulated and real-life\ndatasets and then applied to a long-standing astronomical dispute regarding the\ndistinct kinds of gamma ray bursts.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:32:42 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berry", "Nicholas S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1904.09626", "submitter": "Sungyeon Kim", "authors": "Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, Suha Kwak", "title": "Deep Metric Learning Beyond Binary Supervision", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric Learning for visual similarity has mostly adopted binary supervision\nindicating whether a pair of images are of the same class or not. Such a binary\nindicator covers only a limited subset of image relations, and is not\nsufficient to represent semantic similarity between images described by\ncontinuous and/or structured labels such as object poses, image captions, and\nscene graphs. Motivated by this, we present a novel method for deep metric\nlearning using continuous labels. First, we propose a new triplet loss that\nallows distance ratios in the label space to be preserved in the learned metric\nspace. The proposed loss thus enables our model to learn the degree of\nsimilarity rather than just the order. Furthermore, we design a triplet mining\nstrategy adapted to metric learning with continuous labels. We address three\ndifferent image retrieval tasks with continuous labels in terms of human poses,\nroom layouts and image captions, and demonstrate the superior performance of\nour approach compared to previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:02:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kim", "Sungyeon", ""], ["Seo", "Minkyo", ""], ["Laptev", "Ivan", ""], ["Cho", "Minsu", ""], ["Kwak", "Suha", ""]]}, {"id": "1904.09633", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Ibrahim Ben-Daya, Kanav Vats, Jeffery Feng, Graham\n  Taylor and, Alexander Wong", "title": "Beyond Explainability: Leveraging Interpretability for Improved\n  Adversarial Learning", "comments": "CVPR 2019 XAI Workshop accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose the leveraging of interpretability for tasks beyond\npurely the purpose of explainability. In particular, this study puts forward a\nnovel strategy for leveraging gradient-based interpretability in the realm of\nadversarial examples, where we use insights gained to aid adversarial learning.\nMore specifically, we introduce the concept of spatially constrained one-pixel\nadversarial perturbations, where we guide the learning of such adversarial\nperturbations towards more susceptible areas identified via gradient-based\ninterpretability. Experimental results using different benchmark datasets show\nthat such a spatially constrained one-pixel adversarial perturbation strategy\ncan noticeably improve the speed of convergence as well as produce successful\nattacks that were also visually difficult to perceive, thus illustrating an\neffective use of interpretability methods for tasks outside of the purpose of\npurely explainability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:32:03 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kumar", "Devinder", ""], ["Ben-Daya", "Ibrahim", ""], ["Vats", "Kanav", ""], ["Feng", "Jeffery", ""], ["and", "Graham Taylor", ""], ["Wong", "Alexander", ""]]}, {"id": "1904.09639", "submitter": "Mustafa Hajij", "authors": "Yunhao Zhang, Haowen Liu, Paul Rosen, Mustafa Hajij", "title": "Mesh Learning Using Persistent Homology on the Laplacian Eigenfunctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use persistent homology along with the eigenfunctions of the Laplacian to\nstudy similarity amongst triangulated 2-manifolds. Our method relies on\nstudying the lower-star filtration induced by the eigenfunctions of the\nLaplacian. This gives us a shape descriptor that inherits the rich information\nencoded in the eigenfunctions of the Laplacian. Moreover, the similarity\nbetween these descriptors can be easily computed using tools that are readily\navailable in Topological Data Analysis. We provide experiments to illustrate\nthe effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 18:18:40 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 22:12:39 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhang", "Yunhao", ""], ["Liu", "Haowen", ""], ["Rosen", "Paul", ""], ["Hajij", "Mustafa", ""]]}, {"id": "1904.09658", "submitter": "Yichun Shi", "authors": "Yichun Shi, Anil K. Jain", "title": "Probabilistic Face Embeddings", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding methods have achieved success in face recognition by comparing\nfacial features in a latent semantic space. However, in a fully unconstrained\nface setting, the facial features learned by the embedding model could be\nambiguous or may not even be present in the input face, leading to noisy\nrepresentations. We propose Probabilistic Face Embeddings (PFEs), which\nrepresent each face image as a Gaussian distribution in the latent space. The\nmean of the distribution estimates the most likely feature values while the\nvariance shows the uncertainty in the feature values. Probabilistic solutions\ncan then be naturally derived for matching and fusing PFEs using the\nuncertainty information. Empirical evaluation on different baseline models,\ntraining datasets and benchmarks show that the proposed method can improve the\nface recognition performance of deterministic embeddings by converting them\ninto PFEs. The uncertainties estimated by PFEs also serve as good indicators of\nthe potential matching accuracy, which are important for a risk-controlled\nrecognition system.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 21:08:00 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 20:26:36 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 21:52:16 GMT"}, {"version": "v4", "created": "Wed, 7 Aug 2019 05:24:06 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Shi", "Yichun", ""], ["Jain", "Anil K.", ""]]}, {"id": "1904.09659", "submitter": "Alessandro Dal Palu'", "authors": "Alessandro Dal Palu'", "title": "An image structure model for exact edge detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new model for single channel images low-level\ninterpretation. The image is decomposed into a graph which captures a complete\nset of structural features. The description allows to accurately identify every\nedge location and its correct connectivity. The key features of the method are:\nvector description of the edges, subpixel precision, and parallelism of the\nunderlying algorithm. The methodology outperforms classical and state of the\nart edge detectors at both conceptual and experimental levels. It also enables\ngraph based algorithms for higher-level feature extraction. Any image\nprocessing pipeline can benefit from such results: e.g., controlled denoising,\nedge preserving filtering, upsampling, compression, vector and graph based\npattern matching, neural network training.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 21:09:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Palu'", "Alessandro Dal", ""]]}, {"id": "1904.09664", "submitter": "Charles Ruizhongtai Qi", "authors": "Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas", "title": "Deep Hough Voting for 3D Object Detection in Point Clouds", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current 3D object detection methods are heavily influenced by 2D detectors.\nIn order to leverage architectures in 2D detectors, they often convert 3D point\nclouds to regular grids (i.e., to voxel grids or to bird's eye view images), or\nrely on detection in 2D images to propose 3D boxes. Few works have attempted to\ndirectly detect objects in point clouds. In this work, we return to first\nprinciples to construct a 3D detection pipeline for point cloud data and as\ngeneric as possible. However, due to the sparse nature of the data -- samples\nfrom 2D manifolds in 3D space -- we face a major challenge when directly\npredicting bounding box parameters from scene points: a 3D object centroid can\nbe far from any surface point thus hard to regress accurately in one step. To\naddress the challenge, we propose VoteNet, an end-to-end 3D object detection\nnetwork based on a synergy of deep point set networks and Hough voting. Our\nmodel achieves state-of-the-art 3D detection on two large datasets of real 3D\nscans, ScanNet and SUN RGB-D with a simple design, compact model size and high\nefficiency. Remarkably, VoteNet outperforms previous methods by using purely\ngeometric information without relying on color images.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 21:36:36 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 20:54:40 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Qi", "Charles R.", ""], ["Litany", "Or", ""], ["He", "Kaiming", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1904.09709", "submitter": "Ming Liu", "authors": "Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding, Wangmeng Zuo,\n  Shilei Wen", "title": "STGAN: A Unified Selective Transfer Network for Arbitrary Image\n  Attribute Editing", "comments": null, "journal-ref": "CVPR 2019; code is available at https://github.com/csmliu/STGAN", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary attribute editing generally can be tackled by incorporating\nencoder-decoder and generative adversarial networks. However, the bottleneck\nlayer in encoder-decoder usually gives rise to blurry and low quality editing\nresult. And adding skip connections improves image quality at the cost of\nweakened attribute manipulation ability. Moreover, existing methods exploit\ntarget attribute vector to guide the flexible translation to desired target\ndomain. In this work, we suggest to address these issues from selective\ntransfer perspective. Considering that specific editing task is certainly only\nrelated to the changed attributes instead of all target attributes, our model\nselectively takes the difference between target and source attribute vectors as\ninput. Furthermore, selective transfer units are incorporated with\nencoder-decoder to adaptively select and modify encoder feature for enhanced\nattribute editing. Experiments show that our method (i.e., STGAN)\nsimultaneously improves attribute manipulation accuracy as well as perception\nquality, and performs favorably against state-of-the-arts in arbitrary facial\nattribute editing and season translation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 03:24:33 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Liu", "Ming", ""], ["Ding", "Yukang", ""], ["Xia", "Min", ""], ["Liu", "Xiao", ""], ["Ding", "Errui", ""], ["Zuo", "Wangmeng", ""], ["Wen", "Shilei", ""]]}, {"id": "1904.09722", "submitter": "Sebastian Agethen", "authors": "Hsin-I Chen, Sebastian Agethen, Chiamin Wu, Winston Hsu, Bing-Yu Chen", "title": "FishNet: A Camera Localizer using Deep Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a robust localization system that employs deep learning\nfor better scene representation, and enhances the accuracy of 6-DOF camera pose\nestimation. Inspired by the fact that global scene structure can be revealed by\nwide field-of-view, we leverage the large overlap of a fisheye camera between\nadjacent frames, and the powerful high-level feature representations of deep\nlearning. Our main contribution is the novel network architecture that extracts\nboth temporal and spatial information using a Recurrent Neural Network.\nSpecifically, we propose a novel pose regularization term combined with LSTM.\nThis leads to smoother pose estimation, especially for large outdoor scenery.\nPromising experimental results on three benchmark datasets manifest the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:11:14 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chen", "Hsin-I", ""], ["Agethen", "Sebastian", ""], ["Wu", "Chiamin", ""], ["Hsu", "Winston", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "1904.09730", "submitter": "Youngwan Lee", "authors": "Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok Bae, Jongyoul Park", "title": "An Energy and GPU-Computation Efficient Backbone Network for Real-Time\n  Object Detection", "comments": "CVPR2019 CEFRL Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As DenseNet conserves intermediate features with diverse receptive fields by\naggregating them with dense connection, it shows good performance on the object\ndetection task. Although feature reuse enables DenseNet to produce strong\nfeatures with a small number of model parameters and FLOPs, the detector with\nDenseNet backbone shows rather slow speed and low energy efficiency. We find\nthe linearly increasing input channel by dense connection leads to heavy memory\naccess cost, which causes computation overhead and more energy consumption. To\nsolve the inefficiency of DenseNet, we propose an energy and computation\nefficient architecture called VoVNet comprised of One-Shot Aggregation (OSA).\nThe OSA not only adopts the strength of DenseNet that represents diversified\nfeatures with multi receptive fields but also overcomes the inefficiency of\ndense connection by aggregating all features only once in the last feature\nmaps. To validate the effectiveness of VoVNet as a backbone network, we design\nboth lightweight and large-scale VoVNet and apply them to one-stage and\ntwo-stage object detectors. Our VoVNet based detectors outperform DenseNet\nbased ones with 2x faster speed and the energy consumptions are reduced by 1.6x\n- 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet\nbackbone with faster speed and better energy efficiency. In particular, the\nsmall object detection performance has been significantly improved over\nDenseNet and ResNet.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:45:57 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Lee", "Youngwan", ""], ["Hwang", "Joong-won", ""], ["Lee", "Sangrok", ""], ["Bae", "Yuseok", ""], ["Park", "Jongyoul", ""]]}, {"id": "1904.09737", "submitter": "Yongpei Zhu", "authors": "Yongpei Zhu, Hongwei Fan, Kehong Yuan", "title": "Facial Expression Recognition Research Based on Deep Learning", "comments": "12 pages,13 figures.The paper is under consideration at Pattern\n  Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, the structure of convolution neural\nnetwork is becoming more and more complex and the performance of object\nrecognition is getting better. However, the classification mechanism of\nconvolution neural networks is still an unsolved core problem. The main problem\nis that convolution neural networks have too many parameters, which makes it\ndifficult to analyze them. In this paper, we design and train a convolution\nneural network based on the expression recognition, and explore the\nclassification mechanism of the network. By using the Deconvolution\nvisualization method, the extremum point of the convolution neural network is\nprojected back to the pixel space of the original image, and we qualitatively\nverify that the trained expression recognition convolution neural network forms\na detector for the specific facial action unit. At the same time, we design the\ndistance function to measure the distance between the presence of facial\nfeature unit and the maximal value of the response on the feature map of\nconvolution neural network. The greater the distance, the more sensitive the\nfeature map is to the facial feature unit. By comparing the maximum distance of\nall facial feature elements in the feature graph, the mapping relationship\nbetween facial feature element and convolution neural network feature map is\ndetermined. Therefore, we have verified that the convolution neural network has\nformed a detector for the facial Action unit in the training process to realize\nthe expression recognition.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 06:13:47 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 15:02:42 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 11:30:32 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhu", "Yongpei", ""], ["Fan", "Hongwei", ""], ["Yuan", "Kehong", ""]]}, {"id": "1904.09739", "submitter": "Xingang Pan", "authors": "Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, Ping Luo", "title": "Switchable Whitening for Deep Representation Learning", "comments": "Accepted to ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization methods are essential components in convolutional neural\nnetworks (CNNs). They either standardize or whiten data using statistics\nestimated in predefined sets of pixels. Unlike existing works that design\nnormalization techniques for specific tasks, we propose Switchable Whitening\n(SW), which provides a general form unifying different whitening methods as\nwell as standardization methods. SW learns to switch among these operations in\nan end-to-end manner. It has several advantages. First, SW adaptively selects\nappropriate whitening or standardization statistics for different tasks (see\nFig.1), making it well suited for a wide range of tasks without manual design.\nSecond, by integrating benefits of different normalizers, SW shows consistent\nimprovements over its counterparts in various challenging benchmarks. Third, SW\nserves as a useful tool for understanding the characteristics of whitening and\nstandardization techniques. We show that SW outperforms other alternatives on\nimage classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K,\nCityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer\n(COCO). For example, without bells and whistles, we achieve state-of-the-art\nperformance with 45.33% mIoU on the ADE20K dataset. Code is available at\nhttps://github.com/XingangPan/Switchable-Whitening.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 06:22:55 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 14:16:30 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 12:11:40 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 11:18:05 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Pan", "Xingang", ""], ["Zhan", "Xiaohang", ""], ["Shi", "Jianping", ""], ["Tang", "Xiaoou", ""], ["Luo", "Ping", ""]]}, {"id": "1904.09740", "submitter": "Dr. Mohammed Javed", "authors": "VB Aswin, Mohammed Javed, Parag Parihar, K Aswanth, CR Druval, Anpam\n  Dagar, CV Aravinda", "title": "NLP Driven Ensemble Based Automatic Subtitle Generation and Semantic\n  Video Summarization Technique", "comments": "Accepted in AIDE2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic subtitle generation and semantic video\nsummarization technique. The importance of automatic video summarization is\nvast in the present era of big data. Video summarization helps in efficient\nstorage and also quick surfing of large collection of videos without losing the\nimportant ones. The summarization of the videos is done with the help of\nsubtitles which is obtained using several text summarization algorithms. The\nproposed technique generates the subtitle for videos with/without subtitles\nusing speech recognition and then applies NLP based Text summarization\nalgorithms on the subtitles. The performance of subtitle generation and video\nsummarization is boosted through Ensemble method with two approaches such as\nIntersection method and Weight based learning method Experimental results\nreported show the satisfactory performance of the proposed method\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 06:32:15 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Aswin", "VB", ""], ["Javed", "Mohammed", ""], ["Parihar", "Parag", ""], ["Aswanth", "K", ""], ["Druval", "CR", ""], ["Dagar", "Anpam", ""], ["Aravinda", "CV", ""]]}, {"id": "1904.09742", "submitter": "Mengdan Feng", "authors": "Mengdan Feng, Sixing Hu, Marcelo Ang, Gim Hee Lee", "title": "2D3D-MatchNet: Learning to Match Keypoints Across 2D Image and 3D Point\n  Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale point cloud generated from 3D sensors is more accurate than its\nimage-based counterpart. However, it is seldom used in visual pose estimation\ndue to the difficulty in obtaining 2D-3D image to point cloud correspondences.\nIn this paper, we propose the 2D3D-MatchNet - an end-to-end deep network\narchitecture to jointly learn the descriptors for 2D and 3D keypoint from image\nand point cloud, respectively. As a result, we are able to directly match and\nestablish 2D-3D correspondences from the query image and 3D point cloud\nreference map for visual pose estimation. We create our Oxford 2D-3D Patches\ndataset from the Oxford Robotcar dataset with the ground truth camera poses and\n2D-3D image to point cloud correspondences for training and testing the deep\nnetwork. Experimental results verify the feasibility of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 06:49:46 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Feng", "Mengdan", ""], ["Hu", "Sixing", ""], ["Ang", "Marcelo", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1904.09747", "submitter": "Jian Zhang", "authors": "Jian Zhang, Jun Yu, Dacheng Tao", "title": "Local Deep-Feature Alignment for Unsupervised Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised deep-learning framework named Local\nDeep-Feature Alignment (LDFA) for dimension reduction. We construct\nneighbourhood for each data sample and learn a local Stacked Contractive\nAuto-encoder (SCAE) from the neighbourhood to extract the local deep features.\nNext, we exploit an affine transformation to align the local deep features of\neach neighbourhood with the global features. Moreover, we derive an approach\nfrom LDFA to map explicitly a new data sample into the learned low-dimensional\nsubspace. The advantage of the LDFA method is that it learns both local and\nglobal characteristics of the data sample set: the local SCAEs capture local\ncharacteristics contained in the data set, while the global alignment\nprocedures encode the interdependencies between neighbourhoods into the final\nlow-dimensional feature representations. Experimental results on data\nvisualization, clustering and classification show that the LDFA method is\ncompetitive with several well-known dimension reduction techniques, and\nexploiting locality in deep learning is a research topic worth further\nexploring.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:04:02 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Jian", ""], ["Yu", "Jun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.09757", "submitter": "Tong Chen", "authors": "Haojie Liu, Tong Chen, Peiyao Guo, Qiu Shen, Xun Cao, Yao Wang, Zhan\n  Ma", "title": "Non-local Attention Optimized Deep Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Non-Local Attention Optimized Deep Image\nCompression (NLAIC) framework, which is built on top of the popular variational\nauto-encoder (VAE) structure. Our NLAIC framework embeds non-local operations\nin the encoders and decoders for both image and latent feature probability\ninformation (known as hyperprior) to capture both local and global\ncorrelations, and apply attention mechanism to generate masks that are used to\nweigh the features for the image and hyperprior, which implicitly adapt bit\nallocation for different features based on their importance. Furthermore, both\nhyperpriors and spatial-channel neighbors of the latent features are used to\nimprove entropy coding. The proposed model outperforms the existing methods on\nKodak dataset, including learned (e.g., Balle2019, Balle2018) and conventional\n(e.g., BPG, JPEG2000, JPEG) image compression methods, for both PSNR and\nMS-SSIM distortion metrics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:51:17 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Liu", "Haojie", ""], ["Chen", "Tong", ""], ["Guo", "Peiyao", ""], ["Shen", "Qiu", ""], ["Cao", "Xun", ""], ["Wang", "Yao", ""], ["Ma", "Zhan", ""]]}, {"id": "1904.09758", "submitter": "Yuxiang Wu", "authors": "Yuxiang Wu, Zehua Cheng, Bin Huang, Yiming Chen, Xinghui Zhu, Weiyang\n  Wang", "title": "FoxNet: A Multi-face Alignment Method", "comments": "Accepted by the 26th IEEE International Conference on Image\n  Processing(ICIP2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-face alignment aims to identify geometry structures of multiple faces\nin an image, and its performance is essential for the many practical tasks,\nsuch as face recognition, face tracking, and face animation. In this work, we\npresent a fast bottom-up multi-face alignment approach, which can\nsimultaneously localize multi-person facial landmarks with high precision.In\nmore detail, our bottom-up architecture maps the landmarks to the\nhigh-dimensional space with which landmarks of all faces are represented. By\nclustering the features belonging to the same face, our approach can align the\nmulti-person facial landmarks synchronously.Extensive experiments show that our\nmethod can achieve high performance in the multi-face landmark alignment task\nwhile our model is extremely fast. Moreover, we propose a new multi-face\ndataset to compare the speed and precision of bottom-up face alignment method\nwith top-down methods. Our dataset is publicly available at\nhttps://github.com/AISAResearch/FoxNet\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:52:04 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 09:50:11 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wu", "Yuxiang", ""], ["Cheng", "Zehua", ""], ["Huang", "Bin", ""], ["Chen", "Yiming", ""], ["Zhu", "Xinghui", ""], ["Wang", "Weiyang", ""]]}, {"id": "1904.09763", "submitter": "Seungjun Jung", "authors": "Seungjun Jung, Muhammad Abul Hasan and Changick Kim", "title": "Water-Filling: An Efficient Algorithm for Digitized Document Shadow\n  Removal", "comments": "Accepted at Asian Conference on Computer Vision (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel algorithm to rectify illumination of the\ndigitized documents by eliminating shading artifacts. Firstly, a topographic\nsurface of an input digitized document is created using luminance value of each\npixel. Then the shading artifact on the document is estimated by simulating an\nimmersion process. The simulation of the immersion process is modeled using a\nnovel diffusion equation with an iterative update rule. After estimating the\nshading artifacts, the digitized document is reconstructed using the Lambertian\nsurface model. In order to evaluate the performance of the proposed algorithm,\nwe conduct rigorous experiments on a set of digitized documents which is\ngenerated using smartphones under challenging lighting conditions. According to\nthe experimental results, it is found that the proposed method produces\npromising illumination correction results and outperforms the results of the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:01:27 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 18:44:59 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Jung", "Seungjun", ""], ["Hasan", "Muhammad Abul", ""], ["Kim", "Changick", ""]]}, {"id": "1904.09764", "submitter": "Jiahui Huang", "authors": "Jiahui Huang, Kshitij Dwivedi, Gemma Roig", "title": "Deep Anchored Convolutional Neural Networks", "comments": "This paper is accepted to 2019 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition Workshops (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been proven to be extremely\nsuccessful at solving computer vision tasks. State-of-the-art methods favor\nsuch deep network architectures for its accuracy performance, with the cost of\nhaving massive number of parameters and high weights redundancy. Previous works\nhave studied how to prune such CNNs weights. In this paper, we go to another\nextreme and analyze the performance of a network stacked with a single\nconvolution kernel across layers, as well as other weights sharing techniques.\nWe name it Deep Anchored Convolutional Neural Network (DACNN). Sharing the same\nkernel weights across layers allows to reduce the model size tremendously, more\nprecisely, the network is compressed in memory by a factor of L, where L is the\ndesired depth of the network, disregarding the fully connected layer for\nprediction. The number of parameters in DACNN barely increases as the network\ngrows deeper, which allows us to build deep DACNNs without any concern about\nmemory costs. We also introduce a partial shared weights network (DACNN-mix) as\nwell as an easy-plug-in module, coined regulators, to boost the performance of\nour architecture. We validated our idea on 3 datasets: CIFAR-10, CIFAR-100 and\nSVHN. Our results show that we can save massive amounts of memory with our\nmodel, while maintaining a high accuracy performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:07:00 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Huang", "Jiahui", ""], ["Dwivedi", "Kshitij", ""], ["Roig", "Gemma", ""]]}, {"id": "1904.09781", "submitter": "Wei Yi", "authors": "Wei Yi, Yaoran Sun, Tao Ding, Sailing He", "title": "Detecting retail products in situ using CNN without human effort\n  labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN is a powerful tool for many computer vision tasks, achieving much better\nresult than traditional methods. Since CNN has a very large capacity, training\nsuch a neural network often requires many data, but it is often expensive to\nobtain labeled images in real practice, especially for object detection, where\ncollecting bounding box of every object in training set requires many human\nefforts. This is the case in detection of retail products where there can be\nmany different categories. In this paper, we focus on applying CNN to detect\n324-categories products in situ, while requiring no extra effort of labeling\nbounding box for any image. Our approach is based on an algorithm that extracts\nbounding box from in-vitro dataset and an algorithm to simulate occlusion. We\nhave successfully shown the effectiveness and usefulness of our methods to\nbuild up a Faster RCNN detection model. Similar idea is also applicable in\nother scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 09:20:30 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yi", "Wei", ""], ["Sun", "Yaoran", ""], ["Ding", "Tao", ""], ["He", "Sailing", ""]]}, {"id": "1904.09791", "submitter": "Seoung Wug Oh", "authors": "Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim", "title": "Fast User-Guided Video Object Segmentation by\n  Interaction-and-Propagation Networks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning method for the interactive video object\nsegmentation. Our method is built upon two core operations, interaction and\npropagation, and each operation is conducted by Convolutional Neural Networks.\nThe two networks are connected both internally and externally so that the\nnetworks are trained jointly and interact with each other to solve the complex\nvideo object segmentation problem. We propose a new multi-round training scheme\nfor the interactive video object segmentation so that the networks can learn\nhow to understand the user's intention and update incorrect estimations during\nthe training. At the testing time, our method produces high-quality results and\nalso runs fast enough to work with users interactively. We evaluated the\nproposed method quantitatively on the interactive track benchmark at the DAVIS\nChallenge 2018. We outperformed other competing methods by a significant margin\nin both the speed and the accuracy. We also demonstrated that our method works\nwell with real user interactions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 10:17:46 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 09:17:22 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Oh", "Seoung Wug", ""], ["Lee", "Joon-Young", ""], ["Xu", "Ning", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1904.09793", "submitter": "Wenxiao Zhang", "authors": "Wenxiao Zhang, Chunxia Xiao", "title": "PCAN: 3D Attention Map Learning Using Contextual Information for Point\n  Cloud Based Retrieval", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud based retrieval for place recognition is an emerging problem in\nvision field. The main challenge is how to find an efficient way to encode the\nlocal features into a discriminative global descriptor. In this paper, we\npropose a Point Contextual Attention Network (PCAN), which can predict the\nsignificance of each local point feature based on point context. Our network\nmakes it possible to pay more attention to the task-relevent features when\naggregating local features. Experiments on various benchmark datasets show that\nthe proposed network can provide outperformance than current state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 10:28:20 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Wenxiao", ""], ["Xiao", "Chunxia", ""]]}, {"id": "1904.09804", "submitter": "Jiaming Zhang", "authors": "Jiaming Zhang, Jitao Sang, Kaiyuan Xu, Shangxi Wu, Yongli Hu, Yanfeng\n  Sun and Jian Yu", "title": "blessing in disguise: Designing Robust Turing Test by Employing\n  Algorithm Unrobustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turing test was originally proposed to examine whether machine's behavior is\nindistinguishable from a human. The most popular and practical Turing test is\nCAPTCHA, which is to discriminate algorithm from human by offering\nrecognition-alike questions. The recent development of deep learning has\nsignificantly advanced the capability of algorithm in solving CAPTCHA\nquestions, forcing CAPTCHA designers to increase question complexity. Instead\nof designing questions difficult for both algorithm and human, this study\nattempts to employ the limitations of algorithm to design robust CAPTCHA\nquestions easily solvable to human. Specifically, our data analysis observes\nthat human and algorithm demonstrates different vulnerability to visual\ndistortions: adversarial perturbation is significantly annoying to algorithm\nyet friendly to human. We are motivated to employ adversarially perturbed\nimages for robust CAPTCHA design in the context of character-based questions.\nThree modules of multi-target attack, ensemble adversarial training, and image\npreprocessing differentiable approximation are proposed to address the\ncharacteristics of character-based CAPTCHA cracking. Qualitative and\nquantitative experimental results demonstrate the effectiveness of the proposed\nsolution. We hope this study can lead to the discussions around adversarial\nattack/defense in CAPTCHA design and also inspire the future attempts in\nemploying algorithm limitation for practical usage.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 11:41:30 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Jiaming", ""], ["Sang", "Jitao", ""], ["Xu", "Kaiyuan", ""], ["Wu", "Shangxi", ""], ["Hu", "Yongli", ""], ["Sun", "Yanfeng", ""], ["Yu", "Jian", ""]]}, {"id": "1904.09811", "submitter": "Kateryna Chumachenko", "authors": "Kateryna Chumachenko, Anssi M\\\"annist\\\"o, Alexandros Iosifidis, Jenni\n  Raitoharju", "title": "Machine Learning Based Analysis of Finnish World War II Photographers", "comments": null, "journal-ref": "IEEE Access (2020), Vol. 8, pp. 144184-144196", "doi": "10.1109/ACCESS.2020.3014458", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate the benefits of using state-of-the-art machine\nlearning methods in the analysis of historical photo archives. Specifically, we\nanalyze prominent Finnish World War II photographers, who have captured high\nnumbers of photographs in the publicly available Finnish Wartime Photograph\nArchive, which contains 160,000 photographs from Finnish Winter, Continuation,\nand Lapland Wars captures in 1939-1945. We were able to find some special\ncharacteristics for different photographers in terms of their typical photo\ncontent and framing (e.g., close-ups vs. overall shots, number of people).\nFurthermore, we managed to train a neural network that can successfully\nrecognize the photographer from some of the photos, which shows that such\nphotos are indeed characteristic for certain photographers. We further analyzed\nthe similarities and differences between the photographers using the features\nextracted from the photographer classifier network. We make our annotations and\nanalysis pipeline publicly available, in an effort to introduce this new\nresearch problem to the machine learning and computer vision communities and\nfacilitate future research in historical and societal studies over the photo\narchives.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:07:03 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 15:17:57 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 14:31:51 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2020 17:13:48 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chumachenko", "Kateryna", ""], ["M\u00e4nnist\u00f6", "Anssi", ""], ["Iosifidis", "Alexandros", ""], ["Raitoharju", "Jenni", ""]]}, {"id": "1904.09823", "submitter": "Yingchao Feng", "authors": "Yingchao Feng, Wenhui Diao, Zhonghan Chang, Menglong Yan, Xian Sun,\n  Xin Gao", "title": "Ship Instance Segmentation From Remote Sensing Images Using Sequence\n  Local Context Module", "comments": "4 pages, 5 figures, IEEE Geoscience and Remote Sensing Society 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of object instance segmentation in remote sensing images has\nbeen greatly improved through the introduction of many landmark frameworks\nbased on convolutional neural network. However, the object densely issue still\naffects the accuracy of such segmentation frameworks. Objects of the same class\nare easily confused, which is most likely due to the close docking between\nobjects. We think context information is critical to address this issue. So, we\npropose a novel framework called SLCMASK-Net, in which a sequence local context\nmodule (SLC) is introduced to avoid confusion between objects of the same\nclass. The SLC module applies a sequence of dilation convolution blocks to\nprogressively learn multi-scale context information in the mask branch.\nBesides, we try to add SLC module to different locations in our framework and\nexperiment with the effect of different parameter settings. Comparative\nexperiments are conducted on remote sensing images acquired by QuickBird with a\nresolution of $0.5m-1m$ and the results show that the proposed method achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:33:06 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Feng", "Yingchao", ""], ["Diao", "Wenhui", ""], ["Chang", "Zhonghan", ""], ["Yan", "Menglong", ""], ["Sun", "Xian", ""], ["Gao", "Xin", ""]]}, {"id": "1904.09843", "submitter": "Varun Jain", "authors": "Varun Jain, Gaurav Garg, Ramakrishna Perla, Ramya Hebbalaguppe", "title": "GestARLite: An On-Device Pointing Finger Based Gestural Interface for\n  Smartphones and Video See-Through Head-Mounts", "comments": "The AAAI 2019 Workshop on Plan, Activity, and Intent Recognition.\n  arXiv admin note: substantial text overlap with arXiv:1904.06122", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gestures form an intuitive means of interaction in Mixed Reality (MR)\napplications. However, accurate gesture recognition can be achieved only\nthrough state-of-the-art deep learning models or with the use of expensive\nsensors. Despite the robustness of these deep learning models, they are\ngenerally computationally expensive and obtaining real-time performance\non-device is still a challenge. To this end, we propose a novel lightweight\nhand gesture recognition framework that works in First Person View for wearable\ndevices. The models are trained on a GPU machine and ported on an Android\nsmartphone for its use with frugal wearable devices such as the Google\nCardboard and VR Box. The proposed hand gesture recognition framework is driven\nby a cascade of state-of-the-art deep learning models: MobileNetV2 for hand\nlocalisation, our custom fingertip regression architecture followed by a\nBi-LSTM model for gesture classification. We extensively evaluate the framework\non our EgoGestAR dataset. The overall framework works in real-time on mobile\ndevices and achieves a classification accuracy of 80% on EgoGestAR video\ndataset with an average latency of only 0.12 s.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:32:40 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Jain", "Varun", ""], ["Garg", "Gaurav", ""], ["Perla", "Ramakrishna", ""], ["Hebbalaguppe", "Ramya", ""]]}, {"id": "1904.09853", "submitter": "Mingnan Luo", "authors": "Mingnan Luo, Guihua Wen, Yang Hu, Dan Dai, Yingxue Xu", "title": "Stochastic Region Pooling: Make Attention More Expressive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Average Pooling (GAP) is used by default on the channel-wise attention\nmechanism to extract channel descriptors. However, the simple global\naggregation method of GAP is easy to make the channel descriptors have\nhomogeneity, which weakens the detail distinction between feature maps, thus\naffecting the performance of the attention mechanism. In this work, we propose\na novel method for channel-wise attention network, called Stochastic Region\nPooling (SRP), which makes the channel descriptors more representative and\ndiversity by encouraging the feature map to have more or wider important\nfeature responses. Also, SRP is the general method for the attention mechanisms\nwithout any additional parameters or computation. It can be widely applied to\nattention networks without modifying the network structure. Experimental\nresults on image recognition datasets including CIAFR-10/100, ImageNet and\nthree Fine-grained datasets (CUB-200-2011, Stanford Cars and Stanford Dogs)\nshow that SRP brings the significant improvements of the performance over\nefficient CNNs and achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:17:19 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Luo", "Mingnan", ""], ["Wen", "Guihua", ""], ["Hu", "Yang", ""], ["Dai", "Dan", ""], ["Xu", "Yingxue", ""]]}, {"id": "1904.09856", "submitter": "Zhucun Xue", "authors": "Zhucun Xue, Nan Xue, Gui-Song Xia, Weiming Shen", "title": "Learning to Calibrate Straight Lines for Fisheye Image Rectification", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new deep-learning based method to simultaneously\ncalibrate the intrinsic parameters of fisheye lens and rectify the distorted\nimages. Assuming that the distorted lines generated by fisheye projection\nshould be straight after rectification, we propose a novel deep neural network\nto impose explicit geometry constraints onto processes of the fisheye lens\ncalibration and the distorted image rectification. In addition, considering the\nnonlinearity of distortion distribution in fisheye images, the proposed network\nfully exploits multi-scale perception to equalize the rectification effects on\nthe whole image. To train and evaluate the proposed model, we also create a new\nlargescale dataset labeled with corresponding distortion parameters and\nwell-annotated distorted lines. Compared with the state-of-the-art methods, our\nmodel achieves the best published rectification quality and the most accurate\nestimation of distortion parameters on a large set of synthetic and real\nfisheye images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:25:36 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 13:11:51 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Xue", "Zhucun", ""], ["Xue", "Nan", ""], ["Xia", "Gui-Song", ""], ["Shen", "Weiming", ""]]}, {"id": "1904.09862", "submitter": "Khaled Saleh", "authors": "Khaled Saleh, Mohammed Hossny, Saeid Nahavandi", "title": "Real-time Intent Prediction of Pedestrians for Autonomous Ground\n  Vehicles via Spatio-Temporal DenseNet", "comments": "Accepted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behaviors and intentions of humans are one of the main\nchallenges autonomous ground vehicles still faced with. More specifically, when\nit comes to complex environments such as urban traffic scenes, inferring the\nintentions and actions of vulnerable road users such as pedestrians become even\nharder. In this paper, we address the problem of intent action prediction of\npedestrians in urban traffic environments using only image sequences from a\nmonocular RGB camera. We propose a real-time framework that can accurately\ndetect, track and predict the intended actions of pedestrians based on a\ntracking-by-detection technique in conjunction with a novel spatio-temporal\nDenseNet model. We trained and evaluated our framework based on real data\ncollected from urban traffic environments. Our framework has shown resilient\nand competitive results in comparison to other baseline approaches. Overall, we\nachieved an average precision score of 84.76% with a real-time performance at\n20 FPS.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:30:34 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Saleh", "Khaled", ""], ["Hossny", "Mohammed", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1904.09872", "submitter": "Evgenii Zheltonozhskii", "authors": "Yochai Zur, Chaim Baskin, Evgenii Zheltonozhskii, Brian Chmiel, Itay\n  Evron, Alex M. Bronstein and Avi Mendelson", "title": "Towards Learning of Filter-Level Heterogeneous Compression of\n  Convolutional Neural Networks", "comments": "Accepted to ICML Workshop on AutoML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, deep learning has become a de facto standard in machine learning\nwith convolutional neural networks (CNNs) demonstrating spectacular success on\na wide variety of tasks. However, CNNs are typically very demanding\ncomputationally at inference time. One of the ways to alleviate this burden on\ncertain hardware platforms is quantization relying on the use of low-precision\narithmetic representation for the weights and the activations. Another popular\nmethod is the pruning of the number of filters in each layer. While mainstream\ndeep learning methods train the neural networks weights while keeping the\nnetwork architecture fixed, the emerging neural architecture search (NAS)\ntechniques make the latter also amenable to training. In this paper, we\nformulate optimal arithmetic bit length allocation and neural network pruning\nas a NAS problem, searching for the configurations satisfying a computational\ncomplexity budget while maximizing the accuracy. We use a differentiable search\nmethod based on the continuous relaxation of the search space proposed by Liu\net al. (arXiv:1806.09055). We show, by grid search, that heterogeneous\nquantized networks suffer from a high variance which renders the benefit of the\nsearch questionable. For pruning, improvement over homogeneous cases is\npossible, but it is still challenging to find those configurations with the\nproposed method. The code is publicly available at\nhttps://github.com/yochaiz/Slimmable and https://github.com/yochaiz/darts-UNIQ\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:43:34 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 09:24:07 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 10:40:41 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 14:45:46 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zur", "Yochai", ""], ["Baskin", "Chaim", ""], ["Zheltonozhskii", "Evgenii", ""], ["Chmiel", "Brian", ""], ["Evron", "Itay", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1904.09879", "submitter": "Alexander Wong", "authors": "Audrey Chung, Paul Fieguth, and Alexander Wong", "title": "Assessing Architectural Similarity in Populations of Deep Neural\n  Networks", "comments": "3 pages. arXiv admin note: text overlap with arXiv:1811.07966", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary deep intelligence has recently shown great promise for producing\nsmall, powerful deep neural network models via the synthesis of increasingly\nefficient architectures over successive generations. Despite recent research\nshowing the efficacy of multi-parent evolutionary synthesis, little has been\ndone to directly assess architectural similarity between networks during the\nsynthesis process for improved parent network selection. In this work, we\npresent a preliminary study into quantifying architectural similarity via the\npercentage overlap of architectural clusters. Results show that networks\nsynthesized using architectural alignment (via gene tagging) maintain higher\narchitectural similarities within each generation, potentially restricting the\nsearch space of highly efficient network architectures.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 13:33:49 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chung", "Audrey", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1904.09882", "submitter": "Evonne Ng", "authors": "Evonne Ng, Donglai Xiang, Hanbyul Joo and Kristen Grauman", "title": "You2Me: Inferring Body Pose in Egocentric Video via First and Second\n  Person Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The body pose of a person wearing a camera is of great interest for\napplications in augmented reality, healthcare, and robotics, yet much of the\nperson's body is out of view for a typical wearable camera. We propose a\nlearning-based approach to estimate the camera wearer's 3D body pose from\negocentric video sequences. Our key insight is to leverage interactions with\nanother person---whose body pose we can directly observe---as a signal\ninherently linked to the body pose of the first-person subject. We show that\nsince interactions between individuals often induce a well-ordered series of\nback-and-forth responses, it is possible to learn a temporal model of the\ninterlinked poses even though one party is largely out of view. We demonstrate\nour idea on a variety of domains with dyadic interaction and show the\nsubstantial impact on egocentric body pose estimation, which improves the state\nof the art. Video results are available at\nhttp://vision.cs.utexas.edu/projects/you2me/\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:58:49 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 03:56:28 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ng", "Evonne", ""], ["Xiang", "Donglai", ""], ["Joo", "Hanbyul", ""], ["Grauman", "Kristen", ""]]}, {"id": "1904.09901", "submitter": "Adam Van Etten", "authors": "Adam Van Etten", "title": "City-scale Road Extraction from Satellite Imagery", "comments": "6 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated road network extraction from remote sensing imagery remains a\nsignificant challenge despite its importance in a broad array of applications.\nTo this end, we leverage recent open source advances and the high quality\nSpaceNet dataset to explore road network extraction at scale, an approach we\ncall City-scale Road Extraction from Satellite Imagery (CRESI). Specifically,\nwe create an algorithm to extract road networks directly from imagery over\ncity-scale regions, which can subsequently be used for routing purposes. We\nquantify the performance of our algorithm with the APLS and TOPO\ngraph-theoretic metrics over a diverse 608 square kilometer test area covering\nfour cities. We find an aggregate score of APLS = 0.73, and a TOPO score of\n0.58 (a significant improvement over existing methods). Inference speed is 160\nsquare kilometers per hour on modest hardware. Finally, we demonstrate that one\ncan use the extracted road network for any number of applications, such as\noptimized routing.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 14:36:57 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 16:00:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Van Etten", "Adam", ""]]}, {"id": "1904.09925", "submitter": "Irwan Bello", "authors": "Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V. Le", "title": "Attention Augmented Convolutional Networks", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks have been the paradigm of choice in many computer\nvision applications. The convolution operation however has a significant\nweakness in that it only operates on a local neighborhood, thus missing global\ninformation. Self-attention, on the other hand, has emerged as a recent advance\nto capture long range interactions, but has mostly been applied to sequence\nmodeling and generative modeling tasks. In this paper, we consider the use of\nself-attention for discriminative visual tasks as an alternative to\nconvolutions. We introduce a novel two-dimensional relative self-attention\nmechanism that proves competitive in replacing convolutions as a stand-alone\ncomputational primitive for image classification. We find in control\nexperiments that the best results are obtained when combining both convolutions\nand self-attention. We therefore propose to augment convolutional operators\nwith this self-attention mechanism by concatenating convolutional feature maps\nwith a set of feature maps produced via self-attention. Extensive experiments\nshow that Attention Augmentation leads to consistent improvements in image\nclassification on ImageNet and object detection on COCO across many different\nmodels and scales, including ResNets and a state-of-the art mobile constrained\nnetwork, while keeping the number of parameters similar. In particular, our\nmethod achieves a $1.3\\%$ top-1 accuracy improvement on ImageNet classification\nover a ResNet50 baseline and outperforms other attention mechanisms for images\nsuch as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in\nCOCO Object Detection on top of a RetinaNet baseline.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:31:15 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 20:10:55 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 16:38:19 GMT"}, {"version": "v4", "created": "Sat, 28 Sep 2019 01:12:10 GMT"}, {"version": "v5", "created": "Wed, 9 Sep 2020 18:52:40 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Bello", "Irwan", ""], ["Zoph", "Barret", ""], ["Vaswani", "Ashish", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.09935", "submitter": "Ksenia Bittner", "authors": "Ksenia Bittner, Marco K\\\"orner, Peter Reinartz", "title": "Late or Earlier Information Fusion from Depth and Spectral Data?\n  Large-Scale Digital Surface Model Refinement by Hybrid-cGAN", "comments": "8 pages, This work was accepted to be presented at the IEEE/ISPRS\n  Workshop on Large Scale Computer Vision for Remote Sensing Imagery\n  (EarthVision) to be held at the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the workflow of a DSM refinement methodology using a Hybrid-cGAN\nwhere the generative part consists of two encoders and a common decoder which\nblends the spectral and height information within one network. The inputs to\nthe Hybrid-cGAN are single-channel photogrammetric DSMs with continuous values\nand single-channel pan-chromatic (PAN) half-meter resolution satellite images.\nExperimental results demonstrate that the earlier information fusion from data\nwith different physical meanings helps to propagate fine details and complete\nan inaccurate or missing 3D information about building forms. Moreover, it\nimproves the building boundaries making them more rectilinear.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:51:18 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Bittner", "Ksenia", ""], ["K\u00f6rner", "Marco", ""], ["Reinartz", "Peter", ""]]}, {"id": "1904.09936", "submitter": "Meera Hahn", "authors": "Meera Hahn, Asim Kadav, James M. Rehg and Hans Peter Graf", "title": "Tripping through time: Efficient Localization of Activities in Videos", "comments": "Presented at BMVC, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing moments in untrimmed videos via language queries is a new and\ninteresting task that requires the ability to accurately ground language into\nvideo. Previous works have approached this task by processing the entire video,\noften more than once, to localize relevant activities. In the real world\napplications of this approach, such as video surveillance, efficiency is a key\nsystem requirement. In this paper, we present TripNet, an end-to-end system\nthat uses a gated attention architecture to model fine-grained textual and\nvisual representations in order to align text and video content. Furthermore,\nTripNet uses reinforcement learning to efficiently localize relevant activity\nclips in long videos, by learning how to intelligently skip around the video.\nIt extracts visual features for few frames to perform activity classification.\nIn our evaluation over Charades-STA, ActivityNet Captions and the TACoS\ndataset, we find that TripNet achieves high accuracy and saves processing time\nby only looking at 32-41% of the entire video.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:53:13 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 18:41:21 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 17:06:49 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 17:49:05 GMT"}, {"version": "v5", "created": "Tue, 18 Aug 2020 16:56:23 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hahn", "Meera", ""], ["Kadav", "Asim", ""], ["Rehg", "James M.", ""], ["Graf", "Hans Peter", ""]]}, {"id": "1904.09939", "submitter": "Guanbin Li", "authors": "Guanbin Li, Xin Zhu, Yirui Zeng, Qing Wang, Liang Lin", "title": "Semantic Relationships Guided Representation Learning for Facial Action\n  Unit Recognition", "comments": "Accepted by AAAI2019 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit (AU) recognition is a crucial task for facial expressions\nanalysis and has attracted extensive attention in the field of artificial\nintelligence and computer vision. Existing works have either focused on\ndesigning or learning complex regional feature representations, or delved into\nvarious types of AU relationship modeling. Albeit with varying degrees of\nprogress, it is still arduous for existing methods to handle complex\nsituations. In this paper, we investigate how to integrate the semantic\nrelationship propagation between AUs in a deep neural network framework to\nenhance the feature representation of facial regions, and propose an AU\nsemantic relationship embedded representation learning (SRERL) framework.\nSpecifically, by analyzing the symbiosis and mutual exclusion of AUs in various\nfacial expressions, we organize the facial AUs in the form of structured\nknowledge-graph and integrate a Gated Graph Neural Network (GGNN) in a\nmulti-scale CNN framework to propagate node information through the graph for\ngenerating enhanced AU representation. As the learned feature involves both the\nappearance characteristics and the AU relationship reasoning, the proposed\nmodel is more robust and can cope with more challenging cases, e.g.,\nillumination change and partial occlusion. Extensive experiments on the two\npublic benchmarks demonstrate that our method outperforms the previous work and\nachieves state of the art performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:26:20 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Guanbin", ""], ["Zhu", "Xin", ""], ["Zeng", "Yirui", ""], ["Wang", "Qing", ""], ["Lin", "Liang", ""]]}, {"id": "1904.09947", "submitter": "Nicholas Turner", "authors": "Nicholas Turner, Kisuk Lee, Ran Lu, Jingpeng Wu, Dodam Ih, H.\n  Sebastian Seung", "title": "Synaptic Partner Assignment Using Attentional Voxel Association Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectomics aims to recover a complete set of synaptic connections within a\ndataset imaged by volume electron microscopy. Many systems have been proposed\nfor locating synapses, and recent research has included a way to identify the\nsynaptic partners that communicate at a synaptic cleft. We re-frame the problem\nof identifying synaptic partners as directly generating the mask of the\nsynaptic partners from a given cleft. We train a convolutional network to\nperform this task. The network takes the local image context and a binary mask\nrepresenting a single cleft as input. It is trained to produce two binary\noutput masks: one which labels the voxels of the presynaptic partner within the\ninput image, and another similar labeling for the postsynaptic partner. The\ncleft mask acts as an attentional gating signal for the network. We find that\nan implementation of this approach performs well on a dataset of mouse\nsomatosensory cortex, and evaluate it as part of a combined system to predict\nboth clefts and connections.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:49:41 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 01:39:03 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Turner", "Nicholas", ""], ["Lee", "Kisuk", ""], ["Lu", "Ran", ""], ["Wu", "Jingpeng", ""], ["Ih", "Dodam", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1904.09970", "submitter": "Despoina Paschalidou", "authors": "Despoina Paschalidou and Ali Osman Ulusoy and Andreas Geiger", "title": "Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids", "comments": "CVPR 2019 Camera Ready. Project\n  Page:https://github.com/paschalidoud/superquadric_parsing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstracting complex 3D shapes with parsimonious part-based representations\nhas been a long standing goal in computer vision. This paper presents a\nlearning-based solution to this problem which goes beyond the traditional 3D\ncuboid representation by exploiting superquadrics as atomic elements. We\ndemonstrate that superquadrics lead to more expressive 3D scene parses while\nbeing easier to learn than 3D cuboid representations. Moreover, we provide an\nanalytical solution to the Chamfer loss which avoids the need for computational\nexpensive reinforcement learning or iterative prediction. Our model learns to\nparse 3D objects into consistent superquadric representations without\nsupervision. Results on various ShapeNet categories as well as the SURREAL\nhuman body dataset demonstrate the flexibility of our model in capturing fine\ndetails and complex poses that could not have been modelled using cuboids.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 17:54:06 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Paschalidou", "Despoina", ""], ["Ulusoy", "Ali Osman", ""], ["Geiger", "Andreas", ""]]}, {"id": "1904.09974", "submitter": "Soonam Lee", "authors": "Soonam Lee and Shuo Han and Paul Salama and Kenneth W. Dunn and Edward\n  J. Delp", "title": "Three dimensional blind image deconvolution for fluorescence microscopy\n  using generative adversarial networks", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2019", "journal-ref": null, "doi": "10.1109/ISBI.2019.8759250", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to image blurring image deconvolution is often used for studying\nbiological structures in fluorescence microscopy. Fluorescence microscopy image\nvolumes inherently suffer from intensity inhomogeneity, blur, and are corrupted\nby various types of noise which exacerbate image quality at deeper tissue\ndepth. Therefore, quantitative analysis of fluorescence microscopy in deeper\ntissue still remains a challenge. This paper presents a three dimensional blind\nimage deconvolution method for fluorescence microscopy using 3-way spatially\nconstrained cycle-consistent adversarial networks. The restored volumes of the\nproposed deconvolution method and other well-known deconvolution methods,\ndenoising methods, and an inhomogeneity correction method are visually and\nnumerically evaluated. Experimental results indicate that the proposed method\ncan restore and improve the quality of blurred and noisy deep depth microscopy\nimage visually and quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 02:02:13 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Lee", "Soonam", ""], ["Han", "Shuo", ""], ["Salama", "Paul", ""], ["Dunn", "Kenneth W.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1904.09977", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Brian Grinstead, Qing He, Ye Duan", "title": "Web Based Brain Volume Calculation for Magnetic Resonance Images", "comments": null, "journal-ref": null, "doi": "10.1109/IEMBS.2008.4649380", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain volume calculations are crucial in modern medical research, especially\nin the study of neurodevelopmental disorders. In this paper, we present an\nalgorithm for calculating two classifications of brain volume, total brain\nvolume (TBV) and intracranial volume (ICV). Our algorithm takes MRI data as\ninput, performs several preprocessing and intermediate steps, and then returns\neach of the two calculated volumes. To simplify this process and make our\nalgorithm publicly accessible to anyone, we have created a web-based interface\nthat allows users to upload their own MRI data and calculate the TBV and ICV\nfor the given data. This interface provides a simple and efficient method for\ncalculating these two classifications of brain volume, and it also removes the\nneed for the user to download or install any applications.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 01:58:04 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Karsch", "Kevin", ""], ["Grinstead", "Brian", ""], ["He", "Qing", ""], ["Duan", "Ye", ""]]}, {"id": "1904.09978", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Qing He, Ye Duan", "title": "A Fast, Semi-Automatic Brain Structure Segmentation Algorithm for\n  Magnetic Resonance Imaging", "comments": null, "journal-ref": null, "doi": "10.1109/BIBM.2009.40", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation has become an essential technique in clinical and\nresearch-oriented applications. Because manual segmentation methods are\ntedious, and fully automatic segmentation lacks the flexibility of human\nintervention or correction, semi-automatic methods have become the preferred\ntype of medical image segmentation. We present a hybrid, semi-automatic\nsegmentation method in 3D that integrates both region-based and boundary-based\nprocedures. Our method differs from previous hybrid methods in that we perform\nregion-based and boundary-based approaches separately, which allows for more\nefficient segmentation. A region-based technique is used to generate an initial\nseed contour that roughly represents the boundary of a target brain structure,\nalleviating the local minima problem in the subsequent model deformation phase.\nThe contour is deformed under a unique force equation independent of image\nedges. Experiments on MRI data show that this method can achieve high accuracy\nand efficiency primarily due to the unique seed initialization technique.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 01:59:46 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Karsch", "Kevin", ""], ["He", "Qing", ""], ["Duan", "Ye", ""]]}, {"id": "1904.10014", "submitter": "Kuangen Zhang", "authors": "Kuangen Zhang, Ming Hao, Jing Wang, Clarence W. de Silva, Chenglong Fu", "title": "Linked Dynamic Graph CNN: Learning on Point Cloud via Linking\n  Hierarchical Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning on point cloud is eagerly in demand because the point cloud is a\ncommon type of geometric data and can aid robots to understand environments\nrobustly. However, the point cloud is sparse, unstructured, and unordered,\nwhich cannot be recognized accurately by a traditional convolutional neural\nnetwork (CNN) nor a recurrent neural network (RNN). Fortunately, a graph\nconvolutional neural network (Graph CNN) can process sparse and unordered data.\nHence, we propose a linked dynamic graph CNN (LDGCNN) to classify and segment\npoint cloud directly in this paper. We remove the transformation network, link\nhierarchical features from dynamic graphs, freeze feature extractor, and\nretrain the classifier to increase the performance of LDGCNN. We explain our\nnetwork using theoretical analysis and visualization. Through experiments, we\nshow that the proposed LDGCNN achieves state-of-art performance on two standard\ndatasets: ModelNet40 and ShapeNet.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 18:16:34 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 03:56:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhang", "Kuangen", ""], ["Hao", "Ming", ""], ["Wang", "Jing", ""], ["de Silva", "Clarence W.", ""], ["Fu", "Chenglong", ""]]}, {"id": "1904.10016", "submitter": "Jake Goldenfein", "authors": "Jake Goldenfein", "title": "The Profiling Potential of Computer Vision and the Challenge of\n  Computational Empiricism", "comments": null, "journal-ref": "Proceedings of the 2019 Conference on Fairness, Accountability,\n  and Transparency", "doi": "10.1145/3287560.3287568", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision and other biometrics data science applications have commenced\na new project of profiling people. Rather than using 'transaction generated\ninformation', these systems measure the 'real world' and produce an assessment\nof the 'world state' - in this case an assessment of some individual trait.\nInstead of using proxies or scores to evaluate people, they increasingly deploy\na logic of revealing the truth about reality and the people within it. While\nthese profiling knowledge claims are sometimes tentative, they increasingly\nsuggest that only through computation can these excesses of reality be captured\nand understood. This article explores the bases of those claims in the systems\nof measurement, representation, and classification deployed in computer vision.\nIt asks if there is something new in this type of knowledge claim, sketches an\naccount of a new form of computational empiricism being operationalised, and\nquestions what kind of human subject is being constructed by these\ntechnological systems and practices. Finally, the article explores legal\nmechanisms for contesting the emergence of computational empiricism as the\ndominant knowledge platform for understanding the world and the people within\nit.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 18:20:38 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Goldenfein", "Jake", ""]]}, {"id": "1904.10032", "submitter": "Javed Iqbal", "authors": "Javed Iqbal, Muhammad Akhtar Munir, Arif Mahmood, Afsheen Rafaqat Ali,\n  Mohsen Ali", "title": "Leveraging Orientation for Weakly Supervised Object Detection with\n  Application to Firearm Localization", "comments": "Accepted for Publication in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic detection of firearms is important for enhancing the security and\nsafety of people, however, it is a challenging task owing to the wide\nvariations in shape, size, and appearance of firearms. Also, most of the\ngeneric object detectors process axis-aligned rectangular areas though, a thin\nand long rifle may actually cover only a small percentage of that area and the\nrest may contain irrelevant details suppressing the required object signatures.\nTo handle these challenges, we propose a weakly supervised Orientation Aware\nObject Detection (OAOD) algorithm which learns to detect oriented object\nbounding boxes (OBB) while using AxisAligned Bounding Boxes (AABB) for\ntraining. The proposed OAOD is different from the existing oriented object\ndetectors which strictly require OBB during training which may not always be\npresent. The goal of training on AABB and detection of OBB is achieved by\nemploying a multistage scheme, with Stage-1 predicting the AABB and Stage-2\npredicting OBB. In-between the two stages, the oriented proposal generation\nmodule along with the object aligned RoI pooling is designed to extract\nfeatures based on the predicted orientation and to make these features\norientation invariant. A diverse and challenging dataset consisting of eleven\nthousand images is also proposed for firearm detection which is manually\nannotated for firearm classification and localization. The proposed ITU Firearm\ndataset (ITUF) contains a wide range of guns and rifles. The OAOD algorithm is\nevaluated on the ITUF dataset and compared with current state-of-the-art object\ndetectors, including fully supervised oriented object detectors. OAOD has\noutperformed both types of object detectors with a significant margin. The\nexperimental results (mAP: 88.3 on AABB & mAP: 77.5 on OBB) demonstrate the\neffectiveness of the proposed algorithm for firearm detection.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 18:56:43 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 19:40:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Iqbal", "Javed", ""], ["Munir", "Muhammad Akhtar", ""], ["Mahmood", "Arif", ""], ["Ali", "Afsheen Rafaqat", ""], ["Ali", "Mohsen", ""]]}, {"id": "1904.10037", "submitter": "Chun-Liang Li", "authors": "Chun-Liang Li, Tomas Simon, Jason Saragih, Barnab\\'as P\\'oczos, Yaser\n  Sheikh", "title": "LBS Autoencoder: Self-supervised Fitting of Articulated Meshes to Point\n  Clouds", "comments": "In the Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LBS-AE; a self-supervised autoencoding algorithm for fitting\narticulated mesh models to point clouds. As input, we take a sequence of point\nclouds to be registered as well as an artist-rigged mesh, i.e. a template mesh\nequipped with a linear-blend skinning (LBS) deformation space parameterized by\na skeleton hierarchy. As output, we learn an LBS-based autoencoder that\nproduces registered meshes from the input point clouds. To bridge the gap\nbetween the artist-defined geometry and the captured point clouds, our\nautoencoder models pose-dependent deviations from the template geometry. During\ntraining, instead of using explicit correspondences, such as key points or pose\nsupervision, our method leverages LBS deformations to bootstrap the learning\nprocess. To avoid poor local minima from erroneous point-to-point\ncorrespondences, we utilize a structured Chamfer distance based on\npart-segmentations, which are learned concurrently using self-supervision. We\ndemonstrate qualitative results on real captured hands, and report quantitative\nevaluations on the FAUST benchmark for body registration. Our method achieves\nperformance that is superior to other unsupervised approaches and comparable to\nmethods using supervised examples.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 19:12:18 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Li", "Chun-Liang", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1904.10044", "submitter": "Can Pu", "authors": "Can Pu and Robert B. Fisher", "title": "UDFNet: Unsupervised Disparity Fusion with Adversarial Networks", "comments": "13 pages. arXiv admin note: text overlap with arXiv:1803.06657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing disparity fusion methods based on deep learning achieve\nstate-of-the-art performance, but they require ground truth disparity data to\ntrain. As far as I know, this is the first time an unsupervised disparity\nfusion not using ground truth disparity data has been proposed. In this paper,\na mathematical model for disparity fusion is proposed to guide an adversarial\nnetwork to train effectively without ground truth disparity data. The initial\ndisparity maps are inputted from the left view along with auxiliary information\n(gradient, left & right intensity image) into the refiner and the refiner is\ntrained to output the refined disparity map registered on the left view. The\nrefined left disparity map and left intensity image are used to reconstruct a\nfake right intensity image. Finally, the fake and real right intensity images\n(from the right stereo vision camera) are fed into the discriminator. In the\nmodel, the refiner is trained to output a refined disparity value close to the\nweighted sum of the disparity inputs for global initialisation. Then, three\nrefinement principles are adopted to refine the results further. (1) The\nreconstructed intensity error between the fake and real right intensity image\nis minimised. (2) The similarities between the fake and real right image in\ndifferent receptive fields are maximised. (3) The refined disparity map is\nsmoothed based on the corresponding intensity image. The adversarial networks'\narchitectures are effective for the fusion task. The fusion time using the\nproposed network is small. The network can achieve 90 fps using Nvidia Geforce\nGTX 1080Ti on the Kitti2015 dataset when the input resolution is 1242 * 375\n(Width * Height) without downsampling and cropping. The accuracy of this work\nis equal to (or better than) the state-of-the-art supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 19:40:04 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Pu", "Can", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1904.10056", "submitter": "Yizhe Zhu", "authors": "Yizhe Zhu and Jianwen Xie and Bingchen Liu and Ahmed Elgammal", "title": "Learning Feature-to-Feature Translator by Alternating Back-Propagation\n  for Generative Zero-Shot Learning", "comments": "accepted to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate learning feature-to-feature translator networks by alternating\nback-propagation as a general-purpose solution to zero-shot learning (ZSL)\nproblems. It is a generative model-based ZSL framework. In contrast to models\nbased on generative adversarial networks (GAN) or variational autoencoders\n(VAE) that require auxiliary networks to assist the training, our model\nconsists of a single conditional generator that maps class-level semantic\nfeatures and Gaussian white noise vector accounting for instance-level latent\nfactors to visual features, and is trained by maximum likelihood estimation.\nThe training process is a simple yet effective alternating back-propagation\nprocess that iterates the following two steps: (i) the inferential\nback-propagation to infer the latent factors of each observed example, and (ii)\nthe learning back-propagation to update the model parameters. We show that,\nwith slight modifications, our model is capable of learning from incomplete\nvisual features for ZSL. We conduct extensive comparisons with existing\ngenerative ZSL methods on five benchmarks, demonstrating the superiority of our\nmethod in not only ZSL performance but also convergence speed and computational\ncost. Specifically, our model outperforms the existing state-of-the-art methods\nby a remarkable margin up to 3.1% and 4.0% in ZSL and generalized ZSL settings,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 20:30:21 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 02:12:32 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 04:49:15 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zhu", "Yizhe", ""], ["Xie", "Jianwen", ""], ["Liu", "Bingchen", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1904.10066", "submitter": "Marcus Scheunemann", "authors": "Marcus M. Scheunemann and Sander G. van Dijk and Rebecca Miko and\n  Daniel Barry and George M. Evans and Alessandra Rossi and Daniel Polani", "title": "Bold Hearts Team Description for RoboCup 2019 (Humanoid Kid Size League)", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participated in the RoboCup 2018 competition in Montreal with our newly\ndeveloped BoldBot based on the Darwin-OP and mostly self-printed custom parts.\nThis paper is about the lessons learnt from that competition and further\ndevelopments for the RoboCup 2019 competition. Firstly, we briefly introduce\nthe team along with an overview of past achievements. We then present a simple,\nstandalone 2D simulator we use for simplifying the entry for new members with\nmaking basic RoboCup concepts quickly accessible. We describe our approach for\nsemantic-segmentation for our vision used in the 2018 competition, which\nreplaced the lookup-table (LUT) implementation we had before. We also discuss\nthe extra structural support we plan to add to the printed parts of the BoldBot\nand our transition to ROS 2 as our new middleware. Lastly, we will present a\ncollection of open-source contributions of our team.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 21:10:25 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Scheunemann", "Marcus M.", ""], ["van Dijk", "Sander G.", ""], ["Miko", "Rebecca", ""], ["Barry", "Daniel", ""], ["Evans", "George M.", ""], ["Rossi", "Alessandra", ""], ["Polani", "Daniel", ""]]}, {"id": "1904.10076", "submitter": "Keren Gu", "authors": "Keren Gu, Brandon Yang, Jiquan Ngiam, Quoc Le, Jonathon Shlens", "title": "Using Videos to Evaluate Image Model Robustness", "comments": "Video Robustness Dataset included in directory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual systems are robust to a wide range of image transformations that\nare challenging for artificial networks. We present the first study of image\nmodel robustness to the minute transformations found across video frames, which\nwe term \"natural robustness\". Compared to previous studies on adversarial\nexamples and synthetic distortions, natural robustness captures a more diverse\nset of common image transformations that occur in the natural environment. Our\nstudy across a dozen model architectures shows that more accurate models are\nmore robust to natural transformations, and that robustness to synthetic color\ndistortions is a good proxy for natural robustness. In examining brittleness in\nvideos, we find that majority of the brittleness found in videos lies outside\nthe typical definition of adversarial examples (99.9\\%). Finally, we\ninvestigate training techniques to reduce brittleness and find that no single\ntechnique systematically improves natural robustness across twelve tested\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:13:22 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 16:58:54 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 23:18:47 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Gu", "Keren", ""], ["Yang", "Brandon", ""], ["Ngiam", "Jiquan", ""], ["Le", "Quoc", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1904.10082", "submitter": "Tiantong Guo", "authors": "Tiantong Guo, Hojjat S. Mousavi, and Vishal Monga", "title": "Adaptive Transform Domain Image Super-resolution Via Orthogonally\n  Regularized Deep Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2913500", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods, in particular, trained Convolutional Neural Networks\n(CNN) have recently been shown to produce compelling results for single image\nSuper-Resolution (SR). Invariably, a CNN is learned to map the Low Resolution\n(LR) image to its corresponding High Resolution (HR) version in the spatial\ndomain. We propose a novel network structure for learning the SR mapping\nfunction in an image transform domain, specifically the Discrete Cosine\nTransform (DCT). As the first contribution, we show that DCT can be integrated\ninto the network structure as a Convolutional DCT (CDCT) layer. With the CDCT\nlayer, we construct the DCT Deep SR (DCT-DSR) network. We further extend the\nDCT-DSR to allow the CDCT layer to become trainable (i.e., optimizable).\nBecause this layer represents an image transform, we enforce pairwise\northogonality constraints and newly formulated complexity order constraints on\nthe individual basis functions/filters. This Orthogonally Regularized Deep SR\nnetwork (ORDSR) simplifies the SR task by taking advantage of image transform\ndomain while adapting the design of transform basis to the training image set.\nExperimental results show ORDSR achieves state-of-the-art SR image quality with\nfewer parameters than most of the deep CNN methods. A particular success of\nORDSR is in overcoming the artifacts introduced by bicubic interpolation. A key\nburden of deep SR has been identified as the requirement of generous training\nLR and HR image pairs; ORSDR exhibits a much more graceful degradation as\ntraining size is reduced with significant benefits in the regime of limited\ntraining. Analysis of memory and computation requirements confirms that ORDSR\ncan allow for a more efficient network with faster inference.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:29:52 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Guo", "Tiantong", ""], ["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""]]}, {"id": "1904.10085", "submitter": "Samuel-Hunter Berndt", "authors": "Samuel-Hunter Berndt, Douglas Kirkpatrick, Timothy Taviano, Oleg\n  Komogortsev", "title": "Tertiary Eye Movement Classification by a Hybrid Algorithm", "comments": "10 pages, 18 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proper classification of major eye movements, saccades, fixations, and\nsmooth pursuits, remains essential to utilizing eye-tracking data. There is\ndifficulty in separating out smooth pursuits from the other behavior types,\nparticularly from fixations. To this end, we propose a new offline algorithm,\nI-VDT-HMM, for tertiary classification of eye movements. The algorithm combines\nthe simplicity of two foundational algorithms, I-VT and I-DT, as has been\nimplemented in I-VDT, with the statistical predictive power of the Viterbi\nalgorithm. We evaluate the fitness across a dataset of eight eye movement\nrecords at eight sampling rates gathered from previous research, with a\ncomparison to the current state-of-the-art using the proposed quantitative and\nqualitative behavioral scores. The proposed algorithm achieves promising\nresults in clean high sampling frequency data and with slight modifications\ncould show similar results with lower quality data. Though, the statistical\naspect of the algorithm comes at a cost of classification time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:51:55 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Berndt", "Samuel-Hunter", ""], ["Kirkpatrick", "Douglas", ""], ["Taviano", "Timothy", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "1904.10097", "submitter": "Rui Wang", "authors": "Rui Wang, Nan Yang, Joerg Stueckler, Daniel Cremers", "title": "DirectShape: Direct Photometric Alignment of Shape Priors for Visual\n  Vehicle Pose and Shape Estimation", "comments": "Accepted by IEEE International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding from images is a challenging problem encountered in\nautonomous driving. On the object level, while 2D methods have gradually\nevolved from computing simple bounding boxes to delivering finer grained\nresults like instance segmentations, the 3D family is still dominated by\nestimating 3D bounding boxes. In this paper, we propose a novel approach to\njointly infer the 3D rigid-body poses and shapes of vehicles from a stereo\nimage pair using shape priors. Unlike previous works that geometrically align\nshapes to point clouds from dense stereo reconstruction, our approach works\ndirectly on images by combining a photometric and a silhouette alignment term\nin the energy function. An adaptive sparse point selection scheme is proposed\nto efficiently measure the consistency with both terms. In experiments, we show\nsuperior performance of our method on 3D pose and shape estimation over the\nprevious geometric approach and demonstrate that our method can also be applied\nas a refinement step and significantly boost the performances of several\nstate-of-the-art deep learning based 3D object detectors. All related materials\nand demonstration videos are available at the project page\nhttps://vision.in.tum.de/research/vslam/direct-shape.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 23:58:25 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 20:50:08 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Wang", "Rui", ""], ["Yang", "Nan", ""], ["Stueckler", "Joerg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1904.10100", "submitter": "Weifeng Liu", "authors": "Weifeng Liu and Dacheng Tao", "title": "Multiview Hessian Regularization for Image Annotation", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 22, no. 7, pp. 2676 -\n  2687, 2013", "doi": "10.1109/TIP.2013.2255302", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of computer hardware and Internet technology makes\nlarge scale data dependent models computationally tractable, and opens a bright\navenue for annotating images through innovative machine learning algorithms.\nSemi-supervised learning (SSL) has consequently received intensive attention in\nrecent years and has been successfully deployed in image annotation. One\nrepresentative work in SSL is Laplacian regularization (LR), which smoothes the\nconditional distribution for classification along the manifold encoded in the\ngraph Laplacian, however, it has been observed that LR biases the\nclassification function towards a constant function which possibly results in\npoor generalization. In addition, LR is developed to handle uniformly\ndistributed data (or single view data), although instances or objects, such as\nimages and videos, are usually represented by multiview features, such as\ncolor, shape and texture. In this paper, we present multiview Hessian\nregularization (mHR) to address the above two problems in LR-based image\nannotation. In particular, mHR optimally combines multiple Hessian\nregularizations, each of which is obtained from a particular view of instances,\nand steers the classification function which varies linearly along the data\nmanifold. We apply mHR to kernel least squares and support vector machines as\ntwo examples for image annotation. Extensive experiments on the PASCAL VOC'07\ndataset validate the effectiveness of mHR by comparing it with baseline\nalgorithms, including LR and HR.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 00:08:43 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Liu", "Weifeng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.10117", "submitter": "Limin Wang", "authors": "Jianchao Wu, Limin Wang, Li Wang, Jie Guo, Gangshan Wu", "title": "Learning Actor Relation Graphs for Group Activity Recognition", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling relation between actors is important for recognizing group activity\nin a multi-person scene. This paper aims at learning discriminative relation\nbetween actors efficiently using deep models. To this end, we propose to build\na flexible and efficient Actor Relation Graph (ARG) to simultaneously capture\nthe appearance and position relation between actors. Thanks to the Graph\nConvolutional Network, the connections in ARG could be automatically learned\nfrom group activity videos in an end-to-end manner, and the inference on ARG\ncould be efficiently performed with standard matrix operations. Furthermore, in\npractice, we come up with two variants to sparsify ARG for more effective\nmodeling in videos: spatially localized ARG and temporal randomized ARG. We\nperform extensive experiments on two standard group activity recognition\ndatasets: the Volleyball dataset and the Collective Activity dataset, where\nstate-of-the-art performance is achieved on both datasets. We also visualize\nthe learned actor graphs and relation features, which demonstrate that the\nproposed ARG is able to capture the discriminative relation information for\ngroup activity recognition.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 01:44:28 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Wu", "Jianchao", ""], ["Wang", "Limin", ""], ["Wang", "Li", ""], ["Guo", "Jie", ""], ["Wu", "Gangshan", ""]]}, {"id": "1904.10126", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Boon Leong Lan, Wai Yee Chan, Kwan-Hoong Ng, Maxine\n  Tan", "title": "Lung Nodule Classification using Deep Local-Global Networks", "comments": "Code and dataset available here\n  https://github.com/mundher/local-global", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Lung nodules have very diverse shapes and sizes, which makes\nclassifying them as benign/malignant a challenging problem. In this paper, we\npropose a novel method to predict the malignancy of nodules that have the\ncapability to analyze the shape and size of a nodule using a global feature\nextractor, as well as the density and structure of the nodule using a local\nfeature extractor. Methods: We propose to use Residual Blocks with a 3x3 kernel\nsize for local feature extraction, and Non-Local Blocks to extract the global\nfeatures. The Non-Local Block has the ability to extract global features\nwithout using a huge number of parameters. The key idea behind the Non-Local\nBlock is to apply matrix multiplications between features on the same feature\nmaps. Results: We trained and validated the proposed method on the LIDC-IDRI\ndataset which contains 1,018 computed tomography (CT) scans. We followed a\nrigorous procedure for experimental setup namely, 10-fold cross-validation and\nignored the nodules that had been annotated by less than 3 radiologists. The\nproposed method achieved state-of-the-art results with AUC=95.62%, while\nsignificantly outperforming other baseline methods. Conclusions: Our proposed\nDeep Local-Global network has the capability to accurately extract both local\nand global features. Our new method outperforms state-of-the-art architecture\nincluding Densenet and Resnet with transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:49:37 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Lan", "Boon Leong", ""], ["Chan", "Wai Yee", ""], ["Ng", "Kwan-Hoong", ""], ["Tan", "Maxine", ""]]}, {"id": "1904.10128", "submitter": "Peng Gao", "authors": "Peng Gao, Ruyue Yuan, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang", "title": "Siamese Attentional Keypoint Network for High Performance Visual\n  Tracking", "comments": "Accepted by Knowledge-Based SYSTEMS", "journal-ref": null, "doi": "10.1016/j.knosys.2019.105448", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the impacts of three main aspects of visual\ntracking, i.e., the backbone network, the attentional mechanism, and the\ndetection component, and propose a Siamese Attentional Keypoint Network, dubbed\nSATIN, for efficient tracking and accurate localization. Firstly, a new Siamese\nlightweight hourglass network is specially designed for visual tracking. It\ntakes advantage of the benefits of the repeated bottom-up and top-down\ninference to capture more global and local contextual information at multiple\nscales. Secondly, a novel cross-attentional module is utilized to leverage both\nchannel-wise and spatial intermediate attentional information, which can\nenhance both discriminative and localization capabilities of feature maps.\nThirdly, a keypoints detection approach is invented to trace any target object\nby detecting the top-left corner point, the centroid point, and the\nbottom-right corner point of its bounding box. Therefore, our SATIN tracker not\nonly has a strong capability to learn more effective object representations,\nbut also is computational and memory storage efficiency, either during the\ntraining or testing stages. To the best of our knowledge, we are the first to\npropose this approach. Without bells and whistles, experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nrecent benchmark datasets, at a speed far exceeding 27 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:02:34 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 03:03:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gao", "Peng", ""], ["Yuan", "Ruyue", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""], ["Fujita", "Hamido", ""], ["Zhang", "Yan", ""]]}, {"id": "1904.10130", "submitter": "John Brandt", "authors": "John Brandt", "title": "Spatio-temporal crop classification of low-resolution satellite imagery\n  with capsule layers and distributed attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land use classification of low resolution spatial imagery is one of the most\nextensively researched fields in remote sensing. Despite significant\nadvancements in satellite technology, high resolution imagery lacks global\ncoverage and can be prohibitively expensive to procure for extended time\nperiods. Accurately classifying land use change without high resolution imagery\noffers the potential to monitor vital aspects of global development agenda\nincluding climate smart agriculture, drought resistant crops, and sustainable\nland management. Utilizing a combination of capsule layers and long-short term\nmemory layers with distributed attention, the present paper achieves\nstate-of-the-art accuracy on temporal crop type classification at a 30x30m\nresolution with Sentinel 2 imagery.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:05:31 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Brandt", "John", ""]]}, {"id": "1904.10151", "submitter": "Qi Wu", "authors": "Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang,\n  Chunhua Shen, and Anton van den Hengel", "title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the long-term challenges of robotics is to enable robots to interact\nwith humans in the visual world via natural language, as humans are visual\nanimals that communicate through language. Overcoming this challenge requires\nthe ability to perform a wide variety of complex tasks in response to\nmultifarious instructions from humans. In the hope that it might drive progress\ntowards more flexible and powerful human interactions with robots, we propose a\ndataset of varied and complex robot tasks, described in natural language, in\nterms of objects visible in a large set of real images. Given an instruction,\nsuccess requires navigating through a previously-unseen environment to identify\nan object. This represents a practical challenge, but one that closely reflects\none of the core visual problems in robotics. Several state-of-the-art\nvision-and-language navigation, and referring-expression models are tested to\nverify the difficulty of this new task, but none of them show promising results\nbecause there are many fundamental differences between our task and previous\nones. A novel Interactive Navigator-Pointer model is also proposed that\nprovides a strong baseline on the task. The proposed model especially achieves\nthe best performance on the unseen test split, but still leaves substantial\nroom for improvement compared to the human performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 04:45:28 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 01:38:43 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Qi", "Yuankai", ""], ["Wu", "Qi", ""], ["Anderson", "Peter", ""], ["Wang", "Xin", ""], ["Wang", "William Yang", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1904.10161", "submitter": "Feng Xue", "authors": "Feng Xue, Anlong Ming, Menghan Zhou and Yu Zhou", "title": "A Novel Multi-layer Framework for Tiny Obstacle Discovery", "comments": "Accepted to 2019 International Conference on Robotics and Automation\n  (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For tiny obstacle discovery in a monocular image, edge is a fundamental\nvisual element. Nevertheless, because of various reasons, e.g., noise and\nsimilar color distribution with background, it is still difficult to detect the\nedges of tiny obstacles at long distance. In this paper, we propose an\nobstacle-aware discovery method to recover the missing contours of these\nobstacles, which helps to obtain obstacle proposals as much as possible. First,\nby using visual cues in monocular images, several multi-layer regions are\nelaborately inferred to reveal the distances from the camera. Second, several\nnovel obstacle-aware occlusion edge maps are constructed to well capture the\ncontours of tiny obstacles, which combines cues from each layer. Third, to\nensure the existence of the tiny obstacle proposals, the maps from all layers\nare used for proposals extraction. Finally, based on these proposals containing\ntiny obstacles, a novel obstacle-aware regressor is proposed to generate an\nobstacle occupied probability map with high confidence. The convincing\nexperimental results with comparisons on the Lost and Found dataset demonstrate\nthe effectiveness of our approach, achieving around 9.5% improvement on the\naccuracy than FPHT and PHT, it even gets comparable performance to MergeNet.\nMoreover, our method outperforms the state-of-the-art algorithms and\nsignificantly improves the discovery ability for tiny obstacles at long\ndistance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:54:30 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 09:23:27 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 12:28:52 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Xue", "Feng", ""], ["Ming", "Anlong", ""], ["Zhou", "Menghan", ""], ["Zhou", "Yu", ""]]}, {"id": "1904.10165", "submitter": "Tao Li", "authors": "Tao Li, Jinwen Ma", "title": "T-SVD Based Non-convex Tensor Completion and Robust Principal Component\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion and robust principal component analysis have been widely\nused in machine learning while the key problem relies on the minimization of a\ntensor rank that is very challenging. A common way to tackle this difficulty is\nto approximate the tensor rank with the $\\ell_1-$norm of singular values based\non its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a\ntensor is also measured by its $\\ell_1-$norm. However, the $\\ell_1$ penalty is\nessentially biased and thus the result will deviate. In order to sidestep the\nbias, we propose a novel non-convex tensor rank surrogate function and a novel\nnon-convex sparsity measure. In this new setting by using the concavity instead\nof the convexity, a majorization minimization algorithm is further designed for\ntensor completion and robust principal component analysis. Furthermore, we\nanalyze its theoretical properties. Finally, the experiments on natural and\nhyperspectral images demonstrate the efficacy and efficiency of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:09:27 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 06:08:52 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Tao", ""], ["Ma", "Jinwen", ""]]}, {"id": "1904.10167", "submitter": "Jingwen Ye", "authors": "Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, Mingli Song", "title": "Student Becoming the Master: Knowledge Amalgamation for Joint Scene\n  Parsing, Depth Estimation, and More", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a novel deep-model reusing task. Our goal is to\ntrain a lightweight and versatile student model, without human-labelled\nannotations, that amalgamates the knowledge and masters the expertise of two\npretrained teacher models working on heterogeneous problems, one on scene\nparsing and the other on depth estimation. To this end, we propose an\ninnovative training strategy that learns the parameters of the student\nintertwined with the teachers, achieved by 'projecting' its amalgamated\nfeatures onto each teacher's domain and computing the loss. We also introduce\ntwo options to generalize the proposed training strategy to handle three or\nmore tasks simultaneously. The proposed scheme yields very encouraging results.\nAs demonstrated on several benchmarks, the trained student model achieves\nresults even superior to those of the teachers in their own expertise domains\nand on par with the state-of-the-art fully supervised models relying on\nhuman-labelled annotations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:12:58 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Ye", "Jingwen", ""], ["Ji", "Yixin", ""], ["Wang", "Xinchao", ""], ["Ou", "Kairi", ""], ["Tao", "Dapeng", ""], ["Song", "Mingli", ""]]}, {"id": "1904.10180", "submitter": "Sebastien Blandin", "authors": "Karthik Nandakumar, Sebastien Blandin, Laura Wynter", "title": "High-frequency crowd insights for public safety and congestion control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results from several projects aimed at enabling the real-time\nunderstanding of crowds and their behaviour in the built environment. We make\nuse of CCTV video cameras that are ubiquitous throughout the developed and\ndeveloping world and as such are able to play the role of a reliable sensing\nmechanism. We outline the novel methods developed for our crowd insights\nengine, and illustrate examples of its use in different contexts in the urban\nlandscape. Applications of the technology range from maintaining security in\npublic spaces to quantifying the adequacy of public transport level of service.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 07:11:48 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Nandakumar", "Karthik", ""], ["Blandin", "Sebastien", ""], ["Wynter", "Laura", ""]]}, {"id": "1904.10230", "submitter": "Jaehoon Cho", "authors": "Jaehoon Cho, Dongbo Min, Youngjung Kim, Kwanghoon Sohn", "title": "A Large RGB-D Dataset for Semi-supervised Monocular Depth Estimation", "comments": "Submitted to the IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advance of monocular depth estimation is largely based on deeply\nnested convolutional networks, combined with supervised training. However, it\nstill remains arduous to collect large-scale ground truth depth (or disparity)\nmaps for supervising the networks. This paper presents a simple yet effective\nsemi-supervised approach for monocular depth estimation. Inspired by the human\nvisual system, we propose a student-teacher strategy in which a shallow student\nnetwork is trained with the auxiliary information obtained from a deeper and\naccurate teacher network. Specifically, we first train the stereo teacher\nnetwork fully utilizing the binocular perception of 3D geometry, and then use\ndepth predictions of the teacher network for supervising the student network\nfor monocular depth inference. This enables us to exploit all available depth\ndata from massive unlabeled stereo pairs that are relatively easier-to-obtain.\nWe further introduce a data ensemble strategy that merges multiple depth\npredictions of the teacher network to improve the training samples for the\nstudent network. Additionally, stereo confidence maps are provided to avoid\ninaccurate depth estimates being used when supervising the student network. Our\nnew training data, consisting of 1 million outdoor stereo images taken using\nhand-held stereo cameras, is hosted at the project webpage. Lastly, we\ndemonstrate that the monocular depth estimation network provides feature\nrepresentations that are suitable for some high-level vision tasks such as\nsemantic segmentation and road detection. Extensive experiments demonstrate the\neffectiveness and flexibility of the proposed method in various outdoor\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 10:02:39 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Cho", "Jaehoon", ""], ["Min", "Dongbo", ""], ["Kim", "Youngjung", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1904.10235", "submitter": "R\\'emi Cogranne Dr.", "authors": "R\\'emi Cogranne, R\\'emi Slysz, Laurence Moreau, Houman Borouchaki", "title": "A new Edge Detector Based on Parametric Surface Model: Regression\n  Surface Descriptor", "comments": "21 pages, 13 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new methodology for edge detection in digital\nimages. The first originality of the proposed method is to consider image\ncontent as a parametric surface. Then, an original parametric local model of\nthis surface representing image content is proposed. The few parameters\ninvolved in the proposed model are shown to be very sensitive to\ndiscontinuities in surface which correspond to edges in image content. This\nnaturally leads to the design of an efficient edge detector. Moreover, a\nthorough analysis of the proposed model also allows us to explain how these\nparameters can be used to obtain edge descriptors such as orientations and\ncurvatures.\n  In practice, the proposed methodology offers two main advantages. First, it\nhas high customization possibilities in order to be adjusted to a wide range of\ndifferent problems, from coarse to fine scale edge detection. Second, it is\nvery robust to blurring process and additive noise. Numerical results are\npresented to emphasis these properties and to confirm efficiency of the\nproposed method through a comparative study with other edge detectors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 10:18:25 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Cogranne", "R\u00e9mi", ""], ["Slysz", "R\u00e9mi", ""], ["Moreau", "Laurence", ""], ["Borouchaki", "Houman", ""]]}, {"id": "1904.10247", "submitter": "Ya-Liang Chang", "authors": "Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee and Winston Hsu", "title": "Free-form Video Inpainting with 3D Gated Convolution and Temporal\n  PatchGAN", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-form video inpainting is a very challenging task that could be widely\nused for video editing such as text removal. Existing patch-based methods could\nnot handle non-repetitive structures such as faces, while directly applying\nimage-based inpainting models to videos will result in temporal inconsistency\n(see http://bit.ly/2Fu1n6b ). In this paper, we introduce a deep learn-ing\nbased free-form video inpainting model, with proposed 3D gated convolutions to\ntackle the uncertainty of free-form masks and a novel Temporal PatchGAN loss to\nenhance temporal consistency. In addition, we collect videos and design a\nfree-form mask generation algorithm to build the free-form video inpainting\n(FVI) dataset for training and evaluation of video inpainting models. We\ndemonstrate the benefits of these components and experiments on both the\nFaceForensics and our FVI dataset suggest that our method is superior to\nexisting ones. Related source code, full-resolution result videos and the FVI\ndataset could be found on Github\nhttps://github.com/amjltc295/Free-Form-Video-Inpainting .\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 11:14:18 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 05:04:02 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 05:53:03 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chang", "Ya-Liang", ""], ["Liu", "Zhe Yu", ""], ["Lee", "Kuan-Ying", ""], ["Hsu", "Winston", ""]]}, {"id": "1904.10255", "submitter": "Ahmed Imtiaz Humayun", "authors": "Ahmed Imtiaz Humayun, Asif Shahriyar Sushmit, Taufiq Hasan and\n  Mohammed Imamul Hassan Bhuiyan", "title": "End-to-end Sleep Staging with Raw Single Channel EEG using Deep Residual\n  ConvNets", "comments": "5 pages, 3 Figures, Appendix, IEEE BHI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans approximately spend a third of their life sleeping, which makes\nmonitoring sleep an integral part of well-being. In this paper, a 34-layer deep\nresidual ConvNet architecture for end-to-end sleep staging is proposed. The\nnetwork takes raw single channel electroencephalogram (Fpz-Cz) signal as input\nand yields hypnogram annotations for each 30s segments as output. Experiments\nare carried out for two different scoring standards (5 and 6 stage\nclassification) on the expanded PhysioNet Sleep-EDF dataset, which contains\nmulti-source data from hospital and household polysomnography setups. The\nperformance of the proposed network is compared with that of the\nstate-of-the-art algorithms in patient independent validation tasks. The\nexperimental results demonstrate the superiority of the proposed network\ncompared to the best existing method, providing a relative improvement in\nepoch-wise average accuracy of 6.8% and 6.3% on the household data and\nmulti-source data, respectively. Codes are made publicly available on Github.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 11:32:46 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Humayun", "Ahmed Imtiaz", ""], ["Sushmit", "Asif Shahriyar", ""], ["Hasan", "Taufiq", ""], ["Bhuiyan", "Mohammed Imamul Hassan", ""]]}, {"id": "1904.10261", "submitter": "Aleksander Lukashou", "authors": "Aleksander Lukashou", "title": "Improving benchmarks for autonomous vehicles testing using synthetically\n  generated images", "comments": "4 pages, 38 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays autonomous technologies are a very heavily explored area and\nparticularly computer vision as the main component of vehicle perception. The\nquality of the whole vision system based on neural networks relies on the\ndataset it was trained on. It is extremely difficult to find traffic sign\ndatasets from most of the counties of the world. Meaning autonomous vehicle\nfrom the USA will not be able to drive though Lithuania recognizing all road\nsigns on the way. In this paper, we propose a solution on how to update model\nusing a small dataset from the country vehicle will be used in. It is important\nto mention that is not panacea, rather small upgrade which can boost autonomous\ncar development in countries with limited data access. We achieved about 10\npercent quality raise and expect even better results during future experiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 11:59:36 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Lukashou", "Aleksander", ""]]}, {"id": "1904.10293", "submitter": "Dong Gong", "authors": "Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua\n  Shen, Ian Reid, Yanning Zhang", "title": "Attention-guided Network for Ghost-free High Dynamic Range Imaging", "comments": "Accepted to appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ghosting artifacts caused by moving objects or misalignments is a key\nchallenge in high dynamic range (HDR) imaging for dynamic scenes. Previous\nmethods first register the input low dynamic range (LDR) images using optical\nflow before merging them, which are error-prone and cause ghosts in results. A\nvery recent work tries to bypass optical flows via a deep network with\nskip-connections, however, which still suffers from ghosting artifacts for\nsevere movement. To avoid the ghosting from the source, we propose a novel\nattention-guided end-to-end deep neural network (AHDRNet) to produce\nhigh-quality ghost-free HDR images. Unlike previous methods directly stacking\nthe LDR images or features for merging, we use attention modules to guide the\nmerging according to the reference image. The attention modules automatically\nsuppress undesired components caused by misalignments and saturation and\nenhance desirable fine details in the non-reference images. In addition to the\nattention model, we use dilated residual dense block (DRDB) to make full use of\nthe hierarchical features and increase the receptive field for hallucinating\nthe missing details. The proposed AHDRNet is a non-flow-based method, which can\nalso avoid the artifacts generated by optical-flow estimation error.\nExperiments on different datasets show that the proposed AHDRNet can achieve\nstate-of-the-art quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:04:58 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Yan", "Qingsen", ""], ["Gong", "Dong", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Zhang", "Yanning", ""]]}, {"id": "1904.10300", "submitter": "Yew Siang Tang", "authors": "Yew Siang Tang, Gim Hee Lee", "title": "Transferable Semi-supervised 3D Object Detection from RGB-D Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the direction of training a 3D object detector for new object\nclasses from only 2D bounding box labels of these new classes, while\nsimultaneously transferring information from 3D bounding box labels of the\nexisting classes. To this end, we propose a transferable semi-supervised 3D\nobject detection model that learns a 3D object detector network from training\ndata with two disjoint sets of object classes - a set of strong classes with\nboth 2D and 3D box labels, and another set of weak classes with only 2D box\nlabels. In particular, we suggest a relaxed reprojection loss, box prior loss\nand a Box-to-Point Cloud Fit network that allow us to effectively transfer\nuseful 3D information from the strong classes to the weak classes during\ntraining, and consequently, enable the network to detect 3D objects in the weak\nclasses during inference. Experimental results show that our proposed algorithm\noutperforms baseline approaches and achieves promising results compared to\nfully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we\nshow that our Box-to-Point Cloud Fit network improves performances of the\nfully-supervised approaches on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:24:15 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Tang", "Yew Siang", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1904.10324", "submitter": "Masashi Yokozuka", "authors": "Masashi Yokozuka, Shuji Oishi, Thompson Simon, Atsuhiko Banno", "title": "VITAMIN-E: VIsual Tracking And MappINg with Extremely Dense Feature\n  Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel indirect monocular SLAM algorithm called\n\"VITAMIN-E,\" which is highly accurate and robust as a result of tracking\nextremely dense feature points. Typical indirect methods have difficulty in\nreconstructing dense geometry because of their careful feature point selection\nfor accurate matching. Unlike conventional methods, the proposed method\nprocesses an enormous number of feature points by tracking the local extrema of\ncurvature informed by dominant flow estimation. Because this may lead to high\ncomputational cost during bundle adjustment, we propose a novel optimization\ntechnique, the \"subspace Gauss--Newton method\", that significantly improves the\ncomputational efficiency of bundle adjustment by partially updating the\nvariables. We concurrently generate meshes from the reconstructed points and\nmerge them for an entire 3D model. The experimental results on the SLAM\nbenchmark dataset EuRoC demonstrated that the proposed method outperformed\nstate-of-the-art SLAM methods, such as DSO, ORB-SLAM, and LSD-SLAM, both in\nterms of accuracy and robustness in trajectory estimation. The proposed method\nsimultaneously generated significantly detailed 3D geometry from the dense\nfeature points in real time using only a CPU.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:39:10 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 09:09:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yokozuka", "Masashi", ""], ["Oishi", "Shuji", ""], ["Simon", "Thompson", ""], ["Banno", "Atsuhiko", ""]]}, {"id": "1904.10327", "submitter": "Marzieh Gheisari", "authors": "Marzieh Gheisari, Teddy Furon, Laurent Amsaleg", "title": "Privacy Preserving Group Membership Verification and Identification", "comments": "Accepted at CVPR Workshops 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When convoking privacy, group membership verification checks if a biometric\ntrait corresponds to one member of a group without revealing the identity of\nthat member. Similarly, group membership identification states which group the\nindividual belongs to, without knowing his/her identity. A recent contribution\nprovides privacy and security for group membership protocols through the joint\nuse of two mechanisms: quantizing biometric templates into discrete embeddings\nand aggregating several templates into one group representation. This paper\nsignificantly improves that contribution because it jointly learns how to embed\nand aggregate instead of imposing fixed and hard coded rules. This is\ndemonstrated by exposing the mathematical underpinnings of the learning stage\nbefore showing the improvements through an extensive series of experiments\ntargeting face recognition. Overall, experiments show that learning yields an\nexcellent trade-off between security /privacy and verification /identification\nperformances.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:41:14 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Gheisari", "Marzieh", ""], ["Furon", "Teddy", ""], ["Amsaleg", "Laurent", ""]]}, {"id": "1904.10343", "submitter": "Ke Yu", "authors": "Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, Chen Change Loy", "title": "Path-Restore: Learning Network Path Selection for Image Restoration", "comments": "IEEE TPAMI 2021. Project page:\n  https://www.mmlab-ntu.com/project/pathrestore/", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3096255", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep Convolutional Neural Networks (CNNs) have greatly improved the\nperformance on various image restoration tasks. However, this comes at a price\nof increasing computational burden, hence limiting their practical usages. We\nobserve that some corrupted image regions are inherently easier to restore than\nothers since the distortion and content vary within an image. To leverage this,\nwe propose Path-Restore, a multi-path CNN with a pathfinder that can\ndynamically select an appropriate route for each image region. We train the\npathfinder using reinforcement learning with a difficulty-regulated reward.\nThis reward is related to the performance, complexity and \"the difficulty of\nrestoring a region\". A policy mask is further investigated to jointly process\nall the image regions. We conduct experiments on denoising and mixed\nrestoration tasks. The results show that our method achieves comparable or\nsuperior performance to existing approaches with less computational cost. In\nparticular, Path-Restore is effective for real-world denoising, where the noise\ndistribution varies across different regions on a single image. Compared to the\nstate-of-the-art RIDNet, our method achieves comparable performance and runs\n2.7x faster on the realistic Darmstadt Noise Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:07:11 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 04:05:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Yu", "Ke", ""], ["Wang", "Xintao", ""], ["Dong", "Chao", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1904.10348", "submitter": "Yann Labb\\'e", "authors": "Yann Labb\\'e, Sergey Zagoruyko, Igor Kalevatykh, Ivan Laptev, Justin\n  Carpentier, Mathieu Aubry, Josef Sivic", "title": "Monte-Carlo Tree Search for Efficient Visually Guided Rearrangement\n  Planning", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of visually guided rearrangement planning with many\nmovable objects, i.e., finding a sequence of actions to move a set of objects\nfrom an initial arrangement to a desired one, while relying on visual inputs\ncoming from an RGB camera. To do so, we introduce a complete pipeline relying\non two key contributions. First, we introduce an efficient and scalable\nrearrangement planning method, based on a Monte-Carlo Tree Search exploration\nstrategy. We demonstrate that because of its good trade-off between exploration\nand exploitation our method (i) scales well with the number of objects while\n(ii) finding solutions which require a smaller number of moves compared to the\nother state-of-the-art approaches. Note that on the contrary to many\napproaches, we do not require any buffer space to be available. Second, to\nprecisely localize movable objects in the scene, we develop an integrated\napproach for robust multi-object workspace state estimation from a single\nuncalibrated RGB camera using a deep neural network trained only with synthetic\ndata. We validate our multi-object visually guided manipulation pipeline with\nseveral experiments on a real UR-5 robotic arm by solving various rearrangement\nplanning instances, requiring only 60 ms to compute the plan to rearrange 25\nobjects. In addition, we show that our system is insensitive to camera\nmovements and can successfully recover from external perturbations.\nSupplementary video, source code and pre-trained models are available at\nhttps://ylabbe.github.io/rearrangement-planning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:15:37 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 16:11:27 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Labb\u00e9", "Yann", ""], ["Zagoruyko", "Sergey", ""], ["Kalevatykh", "Igor", ""], ["Laptev", "Ivan", ""], ["Carpentier", "Justin", ""], ["Aubry", "Mathieu", ""], ["Sivic", "Josef", ""]]}, {"id": "1904.10351", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Nishant Kumar, Pankaj Kumar Sahu, Venkanna\n  Udutalapally", "title": "Drishtikon: An advanced navigational aid system for visually impaired\n  people", "comments": "Pre-print of the presented article at IEEE Conference on Information\n  and Communication Technology (CICT-2018), 6 Pages, 7 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, many of the aid systems deployed for visually impaired people are\nmostly made for a single purpose. Be it navigation, object detection, or\ndistance perceiving. Also, most of the deployed aid systems use indoor\nnavigation which requires a pre-knowledge of the environment. These aid systems\noften fail to help visually impaired people in the unfamiliar scenario. In this\npaper, we propose an aid system developed using object detection and depth\nperceivement to navigate a person without dashing into an object. The prototype\ndeveloped detects 90 different types of objects and compute their distances\nfrom the user. We also, implemented a navigation feature to get input from the\nuser about the target destination and hence, navigate the impaired person to\nhis/her destination using Google Directions API. With this system, we built a\nmulti-feature, high accuracy navigational aid system which can be deployed in\nthe wild and help the visually impaired people in their daily life by\nnavigating them effortlessly to their desired destination.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:23:49 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Kotyan", "Shashank", ""], ["Kumar", "Nishant", ""], ["Sahu", "Pankaj Kumar", ""], ["Udutalapally", "Venkanna", ""]]}, {"id": "1904.10354", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Nishant Kumar, Pankaj Kumar Sahu, Venkanna\n  Udutalapally", "title": "HAUAR: Home Automation Using Action Recognition", "comments": "Pre-print of the presented article at IEEE Conference on Information\n  and Communication Technology (CICT-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, many of the home automation systems deployed are mostly controlled by\nhumans. This control by humans restricts the automation of home appliances to\nan extent. Also, most of the deployed home automation systems use the Internet\nof Things technology to control the appliances. In this paper, we propose a\nsystem developed using action recognition to fully automate the home\nappliances. We recognize the three actions of a person (sitting, standing and\nlying) along with the recognition of an empty room. The accuracy of the system\nwas 90% in the real-life test experiments. With this system, we remove the\nhuman intervention in home automation systems for controlling the home\nappliances and at the same time we ensure the data privacy and reduce the\nenergy consumption by efficiently and optimally using home appliances.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:26:18 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 04:50:47 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kotyan", "Shashank", ""], ["Kumar", "Nishant", ""], ["Sahu", "Pankaj Kumar", ""], ["Udutalapally", "Venkanna", ""]]}, {"id": "1904.10379", "submitter": "Eran Treister", "authors": "Moshe Eliasof, Andrei Sharf, Eran Treister", "title": "Multi-modal 3D Shape Reconstruction Under Calibration Uncertainty using\n  Parametric Level Set Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of 3D shape reconstruction from multi-modal data,\ngiven uncertain calibration parameters. Typically, 3D data modalities can be in\ndiverse forms such as sparse point sets, volumetric slices, 2D photos and so\non. To jointly process these data modalities, we exploit a parametric level set\nmethod that utilizes ellipsoidal radial basis functions. This method not only\nallows us to analytically and compactly represent the object, it also confers\non us the ability to overcome calibration related noise that originates from\ninaccurate acquisition parameters. This essentially implicit regularization\nleads to a highly robust and scalable reconstruction, surpassing other\ntraditional methods. In our results we first demonstrate the ability of the\nmethod to compactly represent complex objects. We then show that our\nreconstruction method is robust both to a small number of measurements and to\nnoise in the acquisition parameters. Finally, we demonstrate our reconstruction\nabilities from diverse modalities such as volume slices obtained from liquid\ndisplacement (similar to CTscans and XRays), and visual measurements obtained\nfrom shape silhouettes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:19:39 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 12:52:17 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Eliasof", "Moshe", ""], ["Sharf", "Andrei", ""], ["Treister", "Eran", ""]]}, {"id": "1904.10390", "submitter": "Kostiantyn Khabarlak", "authors": "Kostiantyn Khabarlak, Larysa Koriashkina", "title": "Minimizing Perceived Image Quality Loss Through Adversarial Attack\n  Scoping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are now actively being used for computer vision tasks in\nsecurity critical areas such as robotics, face recognition, autonomous vehicles\nyet their safety is under question after the discovery of adversarial attacks.\nIn this paper we develop simplified adversarial attack algorithms based on a\nscoping idea, which enables execution of fast adversarial attacks that minimize\nstructural image quality (SSIM) loss, allows performing efficient transfer\nattacks with low target inference network call count and opens a possibility of\nan attack using pen-only drawings on a paper for the MNIST handwritten digit\ndataset. The presented adversarial attack analysis and the idea of attack\nscoping can be easily expanded to different datasets, thus making the paper's\nresults applicable to a wide range of practical tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:42:00 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Khabarlak", "Kostiantyn", ""], ["Koriashkina", "Larysa", ""]]}, {"id": "1904.10411", "submitter": "Bolun Cai", "authors": "Bolun Cai, Xiangmin Xu, Xiaofen Xing, Kui Jia, Jie Miao, Dacheng Tao", "title": "BIT: Biologically Inspired Tracker", "comments": null, "journal-ref": "IEEE Trans. on Image Proc. 25 (2016) 1327-1339", "doi": "10.1109/TIP.2016.2520358", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is challenging due to image variations caused by various\nfactors, such as object deformation, scale change, illumination change and\nocclusion. Given the superior tracking performance of human visual system\n(HVS), an ideal design of biologically inspired model is expected to improve\ncomputer visual tracking. This is however a difficult task due to the\nincomplete understanding of neurons' working mechanism in HVS. This paper aims\nto address this challenge based on the analysis of visual cognitive mechanism\nof the ventral stream in the visual cortex, which simulates shallow neurons (S1\nunits and C1 units) to extract low-level biologically inspired features for the\ntarget appearance and imitates an advanced learning mechanism (S2 units and C2\nunits) to combine generative and discriminative models for target location. In\naddition, fast Gabor approximation (FGA) and fast Fourier transform (FFT) are\nadopted for real-time learning and detection in this framework. Extensive\nexperiments on large-scale benchmark datasets show that the proposed\nbiologically inspired tracker performs favorably against state-of-the-art\nmethods in terms of efficiency, accuracy, and robustness. The acceleration\ntechnique in particular ensures that BIT maintains a speed of approximately 45\nframes per second.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:32:24 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Cai", "Bolun", ""], ["Xu", "Xiangmin", ""], ["Xing", "Xiaofen", ""], ["Jia", "Kui", ""], ["Miao", "Jie", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.10424", "submitter": "Shengcai Liao", "authors": "Shengcai Liao and Ling Shao", "title": "Interpretable and Generalizable Person Re-Identification with\n  Query-Adaptive Convolution and Temporal Lifting", "comments": "This is the ECCV 2020 version, including the appendix", "journal-ref": "Vedaldi A., Bischof H., Brox T., Frahm JM. (eds). European\n  Conference on Computer Vision. ECCV 2020. Lecture Notes in Computer Science,\n  vol 12356. Springer, Cham", "doi": "10.1007/978-3-030-58621-8_27", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For person re-identification, existing deep networks often focus on\nrepresentation learning. However, without transfer learning, the learned model\nis fixed as is, which is not adaptable for handling various unseen scenarios.\nIn this paper, beyond representation learning, we consider how to formulate\nperson image matching directly in deep feature maps. We treat image matching as\nfinding local correspondences in feature maps, and construct query-adaptive\nconvolution kernels on the fly to achieve local matching. In this way, the\nmatching process and results are interpretable, and this explicit matching is\nmore generalizable than representation features to unseen scenarios, such as\nunknown misalignments, pose or viewpoint changes. To facilitate end-to-end\ntraining of this architecture, we further build a class memory module to cache\nfeature maps of the most recent samples of each class, so as to compute image\nmatching losses for metric learning. Through direct cross-dataset evaluation,\nthe proposed Query-Adaptive Convolution (QAConv) method gains large\nimprovements over popular learning methods (about 10%+ mAP), and achieves\ncomparable results to many transfer learning methods. Besides, a model-free\ntemporal cooccurrence based score weighting method called TLift is proposed,\nwhich improves the performance to a further extent, achieving state-of-the-art\nresults in cross-dataset person re-identification. Code is available at\nhttps://github.com/ShengcaiLiao/QAConv.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 17:03:13 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 11:43:28 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 17:25:54 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 07:36:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "1904.10429", "submitter": "Nishad Rajmalwar", "authors": "Zoheb Abai, Nishad Rajmalwar", "title": "DenseNet Models for Tiny ImageNet Classification", "comments": "7 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present two image classification models on the Tiny\nImageNet dataset. We built two very different networks from scratch based on\nthe idea of Densely Connected Convolution Networks. The architecture of the\nnetworks is designed based on the image resolution of this specific dataset and\nby calculating the Receptive Field of the convolution layers. We also used some\nnon-conventional techniques related to image augmentation and Cyclical Learning\nRate to improve the accuracy of our models. The networks are trained under high\nconstraints and low computation resources. We aimed to achieve top-1 validation\naccuracy of 60%; the results and error analysis are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 17:20:35 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 11:40:40 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Abai", "Zoheb", ""], ["Rajmalwar", "Nishad", ""]]}, {"id": "1904.10489", "submitter": "Jingpeng Wu", "authors": "Jingpeng Wu, William M. Silversmith, Kisuk Lee, H. Sebastian Seung", "title": "Chunkflow: Distributed Hybrid Cloud Processing of Large 3D Images by\n  Convolutional Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now common to process volumetric biomedical images using 3D\nConvolutional Networks (ConvNets). This can be challenging for the teravoxel\nand even petavoxel images that are being acquired today by light or electron\nmicroscopy. Here we introduce chunkflow, a software framework for distributing\nConvNet processing over local and cloud GPUs and CPUs. The image volume is\ndivided into overlapping chunks, each chunk is processed by a ConvNet, and the\nresults are blended together to yield the output image. The frontend submits\nConvNet tasks to a cloud queue. The tasks are executed by local and cloud GPUs\nand CPUs. Thanks to the fault-tolerant architecture of Chunkflow, cost can be\ngreatly reduced by utilizing cheap unstable cloud instances. Chunkflow\ncurrently supports PyTorch for GPUs and PZnet for CPUs. To illustrate its\nusage, a large 3D brain image from serial section electron microscopy was\nprocessed by a 3D ConvNet with a U-Net style architecture. Chunkflow provides\nsome chunk operations for general use, and the operations can be composed\nflexibly in a command line interface.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 18:47:57 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 14:20:52 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 13:48:34 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Wu", "Jingpeng", ""], ["Silversmith", "William M.", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1904.10499", "submitter": "Mar\\'ia Juliana Gambini", "authors": "Alejandro C. Frery, Juliana Gambini", "title": "Comparing Samples from the $\\mathcal{G}^0$ Distribution using a Geodesic\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathcal{G}^0$ distribution is widely used for monopolarized SAR image\nmodeling because it can characterize regions with different degree of texture\naccurately. It is indexed by three parameters: the number of looks (which can\nbe estimated for the whole image), a scale parameter and a texture parameter.\nThis paper presents a new proposal for comparing samples from the\n$\\mathcal{G}^0$ distribution using a Geodesic Distance (GD) as a measure of\ndissimilarity between models. The objective is quantifying the difference\nbetween pairs of samples from SAR data using both local parameters (scale and\ntexture) of the $\\mathcal{G}^0$ distribution. We propose three tests based on\nthe GD which combine the tests presented in~\\cite{GeodesicDistanceGI0JSTARS},\nand we estimate their probability distributions using permutation methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:13:29 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Frery", "Alejandro C.", ""], ["Gambini", "Juliana", ""]]}, {"id": "1904.10504", "submitter": "Li Chen", "authors": "Li Chen", "title": "Understanding the efficacy, reliability and resiliency of computer\n  vision techniques for malware detection and future research directions", "comments": "Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  My research lies in the intersection of security and machine learning. This\noverview summarizes one component of my research: combining computer vision\nwith malware exploit detection for enhanced security solutions. I will present\nthe perspectives of efficacy, reliability and resiliency to formulate threat\ndetection as computer vision problems and develop state-of-the-art image-based\nmalware classification. Representing malware binary as images provides a direct\nvisualization of data samples, reduces the efforts for feature extraction, and\nconsumes the whole binary for holistic structural analysis. Employing transfer\nlearning of deep neural networks effective for large scale image classification\nto malware classification demonstrates superior classification efficacy\ncompared with classical machine learning algorithms. To enhance reliability of\nthese vision-based malware detectors, interpretation frameworks can be\nconstructed on the malware visual representations and useful for extracting\nfaithful explanation, so that security practitioners have confidence in the\nmodel before deployment. In cyber-security applications, we should always\nassume that a malware writer constantly modifies code to bypass detection.\nAddressing the resiliency of the malware detectors is equivalently important as\nefficacy and reliability. Via understanding the attack surfaces of machine\nlearning models used for malware detection, we can greatly improve the\nrobustness of the algorithms to combat malware adversaries in the wild. Finally\nI will discuss future research directions worth pursuing in this research\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:34:20 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Li", ""]]}, {"id": "1904.10506", "submitter": "Sen Wang", "authors": "Hao Zhu and Xinxin Zuo and Sen Wang and Xun Cao and Ruigang Yang", "title": "Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh\n  Deformation", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework to recover detailed human body shapes\nfrom a single image. It is a challenging task due to factors such as variations\nin human shapes, body poses, and viewpoints. Prior methods typically attempt to\nrecover the human body shape using a parametric based template that lacks the\nsurface details. As such the resulting body shape appears to be without\nclothing. In this paper, we propose a novel learning-based framework that\ncombines the robustness of parametric model with the flexibility of free-form\n3D deformation. We use the deep neural networks to refine the 3D shape in a\nHierarchical Mesh Deformation (HMD) framework, utilizing the constraints from\nbody joints, silhouettes, and per-pixel shading information. We are able to\nrestore detailed human body shapes beyond skinned models. Experiments\ndemonstrate that our method has outperformed previous state-of-the-art\napproaches, achieving better accuracy in terms of both 2D IoU number and 3D\nmetric distance. The code is available in https://github.com/zhuhao-nju/hmd.git\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:48:17 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 23:11:30 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhu", "Hao", ""], ["Zuo", "Xinxin", ""], ["Wang", "Sen", ""], ["Cao", "Xun", ""], ["Yang", "Ruigang", ""]]}, {"id": "1904.10596", "submitter": "Pan Ji", "authors": "Tong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, Hongdong Li", "title": "Neural Collaborative Subspace Clustering", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Neural Collaborative Subspace Clustering, a neural model\nthat discovers clusters of data points drawn from a union of low-dimensional\nsubspaces. In contrast to previous attempts, our model runs without the aid of\nspectral clustering. This makes our algorithm one of the kinds that can\ngracefully scale to large datasets. At its heart, our neural model benefits\nfrom a classifier which determines whether a pair of points lies on the same\nsubspace or not. Essential to our model is the construction of two affinity\nmatrices, one from the classifier and the other from a notion of subspace\nself-expressiveness, to supervise training in a collaborative scheme. We\nthoroughly assess and contrast the performance of our model against various\nstate-of-the-art clustering algorithms including deep subspace-based ones.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 01:29:17 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhang", "Tong", ""], ["Ji", "Pan", ""], ["Harandi", "Mehrtash", ""], ["Huang", "Wenbing", ""], ["Li", "Hongdong", ""]]}, {"id": "1904.10613", "submitter": "Wei-Na Li", "authors": "Wei-Na Li, Zhengyun Zhang, Jianshe Ma, Xiaohao Wang, and Ping Su", "title": "Defocused images removal of axial overlapping scattering particles by\n  using three-dimensional nonlinear diffusion based on digital holography", "comments": "no", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a three-dimensional nonlinear diffusion method to implement the\nsimilar autofocusing function of multiple micro-objects and simultaneously\nremove the defocused images, which can distinguish the locations of certain\nsized scattering particles that are overlapping along z-axis. It is applied to\nall of the reconstruction slices that are generated from the captured hologram\nafter each back propagation. For certain small sized particles, the maxima of\nmaximum gradient magnitude of each reconstruction slice appears at the ground\ntruth z position after applying the proposed scheme when the reconstruction\nrange along z-axis is sufficiently long and the reconstruction depth spacing is\nsufficiently fine. Therefore, the reconstructed image at ground truth z\nposition is remained, while the defocused images are diffused out. The results\ndemonstrated that the proposed scheme can diffuse out the defocused images\nwhich are 20 um away from the ground truth z position in spite of that several\nscattering particles with different diameters are completely overlapping along\nz-axis with a distance of 800 um when the hologram pixel pitch is 2 um. It also\ndemonstrated that the sparsity distribution of the ground truth z slice cannot\nbe affected by the sparsity distribution of corresponding defocused images when\nthe diameter of the particle is not more than 35um and the reconstruction depth\nspacing is not less than 20 um.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:43:03 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 09:49:18 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 09:43:37 GMT"}, {"version": "v4", "created": "Wed, 14 Aug 2019 08:53:41 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Li", "Wei-Na", ""], ["Zhang", "Zhengyun", ""], ["Ma", "Jianshe", ""], ["Wang", "Xiaohao", ""], ["Su", "Ping", ""]]}, {"id": "1904.10615", "submitter": "Noa Garcia", "authors": "Noa Garcia, Benjamin Renoust, Yuta Nakashima", "title": "Understanding Art through Multi-Modal Retrieval in Paintings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, visual arts are often studied from a purely aesthetics\nperspective, mostly by analysing the visual appearance of an artistic\nreproduction to infer its style, its author, or its representative features. In\nthis work, however, we explore art from both a visual and a language\nperspective. Our aim is to bridge the gap between the visual appearance of an\nartwork and its underlying meaning, by jointly analysing its aesthetics and its\nsemantics. We introduce the use of multi-modal techniques in the field of\nautomatic art analysis by 1) collecting a multi-modal dataset with fine-art\npaintings and comments, and 2) exploring robust visual and textual\nrepresentations in artistic images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:45:18 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Garcia", "Noa", ""], ["Renoust", "Benjamin", ""], ["Nakashima", "Yuta", ""]]}, {"id": "1904.10619", "submitter": "Hongzhu Li", "authors": "Hongzhu Li, Weiqiang Wang", "title": "Reinterpreting CTC training as iterative fitting", "comments": "to be published in Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107392", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connectionist temporal classification (CTC) enables end-to-end sequence\nlearning by maximizing the probability of correctly recognizing sequences\nduring training. The outputs of a CTC-trained model tend to form a series of\nspikes separated by strongly predicted blanks, know as the spiky problem. To\nfigure out the reason for it, we reinterpret the CTC training process as an\niterative fitting task that is based on frame-wise cross-entropy loss. It\noffers us an intuitive way to compare target probabilities with model outputs\nfor each iteration, and explain how the model outputs gradually turns spiky.\nInspired by it, we put forward two ways to modify the CTC training. The\nexperiments demonstrate that our method can well solve the spiky problem and\nmoreover, lead to faster convergence over various training settings. Beside\nthis, the reinterpretation of CTC, as a brand new perspective, may be\npotentially useful in other situations. The code is publicly available at\nhttps://github.com/hzli-ucas/caffe/tree/ctc.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:50:29 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:18:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Li", "Hongzhu", ""], ["Wang", "Weiqiang", ""]]}, {"id": "1904.10620", "submitter": "Yunsheng Li", "authors": "Yunsheng Li, Lu Yuan, Nuno Vasconcelos", "title": "Bidirectional Learning for Domain Adaptation of Semantic Segmentation", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation for semantic image segmentation is very necessary since\nmanually labeling large datasets with pixel-level labels is expensive and time\nconsuming. Existing domain adaptation techniques either work on limited\ndatasets, or yield not so good performance compared with supervised learning.\nIn this paper, we propose a novel bidirectional learning framework for domain\nadaptation of segmentation. Using the bidirectional learning, the image\ntranslation model and the segmentation adaptation model can be learned\nalternatively and promote to each other. Furthermore, we propose a\nself-supervised learning algorithm to learn a better segmentation adaptation\nmodel and in return improve the image translation model. Experiments show that\nour method is superior to the state-of-the-art methods in domain adaptation of\nsegmentation with a big margin. The source code is available at\nhttps://github.com/liyunsheng13/BDL.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:00:22 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Li", "Yunsheng", ""], ["Yuan", "Lu", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1904.10626", "submitter": "Yutao Ma", "authors": "Hao Sun, Xianxu Zeng, Tao Xu, Gang Peng, and Yutao Ma", "title": "Computer-aided diagnosis in histopathological images of the endometrium\n  using a convolutional neural network and attention mechanisms", "comments": "22 pages, 8 figures, and 4 tables", "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 2020, 26(6):\n  1664-1676", "doi": "10.1109/JBHI.2019.2944977", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uterine cancer, also known as endometrial cancer, can seriously affect the\nfemale reproductive organs, and histopathological image analysis is the gold\nstandard for diagnosing endometrial cancer. However, due to the limited\ncapability of modeling the complicated relationships between histopathological\nimages and their interpretations, these computer-aided diagnosis (CADx)\napproaches based on traditional machine learning algorithms often failed to\nachieve satisfying results. In this study, we developed a CADx approach using a\nconvolutional neural network (CNN) and attention mechanisms, called HIENet.\nBecause HIENet used the attention mechanisms and feature map visualization\ntechniques, it can provide pathologists better interpretability of diagnoses by\nhighlighting the histopathological correlations of local (pixel-level) image\nfeatures to morphological characteristics of endometrial tissue. In the\nten-fold cross-validation process, the CADx approach, HIENet, achieved a 76.91\n$\\pm$ 1.17% (mean $\\pm$ s. d.) classification accuracy for four classes of\nendometrial tissue, namely normal endometrium, endometrial polyp, endometrial\nhyperplasia, and endometrial adenocarcinoma. Also, HIENet achieved an\narea-under-the-curve (AUC) of 0.9579 $\\pm$ 0.0103 with an 81.04 $\\pm$ 3.87%\nsensitivity and 94.78 $\\pm$ 0.87% specificity in a binary classification task\nthat detected endometrioid adenocarcinoma (Malignant). Besides, in the external\nvalidation process, HIENet achieved an 84.50% accuracy in the four-class\nclassification task, and it achieved an AUC of 0.9829 with a 77.97% (95% CI,\n65.27%-87.71%) sensitivity and 100% (95% CI, 97.42%-100.00%) specificity. In\nsummary, the proposed CADx approach, HIENet, outperformed three human experts\nand four end-to-end CNN-based classifiers on this small-scale dataset composed\nof 3,500 hematoxylin and eosin (H&E) images regarding overall classification\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:29:12 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sun", "Hao", ""], ["Zeng", "Xianxu", ""], ["Xu", "Tao", ""], ["Peng", "Gang", ""], ["Ma", "Yutao", ""]]}, {"id": "1904.10633", "submitter": "Dezhong Xu", "authors": "Yonghao He and Dezhong Xu, Lifang Wu, Meng Jian, Shiming Xiang,\n  Chunhong Pan", "title": "LFFD: A Light and Fast Face Detector for Edge Devices", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection, as a fundamental technology for various applications, is\nalways deployed on edge devices which have limited memory storage and low\ncomputing power. This paper introduces a Light and Fast Face Detector (LFFD)\nfor edge devices. The proposed method is anchor-free and belongs to the\none-stage category. Specifically, we rethink the importance of receptive field\n(RF) and effective receptive field (ERF) in the background of face detection.\nEssentially, the RFs of neurons in a certain layer are distributed regularly in\nthe input image and theses RFs are natural \"anchors\". Combining RF \"anchors\"\nand appropriate RF strides, the proposed method can detect a large range of\ncontinuous face scales with 100% coverage in theory. The insightful\nunderstanding of relations between ERF and face scales motivates an efficient\nbackbone for one-stage detection. The backbone is characterized by eight\ndetection branches and common layers, resulting in efficient computation.\nComprehensive and extensive experiments on popular benchmarks: WIDER FACE and\nFDDB are conducted. A new evaluation schema is proposed for\napplication-oriented scenarios. Under the new schema, the proposed method can\nachieve superior accuracy (WIDER FACE Val/Test -- Easy: 0.910/0.896, Medium:\n0.881/0.865, Hard: 0.780/0.770; FDDB -- discontinuous: 0.973, continuous:\n0.724). Multiple hardware platforms are introduced to evaluate the running\nefficiency. The proposed method can obtain fast inference speed (NVIDIA TITAN\nXp: 131.45 FPS at 640x480; NVIDIA TX2: 136.99 PFS at 160x120; Raspberry Pi 3\nModel B+: 8.44 FPS at 160x120) with model size of 9 MB.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:47:24 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 06:34:22 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 08:09:32 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["He", "Yonghao", ""], ["Xu", "Dezhong", ""], ["Wu", "Lifang", ""], ["Jian", "Meng", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1904.10638", "submitter": "Gang Liu", "authors": "Yu Yu, Gang Liu, Jean-Marc Odobez", "title": "Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection\n  Synthesis", "comments": "Work started in June 2018, Submitted to CVPR on November 15th 2018,\n  accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an indicator of human attention gaze is a subtle behavioral cue which can\nbe exploited in many applications. However, inferring 3D gaze direction is\nchallenging even for deep neural networks given the lack of large amount of\ndata (groundtruthing gaze is expensive and existing datasets use different\nsetups) and the inherent presence of gaze biases due to person-specific\ndifference. In this work, we address the problem of person-specific gaze model\nadaptation from only a few reference training samples. The main and novel idea\nis to improve gaze adaptation by generating additional training samples through\nthe synthesis of gaze-redirected eye images from existing reference samples. In\ndoing so, our contributions are threefold: (i) we design our gaze redirection\nframework from synthetic data, allowing us to benefit from aligned training\nsample pairs to predict accurate inverse mapping fields; (ii) we proposed a\nself-supervised approach for domain adaptation; (iii) we exploit the gaze\nredirection to improve the performance of person-specific gaze estimation.\nExtensive experiments on two public datasets demonstrate the validity of our\ngaze retargeting and gaze estimation framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 04:38:30 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Yu", "Yu", ""], ["Liu", "Gang", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1904.10654", "submitter": "Yue Cheng", "authors": "Xuan Zhu, Yue Cheng, Jinye Peng, Rongzhi Wang, Mingnan Le, Xin Liu", "title": "Super-Resolved Image Perceptual Quality Improvement via Multi-Feature\n  Discriminators", "comments": "18 pages, 10 figures, 6 tables", "journal-ref": null, "doi": "10.1117/1.JEI.29.1.013017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial network (GAN) for image super-resolution (SR) has\nattracted enormous interests in recent years. However, the GAN-based SR methods\nonly use image discriminator to distinguish SR images and high-resolution (HR)\nimages. Image discriminator fails to discriminate images accurately since image\nfeatures cannot be fully expressed. In this paper, we design a new GAN-based SR\nframework GAN-IMC which includes generator, image discriminator, morphological\ncomponent discriminator and color discriminator. The combination of multiple\nfeature discriminators improves the accuracy of image discrimination.\nAdversarial training between the generator and multi-feature discriminators\nforces SR images to converge with HR images in terms of data and features\ndistribution. Moreover, in some cases, feature enhancement of salient regions\nis also worth considering. GAN-IMC is further optimized by weighted content\nloss (GAN-IMCW), which effectively restores and enhances salient regions in SR\nimages. The effectiveness and robustness of our method are confirmed by\nextensive experiments on public datasets. Compared with state-of-the-art\nmethods, the proposed method not only achieves competitive Perceptual Index\n(PI) and Natural Image Quality Evaluator (NIQE) values but also obtains\npleasant visual perception in image edge, texture, color and salient regions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 06:28:57 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 07:53:07 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Zhu", "Xuan", ""], ["Cheng", "Yue", ""], ["Peng", "Jinye", ""], ["Wang", "Rongzhi", ""], ["Le", "Mingnan", ""], ["Liu", "Xin", ""]]}, {"id": "1904.10666", "submitter": "Ehsan Adeli", "authors": "Hsu-kuang Chiu, Ehsan Adeli, Juan Carlos Niebles", "title": "Segmenting the Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future is an important aspect for decision-making in robotics\nor autonomous driving systems, which heavily rely upon visual scene\nunderstanding. While prior work attempts to predict future video pixels,\nanticipate activities or forecast future scene semantic segments from\nsegmentation of the preceding frames, methods that predict future semantic\nsegmentation solely from the previous frame RGB data in a single end-to-end\ntrainable model do not exist. In this paper, we propose a temporal\nencoder-decoder network architecture that encodes RGB frames from the past and\ndecodes the future semantic segmentation. The network is coupled with a new\nknowledge distillation training framework specific for the forecasting task.\nOur method, only seeing preceding video frames, implicitly models the scene\nsegments while simultaneously accounting for the object dynamics to infer the\nfuture scene semantic segments. Our results on Cityscapes and Apolloscape\noutperform the baseline and current state-of-the-art methods. Code is available\nat https://github.com/eddyhkchiu/segmenting_the_future/.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 07:30:34 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:10:20 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Chiu", "Hsu-kuang", ""], ["Adeli", "Ehsan", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1904.10669", "submitter": "Bin Zhao", "authors": "Xuelong Li, Bin Zhao, Xiaoqiang Lu", "title": "A General Framework for Edited Video and Raw Video Summarization", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build a general summarization framework for both of edited\nvideo and raw video summarization. Overall, our work can be divided into three\nfolds: 1) Four models are designed to capture the properties of video\nsummaries, i.e., containing important people and objects (importance),\nrepresentative to the video content (representativeness), no similar key-shots\n(diversity) and smoothness of the storyline (storyness). Specifically, these\nmodels are applicable to both edited videos and raw videos. 2) A comprehensive\nscore function is built with the weighted combination of the aforementioned\nfour models. Note that the weights of the four models in the score function,\ndenoted as property-weight, are learned in a supervised manner. Besides, the\nproperty-weights are learned for edited videos and raw videos, respectively. 3)\nThe training set is constructed with both edited videos and raw videos in order\nto make up the lack of training data. Particularly, each training video is\nequipped with a pair of mixing-coefficients which can reduce the structure mess\nin the training set caused by the rough mixture. We test our framework on three\ndatasets, including edited videos, short raw videos and long raw videos.\nExperimental results have verified the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 07:42:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Li", "Xuelong", ""], ["Zhao", "Bin", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1904.10674", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX), Bertrand Saux, S\\'ebastien Lef\\`evre\n  (OBELIX)", "title": "Deep Learning for Classification of Hyperspectral Data: A Comparative\n  Review", "comments": null, "journal-ref": null, "doi": "10.1109/MGRS.2019.2912563", "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning techniques revolutionized the way remote\nsensing data are processed. Classification of hyperspectral data is no\nexception to the rule, but has intrinsic specificities which make application\nof deep learning less straightforward than with other optical data. This\narticle presents a state of the art of previous machine learning approaches,\nreviews the various deep learning approaches currently proposed for\nhyperspectral classification, and identifies the problems and difficulties\nwhich arise to implement deep neural networks for this task. In particular, the\nissues of spatial and spectral resolution, data volume, and transfer of models\nfrom multimedia images to hyperspectral data are addressed. Additionally, a\ncomparative study of various families of network architectures is provided and\na software toolbox is publicly released to allow experimenting with these\nmethods. 1 This article is intended for both data scientists with interest in\nhyperspectral data and remote sensing experts eager to apply deep learning\ntechniques to their own dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 07:56:37 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX"], ["Saux", "Bertrand", "", "OBELIX"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1904.10681", "submitter": "Yanli Ji", "authors": "Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao Shen, Wei-Shi\n  Zheng", "title": "A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human\n  Action Recognition", "comments": "Origianl version has been published by ACMMM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current researches of action recognition mainly focus on single-view and\nmulti-view recognition, which can hardly satisfies the requirements of\nhuman-robot interaction (HRI) applications to recognize actions from arbitrary\nviews. The lack of datasets also sets up barriers. To provide data for\narbitrary-view action recognition, we newly collect a large-scale RGB-D action\ndataset for arbitrary-view action analysis, including RGB videos, depth and\nskeleton sequences. The dataset includes action samples captured in 8 fixed\nviewpoints and varying-view sequences which covers the entire 360 degree view\nangles. In total, 118 persons are invited to act 40 action categories, and\n25,600 video samples are collected. Our dataset involves more participants,\nmore viewpoints and a large number of samples. More importantly, it is the\nfirst dataset containing the entire 360 degree varying-view sequences. The\ndataset provides sufficient data for multi-view, cross-view and arbitrary-view\naction analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to\ntackle the problem of arbitrary-view action recognition. Experiment results\nshow that the VS-CNN achieves superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:09:35 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Ji", "Yanli", ""], ["Xu", "Feixiang", ""], ["Yang", "Yang", ""], ["Shen", "Fumin", ""], ["Shen", "Heng Tao", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1904.10698", "submitter": "Xiahai Zhuang", "authors": "Shangqi Gao and Xiahai Zhuang", "title": "Multi-scale deep neural networks for real image super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SR) is extremely difficult if the upscaling\nfactors of image pairs are unknown and different from each other, which is\ncommon in real image SR. To tackle the difficulty, we develop two multi-scale\ndeep neural networks (MsDNN) in this work. Firstly, due to the high computation\ncomplexity in high-resolution spaces, we process an input image mainly in two\ndifferent downscaling spaces, which could greatly lower the usage of GPU\nmemory. Then, to reconstruct the details of an image, we design a multi-scale\nresidual network (MsRN) in the downscaling spaces based on the residual blocks.\nBesides, we propose a multi-scale dense network based on the dense blocks to\ncompare with MsRN. Finally, our empirical experiments show the robustness of\nMsDNN for image SR when the upscaling factor is unknown. According to the\npreliminary results of NTIRE 2019 image SR challenge, our team\n(ZXHresearch@fudan) ranks 21-st among all participants. The implementation of\nMsDNN is released https://github.com/shangqigao/gsq-image-SR\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:43:18 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Gao", "Shangqi", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "1904.10699", "submitter": "Abhishek Dutta", "authors": "Abhishek Dutta and Andrew Zisserman", "title": "The VIA Annotation Software for Images, Audio and Video", "comments": "to appear in Proceedings of the 27th ACM International Conference on\n  Multimedia (MM '19), October 21-25, 2019, Nice, France. ACM, New York, NY,\n  USA, 4 pages", "journal-ref": null, "doi": "10.1145/3343031.3350535", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a simple and standalone manual annotation tool\nfor images, audio and video: the VGG Image Annotator (VIA). This is a light\nweight, standalone and offline software package that does not require any\ninstallation or setup and runs solely in a web browser. The VIA software allows\nhuman annotators to define and describe spatial regions in images or video\nframes, and temporal segments in audio or video. These manual annotations can\nbe exported to plain text data formats such as JSON and CSV and therefore are\namenable to further processing by other software tools. VIA also supports\ncollaborative annotation of a large dataset by a group of human annotators. The\nBSD open source license of this software allows it to be used in any academic\nproject or commercial application.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:55:27 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 13:41:25 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 10:29:56 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Dutta", "Abhishek", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1904.10709", "submitter": "Bin Zhao", "authors": "Bin Zhao, Xuelong Li, Xiaoqiang Lu, Zhigang Wang", "title": "A CNN-RNN Architecture for Multi-Label Weather Recognition", "comments": "One weather recognition dataset is constructed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather Recognition plays an important role in our daily lives and many\ncomputer vision applications. However, recognizing the weather conditions from\na single image remains challenging and has not been studied thoroughly.\nGenerally, most previous works treat weather recognition as a single-label\nclassification task, namely, determining whether an image belongs to a specific\nweather class or not. This treatment is not always appropriate, since more than\none weather conditions may appear simultaneously in a single image. To address\nthis problem, we make the first attempt to view weather recognition as a\nmulti-label classification task, i.e., assigning an image more than one labels\naccording to the displayed weather conditions. Specifically, a CNN-RNN based\nmulti-label classification approach is proposed in this paper. The\nconvolutional neural network (CNN) is extended with a channel-wise attention\nmodel to extract the most correlated visual features. The Recurrent Neural\nNetwork (RNN) further processes the features and excavates the dependencies\namong weather classes. Finally, the weather labels are predicted step by step.\nBesides, we construct two datasets for the weather recognition task and explore\nthe relationships among different weather conditions. Experimental results\ndemonstrate the superiority and effectiveness of the proposed approach. The new\nconstructed datasets will be available at\nhttps://github.com/wzgwzg/Multi-Label-Weather-Recognition.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:27:29 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhao", "Bin", ""], ["Li", "Xuelong", ""], ["Lu", "Xiaoqiang", ""], ["Wang", "Zhigang", ""]]}, {"id": "1904.10754", "submitter": "Ruqi Huang", "authors": "Ruqi Huang and Marie-Julie Rakotosaona and Panos Achlioptas and\n  Leonidas Guibas and Maks Ovsjanikov", "title": "OperatorNet: Recovering 3D Shapes From Difference Operators", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning-based framework for reconstructing 3D shapes\nfrom functional operators, compactly encoded as small-sized matrices. To this\nend we introduce a novel neural architecture, called OperatorNet, which takes\nas input a set of linear operators representing a shape and produces its 3D\nembedding. We demonstrate that this approach significantly outperforms previous\npurely geometric methods for the same problem. Furthermore, we introduce a\nnovel functional operator, which encodes the extrinsic or pose-dependent shape\ninformation, and thus complements purely intrinsic pose-oblivious operators,\nsuch as the classical Laplacian. Coupled with this novel operator, our\nreconstruction network achieves very high reconstruction accuracy, even in the\npresence of incomplete information about a shape, given a soft or functional\nmap expressed in a reduced basis. Finally, we demonstrate that the\nmultiplicative functional algebra enjoyed by these operators can be used to\nsynthesize entirely new unseen shapes, in the context of shape interpolation\nand shape analogy applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:47:08 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 16:27:34 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Huang", "Ruqi", ""], ["Rakotosaona", "Marie-Julie", ""], ["Achlioptas", "Panos", ""], ["Guibas", "Leonidas", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1904.10772", "submitter": "Cedric Scheerlinck", "authors": "Cedric Scheerlinck, Henri Rebecq, Timo Stoffregen, Nick Barnes, Robert\n  Mahony, Davide Scaramuzza", "title": "CED: Color Event Camera Dataset", "comments": "Conference on Computer Vision and Pattern Recognition Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel, bio-inspired visual sensors, whose pixels output\nasynchronous and independent timestamped spikes at local intensity changes,\ncalled 'events'. Event cameras offer advantages over conventional frame-based\ncameras in terms of latency, high dynamic range (HDR) and temporal resolution.\nUntil recently, event cameras have been limited to outputting events in the\nintensity channel, however, recent advances have resulted in the development of\ncolor event cameras, such as the Color-DAVIS346. In this work, we present and\nrelease the first Color Event Camera Dataset (CED), containing 50 minutes of\nfootage with both color frames and events. CED features a wide variety of\nindoor and outdoor scenes, which we hope will help drive forward event-based\nvision research. We also present an extension of the event camera simulator\nESIM that enables simulation of color events. Finally, we present an evaluation\nof three state-of-the-art image reconstruction methods that can be used to\nconvert the Color-DAVIS346 into a continuous-time, HDR, color video camera to\nvisualise the event stream, and for use in downstream vision applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 12:42:12 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Scheerlinck", "Cedric", ""], ["Rebecq", "Henri", ""], ["Stoffregen", "Timo", ""], ["Barnes", "Nick", ""], ["Mahony", "Robert", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.10781", "submitter": "Dwarikanath Mahapatra", "authors": "Behzad Bozorgtabar, Dwarikanath Mahapatra, Hendrik von Teng, Alexander\n  Pollinger, Lukas Ebner, Jean-Phillipe Thiran, Mauricio Reyes", "title": "Informative sample generation using class aware generative adversarial\n  networks for classification of chest Xrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training robust deep learning (DL) systems for disease detection from medical\nimages is challenging due to limited images covering different disease types\nand severity. The problem is especially acute, where there is a severe class\nimbalance. We propose an active learning (AL) framework to select most\ninformative samples for training our model using a Bayesian neural network.\nInformative samples are then used within a novel class aware generative\nadversarial network (CAGAN) to generate realistic chest xray images for data\naugmentation by transferring characteristics from one class label to another.\nExperiments show our proposed AL framework is able to achieve state-of-the-art\nperformance by using about $35\\%$ of the full dataset, thus saving significant\ntime and effort over conventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:05:38 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 04:16:32 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bozorgtabar", "Behzad", ""], ["Mahapatra", "Dwarikanath", ""], ["von Teng", "Hendrik", ""], ["Pollinger", "Alexander", ""], ["Ebner", "Lukas", ""], ["Thiran", "Jean-Phillipe", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1904.10787", "submitter": "Janez Krizaj", "authors": "Janez Kri\\v{z}aj and Peter Peer and Vitomir \\v{S}truc and Simon\n  Dobri\\v{s}ek", "title": "Simultaneous regression and feature learning for facial landmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment (or facial landmarking) is an important task in many\nface-related applications, ranging from registration, tracking and animation to\nhigher-level classification problems such as face, expression or attribute\nrecognition. While several solutions have been presented in the literature for\nthis task so far, reliably locating salient facial features across a wide range\nof posses still remains challenging. To address this issue, we propose in this\npaper a novel method for automatic facial landmark localization in 3D face data\ndesigned specifically to address appearance variability caused by significant\npose variations. Our method builds on recent cascaded-regression-based methods\nto facial landmarking and uses a gating mechanism to incorporate multiple\nlinear cascaded regression models each trained for a limited range of poses\ninto a single powerful landmarking model capable of processing arbitrary posed\ninput data. We develop two distinct approaches around the proposed gating\nmechanism: i) the first uses a gated multiple ridge descent (GRID) mechanism in\nconjunction with established (hand-crafted) HOG features for face alignment and\nachieves state-of-the-art landmarking performance across a wide range of facial\nposes, ii) the second simultaneously learns multiple-descent directions as well\nas binary features (SMUF) that are optimal for the alignment tasks and in\naddition to competitive landmarking results also ensures extremely rapid\nprocessing. We evaluate both approaches in rigorous experiments on several\npopular datasets of 3D face images, i.e., the FRGCv2 and Bosphorus 3D Face\ndatasets and image collections F and G from the University of Notre Dame. The\nresults of our evaluation show that both approaches are competitive in\ncomparison to the state-of-the-art, while exhibiting considerable robustness to\npose variations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:15:30 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Kri\u017eaj", "Janez", ""], ["Peer", "Peter", ""], ["\u0160truc", "Vitomir", ""], ["Dobri\u0161ek", "Simon", ""]]}, {"id": "1904.10795", "submitter": "Zeqing Fu", "authors": "Zeqing Fu, Wei Hu, Zongming Guo", "title": "3D Dynamic Point Cloud Inpainting via Temporal Consistency on Graphs", "comments": "7 pages, 5 figures, accepted by IEEE ICME 2020 at 2020.04.03. arXiv\n  admin note: text overlap with arXiv:1810.03973", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of 3D laser scanning techniques and depth sensors, 3D\ndynamic point clouds have attracted increasing attention as a representation of\n3D objects in motion, enabling various applications such as 3D immersive\ntele-presence, gaming and navigation. However, dynamic point clouds usually\nexhibit holes of missing data, mainly due to the fast motion, the limitation of\nacquisition and complicated structure. Leveraging on graph signal processing\ntools, we represent irregular point clouds on graphs and propose a novel\ninpainting method exploiting both intra-frame self-similarity and inter-frame\nconsistency in 3D dynamic point clouds. Specifically, for each missing region\nin every frame of the point cloud sequence, we search for its self-similar\nregions in the current frame and corresponding ones in adjacent frames as\nreferences. Then we formulate dynamic point cloud inpainting as an optimization\nproblem based on the two types of references, which is regularized by a\ngraph-signal smoothness prior. Experimental results show the proposed approach\noutperforms three competing methods significantly, both in objective and\nsubjective quality.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 08:18:56 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 15:01:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Fu", "Zeqing", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""]]}, {"id": "1904.10816", "submitter": "Flavio De Barros Vidal", "authors": "Lucas Faria Porto, Laise Nascimento Correia Lima, Marta Flores, Andrea\n  Valsecchi, Oscar Ibanez, Carlos Eduardo Machado Palhares, Flavio de Barros\n  Vidal", "title": "Automatic cephalometric landmarks detection on frontal faces: an\n  approach based on supervised learning techniques", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmarks are employed in many research areas such as facial\nrecognition, craniofacial identification, age and sex estimation among the most\nimportant. In the forensic field, the focus is on the analysis of a particular\nset of facial landmarks, defined as cephalometric landmarks. Previous works\ndemonstrated that the descriptive adequacy of these anatomical references for\nan indirect application (photo-anthropometric description) increased the\nmarking precision of these points, contributing to a greater reliability of\nthese analyzes. However, most of them are performed manually and all of them\nare subjectivity inherent to the expert examiners. In this sense, the purpose\nof this work is the development and validation of automatic techniques to\ndetect cephalometric landmarks from digital images of frontal faces in forensic\nfield. The presented approach uses a combination of computer vision and image\nprocessing techniques within a supervised learning procedures. The proposed\nmethodology obtains similar precision to a group of human manual cephalometric\nreference markers and result to be more accurate against others\nstate-of-the-art facial landmark detection frameworks. It achieves a normalized\nmean distance (in pixel) error of 0.014, similar to the mean inter-expert\ndispersion (0.009) and clearly better than other automatic approaches also\nanalyzed along of this work (0.026 and 0.101).\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:44:28 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Porto", "Lucas Faria", ""], ["Lima", "Laise Nascimento Correia", ""], ["Flores", "Marta", ""], ["Valsecchi", "Andrea", ""], ["Ibanez", "Oscar", ""], ["Palhares", "Carlos Eduardo Machado", ""], ["Vidal", "Flavio de Barros", ""]]}, {"id": "1904.10851", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Jun Feng, Yang Gao, Ting Lei, Zhenwei Xie, Xiaocong Yuan", "title": "Optical machine learning with incoherent light and a single-pixel\n  detector", "comments": null, "journal-ref": "Optics Letters 44(21) 2019", "doi": "10.1364/OL.44.005186", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An optical diffractive neural network (DNN) can be implemented with a\ncascaded phase mask architecture. Like an optical computer, the system can\nperform machine learning tasks such as number digit recognition in an\nall-optical manner. However, the system can only work under coherent light\nillumination and the precision requirement in practical experiments is quite\nhigh. This paper proposes an optical machine learning framework based on\nsingle-pixel imaging (MLSPI). The MLSPI system can perform the same linear\npattern recognition task as DNN. Furthermore, it can work under incoherent\nlighting conditions, has lower experimental complexity and can be easily\nprogrammable.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 14:49:36 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 08:48:29 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 20:01:45 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Jiao", "Shuming", ""], ["Feng", "Jun", ""], ["Gao", "Yang", ""], ["Lei", "Ting", ""], ["Xie", "Zhenwei", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1904.10863", "submitter": "Artjom Zern", "authors": "Artjom Zern, Matthias Zisler, Stefania Petra, Christoph Schn\\\"orr", "title": "Unsupervised Assignment Flow: Label Learning on Feature Manifolds by\n  Spatially Regularized Geometric Assignment", "comments": "34 pages, 13 figures, published in Journal of Mathematical Imaging\n  and Vision (JMIV)", "journal-ref": null, "doi": "10.1007/s10851-019-00935-7", "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the unsupervised assignment flow that couples the\nassignment flow for supervised image labeling with Riemannian gradient flows\nfor label evolution on feature manifolds. The latter component of the approach\nencompasses extensions of state-of-the-art clustering approaches to\nmanifold-valued data. Coupling label evolution with the spatially regularized\nassignment flow induces a sparsifying effect that enables to learn compact\nlabel dictionaries in an unsupervised manner. Our approach alleviates the\nrequirement for supervised labeling to have proper labels at hand, because an\ninitial set of labels can evolve and adapt to better values while being\nassigned to given data. The separation between feature and assignment manifolds\nenables the flexible application which is demonstrated for three scenarios with\nmanifold-valued features. Experiments demonstrate a beneficial effect in both\ndirections: adaptivity of labels improves image labeling, and steering label\nevolution by spatially regularized assignments leads to proper labels, because\nthe assignment flow for supervised labeling is exactly used without any\napproximation for label learning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:08:01 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 13:58:53 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 15:44:49 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zern", "Artjom", ""], ["Zisler", "Matthias", ""], ["Petra", "Stefania", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1904.10873", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Donghao Li, Xinwei Sun, Shun Zhang, Yizhou Wang, Yuan Yao", "title": "$S^{2}$-LBI: Stochastic Split Linearized Bregman Iterations for\n  Parsimonious Deep Learning", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Stochastic Split Linearized Bregman Iteration\n($S^{2}$-LBI) algorithm to efficiently train the deep network. The $S^{2}$-LBI\nintroduces an iterative regularization path with structural sparsity. Our\n$S^{2}$-LBI combines the computational efficiency of the LBI, and model\nselection consistency in learning the structural sparsity. The computed\nsolution path intrinsically enables us to enlarge or simplify a network, which\ntheoretically, is benefited from the dynamics property of our $S^{2}$-LBI\nalgorithm. The experimental results validate our $S^{2}$-LBI on MNIST and\nCIFAR-10 dataset. For example, in MNIST, we can either boost a network with\nonly 1.5K parameters (1 convolutional layer of 5 filters, and 1 FC layer),\nachieves 98.40\\% recognition accuracy; or we simplify $82.5\\%$ of parameters in\nLeNet-5 network, and still achieves the 98.47\\% recognition accuracy. In\naddition, we also have the learning results on ImageNet, which will be added in\nthe next version of our report.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:31:55 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Fu", "Yanwei", ""], ["Li", "Donghao", ""], ["Sun", "Xinwei", ""], ["Zhang", "Shun", ""], ["Wang", "Yizhou", ""], ["Yao", "Yuan", ""]]}, {"id": "1904.10898", "submitter": "Michele Claus", "authors": "Michele Claus and Jan van Gemert", "title": "ViDeNN: Deep Blind Video Denoising", "comments": "Submission of NTIRE: New Trends in Image Restoration and Enhancement\n  workshop and challenges at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ViDeNN: a CNN for Video Denoising without prior knowledge on the\nnoise distribution (blind denoising). The CNN architecture uses a combination\nof spatial and temporal filtering, learning to spatially denoise the frames\nfirst and at the same time how to combine their temporal information, handling\nobjects motion, brightness changes, low-light conditions and temporal\ninconsistencies. We demonstrate the importance of the data used for CNNs\ntraining, creating for this purpose a specific dataset for low-light\nconditions. We test ViDeNN on common benchmarks and on self-collected data,\nachieving good results comparable with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 16:08:27 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Claus", "Michele", ""], ["van Gemert", "Jan", ""]]}, {"id": "1904.10917", "submitter": "Dong Wang", "authors": "Dong Wang and Xiao-Ping Wang", "title": "The iterative convolution-thresholding method (ICTM) for image\n  segmentation", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel iterative convolution-thresholding method\n(ICTM) that is applicable to a range of variational models for image\nsegmentation. A variational model usually minimizes an energy functional\nconsisting of a fidelity term and a regularization term. In the ICTM, the\ninterface between two different segment domains is implicitly represented by\ntheir characteristic functions. The fidelity term is then usually written as a\nlinear functional of the characteristic functions and the regularized term is\napproximated by a functional of characteristic functions in terms of heat\nkernel convolution. This allows us to design an iterative\nconvolution-thresholding method to minimize the approximate energy. The method\nis simple, efficient and enjoys the energy-decaying property. Numerical\nexperiments show that the method is easy to implement, robust and applicable to\nvarious image segmentation models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 16:50:12 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Wang", "Dong", ""], ["Wang", "Xiao-Ping", ""]]}, {"id": "1904.11005", "submitter": "Modar Alfadly", "authors": "Modar Alfadly, Adel Bibi and Bernard Ghanem", "title": "Analytical Moment Regularizer for Gaussian Robust Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive performance of deep neural networks (DNNs) on numerous\nvision tasks, they still exhibit yet-to-understand uncouth behaviours. One\npuzzling behaviour is the subtle sensitive reaction of DNNs to various noise\nattacks. Such a nuisance has strengthened the line of research around\ndeveloping and training noise-robust networks. In this work, we propose a new\ntraining regularizer that aims to minimize the probabilistic expected training\nloss of a DNN subject to a generic Gaussian input. We provide an efficient and\nsimple approach to approximate such a regularizer for arbitrary deep networks.\nThis is done by leveraging the analytic expression of the output mean of a\nshallow neural network; avoiding the need for the memory and computationally\nexpensive data augmentation. We conduct extensive experiments on LeNet and\nAlexNet on various datasets including MNIST, CIFAR10, and CIFAR100\ndemonstrating the effectiveness of our proposed regularizer. In particular, we\nshow that networks that are trained with the proposed regularizer benefit from\na boost in robustness equivalent to performing 3-21 folds of data augmentation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 18:37:36 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Alfadly", "Modar", ""], ["Bibi", "Adel", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1904.11008", "submitter": "Arash Kalatian", "authors": "Arash Kalatian, Bilal Farooq", "title": "DeepWait: Pedestrian Wait Time Estimation in Mixed Traffic Conditions\n  Using Deep Survival Analysis", "comments": "Accepted for publication in the proceedings of IEEE Intelligent\n  Transportation Systems Conference - ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian's road crossing behaviour is one of the important aspects of urban\ndynamics that will be affected by the introduction of autonomous vehicles. In\nthis study we introduce DeepSurvival, a novel framework for estimating\npedestrian's waiting time at unsignalized mid-block crosswalks in mixed traffic\nconditions. We exploit the strengths of deep learning in capturing the\nnonlinearities in the data and develop a cox proportional hazard model with a\ndeep neural network as the log-risk function. An embedded feature selection\nalgorithm for reducing data dimensionality and enhancing the interpretability\nof the network is also developed. We test our framework on a dataset collected\nfrom 160 participants using an immersive virtual reality environment.\nValidation results showed that with a C-index of 0.64 our proposed framework\noutperformed the standard cox proportional hazard-based model with a C-index of\n0.58.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 00:04:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 01:39:17 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 20:19:10 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kalatian", "Arash", ""], ["Farooq", "Bilal", ""]]}, {"id": "1904.11031", "submitter": "Bahareh Behboodi", "authors": "Bahareh Behboodi and Hassan Rivaz", "title": "Ultrasound segmentation using U-Net: learning from simulated data and\n  testing on real data", "comments": "Accepted in EMBC 2019", "journal-ref": null, "doi": "10.1109/EMBC.2019.8857218", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of ultrasound images is an essential task in both diagnosis and\nimage-guided interventions given the ease-of-use and low cost of this imaging\nmodality. As manual segmentation is tedious and time consuming, a growing body\nof research has focused on the development of automatic segmentation\nalgorithms. Deep learning algorithms have shown remarkable achievements in this\nregard; however, they need large training datasets. Unfortunately, preparing\nlarge labeled datasets in ultrasound images is prohibitively difficult.\nTherefore, in this study, we propose the use of simulated ultrasound (US)\nimages for training the U-Net deep learning segmentation architecture and test\non tissue-mimicking phantom data collected by an ultrasound machine. We\ndemonstrate that the trained architecture on the simulated data is\ntransferrable to real data, and therefore, simulated data can be considered as\nan alternative training dataset when real datasets are not available. The\nsecond contribution of this paper is that we train our U- Net network on\nenvelope and B-mode images of the simulated dataset, and test the trained\nnetwork on real envelope and B- mode images of phantom, respectively. We show\nthat test results are superior for the envelope data compared to B-mode image.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 19:24:01 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Behboodi", "Bahareh", ""], ["Rivaz", "Hassan", ""]]}, {"id": "1904.11041", "submitter": "Zhiguan Wang", "authors": "Honglong Cai, Zhiguan Wang, Jinxing Cheng", "title": "Multi-Scale Body-Part Mask Guided Attention for Person Re-identification", "comments": null, "journal-ref": "CVPRW 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Person re-identification becomes a more and more important task due to its\nwide applications. In practice, person re-identification still remains\nchallenging due to the variation of person pose, different lighting, occlusion,\nmisalignment, background clutter, etc. In this paper, we propose a multi-scale\nbody-part mask guided attention network (MMGA), which jointly learns whole-body\nand part body attention to help extract global and local features\nsimultaneously. In MMGA, body-part masks are used to guide the training of\ncorresponding attention. Experiments show that our proposed method can reduce\nthe negative influence of variation of person pose, misalignment and background\nclutter. Our method achieves rank-1/mAP of 95.0%/87.2% on the Market1501\ndataset, 89.5%/78.1% on the DukeMTMC-reID dataset, outperforming current\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 19:56:28 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Cai", "Honglong", ""], ["Wang", "Zhiguan", ""], ["Cheng", "Jinxing", ""]]}, {"id": "1904.11042", "submitter": "Rey Wiyatno", "authors": "Rey Reza Wiyatno, Anqi Xu", "title": "Physical Adversarial Textures that Fool Visual Object Tracking", "comments": "Accepted to the International Conference on Computer Vision (ICCV)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for generating inconspicuous-looking textures that, when\ndisplayed in the physical world as digital or printed posters, cause visual\nobject tracking systems to become confused. For instance, as a target being\ntracked by a robot's camera moves in front of such a poster, our generated\ntexture makes the tracker lock onto it and allows the target to evade. This\nwork aims to fool seldom-targeted regression tasks, and in particular compares\ndiverse optimization strategies: non-targeted, targeted, and a new family of\nguided adversarial losses. While we use the Expectation Over Transformation\n(EOT) algorithm to generate physical adversaries that fool tracking models when\nimaged under diverse conditions, we compare the impacts of different\nconditioning variables, including viewpoint, lighting, and appearances, to find\npractical attack setups with high resulting adversarial strength and\nconvergence speed. We further showcase textures optimized solely using\nsimulated scenes can confuse real-world tracking systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 19:56:57 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 20:12:35 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Wiyatno", "Rey Reza", ""], ["Xu", "Anqi", ""]]}, {"id": "1904.11045", "submitter": "Krishna Regmi", "authors": "Krishna Regmi, Mubarak Shah", "title": "Bridging the Domain Gap for Ground-to-Aerial Image Matching", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual entities in cross-view images exhibit drastic domain changes due\nto the difference in viewpoints each set of images is captured from. Existing\nstate-of-the-art methods address the problem by learning view-invariant\ndescriptors for the images. We propose a novel method for solving this task by\nexploiting the generative powers of conditional GANs to synthesize an aerial\nrepresentation of a ground level panorama and use it to minimize the domain gap\nbetween the two views. The synthesized image being from the same view as the\ntarget image helps the network to preserve important cues in aerial images\nfollowing our Joint Feature Learning approach. Our Feature Fusion method\ncombines the complementary features from a synthesized aerial image with the\ncorresponding ground features to obtain a robust query representation. In\naddition, multi-scale feature aggregation preserves image representations at\ndifferent feature scales useful for solving this complex task. Experimental\nresults show that our proposed approach performs significantly better than the\nstate-of-the-art methods on the challenging CVUSA dataset in terms of top-1 and\ntop-1% retrieval accuracies. Furthermore, to evaluate the generalization of our\nmethod on urban landscapes, we collected a new cross-view localization dataset\nwith geo-reference information.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 20:02:44 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 09:05:02 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Regmi", "Krishna", ""], ["Shah", "Mubarak", ""]]}, {"id": "1904.11084", "submitter": "Rodolfo Migon Favaretto", "authors": "Victor Araujo and Rodolfo Migon Favaretto and Paulo Knob and Soraia\n  Raupp Musse and Felipe Vilanova and Angelo Brandelli Costa", "title": "How much do you perceive this? An analysis on perceptions of geometric\n  features, personalities and emotions in virtual humans (Extended Version)", "comments": "Extended Version of a paper published at IVA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to evaluate people's perception regarding geometric features,\npersonalities and emotions characteristics in virtual humans. For this, we use\nas a basis, a dataset containing the tracking files of pedestrians captured\nfrom spontaneous videos and visualized them as identical virtual humans. The\ngoal is to focus on their behavior and not being distracted by other features.\nIn addition to tracking files containing their positions, the dataset also\ncontains pedestrian emotions and personalities detected using Computer Vision\nand Pattern Recognition techniques. We proceed with our analysis in order to\nanswer the question if subjects can perceive geometric features as\ndistances/speeds as well as emotions and personalities in video sequences when\npedestrians are represented by virtual humans. Regarding the participants, an\namount of 73 people volunteered for the experiment. The analysis was divided in\ntwo parts: i) evaluation on perception of geometric characteristics, such as\ndensity, angular variation, distances and speeds, and ii) evaluation on\npersonality and emotion perceptions. Results indicate that, even without\nexplaining to the participants the concepts of each personality or emotion and\nhow they were calculated (considering geometric characteristics), in most of\nthe cases, participants perceived the personality and emotion expressed by the\nvirtual agents, in accordance with the available ground truth.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:50:34 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Araujo", "Victor", ""], ["Favaretto", "Rodolfo Migon", ""], ["Knob", "Paulo", ""], ["Musse", "Soraia Raupp", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo Brandelli", ""]]}, {"id": "1904.11093", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani and Vishal M. Patel", "title": "Deep Sparse Representation-based Classification", "comments": null, "journal-ref": "IEEE Signal Processing Letters, 2019", "doi": "10.1109/LSP.2019.2913022", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transductive deep learning-based formulation for the sparse\nrepresentation-based classification (SRC) method. The proposed network consists\nof a convolutional autoencoder along with a fully-connected layer. The role of\nthe autoencoder network is to learn robust deep features for classification. On\nthe other hand, the fully-connected layer, which is placed in between the\nencoder and the decoder networks, is responsible for finding the sparse\nrepresentation. The estimated sparse codes are then used for classification.\nVarious experiments on three different datasets show that the proposed network\nleads to sparse representations that give better classification results than\nstate-of-the-art SRC methods. The source code is available at:\ngithub.com/mahdiabavisani/DSRC.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 22:52:18 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1904.11111", "submitter": "Zhengqi Li", "authors": "Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely,\n  Ce Liu, William T. Freeman", "title": "Learning the Depths of Moving People by Watching Frozen People", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for predicting dense depth in scenarios where both a\nmonocular camera and people in the scene are freely moving. Existing methods\nfor recovering depth for dynamic, non-rigid objects from monocular video impose\nstrong assumptions on the objects' motion and may only recover sparse depth. In\nthis paper, we take a data-driven approach and learn human depth priors from a\nnew source of data: thousands of Internet videos of people imitating\nmannequins, i.e., freezing in diverse, natural poses, while a hand-held camera\ntours the scene. Because people are stationary, training data can be generated\nusing multi-view stereo reconstruction. At inference time, our method uses\nmotion parallax cues from the static areas of the scenes to guide the depth\nprediction. We demonstrate our method on real-world sequences of complex human\nactions captured by a moving hand-held camera, show improvement over\nstate-of-the-art monocular depth prediction methods, and show various 3D\neffects produced using our predicted depth.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 01:10:24 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Li", "Zhengqi", ""], ["Dekel", "Tali", ""], ["Cole", "Forrester", ""], ["Tucker", "Richard", ""], ["Snavely", "Noah", ""], ["Liu", "Ce", ""], ["Freeman", "William T.", ""]]}, {"id": "1904.11112", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang", "title": "Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully data-driven method to compute depth from diverse monocular\nvideo sequences that contain large amounts of non-rigid objects, e.g., people.\nIn order to learn reconstruction cues for non-rigid scenes, we introduce a new\ndataset consisting of stereo videos scraped in-the-wild. This dataset has a\nwide variety of scene types, and features large amounts of nonrigid objects,\nespecially people. From this, we compute disparity maps to be used as\nsupervision to train our approach. We propose a loss function that allows us to\ngenerate a depth prediction even with unknown camera intrinsics and stereo\nbaselines in the dataset. We validate the use of large amounts of Internet\nvideo by evaluating our method on existing video datasets with depth\nsupervision, including SINTEL, and KITTI, and show that our approach\ngeneralizes better to natural scenes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 01:13:16 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Wang", "Chaoyang", ""], ["Lucey", "Simon", ""], ["Perazzi", "Federico", ""], ["Wang", "Oliver", ""]]}, {"id": "1904.11126", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Theus Aspiras, Tarek M. Taha, and Vijayan K. Asari", "title": "Skin Cancer Segmentation and Classification with NABLA-N and Inception\n  Recurrent Residual Convolutional Networks", "comments": "7 pages, 7 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last few years, Deep Learning (DL) has been showing superior\nperformance in different modalities of biomedical image analysis. Several DL\narchitectures have been proposed for classification, segmentation, and\ndetection tasks in medical imaging and computational pathology. In this paper,\nwe propose a new DL architecture, the NABLA-N network, with better feature\nfusion techniques in decoding units for dermoscopic image segmentation tasks.\nThe NABLA-N network has several advances for segmentation tasks. First, this\nmodel ensures better feature representation for semantic segmentation with a\ncombination of low to high-level feature maps. Second, this network shows\nbetter quantitative and qualitative results with the same or fewer network\nparameters compared to other methods. In addition, the Inception Recurrent\nResidual Convolutional Neural Network (IRRCNN) model is used for skin cancer\nclassification. The proposed NABLA-N network and IRRCNN models are evaluated\nfor skin cancer segmentation and classification on the benchmark datasets from\nthe International Skin Imaging Collaboration 2018 (ISIC-2018). The experimental\nresults show superior performance on segmentation tasks compared to the\nRecurrent Residual U-Net (R2U-Net). The classification model shows around 87%\ntesting accuracy for dermoscopic skin cancer classification on ISIC2018\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:24:55 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Aspiras", "Theus", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1904.11128", "submitter": "Yunxiang Zhao", "authors": "Yunxiang Zhao, Jianzhong Qi, Rui Zhang", "title": "CBHE: Corner-based Building Height Estimation for Complex Street Scene\n  Images", "comments": null, "journal-ref": null, "doi": "10.1145/3308558.3313394", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building height estimation is important in many applications such as 3D city\nreconstruction, urban planning, and navigation. Recently, a new building height\nestimation method using street scene images and 2D maps was proposed. This\nmethod is more scalable than traditional methods that use high-resolution\noptical data, LiDAR data, or RADAR data which are expensive to obtain. The\nmethod needs to detect building rooflines and then compute building height via\nthe pinhole camera model. We observe that this method has limitations in\nhandling complex street scene images in which buildings overlap with each other\nand the rooflines are difficult to locate. We propose CBHE, a building height\nestimation algorithm considering both building corners and rooflines. CBHE\nfirst obtains building corner and roofline candidates in street scene images\nbased on building footprints from 2D maps and the camera parameters. Then, we\nuse a deep neural network named BuildingNet to classify and filter corner and\nroofline candidates. Based on the valid corners and rooflines from BuildingNet,\nCBHE computes building height via the pinhole camera model. Experimental\nresults show that the proposed BuildingNet yields a higher accuracy on building\ncorner and roofline candidate filtering compared with the state-of-the-art open\nset classifiers. Meanwhile, CBHE outperforms the baseline algorithm by over 10%\nin building height estimation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:30:50 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhao", "Yunxiang", ""], ["Qi", "Jianzhong", ""], ["Zhang", "Rui", ""]]}, {"id": "1904.11136", "submitter": "Youshan Zhang", "authors": "Youshan Zhang", "title": "Corticospinal Tract (CST) reconstruction based on fiber orientation\n  distributions(FODs) tractography", "comments": null, "journal-ref": "2018 IEEE 18th International Conference on Bioinformatics and\n  Bioengineering (BIBE), Taichung, 2018, pp. 305-310", "doi": "10.1109/BIBE.2018.00066", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Corticospinal Tract (CST) is a part of pyramidal tract (PT), and it can\ninnervate the voluntary movement of skeletal muscle through spinal interneurons\n(the 4th layer of the Rexed gray board layers), and anterior horn motorneurons\n(which control trunk and proximal limb muscles). Spinal cord injury (SCI) is a\nhighly disabling disease often caused by traffic accidents. The recovery of CST\nand the functional reconstruction of spinal anterior horn motor neurons play an\nessential role in the treatment of SCI. However, the localization and\nreconstruction of CST are still challenging issues; the accuracy of the\ngeometric reconstruction can directly affect the results of the surgery. The\nmain contribution of this paper is the reconstruction of the CST based on the\nfiber orientation distributions (FODs) tractography. Differing from\ntensor-based tractography in which the primary direction is a determined\norientation, the direction of FODs tractography is determined by the\nprobability. The spherical harmonics (SPHARM) can be used to approximate the\nefficiency of FODs tractography. We manually delineate the three ROIs (the\nposterior limb of the internal capsule, the cerebral peduncle, and the anterior\npontine area) by the ITK-SNAP software, and use the pipeline software to\nreconstruct both the left and right sides of the CST fibers. Our results\ndemonstrate that FOD-based tractography can show more and correct anatomical\nCST fiber bundles.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:19:06 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Zhang", "Youshan", ""]]}, {"id": "1904.11138", "submitter": "XiaoBin Li", "authors": "XiaoBin Li, WeiQiang Wang", "title": "Learning Discriminative Features Via Weights-biased Softmax Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss functions play a key role in training superior deep neural networks. In\nconvolutional neural networks (CNNs), the popular cross entropy loss together\nwith softmax does not explicitly guarantee minimization of intra-class variance\nor maximization of inter-class variance. In the early studies, there is no\ntheoretical analysis and experiments explicitly indicating how to choose the\nnumber of units in fully connected layer. To help CNNs learn features more fast\nand discriminative, there are two contributions in this paper. First, we\ndetermine the minimum number of units in FC layer by rigorous theoretical\nanalysis and extensive experiment, which reduces CNNs' parameter memory and\ntraining time. Second, we propose a negative-focused weights-biased softmax\n(W-Softmax) loss to help CNNs learn more discriminative features. The proposed\nW-Softmax loss not only theoretically formulates the intraclass compactness and\ninter-class separability, but also can avoid overfitting by enlarging decision\nmargins. Moreover, the size of decision margins can be flexibly controlled by\nadjusting a hyperparameter $\\alpha$. Extensive experimental results on several\nbenchmark datasets show the superiority of W-Softmax in image classification\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 03:24:53 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Li", "XiaoBin", ""], ["Wang", "WeiQiang", ""]]}, {"id": "1904.11141", "submitter": "Yali Li", "authors": "Ya-Li Li and Shengjin Wang", "title": "HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object\n  Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2957850", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has been a challenging task in computer vision. Although\nsignificant progress has been made in object detection with deep neural\nnetworks, the attention mechanism is far from development. In this paper, we\npropose the hybrid attention mechanism for single-stage object detection.\nFirst, we present the modules of spatial attention, channel attention and\naligned attention for single-stage object detection. In particular, stacked\ndilated convolution layers with symmetrically fixed rates are constructed to\nlearn spatial attention. The channel attention is proposed with the cross-level\ngroup normalization and squeeze-and-excitation module. Aligned attention is\nconstructed with organized deformable filters. Second, the three kinds of\nattention are unified to construct the hybrid attention mechanism. We then\nembed the hybrid attention into Retina-Net and propose the efficient\nsingle-stage HAR-Net for object detection. The attention modules and the\nproposed HAR-Net are evaluated on the COCO detection dataset. Experiments\ndemonstrate that hybrid attention can significantly improve the detection\naccuracy and the HAR-Net can achieve the state-of-the-art 45.8\\% mAP,\noutperform existing single-stage object detectors.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 03:37:19 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Li", "Ya-Li", ""], ["Wang", "Shengjin", ""]]}, {"id": "1904.11150", "submitter": "Shan Li", "authors": "Shan Li and Weihong Deng", "title": "A Deeper Look at Facial Expression Dataset Bias", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing 2020", "doi": "10.1109/TAFFC.2020.2973158", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets play an important role in the progress of facial expression\nrecognition algorithms, but they may suffer from obvious biases caused by\ndifferent cultures and collection conditions. To look deeper into this bias, we\nfirst conduct comprehensive experiments on dataset recognition and crossdataset\ngeneralization tasks, and for the first time explore the intrinsic causes of\nthe dataset discrepancy. The results quantitatively verify that current\ndatasets have a strong buildin bias and corresponding analyses indicate that\nthe conditional probability distributions between source and target datasets\nare different. However, previous researches are mainly based on shallow\nfeatures with limited discriminative ability under the assumption that the\nconditional distribution remains unchanged across domains. To address these\nissues, we further propose a novel deep Emotion-Conditional Adaption Network\n(ECAN) to learn domain-invariant and discriminative feature representations,\nwhich can match both the marginal and the conditional distributions across\ndomains simultaneously. In addition, the largely ignored expression class\ndistribution bias is also addressed by a learnable re-weighting parameter, so\nthat the training and testing domains can share similar class distribution.\nExtensive cross-database experiments on both lab-controlled datasets (CK+,\nJAFFE, MMI and Oulu-CASIA) and real-world databases (AffectNet, FER2013, RAF-DB\n2.0 and SFEW 2.0) demonstrate that our ECAN can yield competitive performances\nacross various facial expression transfer tasks and outperform the\nstate-of-theart methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 04:07:13 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Li", "Shan", ""], ["Deng", "Weihong", ""]]}, {"id": "1904.11151", "submitter": "Kui Jia", "authors": "Kui Jia, Jiehong Lin, Mingkui Tan, and Dacheng Tao", "title": "Deep Multi-View Learning using Neuron-Wise Correlation-Maximizing\n  Regularizers", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2912356", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems concern with discovering or associating common\npatterns in data of multiple views or modalities. Multi-view learning is of the\nmethods to achieve such goals. Recent methods propose deep multi-view networks\nvia adaptation of generic Deep Neural Networks (DNNs), which concatenate\nfeatures of individual views at intermediate network layers (i.e., fusion\nlayers). In this work, we study the problem of multi-view learning in such\nend-to-end networks. We take a regularization approach via multi-view learning\ncriteria, and propose a novel, effective, and efficient neuron-wise\ncorrelation-maximizing regularizer. We implement our proposed regularizers\ncollectively as a correlation-regularized network layer (CorrReg). CorrReg can\nbe applied to either fully-connected or convolutional fusion layers, simply by\nreplacing them with their CorrReg counterparts. By partitioning neurons of a\nhidden layer in generic DNNs into multiple subsets, we also consider a\nmulti-view feature learning perspective of generic DNNs. Such a perspective\nenables us to study deep multi-view learning in the context of regularized\nnetwork training, for which we present control experiments of benchmark image\nclassification to show the efficacy of our proposed CorrReg. To investigate how\nCorrReg is useful for practical multi-view learning problems, we conduct\nexperiments of RGB-D object/scene recognition and multi-view based 3D object\nrecognition, using networks with fusion layers that concatenate intermediate\nfeatures of individual modalities or views for subsequent classification.\nApplying CorrReg to fusion layers of these networks consistently improves\nclassification performance. In particular, we achieve the new state of the art\non the benchmark RGB-D object and RGB-D scene datasets. We make the\nimplementation of CorrReg publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 04:10:10 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Jia", "Kui", ""], ["Lin", "Jiehong", ""], ["Tan", "Mingkui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.11157", "submitter": "Rohit Jena", "authors": "Rohit Jena", "title": "Out of the Box: A combined approach for handling occlusion in Human Pose\n  Estimation", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human Pose estimation is a challenging problem, especially in the case of 3D\npose estimation from 2D images due to many different factors like occlusion,\ndepth ambiguities, intertwining of people, and in general crowds. 2D\nmulti-person human pose estimation in the wild also suffers from the same\nproblems - occlusion, ambiguities, and disentanglement of people's body parts.\nBeing a fundamental problem with loads of applications, including but not\nlimited to surveillance, economical motion capture for video games and movies,\nand physiotherapy, this is an interesting problem to be solved both from a\npractical perspective and from an intellectual perspective as well. Although\nthere are cases where no pose estimation can ever predict with 100% accuracy\n(cases where even humans would fail), there are several algorithms that have\nbrought new state-of-the-art performance in human pose estimation in the wild.\nWe look at a few algorithms with different approaches and also formulate our\nown approach to tackle a consistently bugging problem, i.e. occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 05:10:18 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Jena", "Rohit", ""]]}, {"id": "1904.11163", "submitter": "Ravi Kumar Thakur", "authors": "Ravi Kumar Thakur and Snehasis Mukherjee", "title": "A Conditional Adversarial Network for Scene Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Scene flow estimation in depth videos has been attracting\nattention of researchers of robot vision, due to its potential application in\nvarious areas of robotics. The conventional scene flow methods are difficult to\nuse in reallife applications due to their long computational overhead. We\npropose a conditional adversarial network SceneFlowGAN for scene flow\nestimation. The proposed SceneFlowGAN uses loss function at two ends: both\ngenerator and descriptor ends. The proposed network is the first attempt to\nestimate scene flow using generative adversarial networks, and is able to\nestimate both the optical flow and disparity from the input stereo images\nsimultaneously. The proposed method is experimented on a large RGB-D benchmark\nsceneflow dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:03:06 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Thakur", "Ravi Kumar", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1904.11175", "submitter": "Brojeshwar Bhowmick", "authors": "Arindam Saha, Soumyadip Maity, Brojeshwar Bhowmick", "title": "Indoor dense depth map at drone hovering", "comments": "Published on ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Micro Aerial Vehicles (MAVs) gained tremendous attention in recent\nyears. Autonomous flight in indoor requires a dense depth map for navigable\nspace detection which is the fundamental component for autonomous navigation.\nIn this paper, we address the problem of reconstructing dense depth while a\ndrone is hovering (small camera motion) in indoor scenes using already\nestimated cameras and sparse point cloud obtained from a vSLAM. We start by\nsegmenting the scene based on sudden depth variation using sparse 3D points and\nintroduce a patch-based local plane fitting via energy minimization which\ncombines photometric consistency and co-planarity with neighbouring patches.\nThe method also combines a plane sweep technique for image segments having\nalmost no sparse point for initialization. Experiments show, the proposed\nmethod produces better depth for indoor in artificial lighting condition,\nlow-textured environment compared to earlier literature in small motion.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:48:08 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Saha", "Arindam", ""], ["Maity", "Soumyadip", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1904.11176", "submitter": "Soo Ye Kim", "authors": "Soo Ye Kim, Jihyong Oh, and Munchurl Kim", "title": "Deep SR-ITM: Joint Learning of Super-Resolution and Inverse Tone-Mapping\n  for 4K UHD HDR Applications", "comments": "Accepted at ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent modern displays are now able to render high dynamic range (HDR), high\nresolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently,\nUHD HDR broadcasting and streaming have emerged as high quality premium\nservices. However, due to the lack of original UHD HDR video content,\nappropriate conversion technologies are urgently needed to transform the legacy\nlow resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions.\nIn this paper, we propose a joint super-resolution (SR) and inverse\ntone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct\nmapping from LR SDR video to their HR HDR version. Joint SR and ITM is an\nintricate task, where high frequency details must be restored for SR, jointly\nwith the local contrast, for ITM. Our network is able to restore fine details\nby decomposing the input image and focusing on the separate base (low\nfrequency) and detail (high frequency) layers. Moreover, the proposed\nmodulation blocks apply location-variant operations to enhance local contrast.\nThe Deep SR-ITM shows good subjective quality with increased contrast and\ndetails, outperforming the previous joint SR-ITM method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:51:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 01:19:20 GMT"}, {"version": "v3", "created": "Sat, 31 Aug 2019 09:44:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kim", "Soo Ye", ""], ["Oh", "Jihyong", ""], ["Kim", "Munchurl", ""]]}, {"id": "1904.11187", "submitter": "Parth Shah", "authors": "Parth Shah, Vishvajit Bakrola, Supriya Pati", "title": "Optimal Approach for Image Recognition using Deep Convolutional\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent time deep learning has achieved huge popularity due to its\nperformance in various machine learning algorithms. Deep learning as\nhierarchical or structured learning attempts to model high level abstractions\nin data by using a group of processing layers. The foundation of deep learning\narchitectures is inspired by the understanding of information processing and\nneural responses in human brain. The architectures are created by stacking\nmultiple linear or non-linear operations. The article mainly focuses on the\nstate-of-art deep learning models and various real world applications specific\ntraining methods. Selecting optimal architecture for specific problem is a\nchallenging task, at a closing stage of the article we proposed optimal\napproach to deep convolutional architecture for the application of image\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 07:40:30 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Shah", "Parth", ""], ["Bakrola", "Vishvajit", ""], ["Pati", "Supriya", ""]]}, {"id": "1904.11227", "submitter": "Ting Yao", "authors": "Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, Tao Mei", "title": "Transferrable Prototypical Networks for Unsupervised Domain Adaptation", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new idea for unsupervised domain adaptation via\na remold of Prototypical Networks, which learn an embedding space and perform\nclassification via a remold of the distances to the prototype of each class.\nSpecifically, we present Transferrable Prototypical Networks (TPN) for\nadaptation such that the prototypes for each class in source and target domains\nare close in the embedding space and the score distributions predicted by\nprototypes separately on source and target data are similar. Technically, TPN\ninitially matches each target example to the nearest prototype in the source\ndomain and assigns an example a \"pseudo\" label. The prototype of each class\ncould then be computed on source-only, target-only and source-target data,\nrespectively. The optimization of TPN is end-to-end trained by jointly\nminimizing the distance across the prototypes on three types of data and\nKL-divergence of score distributions output by each pair of the prototypes.\nExtensive experiments are conducted on the transfers across MNIST, USPS and\nSVHN datasets, and superior results are reported when comparing to\nstate-of-the-art approaches. More remarkably, we obtain an accuracy of 80.4% of\nsingle model on VisDA 2017 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 09:21:16 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Li", "Yehao", ""], ["Wang", "Yu", ""], ["Ngo", "Chong-Wah", ""], ["Mei", "Tao", ""]]}, {"id": "1904.11238", "submitter": "Diego Ortego", "authors": "Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, Kevin\n  McGuinness", "title": "Unsupervised Label Noise Modeling and Loss Correction", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being robust to small amounts of label noise, convolutional neural\nnetworks trained with stochastic gradient methods have been shown to easily fit\nrandom labels. When there are a mixture of correct and mislabelled targets,\nnetworks tend to fit the former before the latter. This suggests using a\nsuitable two-component mixture model as an unsupervised generative model of\nsample loss values during training to allow online estimation of the\nprobability that a sample is mislabelled. Specifically, we propose a beta\nmixture to estimate this probability and correct the loss by relying on the\nnetwork prediction (the so-called bootstrapping loss). We further adapt mixup\naugmentation to drive our approach a step further. Experiments on CIFAR-10/100\nand TinyImageNet demonstrate a robustness to label noise that substantially\noutperforms recent state-of-the-art. Source code is available at\nhttps://git.io/fjsvE\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 09:53:48 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 14:23:24 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Arazo", "Eric", ""], ["Ortego", "Diego", ""], ["Albert", "Paul", ""], ["O'Connor", "Noel E.", ""], ["McGuinness", "Kevin", ""]]}, {"id": "1904.11245", "submitter": "Ting Yao", "authors": "Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu Duan, Ting Yao", "title": "Exploring Object Relation in Mean Teacher for Cross-Domain Detection", "comments": "CVPR 2019; The codes and model of our MTOR are publicly available at:\n  https://github.com/caiqi/mean-teacher-cross-domain-detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering synthetic data (e.g., 3D CAD-rendered images) to generate\nannotations for learning deep models in vision tasks has attracted increasing\nattention in recent years. However, simply applying the models learnt on\nsynthetic images may lead to high generalization error on real images due to\ndomain shift. To address this issue, recent progress in cross-domain\nrecognition has featured the Mean Teacher, which directly simulates\nunsupervised domain adaptation as semi-supervised learning. The domain gap is\nthus naturally bridged with consistency regularization in a teacher-student\nscheme. In this work, we advance this Mean Teacher paradigm to be applicable\nfor cross-domain detection. Specifically, we present Mean Teacher with Object\nRelations (MTOR) that novelly remolds Mean Teacher under the backbone of Faster\nR-CNN by integrating the object relations into the measure of consistency cost\nbetween teacher and student modules. Technically, MTOR firstly learns\nrelational graphs that capture similarities between pairs of regions for\nteacher and student respectively. The whole architecture is then optimized with\nthree consistency regularizations: 1) region-level consistency to align the\nregion-level predictions between teacher and student, 2) inter-graph\nconsistency for matching the graph structures between teacher and student, and\n3) intra-graph consistency to enhance the similarity between regions of same\nclass within the graph of student. Extensive experiments are conducted on the\ntransfers across Cityscapes, Foggy Cityscapes, and SIM10k, and superior results\nare reported when comparing to state-of-the-art approaches. More remarkably, we\nobtain a new record of single model: 22.8% of mAP on Syn2Real detection\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 10:03:44 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 05:20:30 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Cai", "Qi", ""], ["Pan", "Yingwei", ""], ["Ngo", "Chong-Wah", ""], ["Tian", "Xinmei", ""], ["Duan", "Lingyu", ""], ["Yao", "Ting", ""]]}, {"id": "1904.11251", "submitter": "Ting Yao", "authors": "Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, Tao Mei", "title": "Pointing Novel Objects in Image Captioning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has received significant attention with remarkable\nimprovements in recent advances. Nevertheless, images in the wild encapsulate\nrich knowledge and cannot be sufficiently described with models built on\nimage-caption pairs containing only in-domain objects. In this paper, we\npropose to address the problem by augmenting standard deep captioning\narchitectures with object learners. Specifically, we present Long Short-Term\nMemory with Pointing (LSTM-P) --- a new architecture that facilitates\nvocabulary expansion and produces novel objects via pointing mechanism.\nTechnically, object learners are initially pre-trained on available object\nrecognition data. Pointing in LSTM-P then balances the probability between\ngenerating a word through LSTM and copying a word from the recognized objects\nat each time step in decoder stage. Furthermore, our captioning encourages\nglobal coverage of objects in the sentence. Extensive experiments are conducted\non both held-out COCO image captioning and ImageNet datasets for describing\nnovel objects, and superior results are reported when comparing to\nstate-of-the-art approaches. More remarkably, we obtain an average of 60.9% in\nF1 score on held-out COCO~dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 10:22:35 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Chao", "Hongyang", ""], ["Mei", "Tao", ""]]}, {"id": "1904.11256", "submitter": "Diego Ortego", "authors": "Diego Ortego, Kevin McGuinness, Juan C. SanMiguel, Eric Arazo, Jos\\'e\n  M. Mart\\'inez, Noel E. O'Connor", "title": "On guiding video object segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for segmenting moving objects in\nunconstrained environments using guided convolutional neural networks. This\nguiding process relies on foreground masks from independent algorithms (i.e.\nstate-of-the-art algorithms) to implement an attention mechanism that\nincorporates the spatial location of foreground and background to compute their\nseparated representations. Our approach initially extracts two kinds of\nfeatures for each frame using colour and optical flow information. Such\nfeatures are combined following a multiplicative scheme to benefit from their\ncomplementarity. These unified colour and motion features are later processed\nto obtain the separated foreground and background representations. Then, both\nindependent representations are concatenated and decoded to perform foreground\nsegmentation. Experiments conducted on the challenging DAVIS 2016 dataset\ndemonstrate that our guided representations not only outperform non-guided, but\nalso recent and top-performing video object segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:03:16 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Ortego", "Diego", ""], ["McGuinness", "Kevin", ""], ["SanMiguel", "Juan C.", ""], ["Arazo", "Eric", ""], ["Mart\u00ednez", "Jos\u00e9 M.", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1904.11272", "submitter": "Guanzhi Wang", "authors": "Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, Chi-Keung Tang", "title": "LADN: Local Adversarial Disentangling Network for Facial Makeup and\n  De-Makeup", "comments": "Qiao and Guanzhi have equal contribution. Accepted to ICCV 2019.\n  Project website: https://georgegu1997.github.io/LADN-project-page/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a local adversarial disentangling network (LADN) for facial makeup\nand de-makeup. Central to our method are multiple and overlapping local\nadversarial discriminators in a content-style disentangling network for\nachieving local detail transfer between facial images, with the use of\nasymmetric loss functions for dramatic makeup styles with high-frequency\ndetails. Existing techniques do not demonstrate or fail to transfer\nhigh-frequency details in a global adversarial setting, or train a single local\ndiscriminator only to ensure image structure consistency and thus work only for\nrelatively simple styles. Unlike others, our proposed local adversarial\ndiscriminators can distinguish whether the generated local image details are\nconsistent with the corresponding regions in the given reference image in\ncross-image style transfer in an unsupervised setting. Incorporating these\ntechnical contributions, we achieve not only state-of-the-art results on\nconventional styles but also novel results involving complex and dramatic\nstyles with high-frequency details covering large areas across multiple facial\nfeatures. A carefully designed dataset of unpaired before and after makeup\nimages is released.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:52:06 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 07:31:36 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Gu", "Qiao", ""], ["Wang", "Guanzhi", ""], ["Chiu", "Mang Tik", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1904.11276", "submitter": "Axel Davy", "authors": "Axel Davy, Thibaud Ehret, Jean-Michel Morel, Mauricio Delbracio", "title": "Reducing Anomaly Detection in Images to Detection in Noise", "comments": "ICIP 2018", "journal-ref": "25th IEEE International Conference on Image Processing (2018)\n  1058-1062", "doi": "10.1109/ICIP.2018.8451059", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detectors address the difficult problem of detecting automatically\nexceptions in an arbitrary background image. Detection methods have been\nproposed by the thousands because each problem requires a different background\nmodel. By analyzing the existing approaches, we show that the problem can be\nreduced to detecting anomalies in residual images (extracted from the target\nimage) in which noise and anomalies prevail. Hence, the general and impossible\nbackground modeling problem is replaced by simpler noise modeling, and allows\nthe calculation of rigorous thresholds based on the a contrario detection\ntheory. Our approach is therefore unsupervised and works on arbitrary images.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:59:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Davy", "Axel", ""], ["Ehret", "Thibaud", ""], ["Morel", "Jean-Michel", ""], ["Delbracio", "Mauricio", ""]]}, {"id": "1904.11309", "submitter": "Zhidong Zhu", "authors": "Zhidong Zhu and Mingyi He and Yuchao Dai and Zhibo Rao and Bo Li", "title": "Multi-scale Cross-form Pyramid Network for Stereo Matching", "comments": "Accepted by ICIEA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching plays an indispensable part in autonomous driving, robotics\nand 3D scene reconstruction. We propose a novel deep learning architecture,\nwhich called CFP-Net, a Cross-Form Pyramid stereo matching network for\nregressing disparity from a rectified pair of stereo images. The network\nconsists of three modules: Multi-Scale 2D local feature extraction module,\nCross-form spatial pyramid module and Multi-Scale 3D Feature Matching and\nFusion module. The Multi-Scale 2D local feature extraction module can extract\nenough multi-scale features. The Cross-form spatial pyramid module aggregates\nthe context information in different scales and locations to form a cost\nvolume. Moreover, it is proved to be more effective than SPP and ASPP in\nill-posed regions. The Multi-Scale 3D feature matching and fusion module is\nproved to regularize the cost volume using two parallel 3D deconvolution\nstructure with two different receptive fields. Our proposed method has been\nevaluated on the Scene Flow and KITTI datasets. It achieves state-of-the-art\nperformance on the KITTI 2012 and 2015 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:02:01 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 08:35:46 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 05:44:34 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhu", "Zhidong", ""], ["He", "Mingyi", ""], ["Dai", "Yuchao", ""], ["Rao", "Zhibo", ""], ["Li", "Bo", ""]]}, {"id": "1904.11315", "submitter": "Hiroyuki Kobayashi", "authors": "Hiroyuki Kobayashi, Hitoshi Kiya", "title": "JPEG XT Image Compression with Hue Compensation for Two-Layer HDR Coding", "comments": "To appear in The 4th IEEE International Conference on Consumer\n  Electronics (ICCE) Asia, Bangkok Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel JPEG XT image compression with hue compensation for\ntwo-layer HDR coding. LDR images produced from JPEG XT bitstreams have some\ndistortion in hue due to tone mapping operations. In order to suppress the\ncolor distortion, we apply a novel hue compensation method based on the\nmaximally saturated colors. Moreover, the bitstreams generated by using the\nproposed method are fully compatible with the JPEG XT standard. In an\nexperiment, the proposed method is demonstrated not only to produce images with\nsmall hue degradation but also to maintain well-mapped luminance, in terms of\nthree kinds of criterion: TMQI, hue value in CIEDE2000, and the maximally\nsaturated color on the constant-hue plane.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:07:31 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1904.11319", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Evan Yu, Polina Golland, Bruce Fischl, Mert R.\n  Sabuncu, Juan Eugenio Iglesias", "title": "Unsupervised Deep Learning for Bayesian Brain MRI Segmentation", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic atlas priors have been commonly used to derive adaptive and\nrobust brain MRI segmentation algorithms. Widely-used neuroimage analysis\npipelines rely heavily on these techniques, which are often computationally\nexpensive. In contrast, there has been a recent surge of approaches that\nleverage deep learning to implement segmentation tools that are computationally\nefficient at test time. However, most of these strategies rely on learning from\nmanually annotated images. These supervised deep learning methods are therefore\nsensitive to the intensity profiles in the training dataset. To develop a deep\nlearning-based segmentation model for a new image dataset (e.g., of different\ncontrast), one usually needs to create a new labeled training dataset, which\ncan be prohibitively expensive, or rely on suboptimal ad hoc adaptation or\naugmentation approaches. In this paper, we propose an alternative strategy that\ncombines a conventional probabilistic atlas-based segmentation with deep\nlearning, enabling one to train a segmentation model for new MRI scans without\nthe need for any manually segmented images. Our experiments include thousands\nof brain MRI scans and demonstrate that the proposed method achieves good\naccuracy for a brain MRI segmentation task for different MRI contrasts,\nrequiring only approximately 15 seconds at test time on a GPU. The code is\nfreely available at http://voxelmorph.mit.edu.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:13:51 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 17:26:09 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Yu", "Evan", ""], ["Golland", "Polina", ""], ["Fischl", "Bruce", ""], ["Sabuncu", "Mert R.", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "1904.11322", "submitter": "Zhihao Fang", "authors": "Zhihao Fang, Wanyi Zhang and He Ma", "title": "Breast Cancer Classification with Ultrasound Images Based on SLIC", "comments": "This is a pre-print of a contribution published in Frontier Computing\n  - Theory, Technologies and Applications (FC 2019) published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound image diagnosis of breast tumors has been widely used in recent\nyears. However, there are some problems of it, for instance, poor quality,\nintense noise and uneven echo distribution, which has created a huge obstacle\nto diagnosis. To overcome these problems, we propose a novel method, a breast\ncancer classification with ultrasound images based on SLIC (BCCUI). We first\nutilize the Region of Interest (ROI) extraction based on Simple Linear\nIterative Clustering (SLIC) algorithm and region growing algorithm to extract\nthe ROI at the super-pixel level. Next, the features of ROI are extracted.\nFurthermore, the Support Vector Machine (SVM) classifier is applied. The\ncalculation states that the accuracy of this segment algorithm is up to 88.00%\nand the sensitivity of the algorithm is up to 92.05%, which proves that the\nclassifier presents in this paper has certain research meaning and applied\nworthiness.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:23:29 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 10:39:11 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Fang", "Zhihao", ""], ["Zhang", "Wanyi", ""], ["Ma", "He", ""]]}, {"id": "1904.11397", "submitter": "Leuleseged Alemu", "authors": "Leulseged Tesfaye Alemu, Marcello Pelillo, Mubarak Shah", "title": "Deep Constrained Dominant Sets for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an end-to-end constrained clustering scheme to\ntackle the person re-identification (re-id) problem. Deep neural networks (DNN)\nhave recently proven to be effective on person re-identification task. In\nparticular, rather than leveraging solely a probe-gallery similarity, diffusing\nthe similarities among the gallery images in an end-to-end manner has proven to\nbe effective in yielding a robust probe-gallery affinity. However, existing\nmethods do not apply probe image as a constraint, and are prone to noise\npropagation during the similarity diffusion process. To overcome this, we\npropose an intriguing scheme which treats person-image retrieval problem as a\n{\\em constrained clustering optimization} problem, called deep constrained\ndominant sets (DCDS). Given a probe and gallery images, we re-formulate person\nre-id problem as finding a constrained cluster, where the probe image is taken\nas a constraint (seed) and each cluster corresponds to a set of images\ncorresponding to the same person. By optimizing the constrained clustering in\nan end-to-end manner, we naturally leverage the contextual knowledge of a set\nof images corresponding to the given person-images. We further enhance the\nperformance by integrating an auxiliary net alongside DCDS, which employs a\nmulti-scale Resnet. To validate the effectiveness of our method we present\nexperiments on several benchmark datasets and show that the proposed method can\noutperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:07:13 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 21:27:08 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Alemu", "Leulseged Tesfaye", ""], ["Pelillo", "Marcello", ""], ["Shah", "Mubarak", ""]]}, {"id": "1904.11407", "submitter": "Ali Diba", "authors": "Ali Diba, Vivek Sharma, Luc Van Gool, Rainer Stiefelhagen", "title": "DynamoNet: Dynamic Action and Motion Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in self-supervised learning the motion cues\nin videos using dynamic motion filters for a better motion representation to\nfinally boost human action recognition in particular. Thus far, the vision\ncommunity has focused on spatio-temporal approaches using standard filters,\nrather we here propose dynamic filters that adaptively learn the video-specific\ninternal motion representation by predicting the short-term future frames. We\nname this new motion representation, as dynamic motion representation (DMR) and\nis embedded inside of 3D convolutional network as a new layer, which captures\nthe visual appearance and motion dynamics throughout entire video clip via\nend-to-end network learning. Simultaneously, we utilize these motion\nrepresentation to enrich video classification. We have designed the frame\nprediction task as an auxiliary task to empower the classification problem.\nWith these overall objectives, to this end, we introduce a novel unified\nspatio-temporal 3D-CNN architecture (DynamoNet) that jointly optimizes the\nvideo classification and learning motion representation by predicting future\nframes as a multi-task learning problem. We conduct experiments on challenging\nhuman action datasets: Kinetics 400, UCF101, HMDB51. The experiments using the\nproposed DynamoNet show promising results on all the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:27:12 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Diba", "Ali", ""], ["Sharma", "Vivek", ""], ["Van Gool", "Luc", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1904.11451", "submitter": "Ali Diba", "authors": "Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri, Jurgen Gall,\n  Rainer Stiefelhagen, Luc Van Gool", "title": "Large Scale Holistic Video Understanding", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video recognition has been advanced in recent years by benchmarks with rich\nannotations. However, research is still mainly limited to human action or\nsports recognition - focusing on a highly specific video understanding task and\nthus leaving a significant gap towards describing the overall content of a\nvideo. We fill this gap by presenting a large-scale \"Holistic Video\nUnderstanding Dataset\"~(HVU). HVU is organized hierarchically in a semantic\ntaxonomy that focuses on multi-label and multi-task video understanding as a\ncomprehensive problem that encompasses the recognition of multiple semantic\naspects in the dynamic scene. HVU contains approx.~572k videos in total with 9\nmillion annotations for training, validation, and test set spanning over 3142\nlabels. HVU encompasses semantic aspects defined on categories of scenes,\nobjects, actions, events, attributes, and concepts which naturally captures the\nreal-world scenarios.\n  We demonstrate the generalization capability of HVU on three challenging\ntasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering\ntasks. In particular for video classification, we introduce a new\nspatio-temporal deep neural network architecture called \"Holistic Appearance\nand Temporal Network\"~(HATNet) that builds on fusing 2D and 3D architectures\ninto one by combining intermediate representations of appearance and temporal\ncues. HATNet focuses on the multi-label and multi-task learning problem and is\ntrained in an end-to-end manner. Via our experiments, we validate the idea that\nholistic representation learning is complementary, and can play a key role in\nenabling many real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:49:13 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 15:47:12 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 06:53:39 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Diba", "Ali", ""], ["Fayyaz", "Mohsen", ""], ["Sharma", "Vivek", ""], ["Paluri", "Manohar", ""], ["Gall", "Jurgen", ""], ["Stiefelhagen", "Rainer", ""], ["Van Gool", "Luc", ""]]}, {"id": "1904.11466", "submitter": "Ankit Laddha", "authors": "Gregory P. Meyer, Jake Charland, Darshan Hegde, Ankit Laddha, Carlos\n  Vallespi-Gonzalez", "title": "Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation", "comments": "Accepted for publication at CVPR Workshop on Autonomous Driving 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an extension to LaserNet, an efficient and\nstate-of-the-art LiDAR based 3D object detector. We propose a method for fusing\nimage data with the LiDAR data and show that this sensor fusion method improves\nthe detection performance of the model especially at long ranges. The addition\nof image data is straightforward and does not require image labels.\nFurthermore, we expand the capabilities of the model to perform 3D semantic\nsegmentation in addition to 3D object detection. On a large benchmark dataset,\nwe demonstrate our approach achieves state-of-the-art performance on both\nobject detection and semantic segmentation while maintaining a low runtime.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:20:31 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Meyer", "Gregory P.", ""], ["Charland", "Jake", ""], ["Hegde", "Darshan", ""], ["Laddha", "Ankit", ""], ["Vallespi-Gonzalez", "Carlos", ""]]}, {"id": "1904.11476", "submitter": "Sarah Cen", "authors": "Sarah H. Cen and Paul Newman", "title": "Radar-only ego-motion estimation in difficult settings via graph\n  matching", "comments": "6 content pages, 1 page of references, 5 figures, 4 tables, 2019 IEEE\n  International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar detects stable, long-range objects under variable weather and lighting\nconditions, making it a reliable and versatile sensor well suited for\nego-motion estimation. In this work, we propose a radar-only odometry pipeline\nthat is highly robust to radar artifacts (e.g., speckle noise and false\npositives) and requires only one input parameter. We demonstrate its ability to\nadapt across diverse settings, from urban UK to off-road Iceland, achieving a\nscan matching accuracy of approximately 5.20 cm and 0.0929 deg when using GPS\nas ground truth (compared to visual odometry's 5.77 cm and 0.1032 deg). We\npresent algorithms for keypoint extraction and data association, framing the\nlatter as a graph matching optimization problem, and provide an in-depth system\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:40:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cen", "Sarah H.", ""], ["Newman", "Paul", ""]]}, {"id": "1904.11486", "submitter": "Richard Zhang", "authors": "Richard Zhang", "title": "Making Convolutional Networks Shift-Invariant Again", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern convolutional networks are not shift-invariant, as small input shifts\nor translations can cause drastic changes in the output. Commonly used\ndownsampling methods, such as max-pooling, strided-convolution, and\naverage-pooling, ignore the sampling theorem. The well-known signal processing\nfix is anti-aliasing by low-pass filtering before downsampling. However, simply\ninserting this module into deep networks degrades performance; as a result, it\nis seldomly used today. We show that when integrated correctly, it is\ncompatible with existing architectural components, such as max-pooling and\nstrided-convolution. We observe \\textit{increased accuracy} in ImageNet\nclassification, across several commonly-used architectures, such as ResNet,\nDenseNet, and MobileNet, indicating effective regularization. Furthermore, we\nobserve \\textit{better generalization}, in terms of stability and robustness to\ninput corruptions. Our results demonstrate that this classical signal\nprocessing technique has been undeservingly overlooked in modern deep networks.\nCode and anti-aliased versions of popular networks are available at\nhttps://richzhang.github.io/antialiased-cnns/ .\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:56:21 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 00:27:38 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhang", "Richard", ""]]}, {"id": "1904.11487", "submitter": "Evan Shelhamer", "authors": "Evan Shelhamer, Dequan Wang, Trevor Darrell", "title": "Blurring the Line Between Structure and Learning to Optimize and Adapt\n  Receptive Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual world is vast and varied, but its variations divide into\nstructured and unstructured factors. We compose free-form filters and\nstructured Gaussian filters, optimized end-to-end, to factorize deep\nrepresentations and learn both local features and their degree of locality. Our\nsemi-structured composition is strictly more expressive than free-form\nfiltering, and changes in its structured parameters would require changes in\nfree-form architecture. In effect this optimizes over receptive field size and\nshape, tuning locality to the data and task. Dynamic inference, in which the\nGaussian structure varies with the input, adapts receptive field size to\ncompensate for local scale variation. Optimizing receptive field size improves\nsemantic segmentation accuracy on Cityscapes by 1-2 points for strong dilated\nand skip architectures and by up to 10 points for suboptimal designs. Adapting\nreceptive fields by dynamic Gaussian structure further improves results,\nequaling the accuracy of free-form deformation while improving efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:57:10 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Shelhamer", "Evan", ""], ["Wang", "Dequan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1904.11489", "submitter": "Yue Cao", "authors": "Jiarui Xu and Yue Cao and Zheng Zhang and Han Hu", "title": "Spatial-Temporal Relation Networks for Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in multiple object tracking (MOT) has shown that a robust\nsimilarity score is key to the success of trackers. A good similarity score is\nexpected to reflect multiple cues, e.g. appearance, location, and topology,\nover a long period of time. However, these cues are heterogeneous, making them\nhard to be combined in a unified network. As a result, existing methods usually\nencode them in separate networks or require a complex training approach. In\nthis paper, we present a unified framework for similarity measurement which\ncould simultaneously encode various cues and perform reasoning across both\nspatial and temporal domains. We also study the feature representation of a\ntracklet-object pair in depth, showing a proper design of the pair features can\nwell empower the trackers. The resulting approach is named spatial-temporal\nrelation networks (STRN). It runs in a feed-forward way and can be trained in\nan end-to-end manner. The state-of-the-art accuracy was achieved on all of the\nMOT15-17 benchmarks using public detection and online settings.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:59:17 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Xu", "Jiarui", ""], ["Cao", "Yue", ""], ["Zhang", "Zheng", ""], ["Hu", "Han", ""]]}, {"id": "1904.11490", "submitter": "Han Hu", "authors": "Ze Yang and Shaohui Liu and Han Hu and Liwei Wang and Stephen Lin", "title": "RepPoints: Point Set Representation for Object Detection", "comments": "International Conference on Computer Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern object detectors rely heavily on rectangular bounding boxes, such as\nanchors, proposals and the final predictions, to represent objects at various\nrecognition stages. The bounding box is convenient to use but provides only a\ncoarse localization of objects and leads to a correspondingly coarse extraction\nof object features. In this paper, we present \\textbf{RepPoints}\n(representative points), a new finer representation of objects as a set of\nsample points useful for both localization and recognition. Given ground truth\nlocalization and recognition targets for training, RepPoints learn to\nautomatically arrange themselves in a manner that bounds the spatial extent of\nan object and indicates semantically significant local areas. They furthermore\ndo not require the use of anchors to sample a space of bounding boxes. We show\nthat an anchor-free object detector based on RepPoints can be as effective as\nthe state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4\n$AP_{50}$ on the COCO test-dev detection benchmark, using ResNet-101 model.\nCode is available at https://github.com/microsoft/RepPoints.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:59:28 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 14:12:18 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yang", "Ze", ""], ["Liu", "Shaohui", ""], ["Hu", "Han", ""], ["Wang", "Liwei", ""], ["Lin", "Stephen", ""]]}, {"id": "1904.11491", "submitter": "Han Hu", "authors": "Han Hu and Zheng Zhang and Zhenda Xie and Stephen Lin", "title": "Local Relation Networks for Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolution layer has been the dominant feature extractor in computer\nvision for years. However, the spatial aggregation in convolution is basically\na pattern matching process that applies fixed filters which are inefficient at\nmodeling visual elements with varying spatial distributions. This paper\npresents a new image feature extractor, called the local relation layer, that\nadaptively determines aggregation weights based on the compositional\nrelationship of local pixel pairs. With this relational approach, it can\ncomposite visual elements into higher-level entities in a more efficient manner\nthat benefits semantic inference. A network built with local relation layers,\ncalled the Local Relation Network (LR-Net), is found to provide greater\nmodeling capacity than its counterpart built with regular convolution on\nlarge-scale recognition tasks such as ImageNet classification.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:59:35 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hu", "Han", ""], ["Zhang", "Zheng", ""], ["Xie", "Zhenda", ""], ["Lin", "Stephen", ""]]}, {"id": "1904.11492", "submitter": "Yue Cao", "authors": "Yue Cao and Jiarui Xu and Stephen Lin and Fangyun Wei and Han Hu", "title": "GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Non-Local Network (NLNet) presents a pioneering approach for capturing\nlong-range dependencies, via aggregating query-specific global context to each\nquery position. However, through a rigorous empirical analysis, we have found\nthat the global contexts modeled by non-local network are almost the same for\ndifferent query positions within an image. In this paper, we take advantage of\nthis finding to create a simplified network based on a query-independent\nformulation, which maintains the accuracy of NLNet but with significantly less\ncomputation. We further observe that this simplified design shares similar\nstructure with Squeeze-Excitation Network (SENet). Hence we unify them into a\nthree-step general framework for global context modeling. Within the general\nframework, we design a better instantiation, called the global context (GC)\nblock, which is lightweight and can effectively model the global context. The\nlightweight property allows us to apply it for multiple layers in a backbone\nnetwork to construct a global context network (GCNet), which generally\noutperforms both simplified NLNet and SENet on major benchmarks for various\nrecognition tasks. The code and configurations are released at\nhttps://github.com/xvjiarui/GCNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:59:42 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cao", "Yue", ""], ["Xu", "Jiarui", ""], ["Lin", "Stephen", ""], ["Wei", "Fangyun", ""], ["Hu", "Han", ""]]}, {"id": "1904.11521", "submitter": "Kritaphat Songsri-In", "authors": "Kritaphat Songsri-in, Stefanos Zafeiriou", "title": "Face Video Generation from a Single Image and Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are concerned with the challenging problem of producing a\nfull image sequence of a deformable face given only an image and generic facial\nmotions encoded by a set of sparse landmarks. To this end we build upon recent\nbreakthroughs in image-to-image translation such as pix2pix, CycleGAN and\nStarGAN which learn Deep Convolutional Neural Networks (DCNNs) that learn to\nmap aligned pairs or images between different domains (i.e., having different\nlabels) and propose a new architecture which is not driven any more by labels\nbut by spatial maps, facial landmarks. In particular, we propose the MotionGAN\nwhich transforms an input face image into a new one according to a heatmap of\ntarget landmarks. We show that it is possible to create very realistic face\nvideos using a single image and a set of target landmarks. Furthermore, our\nmethod can be used to edit a facial image with arbitrary motions according to\nlandmarks (e.g., expression, speech, etc.). This provides much more flexibility\nto face editing, expression transfer, facial video creation, etc. than models\nbased on discrete expressions, audios or action units.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 18:10:27 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Songsri-in", "Kritaphat", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1904.11567", "submitter": "Jiabo Huang", "authors": "Jiabo Huang, Qi Dong, Shaogang Gong and Xiatian Zhu", "title": "Unsupervised Deep Learning by Neighbourhood Discovery", "comments": "36th International Conference on Machine Learning (ICML'19). Code is\n  available at https://github.com/Raymond-sci/AND", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have demonstrated remarkable\nsuccess in computer vision by supervisedly learning strong visual feature\nrepresentations. However, training CNNs relies heavily on the availability of\nexhaustive training data annotations, limiting significantly their deployment\nand scalability in many application scenarios. In this work, we introduce a\ngeneric unsupervised deep learning approach to training deep models without the\nneed for any manual label supervision. Specifically, we progressively discover\nsample anchored/centred neighbourhoods to reason and learn the underlying class\ndecision boundaries iteratively and accumulatively. Every single neighbourhood\nis specially formulated so that all the member samples can share the same\nunseen class labels at high probability for facilitating the extraction of\nclass discriminative feature representations during training. Experiments on\nimage classification show the performance advantages of the proposed method\nover the state-of-the-art unsupervised learning models on six benchmarks\nincluding both coarse-grained and fine-grained object image categorisation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:09:00 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 13:24:35 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 15:26:41 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Huang", "Jiabo", ""], ["Dong", "Qi", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""]]}, {"id": "1904.11574", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering", "comments": "ACL 2020 camera-ready (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the task of Spatio-Temporal Video Question Answering, which\nrequires intelligent systems to simultaneously retrieve relevant moments and\ndetect referenced visual concepts (people and objects) to answer natural\nlanguage questions about videos. We first augment the TVQA dataset with 310.8K\nbounding boxes, linking depicted objects to visual concepts in questions and\nanswers. We name this augmented version as TVQA+. We then propose\nSpatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework\nthat grounds evidence in both spatial and temporal domains to answer questions\nabout videos. Comprehensive experiments and analyses demonstrate the\neffectiveness of our framework and how the rich annotations in our TVQA+\ndataset can contribute to the question answering task. Moreover, by performing\nthis joint task, our model is able to produce insightful and interpretable\nspatio-temporal attention visualizations. Dataset and code are publicly\navailable at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:37:26 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 19:43:42 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.11578", "submitter": "Guo YuHu", "authors": "Yuhu Guo, Han Xiao, Yidong Chen, Xiaodong Shi", "title": "Asynchronous \"Events\" are Better For Motion Estimation", "comments": "Submitted at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based camera is a bio-inspired vision sensor that records intensity\nchanges (called event) asynchronously in each pixel. As an instance of\nevent-based camera, Dynamic and Active-pixel Vision Sensor (DAVIS) combines a\nstandard camera and an event-based camera. However, traditional models could\nnot deal with the event stream asynchronously. To analyze the event stream\nasynchronously, most existing approaches accumulate events within a certain\ntime interval and treat the accumulated events as a synchronous frame, which\nwastes the intensity change information and weakens the advantages of DAVIS.\nTherefore, in this paper, we present the first neural asynchronous approach to\nprocess event stream for event-based camera. Our method asynchronously extracts\ndynamic information from events by leveraging previous motion and critical\nfeatures of gray-scale frames. To our best knowledge, this is the first neural\nasynchronous method to analyze event stream through a novel deep neural\nnetwork. Extensive experiments demonstrate that our proposed model achieves\nremarkable improvements against the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:10:10 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Guo", "Yuhu", ""], ["Xiao", "Han", ""], ["Chen", "Yidong", ""], ["Shi", "Xiaodong", ""]]}, {"id": "1904.11587", "submitter": "Binghan Li", "authors": "Binghan Li, Wenrui Zhang, Mi Lu", "title": "Multiple Linear Regression Haze-removal Model Based on Dark Channel\n  Prior", "comments": "IEEE CPS (CSCI 2018 Int'l Conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dark Channel Prior (DCP) is a widely recognized traditional dehazing\nalgorithm. However, it may fail in bright region and the brightness of the\nrestored image is darker than hazy image. In this paper, we propose an\neffective method to optimize DCP. We build a multiple linear regression\nhaze-removal model based on DCP atmospheric scattering model and train this\nmodel with RESIDE dataset, which aims to reduce the unexpected errors caused by\nthe rough estimations of transmission map t(x) and atmospheric light A. The\nRESIDE dataset provides enough synthetic hazy images and their corresponding\ngroundtruth images to train and test. We compare the performances of different\ndehazing algorithms in terms of two important full-reference metrics, the\npeak-signal-to-noise ratio (PSNR) as well as the structural similarity index\nmeasure (SSIM). The experiment results show that our model gets highest SSIM\nvalue and its PSNR value is also higher than most of state-of-the-art dehazing\nalgorithms. Our results also overcome the weakness of DCP on real-world hazy\nimages\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:00:33 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Li", "Binghan", ""], ["Zhang", "Wenrui", ""], ["Lu", "Mi", ""]]}, {"id": "1904.11592", "submitter": "Ioan Marius Bilasco PhD", "authors": "Benjamin Allaert and Isaac Ronald Ward and Ioan Marius Bilasco and\n  Chaabane Djeraba and Mohammed Bennamoun", "title": "Optical Flow Techniques for Facial Expression Analysis -- a Practical\n  Evaluation Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow techniques are becoming increasingly performant and robust when\nestimating motion in a scene, but their performance has yet to be proven in the\narea of facial expression recognition. In this work, a variety of optical flow\napproaches are evaluated across multiple facial expression datasets, so as to\nprovide a consistent performance evaluation. The aim of this work is not to\npropose a new expression recognition technique, but to understand better the\nadequacy of existing state-of-the art optical flow for encoding facial motion\nin the context of facial expression recognition. Our evaluations highlight the\nfact that motion approximation methods used to overcome motion discontinuities\nhave a significant impact when optical flows are used to characterize facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:14:13 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 23:40:39 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Allaert", "Benjamin", ""], ["Ward", "Isaac Ronald", ""], ["Bilasco", "Ioan Marius", ""], ["Djeraba", "Chaabane", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1904.11595", "submitter": "Ameya Phalak", "authors": "Ameya Phalak, Zhao Chen, Darvin Yi, Khushi Gupta, Vijay Badrinarayanan\n  and Andrew Rabinovich", "title": "DeepPerimeter: Indoor Boundary Estimation from Posed Monocular Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepPerimeter, a deep learning based pipeline for inferring a full\nindoor perimeter (i.e. exterior boundary map) from a sequence of posed RGB\nimages. Our method relies on robust deep methods for depth estimation and wall\nsegmentation to generate an exterior boundary point cloud, and then uses deep\nunsupervised clustering to fit wall planes to obtain a final boundary map of\nthe room. We demonstrate that DeepPerimeter results in excellent visual and\nquantitative performance on the popular ScanNet and FloorNet datasets and works\nfor room shapes of various complexities as well as in multiroom scenarios. We\nalso establish important baselines for future work on indoor perimeter\nestimation, topics which will become increasingly prevalent as application\nareas like augmented reality and robotics become more significant.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:20:41 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 21:52:11 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Phalak", "Ameya", ""], ["Chen", "Zhao", ""], ["Yi", "Darvin", ""], ["Gupta", "Khushi", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1904.11617", "submitter": "Ming Li", "authors": "Ming Li, Chunyang Ye, Wei Li", "title": "High-Resolution Network for Photorealistic Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic style transfer aims to transfer the style of one image to\nanother, but preserves the original structure and detail outline of the content\nimage, which makes the content image still look like a real shot after the\nstyle transfer. Although some realistic image styling methods have been\nproposed, these methods are vulnerable to lose the details of the content image\nand produce some irregular distortion structures. In this paper, we use a\nhigh-resolution network as the image generation network. Compared to other\nmethods, which reduce the resolution and then restore the high resolution, our\ngeneration network maintains high resolution throughout the process. By\nconnecting high-resolution subnets to low-resolution subnets in parallel and\nrepeatedly multi-scale fusion, high-resolution subnets can continuously receive\ninformation from low-resolution subnets. This allows our network to discard\nless information contained in the image, so the generated images may have a\nmore elaborate structure and less distortion, which is crucial to the visual\nquality. We conducted extensive experiments and compared the results with\nexisting methods. The experimental results show that our model is effective and\nproduces better results than existing methods for photorealistic image\nstylization. Our source code with PyTorch framework will be publicly available\nat https://github.com/limingcv/Photorealistic-Style-Transfer\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 22:59:37 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Li", "Ming", ""], ["Ye", "Chunyang", ""], ["Li", "Wei", ""]]}, {"id": "1904.11619", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Luan Nguyen, Tuan Nguyen, Doyoung Kim, Sarah Eldin,\n  Alexander Huyen, Thomas Lu, Edward Chow", "title": "Small Target Detection for Search and Rescue Operations using\n  Distributed Deep Learning and Synthetic Data Generation", "comments": "6 pages, 4 figures, SPIE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to find the target as soon as possible for search and rescue\noperations. Surveillance camera systems and unmanned aerial vehicles (UAVs) are\nused to support search and rescue. Automatic object detection is important\nbecause a person cannot monitor multiple surveillance screens simultaneously\nfor 24 hours. Also, the object is often too small to be recognized by the human\neye on the surveillance screen. This study used UAVs around the Port of Houston\nand fixed surveillance cameras to build an automatic target detection system\nthat supports the US Coast Guard (USCG) to help find targets (e.g., person\noverboard). We combined image segmentation, enhancement, and convolution neural\nnetworks to reduce detection time to detect small targets. We compared the\nperformance between the auto-detection system and the human eye. Our system\ndetected the target within 8 seconds, but the human eye detected the target\nwithin 25 seconds. Our systems also used synthetic data generation and data\naugmentation techniques to improve target detection accuracy. This solution may\nhelp the search and rescue operations of the first responders in a timely\nmanner.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:10:54 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Yun", "Kyongsik", ""], ["Nguyen", "Luan", ""], ["Nguyen", "Tuan", ""], ["Kim", "Doyoung", ""], ["Eldin", "Sarah", ""], ["Huyen", "Alexander", ""], ["Lu", "Thomas", ""], ["Chow", "Edward", ""]]}, {"id": "1904.11620", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Kevin Yu, Joseph Osborne, Sarah Eldin, Luan Nguyen,\n  Alexander Huyen, Thomas Lu", "title": "Improved visible to IR image transformation using synthetic data\n  augmentation with cycle-consistent adversarial networks", "comments": "8 pages, 6 figures, SPIE", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) images are essential to improve the visibility of dark or\ncamouflaged objects. Object recognition and segmentation based on a neural\nnetwork using IR images provide more accuracy and insight than color visible\nimages. But the bottleneck is the amount of relevant IR images for training. It\nis difficult to collect real-world IR images for special purposes, including\nspace exploration, military and fire-fighting applications. To solve this\nproblem, we created color visible and IR images using a Unity-based 3D game\neditor. These synthetically generated color visible and IR images were used to\ntrain cycle consistent adversarial networks (CycleGAN) to convert visible\nimages to IR images. CycleGAN has the advantage that it does not require\nprecisely matching visible and IR pairs for transformation training. In this\nstudy, we discovered that additional synthetic data can help improve CycleGAN\nperformance. Neural network training using real data (N = 20) performed more\naccurate transformations than training using real (N = 10) and synthetic (N =\n10) data combinations. The result indicates that the synthetic data cannot\nexceed the quality of the real data. Neural network training using real (N =\n10) and synthetic (N = 100) data combinations showed almost the same\nperformance as training using real data (N = 20). At least 10 times more\nsynthetic data than real data is required to achieve the same performance. In\nsummary, CycleGAN is used with synthetic data to improve the IR image\nconversion performance of visible images.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:12:52 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Yun", "Kyongsik", ""], ["Yu", "Kevin", ""], ["Osborne", "Joseph", ""], ["Eldin", "Sarah", ""], ["Nguyen", "Luan", ""], ["Huyen", "Alexander", ""], ["Lu", "Thomas", ""]]}, {"id": "1904.11621", "submitter": "Amlan Kar", "authors": "Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan,\n  Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler", "title": "Meta-Sim: Learning to Generate Synthetic Datasets", "comments": "Webpage: https://nv-tlabs.github.io/meta-sim/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training models to high-end performance requires availability of large\nlabeled datasets, which are expensive to get. The goal of our work is to\nautomatically synthesize labeled datasets that are relevant for a downstream\ntask. We propose Meta-Sim, which learns a generative model of synthetic scenes,\nand obtain images as well as its corresponding ground-truth via a graphics\nengine. We parametrize our dataset generator with a neural network, which\nlearns to modify attributes of scene graphs obtained from probabilistic scene\ngrammars, so as to minimize the distribution gap between its rendered outputs\nand target data. If the real dataset comes with a small labeled validation set,\nwe additionally aim to optimize a meta-objective, i.e. downstream task\nperformance. Experiments show that the proposed method can greatly improve\ncontent generation quality over a human-engineered probabilistic scene grammar,\nboth qualitatively and quantitatively as measured by performance on a\ndownstream task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:18:36 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kar", "Amlan", ""], ["Prakash", "Aayush", ""], ["Liu", "Ming-Yu", ""], ["Cameracci", "Eric", ""], ["Yuan", "Justin", ""], ["Rusiniak", "Matt", ""], ["Acuna", "David", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1904.11622", "submitter": "Vincent Chen", "authors": "Vincent S. Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein,\n  Christopher Re, Li Fei-Fei", "title": "Scene Graph Prediction with Limited Labels", "comments": "ICCV 2019, 10 pages, 9 figures", "journal-ref": "International Conference on Computer Vision, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual knowledge bases such as Visual Genome power numerous applications in\ncomputer vision, including visual question answering and captioning, but suffer\nfrom sparse, incomplete relationships. All scene graph models to date are\nlimited to training on a small set of visual relationships that have thousands\nof training labels each. Hiring human annotators is expensive, and using\ntextual knowledge base completion methods are incompatible with visual data. In\nthis paper, we introduce a semi-supervised method that assigns probabilistic\nrelationship labels to a large number of unlabeled images using few labeled\nexamples. We analyze visual relationships to suggest two types of\nimage-agnostic features that are used to generate noisy heuristics, whose\noutputs are aggregated using a factor graph-based generative model. With as few\nas 10 labeled examples per relationship, the generative model creates enough\ntraining data to train any existing state-of-the-art scene graph model. We\ndemonstrate that our method outperforms all baseline approaches on scene graph\nprediction by 5.16 recall@100 for PREDCLS. In our limited label setting, we\ndefine a complexity metric for relationships that serves as an indicator (R^2 =\n0.778) for conditions under which our method succeeds over transfer learning,\nthe de-facto approach for training with limited labels.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:26:25 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 04:58:45 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 21:52:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Chen", "Vincent S.", ""], ["Varma", "Paroma", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Re", "Christopher", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1904.11634", "submitter": "Shahriar Esmaeili", "authors": "Saeideh Roshanfekr, Shahriar Esmaeili, Hassan Ataeian, Ali Amiri", "title": "Weighted second-order cone programming twin support vector machine for\n  imbalanced data classification", "comments": "This manuscript is under revision at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method of using a Weighted second-order cone programming twin\nsupport vector machine (WSOCP-TWSVM) for imbalanced data classification. This\nmethod constructs a graph based under-sampling method which is utilized to\nremove outliers and reduce the dispensable majority samples. Then, appropriate\nweights are set in order to decrease the impact of samples of the majority\nclass and increase the effect of the minority class in the optimization formula\nof the classifier. These weights are embedded in the optimization problem of\nthe Second Order Cone Programming (SOCP) Twin Support Vector Machine\nformulations. This method is tested, and its performance is compared to\nprevious methods on standard datasets. Results of experiments confirm the\nfeasibility and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 01:04:11 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 21:53:41 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Roshanfekr", "Saeideh", ""], ["Esmaeili", "Shahriar", ""], ["Ataeian", "Hassan", ""], ["Amiri", "Ali", ""]]}, {"id": "1904.11685", "submitter": "Kai Wang", "authors": "Xiang Wang and Kai Wang and Shiguo Lian", "title": "A Survey on Face Data Augmentation", "comments": "26 pages, 22 figures. Neural Comput & Applic (2020)", "journal-ref": null, "doi": "10.1007/s00521-020-04748-3", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality and size of training set have great impact on the results of deep\nlearning-based face related tasks. However, collecting and labeling adequate\nsamples with high quality and balanced distributions still remains a laborious\nand expensive work, and various data augmentation techniques have thus been\nwidely used to enrich the training dataset. In this paper, we systematically\nreview the existing works of face data augmentation from the perspectives of\nthe transformation types and methods, with the state-of-the-art approaches\ninvolved. Among all these approaches, we put the emphasis on the deep\nlearning-based works, especially the generative adversarial networks which have\nbeen recognized as more powerful and effective tools in recent years. We\npresent their principles, discuss the results and show their applications as\nwell as limitations. Different evaluation metrics for evaluating these\napproaches are also introduced. We point out the challenges and opportunities\nin the field of face data augmentation, and provide brief yet insightful\ndiscussions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:23:35 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wang", "Xiang", ""], ["Wang", "Kai", ""], ["Lian", "Shiguo", ""]]}, {"id": "1904.11693", "submitter": "Chunfeng Song", "authors": "Chunfeng Song, Yan Huang, Wanli Ouyang, Liang Wang", "title": "Box-driven Class-wise Region Masking and Filling Rate Guided Loss for\n  Weakly Supervised Semantic Segmentation", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has achieved huge progress via adopting deep Fully\nConvolutional Networks (FCN). However, the performance of FCN based models\nseverely rely on the amounts of pixel-level annotations which are expensive and\ntime-consuming. To address this problem, it is a good choice to learn to\nsegment with weak supervision from bounding boxes. How to make full use of the\nclass-level and region-level supervisions from bounding boxes is the critical\nchallenge for the weakly supervised learning task. In this paper, we first\nintroduce a box-driven class-wise masking model (BCM) to remove irrelevant\nregions of each class. Moreover, based on the pixel-level segment proposal\ngenerated from the bounding box supervision, we could calculate the mean\nfilling rates of each class to serve as an important prior cue, then we propose\na filling rate guided adaptive loss (FR-Loss) to help the model ignore the\nwrongly labeled pixels in proposals. Unlike previous methods directly training\nmodels with the fixed individual segment proposals, our method can adjust the\nmodel learning with global statistical information. Thus it can help reduce the\nnegative impacts from wrongly labeled proposals. We evaluate the proposed\nmethod on the challenging PASCAL VOC 2012 benchmark and compare with other\nmethods. Extensive experimental results show that the proposed method is\neffective and achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:47:58 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Song", "Chunfeng", ""], ["Huang", "Yan", ""], ["Ouyang", "Wanli", ""], ["Wang", "Liang", ""]]}, {"id": "1904.11701", "submitter": "Martin L\\\"angkvist", "authors": "Martin L\\\"angkvist and Jonas Widell and Per Thunberg and Amy Loutfi\n  and Mats Lid\\'en", "title": "Interactive user interface based on Convolutional Auto-encoders for\n  annotating CT-scans", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution computed tomography (HRCT) is the most important imaging\nmodality for interstitial lung diseases, where the radiologists are interested\nin identifying certain patterns, and their volumetric and regional\ndistribution. The use of machine learning can assist the radiologists with both\nthese tasks by performing semantic segmentation. In this paper, we propose an\ninteractive annotation-tool for semantic segmentation that assists the\nradiologist in labeling CT scans. The annotation tool is evaluated by six\nradiologists and radiology residents classifying healthy lung and reticular\npattern i HRCT images. The usability of the system is evaluated with a System\nUsability Score (SUS) and interaction information from the readers that used\nthe tool for annotating the CT volumes. It was discovered that the experienced\nusability and how the users interactied with the system differed between the\nusers. A higher SUS-score was given by users that prioritized learning speed\nover model accuracy and spent less time with manual labeling and instead\nutilized the suggestions provided by the GUI. An analysis of the annotation\nvariations between the readers show substantial agreement (Cohen's kappa=0.69)\nfor classification of healthy and affected lung parenchyma in pulmonary\nfibrosis. The inter-reader variation is a challenge for the definition of\nground truth.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 07:45:48 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["L\u00e4ngkvist", "Martin", ""], ["Widell", "Jonas", ""], ["Thunberg", "Per", ""], ["Loutfi", "Amy", ""], ["Lid\u00e9n", "Mats", ""]]}, {"id": "1904.11740", "submitter": "Gemma Roig", "authors": "Kshitij Dwivedi, Gemma Roig", "title": "Representation Similarity Analysis for Efficient Task taxonomy &\n  Transfer Learning", "comments": "Accepted at CVPR 2019. Code available at\n  https://github.com/kshitijd20/RSA-CVPR19-release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is widely used in deep neural network models when there are\nfew labeled examples available. The common approach is to take a pre-trained\nnetwork in a similar task and finetune the model parameters. This is usually\ndone blindly without a pre-selection from a set of pre-trained models, or by\nfinetuning a set of models trained on different tasks and selecting the best\nperforming one by cross-validation. We address this problem by proposing an\napproach to assess the relationship between visual tasks and their\ntask-specific models. Our method uses Representation Similarity Analysis (RSA),\nwhich is commonly used to find a correlation between neuronal responses from\nbrain data and models. With RSA we obtain a similarity score among tasks by\ncomputing correlations between models trained on different tasks. Our method is\nefficient as it requires only pre-trained models, and a few images with no\nfurther training. We demonstrate the effectiveness and efficiency of our method\nfor generating task taxonomy on Taskonomy dataset. We next evaluate the\nrelationship of RSA with the transfer learning performance on Taskonomy tasks\nand a new task: Pascal VOC semantic segmentation. Our results reveal that\nmodels trained on tasks with higher similarity score show higher transfer\nlearning performance. Surprisingly, the best transfer learning result for\nPascal VOC semantic segmentation is not obtained from the pre-trained model on\nsemantic segmentation, probably due to the domain differences, and our method\nsuccessfully selects the high performing models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:43:11 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Dwivedi", "Kshitij", ""], ["Roig", "Gemma", ""]]}, {"id": "1904.11781", "submitter": "Michael Strecke", "authors": "Michael Strecke and J\\\"org St\\\"uckler", "title": "EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association", "comments": "IEEE/CVF International Conference on Computer Vision (ICCV) 2019,\n  Project page: https://emfusion.is.tue.mpg.de/, Source code:\n  https://github.com/EmbodiedVision/emfusion", "journal-ref": null, "doi": "10.1109/ICCV.2019.00596", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of approaches for acquiring dense 3D environment maps with RGB-D\ncameras assumes static environments or rejects moving objects as outliers. The\nrepresentation and tracking of moving objects, however, has significant\npotential for applications in robotics or augmented reality. In this paper, we\npropose a novel approach to dynamic SLAM with dense object-level\nrepresentations. We represent rigid objects in local volumetric signed distance\nfunction (SDF) maps, and formulate multi-object tracking as direct alignment of\nRGB-D images with the SDF representations. Our main novelty is a probabilistic\nformulation which naturally leads to strategies for data association and\nocclusion handling. We analyze our approach in experiments and demonstrate that\nour approach compares favorably with the state-of-the-art methods in terms of\nrobustness and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:54:50 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:15:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Strecke", "Michael", ""], ["St\u00fcckler", "J\u00f6rg", ""]]}, {"id": "1904.11784", "submitter": "Guojun Yin", "authors": "Guojun Yin, Bin Liu, Huihui Zhu, Tao Gong, Nenghai Yu", "title": "A Large Scale Urban Surveillance Video Dataset for Multiple-Object\n  Tracking and Behavior Analysis", "comments": "6 pages. This dataset are not available due to the data license", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple-object tracking and behavior analysis have been the essential parts\nof surveillance video analysis for public security and urban management. With\nbillions of surveillance video captured all over the world, multiple-object\ntracking and behavior analysis by manual labor are cumbersome and cost\nexpensive. Due to the rapid development of deep learning algorithms in recent\nyears, automatic object tracking and behavior analysis put forward an urgent\ndemand on a large scale well-annotated surveillance video dataset that can\nreflect the diverse, congested, and complicated scenarios in real applications.\nThis paper introduces an urban surveillance video dataset (USVD) which is by\nfar the largest and most comprehensive. The dataset consists of 16 scenes\ncaptured in 7 typical outdoor scenarios: street, crossroads, hospital entrance,\nschool gate, park, pedestrian mall, and public square. Over 200k video frames\nare annotated carefully, resulting in more than 3:7 million object bounding\nboxes and about 7:1 thousand trajectories. We further use this dataset to\nevaluate the performance of typical algorithms for multiple-object tracking and\nanomaly behavior analysis and explore the robustness of these methods in urban\ncongested scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:58:36 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 06:46:58 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Yin", "Guojun", ""], ["Liu", "Bin", ""], ["Zhu", "Huihui", ""], ["Gong", "Tao", ""], ["Yu", "Nenghai", ""]]}, {"id": "1904.11815", "submitter": "Jean-Baptiste Camps", "authors": "Jean-Baptiste Camps (CJM), Gilles Guilhem Couffignal (PLH)", "title": "Producing Corpora of Medieval and Premodern Occitan", "comments": "in French. Actes du XIIe Congr{\\`e}s de l'Association internationale\n  d'{\\'e}tudes occitanes Albi, 2017, Association internationale d'{\\'e}tudes\n  occitanes (AIEO), Jul 2017, Albi, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At a time when the quantity of - more or less freely - available data is\nincreasing significantly, thanks to digital corpora, editions or libraries, the\ndevelopment of data mining tools or deep learning methods allows researchers to\nbuild a corpus of study tailored for their research, to enrich their data and\nto exploit them.Open optical character recognition (OCR) tools can be adapted\nto old prints, incunabula or even manuscripts, with usable results, allowing\nthe rapid creation of textual corpora. The alternation of training and\ncorrection phases makes it possible to improve the quality of the results by\nrapidly accumulating raw text data. These can then be structured, for example\nin XML/TEI, and enriched.The enrichment of the texts with graphic or linguistic\nannotations can also be automated. These processes, known to linguists and\nfunctional for modern languages, present difficulties for languages such as\nMedieval Occitan, due in part to the absence of big enough lemmatized corpora.\nSuggestions for the creation of tools adapted to the considerable spelling\nvariation of ancient languages will be presented, as well as experiments for\nthe lemmatization of Medieval and Premodern Occitan.These techniques open the\nway for many exploitations. The much desired increase in the amount of\navailable quality texts and data makes it possible to improve digital philology\nmethods, if everyone takes the trouble to make their data freely available\nonline and reusable.By exposing different technical solutions and some\nmicro-analyses as examples, this paper aims to show part of what digital\nphilology can offer to researchers in the Occitan domain, while recalling the\nethical issues on which such practices are based.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 12:55:03 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Camps", "Jean-Baptiste", "", "CJM"], ["Couffignal", "Gilles Guilhem", "", "PLH"]]}, {"id": "1904.11820", "submitter": "Tuan N.A. Hoang", "authors": "Thanh-Toan Do, Khoa Le, Tuan Hoang, Huu Le, Tam V. Nguyen, Ngai-Man\n  Cheung", "title": "Simultaneous Feature Aggregating and Hashing for Compact Binary Code\n  Learning", "comments": "Accepted to IEEE Trans. on Image Processing (TIP), 2019. arXiv admin\n  note: substantial text overlap with arXiv:1704.00860", "journal-ref": null, "doi": "10.1109/TIP.2019.2913509", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing images by compact hash codes is an attractive approach for\nlarge-scale content-based image retrieval. In most state-of-the-art\nhashing-based image retrieval systems, for each image, local descriptors are\nfirst aggregated as a global representation vector. This global vector is then\nsubjected to a hashing function to generate a binary hash code. In previous\nworks, the aggregating and the hashing processes are designed independently.\nHence these frameworks may generate suboptimal hash codes. In this paper, we\nfirst propose a novel unsupervised hashing framework in which feature\naggregating and hashing are designed simultaneously and optimized jointly.\nSpecifically, our joint optimization generates aggregated representations that\ncan be better reconstructed by some binary codes. This leads to more\ndiscriminative binary hash codes and improved retrieval accuracy. In addition,\nthe proposed method is flexible. It can be extended for supervised hashing.\nWhen the data label is available, the framework can be adapted to learn binary\ncodes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,\nwe also propose a fast version of the state-of-the-art hashing method Binary\nAutoencoder to be used in our proposed frameworks. Extensive experiments on\nbenchmark datasets under various settings show that the proposed methods\noutperform state-of-the-art unsupervised and supervised hashing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 22:05:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Le", "Khoa", ""], ["Hoang", "Tuan", ""], ["Le", "Huu", ""], ["Nguyen", "Tam V.", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1904.11834", "submitter": "Artur Souza", "authors": "Artur Souza, Leonardo B. Oliveira, Sabine Hollatz, Matt Feldman, Kunle\n  Olukotun, James M. Holton, Aina E. Cohen, Luigi Nardi", "title": "DeepFreak: Learning Crystallography Diffraction Patterns with Automated\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial crystallography is the field of science that studies the structure and\nproperties of crystals via diffraction patterns. In this paper, we introduce a\nnew serial crystallography dataset comprised of real and synthetic images; the\nsynthetic images are generated through the use of a simulator that is both\nscalable and accurate. The resulting dataset is called DiffraNet, and it is\ncomposed of 25,457 512x512 grayscale labeled images. We explore several\ncomputer vision approaches for classification on DiffraNet such as standard\nfeature extraction algorithms associated with Random Forests and Support Vector\nMachines but also an end-to-end CNN topology dubbed DeepFreak tailored to work\non this new dataset. All implementations are publicly available and have been\nfine-tuned using off-the-shelf AutoML optimization tools for a fair comparison.\nOur best model achieves 98.5% accuracy on synthetic images and 94.51% accuracy\non real images. We believe that the DiffraNet dataset and its classification\nmethods will have in the long term a positive impact in accelerating\ndiscoveries in many disciplines, including chemistry, geology, biology,\nmaterials science, metallurgy, and physics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:12:40 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 15:11:32 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Souza", "Artur", ""], ["Oliveira", "Leonardo B.", ""], ["Hollatz", "Sabine", ""], ["Feldman", "Matt", ""], ["Olukotun", "Kunle", ""], ["Holton", "James M.", ""], ["Cohen", "Aina E.", ""], ["Nardi", "Luigi", ""]]}, {"id": "1904.11864", "submitter": "Rania Briq", "authors": "Rania Briq, Andreas Doering, Juergen Gall", "title": "Unifying Part Detection and Association for Recurrent Multi-Person Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint model of human joint detection and association for 2D\nmulti-person pose estimation (MPPE). The approach unifies training of joint\ndetection and association without a need for further processing or\nsophisticated heuristics in order to associate the joints with people\nindividually. The approach consists of two stages, where in the first stage\njoint detection heatmaps and association features are extracted, and in the\nsecond stage, whose input are the extracted features of the first stage, we\nintroduce a recurrent neural network (RNN) which predicts the heatmaps of a\nsingle person's joints in each iteration. In addition, the network learns a\nstopping criterion in order to halt once it has identified all individuals in\nthe image. This approach allowed us to eliminate several heuristic assumptions\nand parameters needed for association which do not necessarily hold true.\nAdditionally, such an end-to-end approach allows the final objective to be\nknown and directly optimized over during training. We evaluated our model on\nthe challenging MSCOCO dataset and obtained an improvement over the baseline,\nparticularly in challenging scenes with occlusions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 14:22:07 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Briq", "Rania", ""], ["Doering", "Andreas", ""], ["Gall", "Juergen", ""]]}, {"id": "1904.11883", "submitter": "Bo Jiang", "authors": "Bo Jiang and Ziyan Zhang and Jin Tang and Bin Luo", "title": "Graph Optimized Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have been widely studied for graph data\nrepresentation and learning tasks. Existing GCNs generally use a fixed single\ngraph which may lead to weak suboptimal for data representation/learning and\nare also hard to deal with multiple graphs. To address these issues, we propose\na novel Graph Optimized Convolutional Network (GOCN) for graph data\nrepresentation and learning. Our GOCN is motivated based on our\nre-interpretation of graph convolution from a regularization/optimization\nframework. The core idea of GOCN is to formulate graph optimization and graph\nconvolutional representation into a unified framework and thus conducts both of\nthem cooperatively to boost their respective performance in GCN learning\nscheme. Moreover, based on the proposed unified graph optimization-convolution\nframework, we propose a novel Multiple Graph Optimized Convolutional Network\n(M-GOCN) to naturally address the data with multiple graphs. Experimental\nresults demonstrate the effectiveness and benefit of the proposed GOCN and\nM-GOCN.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:05:45 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Jiang", "Bo", ""], ["Zhang", "Ziyan", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1904.11898", "submitter": "Keuntaek Lee", "authors": "Keuntaek Lee, Gabriel Nakajima An, Viacheslav Zakharov, Evangelos A.\n  Theodorou", "title": "Perceptual Attention-based Predictive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel information processing architecture for\nsafe deep learning-based visual navigation of autonomous systems. The proposed\ninformation processing architecture is used to support a perceptual\nattention-based predictive control algorithm that leverages model predictive\ncontrol (MPC), convolutional neural networks (CNNs), and uncertainty\nquantification methods. The novelty of our approach lies in using MPC to learn\nhow to place attention on relevant areas of the visual input, which ultimately\nallows the system to more rapidly detect unsafe conditions. We accomplish this\nby using MPC to learn to select regions of interest in the input image, which\nare used to output control actions as well as estimates of epistemic and\naleatoric uncertainty in the attention-aware visual input. We use these\nuncertainty estimates to quantify the safety of our network controller under\nthe current navigation condition. The proposed architecture and algorithm is\ntested on a 1:5 scale terrestrial vehicle. Experimental results show that the\nproposed algorithm outperforms previous approaches on early detection of unsafe\nconditions, such as when novel obstacles are present in the navigation\nenvironment. The proposed architecture is the first step towards using deep\nlearning-based perceptual control policies in safety-critical domains.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:38:37 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 01:05:22 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Lee", "Keuntaek", ""], ["An", "Gabriel Nakajima", ""], ["Zakharov", "Viacheslav", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "1904.11929", "submitter": "Spyridon Bakas", "authors": "Ludovic Venet, Sarthak Pati, Paul Yushkevich, Spyridon Bakas", "title": "Accurate and Robust Alignment of Variable-stained Histologic Images\n  Using a General-purpose Greedy Diffeomorphic Registration Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variously stained histology slices are routinely used by pathologists to\nassess extracted tissue samples from various anatomical sites and determine the\npresence or extent of a disease. Evaluation of sequential slides is expected to\nenable a better understanding of the spatial arrangement and growth patterns of\ncells and vessels. In this paper we present a practical two-step approach based\non diffeomorphic registration to align digitized sequential histopathology\nstained slides to each other, starting with an initial affine step followed by\nthe estimation of a detailed deformation field.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 16:57:11 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Venet", "Ludovic", ""], ["Pati", "Sarthak", ""], ["Yushkevich", "Paul", ""], ["Bakas", "Spyridon", ""]]}, {"id": "1904.11932", "submitter": "Patrick Wenzel", "authors": "Lukas von Stumberg, Patrick Wenzel, Qadeer Khan, Daniel Cremers", "title": "GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct SLAM methods have shown exceptional performance on odometry tasks.\nHowever, they are susceptible to dynamic lighting and weather changes while\nalso suffering from a bad initialization on large baselines. To overcome this,\nwe propose GN-Net: a network optimized with the novel Gauss-Newton loss for\ntraining weather invariant deep features, tailored for direct image alignment.\nOur network can be trained with pixel correspondences between images taken from\ndifferent sequences. Experiments on both simulated and real-world datasets\ndemonstrate that our approach is more robust against bad initialization,\nvariations in day-time, and weather changes thereby outperforming\nstate-of-the-art direct and indirect methods. Furthermore, we release an\nevaluation benchmark for relocalization tracking against different types of\nweather. Our benchmark is available at https://vision.in.tum.de/gn-net.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 16:58:34 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 20:50:35 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 11:10:15 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["von Stumberg", "Lukas", ""], ["Wenzel", "Patrick", ""], ["Khan", "Qadeer", ""], ["Cremers", "Daniel", ""]]}, {"id": "1904.11934", "submitter": "Soomin Kim", "authors": "Soomin Kim, Yuchi Huo, Sung-Eui Yoon", "title": "Single Image Reflection Removal with Physically-Based Training Images", "comments": "Accepted to CVPR 2020, project page:\n  https://sgvr.kaist.ac.kr/~smkim/Reflection_removal_rendering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning-based single image reflection separation methods have\nbeen exploited widely. To benefit the learning approach, a large number of\ntraining image pairs (i.e., with and without reflections) were synthesized in\nvarious ways, yet they are away from a physically-based direction. In this\npaper, physically based rendering is used for faithfully synthesizing the\nrequired training images, and a corresponding network structure and loss term\nare proposed. We utilize existing RGBD/RGB images to estimate meshes, then\nphysically simulate the light transportation between meshes, glass, and lens\nwith path tracing to synthesize training data, which successfully reproduce the\nspatially variant anisotropic visual effect of glass reflection. For guiding\nthe separation better, we additionally consider a module, backtrack network\n(BT-net) for backtracking the reflections, which removes complicated ghosting,\nattenuation, blurred and defocused effect of glass/lens. This enables obtaining\na priori information before having the distortion. The proposed method\nconsidering additional a priori information with physically simulated training\ndata is validated with various real reflection images and shows visually\npleasant and numerical advantages compared with state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:09:38 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:14:01 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kim", "Soomin", ""], ["Huo", "Yuchi", ""], ["Yoon", "Sung-Eui", ""]]}, {"id": "1904.11953", "submitter": "Fei Wang", "authors": "Fei Wang, Yunpeng Song, Jimuyang Zhang, Jinsong Han and Dong Huang", "title": "Temporal Unet: Sample Level Human Action Recognition using WiFi", "comments": "14 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human doing actions will result in WiFi distortion, which is widely explored\nfor action recognition, such as the elderly fallen detection, hand sign\nlanguage recognition, and keystroke estimation. As our best survey, past work\nrecognizes human action by categorizing one complete distortion series into one\naction, which we term as series-level action recognition. In this paper, we\nintroduce a much more fine-grained and challenging action recognition task into\nWiFi sensing domain, i.e., sample-level action recognition. In this task, every\nWiFi distortion sample in the whole series should be categorized into one\naction, which is a critical technique in precise action localization,\ncontinuous action segmentation, and real-time action recognition. To achieve\nWiFi-based sample-level action recognition, we fully analyze approaches in\nimage-based semantic segmentation as well as in video-based frame-level action\nrecognition, then propose a simple yet efficient deep convolutional neural\nnetwork, i.e., Temporal Unet. Experimental results show that Temporal Unet\nachieves this novel task well. Codes have been made publicly available at\nhttps://github.com/geekfeiw/WiSLAR.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 21:23:28 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Wang", "Fei", ""], ["Song", "Yunpeng", ""], ["Zhang", "Jimuyang", ""], ["Han", "Jinsong", ""], ["Huang", "Dong", ""]]}, {"id": "1904.11955", "submitter": "Simon Du", "authors": "Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov,\n  Ruosong Wang", "title": "On Exact Computation with an Infinitely Wide Neural Net", "comments": "In NeurIPS 2019. Code available: https://github.com/ruosongwang/cntk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How well does a classic deep net architecture like AlexNet or VGG19 classify\non a standard dataset such as CIFAR-10 when its width --- namely, number of\nchannels in convolutional layers, and number of nodes in fully-connected\ninternal layers --- is allowed to increase to infinity? Such questions have\ncome to the forefront in the quest to theoretically understand deep learning\nand its mysteries about optimization and generalization. They also connect deep\nlearning to notions such as Gaussian processes and kernels. A recent paper\n[Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures\nthe behavior of fully-connected deep nets in the infinite width limit trained\nby gradient descent; this object was implicit in some other recent papers. An\nattraction of such ideas is that a pure kernel-based method is used to capture\nthe power of a fully-trained deep net of infinite width.\n  The current paper gives the first efficient exact algorithm for computing the\nextension of NTK to convolutional neural nets, which we call Convolutional NTK\n(CNTK), as well as an efficient GPU implementation of this algorithm. This\nresults in a significant new benchmark for the performance of a pure\nkernel-based method on CIFAR-10, being $10\\%$ higher than the methods reported\nin [Novak et al., 2019], and only $6\\%$ lower than the performance of the\ncorresponding finite deep net architecture (once batch normalization, etc. are\nturned off). Theoretically, we also give the first non-asymptotic proof showing\nthat a fully-trained sufficiently wide net is indeed equivalent to the kernel\nregression predictor using NTK.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:29:37 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 15:10:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Arora", "Sanjeev", ""], ["Du", "Simon S.", ""], ["Hu", "Wei", ""], ["Li", "Zhiyuan", ""], ["Salakhutdinov", "Ruslan", ""], ["Wang", "Ruosong", ""]]}, {"id": "1904.11960", "submitter": "Zhixin Shu", "authors": "Mihir Sahasrabudhe, Zhixin Shu, Edward Bartrum, Riza Alp Guler,\n  Dimitris Samaras, Iasonas Kokkinos", "title": "Lifting AutoEncoders: Unsupervised Learning of a Fully-Disentangled 3D\n  Morphable Model using Deep Non-Rigid Structure from Motion", "comments": "19 pages; 12 figures; code will be released; Project page:\n  https://msahasrabudhe.github.io/projects/lae/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce Lifting Autoencoders, a generative 3D surface-based\nmodel of object categories. We bring together ideas from non-rigid structure\nfrom motion, image formation, and morphable models to learn a controllable,\ngeometric model of 3D categories in an entirely unsupervised manner from an\nunstructured set of images. We exploit the 3D geometric nature of our model and\nuse normal information to disentangle appearance into illumination, shading and\nalbedo. We further use weak supervision to disentangle the non-rigid shape\nvariability of human faces into identity and expression. We combine the 3D\nrepresentation with a differentiable renderer to generate RGB images and append\nan adversarially trained refinement network to obtain sharp, photorealistic\nimage reconstruction results. The learned generative model can be controlled in\nterms of interpretable geometry and appearance factors, allowing us to perform\nphotorealistic image manipulation of identity, expression, 3D pose, and\nillumination properties.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:43:51 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Sahasrabudhe", "Mihir", ""], ["Shu", "Zhixin", ""], ["Bartrum", "Edward", ""], ["Guler", "Riza Alp", ""], ["Samaras", "Dimitris", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1904.12007", "submitter": "Ignacio Viedma", "authors": "Ignacio Viedma, Juan Tapia, Andres Iturriaga and Christoph Busch", "title": "Relevant features for Gender Classification in NIR Periocular Images", "comments": "12 pages, Paper accepted by IET Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most gender classifications methods from NIR images have used iris\ninformation. Recent work has explored the use of the whole periocular iris\nregion which has surprisingly achieve better results. This suggests the most\nrelevant information for gender classification is not located in the iris as\nexpected. In this work, we analyze and demonstrate the location of the most\nrelevant features that describe gender in periocular NIR images and evaluate\nits influence its classification. Experiments show that the periocular region\ncontains more gender information than the iris region. We extracted several\nfeatures (intensity, texture, and shape) and classified them according to its\nrelevance using the XgBoost algorithm. Support Vector Machine and nine ensemble\nclassifiers were used for testing gender accuracy when using the most relevant\nfeatures. The best classification results were obtained when 4,000 features\nlocated on the periocular region were used (89.22\\%). Additional experiments\nwith the full periocular iris images versus the iris-Occluded images were\nperformed. The gender classification rates obtained were 84.35\\% and 85.75\\%\nrespectively. We also contribute to the state of the art with a new database\n(UNAB-Gender). From results, we suggest focussing only on the surrounding area\nof the iris. This allows us to realize a faster classification of gender from\nNIR periocular images.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 18:23:58 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Viedma", "Ignacio", ""], ["Tapia", "Juan", ""], ["Iturriaga", "Andres", ""], ["Busch", "Christoph", ""]]}, {"id": "1904.12012", "submitter": "Ji Hou", "authors": "Ji Hou, Angela Dai, Matthias Nie{\\ss}ner", "title": "RevealNet: Seeing Behind Objects in RGB-D Scans", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During 3D reconstruction, it is often the case that people cannot scan each\nindividual object from all views, resulting in missing geometry in the captured\nscan. This missing geometry can be fundamentally limiting for many\napplications, e.g., a robot needs to know the unseen geometry to perform a\nprecise grasp on an object. Thus, we introduce the task of semantic instance\ncompletion: from an incomplete RGB-D scan of a scene, we aim to detect the\nindividual object instances and infer their complete object geometry. This will\nopen up new possibilities for interactions with objects in a scene, for\ninstance for virtual or robotic agents. We tackle this problem by introducing\nRevealNet, a new data-driven approach that jointly detects object instances and\npredicts their complete geometry. This enables a semantically meaningful\ndecomposition of a scanned scene into individual, complete 3D objects,\nincluding hidden and unobserved object parts. RevealNet is an end-to-end 3D\nneural network architecture that leverages joint color and geometry feature\nlearning. The fully-convolutional nature of our 3D network enables efficient\ninference of semantic instance completion for 3D scans at scale of large indoor\nenvironments in a single forward pass. We show that predicting complete object\ngeometry improves both 3D detection and instance segmentation performance. We\nevaluate on both real and synthetic scan benchmark data for the new task, where\nwe outperform state-of-the-art approaches by over 15 in mAP@0.5 on ScanNet, and\nover 18 in mAP@0.5 on SUNCG.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 18:33:34 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 14:23:32 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 19:16:11 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Hou", "Ji", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1904.12019", "submitter": "Yichun Shi", "authors": "Sixue Gong, Yichun Shi, Anil K. Jain", "title": "Recurrent Embedding Aggregation Network for Video Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent networks have been successful in analyzing temporal data and have\nbeen widely used for video analysis. However, for video face recognition, where\nthe base CNNs trained on large-scale data already provide discriminative\nfeatures, using Long Short-Term Memory (LSTM), a popular recurrent network, for\nfeature learning could lead to overfitting and degrade the performance instead.\nWe propose a Recurrent Embedding Aggregation Network (REAN) for set to set face\nrecognition. Compared with LSTM, REAN is robust against overfitting because it\nonly learns how to aggregate the pre-trained embeddings rather than learning\nrepresentations from scratch. Compared with quality-aware aggregation methods,\nREAN can take advantage of the context information to circumvent the noise\nintroduced by redundant video frames. Empirical results on three public domain\nvideo face recognition datasets, IJB-S, YTF, and PaSC show that the proposed\nREAN significantly outperforms naive CNN-LSTM structure and quality-aware\naggregation methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 19:22:41 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 21:56:15 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Gong", "Sixue", ""], ["Shi", "Yichun", ""], ["Jain", "Anil K.", ""]]}, {"id": "1904.12023", "submitter": "Mengyu Dai", "authors": "Mengyu Dai, Zhengwu Zhang and Anuj Srivastava", "title": "Discovering Common Change-Point Patterns in Functional Connectivity\n  Across Subjects", "comments": null, "journal-ref": "Published in Medical Image Analysis, 2019", "doi": "10.1016/j.media.2019.101532", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies change-points in human brain functional connectivity (FC)\nand seeks patterns that are common across multiple subjects under identical\nexternal stimulus. FC relates to the similarity of fMRI responses across\ndifferent brain regions when the brain is simply resting or performing a task.\nWhile the dynamic nature of FC is well accepted, this paper develops a formal\nstatistical test for finding {\\it change-points} in times series associated\nwith FC. It represents short-term connectivity by a symmetric positive-definite\nmatrix, and uses a Riemannian metric on this space to develop a graphical\nmethod for detecting change-points in a time series of such matrices. It also\nprovides a graphical representation of estimated FC for stationary subintervals\nin between the detected change-points. Furthermore, it uses a temporal\nalignment of the test statistic, viewed as a real-valued function over time, to\nremove inter-subject variability and to discover common change-point patterns\nacross subjects. This method is illustrated using data from Human Connectome\nProject (HCP) database for multiple subjects and tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 19:34:52 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Dai", "Mengyu", ""], ["Zhang", "Zhengwu", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1904.12043", "submitter": "Haibin Lin", "authors": "Haibin Lin, Hang Zhang, Yifei Ma, Tong He, Zhi Zhang, Sheng Zha, Mu Li", "title": "Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the\n  Limbo of Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increasing demand for training powers for deep learning algorithms\nand the rapid growth of computation resources in data centers, it is desirable\nto dynamically schedule different distributed deep learning tasks to maximize\nresource utilization and reduce cost. In this process, different tasks may\nreceive varying numbers of machines at different time, a setting we call\nelastic distributed training. Despite the recent successes in large mini-batch\ndistributed training, these methods are rarely tested in elastic distributed\ntraining environments and suffer degraded performance in our experiments, when\nwe adjust the learning rate linearly immediately with respect to the batch\nsize. One difficulty we observe is that the noise in the stochastic momentum\nestimation is accumulated over time and will have delayed effects when the\nbatch size changes. We therefore propose to smoothly adjust the learning rate\nover time to alleviate the influence of the noisy momentum estimation. Our\nexperiments on image classification, object detection and semantic segmentation\nhave demonstrated that our proposed Dynamic SGD method achieves stabilized\nperformance when varying the number of GPUs from 8 to 128. We also provide\ntheoretical understanding on the optimality of linear learning rate scheduling\nand the effects of stochastic momentum.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 20:45:28 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 06:48:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Lin", "Haibin", ""], ["Zhang", "Hang", ""], ["Ma", "Yifei", ""], ["He", "Tong", ""], ["Zhang", "Zhi", ""], ["Zha", "Sheng", ""], ["Li", "Mu", ""]]}, {"id": "1904.12059", "submitter": "John Collomosse", "authors": "Tu Bui, Daniel Cooper, John Collomosse, Mark Bell, Alex Green, John\n  Sheridan, Jez Higgins, Arindra Das, Jared Keller, Olivier Thereaux, Alan\n  Brown", "title": "ARCHANGEL: Tamper-proofing Video Archives using Temporal Content Hashes\n  on the Blockchain", "comments": "Accepted to CVPR Blockchain Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ARCHANGEL; a novel distributed ledger based system for assuring\nthe long-term integrity of digital video archives. First, we describe a novel\ndeep network architecture for computing compact temporal content hashes (TCHs)\nfrom audio-visual streams with durations of minutes or hours. Our TCHs are\nsensitive to accidental or malicious content modification (tampering) but\ninvariant to the codec used to encode the video. This is necessary due to the\ncuratorial requirement for archives to format shift video over time to ensure\nfuture accessibility. Second, we describe how the TCHs (and the models used to\nderive them) are secured via a proof-of-authority blockchain distributed across\nmultiple independent archives. We report on the efficacy of ARCHANGEL within\nthe context of a trial deployment in which the national government archives of\nthe United Kingdom, Estonia and Norway participated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:59:02 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bui", "Tu", ""], ["Cooper", "Daniel", ""], ["Collomosse", "John", ""], ["Bell", "Mark", ""], ["Green", "Alex", ""], ["Sheridan", "John", ""], ["Higgins", "Jez", ""], ["Das", "Arindra", ""], ["Keller", "Jared", ""], ["Thereaux", "Olivier", ""], ["Brown", "Alan", ""]]}, {"id": "1904.12094", "submitter": "Heming Zhang", "authors": "Heming Zhang, Xiaolong Wang, Jingwen Zhu, C.-C. Jay Kuo", "title": "Accelerating Proposal Generation Network for \\\\Fast Face Detection on\n  Mobile Devices", "comments": "ICIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is a widely studied problem over the past few decades.\nRecently, significant improvements have been achieved via the deep neural\nnetwork, however, it is still challenging to directly apply these techniques to\nmobile devices for its limited computational power and memory. In this work, we\npresent a proposal generation acceleration framework for real-time face\ndetection. More specifically, we adopt a popular cascaded convolutional neural\nnetwork (CNN) as the basis, then apply our acceleration approach on the basic\nframework to speed up the model inference time. We are motivated by the\nobservation that the computation bottleneck of this framework arises from the\nproposal generation stage, where each level of the dense image pyramid has to\ngo through the network. In this work, we reduce the number of image pyramid\nlevels by utilizing both global and local facial characteristics (i.e., global\nface and facial parts). Experimental results on public benchmarks WIDER-face\nand FDDB demonstrate the satisfactory performance and faster speed compared to\nthe state-of-the-arts. %the comparable accuracy to state-of-the-arts with\nfaster speed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 02:45:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhang", "Heming", ""], ["Wang", "Xiaolong", ""], ["Zhu", "Jingwen", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1904.12099", "submitter": "Jiaqi Yang", "authors": "Jiaqi Yang, Chen Zhao, Ke Xian, Angfan Zhu, Zhiguo Cao", "title": "Learning to Fuse Local Geometric Features for 3D Rigid Data Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet very effective data-driven approach to fuse\nboth low-level and high-level local geometric features for 3D rigid data\nmatching. It is a common practice to generate distinctive geometric descriptors\nby fusing low-level features from various viewpoints or subspaces, or enhance\ngeometric feature matching by leveraging multiple high-level features. In prior\nworks, they are typically performed via linear operations such as concatenation\nand min pooling. We show that more compact and distinctive representations can\nbe achieved by optimizing a neural network (NN) model under the triplet\nframework that non-linearly fuses local geometric features in Euclidean spaces.\nThe NN model is trained by an improved triplet loss function that fully\nleverages all pairwise relationships within the triplet. Moreover, the fused\ndescriptor by our approach is also competitive to deep learned descriptors from\nraw data while being more lightweight and rotational invariant. Experimental\nresults on four standard datasets with various data modalities and application\ncontexts confirm the advantages of our approach in terms of both feature\nmatching and geometric registration.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 03:23:21 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Jiaqi", ""], ["Zhao", "Chen", ""], ["Xian", "Ke", ""], ["Zhu", "Angfan", ""], ["Cao", "Zhiguo", ""]]}, {"id": "1904.12101", "submitter": "Amod Jog", "authors": "Amod Jog and P. Ellen Grant and Joseph L. Jacobson and Andre van der\n  Kouwe and Ernesta M. Meintjes and Bruce Fischl and Lilla Z\\\"ollei", "title": "Fast Infant MRI Skullstripping with Multiview 2D Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skullstripping is defined as the task of segmenting brain tissue from a full\nhead magnetic resonance image~(MRI). It is a critical component in neuroimage\nprocessing pipelines. Downstream deformable registration and whole brain\nsegmentation performance is highly dependent on accurate skullstripping.\nSkullstripping is an especially challenging task for infant~(age range 0--18\nmonths) head MRI images due to the significant size and shape variability of\nthe head and the brain in that age range. Infant brain tissue development also\nchanges the $T_1$-weighted image contrast over time, making consistent\nskullstripping a difficult task. Existing tools for adult brain MRI\nskullstripping are ill equipped to handle these variations and a specialized\ninfant MRI skullstripping algorithm is necessary. In this paper, we describe a\nsupervised skullstripping algorithm that utilizes three trained fully\nconvolutional neural networks~(CNN), each of which segments 2D $T_1$-weighted\nslices in axial, coronal, and sagittal views respectively. The three\nprobabilistic segmentations in the three views are linearly fused and\nthresholded to produce a final brain mask. We compared our method to existing\nadult and infant skullstripping algorithms and showed significant improvement\nbased on Dice overlap metric~(average Dice of 0.97) with a manually labeled\nground truth data set. Label fusion experiments on multiple, unlabeled data\nsets show that our method is consistent and has fewer failure modes. In\naddition, our method is computationally very fast with a run time of 30 seconds\nper image on NVidia P40/P100/Quadro 4000 GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 03:33:17 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Jog", "Amod", ""], ["Grant", "P. Ellen", ""], ["Jacobson", "Joseph L.", ""], ["van der Kouwe", "Andre", ""], ["Meintjes", "Ernesta M.", ""], ["Fischl", "Bruce", ""], ["Z\u00f6llei", "Lilla", ""]]}, {"id": "1904.12116", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao, Qinghai Liao, Yilong Zhu, Tianyu Liu, Yang Yu, Rui Fan,\n  Lujia Wang, Ming Liu", "title": "A Novel Dual-Lidar Calibration Algorithm Using Planar Surfaces", "comments": "6 pages, 8 figures, accepted by 2019 IEEE Intelligent Vehicles\n  Symposium (IVS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple lidars are prevalently used on mobile vehicles for rendering a broad\nview to enhance the performance of localization and perception systems.\nHowever, precise calibration of multiple lidars is challenging since the\nfeature correspondences in scan points cannot always provide enough\nconstraints. To address this problem, the existing methods require fixed\ncalibration targets in scenes or rely exclusively on additional sensors. In\nthis paper, we present a novel method that enables automatic lidar calibration\nwithout these restrictions. Three linearly independent planar surfaces\nappearing in surroundings is utilized to find correspondences. Two components\nare developed to ensure the extrinsic parameters to be found: a closed-form\nsolver for initialization and an optimizer for refinement by minimizing a\nnonlinear cost function. Simulation and experimental results demonstrate the\nhigh accuracy of our calibration approach with the rotation and translation\nerrors smaller than 0.05rad and 0.1m respectively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 06:44:36 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Jiao", "Jianhao", ""], ["Liao", "Qinghai", ""], ["Zhu", "Yilong", ""], ["Liu", "Tianyu", ""], ["Yu", "Yang", ""], ["Fan", "Rui", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "1904.12144", "submitter": "Soshi Shimada", "authors": "Soshi Shimada, Vladislav Golyanik, Christian Theobalt and Didier\n  Stricker", "title": "IsMo-GAN: Adversarial Learning for Monocular Non-Rigid 3D Reconstruction", "comments": "13 pages, 11 figures, 4 tables, 6 sections, 73 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of the existing methods for non-rigid 3D surface regression from\nmonocular 2D images require an object template or point tracks over multiple\nframes as an input, and are still far from real-time processing rates. In this\nwork, we present the Isometry-Aware Monocular Generative Adversarial Network\n(IsMo-GAN) - an approach for direct 3D reconstruction from a single image,\ntrained for the deformation model in an adversarial manner on a light-weight\nsynthetic dataset. IsMo-GAN reconstructs surfaces from real images under\nvarying illumination, camera poses, textures and shading at over 250 Hz. In\nmultiple experiments, it consistently outperforms several approaches in the\nreconstruction accuracy, runtime, generalisation to unknown surfaces and\nrobustness to occlusions. In comparison to the state-of-the-art, we reduce the\nreconstruction error by 10-30% including the textureless case and our surfaces\nevince fewer artefacts qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 11:04:21 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 11:03:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.12165", "submitter": "Lluis Castrejon", "authors": "Lluis Castrejon, Nicolas Ballas, Aaron Courville", "title": "Improved Conditional VRNNs for Video Prediction", "comments": "Project page: https://sites.google.com/view/videovrnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future frames for a video sequence is a challenging generative\nmodeling task. Promising approaches include probabilistic latent variable\nmodels such as the Variational Auto-Encoder. While VAEs can handle uncertainty\nand model multiple possible future outcomes, they have a tendency to produce\nblurry predictions. In this work we argue that this is a sign of underfitting.\nTo address this issue, we propose to increase the expressiveness of the latent\ndistributions and to use higher capacity likelihood models. Our approach relies\non a hierarchy of latent variables, which defines a family of flexible prior\nand posterior distributions in order to better model the probability of future\nsequences. We validate our proposal through a series of ablation experiments\nand compare our approach to current state-of-the-art latent variable models.\nOur method performs favorably under several metrics in three different\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 15:07:12 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Castrejon", "Lluis", ""], ["Ballas", "Nicolas", ""], ["Courville", "Aaron", ""]]}, {"id": "1904.12175", "submitter": "Ying Qu", "authors": "Ying Qu and Hairong Qi and Chiman Kwan and Naoto Yokoya and Jocelyn\n  Chanussot", "title": "Unsupervised and Unregistered Hyperspectral Image Super-Resolution with\n  Mutual Dirichlet-Net", "comments": "IEEE Transactions on Remote Sensing and Geoscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSI) provide rich spectral information that contributed\nto the successful performance improvement of numerous computer vision tasks.\nHowever, it can only be achieved at the expense of images' spatial resolution.\nHyperspectral image super-resolution (HSI-SR) addresses this problem by fusing\nlow resolution (LR) HSI with multispectral image (MSI) carrying much higher\nspatial resolution (HR). All existing HSI-SR approaches require the LR HSI and\nHR MSI to be well registered and the reconstruction accuracy of the HR HSI\nrelies heavily on the registration accuracy of different modalities. This paper\nexploits the uncharted problem domain of HSI-SR without the requirement of\nmulti-modality registration. Given the unregistered LR HSI and HR MSI with\noverlapped regions, we design a unique unsupervised learning structure linking\nthe two unregistered modalities by projecting them into the same statistical\nspace through the same encoder. The mutual information (MI) is further adopted\nto capture the non-linear statistical dependencies between the representations\nfrom two modalities (carrying spatial information) and their raw inputs. By\nmaximizing the MI, spatial correlations between different modalities can be\nwell characterized to further reduce the spectral distortion. A collaborative\n$l_{2,1}$ norm is employed as the reconstruction error instead of the more\ncommon $l_2$ norm, so that individual pixels can be recovered as accurately as\npossible. With this design, the network allows to extract correlated spectral\nand spatial information from unregistered images that better preserves the\nspectral information. The proposed method is referred to as unregistered and\nunsupervised mutual Dirichlet Net ($u^2$-MDN). Extensive experimental results\nusing benchmark HSI datasets demonstrate the superior performance of $u^2$-MDN\nas compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 16:38:35 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 13:44:25 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 20:02:23 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 10:18:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Qu", "Ying", ""], ["Qi", "Hairong", ""], ["Kwan", "Chiman", ""], ["Yokoya", "Naoto", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1904.12181", "submitter": "Guanbin Li", "authors": "Xiang He, Sibei Yang, Guanbin Li?, Haofeng Li, Huiyou Chang, Yizhou Yu", "title": "Non-Local Context Encoder: Robust Biomedical Image Segmentation against\n  Adversarial Attacks", "comments": "Accepted by AAAI2019 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in biomedical image segmentation based on deep convolutional\nneural networks (CNNs) has drawn much attention. However, its vulnerability\ntowards adversarial samples cannot be overlooked. This paper is the first one\nthat discovers that all the CNN-based state-of-the-art biomedical image\nsegmentation models are sensitive to adversarial perturbations. This limits the\ndeployment of these methods in safety-critical biomedical fields. In this\npaper, we discover that global spatial dependencies and global contextual\ninformation in a biomedical image can be exploited to defend against\nadversarial attacks. To this end, non-local context encoder (NLCE) is proposed\nto model short- and long range spatial dependencies and encode global contexts\nfor strengthening feature activations by channel-wise attention. The NLCE\nmodules enhance the robustness and accuracy of the non-local context encoding\nnetwork (NLCEN), which learns robust enhanced pyramid feature representations\nwith NLCE modules, and then integrates the information across different levels.\nExperiments on both lung and skin lesion segmentation datasets have\ndemonstrated that NLCEN outperforms any other state-of-the-art biomedical image\nsegmentation methods against adversarial attacks. In addition, NLCE modules can\nbe applied to improve the robustness of other CNN-based biomedical image\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 16:59:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["He", "Xiang", ""], ["Yang", "Sibei", ""], ["Li?", "Guanbin", ""], ["Li", "Haofeng", ""], ["Chang", "Huiyou", ""], ["Yu", "Yizhou", ""]]}, {"id": "1904.12200", "submitter": "Anmol Sharma", "authors": "Anmol Sharma, Ghassan Hamarneh", "title": "Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative\n  Adversarial Network", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is being increasingly utilized to assess,\ndiagnose, and plan treatment for a variety of diseases. The ability to\nvisualize tissue in varied contrasts in the form of MR pulse sequences in a\nsingle scan provides valuable insights to physicians, as well as enabling\nautomated systems performing downstream analysis. However many issues like\nprohibitive scan time, image corruption, different acquisition protocols, or\nallergies to certain contrast materials may hinder the process of acquiring\nmultiple sequences for a patient. This poses challenges to both physicians and\nautomated systems since complementary information provided by the missing\nsequences is lost. In this paper, we propose a variant of generative\nadversarial network (GAN) capable of leveraging redundant information contained\nwithin multiple available sequences in order to generate one or more missing\nsequences for a patient scan. The proposed network is designed as a\nmulti-input, multi-output network which combines information from all the\navailable pulse sequences, implicitly infers which sequences are missing, and\nsynthesizes the missing ones in a single forward pass. We demonstrate and\nvalidate our method on two brain MRI datasets each with four sequences, and\nshow the applicability of the proposed method in simultaneously synthesizing\nall missing sequences in any possible scenario where either one, two, or three\nof the four sequences may be missing. We compare our approach with competing\nunimodal and multi-modal methods, and show that we outperform both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:15:15 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 19:08:43 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 00:20:13 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Sharma", "Anmol", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1904.12201", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Yixuan Zhang, Jiebo Luo", "title": "Human-Centered Emotion Recognition in Animated GIFs", "comments": "Accepted to IEEE International Conference on Multimedia and Expo\n  (ICME) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an intuitive way of expression emotion, the animated Graphical Interchange\nFormat (GIF) images have been widely used on social media. Most previous\nstudies on automated GIF emotion recognition fail to effectively utilize GIF's\nunique properties, and this potentially limits the recognition performance. In\nthis study, we demonstrate the importance of human related information in GIFs\nand conduct human-centered GIF emotion recognition with a proposed Keypoint\nAttended Visual Attention Network (KAVAN). The framework consists of a facial\nattention module and a hierarchical segment temporal module. The facial\nattention module exploits the strong relationship between GIF contents and\nhuman characters, and extracts frame-level visual feature with a focus on human\nfaces. The Hierarchical Segment LSTM (HS-LSTM) module is then proposed to\nbetter learn global GIF representations. Our proposed framework outperforms the\nstate-of-the-art on the MIT GIFGIF dataset. Furthermore, the facial attention\nmodule provides reliable facial region mask predictions, which improves the\nmodel's interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:16:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Zhang", "Yixuan", ""], ["Luo", "Jiebo", ""]]}, {"id": "1904.12220", "submitter": "Sachin Vernekar", "authors": "Sachin Vernekar, Ashish Gaurav, Taylor Denouden, Buu Phan, Vahdat\n  Abdelzad, Rick Salay, Krzysztof Czarnecki", "title": "Analysis of Confident-Classifiers for Out-of-distribution Detection", "comments": "SafeML 2019 ICLR workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminatively trained neural classifiers can be trusted, only when the\ninput data comes from the training distribution (in-distribution). Therefore,\ndetecting out-of-distribution (OOD) samples is very important to avoid\nclassification errors. In the context of OOD detection for image\nclassification, one of the recent approaches proposes training a classifier\ncalled \"confident-classifier\" by minimizing the standard cross-entropy loss on\nin-distribution samples and minimizing the KL divergence between the predictive\ndistribution of OOD samples in the low-density regions of in-distribution and\nthe uniform distribution (maximizing the entropy of the outputs). Thus, the\nsamples could be detected as OOD if they have low confidence or high entropy.\nIn this paper, we analyze this setting both theoretically and experimentally.\nWe conclude that the resulting confident-classifier still yields arbitrarily\nhigh confidence for OOD samples far away from the in-distribution. We instead\nsuggest training a classifier by adding an explicit \"reject\" class for OOD\nsamples.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 22:33:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Vernekar", "Sachin", ""], ["Gaurav", "Ashish", ""], ["Denouden", "Taylor", ""], ["Phan", "Buu", ""], ["Abdelzad", "Vahdat", ""], ["Salay", "Rick", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1904.12222", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Giri Narra, Zhifeng Lin, Ganesh Ananthanarayanan, Salman\n  Avestimehr, Murali Annavaram", "title": "Collage Inference: Using Coded Redundancy for Low Variance Distributed\n  Image Classification", "comments": "10 pages, Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLaaS (ML-as-a-Service) offerings by cloud computing platforms are becoming\nincreasingly popular. Hosting pre-trained machine learning models in the cloud\nenables elastic scalability as the demand grows. But providing low latency and\nreducing the latency variance is a key requirement. Variance is harder to\ncontrol in a cloud deployment due to uncertainties in resource allocations\nacross many virtual instances. We propose the collage inference technique which\nuses a novel convolutional neural network model, collage-cnn, to provide\nlow-cost redundancy. A collage-cnn model takes a collage image formed by\ncombining multiple images and performs multi-image classification in one shot,\nalbeit at slightly lower accuracy. We augment a collection of traditional\nsingle image classifier models with a single collage-cnn classifier which acts\nas their low-cost redundant backup. Collage-cnn provides backup classification\nresults if any single image classification requests experience slowdown.\nDeploying the collage-cnn models in the cloud, we demonstrate that the 99th\npercentile tail latency of inference can be reduced by 1.2x to 2x compared to\nreplication based approaches while providing high accuracy. Variation in\ninference latency can be reduced by 1.8x to 15x.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 22:56:10 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 17:25:42 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Narra", "Krishna Giri", ""], ["Lin", "Zhifeng", ""], ["Ananthanarayanan", "Ganesh", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "1904.12228", "submitter": "Tzu-Mao Li", "authors": "Tzu-Mao Li", "title": "Differentiable Visual Computing", "comments": "PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivatives of computer graphics, image processing, and deep learning\nalgorithms have tremendous use in guiding parameter space searches, or solving\ninverse problems. As the algorithms become more sophisticated, we no longer\nonly need to differentiate simple mathematical functions, but have to deal with\ngeneral programs which encode complex transformations of data. This\ndissertation introduces three tools for addressing the challenges that arise\nwhen obtaining and applying the derivatives for complex graphics algorithms.\n  Traditionally, practitioners have been constrained to composing programs with\na limited set of operators, or hand-deriving derivatives. We extend the image\nprocessing language Halide with reverse-mode automatic differentiation, and the\nability to automatically optimize the gradient computations. This enables\nautomatic generation of the gradients of arbitrary Halide programs, at high\nperformance, with little programmer effort.\n  In 3D rendering, the gradient is required with respect to variables such as\ncamera parameters, geometry, and appearance. However, computing the gradient is\nchallenging because the rendering integral includes visibility terms that are\nnot differentiable. We introduce, to our knowledge, the first general-purpose\ndifferentiable ray tracer that solves the full rendering equation, while\ncorrectly taking the geometric discontinuities into account.\n  Finally, we demonstrate that the derivatives of light path throughput can\nalso be useful for guiding sampling in forward rendering. Simulating light\ntransport in the presence of multi-bounce glossy effects and motion in 3D\nrendering is challenging due to the hard-to-sample high-contribution areas. We\npresent a Markov Chain Monte Carlo rendering algorithm that extends Metropolis\nLight Transport by automatically and explicitly adapting to the local\nintegrand, thereby increasing sampling efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 23:43:07 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 11:34:03 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Li", "Tzu-Mao", ""]]}, {"id": "1904.12245", "submitter": "Mingzhu Zhu", "authors": "Zhu Mingzhu, He Bingwei, Liu Jiantao", "title": "Weighted Dark Channel Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dark channel based methods, local constant assumption is widely used to\nmake the algorithms invertible. It inevitably introduces defects since the\nassumption can not perfectly avoid depth discontinuities and meanwhile cover\nenough pixels. Unfortunately, because of the limitation of the prior, which\nonly confirms the existence of dark things but does not specify their locations\nor likelihood, no fidelity measurement is available in refinement thus the\ndefects are either under-corrected or over-corrected. In this paper, we go\ndeeper than the dark channel theory to overcome this problem. We split the\nconcept of dark channel into dark pixels and local constant assumption, and\nthen, control the problematic assumption based on a novel weight map. With such\neffort, our methods show significant improvement on quality and have\ncompetitive speed. In the last, we show that the method is highly robust to\ninitial transmission estimates and can be ever-improved by providing better\ndark pixel locations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 02:30:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mingzhu", "Zhu", ""], ["Bingwei", "He", ""], ["Jiantao", "Liu", ""]]}, {"id": "1904.12251", "submitter": "Bin Zhao", "authors": "Bin Zhao, Xuelong Li, Xiaoqiang Lu", "title": "Hierarchical Recurrent Neural Network for Video Summarization", "comments": "published by ACM Conference on MultiMedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the temporal dependency among video frames or subshots is very\nimportant for the task of video summarization. Practically, RNN is good at\ntemporal dependency modeling, and has achieved overwhelming performance in many\nvideo-based tasks, such as video captioning and classification. However, RNN is\nnot capable enough to handle the video summarization task, since traditional\nRNNs, including LSTM, can only deal with short videos, while the videos in the\nsummarization task are usually in longer duration. To address this problem, we\npropose a hierarchical recurrent neural network for video summarization, called\nH-RNN in this paper. Specifically, it has two layers, where the first layer is\nutilized to encode short video subshots cut from the original video, and the\nfinal hidden state of each subshot is input to the second layer for calculating\nits confidence to be a key subshot. Compared to traditional RNNs, H-RNN is more\nsuitable to video summarization, since it can exploit long temporal dependency\namong frames, meanwhile, the computation operations are significantly lessened.\nThe results on two popular datasets, including the Combined dataset and VTW\ndataset, have demonstrated that the proposed H-RNN outperforms the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 03:32:21 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhao", "Bin", ""], ["Li", "Xuelong", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1904.12254", "submitter": "Dapeng Du", "authors": "Dapeng Du, Limin Wang, Huiling Wang, Kai Zhao, Gangshan Wu", "title": "Translate-to-Recognize Networks for RGB-D Scene Recognition", "comments": "Accepted by CVPR 2019. Project:\n  https://ownstyledu.github.io/Translate-to-Recognize-Networks/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal transfer is helpful to enhance modality-specific discriminative\npower for scene recognition. To this end, this paper presents a unified\nframework to integrate the tasks of cross-modal translation and\nmodality-specific recognition, termed as Translate-to-Recognize Network\n(TRecgNet). Specifically, both translation and recognition tasks share the same\nencoder network, which allows to explicitly regularize the training of\nrecognition task with the help of translation, and thus improve its final\ngeneralization ability. For translation task, we place a decoder module on top\nof the encoder network and it is optimized with a new layer-wise semantic loss,\nwhile for recognition task, we use a linear classifier based on the feature\nembedding from encoder and its training is guided by the standard cross-entropy\nloss. In addition, our TRecgNet allows to exploit large numbers of unlabeled\nRGB-D data to train the translation task and thus improve the representation\npower of encoder network. Empirically, we verify that this new semi-supervised\nsetting is able to further enhance the performance of recognition network. We\nperform experiments on two RGB-D scene recognition benchmarks: NYU Depth v2 and\nSUN RGB-D, demonstrating that TRecgNet achieves superior performance to the\nexisting state-of-the-art methods, especially for recognition solely based on a\nsingle modality.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 03:51:23 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Du", "Dapeng", ""], ["Wang", "Limin", ""], ["Wang", "Huiling", ""], ["Zhao", "Kai", ""], ["Wu", "Gangshan", ""]]}, {"id": "1904.12257", "submitter": "Shangchen Zhou", "authors": "Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie, Wangmeng Zuo,\n  Jimmy Ren", "title": "Spatio-Temporal Filter Adaptive Network for Video Deblurring", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video deblurring is a challenging task due to the spatially variant blur\ncaused by camera shake, object motions, and depth variations, etc. Existing\nmethods usually estimate optical flow in the blurry video to align consecutive\nframes or approximate blur kernels. However, they tend to generate artifacts or\ncannot effectively remove blur when the estimated optical flow is not accurate.\nTo overcome the limitation of separate optical flow estimation, we propose a\nSpatio-Temporal Filter Adaptive Network (STFAN) for the alignment and\ndeblurring in a unified framework. The proposed STFAN takes both blurry and\nrestored images of the previous frame as well as blurry image of the current\nframe as input, and dynamically generates the spatially adaptive filters for\nthe alignment and deblurring. We then propose the new Filter Adaptive\nConvolutional (FAC) layer to align the deblurred features of the previous frame\nwith the current frame and remove the spatially variant blur from the features\nof the current frame. Finally, we develop a reconstruction network which takes\nthe fusion of two transformed features to restore the clear frames. Both\nquantitative and qualitative evaluation results on the benchmark datasets and\nreal-world videos demonstrate that the proposed algorithm performs favorably\nagainst state-of-the-art methods in terms of accuracy, speed as well as model\nsize.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 04:35:19 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 07:34:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zhou", "Shangchen", ""], ["Zhang", "Jiawei", ""], ["Pan", "Jinshan", ""], ["Xie", "Haozhe", ""], ["Zuo", "Wangmeng", ""], ["Ren", "Jimmy", ""]]}, {"id": "1904.12271", "submitter": "Ahmed Imtiaz Humayun", "authors": "Asif Shahriyar Sushmit, Shakib Uz Zaman, Ahmed Imtiaz Humayun, Taufiq\n  Hasan and Mohammed Imamul Hassan Bhuiyan", "title": "X-Ray Image Compression Using Convolutional Recurrent Neural Networks", "comments": "4 pages, 2 figures, IEEE BHI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the advent of a digital health revolution, vast amounts of clinical data\nare being generated, stored and processed on a daily basis. This has made the\nstorage and retrieval of large volumes of health-care data, especially,\nhigh-resolution medical images, particularly challenging. Effective image\ncompression for medical images thus plays a vital role in today's healthcare\ninformation system, particularly in teleradiology. In this work, an X-ray image\ncompression method based on a Convolutional Recurrent Neural Networks RNN-Conv\nis presented. The proposed architecture can provide variable compression rates\nduring deployment while it requires each network to be trained only once for a\nspecific dimension of X-ray images. The model uses a multi-level pooling scheme\nthat learns contextualized features for effective compression. We perform our\nimage compression experiments on the National Institute of Health (NIH)\nChestX-ray8 dataset and compare the performance of the proposed architecture\nwith a state-of-the-art RNN based technique and JPEG 2000. The experimental\nresults depict improved compression performance achieved by the proposed method\nin terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio\n(PSNR) metrics. To the best of our knowledge, this is the first reported\nevaluation on using a deep convolutional RNN for medical image compression.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 07:40:41 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 16:05:36 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Sushmit", "Asif Shahriyar", ""], ["Zaman", "Shakib Uz", ""], ["Humayun", "Ahmed Imtiaz", ""], ["Hasan", "Taufiq", ""], ["Bhuiyan", "Mohammed Imamul Hassan", ""]]}, {"id": "1904.12274", "submitter": "Yongsheng Dong", "authors": "Xuelong Li, Quanmao Lu, Yongsheng Dong, and Dacheng Tao", "title": "Robust subspace clustering by Cauchy loss function", "comments": "13 pages, 5 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2018", "doi": "10.1109/TNNLS.2018.2876327.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is a problem of exploring the low-dimensional subspaces\nof high-dimensional data. State-of-the-arts approaches are designed by\nfollowing the model of spectral clustering based method. These methods pay much\nattention to learn the representation matrix to construct a suitable similarity\nmatrix and overlook the influence of the noise term on subspace clustering.\nHowever, the real data are always contaminated by the noise and the noise\nusually has a complicated statistical distribution. To alleviate this problem,\nwe in this paper propose a subspace clustering method based on Cauchy loss\nfunction (CLF). Particularly, it uses CLF to penalize the noise term for\nsuppressing the large noise mixed in the real data. This is due to that the\nCLF's influence function has a upper bound which can alleviate the influence of\na single sample, especially the sample with a large noise, on estimating the\nresiduals. Furthermore, we theoretically prove the grouping effect of our\nproposed method, which means that highly correlated data can be grouped\ntogether. Finally, experimental results on five real datasets reveal that our\nproposed method outperforms several representative clustering methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 07:58:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Xuelong", ""], ["Lu", "Quanmao", ""], ["Dong", "Yongsheng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.12284", "submitter": "Zehua Wang", "authors": "Wei Hu, Qianjiang Hu, Zehua Wang, and Xiang Gao", "title": "3D Dynamic Point Cloud Denoising via Spatial-Temporal Graph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of accessible depth sensing and 3D laser scanning techniques\nhas enabled the convenient acquisition of 3D dynamic point clouds, which\nprovide efficient representation of arbitrarily-shaped objects in motion.\nNevertheless, dynamic point clouds are often perturbed by noise due to\nhardware, software or other causes. While a plethora of methods have been\nproposed for static point cloud denoising, few efforts are made for the\ndenoising of dynamic point clouds with varying number of irregularly-sampled\npoints in each frame. In this paper, we represent dynamic point clouds\nnaturally on graphs and address the denoising problem by inferring the\nunderlying graph via spatio-temporal graph learning, exploiting both the\nintra-frame similarity and inter-frame consistency. Firstly, assuming the\navailability of a relevant feature vector per node, we pose spatial-temporal\ngraph learning as optimizing a Mahalanobis distance metric $\\mathbf{M}$, which\nis formulated as the minimization of graph Laplacian regularizer. Secondly, to\nease the optimization of the symmetric and positive definite metric matrix\n$\\mathbf{M}$, we decompose it into $\\mathbf{M}=\\mathbf{R}^{\\top}\\mathbf{R}$ and\nsolve $\\mathbf{R}$ instead via proximal gradient. Finally, based on the\nspatial-temporal graph learning, we formulate dynamic point cloud denoising as\nthe joint optimization of the desired point cloud and underlying\nspatio-temporal graph, which leverages both intra-frame affinities and\ninter-frame consistency and is solved via alternating minimization.\nExperimental results show that the proposed method significantly outperforms\nindependent denoising of each frame from state-of-the-art static point cloud\ndenoising approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 09:07:26 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:06:17 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Hu", "Wei", ""], ["Hu", "Qianjiang", ""], ["Wang", "Zehua", ""], ["Gao", "Xiang", ""]]}, {"id": "1904.12294", "submitter": "Kai Wang", "authors": "Kai Wang and Fuyuan Shi and Wenqi Wang and Yibing Nan and Shiguo Lian", "title": "Synthetic Data Generation and Adaption for Object Detection in Smart\n  Vending Machines", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improved scheme for the generation and adaption of\nsynthetic images for the training of deep Convolutional Neural Networks(CNNs)\nto perform the object detection task in smart vending machines. While\ngenerating synthetic data has proved to be effective for complementing the\ntraining data in supervised learning methods, challenges still exist for\ngenerating virtual images which are similar to those of the complex real scenes\nand minimizing redundant training data. To solve these problems, we consider\nthe simulation of cluttered objects placed in a virtual scene and the\nwide-angle camera with distortions used to capture the whole scene in the data\ngeneration process, and post-processed the generated images with a\nelaborately-designed generative network to make them more similar to the real\nimages. Various experiments have been conducted to prove the efficiency of\nusing the generated virtual images to enhance the detection precision on\nexisting datasets with limited real training data and the generalization\nability of applying the trained network to datasets collected in new\nenvironment.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 10:16:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wang", "Kai", ""], ["Shi", "Fuyuan", ""], ["Wang", "Wenqi", ""], ["Nan", "Yibing", ""], ["Lian", "Shiguo", ""]]}, {"id": "1904.12304", "submitter": "Muhammad Sarmad", "authors": "Muhammad Sarmad, Hyunjoo Jenny Lee, Young Min Kim", "title": "RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for\n  Real-Time Point Cloud Shape Completion", "comments": "Accepted to IEEE CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present RL-GAN-Net, where a reinforcement learning (RL) agent provides\nfast and robust control of a generative adversarial network (GAN). Our\nframework is applied to point cloud shape completion that converts noisy,\npartial point cloud data into a high-fidelity completed shape by controlling\nthe GAN. While a GAN is unstable and hard to train, we circumvent the problem\nby (1) training the GAN on the latent space representation whose dimension is\nreduced compared to the raw point cloud input and (2) using an RL agent to find\nthe correct input to the GAN to generate the latent space representation of the\nshape that best fits the current input of incomplete point cloud. The suggested\npipeline robustly completes point cloud with large missing regions. To the best\nof our knowledge, this is the first attempt to train an RL agent to control the\nGAN, which effectively learns the highly nonlinear mapping from the input noise\nof the GAN to the latent space of point cloud. The RL agent replaces the need\nfor complex optimization and consequently makes our technique real time.\nAdditionally, we demonstrate that our pipelines can be used to enhance the\nclassification accuracy of point cloud with missing data.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 11:08:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sarmad", "Muhammad", ""], ["Lee", "Hyunjoo Jenny", ""], ["Kim", "Young Min", ""]]}, {"id": "1904.12319", "submitter": "Ran Bakalo", "authors": "Ran Bakalo, Rami Ben-Ari and Jacob Goldberger", "title": "Classification and Detection in Mammograms with Weak Supervision via\n  Dual Branch Deep Neural Net", "comments": "Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)\n  2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The high cost of generating expert annotations, poses a strong limitation for\nsupervised machine learning methods in medical imaging. Weakly supervised\nmethods may provide a solution to this tangle. In this study, we propose a\nnovel deep learning architecture for multi-class classification of mammograms\naccording to the severity of their containing anomalies, having only a global\ntag over the image. The suggested scheme further allows localization of the\ndifferent types of findings in full resolution. The new scheme contains a dual\nbranch network that combines region-level classification with region ranking.\nWe evaluate our method on a large multi-center mammography dataset including\n$\\sim$3,000 mammograms with various anomalies and demonstrate the advantages of\nthe proposed method over a previous weakly-supervised strategy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:11:10 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bakalo", "Ran", ""], ["Ben-Ari", "Rami", ""], ["Goldberger", "Jacob", ""]]}, {"id": "1904.12323", "submitter": "Rohit Jena", "authors": "Rohit Jena", "title": "An approach to image denoising using manifold approximation without\n  clean images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image restoration has been an extensively researched topic in numerous\nfields. With the advent of deep learning, a lot of the current algorithms were\nreplaced by algorithms that are more flexible and robust. Deep networks have\ndemonstrated impressive performance in a variety of tasks like blind denoising,\nimage enhancement, deblurring, super-resolution, inpainting, among others. Most\nof these learning-based algorithms use a large amount of clean data during the\ntraining process. However, in certain applications in medical image processing,\none may not have access to a large amount of clean data. In this paper, we\npropose a method for denoising that attempts to learn the denoising process by\npushing the noisy data close to the clean data manifold, using only noisy\nimages during training. Furthermore, we use perceptual loss terms and an\niterative refinement step to further refine the clean images without losing\nimportant features.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:53:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Jena", "Rohit", ""]]}, {"id": "1904.12342", "submitter": "Tiantu Xu", "authors": "Mengwei Xu, Tiantu Xu, Yunxin Liu, Felix Xiaozhu Lin", "title": "Video Analytics with Zero-streaming Cameras", "comments": "Mengwei Xu and Tiantu Xu contributed equally to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-cost cameras enable powerful analytics. An unexploited opportunity is\nthat most captured videos remain \"cold\" without being queried. For efficiency,\nwe advocate for these cameras to be zero streaming: capturing videos to local\nstorage and communicating with the cloud only when analytics is requested. How\nto query zero-streaming cameras efficiently? Our response is a camera/cloud\nruntime system called DIVA. It addresses two key challenges: to best use\nlimited camera resource during video capture; to rapidly explore massive videos\nduring query execution. DIVA contributes two unconventional techniques. (1)\nWhen capturing videos, a camera builds sparse yet accurate landmark frames,\nfrom which it learns reliable knowledge for accelerating future queries. (2)\nWhen executing a query, a camera processes frames in multiple passes with\nincreasingly more expensive operators. As such, DIVA presents and keeps\nrefining inexact query results throughout the query's execution. On diverse\nqueries over 15 videos lasting 720 hours in total, DIVA runs at more than 100x\nvideo realtime and outperforms competitive alternative designs. To our\nknowledge, DIVA is the first system for querying large videos stored on\nlow-cost remote cameras.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 16:35:02 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 21:55:59 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 18:32:15 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 06:03:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xu", "Mengwei", ""], ["Xu", "Tiantu", ""], ["Liu", "Yunxin", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "1904.12347", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Zijun Huang, Ximeng Sun, Kate Saenko", "title": "Domain Agnostic Learning with Disentangled Representations", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, Long Beach, California, PMLR 97, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised model transfer has the potential to greatly improve the\ngeneralizability of deep models to novel domains. Yet the current literature\nassumes that the separation of target data into distinct domains is known as a\npriori. In this paper, we propose the task of Domain-Agnostic Learning (DAL):\nHow to transfer knowledge from a labeled source domain to unlabeled data from\narbitrary target domains? To tackle this problem, we devise a novel Deep\nAdversarial Disentangled Autoencoder (DADA) capable of disentangling\ndomain-specific features from class identity. We demonstrate experimentally\nthat when the target domain labels are unknown, DADA leads to state-of-the-art\nperformance on several image classification datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 17:07:10 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Peng", "Xingchao", ""], ["Huang", "Zijun", ""], ["Sun", "Ximeng", ""], ["Saenko", "Kate", ""]]}, {"id": "1904.12356", "submitter": "Justus Thies", "authors": "Justus Thies and Michael Zollh\\\"ofer and Matthias Nie{\\ss}ner", "title": "Deferred Neural Rendering: Image Synthesis using Neural Textures", "comments": "Video: https://youtu.be/z-pVip6WeyY SIGGRAPH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern computer graphics pipeline can synthesize images at remarkable\nvisual quality; however, it requires well-defined, high-quality 3D content as\ninput. In this work, we explore the use of imperfect 3D content, for instance,\nobtained from photo-metric reconstructions with noisy and incomplete surface\ngeometry, while still aiming to produce photo-realistic (re-)renderings. To\naddress this challenging problem, we introduce Deferred Neural Rendering, a new\nparadigm for image synthesis that combines the traditional graphics pipeline\nwith learnable components. Specifically, we propose Neural Textures, which are\nlearned feature maps that are trained as part of the scene capture process.\nSimilar to traditional textures, neural textures are stored as maps on top of\n3D mesh proxies; however, the high-dimensional feature maps contain\nsignificantly more information, which can be interpreted by our new deferred\nneural rendering pipeline. Both neural textures and deferred neural renderer\nare trained end-to-end, enabling us to synthesize photo-realistic images even\nwhen the original 3D content was imperfect. In contrast to traditional,\nblack-box 2D generative neural networks, our 3D representation gives us\nexplicit control over the generated output, and allows for a wide range of\napplication domains. For instance, we can synthesize temporally-consistent\nvideo re-renderings of recorded 3D scenes as our representation is inherently\nembedded in 3D space. This way, neural textures can be utilized to coherently\nre-render or manipulate existing video content in both static and dynamic\nenvironments at real-time rates. We show the effectiveness of our approach in\nseveral experiments on novel view synthesis, scene editing, and facial\nreenactment, and compare to state-of-the-art approaches that leverage the\nstandard graphics pipeline as well as conventional generative neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:00:08 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1904.12359", "submitter": "Ling Zhang", "authors": "Ling Zhang and Zhigang Zhu", "title": "Unsupervised Feature Learning for Point Cloud by Contrasting and\n  Clustering With Graph Convolutional Neural Network", "comments": "Accepted by 3DV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the cost of collecting and annotating large-scale point cloud\ndatasets, we propose an unsupervised learning approach to learn features from\nunlabeled point cloud \"3D object\" dataset by using part contrasting and object\nclustering with deep graph neural networks (GNNs). In the contrast learning\nstep, all the samples in the 3D object dataset are cut into two parts and put\ninto a \"part\" dataset. Then a contrast learning GNN (ContrastNet) is trained to\nverify whether two randomly sampled parts from the part dataset belong to the\nsame object. In the cluster learning step, the trained ContrastNet is applied\nto all the samples in the original 3D object dataset to extract features, which\nare used to group the samples into clusters. Then another GNN for clustering\nlearning (ClusterNet) is trained to predict the cluster ID of all the training\nsamples. The contrasting learning forces the ContrastNet to learn high-level\nsemantic features of objects but probably ignores low-level features, while the\nClusterNet improves the quality of learned features by being trained to\ndiscover objects that probably belong to the same semantic categories by the\nuse of cluster IDs. We have conducted extensive experiments to evaluate the\nproposed framework on point cloud classification tasks. The proposed\nunsupervised learning approach obtained comparable performance to the\nstate-of-the-art unsupervised learning methods that used much more complicated\nnetwork structures. The code of this work is publicly available via:\nhttps://github.com/lingzhang1/ContrastNet.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:21:13 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 00:54:30 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 01:47:15 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Zhang", "Ling", ""], ["Zhu", "Zhigang", ""]]}, {"id": "1904.12368", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin and Ruizhou Ding and Cha Zhang and Diana Marculescu", "title": "Towards Efficient Model Compression via Learned Global Ranking", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning convolutional filters has demonstrated its effectiveness in\ncompressing ConvNets. Prior art in filter pruning requires users to specify a\ntarget model complexity (e.g., model size or FLOP count) for the resulting\narchitecture. However, determining a target model complexity can be difficult\nfor optimizing various embodied AI applications such as autonomous robots,\ndrones, and user-facing applications. First, both the accuracy and the speed of\nConvNets can affect the performance of the application. Second, the performance\nof the application can be hard to assess without evaluating ConvNets during\ninference. As a consequence, finding a sweet-spot between the accuracy and\nspeed via filter pruning, which needs to be done in a trial-and-error fashion,\ncan be time-consuming. This work takes a first step toward making this process\nmore efficient by altering the goal of model compression to producing a set of\nConvNets with various accuracy and latency trade-offs instead of producing one\nConvNet targeting some pre-defined latency constraint. To this end, we propose\nto learn a global ranking of the filters across different layers of the\nConvNet, which is used to obtain a set of ConvNet architectures that have\ndifferent accuracy/latency trade-offs by pruning the bottom-ranked filters. Our\nproposed algorithm, LeGR, is shown to be 2x to 3x faster than prior work while\nhaving comparable or better performance when targeting seven pruned ResNet-56\nwith different accuracy/FLOPs profiles on the CIFAR-100 dataset. Additionally,\nwe have evaluated LeGR on ImageNet and Bird-200 with ResNet-50 and MobileNetV2\nto demonstrate its effectiveness. Code available at\nhttps://github.com/cmu-enyac/LeGR.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:51:26 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 05:53:58 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Ding", "Ruizhou", ""], ["Zhang", "Cha", ""], ["Marculescu", "Diana", ""]]}, {"id": "1904.12374", "submitter": "Masha Itkina", "authors": "Masha Itkina, Katherine Driggs-Campbell, and Mykel J. Kochenderfer", "title": "Dynamic Environment Prediction in Urban Scenes using Recurrent\n  Representation Learning", "comments": "8 pages, updated final draft, accepted into Intelligent\n  Transportation Systems Conference (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge for autonomous driving is safe trajectory planning in\ncluttered, urban environments with dynamic obstacles, such as pedestrians,\nbicyclists, and other vehicles. A reliable prediction of the future\nenvironment, including the behavior of dynamic agents, would allow planning\nalgorithms to proactively generate a trajectory in response to a rapidly\nchanging environment. We present a novel framework that predicts the future\noccupancy state of the local environment surrounding an autonomous agent by\nlearning a motion model from occupancy grid data using a neural network. We\ntake advantage of the temporal structure of the grid data by utilizing a\nconvolutional long-short term memory network in the form of the PredNet\narchitecture. This method is validated on the KITTI dataset and demonstrates\nhigher accuracy and better predictive power than baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 19:41:59 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 20:24:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Itkina", "Masha", ""], ["Driggs-Campbell", "Katherine", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1904.12387", "submitter": "Fady Medhat", "authors": "Fady Medhat, Mahnaz Mohammadi, Sardar Jaf, Chris G. Willcocks, Toby P.\n  Breckon, Peter Matthews, Andrew Stephen McGough, Georgios Theodoropoulos and\n  Boguslaw Obara", "title": "TMIXT: A process flow for Transcribing MIXed handwritten and\n  machine-printed Text", "comments": "big data, unstructured data, Optical Character Recognition (OCR),\n  Handwritten Text Recognition (HTR), machine-printed text recognition, IAM\n  handwriting database, TMIXT", "journal-ref": "IEEE International Conference on Big Data (Big Data) 2018", "doi": "10.1109/BigData.2018.8622136", "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling large corpuses of documents is of significant importance in many\nfields, no more so than in the areas of crime investigation and defence, where\nan organisation may be presented with a large volume of scanned documents which\nneed to be processed in a finite time. However, this problem is exacerbated\nboth by the volume, in terms of scanned documents and the complexity of the\npages, which need to be processed. Often containing many different elements,\nwhich each need to be processed and understood. Text recognition, which is a\nprimary task of this process, is usually dependent upon the type of text, being\neither handwritten or machine-printed. Accordingly, the recognition involves\nprior classification of the text category, before deciding on the recognition\nmethod to be applied. This poses a more challenging task if a document contains\nboth handwritten and machine-printed text. In this work, we present a generic\nprocess flow for text recognition in scanned documents containing mixed\nhandwritten and machine-printed text without the need to classify text in\nadvance. We realize the proposed process flow using several open-source image\nprocessing and text recognition packages1. The evaluation is performed using a\nspecially developed variant, presented in this work, of the IAM handwriting\ndatabase, where we achieve an average transcription accuracy of nearly 80% for\npages containing both printed and handwritten text.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 21:46:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Medhat", "Fady", ""], ["Mohammadi", "Mahnaz", ""], ["Jaf", "Sardar", ""], ["Willcocks", "Chris G.", ""], ["Breckon", "Toby P.", ""], ["Matthews", "Peter", ""], ["McGough", "Andrew Stephen", ""], ["Theodoropoulos", "Georgios", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1904.12426", "submitter": "Taesik Na", "authors": "Taesik Na, Minah Lee, Burhan A. Mudassar, Priyabrata Saha, Jong Hwan\n  Ko, Saibal Mukhopadhyay", "title": "Mixture of Pre-processing Experts Model for Noise Robust Deep Learning\n  on Resource Constrained Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 02:26:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Na", "Taesik", ""], ["Lee", "Minah", ""], ["Mudassar", "Burhan A.", ""], ["Saha", "Priyabrata", ""], ["Ko", "Jong Hwan", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1904.12428", "submitter": "Xinyang Li", "authors": "Xinyang Li, Jie Hu, Shengchuan Zhang, Xiaopeng Hong, Qixiang Ye,\n  Chenglin Wu, Rongrong Ji", "title": "Attribute Guided Unpaired Image-to-Image Translation with\n  Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired Image-to-Image Translation (UIT) focuses on translating images among\ndifferent domains by using unpaired data, which has received increasing\nresearch focus due to its practical usage. However, existing UIT schemes defect\nin the need of supervised training, as well as the lack of encoding domain\ninformation. In this paper, we propose an Attribute Guided UIT model termed\nAGUIT to tackle these two challenges. AGUIT considers multi-modal and\nmulti-domain tasks of UIT jointly with a novel semi-supervised setting, which\nalso merits in representation disentanglement and fine control of outputs.\nEspecially, AGUIT benefits from two-fold: (1) It adopts a novel semi-supervised\nlearning process by translating attributes of labeled data to unlabeled data,\nand then reconstructing the unlabeled data by a cycle consistency operation.\n(2) It decomposes image representation into domain-invariant content code and\ndomain-specific style code. The redesigned style code embeds image style into\ntwo variables drawn from standard Gaussian distribution and the distribution of\ndomain label, which facilitates the fine control of translation due to the\ncontinuity of both variables. Finally, we introduce a new challenge, i.e.,\ndisentangled transfer, for UIT models, which adopts the disentangled\nrepresentation to translate data less related with the training set. Extensive\nexperiments demonstrate the capacity of AGUIT over existing state-of-the-art\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 02:50:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Xinyang", ""], ["Hu", "Jie", ""], ["Zhang", "Shengchuan", ""], ["Hong", "Xiaopeng", ""], ["Ye", "Qixiang", ""], ["Wu", "Chenglin", ""], ["Ji", "Rongrong", ""]]}, {"id": "1904.12433", "submitter": "Surabhi Verma Ms.", "authors": "Surabhi Verma, Julie Stephany Berrio, Stewart Worrall, Eduardo Nebot", "title": "Automatic extrinsic calibration between a camera and a 3D Lidar using 3D\n  point and plane correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automated method to obtain the extrinsic calibration\nparameters between a camera and a 3D lidar with as low as 16 beams. We use a\ncheckerboard as a reference to obtain features of interest in both sensor\nframes. The calibration board centre point and normal vector are automatically\nextracted from the lidar point cloud by exploiting the geometry of the board.\nThe corresponding features in the camera image are obtained from the camera's\nextrinsic matrix. We explain the reasons behind selecting these features, and\nwhy they are more robust compared to other possibilities. To obtain the optimal\nextrinsic parameters, we choose a genetic algorithm to address the highly\nnon-linear state space. The process is automated after defining the bounds of\nthe 3D experimental region relative to the lidar, and the true board\ndimensions. In addition, the camera is assumed to be intrinsically calibrated.\nOur method requires a minimum of 3 checkerboard poses, and the calibration\naccuracy is demonstrated by evaluating our algorithm using real world and\nsimulated features.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 03:08:33 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Verma", "Surabhi", ""], ["Berrio", "Julie Stephany", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "1904.12434", "submitter": "Masaki Kitayama", "authors": "Masaki Kitayama and Hitoshi Kiya", "title": "HOG feature extraction from encrypted images for privacy-preserving\n  machine learning", "comments": "To appear in The 4th IEEE International Conference on Consumer\n  Electronics (ICCE) Asia, Bankok, Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an extraction method of HOG\n(histograms-of-oriented-gradients) features from encryption-then-compression\n(EtC) images for privacy-preserving machine learning, where EtC images are\nimages encrypted by a block-based encryption method proposed for EtC systems\nwith JPEG compression, and HOG is a feature descriptor used in computer vision\nfor the purpose of object detection and image classification. Recently, cloud\ncomputing and machine learning have been spreading in many fields. However, the\ncloud computing has serious privacy issues for end users, due to unreliability\nof providers and some accidents. Accordingly, we propose a novel block-based\nextraction method of HOG features, and the proposed method enables us to carry\nout any machine learning algorithms without any influence, under some\nconditions. In an experiment, the proposed method is applied to a face image\nrecognition problem under the use of two kinds of classifiers: linear support\nvector machine (SVM), gaussian SVM, to demonstrate the effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 03:11:45 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kitayama", "Masaki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1904.12483", "submitter": "Assaf Hoogi", "authors": "Assaf Hoogi, Brian Wilcox, Yachee Gupta, Daniel L. Rubin", "title": "Self-Attention Capsule Networks for Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture for object classification, called\nSelf-Attention Capsule Networks (SACN). SACN is the first model that\nincorporates the Self-Attention mechanism as an integral layer within the\nCapsule Network (CapsNet). While the Self-Attention mechanism supplies a\nlong-range dependencies, results in selecting the more dominant image regions\nto focus on, the CapsNet analyzes the relevant features and their spatial\ncorrelations inside these regions only. The features are extracted in the\nconvolutional layer. Then, the Self-Attention layer learns to suppress\nirrelevant regions based on features analysis and highlights salient features\nuseful for a specific task. The attention map is then fed into the CapsNet\nprimary layer that is followed by a classification layer. The proposed SACN\nmodel was designed to solve two main limitations of the baseline CapsNet -\nanalysis of complex data and significant computational load. In this work, we\nuse a shallow CapsNet architecture and compensates for the absence of a deeper\nnetwork by using the Self-Attention module to significantly improve the\nresults. The proposed Self-Attention CapsNet architecture was extensively\nevaluated on six different datasets, mainly on three different medical sets, in\naddition to the natural MNIST, SVHN and CIFAR10. The model was able to classify\nimages and their patches with diverse and complex backgrounds better than the\nbaseline CapsNet. As a result, the proposed Self-Attention CapsNet\nsignificantly improved classification performance within and across different\ndatasets and outperformed the baseline CapsNet, ResNet-18 and DenseNet-40 not\nonly in classification accuracy but also in robustness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 08:04:46 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 13:12:08 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Hoogi", "Assaf", ""], ["Wilcox", "Brian", ""], ["Gupta", "Yachee", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1904.12490", "submitter": "Yunxiao Qin", "authors": "Yunxiao Qin, Chenxu Zhao, Xiangyu Zhu, Zezheng Wang, Zitong Yu, Tianyu\n  Fu, Feng Zhou, Jingping Shi, Zhen Lei", "title": "Learning Meta Model for Zero- and Few-shot Face Anti-spoofing", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is crucial to the security of face recognition systems.\nMost previous methods formulate face anti-spoofing as a supervised learning\nproblem to detect various predefined presentation attacks, which need large\nscale training data to cover as many attacks as possible. However, the trained\nmodel is easy to overfit several common attacks and is still vulnerable to\nunseen attacks. To overcome this challenge, the detector should: 1) learn\ndiscriminative features that can generalize to unseen spoofing types from\npredefined presentation attacks; 2) quickly adapt to new spoofing types by\nlearning from both the predefined attacks and a few examples of the new\nspoofing types. Therefore, we define face anti-spoofing as a zero- and few-shot\nlearning problem. In this paper, we propose a novel Adaptive Inner-update Meta\nFace Anti-Spoofing (AIM-FAS) method to tackle this problem through\nmeta-learning. Specifically, AIM-FAS trains a meta-learner focusing on the task\nof detecting unseen spoofing types by learning from predefined living and\nspoofing faces and a few examples of new attacks. To assess the proposed\napproach, we propose several benchmarks for zero- and few-shot FAS. Experiments\nshow its superior performances on the presented benchmarks to existing methods\nin existing zero-shot FAS protocols.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 08:17:00 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 07:43:56 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 03:59:24 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Qin", "Yunxiao", ""], ["Zhao", "Chenxu", ""], ["Zhu", "Xiangyu", ""], ["Wang", "Zezheng", ""], ["Yu", "Zitong", ""], ["Fu", "Tianyu", ""], ["Zhou", "Feng", ""], ["Shi", "Jingping", ""], ["Lei", "Zhen", ""]]}, {"id": "1904.12534", "submitter": "Sinisa Stekovic", "authors": "Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit", "title": "Casting Geometric Constraints in Semantic Segmentation as\n  Semi-Supervised Learning", "comments": "To be presented at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective method to learn to segment new indoor\nscenes from video frames: State-of-the-art methods trained on one dataset, even\nas large as the SUNRGB-D dataset, can perform poorly when applied to images\nthat are not part of the dataset, because of the dataset bias, a common\nphenomenon in computer vision. To make semantic segmentation more useful in\npractice, one can exploit geometric constraints. Our main contribution is to\nshow that these constraints can be cast conveniently as semi-supervised terms,\nwhich enforce the fact that the same class should be predicted for the\nprojections of the same 3D location in different images. This is interesting as\nwe can exploit general existing techniques developed for semi-supervised\nlearning to efficiently incorporate the constraints. We show that this approach\ncan efficiently and accurately learn to segment target sequences of ScanNet and\nour own target sequences using only annotations from SUNRGB-D, and geometric\nrelations between the video frames of target sequences.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:36:12 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 11:15:12 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 08:54:00 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Stekovic", "Sinisa", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1904.12581", "submitter": "Achim J. Lilienthal", "authors": "Achim J. Lilienthal, Maike Schindler", "title": "Current Trends in the Use of Eye Tracking in Mathematics Education\n  Research: A PME Survey", "comments": "It is planned to update this review yearly after the respective PME\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking (ET) is a research method that receives growing interest in\nmathematics education research (MER). This paper aims to give a literature\noverview, specifically focusing on the evolution of interest in this\ntechnology, ET equipment, and analysis methods used in mathematics education.\nTo capture the current state, we focus on papers published in the proceedings\nof PME, one of the primary conferences dedicated to MER, of the last ten years.\nWe identify trends in interest, methodology, and methods of analysis that are\nused in the community, and discuss possible future developments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:41:17 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 10:50:43 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Lilienthal", "Achim J.", ""], ["Schindler", "Maike", ""]]}, {"id": "1904.12584", "submitter": "Jiayuan Mao", "authors": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun\n  Wu", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "comments": "ICLR 2019 (Oral). Project page: http://nscl.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns\nvisual concepts, words, and semantic parsing of sentences without explicit\nsupervision on any of them; instead, our model learns by simply looking at\nimages and reading paired questions and answers. Our model builds an\nobject-based scene representation and translates sentences into executable,\nsymbolic programs. To bridge the learning of two modules, we use a\nneuro-symbolic reasoning module that executes these programs on the latent\nscene representation. Analogical to human concept learning, the perception\nmodule learns visual concepts based on the language description of the object\nbeing referred to. Meanwhile, the learned visual concepts facilitate learning\nnew words and parsing new sentences. We use curriculum learning to guide the\nsearching over the large compositional space of images and language. Extensive\nexperiments demonstrate the accuracy and efficiency of our model on learning\nvisual concepts, word representations, and semantic parsing of sentences.\nFurther, our method allows easy generalization to new object attributes,\ncompositions, language concepts, scenes and questions, and even new program\ndomains. It also empowers applications including visual question answering and\nbidirectional image-text retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:50:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mao", "Jiayuan", ""], ["Gan", "Chuang", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1904.12585", "submitter": "Amirkoushyar Ziabari", "authors": "Amirkoushyar Ziabari, Michael Kirka, Vincent Paquit, Philip Bingham,\n  and Singanallur Venkatakrishnan", "title": "X-Ray CT Reconstruction of Additively Manufactured Parts using 2.5D Deep\n  Learning MBIR", "comments": null, "journal-ref": null, "doi": "10.1017/S1431927619002617", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep learning algorithm to rapidly obtain high\nquality CT reconstructions for AM parts. In particular, we propose to use CAD\nmodels of the parts that are to be manufactured, introduce typical defects and\nsimulate XCT measurements. These simulated measurements were processed using\nFBP (computationally simple but result in noisy images) and the MBIR technique.\nWe then train a 2.5D deep convolutional neural network [4], deemed 2.5D Deep\nLearning MBIR (2.5D DL-MBIR), on these pairs of noisy and high-quality 3D\nvolumes to learn a fast, non-linear mapping function. The 2.5D DL-MBIR\nreconstructs a 3D volume in a 2.5D scheme where each slice is reconstructed\nfrom multiple inputs slices of the FBP input. Given this trained system, we can\ntake a small set of measurements on an actual part, process it using a\ncombination of FBP followed by 2.5D DL-MBIR. Both steps can be rapidly\nperformed using GPUs, resulting in a real-time algorithm that achieves the\nhigh-quality of MBIR as fast as standard techniques. Intuitively, since CAD\nmodels are typically available for parts to be manufactured, this provides a\nstrong constraint \"prior\" which can be leveraged to improve the reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 01:16:29 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 10:45:34 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ziabari", "Amirkoushyar", ""], ["Kirka", "Michael", ""], ["Paquit", "Vincent", ""], ["Bingham", "Philip", ""], ["Venkatakrishnan", "Singanallur", ""]]}, {"id": "1904.12586", "submitter": "Sophie Crommelinck", "authors": "Sophie Crommelinck, Mila Koeva, Michael Ying Yang, George Vosselman", "title": "Robust object extraction from remote sensing data", "comments": "unpublished study (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of object outlines has been a research topic during the last\ndecades. In spite of advances in photogrammetry, remote sensing and computer\nvision, this task remains challenging due to object and data complexity. The\ndevelopment of object extraction approaches is promoted through publically\navailable benchmark datasets and evaluation frameworks. Many aspects of\nperformance evaluation have already been studied. This study collects the best\npractices from literature, puts the various aspects in one evaluation\nframework, and demonstrates its usefulness to a case study on mapping object\noutlines. The evaluation framework includes five dimensions: the robustness to\nchanges in resolution, input, location, parameters, and application. Examples\nfor investigating these dimensions are provided, as well as accuracy measures\nfor their qualitative analysis. The measures consist of time efficiency and a\nprocedure for line-based accuracy assessment regarding quantitative\ncompleteness and spatial correctness. The delineation approach to which the\nevaluation framework is applied, was previously introduced and is substantially\nimproved in this study.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:08:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Crommelinck", "Sophie", ""], ["Koeva", "Mila", ""], ["Yang", "Michael Ying", ""], ["Vosselman", "George", ""]]}, {"id": "1904.12589", "submitter": "Ran Bakalo", "authors": "Ran Bakalo, Jacob Goldberger and Rami Ben-Ari", "title": "Weakly and Semi Supervised Detection in Medical Imaging via Deep Dual\n  Branch Net", "comments": null, "journal-ref": "Neurocomputing, Volume 421, 15 January 2021, Pages 15-25", "doi": "10.1016/j.neucom.2020.09.037", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study presents a novel deep learning architecture for multi-class\nclassification and localization of abnormalities in medical imaging illustrated\nthrough experiments on mammograms. The proposed network combines two learning\nbranches. One branch is for region classification with a newly added\nnormal-region class. Second branch is region detection branch for ranking\nregions relative to one another. Our method enables detection of abnormalities\nat full mammogram resolution for both weakly and semi-supervised settings. A\nnovel objective function allows for the incorporation of local annotations into\nthe model. We present the impact of our schemes on several performance measures\nfor classification and localization, to evaluate the cost effectiveness of the\nlesion annotation effort. Our evaluation was primarily conducted over a large\nmulti-center mammography dataset of $\\sim$3,000 mammograms with various\nfindings. The results for weakly supervised learning showed significant\nimprovement compared to previous approaches. We show that the time consuming\nlocal annotations involved in supervised learning can be addressed by a weakly\nsupervised method that can leverage a subset of locally annotated data. Weakly\nand semi-supervised methods coupled with detection can produce a cost effective\nand explainable model to be adopted by radiologists in the field.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 12:16:45 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 09:44:49 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 08:30:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Bakalo", "Ran", ""], ["Goldberger", "Jacob", ""], ["Ben-Ari", "Rami", ""]]}, {"id": "1904.12592", "submitter": "Amjad Rehman Dr", "authors": "Amjad Rehman", "title": "An Ensemble of Neural Networks for Non-Linear Segmentation of Overlapped\n  Cursive Script", "comments": "15 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise character segmentation is the only solution towards higher Optical\nCharacter Recognition (OCR) accuracy. In cursive script, overlapped characters\nare serious issue in the process of character segmentations as characters are\ndeprived from their discriminative parts using conventional linear segmentation\nstrategy. Hence, non-linear segmentation is an utmost need to avoid loss of\ncharacters parts and to enhance character/script recognition accuracy. This\npaper presents an improved approach for non-linear segmentation of the\noverlapped characters in handwritten roman script. The proposed technique is\ncomposed of a sequence of heuristic rules based on geometrical features of\ncharacters to locate possible non-linear character boundaries in a cursive\nscript word. However, to enhance efficiency, heuristic approach is integrated\nwith trained ensemble neural network validation strategy for verification of\ncharacter boundaries. Accordingly, correct boundaries are retained and\nincorrect are removed based on ensemble neural networks vote. Finally, based on\nverified valid segmentation points, characters are segmented non-linearly. For\nfair comparison CEDAR benchmark database is experimented. The experimental\nresults are much better than conventional linear character segmentation\ntechniques reported in the state of art. Ensemble neural network play vital\nrole to enhance character segmentation accuracy as compared to individual\nneural networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 08:32:37 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rehman", "Amjad", ""]]}, {"id": "1904.12597", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI), Michel Jourlin (IPRI)", "title": "Region homogeneity in the Logarithmic Image Processing framework:\n  application to region growing algorithms", "comments": "The original publication is available at www.ias-iss.org", "journal-ref": "Image Analysis and Stereology, International Society for\n  Stereology, 2019, 38 (1), pp.43-52", "doi": "10.5566/ias.2038", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to create an image segmentation method robust to lighting changes,\ntwo novel homogeneity criteria of an image region were studied. Both were\ndefined using the Logarithmic Image Processing (LIP) framework whose laws model\nlighting changes. The first criterion estimates the LIP-additive homogeneity\nand is based on the LIP-additive law. It is theoretically insensitive to\nlighting changes caused by variations of the camera exposure-time or source\nintensity. The second, the LIP-multiplicative homogeneity criterion, is based\non the LIP-multiplicative law and is insensitive to changes due to variations\nof the object thickness or opacity. Each criterion is then applied in Revol and\nJourlin's (1997) region growing method which is based on the homogeneity of an\nimage region. The region growing method becomes therefore robust to the\nlighting changes specific to each criterion. Experiments on simulated and on\nreal images presenting lighting variations prove the robustness of the criteria\nto those variations. Compared to a state-of the art method based on the image\ncomponent-tree, ours is more robust. These results open the way to numerous\napplications where the lighting is uncontrolled or partially controlled.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:34:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"], ["Jourlin", "Michel", "", "IPRI"]]}, {"id": "1904.12599", "submitter": "Sascha Wirges", "authors": "Sascha Wirges, Johannes Gr\\\"ater, Qiuhao Zhang, Christoph Stiller", "title": "Self-Supervised Flow Estimation using Geometric Regularization with\n  Applications to Camera Image and Grid Map Sequences", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised approach to estimate flow in camera image and\ntop-view grid map sequences using fully convolutional neural networks in the\ndomain of automated driving. We extend existing approaches for self-supervised\noptical flow estimation by adding a regularizer expressing motion consistency\nassuming a static environment. However, as this assumption is violated for\nother moving traffic participants we also estimate a mask to scale this\nregularization. Adding a regularization towards motion consistency improves\nconvergence and flow estimation accuracy. Furthermore, we scale the errors due\nto spatial flow inconsistency by a mask that we derive from the motion mask.\nThis improves accuracy in regions where the flow drastically changes due to a\nbetter separation between static and dynamic environment. We apply our approach\nto optical flow estimation from camera image sequences, validate on odometry\nestimation and suggest a method to iteratively increase optical flow estimation\naccuracy using the generated motion masks. Finally, we provide quantitative and\nqualitative results based on the KITTI odometry and tracking benchmark for\nscene flow estimation based on grid map sequences. We show that we can improve\naccuracy and convergence when applying motion and spatial consistency\nregularization.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:22:03 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wirges", "Sascha", ""], ["Gr\u00e4ter", "Johannes", ""], ["Zhang", "Qiuhao", ""], ["Stiller", "Christoph", ""]]}, {"id": "1904.12602", "submitter": "Lichen Wang", "authors": "Lichen Wang, Bin Sun, Joseph Robinson, Taotao Jing, Yun Fu", "title": "EV-Action: Electromyography-Vision Multi-Modal Action Dataset", "comments": "IEEE International Conference on Automatic Face & Gesture Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal human action analysis is a critical and attractive research\ntopic. However, the majority of the existing datasets only provide visual\nmodalities (i.e., RGB, depth and skeleton). To make up this, we introduce a\nnew, large-scale EV-Action dataset in this work, which consists of RGB, depth,\nelectromyography (EMG), and two skeleton modalities. Compared with the\nconventional datasets, EV-Action dataset has two major improvements: (1) we\ndeploy a motion capturing system to obtain high quality skeleton modality,\nwhich provides more comprehensive motion information including skeleton,\ntrajectory, acceleration with higher accuracy, sampling frequency, and more\nskeleton markers. (2) we introduce an EMG modality which is usually used as an\neffective indicator in the biomechanics area, also it has yet to be well\nexplored in motion related research. To the best of our knowledge, this is the\nfirst action dataset with EMG modality. The details of EV-Action dataset are\nclarified, meanwhile, a simple yet effective framework for EMG-based action\nrecognition is proposed. Moreover, state-of-the-art baselines are applied to\nevaluate the effectiveness of all the modalities. The obtained result clearly\nshows the validity of EMG modality in human action analysis tasks. We hope this\ndataset can make significant contributions to human motion analysis, computer\nvision, machine learning, biomechanics, and other interdisciplinary fields.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 08:06:40 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 02:37:33 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Wang", "Lichen", ""], ["Sun", "Bin", ""], ["Robinson", "Joseph", ""], ["Jing", "Taotao", ""], ["Fu", "Yun", ""]]}, {"id": "1904.12603", "submitter": "Zohaib Khan", "authors": "Zohaib Khan, Faisal Shafait and Ajmal Mian", "title": "Converting a Common Document Scanner to a Multispectral Scanner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose the construction of a prototype scanner designed to capture\nmultispectral images of documents. A standard sheet-feed scanner is modified by\ndisconnecting its internal light source and connecting an external\nmultispectral light source comprising of narrow band light emitting diodes\n(LED). A document is scanned by illuminating the scanner light guide\nsuccessively with different LEDs and capturing a scan of the document. The\nsystem is portable and can be used for potential applications in verification\nof questioned documents, cheques, receipts and bank notes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 04:41:21 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Khan", "Zohaib", ""], ["Shafait", "Faisal", ""], ["Mian", "Ajmal", ""]]}, {"id": "1904.12613", "submitter": "Kyle Mott", "authors": "Kyle Mott", "title": "State Classification of Cooking Objects Using a VGG CNN", "comments": "5 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, it is very important for a robot to know the state of an\nobject and recognize particular desired states. This is an image classification\nproblem that can be solved using a convolutional neural network. In this paper,\nwe will discuss the use of a VGG convolutional neural network to recognize\nthose states of cooking objects. We will discuss the uses of activation\nfunctions, optimizers, data augmentation, layer additions, and other different\nversions of architectures. The results of this paper will be used to identify\nalternatives to the VGG convolutional neural network to improve accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 23:32:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mott", "Kyle", ""]]}, {"id": "1904.12615", "submitter": "Xinyu Li", "authors": "Xinyu Li, Wei Zhang, Tong Shen, Tao Mei", "title": "Everyone is a Cartoonist: Selfie Cartoonization with Attentive\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selfie and cartoon are two popular artistic forms that are widely presented\nin our daily life. Despite the great progress in image translation/stylization,\nfew techniques focus specifically on selfie cartoonization, since cartoon\nimages usually contain artistic abstraction (e.g., large smoothing areas) and\nexaggeration (e.g., large/delicate eyebrows). In this paper, we address this\nproblem by proposing a selfie cartoonization Generative Adversarial Network\n(scGAN), which mainly uses an attentive adversarial network (AAN) to emphasize\nspecific facial regions and ignore low-level details. More specifically, we\nfirst design a cycle-like architecture to enable training with unpaired data.\nThen we design three losses from different aspects. A total variation loss is\nused to highlight important edges and contents in cartoon portraits. An\nattentive cycle loss is added to lay more emphasis on delicate facial areas\nsuch as eyes. In addition, a perceptual loss is included to eliminate artifacts\nand improve robustness of our method. Experimental results show that our method\nis capable of generating different cartoon styles and outperforms a number of\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 11:23:40 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Xinyu", ""], ["Zhang", "Wei", ""], ["Shen", "Tong", ""], ["Mei", "Tao", ""]]}, {"id": "1904.12618", "submitter": "Ganesan Kaliyaperumal", "authors": "N.S.Manikandan, K.Ganesan", "title": "Deep Learning Based Automatic Video Annotation Tool for Self-Driving Car", "comments": "8 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a self-driving car, objection detection, object classification, lane\ndetection and object tracking are considered to be the crucial modules. In\nrecent times, using the real time video one wants to narrate the scene captured\nby the camera fitted in our vehicle. To effectively implement this task, deep\nlearning techniques and automatic video annotation tools are widely used. In\nthe present paper, we compare the various techniques that are available for\neach module and choose the best algorithm among them by using appropriate\nmetrics. For object detection, YOLO and Retinanet-50 are considered and the\nbest one is chosen based on mean Average Precision (mAP). For object\nclassification, we consider VGG-19 and Resnet-50 and select the best algorithm\nbased on low error rate and good accuracy. For lane detection, Udacity's\n'Finding Lane Line' and deep learning based LaneNet algorithms are compared and\nthe best one that can accurately identify the given lane is chosen for\nimplementation. As far as object tracking is concerned, we compare Udacity's\n'Object Detection and Tracking' algorithm and deep learning based Deep Sort\nalgorithm. Based on the accuracy of tracking the same object in many frames and\npredicting the movement of objects, the best algorithm is chosen. Our automatic\nvideo annotation tool is found to be 83% accurate when compared with a human\nannotator. We considered a video with 530 frames each of resolution 1035 x 1800\npixels. At an average each frame had about 15 objects. Our annotation tool\nconsumed 43 minutes in a CPU based system and 2.58 minutes in a mid-level GPU\nbased system to process all four modules. But the same video took nearly 3060\nminutes for one human annotator to narrate the scene in the given video. Thus\nwe claim that our proposed automatic video annotation tool is reasonably fast\n(about 1200 times in a GPU system) and accurate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 10:48:18 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Manikandan", "N. S.", ""], ["Ganesan", "K.", ""]]}, {"id": "1904.12619", "submitter": "Sun Siyang", "authors": "Siyang Sun, Yingjie Yin, Xingang Wang, De Xu, Yuan Zhao, Haifeng Shen", "title": "Multiple receptive fields and small-object-focusing weakly-supervised\n  segmentation network for fast object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection plays an important role in various visual applications.\nHowever, the precision and speed of detector are usually contradictory. One\nmain reason for fast detectors' precision reduction is that small objects are\nhard to be detected. To address this problem, we propose a multiple receptive\nfield and small-object-focusing weakly-supervised segmentation network\n(MRFSWSnet) to achieve fast object detection. In MRFSWSnet, multiple receptive\nfields block (MRF) is used to pay attention to the object and its adjacent\nbackground's different spatial location with different weights to enhance the\nfeature's discriminability. In addition, in order to improve the accuracy of\nsmall object detection, a small-object-focusing weakly-supervised segmentation\nmodule which only focuses on small object instead of all objects is integrated\ninto the detection network for auxiliary training to improve the precision of\nsmall object detection. Extensive experiments show the effectiveness of our\nmethod on both PASCAL VOC and MS COCO detection datasets. In particular, with a\nlower resolution version of 300x300, MRFSWSnet achieves 80.9% mAP on VOC2007\ntest with an inference speed of 15 milliseconds per frame, which is the\nstate-of-the-art detector among real-time detectors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 10:26:38 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 05:50:37 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Sun", "Siyang", ""], ["Yin", "Yingjie", ""], ["Wang", "Xingang", ""], ["Xu", "De", ""], ["Zhao", "Yuan", ""], ["Shen", "Haifeng", ""]]}, {"id": "1904.12620", "submitter": "Tao Li", "authors": "Tao Li and Lei Lin", "title": "AnonymousNet: Natural Face De-Identification with Measurable Privacy", "comments": "CVPR-19 Workshop on Computer Vision: Challenges and Opportunities for\n  Privacy and Security (CV-COPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With billions of personal images being generated from social media and\ncameras of all sorts on a daily basis, security and privacy are unprecedentedly\nchallenged. Although extensive attempts have been made, existing face image\nde-identification techniques are either insufficient in photo-reality or\nincapable of balancing privacy and usability qualitatively and quantitatively,\ni.e., they fail to answer counterfactual questions such as \"is it private\nnow?\", \"how private is it?\", and \"can it be more private?\" In this paper, we\npropose a novel framework called AnonymousNet, with an effort to address these\nissues systematically, balance usability, and enhance privacy in a natural and\nmeasurable manner. The framework encompasses four stages: facial attribute\nestimation, privacy-metric-oriented face obfuscation, directed natural image\nsynthesis, and adversarial perturbation. Not only do we achieve the\nstate-of-the-arts in terms of image quality and attribute prediction accuracy,\nwe are also the first to show that facial privacy is measurable, can be\nfactorized, and accordingly be manipulated in a photo-realistic fashion to\nfulfill different requirements and application scenarios. Experiments further\ndemonstrate the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:57:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Tao", ""], ["Lin", "Lei", ""]]}, {"id": "1904.12622", "submitter": "Cory Cornelius", "authors": "Cory Cornelius, Shang-Tse Chen, Jason Martin, Duen Horng Chau", "title": "Talk Proposal: Towards the Realistic Evaluation of Evasion Attacks using\n  CARLA", "comments": "Submitted as talk proposal to Dependable and Secure Machine Learning\n  (DSML '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this talk we describe our content-preserving attack on object detectors,\nShapeShifter, and demonstrate how to evaluate this threat in realistic\nscenarios. We describe how we use CARLA, a realistic urban driving simulator,\nto create these scenarios, and how we use ShapeShifter to generate\ncontent-preserving attacks against those scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:01:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Cornelius", "Cory", ""], ["Chen", "Shang-Tse", ""], ["Martin", "Jason", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1904.12625", "submitter": "Tauseef Ali", "authors": "Tauseef Ali and Ahmed B. Altamimi", "title": "Crowd Management in Open Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd analysis and management is a challenging problem to ensure public\nsafety and security. For this purpose, many techniques have been proposed to\ncope with various problems. However, the generalization capabilities of these\ntechniques is limited due to ignoring the fact that the density of crowd\nchanges from low to extreme high depending on the scene under observation. We\npropose robust feature based approach to deal with the problem of crowd\nmanagement for people safety and security. We have evaluated our method using a\nbenchmark dataset and have presented details analysis.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:16:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ali", "Tauseef", ""], ["Altamimi", "Ahmed B.", ""]]}, {"id": "1904.12627", "submitter": "Thomas Hollis", "authors": "Antoine Viscardi, Casey Juanxi Li, Thomas Hollis", "title": "Catch Me If You Can", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As advances in signature recognition have reached a new plateau of\nperformance at around 2% error rate, it is interesting to investigate\nalternative approaches. The approach detailed in this paper looks at using\nVariational Auto-Encoders (VAEs) to learn a latent space representation of\ngenuine signatures. This is then used to pass unlabelled signatures such that\nonly the genuine ones will successfully be reconstructed by the VAE. This\nlatent space representation and the reconstruction loss is subsequently used by\nrandom forest and kNN classifiers for prediction. Subsequently, VAE\ndisentanglement and the possibility of posterior collapse are ascertained and\nanalysed. The final results suggest that while this method performs less well\nthan existing alternatives, further work may allow this to be used as part of\nan ensemble for future models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 04:36:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Viscardi", "Antoine", ""], ["Li", "Casey Juanxi", ""], ["Hollis", "Thomas", ""]]}, {"id": "1904.12628", "submitter": "Onkar Krishna", "authors": "Onkar Krishna, Kiyoharu Aizawa, Go Irie", "title": "Computational Attention System for Children, Adults and Elderly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing computational visual attention systems have focused on the\nobjective to basically simulate and understand the concept of visual attention\nsystem in adults. Consequently, the impact of observer's age in scene viewing\nbehavior has rarely been considered. This study quantitatively analyzed the\nage-related differences in gaze landings during scene viewing for three\ndifferent class of images: naturals, man-made, and fractals. Observer's of\ndifferent age-group have shown different scene viewing tendencies independent\nto the class of the image viewed. Several interesting observations are drawn\nfrom the results. First, gaze landings for man-made dataset showed that whereas\nchild observers focus more on the scene foreground, i.e., locations that are\nnear, elderly observers tend to explore the scene background, i.e., locations\nfarther in the scene. Considering this result a framework is proposed in this\npaper to quantitatively measure the depth bias tendency across age groups.\nSecond, the quantitative analysis results showed that children exhibit the\nlowest exploratory behavior level but the highest central bias tendency among\nthe age groups and across the different scene categories. Third,\ninter-individual similarity metrics reveal that an adult had significantly\nlower gaze consistency with children and elderly compared to other adults for\nall the scene categories. Finally, these analysis results were consequently\nleveraged to develop a more accurate age-adapted saliency model independent to\nthe image type. The prediction accuracy suggests that our model fits better to\nthe collected eye-gaze data of the observers belonging to different age groups\nthan the existing models do.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 06:22:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Krishna", "Onkar", ""], ["Aizawa", "Kiyoharu", ""], ["Irie", "Go", ""]]}, {"id": "1904.12629", "submitter": "Tao Li", "authors": "Tao Li", "title": "Beauty Learning and Counterfactual Inference", "comments": "In CVPR-19 Workshop on Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work showcases a new approach for causal discovery by leveraging user\nexperiments and recent advances in photo-realistic image editing, demonstrating\na potential of identifying causal factors and understanding complex systems\ncounterfactually. We introduce the beauty learning problem as an example, which\nhas been discussed metaphysically for centuries and been proved exists, is\nquantifiable, and can be learned by deep models in our recent paper, where we\nutilize a natural image generator coupled with user studies to infer causal\neffects from facial semantics to beauty outcomes, the results of which also\nalign with existing empirical studies. We expect the proposed framework for a\nbroader application in causal inference.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 05:21:53 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Tao", ""]]}, {"id": "1904.12631", "submitter": "Mkhuseli Ngxande", "authors": "Mkhuseli Ngxande, Jule-Raymond Tapamo, Michael Burke", "title": "Detecting inter-sectional accuracy differences in driver drowsiness\n  detection algorithms", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been used successfully across a\nbroad range of areas including data mining, object detection, and in business.\nThe dominance of CNNs follows a breakthrough by Alex Krizhevsky which showed\nimprovements by dramatically reducing the error rate obtained in a general\nimage classification task from 26.2% to 15.4%. In road safety, CNNs have been\napplied widely to the detection of traffic signs, obstacle detection, and lane\ndeparture checking. In addition, CNNs have been used in data mining systems\nthat monitor driving patterns and recommend rest breaks when appropriate. This\npaper presents a driver drowsiness detection system and shows that there are\npotential social challenges regarding the application of these techniques, by\nhighlighting problems in detecting dark-skinned driver's faces. This is a\nparticularly important challenge in African contexts, where there are more\ndark-skinned drivers. Unfortunately, publicly available datasets are often\ncaptured in different cultural contexts, and therefore do not cover all\nethnicities, which can lead to false detections or racially biased models. This\nwork evaluates the performance obtained when training convolutional neural\nnetwork models on commonly used driver drowsiness detection datasets and\ntesting on datasets specifically chosen for broader representation. Results\nshow that models trained using publicly available datasets suffer extensively\nfrom over-fitting, and can exhibit racial bias, as shown by testing on a more\nrepresentative dataset. We propose a novel visualisation technique that can\nassist in identifying groups of people where there might be the potential of\ndiscrimination, using Principal Component Analysis (PCA) to produce a grid of\nfaces sorted by similarity, and combining these with a model accuracy overlay.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 11:43:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ngxande", "Mkhuseli", ""], ["Tapamo", "Jule-Raymond", ""], ["Burke", "Michael", ""]]}, {"id": "1904.12632", "submitter": "Pablo Barros", "authors": "Pablo Barros, German I. Parisi and Stefan Wermter", "title": "A Personalized Affective Memory Neural Model for Improving Emotion\n  Recognition", "comments": "Accepted by the International Conference on Machine Learning 2019\n  (ICML2019)", "journal-ref": "in PMLR 97:485-494 (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent models of emotion recognition strongly rely on supervised deep\nlearning solutions for the distinction of general emotion expressions. However,\nthey are not reliable when recognizing online and personalized facial\nexpressions, e.g., for person-specific affective understanding. In this paper,\nwe present a neural model based on a conditional adversarial autoencoder to\nlearn how to represent and edit general emotion expressions. We then propose\nGrow-When-Required networks as personalized affective memories to learn\nindividualized aspects of emotion expressions. Our model achieves\nstate-of-the-art performance on emotion recognition when evaluated on\n\\textit{in-the-wild} datasets. Furthermore, our experiments include ablation\nstudies and neural visualizations in order to explain the behavior of our\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 09:42:26 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 08:42:13 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Barros", "Pablo", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""]]}, {"id": "1904.12634", "submitter": "Jianwu Fang", "authors": "Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He Wang and Sen Li", "title": "DADA-2000: Can Driving Accident be Predicted by Driver Attention?\n  Analyzed by A Benchmark", "comments": "Submitted to ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver attention prediction is currently becoming the focus in safe driving\nresearch community, such as the DR(eye)VE project and newly emerged Berkeley\nDeepDrive Attention (BDD-A) database in critical situations. In safe driving,\nan essential task is to predict the incoming accidents as early as possible.\nBDD-A was aware of this problem and collected the driver attention in\nlaboratory because of the rarity of such scenes. Nevertheless, BDD-A focuses\nthe critical situations which do not encounter actual accidents, and just faces\nthe driver attention prediction task, without a close step for accident\nprediction. In contrast to this, we explore the view of drivers' eyes for\ncapturing multiple kinds of accidents, and construct a more diverse and larger\nvideo benchmark than ever before with the driver attention and the driving\naccident annotation simultaneously (named as DADA-2000), which has 2000 video\nclips owning about 658,476 frames on 54 kinds of accidents. These clips are\ncrowd-sourced and captured in various occasions (highway, urban, rural, and\ntunnel), weather (sunny, rainy and snowy) and light conditions (daytime and\nnighttime). For the driver attention representation, we collect the maps of\nfixations, saccade scan path and focusing time. The accidents are annotated by\ntheir categories, the accident window in clips and spatial locations of the\ncrash-objects. Based on the analysis, we obtain a quantitative and positive\nanswer for the question in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:33:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Fang", "Jianwu", ""], ["Yan", "Dingxin", ""], ["Qiao", "Jiahuan", ""], ["Xue", "Jianru", ""], ["Wang", "He", ""], ["Li", "Sen", ""]]}, {"id": "1904.12638", "submitter": "Eloi Zablocki", "authors": "Eloi Zablocki, Patrick Bordes, Benjamin Piwowarski, Laure Soulier,\n  Patrick Gallinari", "title": "Context-Aware Zero-Shot Learning for Object Recognition", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging\nauxiliary knowledge, such as semantic representations. A limitation of previous\napproaches is that only intrinsic properties of objects, e.g. their visual\nappearance, are taken into account while their context, e.g. the surrounding\nobjects in the image, is ignored. Following the intuitive principle that\nobjects tend to be found in certain contexts but not others, we propose a new\nand challenging approach, context-aware ZSL, that leverages semantic\nrepresentations in a new way to model the conditional likelihood of an object\nto appear in a given context. Finally, through extensive experiments conducted\non Visual Genome, we show that contextual information can substantially improve\nthe standard ZSL approach and is robust to unbalanced classes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:50:05 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 11:39:52 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zablocki", "Eloi", ""], ["Bordes", "Patrick", ""], ["Piwowarski", "Benjamin", ""], ["Soulier", "Laure", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1904.12639", "submitter": "Yang Hu Dr.", "authors": "Yang Hu, Guihua Wen, Mingnan Luo, Dan Dai, Wenming Cao, Zhiwen Yu,\n  Wendy Hall", "title": "Inner-Imaging Networks: Put Lenses into Convolutional Structure", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the tremendous success in computer vision, deep convolutional\nnetworks suffer from serious computation costs and redundancies. Although\nprevious works address this issue by enhancing diversities of filters, they\nhave not considered the complementarity and the completeness of the internal\nstructure of the convolutional network. To deal with these problems, a novel\nInner-Imaging architecture is proposed in this paper, which allows\nrelationships between channels to meet the above requirement. Specifically, we\norganize the channel signal points in groups using convolutional kernels to\nmodel both the intra-group and inter-group relationships simultaneously. The\nconvolutional filter is a powerful tool for modeling spatial relations and\norganizing grouped signals, so the proposed methods map the channel signals\nonto a pseudo-image, like putting a lens into convolution internal structure.\nConsequently, not only the diversity of channels is increased, but also the\ncomplementarity and completeness can be explicitly enhanced. The proposed\narchitecture is lightweight and easy to be implemented. It provides an\nefficient self-organization strategy for convolutional networks so as to\nimprove their efficiency and performance. Extensive experiments are conducted\non multiple benchmark image recognition data sets including CIFAR, SVHN and\nImageNet. Experimental results verify the effectiveness of the Inner-Imaging\nmechanism with the most popular convolutional networks as the backbones.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:44:10 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 16:50:47 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hu", "Yang", ""], ["Wen", "Guihua", ""], ["Luo", "Mingnan", ""], ["Dai", "Dan", ""], ["Cao", "Wenming", ""], ["Yu", "Zhiwen", ""], ["Hall", "Wendy", ""]]}, {"id": "1904.12640", "submitter": "Wu Weijia", "authors": "Weijia Wu, Jici Xing, Hong Zhou", "title": "TextCohesion: Detecting Text for Arbitrary Shapes", "comments": "Scene Text Detection Instance Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pixel-wise method named TextCohesion for scene\ntext detection, which splits a text instance into five key components: a Text\nSkeleton and four Directional Pixel Regions. These components are easier to\nhandle than the entire text instance. A confidence scoring mechanism is\ndesigned to filter characters that are similar to text. Our method can\nintegrate text contexts intensively when backgrounds are complex. Experiments\non two curved challenging benchmarks demonstrate that TextCohesion outperforms\nstate-of-the-art methods, achieving the F-measure of 84.6% on Total-Text and\nbfseries86.3% on SCUT-CTW1500.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:21:38 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 07:37:56 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Wu", "Weijia", ""], ["Xing", "Jici", ""], ["Zhou", "Hong", ""]]}, {"id": "1904.12641", "submitter": "Qi Wang", "authors": "Yuan Yuan, Yuwei Lu, Qi Wang", "title": "Tracking as A Whole: Multi-Target Tracking by Modeling Group Behavior\n  with Sequential Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2017.2686871", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based vehicle detection and tracking is one of the most important\ncomponents for Intelligent Transportation Systems (ITS). When it comes to road\njunctions, the problem becomes even more difficult due to the occlusions and\ncomplex interactions among vehicles. In order to get a precise detection and\ntracking result, in this work we propose a novel tracking-by-detection\nframework. In the detection stage, we present a sequential detection model to\ndeal with serious occlusions. In the tracking stage, we model group behavior to\ntreat complex interactions with overlaps and ambiguities. The main\ncontributions of this paper are twofold: 1) Shape prior is exploited in the\nsequential detection model to tackle occlusions in crowded scene. 2) Traffic\nforce is defined in the traffic scene to model group behavior, and it can\nassist to handle complex interactions among vehicles. We evaluate the proposed\napproach on real surveillance videos at road junctions and the performance has\ndemonstrated the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:39:00 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yuan", "Yuan", ""], ["Lu", "Yuwei", ""], ["Wang", "Qi", ""]]}, {"id": "1904.12642", "submitter": "Qi Wang", "authors": "Yuwei Lu, Yuan Yuan, Qi Wang", "title": "Forward Vehicle Collision Warning Based on Quick Camera Calibration", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2018.8461620", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward Vehicle Collision Warning (FCW) is one of the most important\nfunctions for autonomous vehicles. In this procedure, vehicle detection and\ndistance measurement are core components, requiring accurate localization and\nestimation. In this paper, we propose a simple but efficient forward vehicle\ncollision warning framework by aggregating monocular distance measurement and\nprecise vehicle detection. In order to obtain forward vehicle distance, a quick\ncamera calibration method which only needs three physical points to calibrate\nrelated camera parameters is utilized. As for the forward vehicle detection, a\nmulti-scale detection algorithm that regards the result of calibration as\ndistance priori is proposed to improve the precision. Intensive experiments are\nconducted in our established real scene dataset and the results have\ndemonstrated the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:39:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Lu", "Yuwei", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "1904.12654", "submitter": "Alberto Bailoni", "authors": "Steffen Wolf, Alberto Bailoni, Constantin Pape, Nasim Rahaman, Anna\n  Kreshuk, Ullrich K\\\"othe, Fred A. Hamprecht", "title": "The Mutex Watershed and its Objective: Efficient, Parameter-Free Graph\n  Partitioning", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2020) 1-1", "doi": "10.1109/TPAMI.2020.2980827", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image partitioning, or segmentation without semantics, is the task of\ndecomposing an image into distinct segments, or equivalently to detect closed\ncontours. Most prior work either requires seeds, one per segment; or a\nthreshold; or formulates the task as multicut / correlation clustering, an\nNP-hard problem. Here, we propose an efficient algorithm for graph\npartitioning, the \"Mutex Watershed''. Unlike seeded watershed, the algorithm\ncan accommodate not only attractive but also repulsive cues, allowing it to\nfind a previously unspecified number of segments without the need for explicit\nseeds or a tunable threshold. We also prove that this simple algorithm solves\nto global optimality an objective function that is intimately related to the\nmulticut / correlation clustering integer linear programming formulation. The\nalgorithm is deterministic, very simple to implement, and has empirically\nlinearithmic complexity. When presented with short-range attractive and\nlong-range repulsive cues from a deep neural network, the Mutex Watershed gives\nthe best results currently known for the competitive ISBI 2012 EM segmentation\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:29:45 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 13:06:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wolf", "Steffen", ""], ["Bailoni", "Alberto", ""], ["Pape", "Constantin", ""], ["Rahaman", "Nasim", ""], ["Kreshuk", "Anna", ""], ["K\u00f6the", "Ullrich", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1904.12658", "submitter": "Zhidong Zhu", "authors": "Zhibo Rao and Mingyi He and Yuchao Dai and Zhidong Zhu and Bo Li and\n  Renjie He", "title": "MSDC-Net: Multi-Scale Dense and Contextual Networks for Automated\n  Disparity Map for Stereo Matching", "comments": "Accepted at ICIGP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparity prediction from stereo images is essential to computer vision\napplications including autonomous driving, 3D model reconstruction, and object\ndetection. To predict accurate disparity map, we propose a novel deep learning\narchitecture for detectingthe disparity map from a rectified pair of stereo\nimages, called MSDC-Net. Our MSDC-Net contains two modules: multi-scale fusion\n2D convolution and multi-scale residual 3D convolution modules. The multi-scale\nfusion 2D convolution module exploits the potential multi-scale features, which\nextracts and fuses the different scale features by Dense-Net. The multi-scale\nresidual 3D convolution module learns the different scale geometry context from\nthe cost volume which aggregated by the multi-scale fusion 2D convolution\nmodule. Experimental results on Scene Flow and KITTI datasets demonstrate that\nour MSDC-Net significantly outperforms other approaches in the non-occluded\nregion.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 10:12:56 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 02:50:46 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Rao", "Zhibo", ""], ["He", "Mingyi", ""], ["Dai", "Yuchao", ""], ["Zhu", "Zhidong", ""], ["Li", "Bo", ""], ["He", "Renjie", ""]]}, {"id": "1904.12659", "submitter": "Maosen Li", "authors": "Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Actional-Structural Graph Convolutional Networks for Skeleton-based\n  Action Recognition", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition with skeleton data has recently attracted much attention\nin computer vision. Previous studies are mostly based on fixed skeleton graphs,\nonly capturing local physical dependencies among joints, which may miss\nimplicit joint correlations. To capture richer dependencies, we introduce an\nencoder-decoder structure, called A-link inference module, to capture\naction-specific latent dependencies, i.e. actional links, directly from\nactions. We also extend the existing skeleton graphs to represent higher-order\ndependencies, i.e. structural links. Combing the two types of links into a\ngeneralized skeleton graph, we further propose the actional-structural graph\nconvolution network (AS-GCN), which stacks actional-structural graph\nconvolution and temporal convolution as a basic building block, to learn both\nspatial and temporal features for action recognition. A future pose prediction\nhead is added in parallel to the recognition head to help capture more detailed\naction patterns through self-supervision. We validate AS-GCN in action\nrecognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed\nAS-GCN achieves consistently large improvement compared to the state-of-the-art\nmethods. As a side product, AS-GCN also shows promising results for future pose\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:20:07 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Maosen", ""], ["Chen", "Siheng", ""], ["Chen", "Xu", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "1904.12665", "submitter": "Bharath Ramesh", "authors": "Bharath Ramesh, Andres Ussa, Luca Della Vedova, Hong Yang and Garrick\n  Orchard", "title": "PCA-RECT: An Energy-efficient Object Detection Approach for Event\n  Cameras", "comments": "Accepted in ACCV 2018 Workshops, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first purely event-based, energy-efficient approach for object\ndetection and categorization using an event camera. Compared to traditional\nframe-based cameras, choosing event cameras results in high temporal resolution\n(order of microseconds), low power consumption (few hundred mW) and wide\ndynamic range (120 dB) as attractive properties. However, event-based object\nrecognition systems are far behind their frame-based counterparts in terms of\naccuracy. To this end, this paper presents an event-based feature extraction\nmethod devised by accumulating local activity across the image frame and then\napplying principal component analysis (PCA) to the normalized neighborhood\nregion. Subsequently, we propose a backtracking-free k-d tree mechanism for\nefficient feature matching by taking advantage of the low-dimensionality of the\nfeature representation. Additionally, the proposed k-d tree mechanism allows\nfor feature selection to obtain a lower-dimensional dictionary representation\nwhen hardware resources are limited to implement dimensionality reduction.\nConsequently, the proposed system can be realized on a field-programmable gate\narray (FPGA) device leading to high performance over resource ratio. The\nproposed system is tested on real-world event-based datasets for object\ncategorization, showing superior classification performance and relevance to\nstate-of-the-art algorithms. Additionally, we verified the object detection\nmethod and real-time FPGA performance in lab settings under non-controlled\nillumination conditions with limited training data and ground truth\nannotations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:11:22 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ramesh", "Bharath", ""], ["Ussa", "Andres", ""], ["Della Vedova", "Luca", ""], ["Yang", "Hong", ""], ["Orchard", "Garrick", ""]]}, {"id": "1904.12667", "submitter": "Oliver Wasenm\\\"uller", "authors": "Queens Maria Thomas, Oliver Wasenm\\\"uller, Didier Stricker", "title": "DeLiO: Decoupled LiDAR Odometry", "comments": "Accepted at IEEE Intelligent Vehicles Symposium (IV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most LiDAR odometry algorithms estimate the transformation between two\nconsecutive frames by estimating the rotation and translation in an intervening\nfashion. In this paper, we propose our Decoupled LiDAR Odometry (DeLiO), which\n-- for the first time -- decouples the rotation estimation completely from the\ntranslation estimation. In particular, the rotation is estimated by extracting\nthe surface normals from the input point clouds and tracking their\ncharacteristic pattern on a unit sphere. Using this rotation the point clouds\nare unrotated so that the underlying transformation is pure translation, which\ncan be easily estimated using a line cloud approach. An evaluation is performed\non the KITTI dataset and the results are compared against state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 12:54:22 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Thomas", "Queens Maria", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.12681", "submitter": "Lijie Liu", "authors": "Lijie Liu, Jiwen Lu, Chunjing Xu, Qi Tian, Jie Zhou", "title": "Deep Fitting Degree Scoring Network for Monocular 3D Object Detection", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn a deep fitting degree scoring network for\nmonocular 3D object detection, which aims to score fitting degree between\nproposals and object conclusively. Different from most existing monocular\nframeworks which use tight constraint to get 3D location, our approach achieves\nhigh-precision localization through measuring the visual fitting degree between\nthe projected 3D proposals and the object. We first regress the dimension and\norientation of the object using an anchor-based method so that a suitable 3D\nproposal can be constructed. We propose FQNet, which can infer the 3D IoU\nbetween the 3D proposals and the object solely based on 2D cues. Therefore,\nduring the detection process, we sample a large number of candidates in the 3D\nspace and project these 3D bounding boxes on 2D image individually. The best\ncandidate can be picked out by simply exploring the spatial overlap between\nproposals and the object, in the form of the output 3D IoU score of FQNet.\nExperiments on the KITTI dataset demonstrate the effectiveness of our\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:40:22 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 09:49:06 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Liu", "Lijie", ""], ["Lu", "Jiwen", ""], ["Xu", "Chunjing", ""], ["Tian", "Qi", ""], ["Zhou", "Jie", ""]]}, {"id": "1904.12690", "submitter": "Ruairidh Battleday", "authors": "Ruairidh M. Battleday, Joshua C. Peterson, and Thomas L. Griffiths", "title": "Capturing human categorization of natural images at scale by combining\n  deep networks and cognitive models", "comments": "29 pages; 4 figures. arXiv admin note: text overlap with\n  arXiv:1711.04855", "journal-ref": null, "doi": "10.1038/s41467-020-18946-z", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human categorization is one of the most important and successful targets of\ncognitive modeling in psychology, yet decades of development and assessment of\ncompeting models have been contingent on small sets of simple, artificial\nexperimental stimuli. Here we extend this modeling paradigm to the domain of\nnatural images, revealing the crucial role that stimulus representation plays\nin categorization and its implications for conclusions about how people form\ncategories. Applying psychological models of categorization to natural images\nrequired two significant advances. First, we conducted the first large-scale\nexperimental study of human categorization, involving over 500,000 human\ncategorization judgments of 10,000 natural images from ten non-overlapping\nobject categories. Second, we addressed the traditional bottleneck of\nrepresenting high-dimensional images in cognitive models by exploring the best\nof current supervised and unsupervised deep and shallow machine learning\nmethods. We find that selecting sufficiently expressive, data-driven\nrepresentations is crucial to capturing human categorization, and using these\nrepresentations allows simple models that represent categories with abstract\nprototypes to outperform the more complex memory-based exemplar accounts of\ncategorization that have dominated in studies using less naturalistic stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:47:59 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Battleday", "Ruairidh M.", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1904.12717", "submitter": "Yinlong Liu", "authors": "Yinlong Liu, Alois Knoll, Guang Chen", "title": "Globally optimal vertical direction estimation in Atlanta World", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2020.3027047", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In man-made environments, such as indoor and urban scenes, most of the\nobjects and structures are organized in the form of orthogonal and parallel\nplanes. These planes can be approximated by the Atlanta world assumption, in\nwhich the normals of planes can be represented by the Atlanta frames. Atlanta\nworld assumption, which can be considered as a generalized Manhattan world\nassumption, has one vertical frame and multiple horizontal frames.\nConventionally, given a set of inputs such as surface normals, the Atlanta\nframe estimation problem can be solved in one-time by branch-and-bound (BnB).\nHowever, the runtime of the BnB algorithm will increase greatly when the\ndimensionality (i.e., the number of horizontal frames) increases. In this\npaper, we estimate only the vertical direction instead of all Atlanta frames at\nonce. Accordingly, we propose a vertical direction estimation method by\nconsidering the relationship between the vertical frame and horizontal frames.\nConcretely, our approach employs a BnB algorithm to search the vertical\ndirection guaranteeing global optimality without requiring prior knowledge of\nthe number of Atlanta frames. Four novel bounds by mapping 3D-hemisphere to a\n2D region are investigated to guarantee convergence. We verify the validity of\nthe proposed method in various challenging synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 13:56:36 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 11:52:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Yinlong", ""], ["Knoll", "Alois", ""], ["Chen", "Guang", ""]]}, {"id": "1904.12724", "submitter": "F\\'abio Vin\\'icius Moreira Perez", "authors": "F\\'abio Perez and Sandra Avila and Eduardo Valle", "title": "Solo or Ensemble? Choosing a CNN Architecture for Melanoma\n  Classification", "comments": "ISIC Skin Image Analysis Workshop @ CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) deliver exceptional results for computer\nvision, including medical image analysis. With the growing number of available\narchitectures, picking one over another is far from obvious. Existing art\nsuggests that, when performing transfer learning, the performance of CNN\narchitectures on ImageNet correlates strongly with their performance on target\ntasks. We evaluate that claim for melanoma classification, over 9 CNNs\narchitectures, in 5 sets of splits created on the ISIC Challenge 2017 dataset,\nand 3 repeated measures, resulting in 135 models. The correlations we found\nwere, to begin with, much smaller than those reported by existing art, and\ndisappeared altogether when we considered only the top-performing networks:\nuncontrolled nuisances (i.e., splits and randomness) overcome any of the\nanalyzed factors. Whenever possible, the best approach for melanoma\nclassification is still to create ensembles of multiple models. We compared two\nchoices for selecting which models to ensemble: picking them at random (among a\npool of high-quality ones) vs. using the validation set to determine which ones\nto pick first. For small ensembles, we found a slight advantage on the second\napproach but found that random choice was also competitive. Although our aim in\nthis paper was not to maximize performance, we easily reached AUCs comparable\nto the first place on the ISIC Challenge 2017.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:06:19 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Perez", "F\u00e1bio", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1904.12732", "submitter": "Mhd Hasan Sarhan", "authors": "Mhd Hasan Sarhan, Shadi Albarqouni, Mehmet Yigitsoy, Nassir Navab,\n  Abouzar Eslami", "title": "Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques are recently being used in fundus image analysis and\ndiabetic retinopathy detection. Microaneurysms are an important indicator of\ndiabetic retinopathy progression. We introduce a two-stage deep learning\napproach for microaneurysms segmentation using multiple scales of the input\nwith selective sampling and embedding triplet loss. The model first segments on\ntwo scales and then the segmentations are refined with a classification model.\nTo enhance the discriminative power of the classification model, we incorporate\ntriplet embedding loss with a selective sampling routine. The model is\nevaluated quantitatively to assess the segmentation performance and\nqualitatively to analyze the model predictions. This approach introduces a\n30.29% relative improvement over the fully convolutional neural network.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 12:20:53 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 09:38:52 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Sarhan", "Mhd Hasan", ""], ["Albarqouni", "Shadi", ""], ["Yigitsoy", "Mehmet", ""], ["Navab", "Nassir", ""], ["Eslami", "Abouzar", ""]]}, {"id": "1904.12733", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI), R Thomas, S Iles (DESW), G Bhakta\n  (DESW), A Crowder (DESW), D. Owens, P. Boyle (IPRI, SIGPH@iPRI)", "title": "Registration of retinal images from Public Health by minimising an error\n  between vessels using an affine model with radial distortions", "comments": null, "journal-ref": "IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019), IEEE, Apr 2019, Venice, Italy. pp.561-564", "doi": "10.1109/ISBI.2019.8759415", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to estimate a registration model of eye fundus images made of an\naffinity and two radial distortions, we introduce an estimation criterion based\non an error between the vessels. In [1], we estimated this model by minimising\nthe error between characteristics points. In this paper, the detected vessels\nare selected using the circle and ellipse equations of the overlap area\nboundaries deduced from our model. Our method successfully registers 96 % of\nthe 271 pairs in a Public Health dataset acquired mostly with different\ncameras. This is better than our previous method [1] and better than three\nother state-of-the-art methods. On a publicly available dataset, ours still\nbetter register the images than the reference method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:30:38 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"], ["Thomas", "R", "", "DESW"], ["Iles", "S", "", "DESW"], ["Bhakta", "G", "", "DESW"], ["Crowder", "A", "", "DESW"], ["Owens", "D.", "", "IPRI, SIGPH@iPRI"], ["Boyle", "P.", "", "IPRI, SIGPH@iPRI"]]}, {"id": "1904.12735", "submitter": "Mingliang Fu", "authors": "Mingliang Fu and Weijia Zhou", "title": "DeepHMap++: Combined Projection Grouping and Correspondence Learning for\n  Full DoF Pose Estimation", "comments": "21 pages, 10 figures", "journal-ref": "Sensors 19.5 (2019): 1032", "doi": "10.3390/s19051032", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, estimating the 6D pose of object instances with\nconvolutional neural network (CNN) has received considerable attention.\nDepending on whether intermediate cues are used, the relevant literature can be\nroughly divided into two broad categories: direct methods and two stage\npipelines. For the latter, intermediate cues, such as 3D object coordinates,\nsemantic keypoints, or virtual control points instead of pose parameters are\nregressed by CNN in the first stage. Object pose can then be solved by\ncorrespondence constraints constructed with these intermediate cues. In this\npaper, we focus on the postprocessing of a two-stage pipeline and propose to\ncombine two learning concepts for estimating object pose under challenging\nscenes: projection grouping on one side, and correspondence learning on the\nother. We firstly employ a local patch based method to predict projection\nheatmaps which denote the confidence distribution of projection of 3D bounding\nbox's corners. A projection grouping module is then proposed to remove\nredundant local maxima from each layer of heatmaps. Instead of directly feeding\n2D-3D correspondences to the perspective-n-point (PnP) algorithm, multiple\ncorrespondence hypotheses are sampled from local maxima and its corresponding\nneighborhood and ranked by a correspondence-evaluation network. Finally,\ncorrespondences with higher confidence are selected to determine object pose.\nExtensive experiments on three public datasets demonstrate that the proposed\nframework outperforms several state of the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:22:07 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Fu", "Mingliang", ""], ["Zhou", "Weijia", ""]]}, {"id": "1904.12738", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Danilo Vasconcellos Vargas and Venkanna U", "title": "Self Training Autonomous Driving Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsically, driving is a Markov Decision Process which suits well the\nreinforcement learning paradigm. In this paper, we propose a novel agent which\nlearns to drive a vehicle without any human assistance. We use the concept of\nreinforcement learning and evolutionary strategies to train our agent in a 2D\nsimulation environment. Our model's architecture goes beyond the World Model's\nby introducing difference images in the auto encoder. This novel involvement of\ndifference images in the auto-encoder gives better representation of the latent\nspace with respect to the motion of vehicle and helps an autonomous agent to\nlearn more efficiently how to drive a vehicle. Results show that our method\nrequires fewer (96% less) total agents, (87.5% less) agents per generations,\n(70% less) generations and (90% less) rollouts than the original architecture\nwhile achieving the same accuracy of the original.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 05:22:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kotyan", "Shashank", ""], ["Vargas", "Danilo Vasconcellos", ""], ["U", "Venkanna", ""]]}, {"id": "1904.12739", "submitter": "David Dov", "authors": "David Dov, Shahar Ziv Kovalsky, Serge Assaad, Avani A. Pendse Jonathan\n  Cohen, Danielle Elliott Range, Ricardo Henao, Lawrence Carin", "title": "Weakly Supervised Instance Learning for Thyroid Malignancy Prediction\n  from Whole Slide Cytopathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine-learning-based thyroid-malignancy prediction from\ncytopathology whole-slide images (WSI). Multiple instance learning (MIL)\napproaches, typically used for the analysis of WSIs, divide the image (bag)\ninto patches (instances), which are used to predict a single bag-level label.\nThese approaches perform poorly in cytopathology slides due to a unique bag\nstructure: sparsely located informative instances with varying characteristics\nof abnormality. We address these challenges by considering multiple types of\nlabels: bag-level malignancy and ordered diagnostic scores, as well as\ninstance-level informativeness and abnormality labels. We study their\ncontribution beyond the MIL setting by proposing a maximum likelihood\nestimation (MLE) framework, from which we derive a two-stage\ndeep-learning-based algorithm. The algorithm identifies informative instances\nand assigns them local malignancy scores that are incorporated into a global\nmalignancy prediction. We derive a lower bound of the MLE, leading to an\nimproved training strategy based on weak supervision, that we motivate through\nstatistical analysis. The lower bound further allows us to extend the proposed\nalgorithm to simultaneously predict multiple bag and instance-level labels from\na single output of a neural network. Experimental results demonstrate that the\nproposed algorithm provides competitive performance compared to several\ncompeting methods, achieves (expert) human-level performance, and allows\naugmentation of human decisions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 03:03:20 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 20:31:50 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Dov", "David", ""], ["Kovalsky", "Shahar Ziv", ""], ["Assaad", "Serge", ""], ["Cohen", "Avani A. Pendse Jonathan", ""], ["Range", "Danielle Elliott", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1904.12743", "submitter": "Giorgio Luigi Morales Luna", "authors": "Giorgio Morales, Alejandro Ram\\'irez, Joel Telles", "title": "End-to-end Cloud Segmentation in High-Resolution Multispectral Satellite\n  Imagery Using Deep Learning", "comments": "Submitted to INTERCON2019 conference. Lima, Peru", "journal-ref": "2019 IEEE XXVI International Conference on Electronics, Electrical\n  Engineering and Computing (INTERCON)", "doi": "10.1109/INTERCON.2019.8853549", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmenting clouds in high-resolution satellite images is an arduous and\nchallenging task due to the many types of geographies and clouds a satellite\ncan capture. Therefore, it needs to be automated and optimized, specially for\nthose who regularly process great amounts of satellite images, such as\ngovernmental institutions. In that sense, the contribution of this work is\ntwofold: We present the CloudPeru2 dataset, consisting of 22,400 images of\n512x512 pixels and their respective hand-drawn cloud masks, as well as the\nproposal of an end-to-end segmentation method for clouds using a Convolutional\nNeural Network (CNN) based on the Deeplab v3+ architecture. The results over\nthe test set achieved an accuracy of 96.62%, precision of 96.46%, specificity\nof 98.53%, and sensitivity of 96.72% which is superior to the compared methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:28:32 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Morales", "Giorgio", ""], ["Ram\u00edrez", "Alejandro", ""], ["Telles", "Joel", ""]]}, {"id": "1904.12760", "submitter": "Xin Chen", "authors": "Xin Chen, Lingxi Xie, Jun Wu and Qi Tian", "title": "Progressive Differentiable Architecture Search: Bridging the Depth Gap\n  between Search and Evaluation", "comments": "10 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, differentiable search methods have made major progress in reducing\nthe computational costs of neural architecture search. However, these\napproaches often report lower accuracy in evaluating the searched architecture\nor transferring it to another dataset. This is arguably due to the large gap\nbetween the architecture depths in search and evaluation scenarios. In this\npaper, we present an efficient algorithm which allows the depth of searched\narchitectures to grow gradually during the training procedure. This brings two\nissues, namely, heavier computational overheads and weaker search stability,\nwhich we solve using search space approximation and regularization,\nrespectively. With a significantly reduced search time (~7 hours on a single\nGPU), our approach achieves state-of-the-art performance on both the proxy\ndataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is\navailable at https://github.com/chenxin061/pdarts.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:59:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Xin", ""], ["Xie", "Lingxi", ""], ["Wu", "Jun", ""], ["Tian", "Qi", ""]]}, {"id": "1904.12785", "submitter": "Nicholas Kolkin", "authors": "Nicholas Kolkin, Jason Salavon, Greg Shakhnarovich", "title": "Style Transfer by Relaxed Optimal Transport and Self-Similarity", "comments": "To Appear CVPR 2019, Webdemo Available at http://style.ttic.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Style transfer algorithms strive to render the content of one image using the\nstyle of another. We propose Style Transfer by Relaxed Optimal Transport and\nSelf-Similarity (STROTSS), a new optimization-based style transfer algorithm.\nWe extend our method to allow user-specified point-to-point or region-to-region\ncontrol over visual similarity between the style image and the output. Such\nguidance can be used to either achieve a particular visual effect or correct\nerrors made by unconstrained style transfer. In order to quantitatively compare\nour method to prior work, we conduct a large-scale user study designed to\nassess the style-content tradeoff across settings in style transfer algorithms.\nOur results indicate that for any desired level of content preservation, our\nmethod provides higher quality stylization than prior work. Code is available\nat https://github.com/nkolkin13/STROTSS\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:55:59 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 19:00:22 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Kolkin", "Nicholas", ""], ["Salavon", "Jason", ""], ["Shakhnarovich", "Greg", ""]]}, {"id": "1904.12795", "submitter": "Anna Fr\\\"uhst\\\"uck", "authors": "Anna Fr\\\"uhst\\\"uck and Ibraheem Alhashim and Peter Wonka", "title": "TileGAN: Synthesis of Large-Scale Non-Homogeneous Textures", "comments": "Code is available at http://github.com/afruehstueck/tileGAN", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)", "doi": "10.1145/3306346.3322993", "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the problem of texture synthesis in the setting where many input\nimages are given and a large-scale output is required. We build on recent\ngenerative adversarial networks and propose two extensions in this paper.\nFirst, we propose an algorithm to combine outputs of GANs trained on a smaller\nresolution to produce a large-scale plausible texture map with virtually no\nboundary artifacts. Second, we propose a user interface to enable artistic\ncontrol. Our quantitative and qualitative results showcase the generation of\nsynthesized high-resolution maps consisting of up to hundreds of megapixels as\na case in point.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:15:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Fr\u00fchst\u00fcck", "Anna", ""], ["Alhashim", "Ibraheem", ""], ["Wonka", "Peter", ""]]}, {"id": "1904.12843", "submitter": "Mahyar Najibi", "authors": "Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson,\n  Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein", "title": "Adversarial Training for Free!", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training, in which a network is trained on adversarial examples,\nis one of the few defenses against adversarial attacks that withstands strong\nattacks. Unfortunately, the high cost of generating strong adversarial examples\nmakes standard adversarial training impractical on large-scale problems like\nImageNet. We present an algorithm that eliminates the overhead cost of\ngenerating adversarial examples by recycling the gradient information computed\nwhen updating model parameters. Our \"free\" adversarial training algorithm\nachieves comparable robustness to PGD adversarial training on the CIFAR-10 and\nCIFAR-100 datasets at negligible additional cost compared to natural training,\nand can be 7 to 30 times faster than other strong adversarial training methods.\nUsing a single workstation with 4 P100 GPUs and 2 days of runtime, we can train\na robust model for the large-scale ImageNet classification task that maintains\n40% accuracy against PGD attacks. The code is available at\nhttps://github.com/ashafahi/free_adv_train.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:50:32 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 21:26:19 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Shafahi", "Ali", ""], ["Najibi", "Mahyar", ""], ["Ghiasi", "Amin", ""], ["Xu", "Zheng", ""], ["Dickerson", "John", ""], ["Studer", "Christoph", ""], ["Davis", "Larry S.", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "1904.12848", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le", "title": "Unsupervised Data Augmentation for Consistency Training", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning lately has shown much promise in improving deep\nlearning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to\nconstrain model predictions to be invariant to input noise. In this work, we\npresent a new perspective on how to effectively noise unlabeled examples and\nargue that the quality of noising, specifically those produced by advanced data\naugmentation methods, plays a crucial role in semi-supervised learning. By\nsubstituting simple noising operations with advanced data augmentation methods\nsuch as RandAugment and back-translation, our method brings substantial\nimprovements across six language and three vision tasks under the same\nconsistency training framework. On the IMDb text classification dataset, with\nonly 20 labeled examples, our method achieves an error rate of 4.20,\noutperforming the state-of-the-art model trained on 25,000 labeled examples. On\na standard semi-supervised learning benchmark, CIFAR-10, our method outperforms\nall previous approaches and achieves an error rate of 5.43 with only 250\nexamples. Our method also combines well with transfer learning, e.g., when\nfinetuning from BERT, and yields improvements in high-data regime, such as\nImageNet, whether when there is only 10% labeled data or when a full labeled\nset with 1.3M extra unlabeled examples is used. Code is available at\nhttps://github.com/google-research/uda.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:56:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 17:53:48 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 15:32:11 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 15:40:40 GMT"}, {"version": "v5", "created": "Thu, 25 Jun 2020 17:58:43 GMT"}, {"version": "v6", "created": "Thu, 5 Nov 2020 15:11:02 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""], ["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.12856", "submitter": "Utkarsh Porwal", "authors": "Utkarsh Porwal", "title": "Learning Image Information for eCommerce Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing similarity between a query and a document is fundamental in any\ninformation retrieval system. In search engines, computing query-document\nsimilarity is an essential step in both retrieval and ranking stages. In eBay\nsearch, document is an item and the query-item similarity can be computed by\ncomparing different facets of the query-item pair. Query text can be compared\nwith the text of the item title. Likewise, a category constraint applied on the\nquery can be compared with the listing category of the item. However, images\nare one signal that are usually present in the items but are not present in the\nquery. Images are one of the most intuitive signals used by users to determine\nthe relevance of the item given a query. Including this signal in estimating\nsimilarity between the query-item pair is likely to improve the relevance of\nthe search engine. We propose a novel way of deriving image information for\nqueries. We attempt to learn image information for queries from item images\ninstead of generating explicit image features or an image for queries. We use\ncanonical correlation analysis (CCA) to learn a new subspace where projecting\nthe original data will give us a new query and item representation. We\nhypothesize that this new query representation will also have image information\nabout the query. We estimate the query-item similarity using a vector space\nmodel and report the performance of the proposed method on eBay's search data.\nWe show 11.89\\% relevance improvement over the baseline using area under the\nreceiver operating characteristic curve (AUROC) as the evaluation metric. We\nalso show 3.1\\% relevance improvement over the baseline with area under the\nprecision recall curve (AUPRC) .\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 23:48:31 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Porwal", "Utkarsh", ""]]}, {"id": "1904.12894", "submitter": "Hongwei Li", "authors": "Hongwei Li, Johannes C. Paetzold, Anjany Sekuboyina, Florian Kofler,\n  Jianguo Zhang, Jan S. Kirschke, Benedikt Wiestler, and Bjoern Menze", "title": "DiamondGAN: Unified Multi-Modal Generative Adversarial Networks for MRI\n  Sequences Synthesis", "comments": "accepted by miccai 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing MR imaging sequences is highly relevant in clinical practice, as\nsingle sequences are often missing or are of poor quality (e.g. due to motion).\nNaturally, the idea arises that a target modality would benefit from\nmulti-modal input, as proprietary information of individual modalities can be\nsynergistic. However, existing methods fail to scale up to multiple non-aligned\nimaging modalities, facing common drawbacks of complex imaging sequences. We\npropose a novel, scalable and multi-modal approach called DiamondGAN. Our model\nis capable of performing exible non-aligned cross-modality synthesis and data\ninfill, when given multiple modalities or any of their arbitrary subsets,\nlearning structured information in an end-to-end fashion. We synthesize two MRI\nsequences with clinical relevance (i.e., double inversion recovery (DIR) and\ncontrast-enhanced T1 (T1-c)), reconstructed from three common sequences. In\naddition, we perform a multi-rater visual evaluation experiment and find that\ntrained radiologists are unable to distinguish synthetic DIR images from real\nones.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:21:35 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 10:49:30 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 10:39:08 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 09:02:10 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Li", "Hongwei", ""], ["Paetzold", "Johannes C.", ""], ["Sekuboyina", "Anjany", ""], ["Kofler", "Florian", ""], ["Zhang", "Jianguo", ""], ["Kirschke", "Jan S.", ""], ["Wiestler", "Benedikt", ""], ["Menze", "Bjoern", ""]]}, {"id": "1904.12936", "submitter": "Amirreza Shaban", "authors": "Amirreza Shaban, Amir Rahimi, Shray Bansal, Stephen Gould, Byron\n  Boots, Richard Hartley", "title": "Learning to Find Common Objects Across Few Image Collections", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of bags where each bag is a set of images, our goal is to\nselect one image from each bag such that the selected images are from the same\nobject class. We model the selection as an energy minimization problem with\nunary and pairwise potential functions. Inspired by recent few-shot learning\nalgorithms, we propose an approach to learn the potential functions directly\nfrom the data. Furthermore, we propose a fast greedy inference algorithm for\nenergy minimization. We evaluate our approach on few-shot common object\nrecognition as well as object co-localization tasks. Our experiments show that\nlearning the pairwise and unary terms greatly improves the performance of the\nmodel over several well-known methods for these tasks. The proposed greedy\noptimization algorithm achieves performance comparable to state-of-the-art\nstructured inference algorithms while being ~10 times faster. The code is\npublicly available on https://github.com/haamoon/finding_common_object.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:26:40 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 01:08:21 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Shaban", "Amirreza", ""], ["Rahimi", "Amir", ""], ["Bansal", "Shray", ""], ["Gould", "Stephen", ""], ["Boots", "Byron", ""], ["Hartley", "Richard", ""]]}, {"id": "1904.12945", "submitter": "Jiaming Liu", "authors": "Jiaming Liu, Chi-Hao Wu, Yuzhi Wang, Qin Xu, Yuqian Zhou, Haibin\n  Huang, Chuan Wang, Shaofan Cai, Yifan Ding, Haoqiang Fan, Jue Wang", "title": "Learning Raw Image Denoising with Bayer Pattern Unification and Bayer\n  Preserving Augmentation", "comments": "Accepted by CVPRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present new data pre-processing and augmentation techniques\nfor DNN-based raw image denoising. Compared with traditional RGB image\ndenoising, performing this task on direct camera sensor readings presents new\nchallenges such as how to effectively handle various Bayer patterns from\ndifferent data sources, and subsequently how to perform valid data augmentation\nwith raw images. To address the first problem, we propose a Bayer pattern\nunification (BayerUnify) method to unify different Bayer patterns. This allows\nus to fully utilize a heterogeneous dataset to train a single denoising model\ninstead of training one model for each pattern. Furthermore, while it is\nessential to augment the dataset to improve model generalization and\nperformance, we discovered that it is error-prone to modify raw images by\nadapting augmentation methods designed for RGB images. Towards this end, we\npresent a Bayer preserving augmentation (BayerAug) method as an effective\napproach for raw image augmentation. Combining these data processing technqiues\nwith a modified U-Net, our method achieves a PSNR of 52.11 and a SSIM of 0.9969\nin NTIRE 2019 Real Image Denoising Challenge, demonstrating the\nstate-of-the-art performance. Our code is available at\nhttps://github.com/Jiaming-Liu/BayerUnifyAug.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 21:01:40 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 18:31:13 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Liu", "Jiaming", ""], ["Wu", "Chi-Hao", ""], ["Wang", "Yuzhi", ""], ["Xu", "Qin", ""], ["Zhou", "Yuqian", ""], ["Huang", "Haibin", ""], ["Wang", "Chuan", ""], ["Cai", "Shaofan", ""], ["Ding", "Yifan", ""], ["Fan", "Haoqiang", ""], ["Wang", "Jue", ""]]}, {"id": "1904.12966", "submitter": "Nicholas Turner", "authors": "Kisuk Lee, Nicholas Turner, Thomas Macrina, Jingpeng Wu, Ran Lu, H.\n  Sebastian Seung", "title": "Convolutional nets for reconstructing neural circuits from brain images\n  acquired by serial section electron microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural circuits can be reconstructed from brain images acquired by serial\nsection electron microscopy. Image analysis has been performed by manual labor\nfor half a century, and efforts at automation date back almost as far.\nConvolutional nets were first applied to neuronal boundary detection a dozen\nyears ago, and have now achieved impressive accuracy on clean images. Robust\nhandling of image defects is a major outstanding challenge. Convolutional nets\nare also being employed for other tasks in neural circuit reconstruction:\nfinding synapses and identifying synaptic partners, extending or pruning\nneuronal reconstructions, and aligning serial section images to create a 3D\nimage stack. Computational systems are being engineered to handle petavoxel\nimages of cubic millimeter brain volumes.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 21:54:58 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lee", "Kisuk", ""], ["Turner", "Nicholas", ""], ["Macrina", "Thomas", ""], ["Wu", "Jingpeng", ""], ["Lu", "Ran", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1904.12970", "submitter": "S\\'ebastien Bougleux", "authors": "Xuan Son Nguyen and Luc Brun and Olivier L\\'ezoray and S\\'ebastien\n  Bougleux", "title": "A neural network based on SPD manifold learning for skeleton-based hand\n  gesture recognition", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a new neural network based on SPD manifold learning for\nskeleton-based hand gesture recognition. Given the stream of hand's joint\npositions, our approach combines two aggregation processes on respectively\nspatial and temporal domains. The pipeline of our network architecture consists\nin three main stages. The first stage is based on a convolutional layer to\nincrease the discriminative power of learned features. The second stage relies\non different architectures for spatial and temporal Gaussian aggregation of\njoint features. The third stage learns a final SPD matrix from skeletal data. A\nnew type of layer is proposed for the third stage, based on a variant of\nstochastic gradient descent on Stiefel manifolds. The proposed network is\nvalidated on two challenging datasets and shows state-of-the-art accuracies on\nboth datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 21:59:14 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Nguyen", "Xuan Son", ""], ["Brun", "Luc", ""], ["L\u00e9zoray", "Olivier", ""], ["Bougleux", "S\u00e9bastien", ""]]}, {"id": "1904.12993", "submitter": "Yubo Zhang", "authors": "Yubo Zhang, Pavel Tokmakov, Martial Hebert, Cordelia Schmid", "title": "A Study on Action Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of the AVA dataset for action detection has caused a\nrenewed interest to this problem. Several approaches have been recently\nproposed that improved the performance. However, all of them have ignored the\nmain difficulty of the AVA dataset - its realistic distribution of training and\ntest examples. This dataset was collected by exhaustive annotation of human\naction in uncurated videos. As a result, the most common categories, such as\n`stand' or `sit', contain tens of thousands of examples, whereas rare ones have\nonly dozens. In this work we study the problem of action detection in a\nhighly-imbalanced dataset. Differently from previous work on handling long-tail\ncategory distributions, we begin by analyzing the imbalance in the test set. We\ndemonstrate that the standard AP metric is not informative for the categories\nin the tail, and propose an alternative one - Sampled AP. Armed with this new\nmeasure, we study the problem of transferring representations from the\ndata-rich head to the rare tail categories and propose a simple but effective\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 23:59:22 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 19:55:45 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhang", "Yubo", ""], ["Tokmakov", "Pavel", ""], ["Hebert", "Martial", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1904.13003", "submitter": "He Chen", "authors": "He Chen and Gregory S. Chirikjian", "title": "Curvature: A signature for Action Recognition in Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel signature of human action recognition, namely the\ncurvature of a video sequence, is introduced. In this way, the distribution of\nsequential data is modeled, which enables few-shot learning. Instead of\ndepending on recognizing features within images, our algorithm views actions as\nsequences on the universal time scale across a whole sequence of images. The\nvideo sequence, viewed as a curve in pixel space, is aligned by\nreparameterization using the arclength of the curve in pixel space. Once such\ncurvatures are obtained, statistical indexes are extracted and fed into a\nlearning-based classifier. Overall, our method is simple but powerful.\nPreliminary experimental results show that our method is effective and achieves\nstate-of-the-art performance in video-based human action recognition. Moreover,\nwe see latent capacity in transferring this idea into other sequence-based\nrecognition applications such as speech recognition, machine translation, and\ntext generation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 00:27:13 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 05:41:32 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Chen", "He", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1904.13028", "submitter": "Jinqiang Bai", "authors": "Jinqiang Bai, Shiguo Lian, Zhaoxiang Liu, Kai Wang, Dijun Liu", "title": "Virtual-Blind-Road Following Based Wearable Navigation Device for Blind\n  People", "comments": "8 pages, 9 figures, TCE accepted", "journal-ref": null, "doi": "10.1109/TCE.2018.2812498", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help the blind people walk to the destination efficiently and safely in\nindoor environment, a novel wearable navigation device is presented in this\npaper. The locating, way-finding, route following and obstacle avoiding modules\nare the essential components in a navigation system, while it remains a\nchallenging task to consider obstacle avoiding during route following, as the\nindoor environment is complex, changeable and possibly with dynamic objects. To\naddress this issue, we propose a novel scheme which utilizes a dynamic sub-goal\nselecting strategy to guide the users to the destination and help them bypass\nobstacles at the same time. This scheme serves as the key component of a\ncomplete navigation system deployed on a pair of wearable optical see-through\nglasses for the ease of use of blind people's daily walks. The proposed\nnavigation device has been tested on a collection of individuals and proved to\nbe effective on indoor navigation tasks. The sensors embedded are of low cost,\nsmall volume and easy integration, making it possible for the glasses to be\nwidely used as a wearable consumer device.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:07:01 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bai", "Jinqiang", ""], ["Lian", "Shiguo", ""], ["Liu", "Zhaoxiang", ""], ["Wang", "Kai", ""], ["Liu", "Dijun", ""]]}, {"id": "1904.13030", "submitter": "Zhe Liu", "authors": "Zhe Liu and Chuanzhe Suo and Shunbo Zhou and Huanshu Wei and Yingtian\n  Liu and Hesheng Wang and Yun-Hui Liu", "title": "SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on\n  Large-Scale Point Cloud Description for Self-Driving Vehicles", "comments": "This paper has been accepted by IROS-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition and loop-closure detection are main challenges in the\nlocalization, mapping and navigation tasks of self-driving vehicles. In this\npaper, we solve the loop-closure detection problem by incorporating the\ndeep-learning based point cloud description method and the coarse-to-fine\nsequence matching strategy. More specifically, we propose a deep neural network\nto extract a global descriptor from the original large-scale 3D point cloud,\nthen based on which, a typical place analysis approach is presented to\ninvestigate the feature space distribution of the global descriptors and select\nseveral super keyframes. Finally, a coarse-to-fine strategy, which includes a\nsuper keyframe based coarse matching stage and a local sequence matching stage,\nis presented to ensure the loop-closure detection accuracy and real-time\nperformance simultaneously. Thanks to the sequence matching operation, the\nproposed approach obtains an improvement against the existing deep-learning\nbased methods. Experiment results on a self-driving vehicle validate the\neffectiveness of the proposed loop-closure detection algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:09:01 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:31:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Liu", "Zhe", ""], ["Suo", "Chuanzhe", ""], ["Zhou", "Shunbo", ""], ["Wei", "Huanshu", ""], ["Liu", "Yingtian", ""], ["Wang", "Hesheng", ""], ["Liu", "Yun-Hui", ""]]}, {"id": "1904.13034", "submitter": "Jinqiang Bai", "authors": "Jinqiang Bai, Shiguo Lian, Zhaoxiang Liu, Kai Wang, Dijun Liu", "title": "Deep Learning Based Robot for Automatically Picking up Garbage on the\n  Grass", "comments": "8 pages, 13 figures,TCE accepted", "journal-ref": null, "doi": "10.1109/TCE.2018.2859629", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel garbage pickup robot which operates on the grass.\nThe robot is able to detect the garbage accurately and autonomously by using a\ndeep neural network for garbage recognition. In addition, with the ground\nsegmentation using a deep neural network, a novel navigation strategy is\nproposed to guide the robot to move around. With the garbage recognition and\nautomatic navigation functions, the robot can clean garbage on the ground in\nplaces like parks or schools efficiently and autonomously. Experimental results\nshow that the garbage recognition accuracy can reach as high as 95%, and even\nwithout path planning, the navigation strategy can reach almost the same\ncleaning efficiency with traditional methods. Thus, the proposed robot can\nserve as a good assistance to relieve dustman's physical labor on garbage\ncleaning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:21:37 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bai", "Jinqiang", ""], ["Lian", "Shiguo", ""], ["Liu", "Zhaoxiang", ""], ["Wang", "Kai", ""], ["Liu", "Dijun", ""]]}, {"id": "1904.13037", "submitter": "Jinqiang Bai", "authors": "Jinqiang Bai, Zhaoxiang Liu, Yimin Lin, Ye Li, Shiguo Lian, Dijun Liu", "title": "Wearable Travel Aid for Environment Perception and Navigation of\n  Visually Impaired People", "comments": "7 pages, 12 figures", "journal-ref": "2019 Electronics", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a wearable assistive device with the shape of a pair of\neyeglasses that allows visually impaired people to navigate safely and quickly\nin unfamiliar environment, as well as perceive the complicated environment to\nautomatically make decisions on the direction to move. The device uses a\nconsumer Red, Green, Blue and Depth (RGB-D) camera and an Inertial Measurement\nUnit (IMU) to detect obstacles. As the device leverages the ground height\ncontinuity among adjacent image frames, it is able to segment the ground from\nobstacles accurately and rapidly. Based on the detected ground, the optimal\nwalkable direction is computed and the user is then informed via converted beep\nsound. Moreover, by utilizing deep learning techniques, the device can\nsemantically categorize the detected obstacles to improve the users' perception\nof surroundings. It combines a Convolutional Neural Network (CNN) deployed on a\nsmartphone with a depth-image-based object detection to decide what the object\ntype is and where the object is located, and then notifies the user of such\ninformation via speech. We evaluated the device's performance with different\nexperiments in which 20 visually impaired people were asked to wear the device\nand move in an office, and found that they were able to avoid obstacle\ncollisions and find the way in complicated scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:33:45 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Bai", "Jinqiang", ""], ["Liu", "Zhaoxiang", ""], ["Lin", "Yimin", ""], ["Li", "Ye", ""], ["Lian", "Shiguo", ""], ["Liu", "Dijun", ""]]}, {"id": "1904.13072", "submitter": "Qi Wang", "authors": "Dong Wang and Yuan Yuan and Qi Wang", "title": "Cross-Modal Message Passing for Two-stream Fusion", "comments": "2018 IEEE International Conference on Acoustics, Speech and Signal\n  Processing", "journal-ref": null, "doi": "10.1109/ICASSP.2018.8461792", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing and fusing information among multi-modal is a very useful\ntechnique for achieving high performance in many computer vision problems. In\norder to tackle multi-modal information more effectively, we introduce a novel\nframework for multi-modal fusion: Cross-modal Message Passing (CMMP).\nSpecifically, we propose a cross-modal message passing mechanism to fuse\ntwo-stream network for action recognition, which composes of an appearance\nmodal network (RGB image) and a motion modal (optical flow image) network. The\nobjectives of individual networks in this framework are two-fold: a standard\nclassification objective and a competing objective. The classification object\nensures that each modal network predicts the true action category while the\ncompeting objective encourages each modal network to outperform the other one.\nWe quantitatively show that the proposed CMMP fuses the traditional two-stream\nnetwork more effectively, and outperforms all existing two-stream fusion method\non UCF-101 and HMDB-51 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 06:57:04 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Wang", "Dong", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "1904.13073", "submitter": "Wei Gao", "authors": "Wei Gao and Russ Tedrake", "title": "SurfelWarp: Efficient Non-Volumetric Single View Dynamic Reconstruction", "comments": "RSS 2018. The video and source code are available on\n  https://sites.google.com/view/surfelwarp/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute a dense SLAM system that takes a live stream of depth images as\ninput and reconstructs non-rigid deforming scenes in real time, without\ntemplates or prior models. In contrast to existing approaches, we do not\nmaintain any volumetric data structures, such as truncated signed distance\nfunction (TSDF) fields or deformation fields, which are performance and memory\nintensive. Our system works with a flat point (surfel) based representation of\ngeometry, which can be directly acquired from commodity depth sensors. Standard\ngraphics pipelines and general purpose GPU (GPGPU) computing are leveraged for\nall central operations: i.e., nearest neighbor maintenance, non-rigid\ndeformation field estimation and fusion of depth measurements. Our pipeline\ninherently avoids expensive volumetric operations such as marching cubes,\nvolumetric fusion and dense deformation field update, leading to significantly\nimproved performance. Furthermore, the explicit and flexible surfel based\ngeometry representation enables efficient tackling of topology changes and\ntracking failures, which makes our reconstructions consistent with updated\ndepth observations. Our system allows robots to maintain a scene description\nwith non-rigidly deformed objects that potentially enables interactions with\ndynamic working environments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 06:57:53 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Gao", "Wei", ""], ["Tedrake", "Russ", ""]]}, {"id": "1904.13078", "submitter": "Masanari Kimura", "authors": "Masanari Kimura, Masayuki Tanaka", "title": "Interpretation of Feature Space using Multi-Channel Attentional\n  Sub-Networks", "comments": "CVPR2019 Workshop on Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved impressive results in various\ntasks, but interpreting the internal mechanism is a challenging problem. To\ntackle this problem, we exploit a multi-channel attention mechanism in feature\nspace. Our network architecture allows us to obtain an attention mask for each\nfeature while existing CNN visualization methods provide only a common\nattention mask for all features. We apply the proposed multi-channel attention\nmechanism to multi-attribute recognition task. We can obtain different\nattention mask for each feature and for each attribute. Those analyses give us\ndeeper insight into the feature space of CNNs. The experimental results for the\nbenchmark dataset show that the proposed method gives high interpretability to\nhumans while accurately grasping the attributes of the data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:13:52 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kimura", "Masanari", ""], ["Tanaka", "Masayuki", ""]]}, {"id": "1904.13079", "submitter": "Qi Wang", "authors": "Yuan Yuan and Dong Wang and Qi Wang", "title": "Anomaly Detection in Traffic Scenes via Spatial-aware Motion\n  Reconstruction", "comments": "IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2016.2601655", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection from a driver's perspective when driving is important to\nautonomous vehicles. As a part of Advanced Driver Assistance Systems (ADAS), it\ncan remind the driver about dangers timely. Compared with traditional studied\nscenes such as the university campus and market surveillance videos, it is\ndifficult to detect abnormal event from a driver's perspective due to camera\nwaggle, abidingly moving background, drastic change of vehicle velocity, etc.\nTo tackle these specific problems, this paper proposes a spatial localization\nconstrained sparse coding approach for anomaly detection in traffic scenes,\nwhich firstly measures the abnormality of motion orientation and magnitude\nrespectively and then fuses these two aspects to obtain a robust detection\nresult. The main contributions are threefold: 1) This work describes the motion\norientation and magnitude of the object respectively in a new way, which is\ndemonstrated to be better than the traditional motion descriptors. 2) The\nspatial localization of object is taken into account of the sparse\nreconstruction framework, which utilizes the scene's structural information and\noutperforms the conventional sparse coding methods. 3) Results of motion\norientation and magnitude are adaptively weighted and fused by a Bayesian\nmodel, which makes the proposed method more robust and handle more kinds of\nabnormal events. The efficiency and effectiveness of the proposed method are\nvalidated by testing on nine difficult video sequences captured by ourselves.\nObserved from the experimental results, the proposed method is more effective\nand efficient than the popular competitors, and yields a higher performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:14:03 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Yuan", "Yuan", ""], ["Wang", "Dong", ""], ["Wang", "Qi", ""]]}, {"id": "1904.13080", "submitter": "Qi Wang", "authors": "Yuan Yuan and Dong Wang and Qi Wang", "title": "Memory-Augmented Temporal Dynamic Learning for Action Recognition", "comments": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions captured in video sequences contain two crucial factors for\naction recognition, i.e., visual appearance and motion dynamics. To model these\ntwo aspects, Convolutional and Recurrent Neural Networks (CNNs and RNNs) are\nadopted in most existing successful methods for recognizing actions. However,\nCNN based methods are limited in modeling long-term motion dynamics. RNNs are\nable to learn temporal motion dynamics but lack effective ways to tackle\nunsteady dynamics in long-duration motion. In this work, we propose a\nmemory-augmented temporal dynamic learning network, which learns to write the\nmost evident information into an external memory module and ignore irrelevant\nones. In particular, we present a differential memory controller to make a\ndiscrete decision on whether the external memory module should be updated with\ncurrent feature. The discrete memory controller takes in the memory history,\ncontext embedding and current feature as inputs and controls information flow\ninto the external memory module. Additionally, we train this discrete memory\ncontroller using straight-through estimator. We evaluate this end-to-end system\non benchmark datasets (UCF101 and HMDB51) of human action recognition. The\nexperimental results show consistent improvements on both datasets over prior\nworks and our baselines.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:19:50 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Yuan", "Yuan", ""], ["Wang", "Dong", ""], ["Wang", "Qi", ""]]}, {"id": "1904.13085", "submitter": "Qi Wang", "authors": "Dong Wang and Yuan Yuan and Qi Wang", "title": "Early Action Prediction with Generative Adversarial Networks", "comments": "IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2904857", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action Prediction is aimed to determine what action is occurring in a video\nas early as possible, which is crucial to many online applications, such as\npredicting a traffic accident before it happens and detecting malicious actions\nin the monitoring system. In this work, we address this problem by developing\nan end-to-end architecture that improves the discriminability of features of\npartially observed videos by assimilating them to features from complete\nvideos. For this purpose, the generative adversarial network is introduced for\ntackling action prediction problem, which improves the recognition accuracy of\npartially observed videos though narrowing the feature difference of partially\nobserved videos from complete ones. Specifically, its generator comprises of\ntwo networks: a CNN for feature extraction and an LSTM for estimating residual\nerror between features of the partially observed videos and complete ones, and\nthen the features from CNN adds the residual error from LSTM, which is regarded\nas the enhanced feature to fool a competing discriminator. Meanwhile, the\ngenerator is trained with an additional perceptual objective, which forces the\nenhanced features of partially observed videos are discriminative enough for\naction prediction. Extensive experimental results on UCF101, BIT and\nUT-Interaction datasets demonstrate that our approach outperforms the\nstate-of-the-art methods, especially for videos that less than 50% portion of\nframes is observed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:36:19 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Wang", "Dong", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "1904.13102", "submitter": "Jinqiang Bai", "authors": "Zhaoxiang Liu, Zezhou Chen, Jinqiang Bai, Shaohua Li, Shiguo Lian", "title": "Facial Pose Estimation by Deep Learning from Label Distributions", "comments": "9 pages,5 figures, Accepted by ICCV 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial pose estimation has gained a lot of attentions in many practical\napplications, such as human-robot interaction, gaze estimation and driver\nmonitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is\nbecoming more and more popular. However, facial pose estimation suffers from a\nkey challenge: the lack of sufficient training data for many poses, especially\nfor large poses. Inspired by the observation that the faces under close poses\nlook similar, we reformulate the facial pose estimation as a label distribution\nlearning problem, considering each face image as an example associated with a\nGaussian label distribution rather than a single label, and construct a\nconvolutional neural network which is trained with a multi-loss function on\nAFLW dataset and 300W-LP dataset to predict the facial poses directly from\ncolor image. Extensive experiments are conducted on several popular benchmarks,\nincluding AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant\nadvantage over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 08:38:20 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:27:03 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 08:11:31 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 02:48:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Zhaoxiang", ""], ["Chen", "Zezhou", ""], ["Bai", "Jinqiang", ""], ["Li", "Shaohua", ""], ["Lian", "Shiguo", ""]]}, {"id": "1904.13113", "submitter": "Xu Yang", "authors": "Xu Yang, Cheng Deng, Feng Zheng, Junchi Yan, Wei Liu", "title": "Deep Spectral Clustering using Dual Autoencoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering methods have recently absorbed even-increasing attention in\nlearning and vision. Deep clustering combines embedding and clustering together\nto obtain optimal embedding subspace for clustering, which can be more\neffective compared with conventional clustering methods. In this paper, we\npropose a joint learning framework for discriminative embedding and spectral\nclustering. We first devise a dual autoencoder network, which enforces the\nreconstruction constraint for the latent representations and their noisy\nversions, to embed the inputs into a latent space for clustering. As such the\nlearned latent representations can be more robust to noise. Then the mutual\ninformation estimation is utilized to provide more discriminative information\nfrom the inputs. Furthermore, a deep spectral clustering method is applied to\nembed the latent representations into the eigenspace and subsequently clusters\nthem, which can fully exploit the relationship between inputs to achieve\noptimal clustering results. Experimental results on benchmark datasets show\nthat our method can significantly outperform state-of-the-art clustering\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 09:12:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Yang", "Xu", ""], ["Deng", "Cheng", ""], ["Zheng", "Feng", ""], ["Yan", "Junchi", ""], ["Liu", "Wei", ""]]}, {"id": "1904.13132", "submitter": "Yuki Asano", "authors": "Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi", "title": "A critical analysis of self-supervision, or what we can learn from a\n  single image", "comments": "Accepted paper at the International Conference on Learning\n  Representations (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look critically at popular self-supervision techniques for learning deep\nconvolutional neural networks without manual labels. We show that three\ndifferent and representative methods, BiGAN, RotNet and DeepCluster, can learn\nthe first few layers of a convolutional network from a single image as well as\nusing millions of images and manual labels, provided that strong data\naugmentation is used. However, for deeper layers the gap with manual\nsupervision cannot be closed even if millions of unlabelled images are used for\ntraining. We conclude that: (1) the weights of the early layers of deep\nnetworks contain limited information about the statistics of natural images,\nthat (2) such low-level statistics can be learned through self-supervision just\nas well as through strong supervision, and that (3) the low-level statistics\ncan be captured via synthetic transformations instead of using a large image\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 10:10:38 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 13:48:48 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 17:56:41 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Asano", "Yuki M.", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1904.13148", "submitter": "Zhennan Wang", "authors": "Zhennan Wang, Wenbin Zou, Chen Xu", "title": "PR Product: A Substitute for Inner Product in Neural Networks", "comments": "ICCV2019 oral", "journal-ref": "Proceedings of the IEEE International Conference on Computer\n  Vision. 2019: 6013-6022", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the inner product of weight vector w and data\nvector x in neural networks from the perspective of vector orthogonal\ndecomposition and prove that the direction gradient of w decreases with the\nangle between them close to 0 or {\\pi}. We propose the Projection and Rejection\nProduct (PR Product) to make the direction gradient of w independent of the\nangle and consistently larger than the one in standard inner product while\nkeeping the forward propagation identical. As a reliable substitute for\nstandard inner product, the PR Product can be applied into many existing deep\nlearning modules, so we develop the PR Product version of fully connected\nlayer, convolutional layer and LSTM layer. In static image classification, the\nexperiments on CIFAR10 and CIFAR100 datasets demonstrate that the PR Product\ncan robustly enhance the ability of various state-of-the-art classification\nnetworks. On the task of image captioning, even without any bells and whistles,\nour PR Product version of captioning model can compete or outperform the\nstate-of-the-art models on MS COCO dataset. Code has been made available\nat:https://github.com/wzn0828/PR_Product.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 10:43:38 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 11:37:06 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wang", "Zhennan", ""], ["Zou", "Wenbin", ""], ["Xu", "Chen", ""]]}, {"id": "1904.13154", "submitter": "Ioan Marius Bilasco PhD", "authors": "Delphine Poux and Benjamin Allaert and Jose Mennesson and Nacim\n  Ihaddadene and Ioan Marius Bilasco and Chaabane Djeraba", "title": "Facial Expressions Analysis Under Occlusions Based on Specificities of\n  Facial Motion Propagation", "comments": null, "journal-ref": null, "doi": "10.1007/s11042-020-08993-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much progress has been made in the facial expression analysis field,\nfacial occlusions are still challenging. The main innovation brought by this\ncontribution consists in exploiting the specificities of facial movement\npropagation for recognizing expressions in presence of important occlusions.\nThe movement induced by an expression extends beyond the movement epicenter.\nThus, the movement occurring in an occluded region propagates towards\nneighboring visible regions. In presence of occlusions, per expression, we\ncompute the importance of each unoccluded facial region and we construct\nadapted facial frameworks that boost the performance of per expression binary\nclassifier. The output of each expression-dependant binary classifier is then\naggregated and fed into a fusion process that aims constructing, per occlusion,\na unique model that recognizes all the facial expressions considered. The\nevaluations highlight the robustness of this approach in presence of\nsignificant facial occlusions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 10:57:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Poux", "Delphine", ""], ["Allaert", "Benjamin", ""], ["Mennesson", "Jose", ""], ["Ihaddadene", "Nacim", ""], ["Bilasco", "Ioan Marius", ""], ["Djeraba", "Chaabane", ""]]}, {"id": "1904.13187", "submitter": "Amy Tabb", "authors": "Amy Tabb and Germ\\'an A Holgu\\'in and Rachel Naegele", "title": "Using cameras for precise measurement of two-dimensional plant features:\n  CASS", "comments": "5 pages, protocol. Code and data:\n  http://doi.org/10.5281/zenodo.3677473", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are used frequently in plant phenotyping to capture measurements. This\nchapter offers a repeatable method for capturing two-dimensional measurements\nof plant parts in field or laboratory settings using a variety of camera styles\n(cellular phone, DSLR), with the addition of a printed calibration pattern. The\nmethod is based on calibrating the camera using information available from the\nEXIF tags from the image, as well as visual information from the pattern. Code\nis provided to implement the method, as well as a dataset for testing. We\ninclude steps to verify protocol correctness by imaging an artifact. The use of\nthis protocol for two-dimensional plant phenotyping will allow data capture\nfrom different cameras and environments, with comparison on the same physical\nscale. We abbreviate this method as CASS, for CAmera aS Scanner. Code and data\nis available at http://doi.org/10.5281/zenodo.3677473.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 12:23:48 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 20:59:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tabb", "Amy", ""], ["Holgu\u00edn", "Germ\u00e1n A", ""], ["Naegele", "Rachel", ""]]}, {"id": "1904.13196", "submitter": "Marjan Alirezaie", "authors": "Marjan Alirezaie, Martin L\\\"angkvist, Michael Sioutis, Amy Loutfi", "title": "Semantic Referee: A Neural-Symbolic Framework for Enhancing Geospatial\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding why machine learning algorithms may fail is usually the task of\nthe human expert that uses domain knowledge and contextual information to\ndiscover systematic shortcomings in either the data or the algorithm. In this\npaper, we propose a semantic referee, which is able to extract qualitative\nfeatures of the errors emerging from deep machine learning frameworks and\nsuggest corrections. The semantic referee relies on ontological reasoning about\nspatial knowledge in order to characterize errors in terms of their spatial\nrelations with the environment. Using semantics, the reasoner interacts with\nthe learning algorithm as a supervisor. In this paper, the proposed method of\nthe interaction between a neural network classifier and a semantic referee\nshows how to improve the performance of semantic segmentation for satellite\nimagery data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 12:44:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Alirezaie", "Marjan", ""], ["L\u00e4ngkvist", "Martin", ""], ["Sioutis", "Michael", ""], ["Loutfi", "Amy", ""]]}, {"id": "1904.13204", "submitter": "Andrey Alekseev", "authors": "Andrey Alekseev and Anatoly Bobe", "title": "GaborNet: Gabor filters with learnable parameters in deep convolutional\n  neural networks", "comments": "10 pages, 6 figures, 3 tables, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes a system for image recognition using deep convolutional\nneural networks. Modified network architecture is proposed that focuses on\nimproving convergence and reducing training complexity. The filters in the\nfirst layer of the network are constrained to fit the Gabor function. The\nparameters of Gabor functions are learnable and are updated by standard\nbackpropagation techniques. The system was implemented on Python, tested on\nseveral datasets and outperformed the common convolutional networks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:12:36 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Alekseev", "Andrey", ""], ["Bobe", "Anatoly", ""]]}, {"id": "1904.13216", "submitter": "Paschalis Bizopoulos", "authors": "Paschalis Bizopoulos, George I Lambrou and Dimitrios Koutsouris", "title": "Signal2Image Modules in Deep Neural Networks for EEG Classification", "comments": "4 pages, 2 figures, 1 table, EMBC 2019", "journal-ref": "2019 41st Annual International Conference of the IEEE Engineering\n  in Medicine and Biology Society (EMBC)", "doi": "10.1109/EMBC.2019.8856620", "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has revolutionized computer vision utilizing the increased\navailability of big data and the power of parallel computational units such as\ngraphical processing units. The vast majority of deep learning research is\nconducted using images as training data, however the biomedical domain is rich\nin physiological signals that are used for diagnosis and prediction problems.\nIt is still an open research question how to best utilize signals to train deep\nneural networks.\n  In this paper we define the term Signal2Image (S2Is) as trainable or\nnon-trainable prefix modules that convert signals, such as\nElectroencephalography (EEG), to image-like representations making them\nsuitable for training image-based deep neural networks defined as `base\nmodels'. We compare the accuracy and time performance of four S2Is (`signal as\nimage', spectrogram, one and two layer Convolutional Neural Networks (CNNs))\ncombined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)\nalong with the depth-wise and 1D variations of the latter. We also provide\nempirical evidence that the one layer CNN S2I performs better in eleven out of\nfifteen tested models than non-trainable S2Is for classifying EEG signals and\nwe present visual comparisons of the outputs of the S2Is.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:05:22 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 20:59:57 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 08:47:01 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 16:40:00 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Bizopoulos", "Paschalis", ""], ["Lambrou", "George I", ""], ["Koutsouris", "Dimitrios", ""]]}, {"id": "1904.13219", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Bahaj Mohamed", "title": "A new algorithm for shape matching and pattern recognition using dynamic\n  programming", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.08501", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for shape recognition and retrieval based on dynamic\nprogramming. Our approach uses the dynamic programming algorithm to compute the\noptimal score and to find the optimal alignment between two strings. First,\neach contour of shape is represented by a set of points. After alignment and\nmatching between two shapes, the contours are transformed into a string of\nsymbols and numbers. Finally we find the best alignment of two complete strings\nand compute the optimal cost of similarity. In general, dynamic programming has\ntwo phases: the forward phase and the backward phase. In the forward phase, we\ncompute the optimal cost for each subproblem. In the backward phase, we\nreconstruct the solution that gives the optimal cost. Our algorithm is tested\nin a database that contains various shapes such as MPEG-7.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 11:27:30 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Mohamed", "Bahaj", ""]]}, {"id": "1904.13268", "submitter": "Chuan Wen", "authors": "Chuan Wen, Jie Chang, Ya Zhang, Siheng Chen, Yanfeng Wang, Mei Han, Qi\n  Tian", "title": "Handwritten Chinese Font Generation with Collaborative Stroke Refinement", "comments": "8 pages(exclude reference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic character generation is an appealing solution for new typeface\ndesign, especially for Chinese typefaces including over 3700 most commonly-used\ncharacters. This task has two main pain points: (i) handwritten characters are\nusually associated with thin strokes of few information and complex structure\nwhich are error prone during deformation; (ii) thousands of characters with\nvarious shapes are needed to synthesize based on a few manually designed\ncharacters. To solve those issues, we propose a novel\nconvolutional-neural-network-based model with three main techniques:\ncollaborative stroke refinement, using collaborative training strategy to\nrecover the missing or broken strokes; online zoom-augmentation, taking the\nadvantage of the content-reuse phenomenon to reduce the size of training set;\nand adaptive pre-deformation, standardizing and aligning the characters. The\nproposed model needs only 750 paired training samples; no pre-trained network,\nextra dataset resource or labels is needed. Experimental results show that the\nproposed method significantly outperforms the state-of-the-art methods under\nthe practical restriction on handwritten font synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:12:58 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 14:55:45 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 11:50:09 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wen", "Chuan", ""], ["Chang", "Jie", ""], ["Zhang", "Ya", ""], ["Chen", "Siheng", ""], ["Wang", "Yanfeng", ""], ["Han", "Mei", ""], ["Tian", "Qi", ""]]}, {"id": "1904.13270", "submitter": "Nico Lang", "authors": "Nico Lang, Konrad Schindler, Jan Dirk Wegner", "title": "Country-wide high-resolution vegetation height mapping with Sentinel-2", "comments": null, "journal-ref": "Remote Sensing of Environment 233 (2019) 111347", "doi": "10.1016/j.rse.2019.111347", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentinel-2 multi-spectral images collected over periods of several months\nwere used to estimate vegetation height for Gabon and Switzerland. A deep\nconvolutional neural network (CNN) was trained to extract suitable spectral and\ntextural features from reflectance images and to regress per-pixel vegetation\nheight. In Gabon, reference heights for training and validation were derived\nfrom airborne LiDAR measurements. In Switzerland, reference heights were taken\nfrom an existing canopy height model derived via photogrammetric surface\nreconstruction. The resulting maps have a mean absolute error (MAE) of 1.7 m in\nSwitzerland and 4.3 m in Gabon (a root mean square error (RMSE) of 3.4 m and\n5.6 m, respectively), and correctly estimate vegetation heights up to >50 m.\nThey also show good qualitative agreement with existing vegetation height maps.\nOur work demonstrates that, given a moderate amount of reference data (i.e.,\n2000 km$^2$ in Gabon and $\\approx$5800 km$^2$ in Switzerland), high-resolution\nvegetation height maps with 10 m ground sampling distance (GSD) can be derived\nat country scale from Sentinel-2 imagery.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:13:13 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 08:28:21 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Lang", "Nico", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "1904.13271", "submitter": "Sami Brandt", "authors": "Sami S. Brandt and Hanno Ackermann", "title": "Non-Rigid Structure-From-Motion by Rank-One Basis Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the affine, non-rigid structure-from-motion\nproblem can be solved by rank-one, thus degenerate, basis shapes. It is a\nnatural reformulation of the classic low-rank method by Bregler et al., where\nit was assumed that the deformable 3D structure is generated by a linear\ncombination of rigid basis shapes. The non-rigid shape will be decomposed into\nthe mean shape and the degenerate shapes, constructed from the right singular\nvectors of the low-rank decomposition. The right singular vectors are affinely\nback-projected into the 3D space, and the affine back-projections will also be\nsolved as part of the factorisation. By construction, a direct interpretation\nfor the right singular vectors of the low-rank decomposition will also follow:\nthey can be seen as principal components, hence, the first variant of our\nmethod is referred to as Rank-1-PCA. The second variant, referred to as\nRank-1-ICA, additionally estimates the orthogonal transform which maps the\ndeformation modes into as statistically independent modes as possible. It has\nthe advantage of pinpointing statistically dependent subspaces related to, for\ninstance, lip movements on human faces. Moreover, in contrast to prior works,\nno predefined dimensionality for the subspaces is imposed. The experiments on\nseveral datasets show that the method achieves better results than the\nstate-of-the-art, it can be computed faster, and it provides an intuitive\ninterpretation for the deformation modes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:23:03 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Brandt", "Sami S.", ""], ["Ackermann", "Hanno", ""]]}, {"id": "1904.13273", "submitter": "David Owen", "authors": "David Owen, Ping-Lin Chang", "title": "Detecting Reflections by Combining Semantic and Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflections in natural images commonly cause false positives in automated\ndetection systems. These false positives can lead to significant impairment of\naccuracy in the tasks of detection, counting and segmentation. Here, inspired\nby the recent panoptic approach to segmentation, we show how fusing instance\nand semantic segmentation can automatically identify reflection false\npositives, without explicitly needing to have the reflective regions labelled.\nWe explore in detail how state of the art two-stage detectors suffer a loss of\nbroader contextual features, and hence are unable to learn to ignore these\nreflections. We then present an approach to fuse instance and semantic\nsegmentations for this application, and subsequently show how this reduces\nfalse positive detections in a real world surveillance data with a large number\nof reflective surfaces. This demonstrates how panoptic segmentation and related\nwork, despite being in its infancy, can already be useful in real world\ncomputer vision problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:25:43 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Owen", "David", ""], ["Chang", "Ping-Lin", ""]]}, {"id": "1904.13281", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin and S. Mazdak Abulnaga", "title": "CT-To-MR Conditional Generative Adversarial Networks for Ischemic Stroke\n  Lesion Segmentation", "comments": "Seventh IEEE International Conference on Healthcare Informatics (ICHI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infarcted brain tissue resulting from acute stroke readily shows up as\nhyperintense regions within diffusion-weighted magnetic resonance imaging\n(DWI). It has also been proposed that computed tomography perfusion (CTP) could\nalternatively be used to triage stroke patients, given improvements in speed\nand availability, as well as reduced cost. However, CTP has a lower signal to\nnoise ratio compared to MR. In this work, we investigate whether a conditional\nmapping can be learned by a generative adversarial network to map CTP inputs to\ngenerated MR DWI that more clearly delineates hyperintense regions due to\nischemic stroke. We detail the architectures of the generator and discriminator\nand describe the training process used to perform image-to-image translation\nfrom multi-modal CT perfusion maps to diffusion weighted MR outputs. We\nevaluate the results both qualitatively by visual comparison of generated MR to\nground truth, as well as quantitatively by training fully convolutional neural\nnetworks that make use of generated MR data inputs to perform ischemic stroke\nlesion segmentation. Segmentation networks trained using generated CT-to-MR\ninputs result in at least some improvement on all metrics used for evaluation,\ncompared with networks that only use CT perfusion input.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:42:53 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Rubin", "Jonathan", ""], ["Abulnaga", "S. Mazdak", ""]]}, {"id": "1904.13300", "submitter": "Zehua Cheng", "authors": "Zehua Cheng, Yuxiang Wu, Zhenghua Xu, Thomas Lukasiewicz, Weiyang Wang", "title": "Segmentation is All You Need", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region proposal mechanisms are essential for existing deep learning\napproaches to object detection in images. Although they can generally achieve a\ngood detection performance under normal circumstances, their recall in a scene\nwith extreme cases is unacceptably low. This is mainly because bounding box\nannotations contain much environment noise information, and non-maximum\nsuppression (NMS) is required to select target boxes. Therefore, in this paper,\nwe propose the first anchor-free and NMS-free object detection model called\nweakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes\nsegmentation models to achieve an accurate and robust object detection without\nNMS. In WSMA-Seg, multimodal annotations are proposed to achieve an\ninstance-aware segmentation using weakly supervised bounding boxes; we also\ndevelop a run-data-based following algorithm to trace contours of objects. In\naddition, we propose a multi-scale pooling segmentation (MSP-Seg) as the\nunderlying segmentation model of WSMA-Seg to achieve a more accurate\nsegmentation and to enhance the detection accuracy of WSMA-Seg. Experimental\nresults on multiple datasets show that the proposed WSMA-Seg approach\noutperforms the state-of-the-art detectors.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:13:01 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 12:40:53 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 02:28:04 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Cheng", "Zehua", ""], ["Wu", "Yuxiang", ""], ["Xu", "Zhenghua", ""], ["Lukasiewicz", "Thomas", ""], ["Wang", "Weiyang", ""]]}, {"id": "1904.13307", "submitter": "Anant Vemuri", "authors": "Anant S. Vemuri", "title": "Survey of Computer Vision and Machine Learning in Gastrointestinal\n  Endoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to provide the reader a place to begin studying the\napplication of computer vision and machine learning to gastrointestinal (GI)\nendoscopy. They have been classified into 18 categories. It should be be noted\nby the reader that this is a review from pre-deep learning era. A lot of deep\nlearning based applications have not been covered in this thesis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 12:46:03 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Vemuri", "Anant S.", ""]]}, {"id": "1904.13342", "submitter": "Christopher Syben", "authors": "Christopher Syben and Markus Michen and Bernhard Stimpel and Stephan\n  Seitz and Stefan Ploner and Andreas K. Maier", "title": "PYRO-NN: Python Reconstruction Operators in Neural Networks", "comments": "V1: Submitted to Medical Physics, 11 pages, 7 figures", "journal-ref": null, "doi": "10.1002/mp.13753", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Recently, several attempts were conducted to transfer deep learning\nto medical image reconstruction. An increasingly number of publications follow\nthe concept of embedding the CT reconstruction as a known operator into a\nneural network. However, most of the approaches presented lack an efficient CT\nreconstruction framework fully integrated into deep learning environments. As a\nresult, many approaches are forced to use workarounds for mathematically\nunambiguously solvable problems. Methods: PYRO-NN is a generalized framework to\nembed known operators into the prevalent deep learning framework Tensorflow.\nThe current status includes state-of-the-art parallel-, fan- and cone-beam\nprojectors and back-projectors accelerated with CUDA provided as Tensorflow\nlayers. On top, the framework provides a high level Python API to conduct FBP\nand iterative reconstruction experiments with data from real CT systems.\nResults: The framework provides all necessary algorithms and tools to design\nend-to-end neural network pipelines with integrated CT reconstruction\nalgorithms. The high level Python API allows a simple use of the layers as\nknown from Tensorflow. To demonstrate the capabilities of the layers, the\nframework comes with three baseline experiments showing a cone-beam short scan\nFDK reconstruction, a CT reconstruction filter learning setup, and a TV\nregularized iterative reconstruction. All algorithms and tools are referenced\nto a scientific publication and are compared to existing non deep learning\nreconstruction frameworks. The framework is available as open-source software\nat \\url{https://github.com/csyben/PYRO-NN}. Conclusions: PYRO-NN comes with the\nprevalent deep learning framework Tensorflow and allows to setup end-to-end\ntrainable neural networks in the medical image reconstruction context. We\nbelieve that the framework will be a step towards reproducible research\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:12:55 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Syben", "Christopher", ""], ["Michen", "Markus", ""], ["Stimpel", "Bernhard", ""], ["Seitz", "Stephan", ""], ["Ploner", "Stefan", ""], ["Maier", "Andreas K.", ""]]}, {"id": "1904.13353", "submitter": "Andre Kelm", "authors": "Andre Peter Kelm, Vijesh Soorya Rao and Udo Zoelzer", "title": "Object Contour and Edge Detection with RefineContourNet", "comments": "Keywords: Object Contour Detection, Edge Detection, Multi-Path\n  Refinement CNN", "journal-ref": null, "doi": "10.1007/978-3-030-29888-3_20", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A ResNet-based multi-path refinement CNN is used for object contour\ndetection. For this task, we prioritise the effective utilization of the\nhigh-level abstraction capability of a ResNet, which leads to state-of-the-art\nresults for edge detection. Keeping our focus in mind, we fuse the high, mid\nand low-level features in that specific order, which differs from many other\napproaches. It uses the tensor with the highest-levelled features as the\nstarting point to combine it layer-by-layer with features of a lower\nabstraction level until it reaches the lowest level. We train this network on a\nmodified PASCAL VOC 2012 dataset for object contour detection and evaluate on a\nrefined PASCAL-val dataset reaching an excellent performance and an Optimal\nDataset Scale (ODS) of 0.752. Furthermore, by fine-training on the BSDS500\ndataset we reach state-of-the-art results for edge-detection with an ODS of\n0.824.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:34:27 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 15:19:10 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Kelm", "Andre Peter", ""], ["Rao", "Vijesh Soorya", ""], ["Zoelzer", "Udo", ""]]}, {"id": "1904.13358", "submitter": "Jeremiah Johnson", "authors": "Faisal Mahmood, Wenhao Xu, Nicholas J. Durr, Jeremiah W. Johnson, Alan\n  Yuille", "title": "Structured Prediction using cGANs with Fusion Discriminator", "comments": "13 pages, 5 figures, 3 tables", "journal-ref": "Workshop on Deep Generative Models for Structured Prediction at\n  ICLR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the fusion discriminator, a single unified framework for\nincorporating conditional information into a generative adversarial network\n(GAN) for a variety of distinct structured prediction tasks, including image\nsynthesis, semantic segmentation, and depth estimation. Much like commonly used\nconvolutional neural network -- conditional Markov random field (CNN-CRF)\nmodels, the proposed method is able to enforce higher-order consistency in the\nmodel, but without being limited to a very specific class of potentials. The\nmethod is conceptually simple and flexible, and our experimental results\ndemonstrate improvement on several diverse structured prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:42:55 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Mahmood", "Faisal", ""], ["Xu", "Wenhao", ""], ["Durr", "Nicholas J.", ""], ["Johnson", "Jeremiah W.", ""], ["Yuille", "Alan", ""]]}, {"id": "1904.13362", "submitter": "Yingjing Lu", "authors": "Yingjing Lu", "title": "The Level Weighted Structural Similarity Loss: A Step Away from the MSE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mean Square Error (MSE) has shown its strength when applied in deep\ngenerative models such as Auto-Encoders to model reconstruction loss. However,\nin image domain especially, the limitation of MSE is obvious: it assumes pixel\nindependence and ignores spatial relationships of samples. This contradicts\nmost architectures of Auto-Encoders which use convolutional layers to extract\nspatial dependent features. We base on the structural similarity metric (SSIM)\nand propose a novel level weighted structural similarity (LWSSIM) loss for\nconvolutional Auto-Encoders. Experiments on common datasets on various\nAuto-Encoder variants show that our loss is able to outperform the MSE loss and\nthe Vanilla SSIM loss. We also provide reasons why our model is able to succeed\nin cases where the standard SSIM loss fails.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:45:35 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lu", "Yingjing", ""]]}, {"id": "1904.13383", "submitter": "Chen Zhao", "authors": "Chen Zhao, Jiaqi Yang, Yang Xiao, and Zhiguo Cao", "title": "Comparative evaluation of 2D feature correspondence selection algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence selection aiming at seeking correct feature correspondences\nfrom raw feature matches is pivotal for a number of feature-matching-based\ntasks. Various 2D (image) correspondence selection algorithms have been\npresented with decades of progress. Unfortunately, the lack of an in-depth\nevaluation makes it difficult for developers to choose a proper algorithm given\na specific application. This paper fills this gap by evaluating eight 2D\ncorrespondence selection algorithms ranging from classical methods to the most\nrecent ones on four standard datasets. The diversity of experimental datasets\nbrings various nuisances including zoom, rotation, blur, viewpoint change, JPEG\ncompression, light change, different rendering styles and multi-structures for\ncomprehensive test. To further create different distributions of initial\nmatches, a set of combinations of detector and descriptor is also taken into\nconsideration. We measure the quality of a correspondence selection algorithm\nfrom four perspectives, i.e., precision, recall, F-measure and efficiency.\nAccording to evaluation results, the current advantages and limitations of all\nconsidered algorithms are aggregately summarized which could be treated as a\n\"user guide\" for the following developers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:31:07 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zhao", "Chen", ""], ["Yang", "Jiaqi", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""]]}]