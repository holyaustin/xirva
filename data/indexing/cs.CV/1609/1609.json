[{"id": "1609.00017", "submitter": "Gordon Christie", "authors": "Gordon Christie, Adam Shoemaker, Kevin Kochersberger, Pratap Tokekar,\n  Lance McLean, Alexander Leonessa", "title": "Radiation Search Operations using Scene Understanding with Autonomous\n  UAV and UGV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomously searching for hazardous radiation sources requires the ability\nof the aerial and ground systems to understand the scene they are scouting. In\nthis paper, we present systems, algorithms, and experiments to perform\nradiation search using unmanned aerial vehicles (UAV) and unmanned ground\nvehicles (UGV) by employing semantic scene segmentation. The aerial data is\nused to identify radiological points of interest, generate an orthophoto along\nwith a digital elevation model (DEM) of the scene, and perform semantic\nsegmentation to assign a category (e.g. road, grass) to each pixel in the\northophoto. We perform semantic segmentation by training a model on a dataset\nof images we collected and annotated, using the model to perform inference on\nimages of the test area unseen to the model, and then refining the results with\nthe DEM to better reason about category predictions at each pixel. We then use\nall of these outputs to plan a path for a UGV carrying a LiDAR to map the\nenvironment and avoid obstacles not present during the flight, and a radiation\ndetector to collect more precise radiation measurements from the ground.\nResults of the analysis for each scenario tested favorably. We also note that\nour approach is general and has the potential to work for a variety of\ndifferent sensing tasks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 20:00:46 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Christie", "Gordon", ""], ["Shoemaker", "Adam", ""], ["Kochersberger", "Kevin", ""], ["Tokekar", "Pratap", ""], ["McLean", "Lance", ""], ["Leonessa", "Alexander", ""]]}, {"id": "1609.00036", "submitter": "Amogh Gudi", "authors": "Agne Grinciunaite, Amogh Gudi, Emrah Tasli, Marten den Uyl", "title": "Human Pose Estimation in Space and Time using 3D CNN", "comments": "Accepted at ECCV 2016 Workshop on: Brave new ideas for motion\n  representations in videos", "journal-ref": null, "doi": "10.1007/978-3-319-49409-8_5", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the capabilities of convolutional neural networks to deal\nwith a task that is easily manageable for humans: perceiving 3D pose of a human\nbody from varying angles. However, in our approach, we are restricted to using\na monocular vision system. For this purpose, we apply a convolutional neural\nnetwork approach on RGB videos and extend it to three dimensional convolutions.\nThis is done via encoding the time dimension in videos as the 3\\ts{rd}\ndimension in convolutional space, and directly regressing to human body joint\npositions in 3D coordinate space. This research shows the ability of such a\nnetwork to achieve state-of-the-art performance on the selected Human3.6M\ndataset, thus demonstrating the possibility of successfully representing\ntemporal data with an additional dimension in the convolutional operation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 20:55:26 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 16:17:15 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 12:44:15 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Grinciunaite", "Agne", ""], ["Gudi", "Amogh", ""], ["Tasli", "Emrah", ""], ["Uyl", "Marten den", ""]]}, {"id": "1609.00053", "submitter": "Laura Rebollo-Neira", "authors": "Laura Rebollo-Neira, Miroslav Rozloznik and Pradip Sasmal", "title": "Analysis of the Self Projected Matching Pursuit Algorithm", "comments": "The routines for implementing the methods, as well as scripts to\n  reproduce the examples in the manuscript, are available on the website:\n  http://www.nonlinear-approx.info/examples/node04.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convergence and numerical analysis of a low memory implementation of the\nOrthogonal Matching Pursuit greedy strategy, which is termed Self Projected\nMatching Pursuit, is presented. This approach renders an iterative way of\nsolving the least squares problem with much less storage requirement than\ndirect linear algebra techniques. Hence, it appropriate for solving large\nlinear systems. The analysis highlights its suitability within the class of\nwell posed problems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:58:17 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 19:54:18 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 15:36:34 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Rebollo-Neira", "Laura", ""], ["Rozloznik", "Miroslav", ""], ["Sasmal", "Pradip", ""]]}, {"id": "1609.00072", "submitter": "Siavash Gorji", "authors": "Siavash Gorji and James J. Clark", "title": "Attentional Push: Augmenting Salience with Shared Attention Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel visual attention tracking technique based on Shared\nAttention modeling. Our proposed method models the viewer as a participant in\nthe activity occurring in the scene. We go beyond image salience and instead of\nonly computing the power of an image region to pull attention to it, we also\nconsider the strength with which other regions of the image push attention to\nthe region in question. We use the term Attentional Push to refer to the power\nof image regions to direct and manipulate the attention allocation of the\nviewer. An attention model is presented that incorporates the Attentional Push\ncues with standard image salience-based attention modeling algorithms to\nimprove the ability to predict where viewers will fixate. Experimental\nevaluation validates significant improvements in predicting viewers' fixations\nusing the proposed methodology in both static and dynamic imagery.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 00:43:11 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Gorji", "Siavash", ""], ["Clark", "James J.", ""]]}, {"id": "1609.00096", "submitter": "Manh Duong Phung", "authors": "Tran Hiep Dinh, Minh Trien Pham, Manh Duong Phung, Duc Manh Nguyen,\n  Van Manh Hoang, Quang Vinh Tran", "title": "Image segmentation based on histogram of depth and an application in\n  driver distraction detection", "comments": "6 pages In 13th International Conference on Control Automation\n  Robotics & Vision (ICARCV), 2014", "journal-ref": null, "doi": "10.1109/ICARCV.2014.7064437", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes an approach to segment human object from a depth image\nbased on histogram of depth values. The region of interest is first extracted\nbased on a predefined threshold for histogram regions. A region growing process\nis then employed to separate multiple human bodies with the same depth\ninterval. Our contribution is the identification of an adaptive growth\nthreshold based on the detected histogram region. To demonstrate the\neffectiveness of the proposed method, an application in driver distraction\ndetection was introduced. After successfully extracting the driver's position\ninside the car, we came up with a simple solution to track the driver motion.\nWith the analysis of the difference between initial and current frame, a change\nof cluster position or depth value in the interested region, which cross the\npreset threshold, is considered as a distracted activity. The experiment\nresults demonstrated the success of the algorithm in detecting typical\ndistracted driving activities such as using phone for calling or texting,\nadjusting internal devices and drinking in real time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 03:19:43 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Dinh", "Tran Hiep", ""], ["Pham", "Minh Trien", ""], ["Phung", "Manh Duong", ""], ["Nguyen", "Duc Manh", ""], ["Hoang", "Van Manh", ""], ["Tran", "Quang Vinh", ""]]}, {"id": "1609.00129", "submitter": "Michael Opitz", "authors": "Michael Opitz, Georg Waltner, Georg Poier, Horst Possegger, Horst\n  Bischof", "title": "Grid Loss: Detecting Occluded Faces", "comments": "accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of partially occluded objects is a challenging computer vision\nproblem. Standard Convolutional Neural Network (CNN) detectors fail if parts of\nthe detection window are occluded, since not every sub-part of the window is\ndiscriminative on its own. To address this issue, we propose a novel loss layer\nfor CNNs, named grid loss, which minimizes the error rate on sub-blocks of a\nconvolution layer independently rather than over the whole feature map. This\nresults in parts being more discriminative on their own, enabling the detector\nto recover if the detection window is partially occluded. By mapping our loss\nlayer back to a regular fully connected layer, no additional computational cost\nis incurred at runtime compared to standard CNNs. We demonstrate our method for\nface detection on several public face detection benchmarks and show that our\nmethod outperforms regular CNNs, is suitable for realtime applications and\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 07:15:13 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Opitz", "Michael", ""], ["Waltner", "Georg", ""], ["Poier", "Georg", ""], ["Possegger", "Horst", ""], ["Bischof", "Horst", ""]]}, {"id": "1609.00153", "submitter": "Limin Wang", "authors": "Zhe Wang, Limin Wang, Yali Wang, Bowen Zhang, Yu Qiao", "title": "Weakly Supervised PatchNets: Describing and Aggregating Local Patches\n  for Scene Recognition", "comments": "To appear in IEEE Transactions on Image Processing. Code and model\n  available at https://github.com/wangzheallen/vsad", "journal-ref": null, "doi": "10.1109/TIP.2017.2666739", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional feature encoding scheme (e.g., Fisher vector) with local\ndescriptors (e.g., SIFT) and recent convolutional neural networks (CNNs) are\ntwo classes of successful methods for image recognition. In this paper, we\npropose a hybrid representation, which leverages the discriminative capacity of\nCNNs and the simplicity of descriptor encoding schema for image recognition,\nwith a focus on scene recognition. To this end, we make three main\ncontributions from the following aspects. First, we propose a patch-level and\nend-to-end architecture to model the appearance of local patches, called {\\em\nPatchNet}. PatchNet is essentially a customized network trained in a weakly\nsupervised manner, which uses the image-level supervision to guide the\npatch-level feature extraction. Second, we present a hybrid visual\nrepresentation, called {\\em VSAD}, by utilizing the robust feature\nrepresentations of PatchNet to describe local patches and exploiting the\nsemantic probabilities of PatchNet to aggregate these local patches into a\nglobal representation. Third, based on the proposed VSAD representation, we\npropose a new state-of-the-art scene recognition approach, which achieves an\nexcellent performance on two standard benchmarks: MIT Indoor67 (86.2\\%) and\nSUN397 (73.0\\%).\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 09:15:41 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 21:12:53 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Wang", "Zhe", ""], ["Wang", "Limin", ""], ["Wang", "Yali", ""], ["Zhang", "Bowen", ""], ["Qiao", "Yu", ""]]}, {"id": "1609.00162", "submitter": "Limin Wang", "authors": "Limin Wang, Zhe Wang, Yu Qiao, Luc Van Gool", "title": "Transferring Object-Scene Convolutional Neural Networks for Event\n  Recognition in Still Images", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event recognition in still images is an intriguing problem and has potential\nfor real applications. This paper addresses the problem of event recognition by\nproposing a convolutional neural network that exploits knowledge of objects and\nscenes for event classification (OS2E-CNN). Intuitively, it stands to reason\nthat there exists a correlation among the concepts of objects, scenes, and\nevents. We empirically demonstrate that the recognition of objects and scenes\nsubstantially contributes to the recognition of events. Meanwhile, we propose\nan iterative selection method to identify a subset of object and scene classes,\nwhich help to more efficiently and effectively transfer their deep\nrepresentations to event recognition. Specifically, we develop three types of\ntransferring techniques: (1) initialization-based transferring, (2)\nknowledge-based transferring, and (3) data-based transferring. These newly\ndesigned transferring techniques exploit multi-task learning frameworks to\nincorporate extra knowledge from other networks and additional datasets into\nthe training procedure of event CNNs. These multi-task learning frameworks turn\nout to be effective in reducing the effect of over-fitting and improving the\ngeneralization ability of the learned CNNs. With OS2E-CNN, we design a\nmulti-ratio and multi-scale cropping strategy, and propose an end-to-end event\nrecognition pipeline. We perform experiments on three event recognition\nbenchmarks: the ChaLearn Cultural Event Recognition dataset, the Web Image\nDataset for Event Recognition (WIDER), and the UIUC Sports Event dataset. The\nexperimental results show that our proposed algorithm successfully adapts\nobject and scene representations towards the event dataset and that it achieves\nthe current state-of-the-art performance on these challenging datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 09:47:22 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Wang", "Limin", ""], ["Wang", "Zhe", ""], ["Qiao", "Yu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1609.00221", "submitter": "Federico Becattini", "authors": "Giovanni Cuffaro, Federico Becattini, Claudio Baecchi, Lorenzo\n  Seidenari, Alberto Del Bimbo", "title": "Segmentation Free Object Discovery in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a simple yet effective approach to extend without\nsupervision any object proposal from static images to videos. Unlike previous\nmethods, these spatio-temporal proposals, to which we refer as tracks, are\ngenerated relying on little or no visual content by only exploiting bounding\nboxes spatial correlations through time. The tracks that we obtain are likely\nto represent objects and are a general-purpose tool to represent meaningful\nvideo content for a wide variety of tasks. For unannotated videos, tracks can\nbe used to discover content without any supervision. As further contribution we\nalso propose a novel and dataset-independent method to evaluate a generic\nobject proposal based on the entropy of a classifier output response. We\nexperiment on two competitive datasets, namely YouTube Objects and ILSVRC-2015\nVID.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 13:08:39 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Cuffaro", "Giovanni", ""], ["Becattini", "Federico", ""], ["Baecchi", "Claudio", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1609.00278", "submitter": "Arsalan Mousavian", "authors": "Arsalan Mousavian, Jana Kosecka", "title": "Semantic Image Based Geolocation Given a Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem visual place recognition is commonly used strategy for\nlocalization. Most successful appearance based methods typically rely on a\nlarge database of views endowed with local or global image descriptors and\nstrive to retrieve the views of the same location. The quality of the results\nis often affected by the density of the reference views and the robustness of\nthe image representation with respect to viewpoint variations, clutter and\nseasonal changes. In this work we present an approach for geo-locating a novel\nview and determining camera location and orientation using a map and a sparse\nset of geo-tagged reference views. We propose a novel technique for detection\nand identification of building facades from geo-tagged reference view using the\nmap and geometry of the building facades. We compute the likelihood of camera\nlocation and orientation of the query images using the detected landmark\n(building) identities from reference views, 2D map of the environment, and\ngeometry of building facades. We evaluate our approach for building\nidentification and geo-localization on a new challenging outdoors urban dataset\nexhibiting large variations in appearance and viewpoint.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 15:27:02 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Mousavian", "Arsalan", ""], ["Kosecka", "Jana", ""]]}, {"id": "1609.00344", "submitter": "Concetto Spampinato Dr", "authors": "Concetto Spampinato and Simone Palazzo and Isaak Kavasidis and Daniela\n  Giordano and Mubarak Shah and Nasim Souly", "title": "Deep Learning Human Mind for Automated Visual Classification", "comments": "CVPR 2017 Accepted Paper", "journal-ref": "CVPR 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What if we could effectively read the mind and transfer human visual\ncapabilities to computer vision methods? In this paper, we aim at addressing\nthis question by developing the first visual object classifier driven by human\nbrain signals. In particular, we employ EEG data evoked by visual object\nstimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative\nbrain activity manifold of visual categories. Afterwards, we train a\nConvolutional Neural Network (CNN)-based regressor to project images onto the\nlearned manifold, thus effectively allowing machines to employ human\nbrain-based features for automated visual classification. We use a 32-channel\nEEG to record brain activity of seven subjects while looking at images of 40\nImageNet object classes. The proposed RNN based approach for discriminating\nobject classes using brain signals reaches an average accuracy of about 40%,\nwhich outperforms existing methods attempting to learn EEG visual object\nrepresentations. As for automated object categorization, our human brain-driven\napproach obtains competitive performance, comparable to those achieved by\npowerful CNN models, both on ImageNet and CalTech 101, thus demonstrating its\nclassification and generalization capabilities. This gives us a real hope that,\nindeed, human mind can be read and transferred to machines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 18:47:05 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 11:39:22 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Spampinato", "Concetto", ""], ["Palazzo", "Simone", ""], ["Kavasidis", "Isaak", ""], ["Giordano", "Daniela", ""], ["Shah", "Mubarak", ""], ["Souly", "Nasim", ""]]}, {"id": "1609.00361", "submitter": "Mona Fathollahi", "authors": "Mona Fathollahi and Rangachar Kasturi", "title": "Autonomous driving challenge: To Infer the property of a dynamic object\n  based on its motion pattern using recurrent neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving applications a critical challenge is to identify action\nto take to avoid an obstacle on collision course. For example, when a heavy\nobject is suddenly encountered it is critical to stop the vehicle or change the\nlane even if it causes other traffic disruptions. However,there are situations\nwhen it is preferable to collide with the object rather than take an action\nthat would result in a much more serious accident than collision with the\nobject. For example, a heavy object which falls from a truck should be avoided\nwhereas a bouncing ball or a soft target such as a foam box need not be.We\npresent a novel method to discriminate between the motion characteristics of\nthese types of objects based on their physical properties such as bounciness,\nelasticity, etc.In this preliminary work, we use recurrent neural net-work with\nLSTM cells to train a classifier to classify objects based on their motion\ntrajectories. We test the algorithm on synthetic data, and, as a proof of\nconcept, demonstrate its effectiveness on a limited set of real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 19:39:08 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 00:01:45 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Fathollahi", "Mona", ""], ["Kasturi", "Rangachar", ""]]}, {"id": "1609.00408", "submitter": "Richard McPherson", "authors": "Richard McPherson, Reza Shokri, and Vitaly Shmatikov", "title": "Defeating Image Obfuscation with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that modern image recognition methods based on artificial\nneural networks can recover hidden information from images protected by various\nforms of obfuscation. The obfuscation techniques considered in this paper are\nmosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a\nrecently proposed system for privacy-preserving photo sharing that encrypts the\nsignificant JPEG coefficients to make images unrecognizable by humans. We\nempirically show how to train artificial neural networks to successfully\nidentify faces and recognize objects and handwritten digits even if the images\nare protected using any of the above obfuscation techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 21:38:15 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 21:47:12 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["McPherson", "Richard", ""], ["Shokri", "Reza", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "1609.00446", "submitter": "Fatemehsadat Saleh", "authors": "Fatemehsadat Saleh, Mohammad Sadegh Ali Akbarian, Mathieu Salzmann,\n  Lars Petersson, Stephen Gould, Jose M. Alvarez", "title": "Built-in Foreground/Background Prior for Weakly-Supervised Semantic\n  Segmentation", "comments": "Accepted in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-level annotations are expensive and time consuming to obtain. Hence,\nweak supervision using only image tags could have a significant impact in\nsemantic segmentation. Recently, CNN-based methods have proposed to fine-tune\npre-trained networks using image tags. Without additional information, this\nleads to poor localization accuracy. This problem, however, was alleviated by\nmaking use of objectness priors to generate foreground/background masks.\nUnfortunately these priors either require training pixel-level\nannotations/bounding boxes, or still yield inaccurate object boundaries. Here,\nwe propose a novel method to extract markedly more accurate masks from the\npre-trained network itself, forgoing external objectness modules. This is\naccomplished using the activations of the higher-level convolutional layers,\nsmoothed by a dense CRF. We demonstrate that our method, based on these masks\nand a weakly-supervised loss, outperforms the state-of-the-art tag-based\nweakly-supervised semantic segmentation techniques. Furthermore, we introduce a\nnew form of inexpensive weak supervision yielding an additional accuracy boost.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 01:49:51 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Saleh", "Fatemehsadat", ""], ["Akbarian", "Mohammad Sadegh Ali", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""], ["Gould", "Stephen", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "1609.00496", "submitter": "Shu Liu", "authors": "Shu Liu, Bo Li, Yangyu Fan, Zhe Guo, Ashok Samal", "title": "Label distribution based facial attractiveness computation by deep\n  residual learning", "comments": "3 pages, 3 figures. The first two authors are parallel first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two challenges lie in the facial attractiveness computation research: the\nlack of true attractiveness labels (scores), and the lack of an accurate face\nrepresentation. In order to address the first challenge, this paper recasts\nfacial attractiveness computation as a label distribution learning (LDL)\nproblem rather than a traditional single-label supervised learning task. In\nthis way, the negative influence of the label incomplete problem can be\nreduced. Inspired by the recent promising work in face recognition using deep\nneural networks to learn effective features, the second challenge is expected\nto be solved from a deep learning point of view. A very deep residual network\nis utilized to enable automatic learning of hierarchical aesthetics\nrepresentation. Integrating these two ideas, an end-to-end deep learning\nframework is established. Our approach achieves the best results on a standard\nbenchmark SCUT-FBP dataset compared with other state-of-the-art work.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 08:08:39 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 09:06:31 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Liu", "Shu", ""], ["Li", "Bo", ""], ["Fan", "Yangyu", ""], ["Guo", "Zhe", ""], ["Samal", "Ashok", ""]]}, {"id": "1609.00629", "submitter": "Elad Richardson", "authors": "Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky", "title": "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SEBOOST, a technique for boosting the performance of existing\nstochastic optimization methods. SEBOOST applies a secondary optimization\nprocess in the subspace spanned by the last steps and descent directions. The\nmethod was inspired by the SESOP optimization method for large-scale problems,\nand has been adapted for the stochastic learning framework. It can be applied\non top of any existing optimization method with no need to tweak the internal\nalgorithm. We show that the method is able to boost the performance of\ndifferent algorithms, and make them more robust to changes in their\nhyper-parameters. As the boosting steps of SEBOOST are applied between large\nsets of descent steps, the additional subspace optimization hardly increases\nthe overall computational burden. We introduce two hyper-parameters that\ncontrol the balance between the baseline method and the secondary optimization\nprocess. The method was evaluated on several deep learning tasks, demonstrating\npromising results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 14:48:16 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Richardson", "Elad", ""], ["Herskovitz", "Rom", ""], ["Ginsburg", "Boris", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1609.00817", "submitter": "Ru-Ze Liang", "authors": "Jihong Fan, Ru-Ze Liang", "title": "Stochastic Learning of Multi-Instance Dictionary for Earth Mover's\n  Distance based Histogram Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary plays an important role in multi-instance data representation. It\nmaps bags of instances to histograms. Earth mover's distance (EMD) is the most\neffective histogram distance metric for the application of multi-instance\nretrieval. However, up to now, there is no existing multi-instance dictionary\nlearning methods designed for EMD based histogram comparison. To fill this gap,\nwe develop the first EMD-optimal dictionary learning method using stochastic\noptimization method. In the stochastic learning framework, we have one triplet\nof bags, including one basic bag, one positive bag, and one negative bag. These\nbags are mapped to histograms using a multi-instance dictionary. We argue that\nthe EMD between the basic histogram and the positive histogram should be\nsmaller than that between the basic histogram and the negative histogram. Base\non this condition, we design a hinge loss. By minimizing this hinge loss and\nsome regularization terms of the dictionary, we update the dictionary\ninstances. The experiments over multi-instance retrieval applications shows its\neffectiveness when compared to other dictionary learning methods over the\nproblems of medical image retrieval and natural language relation\nclassification.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 11:19:39 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Fan", "Jihong", ""], ["Liang", "Ru-Ze", ""]]}, {"id": "1609.00836", "submitter": "Wei-Chen Chiu", "authors": "Wei-Chen Chiu, Fabio Galasso, Mario Fritz", "title": "Towards Segmenting Consumer Stereo Videos: Benchmark, Baselines and\n  Ensembles", "comments": "accepted by ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are we ready to segment consumer stereo videos? The amount of this data type\nis rapidly increasing and encompasses rich information of appearance, motion\nand depth cues. However, the segmentation of such data is still largely\nunexplored. First, we propose therefore a new benchmark: videos, annotations\nand metrics to measure progress on this emerging challenge. Second, we evaluate\nseveral state of the art segmentation methods and propose a novel ensemble\nmethod based on recent spectral theory. This combines existing image and video\nsegmentation techniques in an efficient scheme. Finally, we propose and\nintegrate into this model a novel regressor, learnt to optimize the stereo\nsegmentation performance directly via a differentiable proxy. The regressor\nmakes our segmentation ensemble adaptive to each stereo video and outperforms\nthe segmentations of the ensemble as well as a most recent RGB-D segmentation\ntechnique.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 15:19:21 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 10:21:54 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Chiu", "Wei-Chen", ""], ["Galasso", "Fabio", ""], ["Fritz", "Mario", ""]]}, {"id": "1609.00866", "submitter": "Mohammad Sabokrou", "authors": "Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, Zahra Moayedd,\n  Reinhard klette", "title": "Deep-Anomaly: Fully Convolutional Neural Network for Fast Anomaly\n  Detection in Crowded Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The detection of abnormal behaviours in crowded scenes has to deal with many\nchallenges. This paper presents an efficient method for detection and\nlocalization of anomalies in videos. Using fully convolutional neural networks\n(FCNs) and temporal data, a pre-trained supervised FCN is transferred into an\nunsupervised FCN ensuring the detection of (global) anomalies in scenes. High\nperformance in terms of speed and accuracy is achieved by investigating the\ncascaded detection as a result of reducing computation complexities. This\nFCN-based architecture addresses two main tasks, feature representation and\ncascaded outlier detection. Experimental results on two benchmarks suggest that\ndetection and localization of the proposed method outperforms existing methods\nin terms of accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 21:31:45 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 20:37:05 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Fayyaz", "Mohsen", ""], ["Fathy", "Mahmood", ""], ["Moayedd", "Zahra", ""], ["klette", "Reinhard", ""]]}, {"id": "1609.00878", "submitter": "Joao Papa", "authors": "Silas E. N. Fernandes, Danillo R. Pereira, Caio C. O. Ramos, Andre N.\n  Souza and Joao P. Papa", "title": "A Probabilistic Optimum-Path Forest Classifier for Binary Classification\n  Problems", "comments": "Submitted to Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic-driven classification techniques extend the role of traditional\napproaches that output labels (usually integer numbers) only. Such techniques\nare more fruitful when dealing with problems where one is not interested in\nrecognition/identification only, but also into monitoring the behavior of\nconsumers and/or machines, for instance. Therefore, by means of probability\nestimates, one can take decisions to work better in a number of scenarios. In\nthis paper, we propose a probabilistic-based Optimum Path Forest (OPF)\nclassifier to handle with binary classification problems, and we show it can be\nmore accurate than naive OPF in a number of datasets. In addition to being just\nmore accurate or not, probabilistic OPF turns to be another useful tool to the\nscientific community.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 00:12:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Fernandes", "Silas E. N.", ""], ["Pereira", "Danillo R.", ""], ["Ramos", "Caio C. O.", ""], ["Souza", "Andre N.", ""], ["Papa", "Joao P.", ""]]}, {"id": "1609.00967", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Vanishing point detection with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the finding that vanishing point (road tangent) guides driver's\ngaze, in our previous work we showed that vanishing point attracts gaze during\nfree viewing of natural scenes as well as in visual search (Borji et al.,\nJournal of Vision 2016). We have also introduced improved saliency models using\nvanishing point detectors (Feng et al., WACV 2016). Here, we aim to predict\nvanishing points in naturalistic environments by training convolutional neural\nnetworks in an end-to-end manner over a large set of road images downloaded\nfrom Youtube with vanishing points annotated. Results demonstrate effectiveness\nof our approach compared to classic approaches of vanishing point detection in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 17:51:27 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "1609.01006", "submitter": "Jianxu Chen", "authors": "Jianxu Chen, Lin Yang, Yizhe Zhang, Mark Alber, Danny Z. Chen", "title": "Combining Fully Convolutional and Recurrent Neural Networks for 3D\n  Biomedical Image Segmentation", "comments": null, "journal-ref": "https://papers.nips.cc/paper/2016/hash/4dcf435435894a4d0972046fc566af76-Abstract.html", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmentation of 3D images is a fundamental problem in biomedical image\nanalysis. Deep learning (DL) approaches have achieved state-of-the-art\nsegmentation perfor- mance. To exploit the 3D contexts using neural networks,\nknown DL segmentation methods, including 3D convolution, 2D convolution on\nplanes orthogonal to 2D image slices, and LSTM in multiple directions, all\nsuffer incompatibility with the highly anisotropic dimensions in common 3D\nbiomedical images. In this paper, we propose a new DL framework for 3D image\nsegmentation, based on a com- bination of a fully convolutional network (FCN)\nand a recurrent neural network (RNN), which are responsible for exploiting the\nintra-slice and inter-slice contexts, respectively. To our best knowledge, this\nis the first DL framework for 3D image segmentation that explicitly leverages\n3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal\nStructure Segmentation Challenge and in-house image stacks for 3D fungus\nsegmentation, our approach achieves promising results comparing to the known\nDL-based 3D segmentation approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 00:33:26 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 13:35:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Jianxu", ""], ["Yang", "Lin", ""], ["Zhang", "Yizhe", ""], ["Alber", "Mark", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1609.01044", "submitter": "Janne V. Kujala", "authors": "Janne V. Kujala, Tuomas J. Lukka, Harri Holopainen", "title": "Classifying and sorting cluttered piles of unknown objects with robots:\n  a learning approach", "comments": "8 pages, 14 figures (pagination changed in arXiv version); accepted\n  for the International Conference on Intelligent Robots and Systems (IROS)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sorting a densely cluttered pile of unknown\nobjects using a robot. This yet unsolved problem is relevant in the robotic\nwaste sorting business.\n  By extending previous active learning approaches to grasping, we show a\nsystem that learns the task autonomously. Instead of predicting just whether a\ngrasp succeeds, we predict the classes of the objects that end up being picked\nand thrown onto the target conveyor. Segmenting and identifying objects from\nthe uncluttered target conveyor, as opposed to the working area, is easier due\nto the added structure since the thrown objects will be the only ones present.\n  Instead of trying to segment or otherwise understand the cluttered working\narea in any way, we simply allow the controller to learn a mapping from an RGBD\nimage in the neighborhood of the grasp to a predicted result---all segmentation\netc. in the working area is implicit in the learned function. The grasp\nselection operates in two stages: The first stage is hardcoded and outputs a\ndistribution of possible grasps that sometimes succeed. The second stage uses a\npurely learned criterion to choose the grasp to make from the proposal\ndistribution created by the first stage.\n  In an experiment, the system quickly learned to make good pickups and predict\ncorrectly, in advance, which class of object it was going to pick up and was\nable to sort the objects from a densely cluttered pile by color.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 07:44:40 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Kujala", "Janne V.", ""], ["Lukka", "Tuomas J.", ""], ["Holopainen", "Harri", ""]]}, {"id": "1609.01064", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra and Rita Cucchiara", "title": "A Deep Multi-Level Network for Saliency Prediction", "comments": "International Conference on Pattern Recognition (ICPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep architecture for saliency prediction.\nCurrent state of the art models for saliency prediction employ Fully\nConvolutional networks that perform a non-linear combination of features\nextracted from the last convolutional layer to predict saliency maps. We\npropose an architecture which, instead, combines features extracted at\ndifferent levels of a Convolutional Neural Network (CNN). Our model is composed\nof three main blocks: a feature extraction CNN, a feature encoding network,\nthat weights low and high level feature maps, and a prior learning network. We\ncompare our solution with state of the art saliency models on two public\nbenchmarks datasets. Results show that our model outperforms under all\nevaluation metrics on the SALICON dataset, which is currently the largest\npublic dataset for saliency prediction, and achieves competitive results on the\nMIT300 benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 08:48:01 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 10:04:18 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Serra", "Giuseppe", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1609.01100", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud and Yoel Shkolnisky", "title": "A max-cut approach to heterogeneity in cryo-electron microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of cryo-electron microscopy has made astounding advancements in the\npast few years, mainly due to advancements in electron detectors' technology.\nYet, one of the key open challenges of the field remains the processing of\nheterogeneous data sets, produced from samples containing particles at several\ndifferent conformational states. For such data sets, the algorithms must\ninclude some classification procedure to identify homogeneous groups within the\ndata, so that the images in each group correspond to the same underlying\nstructure. The fundamental importance of the heterogeneity problem in\ncryo-electron microscopy has drawn many research efforts, and resulted in\nsignificant progress in classification algorithms for heterogeneous data sets.\nWhile these algorithms are extremely useful and effective in practice, they\nlack rigorous mathematical analysis and performance guarantees.\n  In this paper, we attempt to make the first steps towards rigorous\nmathematical analysis of the heterogeneity problem in cryo-electron microscopy.\nTo that end, we present an algorithm for processing heterogeneous data sets,\nand prove accuracy and stability bounds for it. We also suggest an extension of\nthis algorithm that combines the classification and reconstruction steps. We\ndemonstrate it on simulated data, and compare its performance to the\nstate-of-the-art algorithm in RELION.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 11:08:34 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 14:48:05 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1609.01103", "submitter": "Kevis-Kokitsi Maninis", "authors": "Kevis-Kokitsi Maninis and Jordi Pont-Tuset and Pablo Arbel\\'aez and\n  Luc Van Gool", "title": "Deep Retinal Image Understanding", "comments": "MICCAI 2016 Camera Ready", "journal-ref": null, "doi": "10.1007/978-3-319-46723-8_17", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Deep Retinal Image Understanding (DRIU), a unified\nframework of retinal image analysis that provides both retinal vessel and optic\ndisc segmentation. We make use of deep Convolutional Neural Networks (CNNs),\nwhich have proven revolutionary in other fields of computer vision such as\nobject detection and image classification, and we bring their power to the\nstudy of eye fundus images. DRIU uses a base network architecture on which two\nset of specialized layers are trained to solve both the retinal vessel and\noptic disc segmentation. We present experimental validation, both qualitative\nand quantitative, in four public datasets for these tasks. In all of them, DRIU\npresents super-human performance, that is, it shows results more consistent\nwith a gold standard than a second human annotator used as control.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 11:20:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Pont-Tuset", "Jordi", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Van Gool", "Luc", ""]]}, {"id": "1609.01117", "submitter": "Kieran Larkin", "authors": "Kieran G. Larkin", "title": "Reflections on Shannon Information: In search of a natural\n  information-entropy for images", "comments": "47 pages,9 figures, preprint for submission to an image science and\n  optics journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not obvious how to extend Shannon's original information entropy to\nhigher dimensions, and many different approaches have been tried. We replace\nthe English text symbol sequence originally used to illustrate the theory by a\ndiscrete, bandlimited signal. Using Shannon's later theory of sampling we\nderive a new and symmetric version of the second order entropy in 1D. The new\ntheory then naturally extends to 2D and higher dimensions, where by naturally\nwe mean simple, symmetric, isotropic and parsimonious. Simplicity arises from\nthe direct application of Shannon's joint entropy equalities and inequalities\nto the gradient (del) vector field image embodying the second order relations\nof the scalar image. Parsimony is guaranteed by halving of the vector data rate\nusing Papoulis' generalized sampling expansion. The new 2D entropy measure,\nwhich we dub delentropy, is underpinned by a computable probability density\nfunction we call deldensity. The deldensity captures the underlying spatial\nimage structure and pixel co-occurrence. It achieves this because each scalar\nimage pixel value is nonlocally related to the entire gradient vector field.\nBoth deldensity and delentropy are highly tractable and yield many interesting\nconnections and useful inequalities. The new measure explicitly defines a\nrealizable encoding algorithm and a corresponding reconstruction. Initial tests\nshow that delentropy compares favourably with the conventional intensity-based\nhistogram entropy and the compressed data rates of lossless image encoders\n(GIF, PNG, WEBP, JP2K-LS and JPG-LS) for a selection of images. The symmetric\napproach may have applications to higher dimensions and problems concerning\nimage complexity measures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 11:59:47 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Larkin", "Kieran G.", ""]]}, {"id": "1609.01228", "submitter": "Michel Fornaciali", "authors": "Afonso Menegola, Michel Fornaciali, Ramon Pires, Sandra Avila, Eduardo\n  Valle", "title": "Towards Automated Melanoma Screening: Exploring Transfer Learning\n  Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is the current bet for image classification. Its greed for huge\namounts of annotated data limits its usage in medical imaging context. In this\nscenario transfer learning appears as a prominent solution. In this report we\naim to clarify how transfer learning schemes may influence classification\nresults. We are particularly focused in the automated melanoma screening\nproblem, a case of medical imaging in which transfer learning is still not\nwidely used. We explored transfer with and without fine-tuning, sequential\ntransfers and usage of pre-trained models in general and specific datasets.\nAlthough some issues remain open, our findings may drive future researches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 17:31:15 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Menegola", "Afonso", ""], ["Fornaciali", "Michel", ""], ["Pires", "Ramon", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1609.01326", "submitter": "Weichao Qiu", "authors": "Weichao Qiu, Alan Yuille", "title": "UnrealCV: Connecting Computer Vision to Unreal Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer graphics can not only generate synthetic images and ground truth but\nit also offers the possibility of constructing virtual worlds in which: (i) an\nagent can perceive, navigate, and take actions guided by AI algorithms, (ii)\nproperties of the worlds can be modified (e.g., material and reflectance),\n(iii) physical simulations can be performed, and (iv) algorithms can be learnt\nand evaluated. But creating realistic virtual worlds is not easy. The game\nindustry, however, has spent a lot of effort creating 3D worlds, which a player\ncan interact with. So researchers can build on these resources to create\nvirtual worlds, provided we can access and modify the internal data structures\nof the games. To enable this we created an open-source plugin UnrealCV\n(http://unrealcv.github.io) for a popular game engine Unreal Engine 4 (UE4). We\nshow two applications: (i) a proof of concept image dataset, and (ii) linking\nCaffe with the virtual world to test deep network algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 21:09:33 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Qiu", "Weichao", ""], ["Yuille", "Alan", ""]]}, {"id": "1609.01329", "submitter": "Saad Nadeem", "authors": "Saad Nadeem and Arie Kaufman", "title": "Depth Reconstruction and Computer-Aided Polyp Detection in Optical\n  Colonoscopy Video Frames", "comments": "**The title has been modified to highlight the contributions more\n  clearly. The original title is: \"Computer-Aided Detection of Polyps in\n  Optical Colonoscopy Images\". Keywords: Machine learning, computer-aided\n  detection, segmentation, endoscopy, colonoscopy, videos, polyp, detection,\n  medical imaging, depth maps, 3D, reconstruction, computed tomography, virtual\n  colonoscopy, colorectal cancer, SPIE Medical Imaging, 2016", "journal-ref": null, "doi": "10.1117/12.2216996", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computer-aided detection algorithm for polyps in optical\ncolonoscopy images. Polyps are the precursors to colon cancer. In the US alone,\nmore than 14 million optical colonoscopies are performed every year, mostly to\nscreen for polyps. Optical colonoscopy has been shown to have an approximately\n25% polyp miss rate due to the convoluted folds and bends present in the colon.\nIn this work, we present an automatic detection algorithm to detect these\npolyps in the optical colonoscopy images. We use a machine learning algorithm\nto infer a depth map for a given optical colonoscopy image and then use a\ndetailed pre-built polyp profile to detect and delineate the boundaries of\npolyps in this given image. We have achieved the best recall of 84.0% and the\nbest specificity value of 83.4%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 21:12:34 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 16:06:39 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Nadeem", "Saad", ""], ["Kaufman", "Arie", ""]]}, {"id": "1609.01344", "submitter": "Ghassem Tofighi", "authors": "Ghassem Tofighi, Kaamraan Raahemifar, Maria Frank, Haisong Gu", "title": "Vision-based Engagement Detection in Virtual Reality", "comments": "Paper has been published in Digital Media Industry and Academic Forum\n  2016 (DMIAF 2016) http://ieee-digitalmediaforum.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  User engagement modeling for manipulating actions in vision-based interfaces\nis one of the most important case studies of user mental state detection. In a\nVirtual Reality environment that employs camera sensors to recognize human\nactivities, we have to know when user intends to perform an action and when\nnot. Without a proper algorithm for recognizing engagement status, any kind of\nactivities could be interpreted as manipulating actions, called \"Midas Touch\"\nproblem. Baseline approach for solving this problem is activating gesture\nrecognition system using some focus gestures such as waiving or raising hand.\nHowever, a desirable natural user interface should be able to understand user's\nmental status automatically. In this paper, a novel multi-modal model for\nengagement detection, DAIA, is presented. using DAIA, the spectrum of mental\nstatus for performing an action is quantized in a finite number of engagement\nstates. For this purpose, a Finite State Transducer (FST) is designed. This\nengagement framework shows how to integrate multi-modal information from user\nbiometric data streams such as 2D and 3D imaging. FST is employed to make the\nstate transition smoothly using combination of several boolean expressions. Our\nFST true detection rate is 92.3% in total for four different states. Results\nalso show FST can segment user hand gestures more robustly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 22:24:08 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Tofighi", "Ghassem", ""], ["Raahemifar", "Kaamraan", ""], ["Frank", "Maria", ""], ["Gu", "Haisong", ""]]}, {"id": "1609.01345", "submitter": "Andr\\'as B\\'odis-Szomor\\'u", "authors": "Andr\\'as B\\'odis-Szomor\\'u, Hayko Riemenschneider, Luc Van Gool", "title": "Efficient Volumetric Fusion of Airborne and Street-Side Data for Urban\n  Reconstruction", "comments": "To appear in ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airborne acquisition and on-road mobile mapping provide complementary 3D\ninformation of an urban landscape: the former acquires roof structures, ground,\nand vegetation at a large scale, but lacks the facade and street-side details,\nwhile the latter is incomplete for higher floors and often totally misses out\non pedestrian-only areas or undriven districts. In this work, we introduce an\napproach that efficiently unifies a detailed street-side Structure-from-Motion\n(SfM) or Multi-View Stereo (MVS) point cloud and a coarser but more complete\npoint cloud from airborne acquisition in a joint surface mesh. We propose a\npoint cloud blending and a volumetric fusion based on ray casting across a 3D\ntetrahedralization (3DT), extended with data reduction techniques to handle\nlarge datasets. To the best of our knowledge, we are the first to adopt a 3DT\napproach for airborne/street-side data fusion. Our pipeline exploits typical\ncharacteristics of airborne and ground data, and produces a seamless,\nwatertight mesh that is both complete and detailed. Experiments on 3D urban\ndata from multiple sources and different data densities show the effectiveness\nand benefits of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 22:28:49 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["B\u00f3dis-Szomor\u00fa", "Andr\u00e1s", ""], ["Riemenschneider", "Hayko", ""], ["Van Gool", "Luc", ""]]}, {"id": "1609.01360", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee and Alexander Wong", "title": "Evolutionary Synthesis of Deep Neural Networks via Synaptic\n  Cluster-driven Genetic Encoding", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest towards achieving highly efficient\ndeep neural network architectures. A promising paradigm for achieving this is\nthe concept of evolutionary deep intelligence, which attempts to mimic\nbiological evolution processes to synthesize highly-efficient deep neural\nnetworks over successive generations. An important aspect of evolutionary deep\nintelligence is the genetic encoding scheme used to mimic heredity, which can\nhave a significant impact on the quality of offspring deep neural networks.\nMotivated by the neurobiological phenomenon of synaptic clustering, we\nintroduce a new genetic encoding scheme where synaptic probability is driven\ntowards the formation of a highly sparse set of synaptic clusters. Experimental\nresults for the task of image classification demonstrated that the synthesized\noffspring networks using this synaptic cluster-driven genetic encoding scheme\ncan achieve state-of-the-art performance while having network architectures\nthat are not only significantly more efficient (with a ~125-fold decrease in\nsynapses for MNIST) compared to the original ancestor network, but also\ntailored for GPU-accelerated machine learning applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 01:08:03 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 16:00:01 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1609.01366", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Ke Sun, Linlin Shen, Guoping Qiu", "title": "Object Specific Deep Learning Feature and Its Application to Face\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for discovering and exploiting object specific deep\nlearning features and use face detection as a case study. Motivated by the\nobservation that certain convolutional channels of a Convolutional Neural\nNetwork (CNN) exhibit object specific responses, we seek to discover and\nexploit the convolutional channels of a CNN in which neurons are activated by\nthe presence of specific objects in the input image. A method for explicitly\nfine-tuning a pre-trained CNN to induce an object specific channel (OSC) and\nsystematically identifying it for the human face object has been developed.\nBased on the basic OSC features, we introduce a multi-resolution approach to\nconstructing robust face heatmaps for fast face detection in unconstrained\nsettings. We show that multi-resolution OSC can be used to develop state of the\nart face detectors which have the advantage of being simple and compact.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 01:35:13 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Hou", "Xianxu", ""], ["Sun", "Ke", ""], ["Shen", "Linlin", ""], ["Qiu", "Guoping", ""]]}, {"id": "1609.01371", "submitter": "Dimitrios Tzionas", "authors": "Dimitrios Tzionas and Juergen Gall", "title": "Reconstructing Articulated Rigged Models from RGB-D Videos", "comments": "Accepted for publication - European Conference on Computer Vision\n  Workshops 2016 (ECCVW'16) - Workshop on Recovering 6D Object Pose (R6D'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although commercial and open-source software exist to reconstruct a static\nobject from a sequence recorded with an RGB-D sensor, there is a lack of tools\nthat build rigged models of articulated objects that deform realistically and\ncan be used for tracking or animation. In this work, we fill this gap and\npropose a method that creates a fully rigged model of an articulated object\nfrom depth data of a single sensor. To this end, we combine deformable mesh\ntracking, motion segmentation based on spectral clustering and skeletonization\nbased on mean curvature flow. The fully rigged model then consists of a\nwatertight mesh, embedded skeleton, and skinning weights.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 02:10:21 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 15:20:38 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Tzionas", "Dimitrios", ""], ["Gall", "Juergen", ""]]}, {"id": "1609.01380", "submitter": "Hang Yang", "authors": "Hang Yang, Zhongbo Zhang and Yujing Guan", "title": "An Adaptive Parameter Estimation for Guided Filter based Image\n  Deconvolution", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deconvolution is still to be a challenging ill-posed problem for\nrecovering a clear image from a given blurry image, when the point spread\nfunction is known. Although competitive deconvolution methods are numerically\nimpressive and approach theoretical limits, they are becoming more complex,\nmaking analysis, and implementation difficult. Furthermore, accurate estimation\nof the regularization parameter is not easy for successfully solving image\ndeconvolution problems. In this paper, we develop an effective approach for\nimage restoration based on one explicit image filter - guided filter. By\napplying the decouple of denoising and deblurring techniques to the\ndeconvolution model, we reduce the optimization complexity and achieve a simple\nbut effective algorithm to automatically compute the parameter in each\niteration, which is based on Morozov's discrepancy principle. Experimental\nresults demonstrate that the proposed algorithm outperforms many\nstate-of-the-art deconvolution methods in terms of both ISNR and visual\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 03:43:32 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Yang", "Hang", ""], ["Zhang", "Zhongbo", ""], ["Guan", "Yujing", ""]]}, {"id": "1609.01414", "submitter": "Vinay Kumar N", "authors": "N. Vinay Kumar, Pratheek, V. Vijaya Kantha, K. N. Govindaraju, and D.\n  S. Guru", "title": "Features Fusion for Classification of Logos", "comments": "10 pages, 5 figures, 9 tables", "journal-ref": "Procedia Computer Science, Volume 85, 2016, Pages 370-379", "doi": "10.1016/j.procs.2016.05.245", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a logo classification system based on the appearance of logo\nimages is proposed. The proposed classification system makes use of global\ncharacteristics of logo images for classification. Color, texture, and shape of\na logo wholly describe the global characteristics of logo images. The various\ncombinations of these characteristics are used for classification. The\ncombination contains only with single feature or with fusion of two features or\nfusion of all three features considered at a time respectively. Further, the\nsystem categorizes the logo image into: a logo image with fully text or with\nfully symbols or containing both symbols and texts.. The K-Nearest Neighbour\n(K-NN) classifier is used for classification. Due to the lack of color logo\nimage dataset in the literature, the same is created consisting 5044 color logo\nimages. Finally, the performance of the classification system is evaluated\nthrough accuracy, precision, recall and F-measure computed from the confusion\nmatrix. The experimental results show that the most promising results are\nobtained for fusion of features.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 07:29:56 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Kumar", "N. Vinay", ""], ["Pratheek", "", ""], ["Kantha", "V. Vijaya", ""], ["Govindaraju", "K. N.", ""], ["Guru", "D. S.", ""]]}, {"id": "1609.01461", "submitter": "Battista Biggio", "authors": "Battista Biggio and Giorgio Fumera and Gian Luca Marcialis and Fabio\n  Roli", "title": "Statistical Meta-Analysis of Presentation Attacks for Secure\n  Multibiometric Systems", "comments": "Published in: IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2016", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2016", "doi": "10.1109/TPAMI.2016.2558154", "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has shown that multibiometric systems are vulnerable to\npresentation attacks, assuming that their matching score distribution is\nidentical to that of genuine users, without fabricating any fake trait. We have\nrecently shown that this assumption is not representative of current\nfingerprint and face presentation attacks, leading one to overestimate the\nvulnerability of multibiometric systems, and to design less effective fusion\nrules. In this paper, we overcome these limitations by proposing a statistical\nmeta-model of face and fingerprint presentation attacks that characterizes a\nwider family of fake score distributions, including distributions of known and,\npotentially, unknown attacks. This allows us to perform a thorough security\nevaluation of multibiometric systems against presentation attacks, quantifying\nhow their vulnerability may vary also under attacks that are different from\nthose considered during design, through an uncertainty analysis. We empirically\nshow that our approach can reliably predict the performance of multibiometric\nsystems even under never-before-seen face and fingerprint presentation attacks,\nand that the secure fusion rules designed using our approach can exhibit an\nimproved trade-off between the performance in the absence and in the presence\nof attack. We finally argue that our method can be extended to other biometrics\nbesides faces and fingerprints.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 09:44:47 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Biggio", "Battista", ""], ["Fumera", "Giorgio", ""], ["Marcialis", "Gian Luca", ""], ["Roli", "Fabio", ""]]}, {"id": "1609.01465", "submitter": "Adria Ruiz", "authors": "Adria Ruiz, Ognjen Rudovic, Xavier Binefa and Maja Pantic", "title": "Multi-instance Dynamic Ordinal Random Fields for Weakly-Supervised Pain\n  Intensity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the Multi-Instance-Learning (MIL) problem when bag\nlabels are naturally represented as ordinal variables (Multi--Instance--Ordinal\nRegression). Moreover, we consider the case where bags are temporal sequences\nof ordinal instances. To model this, we propose the novel Multi-Instance\nDynamic Ordinal Random Fields (MI-DORF). In this model, we treat\ninstance-labels inside the bag as latent ordinal states. The MIL assumption is\nmodelled by incorporating a high-order cardinality potential relating bag and\ninstance-labels,into the energy function. We show the benefits of the proposed\napproach on the task of weakly-supervised pain intensity estimation from the\nUNBC Shoulder-Pain Database. In our experiments, the proposed approach\nsignificantly outperforms alternative non-ordinal methods that either ignore\nthe MIL assumption, or do not model dynamic information in target data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:00:26 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Ruiz", "Adria", ""], ["Rudovic", "Ognjen", ""], ["Binefa", "Xavier", ""], ["Pantic", "Maja", ""]]}, {"id": "1609.01466", "submitter": "Radu Horaud P", "authors": "Georgios Evangelidis and Radu Horaud", "title": "Joint Alignment of Multiple Point Sets with Batch and Incremental\n  Expectation-Maximization", "comments": "14 pages, 12 figures, 5 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  40(6), 1397 - 1410, 2018", "doi": "10.1109/TPAMI.2017.2717829", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of registering multiple point sets.\nSolutions to this problem are often approximated by repeatedly solving for\npairwise registration, which results in an uneven treatment of the sets forming\na pair: a model set and a data set. The main drawback of this strategy is that\nthe model set may contain noise and outliers, which negatively affects the\nestimation of the registration parameters. In contrast, the proposed\nformulation treats all the point sets on an equal footing. Indeed, all the\npoints are drawn from a central Gaussian mixture, hence the registration is\ncast into a clustering problem. We formally derive batch and incremental EM\nalgorithms that robustly estimate both the GMM parameters and the rotations and\ntranslations that optimally align the sets. Moreover, the mixture's means play\nthe role of the registered set of points while the variances provide rich\ninformation about the contribution of each component to the alignment. We\nthoroughly test the proposed algorithms on simulated data and on challenging\nreal data collected with range sensors. We compare them with several\nstate-of-the-art algorithms, and we show their potential for surface\nreconstruction from depth data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:00:46 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 10:47:50 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Evangelidis", "Georgios", ""], ["Horaud", "Radu", ""]]}, {"id": "1609.01499", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Rolf K\\\"ohler and Bernhard Sch\\\"olkopf and\n  Michael Hirsch", "title": "Depth Estimation Through a Generative Model of Light Field Synthesis", "comments": "German Conference on Pattern Recognition (GCPR) 2016", "journal-ref": null, "doi": "10.1007/978-3-319-45886-1_35", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field photography captures rich structural information that may\nfacilitate a number of traditional image processing and computer vision tasks.\nA crucial ingredient in such endeavors is accurate depth recovery. We present a\nnovel framework that allows the recovery of a high quality continuous depth map\nfrom light field data. To this end we propose a generative model of a light\nfield that is fully parametrized by its corresponding depth map. The model\nallows for the integration of powerful regularization techniques such as a\nnon-local means prior, facilitating accurate depth map estimation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 11:43:08 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["K\u00f6hler", "Rolf", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1609.01524", "submitter": "Thomas K\\\"ohler", "authors": "Cosmin Bercea, Andreas Maier, Thomas K\\\"ohler", "title": "Confidence-aware Levenberg-Marquardt optimization for joint motion\n  estimation and super-resolution", "comments": "accepted for ICIP 2016", "journal-ref": "2016 IEEE International Conference on Image Processing (ICIP)", "doi": "10.1109/ICIP.2016.7532535", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion estimation across low-resolution frames and the reconstruction of\nhigh-resolution images are two coupled subproblems of multi-frame\nsuper-resolution. This paper introduces a new joint optimization approach for\nmotion estimation and image reconstruction to address this interdependence. Our\nmethod is formulated via non-linear least squares optimization and combines two\nprinciples of robust super-resolution. First, to enhance the robustness of the\njoint estimation, we propose a confidence-aware energy minimization framework\naugmented with sparse regularization. Second, we develop a tailor-made\nLevenberg-Marquardt iteration scheme to jointly estimate motion parameters and\nthe high-resolution image along with the corresponding model confidence\nparameters. Our experiments on simulated and real images confirm that the\nproposed approach outperforms decoupled motion estimation and image\nreconstruction as well as related state-of-the-art joint estimation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 12:54:04 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Bercea", "Cosmin", ""], ["Maier", "Andreas", ""], ["K\u00f6hler", "Thomas", ""]]}, {"id": "1609.01571", "submitter": "Shaul Oron", "authors": "Shaul Oron, Tali Dekel, Tianfan Xue, William T. Freeman, Shai Avidan", "title": "Best-Buddies Similarity - Robust Template Matching using Mutual Nearest\n  Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for template matching in unconstrained\nenvironments. Its essence is the Best-Buddies Similarity (BBS), a useful,\nrobust, and parameter-free similarity measure between two sets of points. BBS\nis based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points\nin source and target sets, where each point is the nearest neighbor of the\nother. BBS has several key features that make it robust against complex\ngeometric deformations and high levels of outliers, such as those arising from\nbackground clutter and occlusions. We study these properties, provide a\nstatistical analysis that justifies them, and demonstrate the consistent\nsuccess of BBS on a challenging real-world dataset while using different types\nof features.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 14:24:36 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Oron", "Shaul", ""], ["Dekel", "Tali", ""], ["Xue", "Tianfan", ""], ["Freeman", "William T.", ""], ["Avidan", "Shai", ""]]}, {"id": "1609.01625", "submitter": "Haroldo Ribeiro", "authors": "Luciano Zunino, Haroldo V. Ribeiro", "title": "Discriminating image textures with the multiscale two-dimensional\n  complexity-entropy causality plane", "comments": "Accepted for publication in Chaos, Solitons & Fractals", "journal-ref": "Chaos, Solitons & Fractals 91, 679-688 (2017(", "doi": "10.1016/j.chaos.2016.09.005", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to further explore the usefulness of the\ntwo-dimensional complexity-entropy causality plane as a texture image\ndescriptor. A multiscale generalization is introduced in order to distinguish\nbetween different roughness features of images at small and large spatial\nscales. Numerically generated two-dimensional structures are initially\nconsidered for illustrating basic concepts in a controlled framework. Then,\nmore realistic situations are studied. Obtained results allow us to confirm\nthat intrinsic spatial correlations of images are successfully unveiled by\nimplementing this multiscale symbolic information-theory approach.\nConsequently, we conclude that the proposed representation space is a versatile\nand practical tool for identifying, characterizing and discriminating image\ntextures.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 15:57:16 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 16:44:47 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Zunino", "Luciano", ""], ["Ribeiro", "Haroldo V.", ""]]}, {"id": "1609.01648", "submitter": "Luca Ghiani", "authors": "Luca Ghiani, David A. Yambay, Valerio Mura, Gian Luca Marcialis, Fabio\n  Roli and Stephanie A. Schuckers", "title": "Review of the Fingerprint Liveness Detection (LivDet) competition\n  series: 2009 to 2015", "comments": null, "journal-ref": null, "doi": "10.1016/j.imavis.2016.07.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spoof attack, a subset of presentation attacks, is the use of an artificial\nreplica of a biometric in an attempt to circumvent a biometric sensor. Liveness\ndetection, or presentation attack detection, distinguishes between live and\nfake biometric traits and is based on the principle that additional information\ncan be garnered above and beyond the data procured by a standard authentication\nsystem to determine if a biometric measure is authentic. The goals for the\nLiveness Detection (LivDet) competitions are to compare software-based\nfingerprint liveness detection and artifact detection algorithms (Part 1), as\nwell as fingerprint systems which incorporate liveness detection or artifact\ndetection capabilities (Part 2), using a standardized testing protocol and\nlarge quantities of spoof and live tests. The competitions are open to all\nacademic and industrial institutions which have a solution for either\nsoftwarebased or system-based fingerprint liveness detection. The LivDet\ncompetitions have been hosted in 2009, 2011, 2013 and 2015 and have shown\nthemselves to provide a crucial look at the current state of the art in\nliveness detection schemes. There has been a noticeable increase in the number\nof participants in LivDet competitions as well as a noticeable decrease in\nerror rates across competitions. Participants have grown from four to the most\nrecent thirteen submissions for Fingerprint Part 1. Fingerprints Part 2 has\nheld steady at two submissions each competition in 2011 and 2013 and only one\nfor the 2015 edition. The continuous increase of competitors demonstrates a\ngrowing interest in the topic.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 16:39:37 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Ghiani", "Luca", ""], ["Yambay", "David A.", ""], ["Mura", "Valerio", ""], ["Marcialis", "Gian Luca", ""], ["Roli", "Fabio", ""], ["Schuckers", "Stephanie A.", ""]]}, {"id": "1609.01670", "submitter": "Shaobing Gao", "authors": "Shao-Bing Gao, Ming Zhang, Chao-Yi Li, and Yong-Jie Li", "title": "Improving Color Constancy by Discounting the Variation of Camera\n  Spectral Sensitivity", "comments": "This work was finished in June 2014 and then submitted to IEEE TIP in\n  September 21,2015 and in April 18, 2016", "journal-ref": null, "doi": "10.1364/JOSAA.34.001448", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an ill-posed problem to recover the true scene colors from a color\nbiased image by discounting the effects of scene illuminant and camera spectral\nsensitivity (CSS) at the same time. Most color constancy (CC) models have been\ndesigned to first estimate the illuminant color, which is then removed from the\ncolor biased image to obtain an image taken under white light, without the\nexplicit consideration of CSS effect on CC. This paper first studies the CSS\neffect on illuminant estimation arising in the inter-dataset-based CC\n(inter-CC), i.e., training a CC model on one dataset and then testing on\nanother dataset captured by a distinct CSS. We show the clear degradation of\nexisting CC models for inter-CC application. Then a simple way is proposed to\novercome such degradation by first learning quickly a transform matrix between\nthe two distinct CSSs (CSS-1 and CSS-2). The learned matrix is then used to\nconvert the data (including the illuminant ground truth and the color biased\nimages) rendered under CSS-1 into CSS-2, and then train and apply the CC model\non the color biased images under CSS-2, without the need of burdensome\nacquiring of training set under CSS-2. Extensive experiments on synthetic and\nreal images show that our method can clearly improve the inter-CC performance\nfor traditional CC algorithms. We suggest that by taking the CSS effect into\naccount, it is more likely to obtain the truly color constant images invariant\nto the changes of both illuminant and camera sensors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 17:50:23 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 15:33:30 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Gao", "Shao-Bing", ""], ["Zhang", "Ming", ""], ["Li", "Chao-Yi", ""], ["Li", "Yong-Jie", ""]]}, {"id": "1609.01693", "submitter": "Silvia-Laura Pintea", "authors": "S. L. Pintea and J. C. van Gemert", "title": "Making a Case for Learning Motion Representations with Phase", "comments": "ECCV 2016 Workshop on Brave new ideas for motion representations in\n  videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work advocates Eulerian motion representation learning over the current\nstandard Lagrangian optical flow model. Eulerian motion is well captured by\nusing phase, as obtained by decomposing the image through a complex-steerable\npyramid. We discuss the gain of Eulerian motion in a set of practical use\ncases: (i) action recognition, (ii) motion prediction in static images, (iii)\nmotion transfer in static images and, (iv) motion transfer in video. For each\ntask we motivate the phase-based direction and provide a possible approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 18:47:40 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 11:59:03 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Pintea", "S. L.", ""], ["van Gemert", "J. C.", ""]]}, {"id": "1609.01710", "submitter": "Kardi Teknomo", "authors": "Saman Saadat and Kardi Teknomo", "title": "Automation of Pedestrian Tracking in a Crowded Situation", "comments": "10 Pages, Saadat, S., and Teknomo, K., Automation of Pedestrian\n  Tracking in a Crowded Situation, the Fifth International Conference on\n  Pedestrian and Evacuation Dynamics, March 8-10, 2010, National Institute of\n  Standards and Technology, Gaithersburg, MD USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on microscopic pedestrian requires large amounts of trajectory data\nfrom real-world pedestrian crowds. Such data collection, if done manually,\nneeds tremendous effort and is very time consuming. Though many studies have\nasserted the possibility of automating this task using video cameras, we found\nthat only a few have demonstrated good performance in very crowded situations\nor from a top-angled view scene. This paper deals with tracking pedestrian\ncrowd under heavy occlusions from an angular scene. Our automated tracking\nsystem consists of two modules that perform sequentially. The first module\ndetects moving objects as blobs. The second module is a tracking system. We\nemploy probability distribution from the detection of each pedestrian and use\nBayesian update to track the next position. The result of such tracking is a\ndatabase of pedestrian trajectories over time and space. With certain prior\ninformation, we showed that the system can track a large number of people under\nocclusion and clutter scene.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:36:23 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Saadat", "Saman", ""], ["Teknomo", "Kardi", ""]]}, {"id": "1609.01743", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Human pose estimation via Convolutional Part Heatmap Regression", "comments": "accepted to ECCV 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46478-7_44", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on human pose estimation using Convolutional Neural Networks.\nOur main contribution is a CNN cascaded architecture specifically designed for\nlearning part relationships and spatial context, and robustly inferring pose\neven for the case of severe part occlusions. To this end, we propose a\ndetection-followed-by-regression CNN cascade. The first part of our cascade\noutputs part detection heatmaps and the second part performs regression on\nthese heatmaps. The benefits of the proposed architecture are multi-fold: It\nguides the network where to focus in the image and effectively encodes part\nconstraints and context. More importantly, it can effectively cope with\nocclusions because part detection heatmaps for occluded parts provide low\nconfidence scores which subsequently guide the regression part of our network\nto rely on contextual information in order to predict the location of these\nparts. Additionally, we show that the proposed cascade is flexible enough to\nreadily allow the integration of various CNN architectures for both detection\nand regression, including recent ones based on residual learning. Finally, we\nillustrate that our cascade achieves top performance on the MPII and LSP data\nsets. Code can be downloaded from http://www.cs.nott.ac.uk/~psxab5/\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 20:25:30 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1609.01775", "submitter": "Ergys Ristani", "authors": "Ergys Ristani, Francesco Solera, Roger S. Zou, Rita Cucchiara, Carlo\n  Tomasi", "title": "Performance Measures and a Data Set for Multi-Target, Multi-Camera\n  Tracking", "comments": "ECCV 2016 Workshop on Benchmarking Multi-Target Tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To help accelerate progress in multi-target, multi-camera tracking systems,\nwe present (i) a new pair of precision-recall measures of performance that\ntreats errors of all types uniformly and emphasizes correct identification over\nsources of error; (ii) the largest fully-annotated and calibrated data set to\ndate with more than 2 million frames of 1080p, 60fps video taken by 8 cameras\nobserving more than 2,700 identities over 85 minutes; and (iii) a reference\nsoftware system as a comparison baseline. We show that (i) our measures\nproperly account for bottom-line identity match performance in the multi-camera\nsetting; (ii) our data set poses realistic challenges to current trackers; and\n(iii) the performance of our system is comparable to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 21:58:25 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 17:27:19 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ristani", "Ergys", ""], ["Solera", "Francesco", ""], ["Zou", "Roger S.", ""], ["Cucchiara", "Rita", ""], ["Tomasi", "Carlo", ""]]}, {"id": "1609.01805", "submitter": "Da Zhou Dr.", "authors": "Shanjun Mao, Da Zhou, Yiping Zhang, Zhihong Zhang, Jingjing Cao", "title": "A Boosting Method to Face Image Super-resolution", "comments": "14 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently sparse representation has gained great success in face image\nsuper-resolution. The conventional sparsity-based methods enforce sparse coding\non face image patches and the representation fidelity is measured by\n$\\ell_{2}$-norm. Such a sparse coding model regularizes all facial patches\nequally, which however ignores distinct natures of different facial patches for\nimage reconstruction. In this paper, we propose a new weighted-patch\nsuper-resolution method based on AdaBoost. Specifically, in each iteration of\nthe AdaBoost operation, each facial patch is weighted automatically according\nto the performance of the model on it, so as to highlight those patches that\nare more critical for improving the reconstruction power in next step. In this\nway, through the AdaBoost training procedure, we can focus more on the patches\n(face regions) with richer information. Various experimental results on\nstandard face database show that our proposed method outperforms\nstate-of-the-art methods in terms of both objective metrics and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 02:12:30 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 02:52:42 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 09:12:13 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Mao", "Shanjun", ""], ["Zhou", "Da", ""], ["Zhang", "Yiping", ""], ["Zhang", "Zhihong", ""], ["Cao", "Jingjing", ""]]}, {"id": "1609.01810", "submitter": "Kardi Teknomo", "authors": "Kardi Teknomo, Yasushi Takeyama, Hajime Inamura", "title": "Tracking System to Automate Data Collection of Microscopic Pedestrian\n  Traffic Flow", "comments": "15 pages, Teknomo, Kardi; Takeyama, Yasushi; Inamura, Hajime,\n  Tracking System to Automate Data Collection of Microscopic Pedestrian Traffic\n  Flow, Proceeding of The 4th Eastern Asia Society For Transportation Studies,\n  Hanoi, Vietnam, pp. 11-25, Oct. 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deal with many pedestrian data, automatic data collection is needed. This\npaper describes how to automate the microscopic pedestrian flow data collection\nfrom video files. The study is restricted only to pedestrians without\nconsidering vehicular - pedestrian interaction. Pedestrian tracking system\nconsists of three sub-systems, which calculates the image processing, object\ntracking and traffic flow variables. The system receives input of stacks of\nimages and parameters. The first sub-system performs Image Processing analysis\nwhile the second sub-system carries out the tracking of pedestrians by matching\nthe features and tracing the pedestrian numbers frame by frame. The last\nsub-system deals with a NTXY database to calculate the pedestrian traffic-flow\ncharacteristic such as flow rate, speed and area module. Comparison with manual\ndata collection method confirmed that the procedures described have significant\npotential to automate the data collection of both microscopic and macroscopic\npedestrian flow variables.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 02:58:43 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Teknomo", "Kardi", ""], ["Takeyama", "Yasushi", ""], ["Inamura", "Hajime", ""]]}, {"id": "1609.01819", "submitter": "Sujith Ravi", "authors": "Harrie Oosterhuis, Sujith Ravi, Michael Bendersky", "title": "Semantic Video Trailers", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based video summarization is the task of creating a brief visual\ntrailer, which captures the parts of the video (or a collection of videos) that\nare most relevant to the user-issued query. In this paper, we propose an\nunsupervised label propagation approach for this task. Our approach effectively\ncaptures the multimodal semantics of queries and videos using state-of-the-art\ndeep neural networks and creates a summary that is both semantically coherent\nand visually attractive. We describe the theoretical framework of our\ngraph-based approach and empirically evaluate its effectiveness in creating\nrelevant and attractive trailers. Finally, we showcase example video trailers\ngenerated by our system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 03:35:54 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["Ravi", "Sujith", ""], ["Bendersky", "Michael", ""]]}, {"id": "1609.01828", "submitter": "Vinay Kumar N", "authors": "Y H Sharath Kumar, N Vinay Kumar, D S Guru", "title": "Delaunay Triangulation on Skeleton of Flowers for Classification", "comments": "10 pages, 5 figures, 1 table", "journal-ref": "Procedia Computer Science, Volume 45, 2015, Pages 226-235", "doi": "10.1016/j.procs.2015.03.125", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a Triangle based approach to classify flower images.\nInitially, flowers are segmented using whorl based region merging segmentation.\nSkeleton of a flower is obtained from the segmented flower using a skeleton\npruning method. The Delaunay triangulation is obtained from the endpoints and\njunction points detected on the skeleton. The length and angle features are\nextracted from the obtained Delaunay triangles and then are aggregated to\nrepresent in the form of interval-valued type data. A suitable classifier has\nbeen explored for the purpose of classification. To corroborate the efficacy of\nthe proposed method, an experiment is conducted on our own data set of 30\nclasses of flowers, containing 3000 samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 06:53:05 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Kumar", "Y H Sharath", ""], ["Kumar", "N Vinay", ""], ["Guru", "D S", ""]]}, {"id": "1609.01829", "submitter": "Manohar N", "authors": "Y H Sharath Kumar, Manohar N, H K Chethan", "title": "Animal Classification System: A Block Based Approach", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": "Procedia Computer Science, Volume 45, 2015, Pages 336-343", "doi": "10.1016/j.procs.2015.03.156", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for the classification of animal in images.\nInitially, a graph cut based method is used to perform segmentation in order to\neliminate the background from the given image. The segmented animal images are\npartitioned in to number of blocks and then the color texture moments are\nextracted from different blocks. Probabilistic neural network and K-nearest\nneighbors are considered here for classification. To corroborate the efficacy\nof the proposed method, an experiment was conducted on our own data set of 25\nclasses of animals, which consisted of 4000 sample images. The experiment was\nconducted by picking images randomly from the database to study the effect of\nclassification accuracy, and the results show that the K-nearest neighbors\nclassifier achieves good performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 11:19:41 GMT"}], "update_date": "2016-09-25", "authors_parsed": [["Kumar", "Y H Sharath", ""], ["N", "Manohar", ""], ["Chethan", "H K", ""]]}, {"id": "1609.01839", "submitter": "Hang Yang", "authors": "Hang Yang, Ming Zhu, Zhongbo Zhang, Heyan Huang", "title": "Guided Filter based Edge-preserving Image Non-blind Deconvolution", "comments": "4 pages, 3 figures, ICIP 2013. arXiv admin note: text overlap with\n  arXiv:1609.01380", "journal-ref": "The 20th IEEE International Conference on Image Processing\n  (ICIP),2013,4593--4596", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new approach for efficient edge-preserving image\ndeconvolution. Our algorithm is based on a novel type of explicit image filter\n- guided filter. The guided filter can be used as an edge-preserving smoothing\noperator like the popular bilateral filter, but has better behaviors near\nedges. We propose an efficient iterative algorithm with the decouple of\ndeblurring and denoising steps in the restoration process. In deblurring step,\nwe proposed two cost function which could be computed with fast Fourier\ntransform efficiently. The solution of the first one is used as the guidance\nimage, and another solution will be filtered in next step. In the denoising\nstep, the guided filter is used with the two obtained images for efficient\nedge-preserving filtering. Furthermore, we derive a simple and effective method\nto automatically adjust the regularization parameter at each iteration. We\ncompare our deconvolution algorithm with many competitive deconvolution\ntechniques in terms of ISNR and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 05:16:35 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Yang", "Hang", ""], ["Zhu", "Ming", ""], ["Zhang", "Zhongbo", ""], ["Huang", "Heyan", ""]]}, {"id": "1609.01859", "submitter": "Ke Sun", "authors": "Ke Sun, Xianxu Hou, Qian Zhang, Guoping Qiu", "title": "Automatic Visual Theme Discovery from Joint Image and Text Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to semantic image understanding is to manually tag images\nwith keywords and then learn a mapping from vi- sual features to keywords.\nManually tagging images is a subjective pro- cess and the same or very similar\nvisual contents are often tagged with different keywords. Furthermore, not all\ntags have the same descriptive power for visual contents and large vocabulary\navailable from natural language could result in a very diverse set of keywords.\nIn this paper, we propose an unsupervised visual theme discovery framework as a\nbetter (more compact, efficient and effective) alternative to semantic\nrepresen- tation of visual contents. We first show that tag based annotation\nlacks consistency and compactness for describing visually similar contents. We\nthen learn the visual similarity between tags based on the visual features of\nthe images containing the tags. At the same time, we use a natural language\nprocessing technique (word embedding) to measure the seman- tic similarity\nbetween tags. Finally, we cluster tags into visual themes based on their visual\nsimilarity and semantic similarity measures using a spectral clustering\nalgorithm. We conduct user studies to evaluate the effectiveness and\nrationality of the visual themes discovered by our unsu- pervised algorithm and\nobtains promising result. We then design three common computer vision tasks,\nexample based image search, keyword based image search and image labelling to\nexplore potential applica- tion of our visual themes discovery framework. In\nexperiments, visual themes significantly outperforms tags on semantic image\nunderstand- ing and achieve state-of-art performance in all three tasks. This\nagain demonstrate the effectiveness and versatility of proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 07:22:09 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Sun", "Ke", ""], ["Hou", "Xianxu", ""], ["Zhang", "Qian", ""], ["Qiu", "Guoping", ""]]}, {"id": "1609.01882", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, Herv\\'e J\\'egou and Florent Perronnin", "title": "Polysemous codes", "comments": "The final (author) version of our ECCV'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90's to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:45:19 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 23:00:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Perronnin", "Florent", ""]]}, {"id": "1609.01885", "submitter": "Abhay Gupta", "authors": "Abhay Gupta, Arjun D'Cunha, Kamal Awasthi, Vineeth Balasubramanian", "title": "DAiSEE: Towards User Engagement Recognition in the Wild", "comments": "12 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DAiSEE, the first multi-label video classification dataset\ncomprising of 9068 video snippets captured from 112 users for recognizing the\nuser affective states of boredom, confusion, engagement, and frustration in the\nwild. The dataset has four levels of labels namely - very low, low, high, and\nvery high for each of the affective states, which are crowd annotated and\ncorrelated with a gold standard annotation created using a team of expert\npsychologists. We have also established benchmark results on this dataset using\nstate-of-the-art video classification methods that are available today. We\nbelieve that DAiSEE will provide the research community with challenges in\nfeature extraction, context-based inference, and development of suitable\nmachine learning methods for related tasks, thus providing a springboard for\nfurther research. The dataset is available for download at\nhttps://iith.ac.in/~daisee-dataset\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:50:11 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 15:24:34 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 11:05:57 GMT"}, {"version": "v4", "created": "Fri, 15 Dec 2017 06:22:10 GMT"}, {"version": "v5", "created": "Thu, 12 Apr 2018 16:40:55 GMT"}, {"version": "v6", "created": "Fri, 13 Apr 2018 04:42:51 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Gupta", "Abhay", ""], ["D'Cunha", "Arjun", ""], ["Awasthi", "Kamal", ""], ["Balasubramanian", "Vineeth", ""]]}, {"id": "1609.01915", "submitter": "Surya Prasath", "authors": "V. B. Surya Prasath", "title": "Polyp Detection and Segmentation from Video Capsule Endoscopy: A Review", "comments": "Project webpage: http://goo.gl/eAUWKJ - Complete Bibliography -\n  Compiled and Continuously Updated", "journal-ref": "Journal of Imaging, 3(1), 1, 2017", "doi": "10.3390/jimaging3010001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video capsule endoscopy (VCE) is used widely nowadays for visualizing the\ngastrointestinal (GI) tract. Capsule endoscopy exams are prescribed usually as\nan additional monitoring mechanism and can help in identifying polyps,\nbleeding, etc. To analyze the large scale video data produced by VCE exams\nautomatic image processing, computer vision, and learning algorithms are\nrequired. Recently, automatic polyp detection algorithms have been proposed\nwith various degrees of success. Though polyp detection in colonoscopy and\nother traditional endoscopy procedure based images is becoming a mature field,\ndue to its unique imaging characteristics detecting polyps automatically in VCE\nis a hard problem. We review different polyp detection approaches for VCE\nimagery and provide systematic analysis with challenges faced by standard image\nprocessing and computer vision methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 10:09:06 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Prasath", "V. B. Surya", ""]]}, {"id": "1609.01932", "submitter": "Toni Heidenreich", "authors": "Toni Heidenreich and Michael W. Spratling", "title": "A three-dimensional approach to Visual Speech Recognition using Discrete\n  Cosine Transforms", "comments": "Revised and shortened version of the Master's thesis of the author\n  (King's College London, 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual speech recognition aims to identify the sequence of phonemes from\ncontinuous speech. Unlike the traditional approach of using 2D image feature\nextraction methods to derive features of each video frame separately, this\npaper proposes a new approach using a 3D (spatio-temporal) Discrete Cosine\nTransform to extract features of each feasible sub-sequence of an input video\nwhich are subsequently classified individually using Support Vector Machines\nand combined to find the most likely phoneme sequence using a tailor-made\nHidden Markov Model. The algorithm is trained and tested on the VidTimit\ndatabase to recognise sequences of phonemes as well as visemes (visual speech\nunits). Furthermore, the system is extended with the training on phoneme or\nviseme pairs (biphones) to counteract the human speech ambiguity of\nco-articulation. The test set accuracy for the recognition of phoneme sequences\nis 20%, and the accuracy of viseme sequences is 39%. Both results improve the\nbest values reported in other papers by approximately 2%. The contribution of\nthe result is three-fold: Firstly, this paper is the first to show that 3D\nfeature extraction methods can be applied to continuous sequence recognition\ntasks despite the unknown start positions and durations of each phoneme.\nSecondly, the result confirms that 3D feature extraction methods improve the\naccuracy compared to 2D features extraction methods. Thirdly, the paper is the\nfirst to specifically compare an otherwise identical method with and without\nusing biphones, verifying that the usage of biphones has a positive impact on\nthe result.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 10:59:19 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Heidenreich", "Toni", ""], ["Spratling", "Michael W.", ""]]}, {"id": "1609.01958", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo and Simone Melzi", "title": "Object Tracking via Dynamic Feature Selection Processes", "comments": "The paper will appear in the USB ECCV workshops proceedings and on\n  the IEEE Xplore. The results will be presented at VOT2016 workshop which will\n  take place on 10.12 at ECCV2016. In the days following the workshop, the raw\n  results of the submitted trackers as well as the results paper will be made\n  publicly available from the VOT homepage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DFST proposes an optimized visual tracking algorithm based on the real-time\nselection of locally and temporally discriminative features. A feature\nselection mechanism is embedded in the Adaptive colour Names (CN) tracking\nsystem that adaptively selects the top-ranked discriminative features for\ntracking. DFST provides a significant gain in accuracy and precision allowing\nthe use of a dynamic set of features that results in an increased system\nflexibility. DFST is based on the unsupervised method \"Infinite Feature\nSelection\" (Inf-FS), which ranks features according with their \"redundancy\"\nwithout using class labels. By using a fast online algorithm for learning\ndictionaries the size of the box is adapted during the processing. At each\nupdate, we use multiple examples around the target (at different positions and\nscales). DFST also improved the CN by adding micro-shift at the predicted\nposition and bounding box adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 12:27:11 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Roffo", "Giorgio", ""], ["Melzi", "Simone", ""]]}, {"id": "1609.01984", "submitter": "Jinyoung Choi", "authors": "Jinyoung Choi, Beom-Jin Lee, and Byoung-Tak Zhang", "title": "Human Body Orientation Estimation using Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal robots are expected to interact with the user by recognizing the\nuser's face. However, in most of the service robot applications, the user needs\nto move himself/herself to allow the robot to see him/her face to face. To\novercome such limitations, a method for estimating human body orientation is\nrequired. Previous studies used various components such as feature extractors\nand classification models to classify the orientation which resulted in low\nperformance. For a more robust and accurate approach, we propose the light\nweight convolutional neural networks, an end to end system, for estimating\nhuman body orientation. Our body orientation estimation model achieved 81.58%\nand 94% accuracy with the benchmark dataset and our own dataset respectively.\nThe proposed method can be used in a wide range of service robot applications\nwhich depend on the ability to estimate human body orientation. To show its\nusefulness in service robot applications, we designed a simple robot\napplication which allows the robot to move towards the user's frontal plane.\nWith this, we demonstrated an improved face detection rate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 13:53:26 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Choi", "Jinyoung", ""], ["Lee", "Beom-Jin", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1609.02001", "submitter": "Da Chen", "authors": "Da Chen, Wenbin Li, Peter Hall", "title": "Dense Motion Estimation for Smoke", "comments": "ACCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motion estimation for highly dynamic phenomena such as smoke is an open\nchallenge for Computer Vision. Traditional dense motion estimation algorithms\nhave difficulties with non-rigid and large motions, both of which are\nfrequently observed in smoke motion. We propose an algorithm for dense motion\nestimation of smoke. Our algorithm is robust, fast, and has better performance\nover different types of smoke compared to other dense motion estimation\nalgorithms, including state of the art and neural network approaches. The key\nto our contribution is to use skeletal flow, without explicit point matching,\nto provide a sparse flow. This sparse flow is upgraded to a dense flow. In this\npaper we describe our algorithm in greater detail, and provide experimental\nevidence to support our claims.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 14:40:08 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 14:19:43 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Chen", "Da", ""], ["Li", "Wenbin", ""], ["Hall", "Peter", ""]]}, {"id": "1609.02035", "submitter": "Tom Z. J. Fu", "authors": "Meihua Wang, Jiaming Mai, Yun Liang, Tom Z. J. Fu, Zhenjie Zhang,\n  Ruichu Cai", "title": "Component-Based Distributed Framework for Coherent and Real-Time Video\n  Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional dehazing techniques, as a well studied topic in image processing,\nare now widely used to eliminate the haze effects from individual images.\nHowever, even the state-of-the-art dehazing algorithms may not provide\nsufficient support to video analytics, as a crucial pre-processing step for\nvideo-based decision making systems (e.g., robot navigation), due to the\nlimitations of these algorithms on poor result coherence and low processing\nefficiency. This paper presents a new framework, particularly designed for\nvideo dehazing, to output coherent results in real time, with two novel\ntechniques. Firstly, we decompose the dehazing algorithms into three generic\ncomponents, namely transmission map estimator, atmospheric light estimator and\nhaze-free image generator. They can be simultaneously processed by multiple\nthreads in the distributed system, such that the processing efficiency is\noptimized by automatic CPU resource allocation based on the workloads.\nSecondly, a cross-frame normalization scheme is proposed to enhance the\ncoherence among consecutive frames, by sharing the parameters of atmospheric\nlight from consecutive frames in the distributed computation platform. The\ncombination of these techniques enables our framework to generate highly\nconsistent and accurate dehazing results in real-time, by using only 3 PCs\nconnected by Ethernet.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 15:56:09 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 16:14:01 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Wang", "Meihua", ""], ["Mai", "Jiaming", ""], ["Liang", "Yun", ""], ["Fu", "Tom Z. J.", ""], ["Zhang", "Zhenjie", ""], ["Cai", "Ruichu", ""]]}, {"id": "1609.02036", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Dahua Lin, Xiaoou Tang", "title": "Deep Markov Random Field for Image Modeling", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Random Fields (MRFs), a formulation widely used in generative image\nmodeling, have long been plagued by the lack of expressive power. This issue is\nprimarily due to the fact that conventional MRFs formulations tend to use\nsimplistic factors to capture local patterns. In this paper, we move beyond\nsuch limitations, and propose a novel MRF model that uses fully-connected\nneurons to express the complex interactions among pixels. Through theoretical\nanalysis, we reveal an inherent connection between this model and recurrent\nneural networks, and thereon derive an approximated feed-forward network that\ncouples multiple RNNs along opposite directions. This formulation combines the\nexpressive power of deep neural networks and the cyclic dependency structure of\nMRF in a unified model, bringing the modeling capability to a new level. The\nfeed-forward approximation also allows it to be efficiently learned from data.\nExperimental results on a variety of low-level vision tasks show notable\nimprovement over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 15:56:36 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Wu", "Zhirong", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1609.02077", "submitter": "Guanbin Li", "authors": "Guanbin Li and Yizhou Yu", "title": "Visual Saliency Detection Based on Multiscale Deep CNN Features", "comments": "Accepted for publication in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2602079", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this paper, we discover that a\nhigh-quality visual saliency model can be learned from multiscale features\nextracted using deep convolutional neural networks (CNNs), which have had many\nsuccesses in visual recognition tasks. For learning such saliency models, we\nintroduce a neural network architecture, which has fully connected layers on\ntop of CNNs responsible for feature extraction at three different scales. The\npenultimate layer of our neural network has been confirmed to be a\ndiscriminative high-level feature vector for saliency detection, which we call\ndeep contrast feature. To generate a more robust feature, we integrate\nhandcrafted low-level features with our deep contrast feature. To promote\nfurther research and evaluation of visual saliency models, we also construct a\nnew large database of 4447 challenging images and their pixelwise saliency\nannotations. Experimental results demonstrate that our proposed method is\ncapable of achieving state-of-the-art performance on all public benchmarks,\nimproving the F- measure by 6.12% and 10.0% respectively on the DUT-OMRON\ndataset and our new dataset (HKU-IS), and lowering the mean absolute error by\n9% and 35.3% respectively on these two datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 17:13:16 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1609.02087", "submitter": "John Paisley", "authors": "Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao and John Paisley", "title": "Clearing the Skies: A deep network architecture for single-image rain\n  removal", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2691802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep network architecture called DerainNet for removing rain\nstreaks from an image. Based on the deep convolutional neural network (CNN), we\ndirectly learn the mapping relationship between rainy and clean image detail\nlayers from data. Because we do not possess the ground truth corresponding to\nreal-world rainy images, we synthesize images with rain for training. In\ncontrast to other common strategies that increase depth or breadth of the\nnetwork, we use image processing domain knowledge to modify the objective\nfunction and improve deraining with a modestly-sized CNN. Specifically, we\ntrain our DerainNet on the detail (high-pass) layer rather than in the image\ndomain. Though DerainNet is trained on synthetic data, we find that the learned\nnetwork translates very effectively to real-world images for testing. Moreover,\nwe augment the CNN framework with image enhancement to improve the visual\nresults. Compared with state-of-the-art single image de-raining methods, our\nmethod has improved rain removal and much faster computation time after network\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 17:35:17 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 19:53:29 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Fu", "Xueyang", ""], ["Huang", "Jiabin", ""], ["Ding", "Xinghao", ""], ["Liao", "Yinghao", ""], ["Paisley", "John", ""]]}, {"id": "1609.02132", "submitter": "Iasonas Kokkinos", "authors": "Iasonas Kokkinos", "title": "UberNet: Training a `Universal' Convolutional Neural Network for Low-,\n  Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a convolutional neural network (CNN) that jointly\nhandles low-, mid-, and high-level vision tasks in a unified architecture that\nis trained end-to-end. Such a universal network can act like a `swiss knife'\nfor vision tasks; we call this architecture an UberNet to indicate its\noverarching nature.\n  We address two main technical challenges that emerge when broadening up the\nrange of tasks handled by a single CNN: (i) training a deep architecture while\nrelying on diverse training sets and (ii) training many (potentially unlimited)\ntasks with a limited memory budget. Properly addressing these two problems\nallows us to train accurate predictors for a host of tasks, without\ncompromising accuracy.\n  Through these advances we train in an end-to-end manner a CNN that\nsimultaneously addresses (a) boundary detection (b) normal estimation (c)\nsaliency estimation (d) semantic segmentation (e) human part segmentation (f)\nsemantic boundary detection, (g) region proposal generation and object\ndetection. We obtain competitive performance while jointly addressing all of\nthese tasks in 0.7 seconds per frame on a single GPU. A demonstration of this\nsystem can be found at http://cvn.ecp.fr/ubernet/.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 19:35:30 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Kokkinos", "Iasonas", ""]]}, {"id": "1609.02135", "submitter": "Alankar Kotwal", "authors": "Alankar Kotwal and Ajit Rajwade", "title": "Optimizing Codes for Source Separation in Color Image Demosaicing and\n  Compressive Video Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There exist several applications in image processing (eg: video compressed\nsensing [Hitomi, Y. et al, \"Video from a single coded exposure photograph using\na learned overcomplete dictionary\"] and color image demosaicing [Moghadam, A.\nA. et al, \"Compressive Framework for Demosaicing of Natural Images\"]) which\nrequire separation of constituent images given measurements in the form of a\ncoded superposition of those images. Physically practical code patterns in\nthese applications are non-negative, systematically structured, and do not\nalways obey the nice incoherence properties of other patterns such as Gaussian\ncodes, which can adversely affect reconstruction performance. The contribution\nof this paper is to design code patterns for video compressed sensing and\ndemosaicing by minimizing the mutual coherence of the matrix $\\boldsymbol{\\Phi\n\\Psi}$ where $\\boldsymbol{\\Phi}$ represents the sensing matrix created from the\ncode, and $\\boldsymbol{\\Psi}$ is the signal representation matrix. Our main\ncontribution is that we explicitly take into account the special structure of\nthose code patterns as required by these applications: (1)~non-negativity,\n(2)~block-diagonal nature, and (3)~circular shifting. In particular, the last\nproperty enables for accurate and seamless patch-wise reconstruction for some\nimportant compressed sensing architectures.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 19:56:44 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 17:47:04 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Kotwal", "Alankar", ""], ["Rajwade", "Ajit", ""]]}, {"id": "1609.02137", "submitter": "Kardi Teknomo", "authors": "Kardi Teknomo, Yasushi Takeyama and Hajime Inamura", "title": "Tracking Algorithm for Microscopic Flow Data Collection", "comments": "2 pages, Teknomo, Kardi; Takeyama, Yasushi; Inamura, Hajime, Tracking\n  Algorithm for Microscopic Flow Data Collection, Proceeding of JSCE\n  Conference, Sendai, Japan Sept 2000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods to automate traffic data collection have recently been\ndeveloped by many researchers. A macroscopic data collection through image\nprocessing has been proposed. For microscopic traffic flow data, such as\nindividual speed and time or distance headway, tracking of individual movement\nis needed. The tracking algorithms for pedestrian or vehicle have been\ndeveloped to trace the movement of one or two pedestrians based on sign\npattern, and feature detection. No research has been done to track many\npedestrians or vehicles at once. This paper describes a new and fast algorithm\nto track the movement of many individual vehicles or pedestrians\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 02:27:52 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Teknomo", "Kardi", ""], ["Takeyama", "Yasushi", ""], ["Inamura", "Hajime", ""]]}, {"id": "1609.02214", "submitter": "Jinming Duan", "authors": "Jinming Duan, Christopher Tench, Irene Gottlob, Frank Proudlock, Li\n  Bai", "title": "Automated Segmentation of Retinal Layers from Optical Coherent\n  Tomography Images Using Geodesic Distance", "comments": "20 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical coherence tomography (OCT) is a non-invasive imaging technique that\ncan produce images of the eye at the microscopic level. OCT image segmentation\nto localise retinal layer boundaries is a fundamental procedure for diagnosing\nand monitoring the progression of retinal and optical nerve disorders. In this\npaper, we introduce a novel and accurate geodesic distance method (GDM) for OCT\nsegmentation of both healthy and pathological images in either two- or\nthree-dimensional spaces. The method uses a weighted geodesic distance by an\nexponential function, taking into account both horizontal and vertical\nintensity variations. The weighted geodesic distance is efficiently calculated\nfrom an Eikonal equation via the fast sweeping method. The segmentation is then\nrealised by solving an ordinary differential equation with the geodesic\ndistance. The results of the GDM are compared with manually segmented retinal\nlayer boundaries/surfaces. Extensive experiments demonstrate that the proposed\nGDM is robust to complex retinal structures with large curvatures and\nirregularities and it outperforms the parametric active contour algorithm as\nwell as the graph theoretic based approaches for delineating the retinal layers\nin both healthy and pathological images.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 22:58:21 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Duan", "Jinming", ""], ["Tench", "Christopher", ""], ["Gottlob", "Irene", ""], ["Proudlock", "Frank", ""], ["Bai", "Li", ""]]}, {"id": "1609.02243", "submitter": "Kardi Teknomo", "authors": "Kardi Teknomo, Yasushi Takeyama, Hajime Inamura", "title": "Determination of Pedestrian Flow Performance Based on Video Tracking and\n  Microscopic Simulations", "comments": "4 pages, Teknomo, Kardi; Takeyama, Yasushi; Inamura, Hajime,\n  Determination of Pedestrian Flow Performance Based on Video Tracking and\n  Microscopic Simulations, Proceedings of Infrastructure Planning Conference\n  Vol. 23 no 1, Ashikaga, Japan, pp. 639-642, Nov 2000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the objectives of understanding pedestrian behavior is to predict the\neffect of proposed changes in the design or evaluation of pedestrian\nfacilities. We want to know the impact to the user of the facilities, as the\ndesign of the facilities change. That impact was traditionally evaluated by\nlevel of service standards. Another design criterion to measure the impact of\ndesign change is measured by the pedestrian flow performance index. This paper\ndescribes the determination of pedestrian flow performance based video tracking\nor any microscopic pedestrian simulation models. Most of pedestrian researches\nhave been done on a macroscopic level, which is an aggregation of all\npedestrian movement in pedestrian areas into flow, average speed and area\nmodule. Macroscopic level, however, does not consider the interaction between\npedestrians. It is also not well suited for prediction of pedestrian flow\nperformance in pedestrian areas or in buildings with some obstruction, that\nreduces the effective width of the walkways. On the other hand, the microscopic\nlevel has a more general usage and considers detail in the design. More\nefficient pedestrian flow can even be reached with less space. Those results\nhave rejected the linearity assumption of space and flow in the macroscopic\nlevel.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 01:58:10 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Teknomo", "Kardi", ""], ["Takeyama", "Yasushi", ""], ["Inamura", "Hajime", ""]]}, {"id": "1609.02271", "submitter": "Anand Sriraman", "authors": "Anand Sriraman, Mandar Kulkarni, Rahul Kumar, Kanika Kalra, Purushotam\n  Radadia, Shirish Karande", "title": "Ashwin: Plug-and-Play System for Machine-Human Image Annotation", "comments": "HCOMP 2016 Demonstrations Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end machine-human image annotation system where each\ncomponent can be attached in a plug-and-play fashion. These components include\nFeature Extraction, Machine Classifier, Task Sampling and Crowd Consensus.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 04:49:31 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 18:37:03 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Sriraman", "Anand", ""], ["Kulkarni", "Mandar", ""], ["Kumar", "Rahul", ""], ["Kalra", "Kanika", ""], ["Radadia", "Purushotam", ""], ["Karande", "Shirish", ""]]}, {"id": "1609.02284", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Ram Nevatia", "title": "Learning Action Concept Trees and Semantic Alignment Networks from\n  Image-Description Data", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action classification in still images has been a popular research topic in\ncomputer vision. Labelling large scale datasets for action classification\nrequires tremendous manual work, which is hard to scale up. Besides, the action\ncategories in such datasets are pre-defined and vocabularies are fixed. However\nhumans may describe the same action with different phrases, which leads to the\ndifficulty of vocabulary expansion for traditional fully-supervised methods. We\nobserve that large amounts of images with sentence descriptions are readily\navailable on the Internet. The sentence descriptions can be regarded as weak\nlabels for the images, which contain rich information and could be used to\nlearn flexible expressions of action categories. We propose a method to learn\nan Action Concept Tree (ACT) and an Action Semantic Alignment (ASA) model for\nclassification from image-description data via a two-stage learning process. A\nnew dataset for the task of learning actions from descriptions is built.\nExperimental results show that our method outperforms several baseline methods\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 05:53:31 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Gao", "Jiyang", ""], ["Nevatia", "Ram", ""]]}, {"id": "1609.02356", "submitter": "Ja-Keoung Koo", "authors": "Byung-Woo Hong, Ja-Keoung Koo, Hendrik Dirks, Martin Burger", "title": "Adaptive Regularization in Convex Composite Optimization for Variational\n  Imaging Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive regularization scheme in a variational framework where\na convex composite energy functional is optimized. We consider a number of\nimaging problems including denoising, segmentation and motion estimation, which\nare considered as optimal solutions of the energy functionals that mainly\nconsist of data fidelity, regularization and a control parameter for their\ntrade-off. We presents an algorithm to determine the relative weight between\ndata fidelity and regularization based on the residual that measures how well\nthe observation fits the model. Our adaptive regularization scheme is designed\nto locally control the regularization at each pixel based on the assumption\nthat the diversity of the residual of a given imaging model spatially varies.\nThe energy optimization is presented in the alternating direction method of\nmultipliers (ADMM) framework where the adaptive regularization is iteratively\napplied along with mathematical analysis of the proposed algorithm. We\ndemonstrate the robustness and effectiveness of our adaptive regularization\nthrough experimental results presenting that the qualitative and quantitative\nevaluation results of each imaging task are superior to the results with a\nconstant regularization scheme. The desired properties, robustness and\neffectiveness, of the regularization parameter selection in a variational\nframework for imaging problems are achieved by merely replacing the static\nregularization parameter with our adaptive one.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 09:45:14 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 12:58:18 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Hong", "Byung-Woo", ""], ["Koo", "Ja-Keoung", ""], ["Dirks", "Hendrik", ""], ["Burger", "Martin", ""]]}, {"id": "1609.02368", "submitter": "William Smith", "authors": "Alassane Seck, William A. P. Smith, Arnaud Dessein, Bernard Tiddeman,\n  Hannah Dee and Abhishek Dutta", "title": "Ear-to-ear Capture of Facial Intrinsics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical approach to capturing ear-to-ear face models\ncomprising both 3D meshes and intrinsic textures (i.e. diffuse and specular\nalbedo). Our approach is a hybrid of geometric and photometric methods and\nrequires no geometric calibration. Photometric measurements made in a\nlightstage are used to estimate view dependent high resolution normal maps. We\novercome the problem of having a single photometric viewpoint by capturing in\nmultiple poses. We use uncalibrated multiview stereo to estimate a coarse base\nmesh to which the photometric views are registered. We propose a novel approach\nto robustly stitching surface normal and intrinsic texture data into a\nseamless, complete and highly detailed face model. The resulting relightable\nmodels provide photorealistic renderings in any view.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 10:24:44 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Seck", "Alassane", ""], ["Smith", "William A. P.", ""], ["Dessein", "Arnaud", ""], ["Tiddeman", "Bernard", ""], ["Dee", "Hannah", ""], ["Dutta", "Abhishek", ""]]}, {"id": "1609.02374", "submitter": "Mohammad Hossein Jafari", "authors": "Mohammad H. Jafari, Ebrahim Nasr-Esfahani, Nader Karimi, S.M. Reza\n  Soroushmehr, Shadrokh Samavi, Kayvan Najarian", "title": "Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1007/s11548-017-1567-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is amongst most aggressive types of cancer. However, it is highly\ncurable if detected in its early stages. Prescreening of suspicious moles and\nlesions for malignancy is of great importance. Detection can be done by images\ncaptured by standard cameras, which are more preferable due to low cost and\navailability. One important step in computerized evaluation of skin lesions is\naccurate detection of lesion region, i.e. segmentation of an image into two\nregions as lesion and normal skin. Accurate segmentation can be challenging due\nto burdens such as illumination variation and low contrast between lesion and\nhealthy skin. In this paper, a method based on deep neural networks is proposed\nfor accurate extraction of a lesion region. The input image is preprocessed and\nthen its patches are fed to a convolutional neural network (CNN). Local texture\nand global structure of the patches are processed in order to assign pixels to\nlesion or normal classes. A method for effective selection of training patches\nis used for more accurate detection of a lesion border. The output segmentation\nmask is refined by some post processing operations. The experimental results of\nqualitative and quantitative evaluations demonstrate that our method can\noutperform other state-of-the-art algorithms exist in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 11:05:27 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Jafari", "Mohammad H.", ""], ["Nasr-Esfahani", "Ebrahim", ""], ["Karimi", "Nader", ""], ["Soroushmehr", "S. M. Reza", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1609.02409", "submitter": "Kardi Teknomo", "authors": "John Boaz Lee and Kardi Teknomo", "title": "Comparison of several short-term traffic speed forecasting models", "comments": "6 pages, Lee, J. B. and Teknomo, K. (2014) A review of various\n  short-term traffic speed forecasting models, Proceeding of the 12th National\n  Conference in Information Technology Education (NCITE 2014), October 23 - 25,\n  2014, Boracay, Philippines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of smartphones in recent years has made it possible\nfor us to collect large amounts of traffic data. Special software installed on\nthe phones of drivers allow us to gather GPS trajectories of their vehicles on\nthe road network. In this paper, we simulate the trajectories of multiple\nagents on a road network and use various models to forecast the short-term\ntraffic speed of various links. Our results show that traditional techniques\nlike multiple regression and artificial neural networks work well but simpler\nadaptive models that do not require prior training also perform comparatively\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:30:37 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Lee", "John Boaz", ""], ["Teknomo", "Kardi", ""]]}, {"id": "1609.02452", "submitter": "Andreas Bulling", "authors": "Sabrina Hoppe, Andreas Bulling", "title": "End-to-End Eye Movement Detection Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common computational methods for automated eye movement detection - i.e. the\ntask of detecting different types of eye movement in a continuous stream of\ngaze data - are limited in that they either involve thresholding on\nhand-crafted signal features, require individual detectors each only detecting\na single movement, or require pre-segmented data. We propose a novel approach\nfor eye movement detection that only involves learning a single detector\nend-to-end, i.e. directly from the continuous gaze data stream and\nsimultaneously for different eye movements without any manual feature crafting\nor segmentation. Our method is based on convolutional neural networks (CNN)\nthat recently demonstrated superior performance in a variety of tasks in\ncomputer vision, signal processing, and machine learning. We further introduce\na novel multi-participant dataset that contains scripted and free-viewing\nsequences of ground-truth annotated saccades, fixations, and smooth pursuits.\nWe show that our CNN-based method outperforms state-of-the-art baselines by a\nlarge margin on this challenging dataset, thereby underlining the significant\npotential of this approach for holistic, robust, and accurate eye movement\nprotocol analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 14:58:15 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Hoppe", "Sabrina", ""], ["Bulling", "Andreas", ""]]}, {"id": "1609.02469", "submitter": "Joseph Antony A", "authors": "Joseph Antony, Kevin McGuinness, Noel E O Connor, Kieran Moran", "title": "Quantifying Radiographic Knee Osteoarthritis Severity using Deep\n  Convolutional Neural Networks", "comments": "Included in ICPR 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to automatically quantify the severity of\nknee osteoarthritis (OA) from radiographs using deep convolutional neural\nnetworks (CNN). Clinically, knee OA severity is assessed using Kellgren \\&\nLawrence (KL) grades, a five point scale. Previous work on automatically\npredicting KL grades from radiograph images were based on training shallow\nclassifiers using a variety of hand engineered features. We demonstrate that\nclassification accuracy can be significantly improved using deep convolutional\nneural network models pre-trained on ImageNet and fine-tuned on knee OA images.\nFurthermore, we argue that it is more appropriate to assess the accuracy of\nautomatic knee OA severity predictions using a continuous distance-based\nevaluation metric like mean squared error than it is to use classification\naccuracy. This leads to the formulation of the prediction of KL grades as a\nregression problem and further improves accuracy. Results on a dataset of X-ray\nimages and KL grades from the Osteoarthritis Initiative (OAI) show a sizable\nimprovement over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 15:39:48 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["Connor", "Noel E O", ""], ["Moran", "Kieran", ""]]}, {"id": "1609.02500", "submitter": "Denis Tom\\`e", "authors": "Denis Tome', Luca Bondi, Emanuele Plebani, Luca Baroffio, Danilo Pau,\n  Stefano Tubaro", "title": "Reduced Memory Region Based Deep Convolutional Neural Network Detection", "comments": "IEEE 2016 ICCE-Berlin", "journal-ref": "2016 IEEE 6th International Conference on Consumer Electronics -\n  Berlin (ICCE-Berlin)", "doi": "10.1109/ICCE-Berlin.2016.7684706", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate pedestrian detection has a primary role in automotive safety: for\nexample, by issuing warnings to the driver or acting actively on car's brakes,\nit helps decreasing the probability of injuries and human fatalities. In order\nto achieve very high accuracy, recent pedestrian detectors have been based on\nConvolutional Neural Networks (CNN). Unfortunately, such approaches require\nvast amounts of computational power and memory, preventing efficient\nimplementations on embedded systems. This work proposes a CNN-based detector,\nadapting a general-purpose convolutional network to the task at hand. By\nthoroughly analyzing and optimizing each step of the detection pipeline, we\ndevelop an architecture that outperforms methods based on traditional image\nfeatures and achieves an accuracy close to the state-of-the-art while having\nlow computational complexity. Furthermore, the model is compressed in order to\nfit the tight constrains of low power devices with a limited amount of embedded\nmemory available. This paper makes two main contributions: (1) it proves that a\nregion based deep neural network can be finely tuned to achieve adequate\naccuracy for pedestrian detection (2) it achieves a very low memory usage\nwithout reducing detection accuracy on the Caltech Pedestrian dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 17:16:38 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Tome'", "Denis", ""], ["Bondi", "Luca", ""], ["Plebani", "Emanuele", ""], ["Baroffio", "Luca", ""], ["Pau", "Danilo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1609.02583", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Philip H.S. Torr", "title": "Bottom-up Instance Segmentation using Deep Higher-Order CRFs", "comments": "British Machine Vision Conference (BMVC) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Scene Understanding problems such as Object Detection and\nSemantic Segmentation have made breakthroughs in recent years due to the\nadoption of deep learning. However, the former task is not able to localise\nobjects at a pixel level, and the latter task has no notion of different\ninstances of objects of the same class. We focus on the task of Instance\nSegmentation which recognises and localises objects down to a pixel level. Our\nmodel is based on a deep neural network trained for semantic segmentation. This\nnetwork incorporates a Conditional Random Field with end-to-end trainable\nhigher order potentials based on object detector outputs. This allows us to\nreason about instances from an initial, category-level semantic segmentation.\nOur simple method effectively leverages the great progress recently made in\nsemantic segmentation and object detection. The accurate instance-level\nsegmentations that our network produces is reflected by the considerable\nimprovements obtained over previous work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 20:37:39 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Arnab", "Anurag", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1609.02612", "submitter": "Carl Vondrick", "authors": "Carl Vondrick and Hamed Pirsiavash and Antonio Torralba", "title": "Generating Videos with Scene Dynamics", "comments": "NIPS 2016. See more at http://web.mit.edu/vondrick/tinyvideo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We capitalize on large amounts of unlabeled video in order to learn a model\nof scene dynamics for both video recognition tasks (e.g. action classification)\nand video generation tasks (e.g. future prediction). We propose a generative\nadversarial network for video with a spatio-temporal convolutional architecture\nthat untangles the scene's foreground from the background. Experiments suggest\nthis model can generate tiny videos up to a second at full frame rate better\nthan simple baselines, and we show its utility at predicting plausible futures\nof static images. Moreover, experiments and visualizations show the model\ninternally learns useful features for recognizing actions with minimal\nsupervision, suggesting scene dynamics are a promising signal for\nrepresentation learning. We believe generative video models can impact many\napplications in video understanding and simulation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 22:29:52 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 03:13:10 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 13:58:10 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1609.02638", "submitter": "Richard Wang", "authors": "Guanghui Wang", "title": "Robust Structure from Motion in the Presence of Outliers and Missing\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure from motion is an import theme in computer vision. Although great\nprogress has been made both in theory and applications, most of the algorithms\nonly work for static scenes and rigid objects. In recent years, structure and\nmotion recovery of non-rigid objects and dynamic scenes have received a lot of\nattention. In this paper, the state-of-the-art techniques for structure and\nmotion factorization of non-rigid objects are reviewed and discussed. First, an\nintroduction of the structure from motion problem is presented, followed by a\ngeneral formulation of non-rigid structure from motion. Second, an augmented\naffined factorization framework, by using homogeneous representation, is\npresented to solve the registration issue in the presence of outlying and\nmissing data. Third, based on the observation that the reprojection residuals\nof outliers are significantly larger than those of inliers, a robust\nfactorization strategy with outlier rejection is proposed by means of the\nreprojection residuals, followed by some comparative experimental evaluations.\nFinally, some future research topics in non-rigid structure from motion are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 02:00:20 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 21:55:01 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Wang", "Guanghui", ""]]}, {"id": "1609.02715", "submitter": "Amin Fehri", "authors": "Amin Fehri (CMM), Santiago Velasco-Forero (CMM), Fernand Meyer (CMM)", "title": "Automatic Selection of Stochastic Watershed Hierarchies", "comments": "in European Conference of Signal Processing (EUSIPCO), 2016,\n  Budapest, Hungary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation, seen as the association of a partition with an image, is a\ndifficult task. It can be decomposed in two steps: at first, a family of\ncontours associated with a series of nested partitions (or hierarchy) is\ncreated and organized, then pertinent contours are extracted. A coarser\npartition is obtained by merging adjacent regions of a finer partition. The\nstrength of a contour is then measured by the level of the hierarchy for which\nits two adjacent regions merge. We present an automatic segmentation strategy\nusing a wide range of stochastic watershed hierarchies. For a given set of\nhomogeneous images, our approach selects automatically the best hierarchy and\ncut level to perform image simplification given an evaluation score.\nExperimental results illustrate the advantages of our approach on several\nreal-life images datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 09:26:22 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Fehri", "Amin", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Meyer", "Fernand", "", "CMM"]]}, {"id": "1609.02744", "submitter": "Joseph Antony A", "authors": "Joseph Antony, Kevin McGuinness, Neil Welch, Joe Coyle, Andy\n  Franklyn-Miller, Noel E. O'Connor, Kieran Moran", "title": "An Interactive Segmentation Tool for Quantifying Fat in Lumbar Muscles\n  using Axial Lumbar-Spine MRI", "comments": null, "journal-ref": "IRBM 37(1), 11-22 (2016)", "doi": "10.1016/j.irbm.2015.10.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an interactive tool that can be used to quantify fat\ninfiltration in lumbar muscles, which is useful in studying fat infiltration\nand lower back pain (LBP) in adults. Currently, a qualitative assessment by\nvisual grading via a 5-point scale is used to study fat infiltration in lumbar\nmuscles from an axial view of lumbar-spine MR Images. However, a quantitative\napproach (on a continuous scale of 0-100\\%) may provide a greater insight. In\nthis paper, we propose a method to precisely quantify the fat deposition /\ninfiltration in a user-defined region of the lumbar muscles, which may aid\nbetter diagnosis and analysis. The key steps are interactively segmenting the\nregion of interest (ROI) from the lumbar muscles using the well known livewire\ntechnique, identifying fatty regions in the segmented region based on\nvariable-selection of threshold and softness levels, automatically detecting\nthe center of the spinal column and fragmenting the lumbar muscles into smaller\nregions with reference to the center of the spinal column, computing key\nparameters [such as total and region-wise fat content percentage, total-cross\nsectional area (TCSA) and functional cross-sectional area (FCSA)] and exporting\nthe computations and associated patient information from the MRI, into a\ndatabase. A standalone application using MATLAB R2014a was developed to perform\nthe required computations along with an intuitive graphical user interface\n(GUI).\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 11:13:58 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["Welch", "Neil", ""], ["Coyle", "Joe", ""], ["Franklyn-Miller", "Andy", ""], ["O'Connor", "Noel E.", ""], ["Moran", "Kieran", ""]]}, {"id": "1609.02770", "submitter": "Andrew Gilbert", "authors": "Andrew Gilbert, Richard Bowden", "title": "Image and Video Mining through Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of image and video recognition, the traditional approach is\na dataset split into fixed training and test partitions. However, the labelling\nof the training set is time-consuming, especially as datasets grow in size and\ncomplexity. Furthermore, this approach is not applicable to the home user, who\nwants to intuitively group their media without tirelessly labelling the\ncontent. Our interactive approach is able to iteratively cluster classes of\nimages and video. Our approach is based around the concept of an image\nsignature which, unlike a standard bag of words model, can express\nco-occurrence statistics as well as symbol frequency. We efficiently compute\nmetric distances between signatures despite their inherent high dimensionality\nand provide discriminative feature selection, to allow common and distinctive\nelements to be identified from a small set of user labelled examples. These\nelements are then accentuated in the image signature to increase similarity\nbetween examples and pull correct classes together. By repeating this process\nin an online learning framework, the accuracy of similarity increases\ndramatically despite labelling only a few training examples. To demonstrate\nthat the approach is agnostic to media type and features used, we evaluate on\nthree image datasets (15 scene, Caltech101 and FG-NET), a mixed text and image\ndataset (ImageTag), a dataset used in active learning (Iris) and on three\naction recognition datasets (UCF11, KTH and Hollywood2). On the UCF11 video\ndataset, the accuracy is 86.7% despite using only 90 labelled examples from a\ndataset of over 1200 videos, instead of the standard 1122 training videos. The\napproach is both scalable and efficient, with a single iteration over the full\nUCF11 dataset of around 1200 videos taking approximately 1 minute on a standard\ndesktop machine.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 12:49:22 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 12:26:30 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Gilbert", "Andrew", ""], ["Bowden", "Richard", ""]]}, {"id": "1609.02781", "submitter": "Gabriel De Barros Paranhos Da Costa", "authors": "Gabriel B. Paranhos da Costa, Welinton A. Contato, Tiago S. Nazare,\n  Jo\\~ao E. S. Batista Neto, Moacir Ponti", "title": "An empirical study on the effects of different types of noise in image\n  classification tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image classification is one of the main research problems in computer vision\nand machine learning. Since in most real-world image classification\napplications there is no control over how the images are captured, it is\nnecessary to consider the possibility that these images might be affected by\nnoise (e.g. sensor noise in a low-quality surveillance camera). In this paper\nwe analyse the impact of three different types of noise on descriptors\nextracted by two widely used feature extraction methods (LBP and HOG) and how\ndenoising the images can help to mitigate this problem. We carry out\nexperiments on two different datasets and consider several types of noise,\nnoise levels, and denoising methods. Our results show that noise can hinder\nclassification performance considerably and make classes harder to separate.\nAlthough denoising methods were not able to reach the same performance of the\nnoise-free scenario, they improved classification results for noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 13:19:41 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["da Costa", "Gabriel B. Paranhos", ""], ["Contato", "Welinton A.", ""], ["Nazare", "Tiago S.", ""], ["Neto", "Jo\u00e3o E. S. Batista", ""], ["Ponti", "Moacir", ""]]}, {"id": "1609.02805", "submitter": "Nicolas Jaccard", "authors": "Nicolas Jaccard, Thomas W. Rogers, Edward J. Morton, Lewis D. Griffin", "title": "Automated detection of smuggled high-risk security threats using Deep\n  Learning", "comments": "Submission for Crime Detection and Prevention conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The security infrastructure is ill-equipped to detect and deter the smuggling\nof non-explosive devices that enable terror attacks such as those recently\nperpetrated in western Europe. The detection of so-called \"small metallic\nthreats\" (SMTs) in cargo containers currently relies on statistical risk\nanalysis, intelligence reports, and visual inspection of X-ray images by\nsecurity officers. The latter is very slow and unreliable due to the difficulty\nof the task: objects potentially spanning less than 50 pixels have to be\ndetected in images containing more than 2 million pixels against very complex\nand cluttered backgrounds. In this contribution, we demonstrate for the first\ntime the use of Convolutional Neural Networks (CNNs), a type of Deep Learning,\nto automate the detection of SMTs in fullsize X-ray images of cargo containers.\nNovel approaches for dataset augmentation allowed to train CNNs from-scratch\ndespite the scarcity of data available. We report fewer than 6% false alarms\nwhen detecting 90% SMTs synthetically concealed in stream-of-commerce images,\nwhich corresponds to an improvement of over an order of magnitude over\nconventional approaches such as Bag-of-Words (BoWs). The proposed scheme offers\npotentially super-human performance for a fraction of the time it would take\nfor a security officers to carry out visual inspection (processing time is\napproximately 3.5s per container image).\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 14:14:52 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Jaccard", "Nicolas", ""], ["Rogers", "Thomas W.", ""], ["Morton", "Edward J.", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "1609.02825", "submitter": "Xi Peng", "authors": "Xi Peng, Qiong Hu, Junzhou Huang, Dimitris N. Metaxas", "title": "Track Facial Points in Unconstrained Videos", "comments": "British Machine Vision Conference (BMVC), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking Facial Points in unconstrained videos is challenging due to the\nnon-rigid deformation that changes over time. In this paper, we propose to\nexploit incremental learning for person-specific alignment in wild conditions.\nOur approach takes advantage of part-based representation and cascade\nregression for robust and efficient alignment on each frame. Unlike existing\nmethods that usually rely on models trained offline, we incrementally update\nthe representation subspace and the cascade of regressors in a unified\nframework to achieve personalized modeling on the fly. To alleviate the\ndrifting issue, the fitting results are evaluated using a deep neural network,\nwhere well-aligned faces are picked out to incrementally update the\nrepresentation and fitting models. Both image and video datasets are employed\nto valid the proposed method. The results demonstrate the superior performance\nof our approach compared with existing approaches in terms of fitting accuracy\nand efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 15:02:08 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Peng", "Xi", ""], ["Hu", "Qiong", ""], ["Huang", "Junzhou", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1609.02932", "submitter": "Muzammil Behzad", "authors": "Muzammil Behzad, Mudassir Masood, Tarig Ballal, Maha Shadaydeh and\n  Tareq Y. Al-Naffouri", "title": "Image Denoising Via Collaborative Support-Agnostic Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel image denoising algorithm using\ncollaborative support-agnostic sparse reconstruction. An observed image is\nfirst divided into patches. Similarly structured patches are grouped together\nto be utilized for collaborative processing. In the proposed collaborative\nschemes, similar patches are assumed to share the same support taps. For sparse\nreconstruction, the likelihood of a tap being active in a patch is computed and\nrefined through a collaboration process with other similar patches in the same\ngroup. This provides very good patch support estimation, hence enhancing the\nquality of image restoration. Performance comparisons with state-of-the-art\nalgorithms, in terms of SSIM and PSNR, demonstrate the superiority of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 20:20:39 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Behzad", "Muzammil", ""], ["Masood", "Mudassir", ""], ["Ballal", "Tarig", ""], ["Shadaydeh", "Maha", ""], ["Al-Naffouri", "Tareq Y.", ""]]}, {"id": "1609.02948", "submitter": "Ruichi Yu", "authors": "Ruichi Yu, Xi Chen, Vlad I. Morariu, Larry S. Davis", "title": "The Role of Context Selection in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the reasons why context in object detection has limited\nutility by isolating and evaluating the predictive power of different context\ncues under ideal conditions in which context provided by an oracle. Based on\nthis study, we propose a region-based context re-scoring method with dynamic\ncontext selection to remove noise and emphasize informative context. We\nintroduce latent indicator variables to select (or ignore) potential contextual\nregions, and learn the selection strategy with latent-SVM. We conduct\nexperiments to evaluate the performance of the proposed context selection\nmethod on the SUN RGB-D dataset. The method achieves a significant improvement\nin terms of mean average precision (mAP), compared with both appearance based\ndetectors and a conventional context model without the selection scheme.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 21:30:14 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Yu", "Ruichi", ""], ["Chen", "Xi", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1609.02974", "submitter": "Nima Khademi Kalantari", "authors": "Nima Khademi Kalantari, Ting-Chun Wang, Ravi Ramamoorthi", "title": "Learning-Based View Synthesis for Light Field Cameras", "comments": "in ACM Transactions on Graphics 2016", "journal-ref": null, "doi": "10.1145/2980179.2980251", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of consumer light field cameras, light field imaging\nhas recently become widespread. However, there is an inherent trade-off between\nthe angular and spatial resolution, and thus, these cameras often sparsely\nsample in either spatial or angular domain. In this paper, we use machine\nlearning to mitigate this trade-off. Specifically, we propose a novel\nlearning-based approach to synthesize new views from a sparse set of input\nviews. We build upon existing view synthesis techniques and break down the\nprocess into disparity and color estimation components. We use two sequential\nconvolutional neural networks to model these two components and train both\nnetworks simultaneously by minimizing the error between the synthesized and\nground truth images. We show the performance of our approach using only four\ncorner sub-aperture views from the light fields captured by the Lytro Illum\ncamera. Experimental results show that our approach synthesizes high-quality\nimages that are superior to the state-of-the-art techniques on a variety of\nchallenging real-world scenes. We believe our method could potentially decrease\nthe required angular resolution of consumer light field cameras, which allows\ntheir spatial resolution to increase.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 23:33:38 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kalantari", "Nima Khademi", ""], ["Wang", "Ting-Chun", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1609.02994", "submitter": "Marco Visentini-Scarzanella", "authors": "Takuto Hirukawa, Marco Visentini-Scarzanella, Hiroshi Kawasaki, Ryo\n  Furukawa, Shinsaku Hiura", "title": "Simultaneous independent image display technique on multiple 3D objects", "comments": "Accepted to ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new system to visualize depth-dependent patterns and images on\nsolid objects with complex geometry using multiple projectors. The system,\ndespite consisting of conventional passive LCD projectors, is able to project\ndifferent images and patterns depending on the spatial location of the object.\nThe technique is based on the simple principle that multiple patterns projected\nfrom multiple projectors interfere constructively with each other when their\npatterns are projected on the same object. Previous techniques based on the\nsame principle can only achieve 1) low resolution volume colorization or 2)\nhigh resolution images but only on a limited number of flat planes. In this\npaper, we discretize a 3D object into a number of 3D points so that high\nresolution images can be projected onto the complex shapes. We also propose a\ndynamic ranges expansion technique as well as an efficient optimization\nprocedure based on epipolar constraints.\n  Such technique can be used to the extend projection mapping to have spatial\ndependency, which is desirable for practical applications. We also demonstrate\nthe system potential as a visual instructor for object placement and\nassembling. Experiments prove the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 02:16:51 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hirukawa", "Takuto", ""], ["Visentini-Scarzanella", "Marco", ""], ["Kawasaki", "Hiroshi", ""], ["Furukawa", "Ryo", ""], ["Hiura", "Shinsaku", ""]]}, {"id": "1609.03024", "submitter": "Keting Zhang", "authors": "Keting Zhang and Liqing Zhang", "title": "Rectifier Neural Network with a Dual-Pathway Architecture for Image\n  Denoising", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks based on tanh activation function have shown\ntheir impressive power in image denoising. In this letter, we try to use\nrectifier function instead of tanh and propose a dual-pathway rectifier neural\nnetwork by combining two rectifier neurons with reversed input and output\nweights in the same hidden layer. We drive the equivalent activation function\nand compare it to some typical activation functions for image denoising under\nthe same network architecture. The experimental results show that our model\nachieves superior performances faster especially when the noise is small.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 10:18:30 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 13:47:36 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 15:11:00 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhang", "Keting", ""], ["Zhang", "Liqing", ""]]}, {"id": "1609.03056", "submitter": "Yemin Shi Shi", "authors": "Yemin Shi, Yonghong Tian, Yaowei Wang, Tiejun Huang", "title": "Sequential Deep Trajectory Descriptor for Action Recognition with\n  Three-stream CNN", "comments": "10 pages, 29 figures, T-MM", "journal-ref": null, "doi": "10.1109/TMM.2017.2666540", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the spatial-temporal representation of motion information is crucial\nto human action recognition. Nevertheless, most of the existing features or\ndescriptors cannot capture motion information effectively, especially for\nlong-term motion. To address this problem, this paper proposes a long-term\nmotion descriptor called sequential Deep Trajectory Descriptor (sDTD).\nSpecifically, we project dense trajectories into two-dimensional planes, and\nsubsequently a CNN-RNN network is employed to learn an effective representation\nfor long-term motion. Unlike the popular two-stream ConvNets, the sDTD stream\nis introduced into a three-stream framework so as to identify actions from a\nvideo sequence. Consequently, this three-stream framework can simultaneously\ncapture static spatial features, short-term motion and long-term motion in the\nvideo. Extensive experiments were conducted on three challenging datasets: KTH,\nHMDB51 and UCF101. Experimental results show that our method achieves\nstate-of-the-art performance on the KTH and UCF101 datasets, and is comparable\nto the state-of-the-art methods on the HMDB51 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 14:24:38 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 02:49:10 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Shi", "Yemin", ""], ["Tian", "Yonghong", ""], ["Wang", "Yaowei", ""], ["Huang", "Tiejun", ""]]}, {"id": "1609.03057", "submitter": "Michael Elad", "authors": "Michael Elad and Peyman Milanfar", "title": "Style-Transfer via Texture-Synthesis", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2678168", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style-transfer is a process of migrating a style from a given image to the\ncontent of another, synthesizing a new image which is an artistic mixture of\nthe two. Recent work on this problem adopting Convolutional Neural-networks\n(CNN) ignited a renewed interest in this field, due to the very impressive\nresults obtained. There exists an alternative path towards handling the\nstyle-transfer task, via generalization of texture-synthesis algorithms. This\napproach has been proposed over the years, but its results are typically less\nimpressive compared to the CNN ones.\n  In this work we propose a novel style-transfer algorithm that extends the\ntexture-synthesis work of Kwatra et. al. (2005), while aiming to get stylized\nimages that get closer in quality to the CNN ones. We modify Kwatra's algorithm\nin several key ways in order to achieve the desired transfer, with emphasis on\na consistent way for keeping the content intact in selected regions, while\nproducing hallucinated and rich style in others. The results obtained are\nvisually pleasing and diverse, shown to be competitive with the recent CNN\nstyle-transfer algorithms. The proposed algorithm is fast and flexible, being\nable to process any pair of content + style images.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 14:26:48 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 14:20:24 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 05:41:08 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1609.03058", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Zhou, Hongteng Xu, Junchi Yan, Mingliang Xu, Jianxin\n  Wu, Zicheng Liu", "title": "A Tube-and-Droplet-based Approach for Representing and Analyzing Motion\n  Trajectories", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, DOI:\n  10.1109/TPAMI.2016.2608884, 2016. Code for our work is available at\n  http://min.sjtu.edu.cn/lwydemo/Trajectory%20analysis.htm", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2608884", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory analysis is essential in many applications. In this paper, we\naddress the problem of representing motion trajectories in a highly informative\nway, and consequently utilize it for analyzing trajectories. Our approach first\nleverages the complete information from given trajectories to construct a\nthermal transfer field which provides a context-rich way to describe the global\nmotion pattern in a scene. Then, a 3D tube is derived which depicts an input\ntrajectory by integrating its surrounding motion patterns contained in the\nthermal transfer field. The 3D tube effectively: 1) maintains the movement\ninformation of a trajectory, 2) embeds the complete contextual motion pattern\naround a trajectory, 3) visualizes information about a trajectory in a clear\nand unified way. We further introduce a droplet-based process. It derives a\ndroplet vector from a 3D tube, so as to characterize the high-dimensional 3D\ntube information in a simple but effective way. Finally, we apply our\ntube-and-droplet representation to trajectory analysis applications including\ntrajectory clustering, trajectory classification & abnormality detection, and\n3D action recognition. Experimental comparisons with state-of-the-art\nalgorithms demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 14:33:06 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 12:17:26 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Lin", "Weiyao", ""], ["Zhou", "Yang", ""], ["Xu", "Hongteng", ""], ["Yan", "Junchi", ""], ["Xu", "Mingliang", ""], ["Wu", "Jianxin", ""], ["Liu", "Zicheng", ""]]}, {"id": "1609.03093", "submitter": "Maciej Wielgosz", "authors": "Maciej Wielgosz and Marcin Pietro\\'n", "title": "Using Spatial Pooler of Hierarchical Temporal Memory to classify noisy\n  videos with predefined complexity", "comments": "submitted to Neurocomputing; paper similar to arXiv:1608.01966", "journal-ref": "Neurocomputing 240 (2017), 84-97", "doi": "10.1016/j.neucom.2017.02.046", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the performance of a Spatial Pooler (SP) of a\nHierarchical Temporal Memory (HTM) in the task of noisy object recognition. To\naddress this challenge, a dedicated custom-designed system based on the SP,\nhistogram calculation module and SVM classifier was implemented. In addition to\nimplementing their own version of HTM, the authors also designed a profiler\nwhich is capable of tracing all of the key parameters of the system. This was\nnecessary, since an analysis and monitoring of the system performance turned\nout to be extremely difficult using conventional testing and debugging tools.\nThe system was initially trained on artificially prepared videos without noise\nand then tested with a set of noisy video streams. This approach was intended\nto mimic a real life scenario where an agent or a system trained to deal with\nideal objects faces a task of classifying distorted and noisy ones in its\nregular working conditions. The authors conducted a series of experiments for\nvarious macro parameters of HTM SP, as well as for different levels of video\nreduction ratios. The experiments allowed them to evaluate the performance of\ntwo different system setups (i.e. 'Multiple HTMs' and 'Single HTM') under\nvarious noise conditions with 32--frame video files. Results of all the tests\nwere compared to SVM baseline setup. It was determined that the system\nfeaturing SP is capable of achieving approximately 12 times the noise reduction\nfor a video signal with with distorted bits accounting for 13\\% of the total.\nFurthermore, the system featuring SP performed better also in the experiments\nwithout a noise component and achieved a max F1 score of 0.96. The experiments\nalso revealed that a rise of column and synapse number of SP has a substantial\nimpact on the performance of the system. Consequently, the highest F1 score\nvalues were obtained for 256 and 4096 synapses and columns respectively.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 21:51:54 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 11:13:40 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Wielgosz", "Maciej", ""], ["Pietro\u0144", "Marcin", ""]]}, {"id": "1609.03140", "submitter": "Davide Modolo", "authors": "Davide Modolo and Vittorio Ferrari", "title": "Learning Semantic Part-Based Models from Google Images", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2017.2724029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique to train semantic part-based models of object classes\nfrom Google Images. Our models encompass the appearance of parts and their\nspatial arrangement on the object, specific to each viewpoint. We learn these\nrich models by collecting training instances for both parts and objects, and\nautomatically connecting the two levels. Our framework works incrementally, by\nlearning from easy examples first, and then gradually adapting to harder ones.\nA key benefit of this approach is that it requires no manual part location\nannotations. We evaluate our models on the challenging PASCAL-Part dataset [1]\nand show how their performance increases at every step of the learning, with\nthe final models more than doubling the performance of directly training from\nimages retrieved by querying for part names (from 12.9 to 27.2 AP). Moreover,\nwe show that our part models can help object detection performance by enriching\nthe R-CNN detector with parts.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 09:52:56 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 17:14:14 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Modolo", "Davide", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1609.03277", "submitter": "Mahamad Suhil", "authors": "Sumithra R, Mahamad Suhil, D.S. Guru", "title": "Segmentation and Classification of Skin Lesions for Disease Diagnosis", "comments": "10 pages, 6 figures, 2 Tables in Elsevier, Proceedia Computer\n  Science, International Conference on Advanced Computing Technologies and\n  Applications (ICACTA-2015)", "journal-ref": null, "doi": "10.1016/j.procs.2015.03.090", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach for automatic segmentation and classification\nof skin lesions is proposed. Initially, skin images are filtered to remove\nunwanted hairs and noise and then the segmentation process is carried out to\nextract lesion areas. For segmentation, a region growing method is applied by\nautomatic initialization of seed points. The segmentation performance is\nmeasured with different well known measures and the results are appreciable.\nSubsequently, the extracted lesion areas are represented by color and texture\nfeatures. SVM and k-NN classifiers are used along with their fusion for the\nclassification using the extracted features. The performance of the system is\ntested on our own dataset of 726 samples from 141 images consisting of 5\ndifferent classes of diseases. The results are very promising with 46.71% and\n34% of F-measure using SVM and k-NN classifier respectively and with 61% of\nF-measure for fusion of SVM and k-NN.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 06:05:55 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["R", "Sumithra", ""], ["Suhil", "Mahamad", ""], ["Guru", "D. S.", ""]]}, {"id": "1609.03279", "submitter": "Yuan Gao", "authors": "Yuan Gao, Jiayi Ma, and Alan L. Yuille", "title": "Semi-Supervised Sparse Representation Based Classification for Face\n  Recognition with Insufficient Labeled Samples", "comments": "To appear in IEEE Transactions on Image Processing, 2017", "journal-ref": null, "doi": "10.1109/TIP.2017.2675341", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of face recognition when there is only few,\nor even only a single, labeled examples of the face that we wish to recognize.\nMoreover, these examples are typically corrupted by nuisance variables, both\nlinear (i.e., additive nuisance variables such as bad lighting, wearing of\nglasses) and non-linear (i.e., non-additive pixel-wise nuisance variables such\nas expression changes). The small number of labeled examples means that it is\nhard to remove these nuisance variables between the training and testing faces\nto obtain good recognition performance. To address the problem we propose a\nmethod called Semi-Supervised Sparse Representation based Classification\n(S$^3$RC). This is based on recent work on sparsity where faces are represented\nin terms of two dictionaries: a gallery dictionary consisting of one or more\nexamples of each person, and a variation dictionary representing linear\nnuisance variables (e.g., different lighting conditions, different glasses).\nThe main idea is that (i) we use the variation dictionary to characterize the\nlinear nuisance variables via the sparsity framework, then (ii) prototype face\nimages are estimated as a gallery dictionary via a Gaussian Mixture Model\n(GMM), with mixed labeled and unlabeled samples in a semi-supervised manner, to\ndeal with the non-linear nuisance variations between labeled and unlabeled\nsamples. We have done experiments with insufficient labeled samples, even when\nthere is only a single labeled sample per person. Our results on the AR,\nMulti-PIE, CAS-PEAL, and LFW databases demonstrate that the proposed method is\nable to deliver significantly improved performance over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 06:45:15 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 06:26:12 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Gao", "Yuan", ""], ["Ma", "Jiayi", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1609.03302", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xin Liu, Ziheng Zhou, Xiaohua Huang, Jingang Shi,\n  Zhenhong Shang, Lan Tang, Yechao Bai, Qiong Wang, Xinggan Zhang", "title": "Image denoising via group sparsity residual constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group sparsity has shown great potential in various low-level vision tasks\n(e.g, image denoising, deblurring and inpainting). In this paper, we propose a\nnew prior model for image denoising via group sparsity residual constraint\n(GSRC). To enhance the performance of group sparse-based image denoising, the\nconcept of group sparsity residual is proposed, and thus, the problem of image\ndenoising is translated into one that reduces the group sparsity residual. To\nreduce the residual, we first obtain some good estimation of the group sparse\ncoefficients of the original image by the first-pass estimation of noisy image,\nand then centralize the group sparse coefficients of noisy image to the\nestimation. Experimental results have demonstrated that the proposed method not\nonly outperforms many state-of-the-art denoising methods such as BM3D and WNNM,\nbut results in a faster speed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 08:34:59 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 01:39:08 GMT"}, {"version": "v3", "created": "Fri, 30 Dec 2016 01:04:50 GMT"}, {"version": "v4", "created": "Thu, 2 Mar 2017 02:08:44 GMT"}, {"version": "v5", "created": "Fri, 3 Mar 2017 01:17:39 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Liu", "Xin", ""], ["Zhou", "Ziheng", ""], ["Huang", "Xiaohua", ""], ["Shi", "Jingang", ""], ["Shang", "Zhenhong", ""], ["Tang", "Lan", ""], ["Bai", "Yechao", ""], ["Wang", "Qiong", ""], ["Zhang", "Xinggan", ""]]}, {"id": "1609.03396", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Aayush Ankit, Parami Wijesinghe, and Kaushik Roy", "title": "FALCON: Feature Driven Selective Classification for Energy-Efficient\n  Image Recognition", "comments": "13 pages, 13 figures, Accepted for publication in IEEE TCAD, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning algorithms have shown outstanding image recognition or\nclassification performance for computer vision applications. However, the\ncompute and energy requirement for implementing such classifier models for\nlarge-scale problems is quite high. In this paper, we propose Feature Driven\nSelective Classification (FALCON) inspired by the biological visual attention\nmechanism in the brain to optimize the energy-efficiency of machine-learning\nclassifiers. We use the consensus in the characteristic features\n(color/texture) across images in a dataset to decompose the original\nclassification problem and construct a tree of classifiers (nodes) with a\ngeneric-to-specific transition in the classification hierarchy. The initial\nnodes of the tree separate the instances based on feature information and\nselectively enable the latter nodes to perform object specific classification.\nThe proposed methodology allows selective activation of only those branches and\nnodes of the classification tree that are relevant to the input while keeping\nthe remaining nodes idle. Additionally, we propose a programmable and scalable\nNeuromorphic Engine (NeuE) that utilizes arrays of specialized neural\ncomputational elements to execute the FALCON based classifier models for\ndiverse datasets. The structure of FALCON facilitates the reuse of nodes while\nscaling up from small classification problems to larger ones thus allowing us\nto construct classifier implementations that are significantly more efficient.\nWe evaluate our approach for a 12-object classification task on the Caltech101\ndataset and 10-object task on CIFAR-10 dataset by constructing FALCON models on\nthe NeuE platform in 45nm technology. Our results demonstrate significant\nimprovement in energy-efficiency and training time for minimal loss in output\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 13:40:13 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 15:16:19 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Ankit", "Aayush", ""], ["Wijesinghe", "Parami", ""], ["Roy", "Kaushik", ""]]}, {"id": "1609.03415", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan and S. Saqib Bukhari and Thomas M. Breuel", "title": "Active Canny: Edge Detection and Recovery with Open Active Contour\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an edge detection and recovery framework based on open active\ncontour models (snakelets). This is motivated by the noisy or broken edges\noutput by standard edge detection algorithms, like Canny. The idea is to\nutilize the local continuity and smoothness cues provided by strong edges and\ngrow them to recover the missing edges. This way, the strong edges are used to\nrecover weak or missing edges by considering the local edge structures, instead\nof blindly linking them if gradient magnitudes are above some threshold. We\ninitialize short snakelets on the gradient magnitudes or binary edges\nautomatically and then deform and grow them under the influence of gradient\nvector flow. The output snakelets are able to recover most of the breaks or\nweak edges, and they provide a smooth edge representation of the image; they\ncan also be used for higher level analysis, like contour segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 14:13:26 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Bastan", "Muhammet", ""], ["Bukhari", "S. Saqib", ""], ["Breuel", "Thomas M.", ""]]}, {"id": "1609.03461", "submitter": "Hossein Ziaei Nafchi", "authors": "Hossein Ziaei Nafchi, Atena Shahkolaei, Rachid Hedjam, Mohamed Cheriet", "title": "MUG: A Parameterless No-Reference JPEG Quality Evaluator Robust to Block\n  Size and Misalignment", "comments": "5 pages, 4 figures, 3 tables", "journal-ref": null, "doi": "10.1109/LSP.2016.2608865", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, a very simple no-reference image quality assessment (NR-IQA)\nmodel for JPEG compressed images is proposed. The proposed metric called median\nof unique gradients (MUG) is based on the very simple facts of unique gradient\nmagnitudes of JPEG compressed images. MUG is a parameterless metric and does\nnot need training. Unlike other NR-IQAs, MUG is independent to block size and\ncropping. A more stable index called MUG+ is also introduced. The experimental\nresults on six benchmark datasets of natural images and a benchmark dataset of\nsynthetic images show that MUG is comparable to the state-of-the-art indices in\nliterature. In addition, its performance remains unchanged for the case of the\ncropped images in which block boundaries are not known. The MATLAB source code\nof the proposed metrics is available at\nhttps://dl.dropboxusercontent.com/u/74505502/MUG.m and\nhttps://dl.dropboxusercontent.com/u/74505502/MUGplus.m.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 16:11:26 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 16:33:48 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Nafchi", "Hossein Ziaei", ""], ["Shahkolaei", "Atena", ""], ["Hedjam", "Rachid", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1609.03500", "submitter": "Sheng Zou", "authors": "Sheng Zou and Alina Zare", "title": "Hyperspectral Unmixing with Endmember Variability using Partial\n  Membership Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Partial Membership Latent Dirichlet Allocation(PM-LDA) for\nhyperspectral endmember estimation and spectral unmixing is presented. PM-LDA\nprovides a model for a hyperspectral image analysis that accounts for spectral\nvariability and incorporates spatial information through the use of\nsuperpixel-based 'documents.' In our application of PM-LDA, we employ the\nNormal Compositional Model in which endmembers are represented as Normal\ndistributions to account for spectral variability and proportion vectors are\nmodeled as random variables governed by a Dirichlet distribution. The use of\nthe Dirichlet distribution enforces positivity and sum-to-one constraints on\nthe proportion values. Algorithm results on real hyperspectral data indicate\nthat PM-LDA produces endmember distributions that represent the ground truth\nclasses and their associated variability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 17:32:41 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Zou", "Sheng", ""], ["Zare", "Alina", ""]]}, {"id": "1609.03529", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Jayadeva, Sumeet Agarwal", "title": "Examining Representational Similarity in ConvNets and the Primate Visual\n  Cortex", "comments": "4 pages, short abstract, Accepted to the Workshop on Biological and\n  Artificial Vision, ECCV, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare several ConvNets with different depth and regularization\ntechniques with multi-unit macaque IT cortex recordings and assess the impact\nof the same on representational similarity with the primate visual cortex. We\nfind that with increasing depth and validation performance, ConvNet features\nare closer to cortical IT representations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:00:24 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Jayadeva", "", ""], ["Agarwal", "Sumeet", ""]]}, {"id": "1609.03532", "submitter": "James Thewlis", "authors": "James Thewlis, Shuai Zheng, Philip H. S. Torr, Andrea Vedaldi", "title": "Fully-Trainable Deep Matching", "comments": "British Machine Vision Conference (BMVC) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Matching (DM) is a popular high-quality method for quasi-dense image\nmatching. Despite its name, however, the original DM formulation does not yield\na deep neural network that can be trained end-to-end via backpropagation. In\nthis paper, we remove this limitation by rewriting the complete DM algorithm as\na convolutional neural network. This results in a novel deep architecture for\nimage matching that involves a number of new layer types and that, similar to\nrecent networks for image segmentation, has a U-topology. We demonstrate the\nutility of the approach by improving the performance of DM by learning it\nend-to-end on an image matching task.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:07:04 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Thewlis", "James", ""], ["Zheng", "Shuai", ""], ["Torr", "Philip H. S.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1609.03536", "submitter": "Zhenheng Yang", "authors": "Zhenheng Yang and Ram Nevatia", "title": "A Multi-Scale Cascade Fully Convolutional Network Face Detector", "comments": "Accepted to ICPR 16'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is challenging as faces in images could be present at\narbitrary locations and in different scales. We propose a three-stage cascade\nstructure based on fully convolutional neural networks (FCNs). It first\nproposes the approximate locations where the faces may be, then aims to find\nthe accurate location by zooming on to the faces. Each level of the FCN cascade\nis a multi-scale fully-convolutional network, which generates scores at\ndifferent locations and in different scales. A score map is generated after\neach FCN stage. Probable regions of face are selected and fed to the next\nstage. The number of proposals is decreased after each level, and the areas of\nregions are decreased to more precisely fit the face. Compared to passing\nproposals directly between stages, passing probable regions can decrease the\nnumber of proposals and reduce the cases where first stage doesn't propose good\nbounding boxes. We show that by using FCN and score map, the FCN cascade face\ndetector can achieve strong performance on public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:13:46 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Yang", "Zhenheng", ""], ["Nevatia", "Ram", ""]]}, {"id": "1609.03545", "submitter": "Hayko Riemenschneider", "authors": "Julien Weissenberg and Hayko Riemenschneider and Ralf Dragon and Luc\n  Van Gool", "title": "Dilemma First Search for Effortless Optimization of NP-Hard Problems", "comments": "To be published at ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle the exponentiality associated with NP-hard problems, two paradigms\nhave been proposed. First, Branch & Bound, like Dynamic Programming, achieve\nefficient exact inference but requires extensive information and analysis about\nthe problem at hand. Second, meta-heuristics are easier to implement but\ncomparatively inefficient. As a result, a number of problems have been left\nunoptimized and plain greedy solutions are used. We introduce a theoretical\nframework and propose a powerful yet simple search method called Dilemma First\nSearch (DFS). DFS exploits the decision heuristic needed for the greedy\nsolution for further optimization. DFS is useful when it is hard to design\nefficient exact inference. We evaluate DFS on two problems: First, the Knapsack\nproblem, for which efficient algorithms exist, serves as a toy example. Second,\nDecision Tree inference, where state-of-the-art algorithms rely on the greedy\nor randomness-based solutions. We further show that decision trees benefit from\noptimizations that are performed in a fraction of the iterations required by a\nrandom-based search.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:36:02 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Weissenberg", "Julien", ""], ["Riemenschneider", "Hayko", ""], ["Dragon", "Ralf", ""], ["Van Gool", "Luc", ""]]}, {"id": "1609.03552", "submitter": "Jun-Yan Zhu", "authors": "Jun-Yan Zhu, Philipp Kr\\\"ahenb\\\"uhl, Eli Shechtman, Alexei A. Efros", "title": "Generative Visual Manipulation on the Natural Image Manifold", "comments": "In European Conference on Computer Vision (ECCV 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic image manipulation is challenging because it requires modifying the\nimage appearance in a user-controlled way, while preserving the realism of the\nresult. Unless the user has considerable artistic skill, it is easy to \"fall\noff\" the manifold of natural images while editing. In this paper, we propose to\nlearn the natural image manifold directly from data using a generative\nadversarial neural network. We then define a class of image editing operations,\nand constrain their output to lie on that learned manifold at all times. The\nmodel automatically adjusts the output keeping all edits as realistic as\npossible. All our manipulations are expressed in terms of constrained\noptimization and are applied in near-real time. We evaluate our algorithm on\nthe task of realistic photo manipulation of shape and color. The presented\nmethod can further be used for changing one image to look like the other, as\nwell as generating novel imagery from scratch based on user's scribbles.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:46:08 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 09:03:31 GMT"}, {"version": "v3", "created": "Sun, 16 Dec 2018 22:00:59 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Shechtman", "Eli", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1609.03605", "submitter": "Weilin Huang", "authors": "Zhi Tian, Weilin Huang, Tong He, Pan He, and Yu Qiao", "title": "Detecting Text in Natural Image with Connectionist Text Proposal Network", "comments": "To appear in ECCV, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Connectionist Text Proposal Network (CTPN) that accurately\nlocalizes text lines in natural image. The CTPN detects a text line in a\nsequence of fine-scale text proposals directly in convolutional feature maps.\nWe develop a vertical anchor mechanism that jointly predicts location and\ntext/non-text score of each fixed-width proposal, considerably improving\nlocalization accuracy. The sequential proposals are naturally connected by a\nrecurrent neural network, which is seamlessly incorporated into the\nconvolutional network, resulting in an end-to-end trainable model. This allows\nthe CTPN to explore rich context information of image, making it powerful to\ndetect extremely ambiguous text. The CTPN works reliably on multi-scale and\nmulti- language text without further post-processing, departing from previous\nbottom-up methods requiring multi-step post-processing. It achieves 0.88 and\n0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpass- ing recent\nresults [8, 35] by a large margin. The CTPN is computationally efficient with\n0:14s/image, by using the very deep VGG16 model [27]. Online demo is available\nat: http://textdet.com/.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 21:12:46 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Tian", "Zhi", ""], ["Huang", "Weilin", ""], ["He", "Tong", ""], ["He", "Pan", ""], ["Qiao", "Yu", ""]]}, {"id": "1609.03619", "submitter": "Wentao Luan", "authors": "Wentao Luan, Yezhou Yang, Cornelia Fermuller, John Baras", "title": "Reliable Attribute-Based Object Recognition Using High Predictive Value\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of object recognition in 3D using an ensemble of\nattribute-based classifiers. We propose two new concepts to improve\nclassification in practical situations, and show their implementation in an\napproach implemented for recognition from point-cloud data. First, the viewing\nconditions can have a strong influence on classification performance. We study\nthe impact of the distance between the camera and the object and propose an\napproach to fuse multiple attribute classifiers, which incorporates distance\ninto the decision making. Second, lack of representative training samples often\nmakes it difficult to learn the optimal threshold value for best positive and\nnegative detection rate. We address this issue, by setting in our attribute\nclassifiers instead of just one threshold value, two threshold values to\ndistinguish a positive, a negative and an uncertainty class, and we prove the\ntheoretical correctness of this approach. Empirical studies demonstrate the\neffectiveness and feasibility of the proposed concepts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 22:20:12 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 03:38:27 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Luan", "Wentao", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Baras", "John", ""]]}, {"id": "1609.03659", "submitter": "Wei Shen", "authors": "Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Xiang Bai and Alan Yuille", "title": "DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for\n  Object Skeleton Extraction in Natural Images", "comments": "submitted to TIP. arXiv admin note: substantial text overlap with\n  arXiv:1603.09446", "journal-ref": null, "doi": "10.1109/TIP.2017.2735182", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object skeletons are useful for object representation and object detection.\nThey are complementary to the object contour, and provide extra information,\nsuch as how object scale (thickness) varies among object parts. But object\nskeleton extraction from natural images is very challenging, because it\nrequires the extractor to be able to capture both local and non-local image\ncontext in order to determine the scale of each skeleton pixel. In this paper,\nwe present a novel fully convolutional network with multiple scale-associated\nside outputs to address this problem. By observing the relationship between the\nreceptive field sizes of the different layers in the network and the skeleton\nscales they can capture, we introduce two scale-associated side outputs to each\nstage of the network. The network is trained by multi-task learning, where one\ntask is skeleton localization to classify whether a pixel is a skeleton pixel\nor not, and the other is skeleton scale prediction to regress the scale of each\nskeleton pixel. Supervision is imposed at different stages by guiding the\nscale-associated side outputs toward the groundtruth skeletons at the\nappropriate scales. The responses of the multiple scale-associated side outputs\nare then fused in a scale-specific way to detect skeleton pixels using multiple\nscales effectively. Our method achieves promising results on two skeleton\nextraction datasets, and significantly outperforms other competitors.\nAdditionally, the usefulness of the obtained skeletons and scales (thickness)\nare verified on two object detection applications: Foreground object\nsegmentation and object proposal detection.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 02:23:39 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 18:55:20 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 19:39:16 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Shen", "Wei", ""], ["Zhao", "Kai", ""], ["Jiang", "Yuan", ""], ["Wang", "Yan", ""], ["Bai", "Xiang", ""], ["Yuille", "Alan", ""]]}, {"id": "1609.03677", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Oisin Mac Aodha and Gabriel J. Brostow", "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency", "comments": "CVPR 2017 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based methods have shown very promising results for the task of\ndepth estimation in single images. However, most existing approaches treat\ndepth prediction as a supervised regression problem and as a result, require\nvast quantities of corresponding ground truth depth data for training. Just\nrecording quality depth data in a range of environments is a challenging\nproblem. In this paper, we innovate beyond existing approaches, replacing the\nuse of explicit depth data during training with easier-to-obtain binocular\nstereo footage.\n  We propose a novel training objective that enables our convolutional neural\nnetwork to learn to perform single image depth estimation, despite the absence\nof ground truth depth data. Exploiting epipolar geometry constraints, we\ngenerate disparity images by training our network with an image reconstruction\nloss. We show that solving for image reconstruction alone results in poor\nquality depth images. To overcome this problem, we propose a novel training\nloss that enforces consistency between the disparities produced relative to\nboth the left and right images, leading to improved performance and robustness\ncompared to existing approaches. Our method produces state of the art results\nfor monocular depth estimation on the KITTI driving dataset, even outperforming\nsupervised methods that have been trained with ground truth depth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 04:48:31 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 17:18:17 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:40:50 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Mac Aodha", "Oisin", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1609.03759", "submitter": "Edward Johns", "authors": "Stephen James and Edward Johns", "title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "comments": "In NIPS 2016 Workshop: Deep Learning for Action and Interaction\n  (https://sites.google.com/site/nips16interaction/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in robot arm control have seen a shift towards end-to-end\nsolutions, using deep reinforcement learning to learn a controller directly\nfrom raw sensor data, rather than relying on a hand-crafted, modular pipeline.\nHowever, the high dimensionality of the state space often means that it is\nimpractical to generate sufficient training data with real-world experiments.\nAs an alternative solution, we propose to learn a robot controller in\nsimulation, with the potential of then transferring this to a real robot.\nBuilding upon the recent success of deep Q-networks, we present an approach\nwhich uses 3D simulations to train a 7-DOF robotic arm in a control task\nwithout any prior knowledge. The controller accepts images of the environment\nas its only input, and outputs motor actions for the task of locating and\ngrasping a cube, over a range of initial configurations. To encourage efficient\nlearning, a structured reward function is designed with intermediate rewards.\nWe also present preliminary results in direct transfer of policies over to a\nreal robot, without any further training.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 10:40:24 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 16:09:17 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["James", "Stephen", ""], ["Johns", "Edward", ""]]}, {"id": "1609.03773", "submitter": "Li Cheng", "authors": "Chi Xu, Lakshmi Narasimhan Govindarajan, Yu Zhang, Li Cheng", "title": "Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking,\n  and Action Recognition on Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation, tracking, and action recognition of articulated objects from\ndepth images are important and challenging problems, which are normally\nconsidered separately. In this paper, a unified paradigm based on Lie group\ntheory is proposed, which enables us to collectively address these related\nproblems. Our approach is also applicable to a wide range of articulated\nobjects. Empirically it is evaluated on lab animals including mouse and fish,\nas well as on human hand. On these applications, it is shown to deliver\ncompetitive results compared to the state-of-the-arts, and non-trivial\nbaselines including convolutional neural networks and regression forest\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:36:26 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Xu", "Chi", ""], ["Govindarajan", "Lakshmi Narasimhan", ""], ["Zhang", "Yu", ""], ["Cheng", "Li", ""]]}, {"id": "1609.03795", "submitter": "Domen Tabernik", "authors": "Domen Tabernik, Matej Kristan, Jeremy L. Wyatt, and Ale\\v{s} Leonardis", "title": "Towards Deep Compositional Networks", "comments": "Published in proceedings of 23th International Conference on Pattern\n  Recognition (ICPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical feature learning based on convolutional neural networks (CNN)\nhas recently shown significant potential in various computer vision tasks.\nWhile allowing high-quality discriminative feature learning, the downside of\nCNNs is the lack of explicit structure in features, which often leads to\noverfitting, absence of reconstruction from partial observations and limited\ngenerative abilities. Explicit structure is inherent in hierarchical\ncompositional models, however, these lack the ability to optimize a\nwell-defined cost function. We propose a novel analytic model of a basic unit\nin a layered hierarchical model with both explicit compositional structure and\na well-defined discriminative cost function. Our experiments on two datasets\nshow that the proposed compositional model performs on a par with standard CNNs\non discriminative tasks, while, due to explicit modeling of the structure in\nthe feature units, affording a straight-forward visualization of parts and\nfaster inference due to separability of the units. Actions\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 12:31:29 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Tabernik", "Domen", ""], ["Kristan", "Matej", ""], ["Wyatt", "Jeremy L.", ""], ["Leonardis", "Ale\u0161", ""]]}, {"id": "1609.03815", "submitter": "Songcan Chen", "authors": "Qing Tian, Songcan Chen, and Xiaoyang Tan", "title": "A Unified Gender-Aware Age Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human age estimation has attracted increasing researches due to its wide\napplicability in such as security monitoring and advertisement recommendation.\nAlthough a variety of methods have been proposed, most of them focus only on\nthe age-specific facial appearance. However, biological researches have shown\nthat not only gender but also the aging difference between the male and the\nfemale inevitably affect the age estimation. To our knowledge, so far there\nhave been two methods that have concerned the gender factor. The first is a\nsequential method which first classifies the gender and then performs age\nestimation respectively for classified male and female. Although it promotes\nage estimation performance because of its consideration on the gender semantic\ndifference, an accumulation risk of estimation errors is unavoidable. To\novercome drawbacks of the sequential strategy, the second is to regress the age\nappended with the gender by concatenating their labels as two dimensional\noutput using Partial Least Squares (PLS). Although leading to promotion of age\nestimation performance, such a concatenation not only likely confuses the\nsemantics between the gender and age, but also ignores the aging discrepancy\nbetween the male and the female. In order to overcome their shortcomings, in\nthis paper we propose a unified framework to perform gender-aware age\nestimation. The proposed method considers and utilizes not only the semantic\nrelationship between the gender and the age, but also the aging discrepancy\nbetween the male and the female. Finally, experimental results demonstrate not\nonly the superiority of our method in performance, but also its good\ninterpretability in revealing the aging discrepancy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 13:10:50 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Tian", "Qing", ""], ["Chen", "Songcan", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1609.03868", "submitter": "\\c{C}a\\u{g}lar Aytekin", "authors": "Caglar Aytekin, Alexandros Iosifidis, Moncef Gabbouj", "title": "Probabilistic Saliency Estimation", "comments": "Submitted to Pattern Recognition", "journal-ref": "Pattern Recognition Volume 74, February 2018, Pages 359-372", "doi": "10.1016/j.patcog.2017.09.023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model the salient object detection problem under a\nprobabilistic framework encoding the boundary connectivity saliency cue and\nsmoothness constraints in an optimization problem. We show that this problem\nhas a closed form global optimum which estimates the salient object. We further\nshow that along with the probabilistic framework, the proposed method also\nenjoys a wide range of interpretations, i.e. graph cut, diffusion maps and\none-class classification. With an analysis according to these interpretations,\nwe also find that our proposed method provides approximations to the global\noptimum to another criterion that integrates local/global contrast and large\narea saliency cues. The proposed approach achieves mostly leading performance\ncompared to the state-of-the-art algorithms over a large set of salient object\ndetection datasets including around 17k images for several evaluation metrics.\nFurthermore, the computational complexity of the proposed method is\nfavorable/comparable to many state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 14:42:46 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 08:08:27 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Aytekin", "Caglar", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1609.03874", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yao Wang", "title": "Image Decomposition Using a Robust Regression Approach", "comments": "arXiv admin note: substantial text overlap with arXiv:1607.02547", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers how to separate text and/or graphics from smooth\nbackground in screen content and mixed content images and proposes an algorithm\nto perform this segmentation task. The proposed methods make use of the fact\nthat the background in each block is usually smoothly varying and can be\nmodeled well by a linear combination of a few smoothly varying basis functions,\nwhile the foreground text and graphics create sharp discontinuity. This\nalgorithm separates the background and foreground pixels by trying to fit pixel\nvalues in the block into a smooth function using a robust regression method.\nThe inlier pixels that can be well represented with the smooth model will be\nconsidered as background, while remaining outlier pixels will be considered\nforeground. We have also created a dataset of screen content images extracted\nfrom HEVC standard test sequences for screen content coding with their ground\ntruth segmentation result which can be used for this task. The proposed\nalgorithm has been tested on the dataset mentioned above and is shown to have\nsuperior performance over other methods, such as the hierarchical k-means\nclustering algorithm, shape primitive extraction and coding, and the least\nabsolute deviation fitting scheme for foreground segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 14:48:41 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 14:19:44 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1609.03892", "submitter": "Xin Liu", "authors": "Xin Liu, and Meina Kan, and Wanglong Wu, and Shiguang Shan, and Xilin\n  Chen", "title": "VIPLFaceNet: An Open Source Deep Face Recognition SDK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust face representation is imperative to highly accurate face recognition.\nIn this work, we propose an open source face recognition method with deep\nrepresentation named as VIPLFaceNet, which is a 10-layer deep convolutional\nneural network with 7 convolutional layers and 3 fully-connected layers.\nCompared with the well-known AlexNet, our VIPLFaceNet takes only 20% training\ntime and 60% testing time, but achieves 40\\% drop in error rate on the\nreal-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean\naccuracy on LFW using one single network. An open-source C++ SDK based on\nVIPLFaceNet is released under BSD license. The SDK takes about 150ms to process\none face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a\nstate-of-the-art start point for both academic and industrial face recognition\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 15:13:55 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Liu", "Xin", ""], ["Kan", "Meina", ""], ["Wu", "Wanglong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1609.03894", "submitter": "Francisco Massa", "authors": "Francisco Massa, Renaud Marlet, Mathieu Aubry", "title": "Crafting a multi-task CNN for viewpoint estimation", "comments": "To appear in BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) were recently shown to provide\nstate-of-the-art results for object category viewpoint estimation. However\ndifferent ways of formulating this problem have been proposed and the competing\napproaches have been explored with very different design choices. This paper\npresents a comparison of these approaches in a unified setting as well as a\ndetailed analysis of the key factors that impact performance. Followingly, we\npresent a new joint training method with the detection task and demonstrate its\nbenefit. We also highlight the superiority of classification approaches over\nregression approaches, quantify the benefits of deeper architectures and\nextended training data, and demonstrate that synthetic data is beneficial even\nwhen using ImageNet training data. By combining all these elements, we\ndemonstrate an improvement of approximately 5% mAVP over previous\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\nchallenging 24 view classification task we improve the results from 31.1% to\n36.1% mAVP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 15:19:38 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Massa", "Francisco", ""], ["Marlet", "Renaud", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1609.03947", "submitter": "Li Yang Ku", "authors": "Li Yang Ku, Erik Learned-Miller, Rod Grupen", "title": "Associating Grasp Configurations with Hierarchical Features in\n  Convolutional Neural Networks", "comments": "8 pages, 9 figures, IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide a solution for posturing the anthropomorphic\nRobonaut-2 hand and arm for grasping based on visual information. A mapping\nfrom visual features extracted from a convolutional neural network (CNN) to\ngrasp points is learned. We demonstrate that a CNN pre-trained for image\nclassification can be applied to a grasping task based on a small set of\ngrasping examples. Our approach takes advantage of the hierarchical nature of\nthe CNN by identifying features that capture the hierarchical support relations\nbetween filters in different CNN layers and locating their 3D positions by\ntracing activations backwards in the CNN. When this backward trace terminates\nin the RGB-D image, important manipulable structures are thereby localized.\nThese features that reside in different layers of the CNN are then associated\nwith controllers that engage different kinematic subchains in the hand/arm\nsystem for grasping. A grasping dataset is collected using demonstrated\nhand/object relationships for Robonaut-2 to evaluate the proposed approach in\nterms of the precision of the resulting preshape postures. We demonstrate that\nthis approach outperforms baseline approaches in cluttered scenarios on the\ngrasping dataset and a point cloud based approach on a grasping task using\nRobonaut-2.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 17:37:38 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 21:00:20 GMT"}, {"version": "v3", "created": "Fri, 21 Oct 2016 18:57:25 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 17:40:17 GMT"}, {"version": "v5", "created": "Wed, 26 Jul 2017 00:41:01 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Ku", "Li Yang", ""], ["Learned-Miller", "Erik", ""], ["Grupen", "Rod", ""]]}, {"id": "1609.03948", "submitter": "Lee Friedman Lee Friedman", "authors": "Lee Friedman, Ioannis Rigas, Mark S. Nixon, Oleg V. Komogortsev", "title": "Method to Assess the Temporal Persistence of Potential Biometric\n  Features: Application to Oculomotor, and Gait-Related Databases", "comments": "13 pages, 8 figures, 5 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0178501", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although temporal persistence, or permanence, is a well understood\nrequirement for optimal biometric features, there is no general agreement on\nhow to assess temporal persistence. We suggest that the best way to assess\ntemporal persistence is to perform a test-retest study, and assess test-retest\nreliability. For ratio-scale features that are normally distributed, this is\nbest done using the Intraclass Correlation Coefficient (ICC). For 10 distinct\ndata sets (8 eye-movement related, and 2 gait related), we calculated the\ntest-retest reliability ('Temporal persistence') of each feature, and compared\nbiometric performance of high-ICC features to lower ICC features, and to the\nset of all features. We demonstrate that using a subset of only high-ICC\nfeatures produced superior Rank-1-Identification Rate (Rank-1-IR) performance\nin 9 of 10 databases (p = 0.01, one-tailed). For Equal Error Rate (EER), using\na subset of only high-ICC features produced superior performance in 8 of 10\ndatabases (p = 0.055, one-tailed). In general, then, prescreening potential\nbiometric features, and choosing only highly reliable features will yield\nbetter performance than lower ICC features or than the set of all features\ncombined. We hypothesize that this would likely be the case for any biometric\nmodality where the features can be expressed as quantitative values on an\ninterval or ratio scale, assuming an adequate number of relatively independent\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 17:38:39 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Friedman", "Lee", ""], ["Rigas", "Ioannis", ""], ["Nixon", "Mark S.", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1609.03986", "submitter": "Tal Hassner", "authors": "Christopher Parker, Matthew Daiter, Kareem Omar, Gil Levi and Tal\n  Hassner", "title": "The CUDA LATCH Binary Descriptor: Because Sometimes Faster Means Better", "comments": "Accepted to ECCV'16 workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy, descriptor size, and the time required for extraction and matching\nare all important factors when selecting local image descriptors. To optimize\nover all these requirements, this paper presents a CUDA port for the recent\nLearned Arrangement of Three Patches (LATCH) binary descriptors to the GPU\nplatform. The design of LATCH makes it well suited for GPU processing. Owing to\nits small size and binary nature, the GPU can further be used to efficiently\nmatch LATCH features. Taken together, this leads to breakneck descriptor\nextraction and matching speeds. We evaluate the trade off between these speeds\nand the quality of results in a feature matching intensive application. To this\nend, we use our proposed CUDA LATCH (CLATCH) to recover structure from motion\n(SfM), comparing 3D reconstructions and speed using different representations.\nOur results show that CLATCH provides high quality 3D reconstructions at\nfractions of the time required by other representations, with little, if any,\nloss of reconstruction quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 19:24:02 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 00:51:19 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Parker", "Christopher", ""], ["Daiter", "Matthew", ""], ["Omar", "Kareem", ""], ["Levi", "Gil", ""], ["Hassner", "Tal", ""]]}, {"id": "1609.04079", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti, Kalyan Sunkavalli", "title": "Single-image RGB Photometric Stereo With Spatially-varying Albedo", "comments": "3DV 2016. Project page at http://www.ttic.edu/chakrabarti/rgbps/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a single-shot system to recover surface geometry of objects with\nspatially-varying albedos, from images captured under a calibrated RGB\nphotometric stereo setup---with three light directions multiplexed across\ndifferent color channels in the observed RGB image. Since the problem is\nill-posed point-wise, we assume that the albedo map can be modeled as\npiece-wise constant with a restricted number of distinct albedo values. We show\nthat under ideal conditions, the shape of a non-degenerate local constant\nalbedo surface patch can theoretically be recovered exactly. Moreover, we\npresent a practical and efficient algorithm that uses this model to robustly\nrecover shape from real images. Our method first reasons about shape locally in\na dense set of patches in the observed image, producing shape distributions for\nevery patch. These local distributions are then combined to produce a single\nconsistent surface normal map. We demonstrate the efficacy of the approach\nthrough experiments on both synthetic renderings as well as real captured\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 00:39:58 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Sunkavalli", "Kalyan", ""]]}, {"id": "1609.04103", "submitter": "Lei Han", "authors": "Lei Han, Juanzhen Sun, Wei Zhang, Yuanyuan Xiu, Hailei Feng, Yinjing\n  Lin", "title": "A Machine Learning Nowcasting Method based on Real-time Reanalysis Data", "comments": "22 pages, 11 figures, submitted to Journal of Geophysical Research:\n  Atmospheres", "journal-ref": "J. Geophys. Res. Atmos., 122, (2017)", "doi": "10.1002/2016JD025783", "report-no": null, "categories": "physics.ao-ph cs.CV physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite marked progress over the past several decades, convective storm\nnowcasting remains a challenge because most nowcasting systems are based on\nlinear extrapolation of radar reflectivity without much consideration for other\nmeteorological fields. The variational Doppler radar analysis system (VDRAS) is\nan advanced convective-scale analysis system capable of providing analysis of\n3-D wind, temperature, and humidity by assimilating Doppler radar observations.\nAlthough potentially useful, it is still an open question as to how to use\nthese fields to improve nowcasting. In this study, we present results from our\nfirst attempt at developing a Support Vector Machine (SVM) Box-based nOWcasting\n(SBOW) method under the machine learning framework using VDRAS analysis data.\nThe key design points of SBOW are as follows: 1) The study domain is divided\ninto many position-fixed small boxes and the nowcasting problem is transformed\ninto one question, i.e., will a radar echo > 35 dBZ appear in a box in 30\nminutes? 2) Box-based temporal and spatial features, which include time trends\nand surrounding environmental information, are elaborately constructed, and 3)\nThe box-based constructed features are used to first train the SVM classifier,\nand then the trained classifier is used to make predictions. Compared with\ncomplicated and expensive expert systems, the above design of SBOW allows the\nsystem to be small, compact, straightforward, and easy to maintain and expand\nat low cost. The experimental results show that, although no complicated\ntracking algorithm is used, SBOW can predict the storm movement trend and storm\ngrowth with reasonable skill.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 01:01:29 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 06:39:31 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Han", "Lei", ""], ["Sun", "Juanzhen", ""], ["Zhang", "Wei", ""], ["Xiu", "Yuanyuan", ""], ["Feng", "Hailei", ""], ["Lin", "Yinjing", ""]]}, {"id": "1609.04104", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Georgios B. Giannakis, and Kamil Ugurbil", "title": "Tracking Tensor Subspaces with Informative Random Sampling for Real-Time\n  MR Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) nowadays serves as an important modality for\ndiagnostic and therapeutic guidance in clinics. However, the {\\it slow\nacquisition} process, the dynamic deformation of organs, as well as the need\nfor {\\it real-time} reconstruction, pose major challenges toward obtaining\nartifact-free images. To cope with these challenges, the present paper\nadvocates a novel subspace learning framework that permeates benefits from\nparallel factor (PARAFAC) decomposition of tensors (multiway data) to low-rank\nmodeling of temporal sequence of images. Treating images as multiway data\narrays, the novel method preserves spatial structures and unravels the latent\ncorrelations across various dimensions by means of the tensor subspace.\nLeveraging the spatio-temporal correlation of images, Tykhonov regularization\nis adopted as a rank surrogate for a least-squares optimization program.\nAlteranating majorization minimization is adopted to develop online algorithms\nthat recursively procure the reconstruction upon arrival of a new undersampled\n$k$-space frame. The developed algorithms are {\\it provably convergent} and\nhighly {\\it parallelizable} with lightweight FFT tasks per iteration. To\nfurther accelerate the acquisition process, randomized subsampling policies are\ndevised that leverage intermediate estimates of the tensor subspace, offered by\nthe online scheme, to {\\it randomly} acquire {\\it informative} $k$-space\nsamples. In a nutshell, the novel approach enables tracking motion dynamics\nunder low acquisition rates `on the fly.' GPU-based tests with real {\\it in\nvivo} MRI datasets of cardiac cine images corroborate the merits of the novel\napproach relative to state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 01:23:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Mardani", "Morteza", ""], ["Giannakis", "Georgios B.", ""], ["Ugurbil", "Kamil", ""]]}, {"id": "1609.04112", "submitter": "C.-C. Jay Kuo", "authors": "C.-C. Jay Kuo", "title": "Understanding Convolutional Neural Networks with A Mathematical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work attempts to address two fundamental questions about the structure\nof the convolutional neural networks (CNN): 1) why a non-linear activation\nfunction is essential at the filter output of every convolutional layer? 2)\nwhat is the advantage of the two-layer cascade system over the one-layer\nsystem? A mathematical model called the \"REctified-COrrelations on a Sphere\"\n(RECOS) is proposed to answer these two questions. After the CNN training\nprocess, the converged filter weights define a set of anchor vectors in the\nRECOS model. Anchor vectors represent the frequently occurring patterns (or the\nspectral components). The necessity of rectification is explained using the\nRECOS model. Then, the behavior of a two-layer RECOS system is analyzed and\ncompared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are\nused to illustrate discussion points. Finally, the RECOS model is generalized\nto a multi-layer system with the AlexNet as an example.\n  Keywords: Convolutional Neural Network (CNN), Nonlinear Activation, RECOS\nModel, Rectified Linear Unit (ReLU), MNIST Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 02:17:09 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 21:55:26 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Kuo", "C. -C. Jay", ""]]}, {"id": "1609.04116", "submitter": "Songcan Chen", "authors": "Qing Tian, Songcan Chen", "title": "Joint Gender Classification and Age Estimation by Nearly Orthogonalizing\n  Their Semantic Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In human face-based biometrics, gender classification and age estimation are\ntwo typical learning tasks. Although a variety of approaches have been proposed\nto handle them, just a few of them are solved jointly, even so, these joint\nmethods do not yet specifically concern the semantic difference between human\ngender and age, which is intuitively helpful for joint learning, consequently\nleaving us a room of further improving the performance. To this end, in this\nwork we firstly propose a general learning framework for jointly estimating\nhuman gender and age by specially attempting to formulate such semantic\nrelationships as a form of near-orthogonality regularization and then\nincorporate it into the objective of the joint learning framework. In order to\nevaluate the effectiveness of the proposed framework, we exemplify it by\nrespectively taking the widely used binary-class SVM for gender classification,\nand two threshold-based ordinal regression methods (i.e., the discriminant\nlearning for ordinal regression and support vector ordinal regression) for age\nestimation, and crucially coupling both through the proposed semantic\nformulation. Moreover, we develop its kernelized nonlinear counterpart by\nderiving a representer theorem for the joint learning strategy. Finally,\nthrough extensive experiments on three aging datasets FG-NET, Morph Album I and\nMorph Album II, we demonstrate the effectiveness and superiority of the\nproposed joint learning strategy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 02:45:37 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Tian", "Qing", ""], ["Chen", "Songcan", ""]]}, {"id": "1609.04167", "submitter": "Laurent Jacques", "authors": "V. Abrol, O. Absil, P.-A. Absil, S. Anthoine, P. Antoine, T. Arildsen,\n  N. Bertin, F. Bleichrodt, J. Bobin, A. Bol, A. Bonnefoy, F. Caltagirone, V.\n  Cambareri, C. Chenot, V. Crnojevi\\'c, M. Da\\v{n}kov\\'a, K. Degraux, J.\n  Eisert, J. M. Fadili, M. Gabri\\'e, N. Gac, D. Giacobello, A. Gonzalez, C. A.\n  Gomez Gonzalez, A. Gonz\\'alez, P.-Y. Gousenbourger, M. Gr{\\ae}sb{\\o}ll\n  Christensen, R. Gribonval, S. Gu\\'erit, S. Huang, P. Irofti, L. Jacques, U.\n  S. Kamilov, S. Kitic\\'c, M. Kliesch, F. Krzakala, J. A. Lee, W. Liao, T.\n  Lindstr{\\o}m Jensen, A. Manoel, H. Mansour, A. Mohammad-Djafari, A.\n  Moshtaghpour, F. Ngol\\`e, B. Pairet, M. Pani\\'c, G. Peyr\\'e, A. Pi\\v{z}urica,\n  P. Rajmic, M. Roblin, I. Roth, A. K. Sao, P. Sharma, J.-L. Starck, E. W.\n  Tramel, T. van Waterschoot, D. Vukobratovic, L. Wang, B. Wirth, G. Wunder, H.\n  Zhang", "title": "Proceedings of the third \"international Traveling Workshop on\n  Interactions between Sparse models and Technology\" (iTWIST'16)", "comments": "69 pages, 22 extended abstracts, iTWIST'16 website:\n  http://www.itwist16.es.aau.dk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The third edition of the \"international - Traveling Workshop on Interactions\nbetween Sparse models and Technology\" (iTWIST) took place in Aalborg, the 4th\nlargest city in Denmark situated beautifully in the northern part of the\ncountry, from the 24th to 26th of August 2016. The workshop venue was at the\nAalborg University campus. One implicit objective of this biennial workshop is\nto foster collaboration between international scientific teams by disseminating\nideas through both specific oral/poster presentations and free discussions. For\nthis third edition, iTWIST'16 gathered about 50 international participants and\nfeatures 8 invited talks, 12 oral presentations, and 12 posters on the\nfollowing themes, all related to the theory, application and generalization of\nthe \"sparsity paradigm\": Sparsity-driven data sensing and processing (e.g.,\noptics, computer vision, genomics, biomedical, digital communication, channel\nestimation, astronomy); Application of sparse models in non-convex/non-linear\ninverse problems (e.g., phase retrieval, blind deconvolution, self\ncalibration); Approximate probabilistic inference for sparse problems; Sparse\nmachine learning and inference; \"Blind\" inverse problems and dictionary\nlearning; Optimization for sparse modelling; Information theory, geometry and\nrandomness; Sparsity? What's next? (Discrete-valued signals; Union of\nlow-dimensional spaces, Cosparsity, mixed/group norm, model-based,\nlow-complexity models, ...); Matrix/manifold sensing/processing (graph,\nlow-rank approximation, ...); Complexity/accuracy tradeoffs in numerical\nmethods/optimization; Electronic/optical compressive sensors (hardware).\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 08:27:11 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Abrol", "V.", ""], ["Absil", "O.", ""], ["Absil", "P. -A.", ""], ["Anthoine", "S.", ""], ["Antoine", "P.", ""], ["Arildsen", "T.", ""], ["Bertin", "N.", ""], ["Bleichrodt", "F.", ""], ["Bobin", "J.", ""], ["Bol", "A.", ""], ["Bonnefoy", "A.", ""], ["Caltagirone", "F.", ""], ["Cambareri", "V.", ""], ["Chenot", "C.", ""], ["Crnojevi\u0107", "V.", ""], ["Da\u0148kov\u00e1", "M.", ""], ["Degraux", "K.", ""], ["Eisert", "J.", ""], ["Fadili", "J. M.", ""], ["Gabri\u00e9", "M.", ""], ["Gac", "N.", ""], ["Giacobello", "D.", ""], ["Gonzalez", "A.", ""], ["Gonzalez", "C. A. Gomez", ""], ["Gonz\u00e1lez", "A.", ""], ["Gousenbourger", "P. -Y.", ""], ["Christensen", "M. Gr\u00e6sb\u00f8ll", ""], ["Gribonval", "R.", ""], ["Gu\u00e9rit", "S.", ""], ["Huang", "S.", ""], ["Irofti", "P.", ""], ["Jacques", "L.", ""], ["Kamilov", "U. S.", ""], ["Kitic\u0107", "S.", ""], ["Kliesch", "M.", ""], ["Krzakala", "F.", ""], ["Lee", "J. A.", ""], ["Liao", "W.", ""], ["Jensen", "T. Lindstr\u00f8m", ""], ["Manoel", "A.", ""], ["Mansour", "H.", ""], ["Mohammad-Djafari", "A.", ""], ["Moshtaghpour", "A.", ""], ["Ngol\u00e8", "F.", ""], ["Pairet", "B.", ""], ["Pani\u0107", "M.", ""], ["Peyr\u00e9", "G.", ""], ["Pi\u017eurica", "A.", ""], ["Rajmic", "P.", ""], ["Roblin", "M.", ""], ["Roth", "I.", ""], ["Sao", "A. K.", ""], ["Sharma", "P.", ""], ["Starck", "J. -L.", ""], ["Tramel", "E. W.", ""], ["van Waterschoot", "T.", ""], ["Vukobratovic", "D.", ""], ["Wang", "L.", ""], ["Wirth", "B.", ""], ["Wunder", "G.", ""], ["Zhang", "H.", ""]]}, {"id": "1609.04331", "submitter": "Vadim Kantorov", "authors": "Vadim Kantorov, Maxime Oquab, Minsu Cho and Ivan Laptev", "title": "ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised\n  Localization", "comments": "Accepted paper at ECCV2016. The website and code is at\n  http://www.di.ens.fr/willow/research/contextlocnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to localize objects in images using image-level supervision only.\nPrevious approaches to this problem mainly focus on discriminative object\nregions and often fail to locate precise object boundaries. We address this\nproblem by introducing two types of context-aware guidance models, additive and\ncontrastive models, that leverage their surrounding context regions to improve\nlocalization. The additive model encourages the predicted object region to be\nsupported by its surrounding context region. The contrastive model encourages\nthe predicted object region to be outstanding from its surrounding context\nregion. Our approach benefits from the recent success of convolutional neural\nnetworks for object recognition and extends Fast R-CNN to weakly supervised\nobject localization. Extensive experimental evaluation on the PASCAL VOC 2007\nand 2012 benchmarks shows hat our context-aware approach significantly improves\nweakly supervised localization and detection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 16:17:03 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Kantorov", "Vadim", ""], ["Oquab", "Maxime", ""], ["Cho", "Minsu", ""], ["Laptev", "Ivan", ""]]}, {"id": "1609.04337", "submitter": "Alexandre Coninx", "authors": "Alexandre Coninx, Pierre Bessi\\`ere and Jacques Droulez", "title": "Quick and energy-efficient Bayesian computing of binocular disparity\n  using stochastic digital signals", "comments": "Preprint of article submitted for publication in International\n  Journal of Approximate Reasoning and accepted pending minor revisions", "journal-ref": null, "doi": "10.1016/j.ijar.2016.11.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of the tridimensional geometry of a visual scene using the\nbinocular disparity information is an important issue in computer vision and\nmobile robotics, which can be formulated as a Bayesian inference problem.\nHowever, computation of the full disparity distribution with an advanced\nBayesian model is usually an intractable problem, and proves computationally\nchallenging even with a simple model. In this paper, we show how probabilistic\nhardware using distributed memory and alternate representation of data as\nstochastic bitstreams can solve that problem with high performance and energy\nefficiency. We put forward a way to express discrete probability distributions\nusing stochastic data representations and perform Bayesian fusion using those\nrepresentations, and show how that approach can be applied to diparity\ncomputation. We evaluate the system using a simulated stochastic implementation\nand discuss possible hardware implementations of such architectures and their\npotential for sensorimotor processing and robotics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 16:41:31 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 15:36:01 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Coninx", "Alexandre", ""], ["Bessi\u00e8re", "Pierre", ""], ["Droulez", "Jacques", ""]]}, {"id": "1609.04356", "submitter": "Xingchao Peng", "authors": "Xingchao Peng and Kate Saenko", "title": "Combining Texture and Shape Cues for Object Recognition With Minimal\n  Supervision", "comments": "ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to object classification and detection which\nrequires minimal supervision and which combines visual texture cues and shape\ninformation learned from freely available unlabeled web search results. The\nexplosion of visual data on the web can potentially make visual examples of\nalmost any object easily accessible via web search. Previous unsupervised\nmethods have utilized either large scale sources of texture cues from the web,\nor shape information from data such as crowdsourced CAD models. We propose a\ntwo-stream deep learning framework that combines these cues, with one stream\nlearning visual texture cues from image search data, and the other stream\nlearning rich shape information from 3D CAD models. To perform classification\nor detection for a novel image, the predictions of the two streams are combined\nusing a late fusion scheme. We present experiments and visualizations for both\ntasks on the standard benchmark PASCAL VOC 2007 to demonstrate that texture and\nshape provide complementary information in our model. Our method outperforms\nprevious web image based models, 3D CAD model based approaches, and weakly\nsupervised models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 17:41:48 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Peng", "Xingchao", ""], ["Saenko", "Kate", ""]]}, {"id": "1609.04375", "submitter": "Ge Wang", "authors": "Ge Wang", "title": "A Perspective on Deep Imaging", "comments": "9 pages, 10 figures, 49 references, and accepted by IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of tomographic imaging and deep learning, or machine learning\nin general, promises to empower not only image analysis but also image\nreconstruction. The latter aspect is considered in this perspective article\nwith an emphasis on medical imaging to develop a new generation of image\nreconstruction theories and techniques. This direction might lead to\nintelligent utilization of domain knowledge from big data, innovative\napproaches for image reconstruction, and superior performance in clinical and\npreclinical applications. To realize the full impact of machine learning on\nmedical imaging, major challenges must be addressed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 15:45:48 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 13:02:27 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Wang", "Ge", ""]]}, {"id": "1609.04382", "submitter": "Jo\\~ao F. Henriques", "authors": "Jo\\~ao F. Henriques, Andrea Vedaldi", "title": "Warped Convolutions: Efficient Invariance to Spatial Transformations", "comments": "Proceedings of the 34th International Conference on Machine Learning,\n  Sydney, Australia, PMLR 70, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are extremely efficient, since they\nexploit the inherent translation-invariance of natural images. However,\ntranslation is just one of a myriad of useful spatial transformations. Can the\nsame efficiency be attained when considering other spatial invariances? Such\ngeneralized convolutions have been considered in the past, but at a high\ncomputational cost. We present a construction that is simple and exact, yet has\nthe same computational complexity that standard convolutions enjoy. It consists\nof a constant image warp followed by a simple convolution, which are standard\nblocks in deep learning toolboxes. With a carefully crafted warp, the resulting\narchitecture can be made equivariant to a wide range of two-parameter spatial\ntransformations. We show encouraging results in realistic scenarios, including\nthe estimation of vehicle poses in the Google Earth dataset (rotation and\nscale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations\nunder perspective).\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:10:14 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 14:23:20 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 20:06:59 GMT"}, {"version": "v4", "created": "Wed, 18 Apr 2018 15:29:30 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Henriques", "Jo\u00e3o F.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1609.04387", "submitter": "Elad Richardson", "authors": "Elad Richardson, Matan Sela, Ron Kimmel", "title": "3D Face Reconstruction by Learning from Synthetic Data", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and robust three-dimensional reconstruction of facial geometric\nstructure from a single image is a challenging task with numerous applications.\nHere, we introduce a learning-based approach for reconstructing a\nthree-dimensional face from a single image. Recent face recovery methods rely\non accurate localization of key characteristic points. In contrast, the\nproposed approach is based on a Convolutional-Neural-Network (CNN) which\nextracts the face geometry directly from its image. Although such deep\narchitectures outperform other models in complex computer vision problems,\ntraining them properly requires a large dataset of annotated examples. In the\ncase of three-dimensional faces, currently, there are no large volume data\nsets, while acquiring such big-data is a tedious task. As an alternative, we\npropose to generate random, yet nearly photo-realistic, facial images for which\nthe geometric form is known. The suggested model successfully recovers facial\nshapes from real images, even for faces with extreme expressions and under\nvarious lighting conditions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:47:12 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 12:12:34 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Richardson", "Elad", ""], ["Sela", "Matan", ""], ["Kimmel", "Ron", ""]]}, {"id": "1609.04392", "submitter": "Michal Balazia", "authors": "Michal Balazia and Petr Sojka", "title": "Learning Robust Features for Gait Recognition by Maximum Margin\n  Criterion", "comments": "Preprint. Full paper published at the 23rd IEEE/IAPR International\n  Conference on Pattern Recognition (ICPR), Cancun, Mexico, Dec 2016. pp.\n  901-906 IEEE Xplore DL, March 2017. DOI: 10.1109/ICPR.2016.7899750", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of gait recognition from motion capture data, designing\nhuman-interpretable gait features is a common practice of many fellow\nresearchers. To refrain from ad-hoc schemes and to find maximally\ndiscriminative features we may need to explore beyond the limits of human\ninterpretability. This paper contributes to the state-of-the-art with a machine\nlearning approach for extracting robust gait features directly from raw joint\ncoordinates. The features are learned by a modification of Linear Discriminant\nAnalysis with Maximum Margin Criterion so that the identities are maximally\nseparated and, in combination with an appropriate classifier, used for gait\nrecognition. Experiments on the CMU MoCap database show that this method\noutperforms eight other relevant methods in terms of the distribution of\nbiometric templates in respective feature spaces expressed in four class\nseparability coefficients. Additional experiments indicate that this method is\na leading concept for rank-based classifier systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:52:10 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 12:18:22 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 13:03:50 GMT"}, {"version": "v4", "created": "Thu, 25 May 2017 01:31:14 GMT"}, {"version": "v5", "created": "Thu, 24 Aug 2017 11:33:04 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Balazia", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "1609.04453", "submitter": "Terrell Mundhenk", "authors": "T. Nathan Mundhenk, Goran Konjevod, Wesam A. Sakla, Kofi Boakye", "title": "A Large Contextual Dataset for Classification, Detection and Counting of\n  Cars with Deep Learning", "comments": "ECCV 2016 Pre-press revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have created a large diverse set of cars from overhead images, which are\nuseful for training a deep learner to binary classify, detect and count them.\nThe dataset and all related material will be made publically available. The set\ncontains contextual matter to aid in identification of difficult targets. We\ndemonstrate classification and detection on this dataset using a neural network\nwe call ResCeption. This network combines residual learning with\nInception-style layers and is used to count cars in one look. This is a new way\nto count objects rather than by localization or density estimation. It is\nfairly accurate, fast and easy to implement. Additionally, the counting method\nis not car or scene specific. It would be easy to train this method to count\nother kinds of objects and counting over new scenes requires no extra set up or\nassumptions about object locations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 21:44:58 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Mundhenk", "T. Nathan", ""], ["Konjevod", "Goran", ""], ["Sakla", "Wesam A.", ""], ["Boakye", "Kofi", ""]]}, {"id": "1609.04541", "submitter": "Johann Bengua", "authors": "Johann A. Bengua and Ho N. Phien and Hoang D. Tuan and Minh N. Do", "title": "Matrix Product State for Higher-Order Tensor Compression and\n  Classification", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2703882", "report-no": null, "categories": "stat.ML cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces matrix product state (MPS) decomposition as a new and\nsystematic method to compress multidimensional data represented by higher-order\ntensors. It solves two major bottlenecks in tensor compression: computation and\ncompression quality. Regardless of tensor order, MPS compresses tensors to\nmatrices of moderate dimension which can be used for classification. Mainly\nbased on a successive sequence of singular value decompositions (SVD), MPS is\nquite simple to implement and arrives at the global optimal matrix, bypassing\nlocal alternating optimization, which is not only computationally expensive but\ncannot yield the global solution. Benchmark results show that MPS can achieve\nbetter classification performance with favorable computation cost compared to\nother tensor compression methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 09:04:25 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1609.04628", "submitter": "Rocco Tripodi", "authors": "Rocco Tripodi, Sebastiano Vascon, Marcello Pelillo", "title": "Context Aware Nonnegative Matrix Factorization Clustering", "comments": "6 pages, 3 figures. Full paper accepted to International Conference\n  on Pattern Recognition ICPR 2016, Canc\\'un, Mexico", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a method to refine the clustering results obtained\nwith the nonnegative matrix factorization (NMF) technique, imposing consistency\nconstraints on the final labeling of the data. The research community focused\nits effort on the initialization and on the optimization part of this method,\nwithout paying attention to the final cluster assignments. We propose a game\ntheoretic framework in which each object to be clustered is represented as a\nplayer, which has to choose its cluster membership. The information obtained\nwith NMF is used to initialize the strategy space of the players and a weighted\ngraph is used to model the interactions among the players. These interactions\nallow the players to choose a cluster which is coherent with the clusters\nchosen by similar players, a property which is not guaranteed by NMF, since it\nproduces a soft clustering of the data. The results on common benchmarks show\nthat our model is able to improve the performances of many NMF formulations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 13:23:43 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Tripodi", "Rocco", ""], ["Vascon", "Sebastiano", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1609.04653", "submitter": "Peter Pinggera", "authors": "Peter Pinggera, Sebastian Ramos, Stefan Gehrig, Uwe Franke, Carsten\n  Rother, Rudolf Mester", "title": "Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles", "comments": "To be presented at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small obstacles on the road ahead is a critical part of the driving\ntask which has to be mastered by fully autonomous cars. In this paper, we\npresent a method based on stereo vision to reliably detect such obstacles from\na moving vehicle. The proposed algorithm performs statistical hypothesis tests\nin disparity space directly on stereo image data, assessing freespace and\nobstacle hypotheses on independent local patches. This detection approach does\nnot depend on a global road model and handles both static and moving obstacles.\nFor evaluation, we employ a novel lost-cargo image sequence dataset comprising\nmore than two thousand frames with pixelwise annotations of obstacle and\nfree-space and provide a thorough comparison to several stereo-based baseline\nmethods. The dataset will be made available to the community to foster further\nresearch on this important topic. The proposed approach outperforms all\nconsidered baselines in our evaluations on both pixel and object level and runs\nat frame rates of up to 20 Hz on 2 mega-pixel stereo imagery. Small obstacles\ndown to the height of 5 cm can successfully be detected at 20 m distance at low\nfalse positive rates.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 14:01:03 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Pinggera", "Peter", ""], ["Ramos", "Sebastian", ""], ["Gehrig", "Stefan", ""], ["Franke", "Uwe", ""], ["Rother", "Carsten", ""], ["Mester", "Rudolf", ""]]}, {"id": "1609.04705", "submitter": "Lee Clement", "authors": "Lee Clement and Valentin Peretroukhin and Jonathan Kelly", "title": "Improving the Accuracy of Stereo Visual Odometry Using Visual\n  Illumination Estimation", "comments": "In: Kuli\\'c D., Nakamura Y., Khatib O., Venture G. (eds) 2016\n  International Symposium on Experimental Robotics. ISER 2016. Springer\n  Proceedings in Advanced Robotics, vol 1. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-319-50115-4_36", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of reliable and accurate GPS, visual odometry (VO) has emerged\nas an effective means of estimating the egomotion of robotic vehicles. Like any\ndead-reckoning technique, VO suffers from unbounded accumulation of drift error\nover time, but this accumulation can be limited by incorporating absolute\norientation information from, for example, a sun sensor. In this paper, we\nleverage recent work on visual outdoor illumination estimation to show that\nestimation error in a stereo VO pipeline can be reduced by inferring the sun\nposition from the same image stream used to compute VO, thereby gaining the\nbenefits of sun sensing without requiring a dedicated sun sensor or the sun to\nbe visible to the camera. We compare sun estimation methods based on\nhand-crafted visual cues and Convolutional Neural Networks (CNNs) and\ndemonstrate our approach on a combined 7.8 km of urban driving from the popular\nKITTI dataset, achieving up to a 43% reduction in translational average root\nmean squared error (ARMSE) and a 59% reduction in final translational drift\nerror compared to pure VO alone.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 15:44:25 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 22:27:49 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 13:17:36 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Clement", "Lee", ""], ["Peretroukhin", "Valentin", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1609.04757", "submitter": "Deepti Ghadiyaram", "authors": "Deepti Ghadiyaram and Alan C. Bovik", "title": "Perceptual Quality Prediction on Authentically Distorted Images Using a\n  Bag of Features Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current top-performing blind perceptual image quality prediction models are\ngenerally trained on legacy databases of human quality opinion scores on\nsynthetically distorted images. Therefore they learn image features that\neffectively predict human visual quality judgments of inauthentic, and usually\nisolated (single) distortions. However, real-world images usually contain\ncomplex, composite mixtures of multiple distortions. We study the perceptually\nrelevant natural scene statistics of such authentically distorted images, in\ndifferent color spaces and transform domains. We propose a bag of feature-maps\napproach which avoids assumptions about the type of distortion(s) contained in\nan image, focusing instead on capturing consistencies, or departures therefrom,\nof the statistics of real world images. Using a large database of authentically\ndistorted images, human opinions of them, and bags of features computed on\nthem, we train a regressor to conduct image quality prediction. We demonstrate\nthe competence of the features towards improving automatic perceptual quality\nprediction by testing a learned algorithm using them on a benchmark legacy\ndatabase as well as on a newly introduced distortion-realistic resource called\nthe LIVE In the Wild Image Quality Challenge Database. We extensively evaluate\nthe perceptual quality prediction model and algorithm and show that it is able\nto achieve good quality prediction power that is better than other leading\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 18:06:21 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Ghadiyaram", "Deepti", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1609.04767", "submitter": "Gustavo Rohde", "authors": "Soheil Kolouri, Serim Park, Matthew Thorpe, Dejan Slep\\v{c}ev, Gustavo\n  K. Rohde", "title": "Transport-based analysis, modeling, and learning from signal and data\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transport-based techniques for signal and data analysis have received\nincreased attention recently. Given their abilities to provide accurate\ngenerative models for signal intensities and other data distributions, they\nhave been used in a variety of applications including content-based retrieval,\ncancer detection, image super-resolution, and statistical machine learning, to\nname a few, and shown to produce state of the art in several applications.\nMoreover, the geometric characteristics of transport-related metrics have\ninspired new kinds of algorithms for interpreting the meaning of data\ndistributions. Here we provide an overview of the mathematical underpinnings of\nmass transport-related methods, including numerical implementation, as well as\na review, with demonstrations, of several applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 18:28:50 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Kolouri", "Soheil", ""], ["Park", "Serim", ""], ["Thorpe", "Matthew", ""], ["Slep\u010dev", "Dejan", ""], ["Rohde", "Gustavo K.", ""]]}, {"id": "1609.04771", "submitter": "Jingwei Liu", "authors": "Yi Liu, Jingwei Liu", "title": "Formula of Volume of Revolution with Integration by Parts and Extension", "comments": "7 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A calculation formula of volume of revolution with integration by parts of\ndefinite integral is derived based on monotone function, and extended to a\ngeneral case that curved trapezoids is determined by continuous, piecewise\nstrictly monotone and differential function. And, two examples are given, ones\ncurvilinear trapezoids is determined by Kepler equation, and the other\ncurvilinear trapezoids is a function transmuted from Kepler equation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 06:13:26 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Liu", "Yi", ""], ["Liu", "Jingwei", ""]]}, {"id": "1609.04802", "submitter": "Christian Ledig", "authors": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew\n  Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,\n  Zehan Wang, Wenzhe Shi", "title": "Photo-Realistic Single Image Super-Resolution Using a Generative\n  Adversarial Network", "comments": "19 pages, 15 figures, 2 tables, accepted for oral presentation at\n  CVPR, main paper + some supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 19:53:07 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 17:22:44 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 18:30:18 GMT"}, {"version": "v4", "created": "Thu, 13 Apr 2017 14:25:44 GMT"}, {"version": "v5", "created": "Thu, 25 May 2017 11:25:41 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Ledig", "Christian", ""], ["Theis", "Lucas", ""], ["Huszar", "Ferenc", ""], ["Caballero", "Jose", ""], ["Cunningham", "Andrew", ""], ["Acosta", "Alejandro", ""], ["Aitken", "Andrew", ""], ["Tejani", "Alykhan", ""], ["Totz", "Johannes", ""], ["Wang", "Zehan", ""], ["Shi", "Wenzhe", ""]]}, {"id": "1609.04830", "submitter": "Lee Prangnell", "authors": "Lee Prangnell", "title": "Visible Light-Based Human Visual System Conceptual Model", "comments": "Discussion Paper, Department of Computer Science, University of\n  Warwick, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a widely accepted set of assertions in the digital image and\nvideo coding literature, which are as follows: the Human Visual System (HVS) is\nmore sensitive to luminance (often confused with brightness) than photon\nenergies (often confused with chromaticity and chrominance). Passages similar\nto the following occur with high frequency in the peer reviewed literature and\nacademic text books: \"the HVS is much more sensitive to brightness than colour\"\nand/or \"the HVS is much more sensitive to luma than chroma\". In this discussion\npaper, a Visible Light-Based Human Visual System (VL-HVS) conceptual model is\ndiscussed. The objectives of VL-HVS are as follows: 1. To provide a deeper\ntheoretical reflection of the fundamental relationship between visible light,\nthe manifestation of colour perception derived from visible light and the\nphysiology of the perception of colour. That is, in terms of the physics of\nvisible light, photobiology and the human subjective interpretation of visible\nlight, it is appropriate to provide comprehensive background information in\nrelation to the natural interactions between visible light, the retinal\nphotoreceptors and the subsequent cortical processing of such. 2. To provide a\nmore wholesome account with respect to colour information in digital image and\nvideo processing applications. 3. To recontextualise colour data in the RGB and\nYCbCr colour spaces, such that novel techniques in digital image and video\nprocessing, including quantisation and artifact reduction techniques, may be\ndeveloped based on both luma and chroma information (not luma data only).\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:00:56 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 08:00:21 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 16:41:29 GMT"}, {"version": "v4", "created": "Wed, 10 May 2017 07:33:29 GMT"}, {"version": "v5", "created": "Mon, 19 Aug 2019 08:48:34 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Prangnell", "Lee", ""]]}, {"id": "1609.04855", "submitter": "Kenji Hata", "authors": "Kenji Hata, Ranjay Krishna, Li Fei-Fei, Michael S. Bernstein", "title": "A Glimpse Far into the Future: Understanding Long-term Crowd Worker\n  Quality", "comments": "10 pages, 11 figures, accepted CSCW 2017", "journal-ref": null, "doi": "10.1145/2998181.2998248", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microtask crowdsourcing is increasingly critical to the creation of extremely\nlarge datasets. As a result, crowd workers spend weeks or months repeating the\nexact same tasks, making it necessary to understand their behavior over these\nlong periods of time. We utilize three large, longitudinal datasets of nine\nmillion annotations collected from Amazon Mechanical Turk to examine claims\nthat workers fatigue or satisfice over these long periods, producing lower\nquality work. We find that, contrary to these claims, workers are extremely\nstable in their quality over the entire period. To understand whether workers\nset their quality based on the task's requirements for acceptance, we then\nperform an experiment where we vary the required quality for a large\ncrowdsourcing task. Workers did not adjust their quality based on the\nacceptance threshold: workers who were above the threshold continued working at\ntheir usual quality level, and workers below the threshold self-selected\nthemselves out of the task. Capitalizing on this consistency, we demonstrate\nthat it is possible to predict workers' long-term quality using just a glimpse\nof their quality on the first five tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:47:51 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 17:34:10 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Hata", "Kenji", ""], ["Krishna", "Ranjay", ""], ["Fei-Fei", "Li", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1609.04861", "submitter": "Wenbin Li", "authors": "Wenbin Li, Ale\\v{s} Leonardis, Mario Fritz", "title": "Visual Stability Prediction and Its Application to Manipulation", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.00066", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding physical phenomena is a key competence that enables humans and\nanimals to act and interact under uncertain perception in previously unseen\nenvironments containing novel objects and their configurations. Developmental\npsychology has shown that such skills are acquired by infants from observations\nat a very early stage.\n  In this paper, we contrast a more traditional approach of taking a\nmodel-based route with explicit 3D representations and physical simulation by\nan {\\em end-to-end} approach that directly predicts stability from appearance.\nWe ask the question if and to what extent and quality such a skill can directly\nbe acquired in a data-driven way---bypassing the need for an explicit\nsimulation at run-time.\n  We present a learning-based approach based on simulated data that predicts\nstability of towers comprised of wooden blocks under different conditions and\nquantities related to the potential fall of the towers. We first evaluate the\napproach on synthetic data and compared the results to human judgments on the\nsame stimuli. Further, we extend this approach to reason about future states of\nsuch towers that in turn enables successful stacking.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 21:12:41 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 10:19:44 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Li", "Wenbin", ""], ["Leonardis", "Ale\u0161", ""], ["Fritz", "Mario", ""]]}, {"id": "1609.04938", "submitter": "Yuntian Deng", "authors": "Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander M. Rush", "title": "Image-to-Markup Generation with Coarse-to-Fine Attention", "comments": "Accepted by ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural encoder-decoder model to convert images into\npresentational markup based on a scalable coarse-to-fine attention mechanism.\nOur method is evaluated in the context of image-to-LaTeX generation, and we\nintroduce a new dataset of real-world rendered mathematical expressions paired\nwith LaTeX markup. We show that unlike neural OCR techniques using CTC-based\nmodels, attention-based approaches can tackle this non-standard OCR task. Our\napproach outperforms classical mathematical OCR systems by a large margin on\nin-domain rendered data, and, with pretraining, also performs well on\nout-of-domain handwritten data. To reduce the inference complexity associated\nwith the attention-based approaches, we introduce a new coarse-to-fine\nattention layer that selects a support region before applying attention.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 08:14:50 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 22:48:53 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Deng", "Yuntian", ""], ["Kanervisto", "Anssi", ""], ["Ling", "Jeffrey", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1609.05001", "submitter": "Yash Bhalgat", "authors": "Yash Bhalgat, Mandar Kulkarni, Shirish Karande, Sachin Lodha", "title": "Stamp processing with examplar features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document digitization is becoming increasingly crucial. In this work, we\npropose a shape based approach for automatic stamp verification/detection in\ndocument images using an unsupervised feature learning. Given a small set of\ntraining images, our algorithm learns an appropriate shape representation using\nan unsupervised clustering. Experimental results demonstrate the effectiveness\nof our framework in challenging scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 11:20:07 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Bhalgat", "Yash", ""], ["Kulkarni", "Mandar", ""], ["Karande", "Shirish", ""], ["Lodha", "Sachin", ""]]}, {"id": "1609.05112", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh, Christopher Mitcheltree, Shujin Zhu, Shamak Dutta", "title": "Barcodes for Medical Image Retrieval Using Autoencoded Radon Transform", "comments": "o appear in proceedings of the 23rd International Conference on\n  Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using content-based binary codes to tag digital images has emerged as a\npromising retrieval technology. Recently, Radon barcodes (RBCs) have been\nintroduced as a new binary descriptor for image search. RBCs are generated by\nbinarization of Radon projections and by assembling them into a vector, namely\nthe barcode. A simple local thresholding has been suggested for binarization.\nIn this paper, we put forward the idea of \"autoencoded Radon barcodes\". Using\nimages in a training dataset, we autoencode Radon projections to perform\nbinarization on outputs of hidden layers. We employed the mini-batch stochastic\ngradient descent approach for the training. Each hidden layer of the\nautoencoder can produce a barcode using a threshold determined based on the\nrange of the logistic function used. The compressing capability of autoencoders\napparently reduces the redundancies inherent in Radon projections leading to\nmore accurate retrieval results. The IRMA dataset with 14,410 x-ray images is\nused to validate the performance of the proposed method. The experimental\nresults, containing comparison with RBCs, SURF and BRISK, show that autoencoded\nRadon barcode (ARBC) has the capacity to capture important information and to\nlearn richer representations resulting in lower retrieval errors for image\nretrieval measured with the accuracy of the first hit only.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 15:51:24 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Tizhoosh", "Hamid R.", ""], ["Mitcheltree", "Christopher", ""], ["Zhu", "Shujin", ""], ["Dutta", "Shamak", ""]]}, {"id": "1609.05115", "submitter": "Christian Richardt", "authors": "Christian Richardt, Hyeongwoo Kim, Levi Valgaerts, Christian Theobalt", "title": "Dense Wide-Baseline Scene Flow From Two Handheld Video Cameras", "comments": "11 pages, supplemental document included as appendix, 3DV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique for computing dense scene flow from two handheld\nvideos with wide camera baselines and different photometric properties due to\ndifferent sensors or camera settings like exposure and white balance. Our\ntechnique innovates in two ways over existing methods: (1) it supports\nindependently moving cameras, and (2) it computes dense scene flow for\nwide-baseline scenarios.We achieve this by combining state-of-the-art\nwide-baseline correspondence finding with a variational scene flow formulation.\nFirst, we compute dense, wide-baseline correspondences using DAISY descriptors\nfor matching between cameras and over time. We then detect and replace occluded\npixels in the correspondence fields using a novel edge-preserving Laplacian\ncorrespondence completion technique. We finally refine the computed\ncorrespondence fields in a variational scene flow formulation. We show dense\nscene flow results computed from challenging datasets with independently\nmoving, handheld cameras of varying camera settings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 15:54:46 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Richardt", "Christian", ""], ["Kim", "Hyeongwoo", ""], ["Valgaerts", "Levi", ""], ["Theobalt", "Christian", ""]]}, {"id": "1609.05118", "submitter": "Hamid Tizhoosh", "authors": "Mina Nouredanesh, H.R. Tizhoosh, Ershad Banijamali, James Tung", "title": "Radon-Gabor Barcodes for Medical Image Retrieval", "comments": "To appear in proceedings of the 23rd International Conference on\n  Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the explosion of digital images on the Web,\ncontent-based retrieval has emerged as a significant research area. Shapes,\ntextures, edges and segments may play a key role in describing the content of\nan image. Radon and Gabor transforms are both powerful techniques that have\nbeen widely studied to extract shape-texture-based information. The combined\nRadon-Gabor features may be more robust against scale/rotation variations,\npresence of noise, and illumination changes. The objective of this paper is to\nharness the potentials of both Gabor and Radon transforms in order to introduce\nexpressive binary features, called barcodes, for image annotation/tagging\ntasks. We propose two different techniques: Gabor-of-Radon-Image Barcodes\n(GRIBCs), and Guided-Radon-of-Gabor Barcodes (GRGBCs). For validation, we\nemploy the IRMA x-ray dataset with 193 classes, containing 12,677 training\nimages and 1,733 test images. A total error score as low as 322 and 330 were\nachieved for GRGBCs and GRIBCs, respectively. This corresponds to $\\approx\n81\\%$ retrieval accuracy for the first hit.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 16:01:43 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Nouredanesh", "Mina", ""], ["Tizhoosh", "H. R.", ""], ["Banijamali", "Ershad", ""], ["Tung", "James", ""]]}, {"id": "1609.05119", "submitter": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk", "authors": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk, Umut G\\\"u\\c{c}l\\\"u, Marcel A. J. van\n  Gerven, Rob van Lier", "title": "Deep Impression: Audiovisual Deep Residual Networks for Multimodal\n  Apparent Personality Trait Recognition", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-49409-8_28", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we develop an audiovisual deep residual network for multimodal apparent\npersonality trait recognition. The network is trained end-to-end for predicting\nthe Big Five personality traits of people from their videos. That is, the\nnetwork does not require any feature engineering or visual analysis such as\nface detection, face landmark alignment or facial expression recognition.\nRecently, the network won the third place in the ChaLearn First Impressions\nChallenge with a test accuracy of 0.9109.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 16:09:20 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["G\u00fc\u00e7l\u00fct\u00fcrk", "Ya\u011fmur", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["van Gerven", "Marcel A. J.", ""], ["van Lier", "Rob", ""]]}, {"id": "1609.05130", "submitter": "John McCormac", "authors": "John McCormac, Ankur Handa, Andrew Davison, Stefan Leutenegger", "title": "SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever more robust, accurate and detailed mapping using visual sensing has\nproven to be an enabling factor for mobile robots across a wide variety of\napplications. For the next level of robot intelligence and intuitive user\ninteraction, maps need extend beyond geometry and appearence - they need to\ncontain semantics. We address this challenge by combining Convolutional Neural\nNetworks (CNNs) and a state of the art dense Simultaneous Localisation and\nMapping (SLAM) system, ElasticFusion, which provides long-term dense\ncorrespondence between frames of indoor RGB-D video even during loopy scanning\ntrajectories. These correspondences allow the CNN's semantic predictions from\nmultiple view points to be probabilistically fused into a map. This not only\nproduces a useful semantic 3D map, but we also show on the NYUv2 dataset that\nfusing multiple predictions leads to an improvement even in the 2D semantic\nlabelling over baseline single frame predictions. We also show that for a\nsmaller reconstruction dataset with larger variation in prediction viewpoint,\nthe improvement over single frame segmentation increases. Our system is\nefficient enough to allow real-time interactive use at frame-rates of\napproximately 25Hz.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 16:46:21 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 14:32:45 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["McCormac", "John", ""], ["Handa", "Ankur", ""], ["Davison", "Andrew", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "1609.05143", "submitter": "Roozbeh Mottaghi", "authors": "Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta,\n  Li Fei-Fei, Ali Farhadi", "title": "Target-driven Visual Navigation in Indoor Scenes using Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two less addressed issues of deep reinforcement learning are (1) lack of\ngeneralization capability to new target goals, and (2) data inefficiency i.e.,\nthe model requires several (and often costly) episodes of trial and error to\nconverge, which makes it impractical to be applied to real-world scenarios. In\nthis paper, we address these two issues and apply our model to the task of\ntarget-driven visual navigation. To address the first issue, we propose an\nactor-critic model whose policy is a function of the goal as well as the\ncurrent state, which allows to better generalize. To address the second issue,\nwe propose AI2-THOR framework, which provides an environment with high-quality\n3D scenes and physics engine. Our framework enables agents to take actions and\ninteract with objects. Hence, we can collect a huge number of training samples\nefficiently.\n  We show that our proposed method (1) converges faster than the\nstate-of-the-art deep reinforcement learning methods, (2) generalizes across\ntargets and across scenes, (3) generalizes to a real robot scenario with a\nsmall amount of fine-tuning (although the model is trained in simulation), (4)\nis end-to-end trainable and does not need feature engineering, feature matching\nbetween frames or 3D reconstruction of the environment.\n  The supplementary video can be accessed at the following link:\nhttps://youtu.be/SmBxMDiOrvs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 17:16:49 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Zhu", "Yuke", ""], ["Mottaghi", "Roozbeh", ""], ["Kolve", "Eric", ""], ["Lim", "Joseph J.", ""], ["Gupta", "Abhinav", ""], ["Fei-Fei", "Li", ""], ["Farhadi", "Ali", ""]]}, {"id": "1609.05158", "submitter": "Wenzhe Shi", "authors": "Wenzhe Shi, Jose Caballero, Ferenc Husz\\'ar, Johannes Totz, Andrew P.\n  Aitken, Rob Bishop, Daniel Rueckert and Zehan Wang", "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient\n  Sub-Pixel Convolutional Neural Network", "comments": "CVPR 2016 paper with updated affiliations and supplemental material,\n  fixed typo in equation 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 17:58:14 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 17:16:37 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Shi", "Wenzhe", ""], ["Caballero", "Jose", ""], ["Husz\u00e1r", "Ferenc", ""], ["Totz", "Johannes", ""], ["Aitken", "Andrew P.", ""], ["Bishop", "Rob", ""], ["Rueckert", "Daniel", ""], ["Wang", "Zehan", ""]]}, {"id": "1609.05257", "submitter": "Marcelo Cicconet", "authors": "Marcelo Cicconet, Vighnesh Birodkar, Mads Lund, Michael Werman, and\n  Davi Geiger", "title": "A convolutional approach to reflection symmetry", "comments": "This paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolutional approach to reflection symmetry detection in 2D.\nOur model, built on the products of complex-valued wavelet convolutions,\nsimplifies previous edge-based pairwise methods. Being parameter-centered, as\nopposed to feature-centered, it has certain computational advantages when the\nobject sizes are known a priori, as demonstrated in an ellipse detection\napplication. The method outperforms the best-performing algorithm on the CVPR\n2013 Symmetry Detection Competition Database in the single-symmetry case. Code\nand a new database for 2D symmetry detection is available.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 00:07:39 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Cicconet", "Marcelo", ""], ["Birodkar", "Vighnesh", ""], ["Lund", "Mads", ""], ["Werman", "Michael", ""], ["Geiger", "Davi", ""]]}, {"id": "1609.05258", "submitter": "J\\\"urgen Leitner", "authors": "J\\\"urgen Leitner, Adam W. Tow, Jake E. Dean, Niko Suenderhauf, Joseph\n  W. Durham, Matthew Cooper, Markus Eich, Christopher Lehnert, Ruben Mangels,\n  Christopher McCool, Peter Kujala, Lachlan Nicholson, Trung Pham, James\n  Sergeant, Liao Wu, Fangyi Zhang, Ben Upcroft, and Peter Corke", "title": "The ACRV Picking Benchmark (APB): A Robotic Shelf Picking Benchmark to\n  Foster Reproducible Research", "comments": "8 pages, submitted to RA:Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA\nChallenges are an established and important way to drive scientific progress.\nThey make research comparable on a well-defined benchmark with equal test\nconditions for all participants. However, such challenge events occur only\noccasionally, are limited to a small number of contestants, and the test\nconditions are very difficult to replicate after the main event. We present a\nnew physical benchmark challenge for robotic picking: the ACRV Picking\nBenchmark (APB). Designed to be reproducible, it consists of a set of 42 common\nobjects, a widely available shelf, and exact guidelines for object arrangement\nusing stencils. A well-defined evaluation protocol enables the comparison of\n\\emph{complete} robotic systems -- including perception and manipulation --\ninstead of sub-systems only. Our paper also describes and reports results\nachieved by an open baseline system based on a Baxter robot.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 00:07:54 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 09:06:49 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Leitner", "J\u00fcrgen", ""], ["Tow", "Adam W.", ""], ["Dean", "Jake E.", ""], ["Suenderhauf", "Niko", ""], ["Durham", "Joseph W.", ""], ["Cooper", "Matthew", ""], ["Eich", "Markus", ""], ["Lehnert", "Christopher", ""], ["Mangels", "Ruben", ""], ["McCool", "Christopher", ""], ["Kujala", "Peter", ""], ["Nicholson", "Lachlan", ""], ["Pham", "Trung", ""], ["Sergeant", "James", ""], ["Wu", "Liao", ""], ["Zhang", "Fangyi", ""], ["Upcroft", "Ben", ""], ["Corke", "Peter", ""]]}, {"id": "1609.05281", "submitter": "Ankit Gandhi", "authors": "Ankit Gandhi, Arjun Sharma, Arijit Biswas, Om Deshmukh", "title": "GeThR-Net: A Generalized Temporally Hybrid Recurrent Neural Network for\n  Multimodal Information Fusion", "comments": "To appear in ECCV workshop on Computer Vision for Audio-Visual Media,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data generated from real world events are usually temporal and contain\nmultimodal information such as audio, visual, depth, sensor etc. which are\nrequired to be intelligently combined for classification tasks. In this paper,\nwe propose a novel generalized deep neural network architecture where temporal\nstreams from multiple modalities are combined. There are total M+1 (M is the\nnumber of modalities) components in the proposed network. The first component\nis a novel temporally hybrid Recurrent Neural Network (RNN) that exploits the\ncomplimentary nature of the multimodal temporal information by allowing the\nnetwork to learn both modality specific temporal dynamics as well as the\ndynamics in a multimodal feature space. M additional components are added to\nthe network which extract discriminative but non-temporal cues from each\nmodality. Finally, the predictions from all of these components are linearly\ncombined using a set of automatically learned weights. We perform exhaustive\nexperiments on three different datasets spanning four modalities. The proposed\nnetwork is relatively 3.5%, 5.7% and 2% better than the best performing\ntemporal multimodal baseline for UCF-101, CCV and Multimodal Gesture datasets\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 04:18:02 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Gandhi", "Ankit", ""], ["Sharma", "Arjun", ""], ["Biswas", "Arijit", ""], ["Deshmukh", "Om", ""]]}, {"id": "1609.05296", "submitter": "Avinash Singh", "authors": "Avinash Kumar Singh, Piyush Joshi, G C Nandi", "title": "Development of a Fuzzy Expert System based Liveliness Detection Scheme\n  for Biometric Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liveliness detection acts as a safe guard against spoofing attacks. Most of\nthe researchers used vision based techniques to detect liveliness of the user,\nbut they are highly sensitive to illumination effects. Therefore it is very\nhard to design a system, which will work robustly under all circumstances.\nLiterature shows that most of the research utilize eye blink or mouth movement\nto detect the liveliness, while the other group used face texture to\ndistinguish between real and imposter. The classification results of all these\napproaches decreases drastically in variable light conditions. Hence in this\npaper we are introducing fuzzy expert system which is sufficient enough to\nhandle most of the cases comes in real time. We have used two testing\nparameters, (a) under bad illumination and (b) less movement in eyes and mouth\nin case of real user to evaluate the performance of the system. The system is\nbehaving well in all, while in first case its False Rejection Rate (FRR) is\n0.28, and in second case its FRR is 0.4.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 08:35:54 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Singh", "Avinash Kumar", ""], ["Joshi", "Piyush", ""], ["Nandi", "G C", ""]]}, {"id": "1609.05317", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Xiao Sun, Wei Zhang, Shuang Liang, Yichen Wei", "title": "Deep Kinematic Pose Regression", "comments": "ECCV Workshop on Geometry Meets Deep Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning articulated object pose is inherently difficult because the pose is\nhigh dimensional but has many structural constraints. Most existing work do not\nmodel such constraints and does not guarantee the geometric validity of their\npose estimation, therefore requiring a post-processing to recover the correct\ngeometry if desired, which is cumbersome and sub-optimal. In this work, we\npropose to directly embed a kinematic object model into the deep neutral\nnetwork learning for general articulated object pose estimation. The kinematic\nfunction is defined on the appropriately parameterized object motion variables.\nIt is differentiable and can be used in the gradient descent based optimization\nin network training. The prior knowledge on the object geometric model is fully\nexploited and the structure is guaranteed to be valid. We show convincing\nexperiment results on a toy example and the 3D human pose estimation problem.\nFor the latter we achieve state-of-the-art result on Human3.6M dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 11:22:11 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Zhou", "Xingyi", ""], ["Sun", "Xiao", ""], ["Zhang", "Wei", ""], ["Liang", "Shuang", ""], ["Wei", "Yichen", ""]]}, {"id": "1609.05342", "submitter": "Reza Borhani", "authors": "Reza Borhani, Jeremy Watt, Aggelos Katsaggelos", "title": "Fast and Effective Algorithms for Symmetric Nonnegative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric Nonnegative Matrix Factorization (SNMF) models arise naturally as\nsimple reformulations of many standard clustering algorithms including the\npopular spectral clustering method. Recent work has demonstrated that an\nelementary instance of SNMF provides superior clustering quality compared to\nmany classic clustering algorithms on a variety of synthetic and real world\ndata sets. In this work, we present novel reformulations of this instance of\nSNMF based on the notion of variable splitting and produce two fast and\neffective algorithms for its optimization using i) the provably convergent\nAccelerated Proximal Gradient (APG) procedure and ii) a heuristic version of\nthe Alternating Direction Method of Multipliers (ADMM) framework. Our two\nalgorithms present an interesting tradeoff between computational speed and\nmathematical convergence guarantee: while the former method is provably\nconvergent it is considerably slower than the latter approach, for which we\nalso provide significant but less stringent mathematical proof regarding its\nconvergence. Through extensive experiments we show not only that the efficacy\nof these approaches is equal to that of the state of the art SNMF algorithm,\nbut also that the latter of our algorithms is extremely fast being one to two\norders of magnitude faster in terms of total computation time than the state of\nthe art approach, outperforming even spectral clustering in terms of\ncomputation time on large data sets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 14:41:32 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Borhani", "Reza", ""], ["Watt", "Jeremy", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "1609.05396", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Benjam\\'in Guti\\'errez-Becker, Diana Mateus, Nassir\n  Navab, Nikos Komodakis", "title": "A Deep Metric for Multimodal Registration", "comments": "Accepted to MICCAI 2016; extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal registration is a challenging problem in medical imaging due the\nhigh variability of tissue appearance under different imaging modalities. The\ncrucial component here is the choice of the right similarity measure. We make a\nstep towards a general learning-based solution that can be adapted to specific\nsituations and present a metric based on a convolutional neural network. Our\nnetwork can be trained from scratch even from a few aligned image pairs. The\nmetric is validated on intersubject deformable registration on a dataset\ndifferent from the one used for training, demonstrating good generalization. In\nthis task, we outperform mutual information by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 21:46:21 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Simonovsky", "Martin", ""], ["Guti\u00e9rrez-Becker", "Benjam\u00edn", ""], ["Mateus", "Diana", ""], ["Navab", "Nassir", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1609.05420", "submitter": "Senthil Purushwalkam", "authors": "Senthil Purushwalkam, Abhinav Gupta", "title": "Pose from Action: Unsupervised Learning of Pose Features based on Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions are comprised of a sequence of poses. This makes videos of\nhumans a rich and dense source of human poses. We propose an unsupervised\nmethod to learn pose features from videos that exploits a signal which is\ncomplementary to appearance and can be used as supervision: motion. The key\nidea is that humans go through poses in a predictable manner while performing\nactions. Hence, given two poses, it should be possible to model the motion that\ncaused the change between them. We represent each of the poses as a feature in\na CNN (Appearance ConvNet) and generate a motion encoding from optical flow\nmaps using a separate CNN (Motion ConvNet). The data for this task is\nautomatically generated allowing us to train without human supervision. We\ndemonstrate the strength of the learned representation by finetuning the\ntrained model for Pose Estimation on the FLIC dataset, for static image action\nrecognition on PASCAL and for action recognition in videos on UCF101 and\nHMDB51.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 04:18:42 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Purushwalkam", "Senthil", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1609.05434", "submitter": "Yoni Choukroun", "authors": "Alex Bronstein, Yoni Choukroun, Ron Kimmel, Matan Sela", "title": "Consistent Discretization and Minimization of the L1 Norm on Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1 norm has been tremendously popular in signal and image processing in\nthe past two decades due to its sparsity-promoting properties. More recently,\nits generalization to non-Euclidean domains has been found useful in shape\nanalysis applications. For example, in conjunction with the minimization of the\nDirichlet energy, it was shown to produce a compactly supported quasi-harmonic\northonormal basis, dubbed as compressed manifold modes. The continuous L1 norm\non the manifold is often replaced by the vector l1 norm applied to sampled\nfunctions. We show that such an approach is incorrect in the sense that it does\nnot consistently discretize the continuous norm and warn against its\nsensitivity to the specific sampling. We propose two alternative\ndiscretizations resulting in an iteratively-reweighed l2 norm. We demonstrate\nthe proposed strategy on the compressed modes problem, which reduces to a\nsequence of simple eigendecomposition problems not requiring non-convex\noptimization on Stiefel manifolds and producing more stable and accurate\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 06:56:57 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Bronstein", "Alex", ""], ["Choukroun", "Yoni", ""], ["Kimmel", "Ron", ""], ["Sela", "Matan", ""]]}, {"id": "1609.05483", "submitter": "Sze Zheng Yong", "authors": "Prince Singh, Sze Zheng Yong, Emilio Frazzoli", "title": "Set-Point Regulation of Linear Continuous-Time Systems using\n  Neuromorphic Vision Sensors", "comments": "Submitted to IEEE Transactions on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed neuromorphic vision sensors have become promising\ncandidates for agile and autonomous robotic applications primarily due to, in\nparticular, their high temporal resolution and low latency. Each pixel of this\nsensor independently fires an asynchronous stream of \"retinal events\" once a\nchange in the light field is detected. Existing computer vision algorithms can\nonly process periodic frames and so a new class of algorithms needs to be\ndeveloped that can efficiently process these events for control tasks. In this\npaper, we investigate the problem of regulating a continuous-time linear time\ninvariant (LTI) system to a desired point using measurements from a\nneuromorphic sensor. We present an $H_\\infty$ controller that regulates the LTI\nsystem to a desired set-point and provide the set of neuromorphic sensor based\ncameras for the given system that fulfill the regulation task. The\neffectiveness of our approach is illustrated on an unstable system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 13:11:19 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Singh", "Prince", ""], ["Yong", "Sze Zheng", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1609.05522", "submitter": "Mona Fathollahi", "authors": "Mona Fathollahi Ghezelghieh, Rangachar Kasturi, Sudeep Sarkar", "title": "Learning camera viewpoint using CNN to improve 3D body pose estimation", "comments": "To appear at the International Conference on 3D Vision (3DV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to estimate 3D human pose from a single RGB\nimage. Extracting image representations which incorporate both spatial relation\nof body parts and their relative depth plays an essential role in accurate3D\npose reconstruction. In this paper, for the first time, we show that camera\nviewpoint in combination to 2D joint lo-cations significantly improves 3D pose\naccuracy without the explicit use of perspective geometry mathematical\nmodels.To this end, we train a deep Convolutional Neural Net-work (CNN) to\nlearn categorical camera viewpoint. To make the network robust against clothing\nand body shape of the subject in the image, we utilized 3D computer rendering\nto synthesize additional training images. We test our framework on the largest\n3D pose estimation bench-mark, Human3.6m, and achieve up to 20% error reduction\ncompared to the state-of-the-art approaches that do not use body part\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 17:56:15 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ghezelghieh", "Mona Fathollahi", ""], ["Kasturi", "Rangachar", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1609.05561", "submitter": "Ricardo Fabbri", "authors": "Anil Usumezbas and Ricardo Fabbri and Benjamin B. Kimia", "title": "From Multiview Image Curves to 3D Drawings", "comments": "Expanded ECCV 2016 version with tweaked figures and including an\n  overview of the supplementary material available at\n  multiview-3d-drawing.sourceforge.net", "journal-ref": "Lecture Notes in Computer Science, 9908, pp 70-87, september 2016", "doi": "10.1007/978-3-319-46493-0_5", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing 3D scenes from multiple views has made impressive strides in\nrecent years, chiefly by correlating isolated feature points, intensity\npatterns, or curvilinear structures. In the general setting - without\ncontrolled acquisition, abundant texture, curves and surfaces following\nspecific models or limiting scene complexity - most methods produce unorganized\npoint clouds, meshes, or voxel representations, with some exceptions producing\nunorganized clouds of 3D curve fragments. Ideally, many applications require\nstructured representations of curves, surfaces and their spatial relationships.\nThis paper presents a step in this direction by formulating an approach that\ncombines 2D image curves into a collection of 3D curves, with topological\nconnectivity between them represented as a 3D graph. This results in a 3D\ndrawing, which is complementary to surface representations in the same sense as\na 3D scaffold complements a tent taut over it. We evaluate our results against\ntruth on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 22:20:35 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Usumezbas", "Anil", ""], ["Fabbri", "Ricardo", ""], ["Kimia", "Benjamin B.", ""]]}, {"id": "1609.05583", "submitter": "Seyed Ali Amirshahi Seyed Ali Amirshahi", "authors": "Seyed Ali Amirshahi, Gregor Uwe Hayn-Leichsenring, Joachim Denzler,\n  Christoph Redies", "title": "Color: A Crucial Factor for Aesthetic Quality Assessment in a Subjective\n  Dataset of Paintings", "comments": "This paper was presented at the AIC 2013 Congress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational aesthetics is an emerging field of research which has attracted\ndifferent research groups in the last few years. In this field, one of the main\napproaches to evaluate the aesthetic quality of paintings and photographs is a\nfeature-based approach. Among the different features proposed to reach this\ngoal, color plays an import role. In this paper, we introduce a novel dataset\nthat consists of paintings of Western provenance from 36 well-known painters\nfrom the 15th to the 20th century. As a first step and to assess this dataset,\nusing a classifier, we investigate the correlation between the subjective\nscores and two widely used features that are related to color perception and in\ndifferent aesthetic quality assessment approaches. Results show a\nclassification rate of up to 73% between the color features and the subjective\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 02:17:34 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Amirshahi", "Seyed Ali", ""], ["Hayn-Leichsenring", "Gregor Uwe", ""], ["Denzler", "Joachim", ""], ["Redies", "Christoph", ""]]}, {"id": "1609.05590", "submitter": "Patrick Poirson", "authors": "Patrick Poirson, Phil Ammirato, Cheng-Yang Fu, Wei Liu, Jana Kosecka,\n  Alexander C. Berg", "title": "Fast Single Shot Detection and Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications in navigation and robotics, estimating the 3D pose of\nobjects is as important as detection. Many approaches to pose estimation rely\non detecting or tracking parts or keypoints [11, 21]. In this paper we build on\na recent state-of-the-art convolutional network for slidingwindow detection\n[10] to provide detection and rough pose estimation in a single shot, without\nintermediate stages of detecting parts or initial bounding boxes. While not the\nfirst system to treat pose estimation as a categorization problem, this is the\nfirst attempt to combine detection and pose estimation at the same level using\na deep learning approach. The key to the architecture is a deep convolutional\nnetwork where scores for the presence of an object category, the offset for its\nlocation, and the approximate pose are all estimated on a regular grid of\nlocations in the image. The resulting system is as accurate as recent work on\npose estimation (42.4% 8 View mAVP on Pascal 3D+ [21] ) and significantly\nfaster (46 frames per second (FPS) on a TITAN X GPU). This approach to\ndetection and rough pose estimation is fast and accurate enough to be widely\napplied as a pre-processing step for tasks including high-accuracy pose\nestimation, object tracking and localization, and vSLAM.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 03:38:15 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Poirson", "Patrick", ""], ["Ammirato", "Phil", ""], ["Fu", "Cheng-Yang", ""], ["Liu", "Wei", ""], ["Kosecka", "Jana", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1609.05600", "submitter": "Damien Teney", "authors": "Damien Teney, Lingqiao Liu, Anton van den Hengel", "title": "Graph-Structured Representations for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to improve visual question answering (VQA) with\nstructured representations of both scene contents and questions. A key\nchallenge in VQA is to require joint reasoning over the visual and text\ndomains. The predominant CNN/LSTM-based approach to VQA is limited by\nmonolithic vector representations that largely ignore structure in the scene\nand in the form of the question. CNN feature vectors cannot effectively capture\nsituations as simple as multiple object instances, and LSTMs process questions\nas series of words, which does not reflect the true complexity of language\nstructure. We instead propose to build graphs over the scene objects and over\nthe question words, and we describe a deep neural network that exploits the\nstructure in these representations. This shows significant benefit over the\nsequential processing of LSTMs. The overall efficacy of our approach is\ndemonstrated by significant improvements over the state-of-the-art, from 71.2%\nto 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and\nfrom 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images\nwith fine-grained differences and opposite yes/no answers to a same question.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 05:21:36 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 04:26:26 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Teney", "Damien", ""], ["Liu", "Lingqiao", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1609.05619", "submitter": "Hassan Alhajj Hassan ALHAJJ", "authors": "Hassan Al Hajj, Gwenol\\'e Quellec, Mathieu Lamard, Guy Cazuguel,\n  B\\'eatrice Cochener", "title": "Coarse-to-fine Surgical Instrument Detection for Cataract Surgery\n  Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of surgical data, recorded during video-monitored surgeries, has\nextremely increased. This paper aims at improving existing solutions for the\nautomated analysis of cataract surgeries in real time. Through the analysis of\na video recording the operating table, it is possible to know which instruments\nexit or enter the operating table, and therefore which ones are likely being\nused by the surgeon. Combining these observations with observations from the\nmicroscope video should enhance the overall performance of the systems. To this\nend, the proposed solution is divided into two main parts: one to detect the\ninstruments at the beginning of the surgery and one to update the list of\ninstruments every time a change is detected in the scene. In the first part,\nthe goal is to separate the instruments from the background and from irrelevant\nobjects. For the second, we are interested in detecting the instruments that\nappear and disappear whenever the surgeon interacts with the table. Experiments\non a dataset of 36 cataract surgeries validate the good performance of the\nproposed solution.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 07:40:41 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Hajj", "Hassan Al", ""], ["Quellec", "Gwenol\u00e9", ""], ["Lamard", "Mathieu", ""], ["Cazuguel", "Guy", ""], ["Cochener", "B\u00e9atrice", ""]]}, {"id": "1609.05672", "submitter": "Masoud Abdi", "authors": "Masoud Abdi and Saeid Nahavandi", "title": "Multi-Residual Networks: Improving the Speed and Accuracy of Residual\n  Networks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we take one step toward understanding the learning behavior\nof deep residual networks, and supporting the observation that deep residual\nnetworks behave like ensembles. We propose a new convolutional neural network\narchitecture which builds upon the success of residual networks by explicitly\nexploiting the interpretation of very deep networks as an ensemble. The\nproposed multi-residual network increases the number of residual functions in\nthe residual blocks. Our architecture generates models that are wider, rather\nthan deeper, which significantly improves accuracy. We show that our model\nachieves an error rate of 3.73% and 19.45% on CIFAR-10 and CIFAR-100\nrespectively, that outperforms almost all of the existing models. We also\ndemonstrate that our model outperforms very deep residual networks by 0.22%\n(top-1 error) on the full ImageNet 2012 classification dataset. Additionally,\ninspired by the parallel structure of multi-residual networks, a model\nparallelism technique has been investigated. The model parallelism method\ndistributes the computation of residual blocks among the processors, yielding\nup to 15% computational complexity improvement.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 11:34:24 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 02:12:35 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 23:44:21 GMT"}, {"version": "v4", "created": "Wed, 15 Mar 2017 08:50:00 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Abdi", "Masoud", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1609.05695", "submitter": "Mengnan Shi", "authors": "Mengnan Shi, Fei Qin, Qixiang Ye, Zhenjun Han, Jianbin Jiao", "title": "A scalable convolutional neural network for task-specified scenarios via\n  knowledge distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the redundancy in convolutional neural network,\nwhich scales with the complexity of vision tasks. Considering that many\nfront-end visual systems are interested in only a limited range of visual\ntargets, the removing of task-specified network redundancy can promote a wide\nrange of potential applications. We propose a task-specified knowledge\ndistillation algorithm to derive a simplified model with pre-set computation\ncost and minimized accuracy loss, which suits the resource constraint front-end\nsystems well. Experiments on the MNIST and CIFAR10 datasets demonstrate the\nfeasibility of the proposed approach as well as the existence of task-specified\nredundancy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 12:43:32 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 13:57:47 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Shi", "Mengnan", ""], ["Qin", "Fei", ""], ["Ye", "Qixiang", ""], ["Han", "Zhenjun", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1609.05722", "submitter": "Yunjin Chen", "authors": "Wensen Feng, Hong Qiao, and Yunjin Chen", "title": "Poisson Noise Reduction with Higher-order Natural Image Prior Model", "comments": "31 pages, 10 figures. To appear in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson denoising is an essential issue for various imaging applications,\nsuch as night vision, medical imaging and microscopy. State-of-the-art\napproaches are clearly dominated by patch-based non-local methods in recent\nyears. In this paper, we aim to propose a local Poisson denoising model with\nboth structure simplicity and good performance. To this end, we consider a\nvariational modeling to integrate the so-called Fields of Experts (FoE) image\nprior, that has proven an effective higher-order Markov Random Fields (MRF)\nmodel for many classic image restoration problems. We exploit several feasible\nvariational variants for this task. We start with a direct modeling in the\noriginal image domain by taking into account the Poisson noise statistics,\nwhich performs generally well for the cases of high SNR. However, this strategy\nencounters problem in cases of low SNR. Then we turn to an alternative modeling\nstrategy by using the Anscombe transform and Gaussian statistics derived data\nterm. We retrain the FoE prior model directly in the transform domain. With the\nnewly trained FoE model, we end up with a local variational model providing\nstrongly competitive results against state-of-the-art non-local approaches,\nmeanwhile bearing the property of simple structure. Furthermore, our proposed\nmodel comes along with an additional advantage, that the inference is very\nefficient as it is well-suited for parallel computation on GPUs. For images of\nsize $512 \\times 512$, our GPU implementation takes less than 1 second to\nproduce state-of-the-art Poisson denoising performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 13:35:14 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Feng", "Wensen", ""], ["Qiao", "Hong", ""], ["Chen", "Yunjin", ""]]}, {"id": "1609.05797", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti and Alexander Krull and Eric Brachmann and Carsten\n  Rother and Philip H.S. Torr", "title": "Random Forests versus Neural Networks - What's Best for Camera\n  Localization?", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the task of camera localization in a known 3D scene given\na single input RGB image. State-of-the-art approaches accomplish this in two\nsteps: firstly, regressing for every pixel in the image its 3D scene coordinate\nand subsequently, using these coordinates to estimate the final 6D camera pose\nvia RANSAC. To solve the first step, Random Forests (RFs) are typically used.\nOn the other hand, Neural Networks (NNs) reign in many dense regression tasks,\nbut are not test-time efficient. We ask the question: which of the two is best\nfor camera localization? To address this, we make two method contributions: (1)\na test-time efficient NN architecture which we term a ForestNet that is derived\nand initialized from a RF, and (2) a new fully-differentiable robust averaging\ntechnique for regression ensembles which can be trained end-to-end with a NN.\nOur experimental findings show that for scene coordinate regression,\ntraditional NN architectures are superior to test-time efficient RFs and\nForestNets, however, this does not translate to final 6D camera pose accuracy\nwhere RFs and ForestNets perform slightly better. To summarize, our best\nmethod, a ForestNet with a robust average, which has an equivalent fast and\nlightweight RF, improves over the state-of-the-art for camera localization on\nthe 7-Scenes dataset. While this work focuses on scene coordinate regression\nfor camera localization, our innovations may also be applied to other\ncontinuous regression tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 15:50:25 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 17:36:00 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 08:52:13 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Massiceti", "Daniela", ""], ["Krull", "Alexander", ""], ["Brachmann", "Eric", ""], ["Rother", "Carsten", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1609.05820", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Emmanuel Candes", "title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment\n  from Pairwise Differences", "comments": "Accepted to Communications on Pure and Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various applications involve assigning discrete label values to a collection\nof objects based on some pairwise noisy data. Due to the discrete---and hence\nnonconvex---structure of the problem, computing the optimal assignment\n(e.g.~maximum likelihood assignment) becomes intractable at first sight. This\npaper makes progress towards efficient computation by focusing on a concrete\njoint alignment problem---that is, the problem of recovering $n$ discrete\nvariables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations\nof their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a\nlow-complexity and model-free procedure, which operates in a lifted space by\nrepresenting distinct label values in orthogonal directions, and which attempts\nto optimize quadratic functions over hypercubes. Starting with a first guess\ncomputed via a spectral method, the algorithm successively refines the iterates\nvia projected power iterations. We prove that for a broad class of statistical\nmodels, the proposed projected power method makes no error---and hence\nconverges to the maximum likelihood estimate---in a suitable regime. Numerical\nexperiments have been carried out on both synthetic and real data to\ndemonstrate the practicality of our algorithm. We expect this algorithmic\nframework to be effective for a broad range of discrete assignment problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:29:46 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 21:59:14 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 20:30:54 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chen", "Yuxin", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1609.05834", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Wentong Liao, Hanno Ackermann and Bodo Rosenhahn", "title": "On Support Relations and Semantic Scene Graphs", "comments": "Accepted in ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2017.07.010", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding is a popular and challenging topic in both computer\nvision and photogrammetry. Scene graph provides rich information for such scene\nunderstanding. This paper presents a novel approach to infer such relations and\nthen to construct the scene graph. Support relations are estimated by\nconsidering important, previously ignored information: the physical stability\nand the prior support knowledge between object classes. In contrast to previous\nmethods for extracting support relations, the proposed approach generates more\naccurate results, and does not require a pixel-wise semantic labeling of the\nscene. The semantic scene graph which describes all the contextual relations\nwithin the scene is constructed using this information. To evaluate the\naccuracy of these graphs, multiple different measures are formulated. The\nproposed algorithms are evaluated using the NYUv2 database. The results\ndemonstrate that the inferred support relations are more precise than\nstate-of-the-art. The scene graphs are compared against ground truth graphs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 17:21:55 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 13:58:12 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 06:44:25 GMT"}, {"version": "v4", "created": "Thu, 16 Nov 2017 11:07:23 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Yang", "Michael Ying", ""], ["Liao", "Wentong", ""], ["Ackermann", "Hanno", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1609.05871", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Abhijit Guha Roy, Debdoot Sheet, Prabir Kumar Biswas", "title": "Deep Neural Ensemble for Retinal Vessel Segmentation in Fundus Images\n  towards Achieving Label-free Angiography", "comments": "Accepted as a conference paper at IEEE EMBC, 2016", "journal-ref": null, "doi": "10.1109/EMBC.2016.7590955", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of retinal blood vessels in label-free fundus images\nentails a pivotal role in computed aided diagnosis of ophthalmic pathologies,\nviz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases.\nThe challenge remains active in medical image analysis research due to varied\ndistribution of blood vessels, which manifest variations in their dimensions of\nphysical appearance against a noisy background.\n  In this paper we formulate the segmentation challenge as a classification\ntask. Specifically, we employ unsupervised hierarchical feature learning using\nensemble of two level of sparsely trained denoised stacked autoencoder. First\nlevel training with bootstrap samples ensures decoupling and second level\nensemble formed by different network architectures ensures architectural\nrevision. We show that ensemble training of auto-encoders fosters diversity in\nlearning dictionary of visual kernels for vessel segmentation. SoftMax\nclassifier is used for fine tuning each member auto-encoder and multiple\nstrategies are explored for 2-level fusion of ensemble members. On DRIVE\ndataset, we achieve maximum average accuracy of 95.33\\% with an impressively\nlow standard deviation of 0.003 and Kappa agreement coefficient of 0.708 .\nComparison with other major algorithms substantiates the high efficacy of our\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:11:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lahiri", "Avisek", ""], ["Roy", "Abhijit Guha", ""], ["Sheet", "Debdoot", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1609.05993", "submitter": "Jonathan Kelly", "authors": "Valentin Peretroukhin, Lee Clement, and Jonathan Kelly", "title": "Reducing Drift in Visual Odometry by Inferring Sun Direction Using a\n  Bayesian Convolutional Neural Network", "comments": "In Proceedings of the IEEE International Conference on Robotics and\n  Automation (ICRA'17), Singapore, May 29-Jun. 3, 2017", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989235", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to incorporate global orientation information from the\nsun into a visual odometry pipeline using only the existing image stream, where\nthe sun is typically not visible. We leverage recent advances in Bayesian\nConvolutional Neural Networks to train and implement a sun detection model that\ninfers a three-dimensional sun direction vector from a single RGB image.\nCrucially, our method also computes a principled uncertainty associated with\neach prediction, using a Monte Carlo dropout scheme. We incorporate this\nuncertainty into a sliding window stereo visual odometry pipeline where\naccurate uncertainty estimates are critical for optimal data fusion. Our\nBayesian sun detection model achieves a median error of approximately 12\ndegrees on the KITTI odometry benchmark training set, and yields improvements\nof up to 42% in translational ARMSE and 32% in rotational ARMSE compared to\nstandard VO. An open source implementation of our Bayesian CNN sun estimator\n(Sun-BCNN) using Caffe is available at https://github.\ncom/utiasSTARS/sun-bcnn-vo\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 02:02:51 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 17:11:12 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 02:39:04 GMT"}, {"version": "v4", "created": "Fri, 28 Jul 2017 02:45:42 GMT"}, {"version": "v5", "created": "Sun, 18 Aug 2019 01:13:36 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Peretroukhin", "Valentin", ""], ["Clement", "Lee", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1609.06018", "submitter": "Junxuan Chen", "authors": "Junxuan Chen, Baigui Sun, Hao Li, Hongtao Lu, Xian-Sheng Hua", "title": "Deep CTR Prediction in Display Advertising", "comments": "This manuscript is the accepted version for ACM Multimedia Conference\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click through rate (CTR) prediction of image ads is the core task of online\ndisplay advertising systems, and logistic regression (LR) has been frequently\napplied as the prediction model. However, LR model lacks the ability of\nextracting complex and intrinsic nonlinear features from handcrafted\nhigh-dimensional image features, which limits its effectiveness. To solve this\nissue, in this paper, we introduce a novel deep neural network (DNN) based\nmodel that directly predicts the CTR of an image ad based on raw image pixels\nand other basic features in one step. The DNN model employs convolution layers\nto automatically extract representative visual features from images, and\nnonlinear CTR features are then learned from visual features and other\ncontextual features by using fully-connected layers. Empirical evaluations on a\nreal world dataset with over 50 million records demonstrate the effectiveness\nand efficiency of this method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 04:50:03 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Chen", "Junxuan", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""], ["Lu", "Hongtao", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1609.06020", "submitter": "Hiroyuki Kudo", "authors": "Hiroyuki Kudo, Keita Takaki, Fukashi Yamazaki, and Takuya Nemoto", "title": "Proposal of fault-tolerant tomographic image reconstruction", "comments": "12 pages, 5 figures, SPIE Optics + Photonics 2016 Conference\n  (Developments in X-Ray Tomography X) Paper No. 9967-55", "journal-ref": null, "doi": "10.1117/12.2237107", "report-no": null, "categories": "physics.med-ph cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with tomographic image reconstruction under the situation\nwhere some of projection data bins are contaminated with abnormal data. Such\nsituations occur in various instances of tomography. We propose a new\nreconstruction algorithm called the Fault-Tolerant reconstruction outlined as\nfollows. The least-squares (L2-norm) error function ||Ax-b||_2^2 used in\nordinary iterative reconstructions is sensitive to the existence of abnormal\ndata. The proposed algorithm utilizes the L1-norm error function ||Ax-b||_1^1\ninstead of the L2-norm, and we develop a row-action-type iterative algorithm\nusing the proximal splitting framework in convex optimization fields. We also\npropose an improved version of the L1-norm reconstruction called the L1-TV\nreconstruction, in which a weak Total Variation (TV) penalty is added to the\ncost function. Simulation results demonstrate that reconstructed images with\nthe L2-norm were severely damaged by the effect of abnormal bins, whereas\nimages with the L1-norm and L1-TV reconstructions were robust to the existence\nof abnormal bins.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 05:09:47 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kudo", "Hiroyuki", ""], ["Takaki", "Keita", ""], ["Yamazaki", "Fukashi", ""], ["Nemoto", "Takuya", ""]]}, {"id": "1609.06024", "submitter": "Minkyoung Cho", "authors": "Minkyoung Cho, Younggi Kim, and Younghee Lee", "title": "Contextual Relationship-based Activity Segmentation on an Event Stream\n  in the IoT Environment with Multi-user Activities", "comments": "6 pages, 6 figures, 3 tables, M4IOT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human activity recognition in the IoT environment plays the central role\nin the ambient assisted living, where the human activities can be represented\nas a concatenated event stream generated from various smart objects. From the\nconcatenated event stream, each activity should be distinguished separately for\nthe human activity recognition to provide services that users may need. In this\nregard, accurately segmenting the entire stream at the precise boundary of each\nactivity is indispensable high priority task to realize the activity\nrecognition. Multiple human activities in an IoT environment generate varying\nevent stream patterns, and the unpredictability of these patterns makes them\ninclude redundant or missing events. In dealing with this complex segmentation\nproblem, we figured out that the dynamic and confusing patterns cause major\nproblems due to: inclusive event stream, redundant events, and shared events.\nTo address these problems, we exploited the contextual relationships associated\nwith the activity status about either ongoing or terminated/started. To\ndiscover the intrinsic relationships between the events in a stream, we\nutilized the LSTM model by rendering it for the activity segmentation. Then,\nthe inferred boundaries were revised by our validation algorithm for a bit\nshifted boundaries. Our experiments show the surprising result of high accuracy\nabove 95%, on our own testbed with various smart objects. This is superior to\nthe prior works that even do not assume the environment with multi-user\nactivities, where their accuracies are slightly above 80% in their test\nenvironment. It proves that our work is feasible enough to be applied in the\nIoT environment.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 05:37:56 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Cho", "Minkyoung", ""], ["Kim", "Younggi", ""], ["Lee", "Younghee", ""]]}, {"id": "1609.06041", "submitter": "Hiroyuki Kudo", "authors": "Hiroyuki Kudo, Fukashi Yamazaki, Takuya Nemoto, and Keita Takaki", "title": "A very fast iterative algorithm for TV-regularized image reconstruction\n  with applications to low-dose and few-view CT", "comments": "16 pages, 8 figures, SPIE Optics + Photonics 2016 Conference\n  (Developments in X-Ray Tomography X) Paper No. 9967-37", "journal-ref": null, "doi": "10.1117/12.2236788", "report-no": null, "categories": "physics.med-ph cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns iterative reconstruction for low-dose and few-view CT by\nminimizing a data-fidelity term regularized with the Total Variation (TV)\npenalty. We propose a very fast iterative algorithm to solve this problem. The\nalgorithm derivation is outlined as follows. First, the original minimization\nproblem is reformulated into the saddle point (primal-dual) problem by using\nthe Lagrangian duality, to which we apply the first-order primal-dual iterative\nmethods. Second, we precondition the iteration formula using the ramp flter of\nFiltered Backprojection (FBP) reconstruction algorithm in such a way that the\nproblem solution is not altered. The resulting algorithm resembles the\nstructure of so-called iterative FBP algorithm, and it converges to the exact\nminimizer of cost function very fast.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 07:21:03 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kudo", "Hiroyuki", ""], ["Yamazaki", "Fukashi", ""], ["Nemoto", "Takuya", ""], ["Takaki", "Keita", ""]]}, {"id": "1609.06074", "submitter": "Nicolas Dobigeon", "authors": "Vinicius Ferraris, Nicolas Dobigeon, Qi Wei and Marie Chabert", "title": "Detecting Changes Between Optical Images of Different Spatial and\n  Spectral Resolutions: a Fusion-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is one of the most challenging issues when analyzing\nremotely sensed images. Comparing several multi-date images acquired through\nthe same kind of sensor is the most common scenario. Conversely, designing\nrobust, flexible and scalable algorithms for change detection becomes even more\nchallenging when the images have been acquired by two different kinds of\nsensors. This situation arises in case of emergency under critical constraints.\nThis paper presents, to the best of authors' knowledge, the first strategy to\ndeal with optical images characterized by dissimilar spatial and spectral\nresolutions. Typical considered scenarios include change detection between\npanchromatic or multispectral and hyperspectral images. The proposed strategy\nconsists of a 3-step procedure: i) inferring a high spatial and spectral\nresolution image by fusion of the two observed images characterized one by a\nlow spatial resolution and the other by a low spectral resolution, ii)\npredicting two images with respectively the same spatial and spectral\nresolutions as the observed images by degradation of the fused one and iii)\nimplementing a decision rule to each pair of observed and predicted images\ncharacterized by the same spatial and spectral resolutions to identify changes.\nThe performance of the proposed framework is evaluated on real images with\nsimulated realistic changes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 09:58:35 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Ferraris", "Vinicius", ""], ["Dobigeon", "Nicolas", ""], ["Wei", "Qi", ""], ["Chabert", "Marie", ""]]}, {"id": "1609.06076", "submitter": "Nicolas Dobigeon", "authors": "Vinicius Ferraris, Nicolas Dobigeon, Qi Wei and Marie Chabert", "title": "Robust Fusion of Multi-Band Images with Different Spatial and Spectral\n  Resolutions for Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal scenarios for change detection generally consider two images\nacquired through sensors of the same modality. However, in some specific cases\nsuch as emergency situations, the only images available may be those acquired\nthrough different kinds of sensors. More precisely, this paper addresses the\nproblem of detecting changes between two multi-band optical images\ncharacterized by different spatial and spectral resolutions. This sensor\ndissimilarity introduces additional issues in the context of operational change\ndetection. To alleviate these issues, classical change detection methods are\napplied after independent preprocessing steps (e.g., resampling) used to get\nthe same spatial and spectral resolutions for the pair of observed images.\nNevertheless, these preprocessing steps tend to throw away relevant\ninformation. Conversely, in this paper, we propose a method that more\neffectively uses the available information by modeling the two observed images\nas spatial and spectral versions of two (unobserved) latent images\ncharacterized by the same high spatial and high spectral resolutions. As they\ncover the same scene, these latent images are expected to be globally similar\nexcept for possible changes in sparse spatial locations. Thus, the change\ndetection task is envisioned through a robust multi-band image fusion method\nwhich enforces the differences between the estimated latent images to be\nspatially sparse. This robust fusion problem is formulated as an inverse\nproblem which is iteratively solved using an efficient block-coordinate descent\nalgorithm. The proposed method is applied to real panchormatic/multispectral\nand hyperspectral images with simulated realistic changes. A comparison with\nstate-of-the-art change detection methods evidences the accuracy of the\nproposed strategy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 10:04:04 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Ferraris", "Vinicius", ""], ["Dobigeon", "Nicolas", ""], ["Wei", "Qi", ""], ["Chabert", "Marie", ""]]}, {"id": "1609.06118", "submitter": "Martin Danelljan", "authors": "Martin Danelljan, Gustav H\\\"ager, Fahad Shahbaz Khan, Michael Felsberg", "title": "Adaptive Decontamination of the Training Set: A Unified Formulation for\n  Discriminative Visual Tracking", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking-by-detection methods have demonstrated competitive performance in\nrecent years. In these approaches, the tracking model heavily relies on the\nquality of the training set. Due to the limited amount of labeled training\ndata, additional samples need to be extracted and labeled by the tracker\nitself. This often leads to the inclusion of corrupted training samples, due to\nocclusions, misalignments and other perturbations. Existing\ntracking-by-detection methods either ignore this problem, or employ a separate\ncomponent for managing the training set.\n  We propose a novel generic approach for alleviating the problem of corrupted\ntraining samples in tracking-by-detection frameworks. Our approach dynamically\nmanages the training set by estimating the quality of the samples. Contrary to\nexisting approaches, we propose a unified formulation by minimizing a single\nloss over both the target appearance model and the sample quality weights. The\njoint formulation enables corrupted samples to be down-weighted while\nincreasing the impact of correct ones. Experiments are performed on three\nbenchmarks: OTB-2015 with 100 videos, VOT-2015 with 60 videos, and Temple-Color\nwith 128 videos. On the OTB-2015, our unified formulation significantly\nimproves the baseline, with a gain of 3.8% in mean overlap precision. Finally,\nour method achieves state-of-the-art results on all three datasets. Code and\nsupplementary material are available at\nhttp://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html .\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:46:17 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Danelljan", "Martin", ""], ["H\u00e4ger", "Gustav", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1609.06141", "submitter": "Martin Danelljan", "authors": "Martin Danelljan, Gustav H\\\"ager, Fahad Shahbaz Khan, Michael Felsberg", "title": "Discriminative Scale Space Tracking", "comments": "To appear in TPAMI. This is the journal extension of the\n  VOT2014-winning DSST tracking method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate scale estimation of a target is a challenging research problem in\nvisual object tracking. Most state-of-the-art methods employ an exhaustive\nscale search to estimate the target size. The exhaustive search strategy is\ncomputationally expensive and struggles when encountered with large scale\nvariations. This paper investigates the problem of accurate and robust scale\nestimation in a tracking-by-detection framework. We propose a novel scale\nadaptive tracking approach by learning separate discriminative correlation\nfilters for translation and scale estimation. The explicit scale filter is\nlearned online using the target appearance sampled at a set of different\nscales. Contrary to standard approaches, our method directly learns the\nappearance change induced by variations in the target scale. Additionally, we\ninvestigate strategies to reduce the computational cost of our approach.\n  Extensive experiments are performed on the OTB and the VOT2014 datasets.\nCompared to the standard exhaustive scale search, our approach achieves a gain\nof 2.5% in average overlap precision on the OTB dataset. Additionally, our\nmethod is computationally efficient, operating at a 50% higher frame rate\ncompared to the exhaustive scale search. Our method obtains the top rank in\nperformance by outperforming 19 state-of-the-art trackers on OTB and 37\nstate-of-the-art trackers on VOT2014.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 12:57:08 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Danelljan", "Martin", ""], ["H\u00e4ger", "Gustav", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1609.06188", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Hendrik P.A. Lensch", "title": "Transfer Learning for Material Classification using Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material classification in natural settings is a challenge due to complex\ninterplay of geometry, reflectance properties, and illumination. Previous work\non material classification relies strongly on hand-engineered features of\nvisual samples. In this work we use a Convolutional Neural Network (convnet)\nthat learns descriptive features for the specific task of material recognition.\nSpecifically, transfer learning from the task of object recognition is\nexploited to more effectively train good features for material classification.\nThe approach of transfer learning using convnets yields significantly higher\nrecognition rates when compared to previous state-of-the-art approaches. We\nthen analyze the relative contribution of reflectance and shading information\nby a decomposition of the image into its intrinsic components. The use of\nconvnets for material classification was hindered by the strong demand for\nsufficient and diverse training data, even with transfer learning approaches.\nTherefore, we present a new data set containing approximately 10k images\ndivided into 10 material categories.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 14:15:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1609.06192", "submitter": "Florian Dubost", "authors": "Florian Dubost, Loic Peter, Christian Rupprecht, Benjamin\n  Gutierrez-Becker, Nassir Navab", "title": "Hands-Free Segmentation of Medical Volumes via Binary Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hands-free method to interactively segment 3D medical\nvolumes. In our scenario, a human user progressively segments an organ by\nanswering a series of questions of the form \"Is this voxel inside the object to\nsegment?\". At each iteration, the chosen question is defined as the one halving\na set of candidate segmentations given the answered questions. For a quick and\nefficient exploration, these segmentations are sampled according to the\nMetropolis-Hastings algorithm. Our sampling technique relies on a combination\nof relaxed shape prior, learnt probability map and consistency with previous\nanswers. We demonstrate the potential of our strategy on a prostate\nsegmentation MRI dataset. Through the study of failure cases with synthetic\nexamples, we demonstrate the adaptation potential of our method. We also show\nthat our method outperforms two intuitive baselines: one based on random\nquestions, the other one being the thresholded probability map.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 14:18:40 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Dubost", "Florian", ""], ["Peter", "Loic", ""], ["Rupprecht", "Christian", ""], ["Gutierrez-Becker", "Benjamin", ""], ["Navab", "Nassir", ""]]}, {"id": "1609.06260", "submitter": "Mohamed Moustafa", "authors": "Mai Tolba, Mohamed Moustafa", "title": "GAdaBoost: Accelerating Adaboost Feature Selection with Genetic\n  Algorithms", "comments": "8th International Conference on Evolutionary Computation Theory and\n  Applications (ECTA 2016). Final paper will appear at the SCITEPRESS Digital\n  Library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosted cascade of simple features, by Viola and Jones, is one of the most\nfamous object detection frameworks. However, it suffers from a lengthy training\nprocess. This is due to the vast features space and the exhaustive search\nnature of Adaboost. In this paper we propose GAdaboost: a Genetic Algorithm to\naccelerate the training procedure through natural feature selection.\nSpecifically, we propose to limit Adaboost search within a subset of the huge\nfeature space, while evolving this subset following a Genetic Algorithm.\nExperiments demonstrate that our proposed GAdaboost is up to 3.7 times faster\nthan Adaboost. We also demonstrate that the price of this speedup is a mere\ndecrease (3%, 4%) in detection accuracy when tested on FDDB benchmark face\ndetection set, and Caltech Web Faces respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 17:31:11 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Tolba", "Mai", ""], ["Moustafa", "Mohamed", ""]]}, {"id": "1609.06323", "submitter": "Benjamin Hughes", "authors": "Benjamin Hughes, Tilo Burghardt", "title": "Automated Visual Fin Identification of Individual Great White Sharks", "comments": "17 pages, 16 figures. To be published in IJCV. Article replaced to\n  update first author contact details and to correct a Figure reference on page\n  6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the automated visual identification of individual great\nwhite sharks from dorsal fin imagery. We propose a computer vision photo ID\nsystem and report recognition results over a database of thousands of\nunconstrained fin images. To the best of our knowledge this line of work\nestablishes the first fully automated contour-based visual ID system in the\nfield of animal biometrics. The approach put forward appreciates shark fins as\ntextureless, flexible and partially occluded objects with an individually\ncharacteristic shape. In order to recover animal identities from an image we\nfirst introduce an open contour stroke model, which extends multi-scale region\nsegmentation to achieve robust fin detection. Secondly, we show that\ncombinatorial, scale-space selective fingerprinting can successfully encode fin\nindividuality. We then measure the species-specific distribution of visual\nindividuality along the fin contour via an embedding into a global `fin space'.\nExploiting this domain, we finally propose a non-linear model for individual\nanimal recognition and combine all approaches into a fine-grained\nmulti-instance framework. We provide a system evaluation, compare results to\nprior work, and report performance and properties in detail.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 20:00:12 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 16:32:53 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Hughes", "Benjamin", ""], ["Burghardt", "Tilo", ""]]}, {"id": "1609.06341", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie", "title": "Markov Random Field Model-Based Salt and Pepper Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem of impulse noise reduction is a very well studied problem in image\nprocessing community and many different approaches have been proposed to tackle\nthis problem. In the current work, the problem of fixed value impulse noise\n(salt and pepper) removal from images is investigated by use of a Markov Random\nField (MRF) models with smoothness priors. After the formulation of the problem\nas an inpainting problem, graph cuts with $\\alpha$-expansion moves are\nconsidered for minimization of the energy functional. As for comparisons,\nseveral other minimization techniques that are widely used for MRF models'\noptimization are considered and the results are compared using\nPeak-Signal-to-Noise-Ratio (PSNR) and Structural Similarity Index (SSIM) as\nmetrics. The investigations show the superiority of graph cuts with\n$\\alpha$-expansion moves over the other techniques both in terms of PSNR and\nalso computational times.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 20:24:41 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Baghaie", "Ahmadreza", ""]]}, {"id": "1609.06371", "submitter": "Peter Meer", "authors": "Xiang Yang, Peter Meer", "title": "Robust Estimation of Multiple Inlier Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust estimator presented in this paper processes each structure\nindependently. The scales of the structures are estimated adaptively and no\nthreshold is involved in spite of different objective functions. The user has\nto specify only the number of elemental subsets for random sampling. After\nclassifying all the input data, the segmented structures are sorted by their\nstrengths and the strongest inlier structures come out at the top. Like any\nrobust estimators, this algorithm also has limitations which are described in\ndetail. Several synthetic and real examples are presented to illustrate every\naspect of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 22:06:58 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 13:42:00 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 19:31:26 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Yang", "Xiang", ""], ["Meer", "Peter", ""]]}, {"id": "1609.06377", "submitter": "Reza Mahjourian", "authors": "Reza Mahjourian, Martin Wicke, Anelia Angelova", "title": "Geometry-Based Next Frame Prediction from Monocular Video", "comments": "To appear in 2017 IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of next frame prediction from video input. A\nrecurrent convolutional neural network is trained to predict depth from\nmonocular video input, which, along with the current video image and the camera\ntrajectory, can then be used to compute the next frame. Unlike prior next-frame\nprediction approaches, we take advantage of the scene geometry and use the\npredicted depth for generating the next frame prediction. Our approach can\nproduce rich next frame predictions which include depth information attached to\neach pixel. Another novel aspect of our approach is that it predicts depth from\na sequence of images (e.g. in a video), rather than from a single still image.\nWe evaluate the proposed approach on the KITTI dataset, a standard dataset for\nbenchmarking tasks relevant to autonomous driving. The proposed method produces\nresults which are visually and numerically superior to existing methods that\ndirectly predict the next frame. We show that the accuracy of depth prediction\nimproves as more prior frames are considered.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 22:49:34 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 21:52:06 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Mahjourian", "Reza", ""], ["Wicke", "Martin", ""], ["Angelova", "Anelia", ""]]}, {"id": "1609.06417", "submitter": "Simeng Liu", "authors": "Simeng Liu, Yanfeng Sun, Yongli Hu, Junbin Gao, Baocai Yin", "title": "Matrix Variate RBM Model with Gaussian Distributions", "comments": "We think we need more mathematical derivation and experiments to\n  support the proposed theory of the paper. In this period, it is not\n  appropriate to publish it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is a particular type of random neural\nnetwork models modeling vector data based on the assumption of Bernoulli\ndistribution. For multi-dimensional and non-binary data, it is necessary to\nvectorize and discretize the information in order to apply the conventional\nRBM. It is well-known that vectorization would destroy internal structure of\ndata, and the binary units will limit the applying performance due to fickle\nreal data. To address the issue, this paper proposes a Matrix variate Gaussian\nRestricted Boltzmann Machine (MVGRBM) model for matrix data whose entries\nfollow Gaussian distributions. Compared with some other RBM algorithm, MVGRBM\ncan model real value data better and it has good performance in image\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 03:42:52 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 01:31:06 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Liu", "Simeng", ""], ["Sun", "Yanfeng", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Yin", "Baocai", ""]]}, {"id": "1609.06426", "submitter": "Zhanpeng Zhang", "authors": "Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "From Facial Expression Recognition to Interpersonal Relation Prediction", "comments": "To appear in International Journal of Computer Vision. We release a\n  large expression dataset (over 90,000 web images with manual annotation) and\n  an interpersonal relation dataset. See\n  http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpersonal relation defines the association, e.g., warm, friendliness, and\ndominance, between two or more people. Motivated by psychological studies, we\ninvestigate if such fine-grained and high-level relation traits can be\ncharacterized and quantified from face images in the wild. We address this\nchallenging problem by first studying a deep network architecture for robust\nrecognition of facial expressions. Unlike existing models that typically learn\nfrom facial expression labels alone, we devise an effective multitask network\nthat is capable of learning from rich auxiliary attributes such as gender, age,\nand head pose, beyond just facial expression data. While conventional\nsupervised training requires datasets with complete labels (e.g., all samples\nmust be labeled with gender, age, and expression), we show that this\nrequirement can be relaxed via a novel attribute propagation method. The\napproach further allows us to leverage the inherent correspondences between\nheterogeneous attribute sources despite the disparate distributions of\ndifferent datasets. With the network we demonstrate state-of-the-art results on\nexisting facial expression recognition benchmarks. To predict inter-personal\nrelation, we use the expression recognition network as branches for a Siamese\nmodel. Extensive experiments show that our model is capable of mining mutual\ncontext of faces for accurate fine-grained interpersonal prediction.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 06:14:17 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 07:28:35 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 06:04:13 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zhang", "Zhanpeng", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1609.06434", "submitter": "Haoran Chen", "authors": "Haoran Chen, Yanfeng Sun, Junbin Gao, Yongli Hu, Baocai Yin", "title": "Partial Least Squares Regression on Riemannian Manifolds and Its\n  Application in Classifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial least squares regression (PLSR) has been a popular technique to\nexplore the linear relationship between two datasets. However, most of\nalgorithm implementations of PLSR may only achieve a suboptimal solution\nthrough an optimization on the Euclidean space. In this paper, we propose\nseveral novel PLSR models on Riemannian manifolds and develop optimization\nalgorithms based on Riemannian geometry of manifolds. This algorithm can\ncalculate all the factors of PLSR globally to avoid suboptimal solutions. In a\nnumber of experiments, we have demonstrated the benefits of applying the\nproposed model and algorithm to a variety of learning tasks in pattern\nrecognition and object classification.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 06:48:07 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Chen", "Haoran", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hu", "Yongli", ""], ["Yin", "Baocai", ""]]}, {"id": "1609.06441", "submitter": "Nian Cai", "authors": "Nian Cai, Zhineng Lin, Fu Zhang, Guandong Cen, Han Wang", "title": "Detecting facial landmarks in the video based on a hybrid framework", "comments": "8 pages, 5 figures, unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To dynamically detect the facial landmarks in the video, we propose a novel\nhybrid framework termed as detection-tracking-detection (DTD). First, the face\nbounding box is achieved from the first frame of the video sequence based on a\ntraditional face detection method. Then, a landmark detector detects the facial\nlandmarks, which is based on a cascaded deep convolution neural network (DCNN).\nNext, the face bounding box in the current frame is estimated and validated\nafter the facial landmarks in the previous frame are tracked based on the\nmedian flow. Finally, the facial landmarks in the current frame are exactly\ndetected from the validated face bounding box via the landmark detector.\nExperimental results indicate that the proposed framework can detect the facial\nlandmarks in the video sequence more effectively and with lower consuming time\ncompared to the frame-by-frame method via the DCNN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 07:29:49 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Cai", "Nian", ""], ["Lin", "Zhineng", ""], ["Zhang", "Fu", ""], ["Cen", "Guandong", ""], ["Wang", "Han", ""]]}, {"id": "1609.06456", "submitter": "Yaoyi Li", "authors": "Yaoyi Li, Hongtao Lu", "title": "Multi-View Constraint Propagation with Consensus Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, the pairwise constraint is a kind of weaker supervisory\ninformation which can be collected easily. The constraint propagation has been\nproved to be a success of exploiting such side-information. In recent years,\nsome methods of multi-view constraint propagation have been proposed. However,\nthe problem of reasonably fusing different views remains unaddressed. In this\npaper, we present a method dubbed Consensus Prior Constraint Propagation\n(CPCP), which can provide the prior knowledge of the robustness of each data\ninstance and its neighborhood. With the robustness generated from the consensus\ninformation of each view, we build a unified affinity matrix as a result of the\npropagation. Specifically, we fuse the affinity of different views at a data\ninstance level instead of a view level. This paper also introduces an approach\nto deal with the imbalance between the positive and negative constraints. The\nproposed method has been tested in clustering tasks on two publicly available\nmulti-view data sets to show the superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 08:13:06 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Li", "Yaoyi", ""], ["Lu", "Hongtao", ""]]}, {"id": "1609.06492", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic, Alessia Amelio, Zoran N. Milivojevic, Milena Jevtic", "title": "Document Image Coding and Clustering for Script Discrimination", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": "ICIC Express Letters Vol. 10 n. 7 July 2016 pp. 1561-1566", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new method for discrimination of documents given in\ndifferent scripts. The document is mapped into a uniformly coded text of\nnumerical values. It is derived from the position of the letters in the text\nline, based on their typographical characteristics. Each code is considered as\na gray level. Accordingly, the coded text determines a 1-D image, on which\ntexture analysis by run-length statistics and local binary pattern is\nperformed. It defines feature vectors representing the script content of the\ndocument. A modified clustering approach employed on document feature vector\ngroups documents written in the same script. Experimentation performed on two\ncustom oriented databases of historical documents in old Cyrillic, angular and\nround Glagolitic as well as Antiqua and Fraktur scripts demonstrates the\nsuperiority of the proposed method with respect to well-known methods in the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 10:52:03 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Brodic", "Darko", ""], ["Amelio", "Alessia", ""], ["Milivojevic", "Zoran N.", ""], ["Jevtic", "Milena", ""]]}, {"id": "1609.06500", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Christopher G. R. Wallis, Jennifer Y. H. Chan, and Jason\n  D. McEwen", "title": "Wavelet-Based Segmentation on the Sphere", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2019.107081", "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation, a useful/powerful technique in pattern recognition, is the\nprocess of identifying object outlines within images. There are a number of\nefficient algorithms for segmentation in Euclidean space that depend on the\nvariational approach and partial differential equation modelling. Wavelets have\nbeen used successfully in various problems in image processing, including\nsegmentation, inpainting, noise removal, super-resolution image restoration,\nand many others. Wavelets on the sphere have been developed to solve such\nproblems for data defined on the sphere, which arise in numerous fields such as\ncosmology and geophysics. In this work, we propose a wavelet-based method to\nsegment images on the sphere, accounting for the underlying geometry of\nspherical data. Our method is a direct extension of the tight-frame based\nsegmentation method used to automatically identify tube-like structures such as\nblood vessels in medical imaging. It is compatible with any arbitrary type of\nwavelet frame defined on the sphere, such as axisymmetric wavelets, directional\nwavelets, curvelets, and hybrid wavelet constructions. Such an approach allows\nthe desirable properties of wavelets to be naturally inherited in the\nsegmentation process. In particular, directional wavelets and curvelets, which\nwere designed to efficiently capture directional signal content, provide\nadditional advantages in segmenting images containing prominent directional and\ncurvilinear features. We present several numerical experiments, applying our\nwavelet-based segmentation method, as well as the common K-means method, on\nreal-world spherical images. These experiments demonstrate the superiority of\nour method and show that it is capable of segmenting different kinds of\nspherical images, including those with prominent directional features.\nMoreover, our algorithm is efficient with convergence usually within a few\niterations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 11:20:19 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 18:16:30 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Cai", "Xiaohao", ""], ["Wallis", "Christopher G. R.", ""], ["Chan", "Jennifer Y. H.", ""], ["McEwen", "Jason D.", ""]]}, {"id": "1609.06536", "submitter": "Samuli Laine", "authors": "Samuli Laine, Tero Karras, Timo Aila, Antti Herva, Shunsuke Saito,\n  Ronald Yu, Hao Li, Jaakko Lehtinen", "title": "Production-Level Facial Performance Capture Using Deep Convolutional\n  Neural Networks", "comments": "Final SCA 2017 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time deep learning framework for video-based facial\nperformance capture -- the dense 3D tracking of an actor's face given a\nmonocular video. Our pipeline begins with accurately capturing a subject using\na high-end production facial capture pipeline based on multi-view stereo\ntracking and artist-enhanced animations. With 5-10 minutes of captured footage,\nwe train a convolutional neural network to produce high-quality output,\nincluding self-occluded regions, from a monocular video sequence of that\nsubject. Since this 3D facial performance capture is fully automated, our\nsystem can drastically reduce the amount of labor involved in the development\nof modern narrative-driven video games or films involving realistic digital\ndoubles of actors and potentially hours of animated dialogue per character. We\ncompare our results with several state-of-the-art monocular real-time facial\ncapture techniques and demonstrate compelling animation inference in\nchallenging areas such as eyes and lips.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:55:59 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 13:54:51 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Laine", "Samuli", ""], ["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Herva", "Antti", ""], ["Saito", "Shunsuke", ""], ["Yu", "Ronald", ""], ["Li", "Hao", ""], ["Lehtinen", "Jaakko", ""]]}, {"id": "1609.06583", "submitter": "Marco Fiorucci", "authors": "Marcello Pelillo, Ismail Elezi, Marco Fiorucci", "title": "Revealing Structure in Large Graphs: Szemer\\'edi's Regularity Lemma and\n  its Use in Pattern Recognition", "comments": "Accepted for publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduced in the mid-1970's as an intermediate step in proving a\nlong-standing conjecture on arithmetic progressions, Szemer\\'edi's regularity\nlemma has emerged over time as a fundamental tool in different branches of\ngraph theory, combinatorics and theoretical computer science. Roughly, it\nstates that every graph can be approximated by the union of a small number of\nrandom-like bipartite graphs called regular pairs. In other words, the result\nprovides us a way to obtain a good description of a large graph using a small\namount of data, and can be regarded as a manifestation of the all-pervading\ndichotomy between structure and randomness. In this paper we will provide an\noverview of the regularity lemma and its algorithmic aspects, and will discuss\nits relevance in the context of pattern recognition research.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:31:26 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Pelillo", "Marcello", ""], ["Elezi", "Ismail", ""], ["Fiorucci", "Marco", ""]]}, {"id": "1609.06585", "submitter": "Yunjin Chen", "authors": "Wensen Feng, Peng Qiao, Xuanyang Xi, and Yunjin Chen", "title": "Image Denoising via Multi-scale Nonlinear Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is a fundamental operation in image processing and holds\nconsiderable practical importance for various real-world applications. Arguably\nseveral thousands of papers are dedicated to image denoising. In the past\ndecade, sate-of-the-art denoising algorithm have been clearly dominated by\nnon-local patch-based methods, which explicitly exploit patch self-similarity\nwithin image. However, in recent two years, discriminatively trained local\napproaches have started to outperform previous non-local models and have been\nattracting increasing attentions due to the additional advantage of\ncomputational efficiency. Successful approaches include cascade of shrinkage\nfields (CSF) and trainable nonlinear reaction diffusion (TNRD). These two\nmethods are built on filter response of linear filters of small size using feed\nforward architectures. Due to the locality inherent in local approaches, the\nCSF and TNRD model become less effective when noise level is high and\nconsequently introduces some noise artifacts. In order to overcome this\nproblem, in this paper we introduce a multi-scale strategy. To be specific, we\nbuild on our newly-developed TNRD model, adopting the multi-scale pyramid image\nrepresentation to devise a multi-scale nonlinear diffusion process. As\nexpected, all the parameters in the proposed multi-scale diffusion model,\nincluding the filters and the influence functions across scales, are learned\nfrom training data through a loss based approach. Numerical results on Gaussian\nand Poisson denoising substantiate that the exploited multi-scale strategy can\nsuccessfully boost the performance of the original TNRD model with single\nscale. As a consequence, the resulting multi-scale diffusion models can\nsignificantly suppress the typical incorrect features for those noisy images\nwith heavy noise.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:42:47 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Feng", "Wensen", ""], ["Qiao", "Peng", ""], ["Xi", "Xuanyang", ""], ["Chen", "Yunjin", ""]]}, {"id": "1609.06591", "submitter": "Hui Ding", "authors": "Hui Ding, Shaohua Kevin Zhou and Rama Chellappa", "title": "FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relatively small data sets available for expression recognition research make\nthe training of deep networks for expression recognition very challenging.\nAlthough fine-tuning can partially alleviate the issue, the performance is\nstill below acceptable levels as the deep features probably contain redun- dant\ninformation from the pre-trained domain. In this paper, we present\nFaceNet2ExpNet, a novel idea to train an expression recognition network based\non static images. We first propose a new distribution function to model the\nhigh-level neurons of the expression network. Based on this, a two-stage\ntraining algorithm is carefully designed. In the pre-training stage, we train\nthe convolutional layers of the expression net, regularized by the face net; In\nthe refining stage, we append fully- connected layers to the pre-trained\nconvolutional layers and train the whole network jointly. Visualization shows\nthat the model trained with our method captures improved high-level expression\nsemantics. Evaluations on four public expression databases, CK+, Oulu-CASIA,\nTFD, and SFEW demonstrate that our method achieves better results than\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 15:04:31 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 00:59:45 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Ding", "Hui", ""], ["Zhou", "Shaohua Kevin", ""], ["Chellappa", "Rama", ""]]}, {"id": "1609.06604", "submitter": "Filippo Arcadu", "authors": "Filippo Arcadu, Jakob Vogel, Marco Stampanoni and Federica Marone", "title": "Improving analytical tomographic reconstructions through consistency\n  conditions", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces and characterizes a fast parameterless filter based on\nthe Helgason-Ludwig consistency conditions, used to improve the accuracy of\nanalytical reconstructions of tomographic undersampled datasets. The filter,\nacting in the Radon domain, extrapolates intermediate projections between those\nexisting. The resulting sinogram, doubled in views, is then reconstructed by a\nstandard analytical method. Experiments with simulated data prove that the\npeak-signal-to-noise ratio of the results computed by filtered backprojection\nis improved up to 5-6 dB, if the filter is used prior to reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 15:34:39 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Arcadu", "Filippo", ""], ["Vogel", "Jakob", ""], ["Stampanoni", "Marco", ""], ["Marone", "Federica", ""]]}, {"id": "1609.06647", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan", "title": "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning\n  Challenge", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.4555", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence (\n  Volume: PP, Issue: 99 , July 2016 )", "doi": "10.1109/TPAMI.2016.2587640", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. Finally, given the recent\nsurge of interest in this task, a competition was organized in 2015 using the\nnewly released COCO dataset. We describe and analyze the various improvements\nwe applied to our own baseline and show the resulting performance in the\ncompetition, which we won ex-aequo with a team from Microsoft Research, and\nprovide an open source implementation in TensorFlow.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 17:40:57 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Vinyals", "Oriol", ""], ["Toshev", "Alexander", ""], ["Bengio", "Samy", ""], ["Erhan", "Dumitru", ""]]}, {"id": "1609.06653", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Land Use Classification using Convolutional Neural Networks Applied to\n  Ground-Level Images", "comments": "ACM SIGSPATIAL 2015, Best Poster Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land use mapping is a fundamental yet challenging task in geographic science.\nIn contrast to land cover mapping, it is generally not possible using overhead\nimagery. The recent, explosive growth of online geo-referenced photo\ncollections suggests an alternate approach to geographic knowledge discovery.\nIn this work, we present a general framework that uses ground-level images from\nFlickr for land use mapping. Our approach benefits from several novel aspects.\nFirst, we address the nosiness of the online photo collections, such as\nimprecise geolocation and uneven spatial distribution, by performing location\nand indoor/outdoor filtering, and semi- supervised dataset augmentation. Our\nindoor/outdoor classifier achieves state-of-the-art performance on several\nbench- mark datasets and approaches human-level accuracy. Second, we utilize\nhigh-level semantic image features extracted using deep learning, specifically\nconvolutional neural net- works, which allow us to achieve upwards of 76%\naccuracy on a challenging eight class land use mapping problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:01:24 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1609.06657", "submitter": "Andrew Shin", "authors": "Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada", "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question\n  Answering (FSVQA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) task has showcased a new stage of interaction\nbetween language and vision, two of the most pivotal components of artificial\nintelligence. However, it has mostly focused on generating short and repetitive\nanswers, mostly single words, which fall short of rich linguistic capabilities\nof humans. We introduce Full-Sentence Visual Question Answering (FSVQA)\ndataset, consisting of nearly 1 million pairs of questions and full-sentence\nanswers for images, built by applying a number of rule-based natural language\nprocessing techniques to original VQA dataset and captions in the MS COCO\ndataset. This poses many additional complexities to conventional VQA task, and\nwe provide a baseline for approaching and evaluating the task, on top of which\nwe invite the research community to build further improvements.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:12:04 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Shin", "Andrew", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1609.06666", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Dushyant Rao, Dominic Zeng Wang, Chi Hay Tong, Ingmar\n  Posner", "title": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient\n  Convolutional Neural Networks", "comments": "To be published at the IEEE International Conference on Robotics and\n  Automation 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computationally efficient approach to detecting objects\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\nparticular, this is achieved by leveraging a feature-centric voting scheme to\nimplement novel convolutional layers which explicitly exploit the sparsity\nencountered in the input. To this end, we examine the trade-off between\naccuracy and speed for different architectures and additionally propose to use\nan L1 penalty on the filter activations to further encourage sparsity in the\nintermediate representations. To the best of our knowledge, this is the first\nwork to propose sparse convolutional layers and L1 regularisation for efficient\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\non the KITTI object detection benchmark and show that Vote3Deep models with as\nfew as three layers outperform the previous state of the art in both laser and\nlaser-vision based approaches by margins of up to 40% while remaining highly\ncompetitive in terms of processing time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:32:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 15:29:45 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Engelcke", "Martin", ""], ["Rao", "Dushyant", ""], ["Wang", "Dominic Zeng", ""], ["Tong", "Chi Hay", ""], ["Posner", "Ingmar", ""]]}, {"id": "1609.06668", "submitter": "Ziyue Xu", "authors": "Mario Buty, Ziyue Xu, Mingchen Gao, Ulas Bagci, Aaron Wu, and Daniel\n  J. Mollura", "title": "Characterization of Lung Nodule Malignancy using Hybrid Shape and\n  Appearance Features", "comments": "Accepted to MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography imaging is a standard modality for detecting and\nassessing lung cancer. In order to evaluate the malignancy of lung nodules,\nclinical practice often involves expert qualitative ratings on several criteria\ndescribing a nodule's appearance and shape. Translating these features for\ncomputer-aided diagnostics is challenging due to their subjective nature and\nthe difficulties in gaining a complete description. In this paper, we propose a\ncomputerized approach to quantitatively evaluate both appearance distinctions\nand 3D surface variations. Nodule shape was modeled and parameterized using\nspherical harmonics, and appearance features were extracted using deep\nconvolutional neural networks. Both sets of features were combined to estimate\nthe nodule malignancy using a random forest classifier. The proposed algorithm\nwas tested on the publicly available Lung Image Database Consortium dataset,\nachieving high accuracy. By providing lung nodule characterization, this method\ncan provide a robust alternative reference opinion for lung cancer diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:33:56 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Buty", "Mario", ""], ["Xu", "Ziyue", ""], ["Gao", "Mingchen", ""], ["Bagci", "Ulas", ""], ["Wu", "Aaron", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1609.06669", "submitter": "Manuel Rodriguez-Vallejo", "authors": "Manuel Rodriguez-Vallejo, Clara Llorens-Quintana, Diego Montagud,\n  Walter D. Furlan and Juan A. Monsoriu", "title": "Fast and reliable stereopsis measurement at multiple distances with iPad", "comments": "14 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To present a new fast and reliable application for iPad (ST) for\nscreening stereopsis at multiple distances.\n  Methods: A new iPad application (app) based on a random dot stereogram was\ndesigned for screening stereopsis at multiple distances. Sixty-five subjects\nwith no ocular diseases and wearing their habitual correction were tested at\ntwo different distances: 3 m and at 0.4 m. Results were compared with other\ncommercial tests: TNO (at near) and Howard Dolman (at distance) Subjects were\ncited one week later in order to repeat the same procedures for assessing\nreproducibility of the tests.\n  Results: Stereopsis at near was better with ST (40 arcsec) than with TNO (60\narcsec), but not significantly (p = 0.36). The agreement was good (k = 0.604)\nand the reproducibility was better with ST (k = 0.801) than with TNO (k =\n0.715), in fact median difference between days was significant only with TNO (p\n= 0.02). On the other hand, poor agreement was obtained between HD and ST at\nfar distance (k=0.04), obtaining significant differences in medians (p = 0.001)\nand poorer reliability with HD (k = 0.374) than with ST (k = 0.502).\n  Conclusions: Screening stereopsis at near with a new iPad app demonstrated to\nbe a fast and realiable. Results were in a good agreement with conventional\ntests as TNO, but it could not be compared at far vision with HD due to the\nlimited resolution of the iPad.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:34:25 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Rodriguez-Vallejo", "Manuel", ""], ["Llorens-Quintana", "Clara", ""], ["Montagud", "Diego", ""], ["Furlan", "Walter D.", ""], ["Monsoriu", "Juan A.", ""]]}, {"id": "1609.06694", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, Deva Ramanan", "title": "PixelNet: Towards a General Pixel-level Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore architectures for general pixel-level prediction problems, from\nlow-level edge detection to mid-level surface normal estimation to high-level\nsemantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that (1) stratified sampling allows us to add diversity during\nbatch updates and (2) sampled multi-scale features allow us to explore more\nnonlinear predictors (multiple fully-connected layers followed by ReLU) that\nimprove overall accuracy. Finally, our objective is to show how a architecture\ncan get performance better than (or comparable to) the architectures designed\nfor a particular task. Interestingly, our single architecture produces\nstate-of-the-art results for semantic segmentation on PASCAL-Context, surface\nnormal estimation on NYUDv2 dataset, and edge detection on BSDS without\ncontextual post-processing.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 19:32:46 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Bansal", "Aayush", ""], ["Chen", "Xinlei", ""], ["Russell", "Bryan", ""], ["Gupta", "Abhinav", ""], ["Ramanan", "Deva", ""]]}, {"id": "1609.06753", "submitter": "Alexandre Sablayrolles", "authors": "Alexandre Sablayrolles, Matthijs Douze, Herv\\'e J\\'egou, Nicolas\n  Usunier", "title": "How should we evaluate supervised hashing?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing produces compact representations for documents, to perform tasks like\nclassification or retrieval based on these short codes. When hashing is\nsupervised, the codes are trained using labels on the training data. This paper\nfirst shows that the evaluation protocols used in the literature for supervised\nhashing are not satisfactory: we show that a trivial solution that encodes the\noutput of a classifier significantly outperforms existing supervised or\nsemi-supervised methods, while using much shorter codes. We then propose two\nalternative protocols for supervised hashing: one based on retrieval on a\ndisjoint set of classes, and another based on transfer learning to new classes.\nWe provide two baseline methods for image-related tasks to assess the\nperformance of (semi-)supervised hashing: without coding and with unsupervised\ncodes. These baselines give a lower- and upper-bound on the performance of a\nsupervised hashing scheme.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 21:03:36 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 16:40:29 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 17:00:50 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Sablayrolles", "Alexandre", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1609.06772", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Spatio-Temporal Sentiment Hotspot Detection Using Geotagged Photos", "comments": "To appear in ACM SIGSPATIAL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform spatio-temporal analysis of public sentiment using geotagged photo\ncollections. We develop a deep learning-based classifier that predicts the\nemotion conveyed by an image. This allows us to associate sentiment with place.\nWe perform spatial hotspot detection and show that different emotions have\ndistinct spatial distributions that match expectations. We also perform\ntemporal analysis using the capture time of the photos. Our spatio-temporal\nhotspot detection correctly identifies emerging concentrations of specific\nemotions and year-by-year analyses of select locations show there are strong\ntemporal correlations between the predicted emotions and known events.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 22:43:54 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1609.06782", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Ting Yao, Yanwei Fu, Yu-Gang Jiang", "title": "Deep Learning for Video Classification and Captioning", "comments": "Book chapter in Frontiers of Multimedia Research", "journal-ref": null, "doi": "10.1145/3122865.3122867", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated by the tremendous increase in Internet bandwidth and storage\nspace, video data has been generated, published and spread explosively,\nbecoming an indispensable part of today's big data. In this paper, we focus on\nreviewing two lines of research aiming to stimulate the comprehension of videos\nwith deep learning: video classification and video captioning. While video\nclassification concentrates on automatically labeling video clips based on\ntheir semantic contents like human actions or complex events, video captioning\nattempts to generate a complete and natural sentence, enriching the single\nlabel as in video classification, to capture the most informative dynamics in\nvideos. In addition, we also provide a review of popular benchmarks and\ncompetitions, which are critical for evaluating the technical progress of this\nvibrant field.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 00:08:59 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 14:59:32 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wu", "Zuxuan", ""], ["Yao", "Ting", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1609.06838", "submitter": "Jia Pan", "authors": "Pinxin Long and Wenxi Liu and Jia Pan", "title": "Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent\n  Navigation", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters 2(2): 656-663 (2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-speed, low-latency obstacle avoidance that is insensitive to sensor\nnoise is essential for enabling multiple decentralized robots to function\nreliably in cluttered and dynamic environments. While other distributed\nmulti-agent collision avoidance systems exist, these systems require online\ngeometric optimization where tedious parameter tuning and perfect sensing are\nnecessary.\n  We present a novel end-to-end framework to generate reactive collision\navoidance policy for efficient distributed multi-agent navigation. Our method\nformulates an agent's navigation strategy as a deep neural network mapping from\nthe observed noisy sensor measurements to the agent's steering commands in\nterms of movement velocity. We train the network on a large number of frames of\ncollision avoidance data collected by repeatedly running a multi-agent\nsimulator with different parameter settings. We validate the learned deep\nneural network policy in a set of simulated and real scenarios with noisy\nmeasurements and demonstrate that our method is able to generate a robust\nnavigation strategy that is insensitive to imperfect sensing and works reliably\nin all situations. We also show that our method can be well generalized to\nscenarios that do not appear in our training data, including scenes with static\nobstacles and agents with different sizes. Videos are available at\nhttps://sites.google.com/view/deepmaca.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:05:56 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 07:41:45 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Long", "Pinxin", ""], ["Liu", "Wenxi", ""], ["Pan", "Jia", ""]]}, {"id": "1609.06845", "submitter": "Sebastien Lefevre", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "On the usability of deep networks for object-based image analysis", "comments": "in International Conference on Geographic Object-Based Image Analysis\n  (GEOBIA), Sep 2016, Enschede, Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computer vision before, remote sensing has been radically changed by the\nintroduction of Convolution Neural Networks. Land cover use, object detection\nand scene understanding in aerial images rely more and more on deep learning to\nachieve new state-of-the-art results. Recent architectures such as Fully\nConvolutional Networks (Long et al., 2015) can even produce pixel level\nannotations for semantic mapping. In this work, we show how to use such deep\nnetworks to detect, segment and classify different varieties of wheeled\nvehicles in aerial images from the ISPRS Potsdam dataset. This allows us to\ntackle object detection and classification on a complex dataset made up of\nvisually similar classes, and to demonstrate the relevance of such a subclass\nmodeling approach. Especially, we want to show that deep learning is also\nsuitable for object-oriented analysis of Earth Observation data. First, we\ntrain a FCN variant on the ISPRS Potsdam dataset and show how the learnt\nsemantic maps can be used to extract precise segmentation of vehicles, which\nallow us studying the repartition of vehicles in the city. Second, we train a\nCNN to perform vehicle classification on the VEDAI (Razakarivony and Jurie,\n2016) dataset, and transfer its knowledge to classify candidate segmented\nvehicles on the Potsdam dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:39:37 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1609.06846", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Semantic Segmentation of Earth Observation Data Using Multimodal and\n  Multi-scale Deep Networks", "comments": "Asian Conference on Computer Vision (ACCV16), Nov 2016, Taipei,\n  Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the use of deep fully convolutional neural networks\n(DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially,\nwe train a variant of the SegNet architecture on remote sensing data over an\nurban area and study different strategies for performing accurate semantic\nsegmentation. Our contributions are the following: 1) we transfer efficiently a\nDFCNN from generic everyday images to remote sensing images; 2) we introduce a\nmulti-kernel convolutional layer for fast aggregation of predictions at\nmultiple scales; 3) we perform data fusion from heterogeneous sensors (optical\nand laser) using residual correction. Our framework improves state-of-the-art\naccuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:42:06 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1609.06861", "submitter": "Sebastien Lefevre", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "How Useful is Region-based Classification of Remote Sensing Images in a\n  Deep Learning Framework?", "comments": "IEEE International Geosciences and Remote Sensing Symposium (IGARSS),\n  Jul 2016, Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the impact of segmentation algorithms as a\npreprocessing step for classification of remote sensing images in a deep\nlearning framework. Especially, we address the issue of segmenting the image\ninto regions to be classified using pre-trained deep neural networks as feature\nextractors for an SVM-based classifier. An efficient segmentation as a\npreprocessing step helps learning by adding a spatially-coherent structure to\nthe data. Therefore, we compare algorithms producing superpixels with more\ntraditional remote sensing segmentation algorithms and measure the variation in\nterms of classification accuracy. We establish that superpixel algorithms allow\nfor a better classification accuracy as a homogenous and compact segmentation\nfavors better generalization of the training samples.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 08:21:20 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1609.06870", "submitter": "Janis Keuper", "authors": "Janis Keuper, Franz-Josef Pfreundt", "title": "Distributed Training of Deep Neural Networks: Theoretical and Practical\n  Limits of Parallel Scalability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a theoretical analysis and practical evaluation of the\nmain bottlenecks towards a scalable distributed solution for the training of\nDeep Neuronal Networks (DNNs). The presented results show, that the current\nstate of the art approach, using data-parallelized Stochastic Gradient Descent\n(SGD), is quickly turning into a vastly communication bound problem. In\naddition, we present simple but fixed theoretic constraints, preventing\neffective scaling of DNN training beyond only a few dozen nodes. This leads to\npoor scalability of DNN training in most practical scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 08:47:58 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 14:22:03 GMT"}, {"version": "v3", "created": "Fri, 11 Nov 2016 12:57:48 GMT"}, {"version": "v4", "created": "Mon, 5 Dec 2016 08:19:11 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Keuper", "Janis", ""], ["Pfreundt", "Franz-Josef", ""]]}, {"id": "1609.06896", "submitter": "Dominik Alexander Klein", "authors": "Dominik Alexander Klein, Dirk Schulz, Armin Bernd Cremers", "title": "Realtime Hierarchical Clustering based on Boundary and Surface\n  Statistics", "comments": "Asian Conf. on Computer Vision (ACCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual grouping is a key mechanism in human scene perception. There, it\nbelongs to the subconscious, early processing and is key prerequisite for other\nhigh level tasks such as recognition. In this paper, we introduce an efficient,\nrealtime capable algorithm which likewise agglomerates a valuable hierarchical\nclustering of a scene, while using purely local appearance statistics. To speed\nup the processing, first we subdivide the image into meaningful, atomic\nsegments using a fast Watershed transform. Starting from there, our rapid,\nagglomerative clustering algorithm prunes and maintains the connectivity graph\nbetween clusters to contain only such pairs, which directly touch in the image\ndomain and are reciprocal nearest neighbors (RNN) wrt. a distance metric. The\ncore of this approach is our novel cluster distance: it combines boundary and\nsurface statistics both in terms of appearance as well as spatial linkage. This\nyields state-of-the-art performance, as we demonstrate in conclusive\nexperiments conducted on BSDS500 and Pascal-Context datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 10:17:30 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Klein", "Dominik Alexander", ""], ["Schulz", "Dirk", ""], ["Cremers", "Armin Bernd", ""]]}, {"id": "1609.06927", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David M. W. Powers", "title": "A quantitative analysis of tilt in the Caf\\'e Wall illusion: a\n  bioplausible model for foveal and peripheral vision", "comments": "8 pages, 9 figures, DICTA2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The biological characteristics of human visual processing can be investigated\nthrough the study of optical illusions and their perception, giving rise to\nintuitions that may improve computer vision to match human performance.\nGeometric illusions are a specific subfamily in which orientations and angles\nare misperceived. This paper reports quantifiable predictions of the degree of\ntilt for a typical geometric illusion called Caf\\'e Wall, in which the mortar\nbetween the tiles seems to tilt or bow. Our study employs a common bioplausible\nmodel of retinal processing and we further develop an analytic processing\npipeline to quantify and thus predict the specific angle of tilt. We further\nstudy the effect of resolution and feature size in order to predict the\ndifferent perceived tilts in different areas of the fovea and periphery, where\nresolution varies as the eye saccades to different parts of the image. In the\nexperiments, several different minimal portions of the pattern, modeling\nmonocular and binocular foveal views, are investigated across multiple scales,\nin order to quantify tilts with confidence intervals and explore the difference\nbetween local and global tilt.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 11:55:14 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David M. W.", ""]]}, {"id": "1609.06936", "submitter": "Michal Balazia", "authors": "Michal Balazia and Petr Sojka", "title": "Walker-Independent Features for Gait Recognition from Motion Capture\n  Data", "comments": "Preprint. Full paper published at the Joint IAPR International\n  Workshops on Structural and Syntactic Pattern Recognition and Statistical\n  Techniques in Pattern Recognition (S+SSPR), Merida, Mexico. pp. 310--321,\n  Springer LNCS volume 10029, Nov 2016. DOI: 10.1007/978-3-319-49055-7_28.\n  arXiv admin note: substantial text overlap with arXiv:1609.04392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MoCap-based human identification, as a pattern recognition discipline, can be\noptimized using a machine learning approach. Yet in some applications such as\nvideo surveillance new identities can appear on the fly and labeled data for\nall encountered people may not always be available. This work introduces the\nconcept of learning walker-independent gait features directly from raw joint\ncoordinates by a modification of the Fisher Linear Discriminant Analysis with\nMaximum Margin Criterion. Our new approach shows not only that these features\ncan discriminate different people than who they are learned on, but also that\nthe number of learning identities can be much smaller than the number of\nwalkers encountered in the real operation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 12:17:34 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 11:53:03 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 00:15:20 GMT"}, {"version": "v4", "created": "Thu, 24 Aug 2017 11:35:09 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Balazia", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "1609.06988", "submitter": "Yuan Gao", "authors": "Yuan Gao, Alan Yuille", "title": "Symmetric Non-Rigid Structure from Motion for Category-Specific Object\n  Structure Estimation", "comments": "Accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objects, especially these made by humans, are symmetric, e.g. cars and\naeroplanes. This paper addresses the estimation of 3D structures of symmetric\nobjects from multiple images of the same object category, e.g. different cars,\nseen from various viewpoints. We assume that the deformation between different\ninstances from the same object category is non-rigid and symmetric. In this\npaper, we extend two leading non-rigid structure from motion (SfM) algorithms\nto exploit symmetry constraints. We model the both methods as energy\nminimization, in which we also recover the missing observations caused by\nocclusions. In particularly, we show that by rotating the coordinate system,\nthe energy can be decoupled into two independent terms, which still exploit\nsymmetry, to apply matrix factorization separately on each of them for\ninitialization. The results on the Pascal3D+ dataset show that our methods\nsignificantly improve performance over baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 13:57:10 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Gao", "Yuan", ""], ["Yuille", "Alan", ""]]}, {"id": "1609.07009", "submitter": "Wenzhe Shi", "authors": "Wenzhe Shi, Jose Caballero, Lucas Theis, Ferenc Huszar, Andrew Aitken,\n  Christian Ledig, Zehan Wang", "title": "Is the deconvolution layer the same as a convolutional layer?", "comments": "This is a note to share some additional insights for our the CVPR\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we want to focus on aspects related to two questions most\npeople asked us at CVPR about the network we presented. Firstly, What is the\nrelationship between our proposed layer and the deconvolution layer? And\nsecondly, why are convolutions in low-resolution (LR) space a better choice?\nThese are key questions we tried to answer in the paper, but we were not able\nto go into as much depth and clarity as we would have liked in the space\nallowance. To better answer these questions in this note, we first discuss the\nrelationships between the deconvolution layer in the forms of the transposed\nconvolution layer, the sub-pixel convolutional layer and our efficient\nsub-pixel convolutional layer. We will refer to our efficient sub-pixel\nconvolutional layer as a convolutional layer in LR space to distinguish it from\nthe common sub-pixel convolutional layer. We will then show that for a fixed\ncomputational budget and complexity, a network with convolutions exclusively in\nLR space has more representation power at the same speed than a network that\nfirst upsamples the input in high resolution space.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:11:11 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Shi", "Wenzhe", ""], ["Caballero", "Jose", ""], ["Theis", "Lucas", ""], ["Huszar", "Ferenc", ""], ["Aitken", "Andrew", ""], ["Ledig", "Christian", ""], ["Wang", "Zehan", ""]]}, {"id": "1609.07028", "submitter": "Ruobing Xie", "authors": "Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun", "title": "Image-embodied Knowledge Representation Learning", "comments": "7 pages; Accepted by IJCAI-2017", "journal-ref": "IJCAI-2017", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity images could provide significant visual information for knowledge\nrepresentation learning. Most conventional methods learn knowledge\nrepresentations merely from structured triples, ignoring rich visual\ninformation extracted from entity images. In this paper, we propose a novel\nImage-embodied Knowledge Representation Learning model (IKRL), where knowledge\nrepresentations are learned with both triple facts and images. More\nspecifically, we first construct representations for all images of an entity\nwith a neural image encoder. These image representations are then integrated\ninto an aggregated image-based representation via an attention-based method. We\nevaluate our IKRL models on knowledge graph completion and triple\nclassification. Experimental results demonstrate that our models outperform all\nbaselines on both tasks, which indicates the significance of visual information\nfor knowledge representations and the capability of our models in learning\nknowledge representations with images.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:37:45 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 08:14:27 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Xie", "Ruobing", ""], ["Liu", "Zhiyuan", ""], ["Luan", "Huanbo", ""], ["Sun", "Maosong", ""]]}, {"id": "1609.07042", "submitter": "Xiang Xiang", "authors": "Xiang Xiang and Trac D. Tran", "title": "Pose-Selective Max Pooling for Measuring Similarity", "comments": "The tutorial and program associated with this paper are available at\n  https://github.com/eglxiang/ytf yet for non-commercial use", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with two challenges for measuring the similarity of\nthe subject identities in practical video-based face recognition - the\nvariation of the head pose in uncontrolled environments and the computational\nexpense of processing videos. Since the frame-wise feature mean is unable to\ncharacterize the pose diversity among frames, we define and preserve the\noverall pose diversity and closeness in a video. Then, identity will be the\nonly source of variation across videos since the pose varies even within a\nsingle video. Instead of simply using all the frames, we select those faces\nwhose pose point is closest to the centroid of the K-means cluster containing\nthat pose point. Then, we represent a video as a bag of frame-wise deep face\nfeatures while the number of features has been reduced from hundreds to K.\nSince the video representation can well represent the identity, now we measure\nthe subject similarity between two videos as the max correlation among all\npossible pairs in the two bags of features. On the official 5,000 video-pairs\nof the YouTube Face dataset for face verification, our algorithm achieves a\ncomparable performance with VGG-face that averages over deep features of all\nframes. Other vision tasks can also benefit from the generic idea of employing\ngeometric cues to improve the descriptiveness of deep features.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:59:38 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 18:21:05 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 04:05:29 GMT"}, {"version": "v4", "created": "Mon, 14 Nov 2016 04:10:09 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1609.07049", "submitter": "Matan Sela", "authors": "Matan Sela, Nadav Toledo, Yaron Honen, Ron Kimmel", "title": "Customized Facial Constant Positive Air Pressure (CPAP) Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep apnea is a syndrome that is characterized by sudden breathing halts\nwhile sleeping. One of the common treatments involves wearing a mask that\ndelivers continuous air flow into the nostrils so as to maintain a steady air\npressure. These masks are designed for an average facial model and are often\ndifficult to adjust due to poor fit to the actual patient. The incompatibility\nis characterized by gaps between the mask and the face, which deteriorates the\nimpermeability of the mask and leads to air leakage. We suggest a fully\nautomatic approach for designing a personalized nasal mask interface using a\nfacial depth scan. The interfaces generated by the proposed method accurately\nfit the geometry of the scanned face, and are easy to manufacture. The proposed\nmethod utilizes cheap commodity depth sensors and 3D printing technologies to\nefficiently design and manufacture customized masks for patients suffering from\nsleep apnea.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:11:57 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Sela", "Matan", ""], ["Toledo", "Nadav", ""], ["Honen", "Yaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1609.07082", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Boris Muzellec and Richard Nock", "title": "Large Margin Nearest Neighbor Classification using Curved Mahalanobis\n  Distances", "comments": "21 pages, 8 figures, 5 tables, extend ICIP 2016 paper entitled\n  \"classification With Mixtures of Curved Mahalanobis Metrics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the supervised classification problem of machine learning in\nCayley-Klein projective geometries: We show how to learn a curved Mahalanobis\nmetric distance corresponding to either the hyperbolic geometry or the elliptic\ngeometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on\nour experimental results, and further consider the case of learning a mixed\ncurved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi\ndiagrams are affine, and can be built from an equivalent (clipped) power\ndiagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced\ncenters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:41:03 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 18:34:42 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Nielsen", "Frank", ""], ["Muzellec", "Boris", ""], ["Nock", "Richard", ""]]}, {"id": "1609.07093", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Neural Photo Editing with Introspective Adversarial Networks", "comments": "10 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 18:07:56 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 13:16:21 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 18:46:50 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1609.07160", "submitter": "Yonghua Yin", "authors": "Yonghua Yin and Erol Gelenbe", "title": "Deep Learning in Multi-Layer Architectures of Dense Nuclei", "comments": "10 pages (a small edit to the abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume that, within the dense clusters of neurons that can be found in\nnuclei, cells may interconnect via soma-to-soma interactions, in addition to\nconventional synaptic connections. We illustrate this idea with a multi-layer\narchitecture (MLA) composed of multiple clusters of recurrent sub-networks of\nspiking Random Neural Networks (RNN) with dense soma-to-soma interactions, and\nuse this RNN-MLA architecture for deep learning. The inputs to the clusters are\nfirst normalised by adjusting the external arrival rates of spikes to each\ncluster. Then we apply this architecture to learning from multi-channel\ndatasets. Numerical results based on both images and sensor based data, show\nthe value of this novel architecture for deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 20:55:16 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 11:19:23 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Yin", "Yonghua", ""], ["Gelenbe", "Erol", ""]]}, {"id": "1609.07170", "submitter": "Alexander Wong", "authors": "Prajna Paramita Dash, Akshaya Mishra, and Alexander Wong", "title": "Deep Quality: A Deep No-reference Quality Assessment System", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) continues to garner great interest in the\nresearch community, particularly given the tremendous rise in consumer video\ncapture and streaming. Despite significant research effort in IQA in the past\nfew decades, the area of no-reference image quality assessment remains a great\nchallenge and is largely unsolved. In this paper, we propose a novel\nno-reference image quality assessment system called Deep Quality, which\nleverages the power of deep learning to model the complex relationship between\nvisual content and the perceived quality. Deep Quality consists of a novel\nmulti-scale deep convolutional neural network, trained to learn to assess image\nquality based on training samples consisting of different distortions and\ndegradations such as blur, Gaussian noise, and compression artifacts.\nPreliminary results using the CSIQ benchmark image quality dataset showed that\nDeep Quality was able to achieve strong quality prediction performance (89%\npatch-level and 98% image-level prediction accuracy), being able to achieve\nsimilar performance as full-reference IQA methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 21:26:21 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Dash", "Prajna Paramita", ""], ["Mishra", "Akshaya", ""], ["Wong", "Alexander", ""]]}, {"id": "1609.07228", "submitter": "Deng Cai", "authors": "Cong Fu and Deng Cai", "title": "EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm\n  Based on kNN Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor (ANN) search is a fundamental problem in many\nareas of data mining, machine learning and computer vision. The performance of\ntraditional hierarchical structure (tree) based methods decreases as the\ndimensionality of data grows, while hashing based methods usually lack\nefficiency in practice. Recently, the graph based methods have drawn\nconsiderable attention. The main idea is that \\emph{a neighbor of a neighbor is\nalso likely to be a neighbor}, which we refer as \\emph{NN-expansion}. These\nmethods construct a $k$-nearest neighbor ($k$NN) graph offline. And at online\nsearch stage, these methods find candidate neighbors of a query point in some\nway (\\eg, random selection), and then check the neighbors of these candidate\nneighbors for closer ones iteratively. Despite some promising results, there\nare mainly two problems with these approaches: 1) These approaches tend to\nconverge to local optima. 2) Constructing a $k$NN graph is time consuming. We\nfind that these two problems can be nicely solved when we provide a good\ninitialization for NN-expansion. In this paper, we propose EFANNA, an extremely\nfast approximate nearest neighbor search algorithm based on $k$NN Graph. Efanna\nnicely combines the advantages of hierarchical structure based methods and\nnearest-neighbor-graph based methods. Extensive experiments have shown that\nEFANNA outperforms the state-of-art algorithms both on approximate nearest\nneighbor search and approximate nearest neighbor graph construction. To the\nbest of our knowledge, EFANNA is the fastest algorithm so far both on\napproximate nearest neighbor graph construction and approximate nearest\nneighbor search. A library EFANNA based on this research is released on Github.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 04:34:54 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 08:08:58 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 06:31:31 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Fu", "Cong", ""], ["Cai", "Deng", ""]]}, {"id": "1609.07304", "submitter": "Shuzhe Wu", "authors": "Shuzhe Wu, Meina Kan, Zhenliang He, Shiguang Shan, Xilin Chen", "title": "Funnel-Structured Cascade for Multi-View Face Detection with\n  Alignment-Awareness", "comments": "Submitted to Neurocomputing (under review). An adapted open source\n  implementation can be found at\n  https://github.com/seetaface/SeetaFaceEngine/tree/master/FaceDetection", "journal-ref": null, "doi": "10.1016/j.neucom.2016.09.072", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view face detection in open environment is a challenging task due to\ndiverse variations of face appearances and shapes. Most multi-view face\ndetectors depend on multiple models and organize them in parallel, pyramid or\ntree structure, which compromise between the accuracy and time-cost. Aiming at\na more favorable multi-view face detector, we propose a novel funnel-structured\ncascade (FuSt) detection framework. In a coarse-to-fine flavor, our FuSt\nconsists of, from top to bottom, 1) multiple view-specific fast LAB cascade for\nextremely quick face proposal, 2) multiple coarse MLP cascade for further\ncandidate window verification, and 3) a unified fine MLP cascade with\nshape-indexed features for accurate face detection. Compared with other\nstructures, on the one hand, the proposed one uses multiple computationally\nefficient distributed classifiers to propose a small number of candidate\nwindows but with a high recall of multi-view faces. On the other hand, by using\na unified MLP cascade to examine proposals of all views in a centralized style,\nit provides a favorable solution for multi-view face detection with high\naccuracy and low time-cost. Besides, the FuSt detector is alignment-aware and\nperforms a coarse facial part prediction which is beneficial for subsequent\nface alignment. Extensive experiments on two challenging datasets, FDDB and\nAFW, demonstrate the effectiveness of our FuSt detector in both accuracy and\nspeed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 10:43:02 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wu", "Shuzhe", ""], ["Kan", "Meina", ""], ["He", "Zhenliang", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1609.07306", "submitter": "Helge Rhodin", "authors": "Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov,\n  Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, Christian Theobalt", "title": "EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras", "comments": "SIGGRAPH Asia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marker-based and marker-less optical skeletal motion-capture methods use an\noutside-in arrangement of cameras placed around a scene, with viewpoints\nconverging on the center. They often create discomfort by possibly needed\nmarker suits, and their recording volume is severely restricted and often\nconstrained to indoor scenes with controlled backgrounds. Alternative\nsuit-based systems use several inertial measurement units or an exoskeleton to\ncapture motion. This makes capturing independent of a confined volume, but\nrequires substantial, often constraining, and hard to set up body\ninstrumentation. We therefore propose a new method for real-time, marker-less\nand egocentric motion capture which estimates the full-body skeleton pose from\na lightweight stereo pair of fisheye cameras that are attached to a helmet or\nvirtual reality headset. It combines the strength of a new generative pose\nestimation framework for fisheye views with a ConvNet-based body-part detector\ntrained on a large new dataset. Our inside-in method captures full-body motion\nin general indoor and outdoor scenes, and also crowded scenes with many people\nin close vicinity. The captured user can freely move around, which enables\nreconstruction of larger-scale activities and is particularly useful in virtual\nreality to freely roam and interact, while seeing the fully motion-captured\nvirtual body.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 10:46:19 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Rhodin", "Helge", ""], ["Richardt", "Christian", ""], ["Casas", "Dan", ""], ["Insafutdinov", "Eldar", ""], ["Shafiei", "Mohammad", ""], ["Seidel", "Hans-Peter", ""], ["Schiele", "Bernt", ""], ["Theobalt", "Christian", ""]]}, {"id": "1609.07370", "submitter": "Yi Ren", "authors": "Yi Ren, Yaniv Romano, Michael Elad", "title": "Example-Based Image Synthesis via Randomized Patch-Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and texture synthesis is a challenging task that has long been drawing\nattention in the fields of image processing, graphics, and machine learning.\nThis problem consists of modelling the desired type of images, either through\ntraining examples or via a parametric modeling, and then generating images that\nbelong to the same statistical origin.\n  This work addresses the image synthesis task, focusing on two specific\nfamilies of images -- handwritten digits and face images. This paper offers two\nmain contributions. First, we suggest a simple and intuitive algorithm capable\nof generating such images in a unified way. The proposed approach taken is\npyramidal, consisting of upscaling and refining the estimated image several\ntimes. For each upscaling stage, the algorithm randomly draws small patches\nfrom a patch database, and merges these to form a coherent and novel image with\nhigh visual quality. The second contribution is a general framework for the\nevaluation of the generation performance, which combines three aspects: the\nlikelihood, the originality and the spread of the synthesized images. We assess\nthe proposed synthesis scheme and show that the results are similar in nature,\nand yet different from the ones found in the training set, suggesting that true\nsynthesis effect has been obtained.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:08:30 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Ren", "Yi", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1609.07371", "submitter": "Jonathan Vitale", "authors": "Jonathan Vitale, Mary-Anne Williams and Benjamin Johnston", "title": "The face-space duality hypothesis: a computational model", "comments": "in 38th Annual Meeting of the Cognitive Science Society, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valentine's face-space suggests that faces are represented in a psychological\nmultidimensional space according to their perceived properties. However, the\nproposed framework was initially designed as an account of invariant facial\nfeatures only, and explanations for dynamic features representation were\nneglected. In this paper we propose, develop and evaluate a computational model\nfor a twofold structure of the face-space, able to unify both identity and\nexpression representations in a single implemented model. To capture both\ninvariant and dynamic facial features we introduce the face-space duality\nhypothesis and subsequently validate it through a mathematical presentation\nusing a general approach to dimensionality reduction. Two experiments with real\nfacial images show that the proposed face-space: (1) supports both identity and\nexpression recognition, and (2) has a twofold structure anticipated by our\nformal argument.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:10:52 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Vitale", "Jonathan", ""], ["Williams", "Mary-Anne", ""], ["Johnston", "Benjamin", ""]]}, {"id": "1609.07420", "submitter": "Marko Linna", "authors": "Marko Linna, Juho Kannala, Esa Rahtu", "title": "Real-time Human Pose Estimation from Video with Convolutional Neural\n  Networks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for real-time multi-person human pose\nestimation from video by utilizing convolutional neural networks. Our method is\naimed for use case specific applications, where good accuracy is essential and\nvariation of the background and poses is limited. This enables us to use a\ngeneric network architecture, which is both accurate and fast. We divide the\nproblem into two phases: (1) pre-training and (2) finetuning. In pre-training,\nthe network is learned with highly diverse input data from publicly available\ndatasets, while in finetuning we train with application specific data, which we\nrecord with Kinect. Our method differs from most of the state-of-the-art\nmethods in that we consider the whole system, including person detector, pose\nestimator and an automatic way to record application specific training material\nfor finetuning. Our method is considerably faster than many of the\nstate-of-the-art methods. Our method can be thought of as a replacement for\nKinect, and it can be used for higher level tasks, such as gesture control,\ngames, person tracking, action recognition and action tracking. We achieved\naccuracy of 96.8\\% (PCK@0.2) with application specific data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 16:22:59 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Linna", "Marko", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "1609.07495", "submitter": "Matteo Ruggero Ronchi", "authors": "Matteo Ruggero Ronchi, Joon Sik Kim and Yisong Yue", "title": "A Rotation Invariant Latent Factor Model for Moveme Discovery from\n  Static Poses", "comments": "Long version of the paper accepted at the IEEE ICDM 2016 conference.\n  10 pages, 9 figures, 1 table. Project page:\n  http://www.vision.caltech.edu/~mronchi/projects/RotationInvariantMovemes/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We tackle the problem of learning a rotation invariant latent factor model\nwhen the training data is comprised of lower-dimensional projections of the\noriginal feature space. The main goal is the discovery of a set of 3-D bases\nposes that can characterize the manifold of primitive human motions, or\nmovemes, from a training set of 2-D projected poses obtained from still images\ntaken at various camera angles. The proposed technique for basis discovery is\ndata-driven rather than hand-designed. The learned representation is rotation\ninvariant, and can reconstruct any training instance from multiple viewing\nangles. We apply our method to modeling human poses in sports (via the Leeds\nSports Dataset), and demonstrate the effectiveness of the learned bases in a\nrange of applications such as activity classification, inference of dynamics\nfrom a single frame, and synthetic representation of movements.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 20:00:23 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Ronchi", "Matteo Ruggero", ""], ["Kim", "Joon Sik", ""], ["Yue", "Yisong", ""]]}, {"id": "1609.07597", "submitter": "Suriya Singh", "authors": "Suriya Singh and Vijay Kumar", "title": "DimensionApp : android app to estimate object dimensions", "comments": "Project Report 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we develop an android app that uses on computer vision\ntechniques to estimate an object dimension present in field of view. The app\nwhile having compact size, is accurate upto +/- 5 mm and robust towards touch\ninputs. We use single-view metrology to compute accurate measurement. Unlike\nprevious approaches, our technique does not rely on line detection and can be\ngeneralize to any object shape easily.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 10:32:30 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Singh", "Suriya", ""], ["Kumar", "Vijay", ""]]}, {"id": "1609.07599", "submitter": "Shenglan Liu", "authors": "Shenglan Liu, Muxin Sun, Lin Feng, Yang Liu, Jun Wu", "title": "Three Tiers Neighborhood Graph and Multi-graph Fusion Ranking for\n  Multi-feature Image Retrieval: A Manifold Aspect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single feature is inefficient to describe content of an image, which is a\nshortcoming in traditional image retrieval task. We know that one image can be\ndescribed by different features. Multi-feature fusion ranking can be utilized\nto improve the ranking list of query. In this paper, we first analyze graph\nstructure and multi-feature fusion re-ranking from manifold aspect. Then, Three\nTiers Neighborhood Graph (TTNG) is constructed to re-rank the original ranking\nlist by single feature and to enhance precision of single feature. Furthermore,\nwe propose Multi-graph Fusion Ranking (MFR) for multi-feature ranking, which\nconsiders the correlation of all images in multiple neighborhood graphs.\nEvaluations are conducted on UK-bench, Corel-1K, Corel-10K and Cifar-10\nbenchmark datasets. The experimental results show that our TTNG and MFR\noutperform than other state-of-the-art methods. For example, we achieve\ncompetitive results N-S score 3.91 and precision 65.00% on UK-bench and\nCorel-10K datasets respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 10:34:36 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Liu", "Shenglan", ""], ["Sun", "Muxin", ""], ["Feng", "Lin", ""], ["Liu", "Yang", ""], ["Wu", "Jun", ""]]}, {"id": "1609.07615", "submitter": "Shenglan Liu", "authors": "Shenglan Liu, Jun Wu, Lin Feng, Yang Liu, Hong Qiao, Wenbo Luo Muxin\n  Sun, and Wei Wang", "title": "Perceptual uniform descriptor and Ranking on manifold: A bridge between\n  image representation and ranking for image retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incompatibility of image descriptor and ranking is always neglected in image\nretrieval. In this paper, manifold learning and Gestalt psychology theory are\ninvolved to solve the incompatibility problem. A new holistic descriptor called\nPerceptual Uniform Descriptor (PUD) based on Gestalt psychology is proposed,\nwhich combines color and gradient direction to imitate the human visual\nuniformity. PUD features in the same class images distributes on one manifold\nin most cases because PUD improves the visual uniformity of the traditional\ndescriptors. Thus, we use manifold ranking and PUD to realize image retrieval.\nExperiments were carried out on five benchmark data sets, and the proposed\nmethod can greatly improve the accuracy of image retrieval. Our experimental\nresults in the Ukbench and Corel-1K datasets demonstrated that N-S score\nreached to 3.58 (HSV 3.4) and mAP to 81.77% (ODBTC 77.9%) respectively by\nutilizing PUD which has only 280 dimension. The results are higher than other\nholistic image descriptors (even some local ones) and state-of-the-arts\nretrieval methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 13:13:38 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Liu", "Shenglan", ""], ["Wu", "Jun", ""], ["Feng", "Lin", ""], ["Liu", "Yang", ""], ["Qiao", "Hong", ""], ["Sun", "Wenbo Luo Muxin", ""], ["Wang", "Wei", ""]]}, {"id": "1609.07630", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake, V. A. Coutinho", "title": "Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform", "comments": "Fixed diagonal matrix, 11 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2515378", "report-no": null, "categories": "cs.MM cs.CV cs.DS stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 14:49:31 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 21:18:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 17:05:20 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Coutinho", "V. A.", ""]]}, {"id": "1609.07727", "submitter": "Sankaraganesh Jonna", "authors": "Sankaraganesh Jonna, Krishna K. Nakka, and Rajiv R. Sahay", "title": "Deep learning based fence segmentation and removal from an image using a\n  video sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to image de-fencing use multiple adjacent frames for\nsegmentation of fences in the reference image and are limited to restoring\nimages of static scenes only. In this paper, we propose a de-fencing algorithm\nfor images of dynamic scenes using an occlusion-aware optical flow method. We\ndivide the problem of image de-fencing into the tasks of automated fence\nsegmentation from a single image, motion estimation under known occlusions and\nfusion of data from multiple frames of a captured video of the scene.\nSpecifically, we use a pre-trained convolutional neural network to segment\nfence pixels from a single image. The knowledge of spatial locations of fences\nis used to subsequently estimate optical flow in the occluded frames of the\nvideo for the final data fusion step. We cast the fence removal problem in an\noptimization framework by modeling the formation of the degraded observations.\nThe inverse problem is solved using fast iterative shrinkage thresholding\nalgorithm (FISTA). Experimental results show the effectiveness of proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 10:35:23 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 13:08:23 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Jonna", "Sankaraganesh", ""], ["Nakka", "Krishna K.", ""], ["Sahay", "Rajiv R.", ""]]}, {"id": "1609.07769", "submitter": "Wenhan Yang", "authors": "Wenhan Yang, Robby T. Tan, Jiashi Feng, Jiaying Liu, Zongming Guo,\n  Shuicheng Yan", "title": "Deep Joint Rain Detection and Removal from a Single Image", "comments": "Preliminary version to appear in CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a rain removal problem from a single image, even in\nthe presence of heavy rain and rain streak accumulation. Our core ideas lie in\nthe new rain image models and a novel deep learning architecture. We first\nmodify an existing model comprising a rain streak layer and a background layer,\nby adding a binary map that locates rain streak regions. Second, we create a\nnew model consisting of a component representing rain streak accumulation\n(where individual streaks cannot be seen, and thus visually similar to mist or\nfog), and another component representing various shapes and directions of\noverlapping rain streaks, which usually happen in heavy rain. Based on the\nfirst model, we develop a multi-task deep learning architecture that learns the\nbinary rain streak map, the appearance of rain streaks, and the clean\nbackground, which is our ultimate output. The additional binary map is\ncritically beneficial, since its loss function can provide additional strong\ninformation to the network. To handle rain streak accumulation (again, a\nphenomenon visually similar to mist or fog) and various shapes and directions\nof overlapping rain streaks, we propose a recurrent rain detection and removal\nnetwork that removes rain streaks and clears up the rain accumulation\niteratively and progressively. In each recurrence of our method, a new\ncontextualized dilated network is developed to exploit regional contextual\ninformation and outputs better representation for rain detection. The\nevaluation on real images, particularly on heavy rain, shows the effectiveness\nof our novel models and architecture, outperforming the state-of-the-art\nmethods significantly. Our codes and data sets will be publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 16:36:39 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 12:28:02 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 01:49:36 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Yang", "Wenhan", ""], ["Tan", "Robby T.", ""], ["Feng", "Jiashi", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1609.07826", "submitter": "Georgios Georgakis", "authors": "Georgios Georgakis, Md Alimoor Reza, Arsalan Mousavian, Phi-Hung Le,\n  Jana Kosecka", "title": "Multiview RGB-D Dataset for Object Instance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new multi-view RGB-D dataset of nine kitchen scenes,\neach containing several objects in realistic cluttered environments including a\nsubset of objects from the BigBird dataset. The viewpoints of the scenes are\ndensely sampled and objects in the scenes are annotated with bounding boxes and\nin the 3D point cloud. Also, an approach for detection and recognition is\npresented, which is comprised of two parts: i) a new multi-view 3D proposal\ngeneration method and ii) the development of several recognition baselines\nusing AlexNet to score our proposals, which is trained either on crops of the\ndataset or on synthetically composited training images. Finally, we compare the\nperformance of the object proposals and a detection baseline to the Washington\nRGB-D Scenes (WRGB-D) dataset and demonstrate that our Kitchen scenes dataset\nis more challenging for object detection and recognition. The dataset is\navailable at: http://cs.gmu.edu/~robot/gmu-kitchens.html.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 01:18:56 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Georgakis", "Georgios", ""], ["Reza", "Md Alimoor", ""], ["Mousavian", "Arsalan", ""], ["Le", "Phi-Hung", ""], ["Kosecka", "Jana", ""]]}, {"id": "1609.07835", "submitter": "Lukas von Stumberg", "authors": "Lukas von Stumberg, Vladyslav Usenko, Jakob Engel, J\\\"org St\\\"uckler,\n  Daniel Cremers", "title": "From Monocular SLAM to Autonomous Drone Exploration", "comments": null, "journal-ref": null, "doi": "10.1109/ECMR.2017.8098709", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro aerial vehicles (MAVs) are strongly limited in their payload and power\ncapacity. In order to implement autonomous navigation, algorithms are therefore\ndesirable that use sensory equipment that is as small, low-weight, and\nlow-power consuming as possible. In this paper, we propose a method for\nautonomous MAV navigation and exploration using a low-cost consumer-grade\nquadrocopter equipped with a monocular camera. Our vision-based navigation\nsystem builds on LSD-SLAM which estimates the MAV trajectory and a semi-dense\nreconstruction of the environment in real-time. Since LSD-SLAM only determines\ndepth at high gradient pixels, texture-less areas are not directly observed so\nthat previous exploration methods that assume dense map information cannot\ndirectly be applied. We propose an obstacle mapping and exploration approach\nthat takes the properties of our semi-dense monocular SLAM system into account.\nIn experiments, we demonstrate our vision-based autonomous navigation and\nexploration system with a Parrot Bebop MAV.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 03:02:35 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 22:34:51 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 14:24:32 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["von Stumberg", "Lukas", ""], ["Usenko", "Vladyslav", ""], ["Engel", "Jakob", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1609.07859", "submitter": "Taewan Kim", "authors": "Taewan Kim, Seyeong Kim, Sangil Na, Hayoon Kim, Moonki Kim, Byoung-Ki\n  Jeon", "title": "Visual Fashion-Product Search at SK Planet", "comments": "13 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a large-scale visual search system which finds similar product\nimages given a fashion item. Defining similarity among arbitrary\nfashion-products is still remains a challenging problem, even there is no exact\nground-truth. To resolve this problem, we define more than 90 fashion-related\nattributes, and combination of these attributes can represent thousands of\nunique fashion-styles. The fashion-attributes are one of the ingredients to\ndefine semantic similarity among fashion-product images. To build our system at\nscale, these fashion-attributes are again used to build an inverted indexing\nscheme. In addition to these fashion-attributes for semantic similarity, we\nextract colour and appearance features in a region-of-interest (ROI) of a\nfashion item for visual similarity. By sharing our approach, we expect active\ndiscussion on that how to apply current computer vision research into the\ne-commerce industry.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 06:53:36 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 04:47:48 GMT"}, {"version": "v3", "created": "Fri, 7 Oct 2016 08:28:22 GMT"}, {"version": "v4", "created": "Wed, 19 Oct 2016 12:50:49 GMT"}, {"version": "v5", "created": "Sun, 6 Nov 2016 07:57:32 GMT"}, {"version": "v6", "created": "Wed, 12 Apr 2017 03:51:23 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Kim", "Taewan", ""], ["Kim", "Seyeong", ""], ["Na", "Sangil", ""], ["Kim", "Hayoon", ""], ["Kim", "Moonki", ""], ["Jeon", "Byoung-Ki", ""]]}, {"id": "1609.07876", "submitter": "Taehwan Kim", "authors": "Taehwan Kim, Jonathan Keane, Weiran Wang, Hao Tang, Jason Riggle,\n  Gregory Shakhnarovich, Diane Brentari, Karen Livescu", "title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and\n  Signer Adaptation", "comments": "arXiv admin note: substantial text overlap with arXiv:1608.08339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognizing video sequences of fingerspelled letters\nin American Sign Language (ASL). Fingerspelling comprises a significant but\nrelatively understudied part of ASL. Recognizing fingerspelling is challenging\nfor a number of reasons: It involves quick, small motions that are often highly\ncoarticulated; it exhibits significant variation between signers; and there has\nbeen a dearth of continuous fingerspelling data collected. In this work we\ncollect and annotate a new data set of continuous fingerspelling videos,\ncompare several types of recognizers, and explore the problem of signer\nvariation. Our best-performing models are segmental (semi-Markov) conditional\nrandom fields using deep neural network-based features. In the signer-dependent\nsetting, our recognizers achieve up to about 92% letter accuracy. The\nmulti-signer setting is much more challenging, but with neural network\nadaptation we achieve up to 83% letter accuracies in this setting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 07:34:24 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Kim", "Taehwan", ""], ["Keane", "Jonathan", ""], ["Wang", "Weiran", ""], ["Tang", "Hao", ""], ["Riggle", "Jason", ""], ["Shakhnarovich", "Gregory", ""], ["Brentari", "Diane", ""], ["Livescu", "Karen", ""]]}, {"id": "1609.07878", "submitter": "Sujoy Kumar Biswas", "authors": "Sujoy Kumar Biswas and Peyman Milanfar", "title": "Linear Support Tensor Machine: Pedestrian Detection in Thermal Infrared\n  Images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2705426", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection in thermal infrared images poses unique challenges\nbecause of the low resolution and noisy nature of the image. Here we propose a\nmid-level attribute in the form of multidimensional template, or tensor, using\nLocal Steering Kernel (LSK) as low-level descriptors for detecting pedestrians\nin far infrared images. LSK is specifically designed to deal with intrinsic\nimage noise and pixel level uncertainty by capturing local image geometry\nsuccinctly instead of collecting local orientation statistics (e.g., histograms\nin HOG). Our second contribution is the introduction of a new image similarity\nkernel in the popular maximum margin framework of support vector machines that\nresults in a relatively short and simple training phase for building a rigid\npedestrian detector. Our third contribution is to replace the sluggish but de\nfacto sliding window based detection methodology with multichannel discrete\nFourier transform, facilitating very fast and efficient pedestrian\nlocalization. The experimental studies on publicly available thermal infrared\nimages justify our proposals and model assumptions. In addition, the proposed\nwork also involves the release of our in-house annotations of pedestrians in\nmore than 17000 frames of OSU Color Thermal database for the purpose of sharing\nwith the research community.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 07:54:00 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Biswas", "Sujoy Kumar", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1609.07916", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Lukas Cavigelli, Fabian Mentzer, Thomas Wiatowski,\n  Luca Benini", "title": "Deep Structured Features for Semantic Segmentation", "comments": "EUSIPCO 2017, 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a highly structured neural network architecture for semantic\nsegmentation with an extremely small model size, suitable for low-power\nembedded and mobile platforms. Specifically, our architecture combines i) a\nHaar wavelet-based tree-like convolutional neural network (CNN), ii) a random\nlayer realizing a radial basis function kernel approximation, and iii) a linear\nclassifier. While stages i) and ii) are completely pre-specified, only the\nlinear classifier is learned from data. We apply the proposed architecture to\noutdoor scene and aerial image semantic segmentation and show that the accuracy\nof our architecture is competitive with conventional pixel classification CNNs.\nFurthermore, we demonstrate that the proposed architecture is data efficient in\nthe sense of matching the accuracy of pixel classification CNNs when trained on\na much smaller data set.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 10:33:13 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 16:23:14 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 15:12:49 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Tschannen", "Michael", ""], ["Cavigelli", "Lukas", ""], ["Mentzer", "Fabian", ""], ["Wiatowski", "Thomas", ""], ["Benini", "Luca", ""]]}, {"id": "1609.07982", "submitter": "Rene Grzeszick", "authors": "Rene Grzeszick, Sebastian Sudholt, Gernot A. Fink", "title": "Optimistic and Pessimistic Neural Networks for Scene and Object\n  Recognition", "comments": "This Version fixes an error in eq. 1,2 and adds some details to the\n  significance testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the application of uncertainty modeling to convolutional neural\nnetworks is evaluated. A novel method for adjusting the network's predictions\nbased on uncertainty information is introduced. This allows the network to be\neither optimistic or pessimistic in its prediction scores. The proposed method\nbuilds on the idea of applying dropout at test time and sampling a predictive\nmean and variance from the network's output. Besides the methodological\naspects, implementation details allowing for a fast evaluation are presented.\nFurthermore, a multilabel network architecture is introduced that strongly\nbenefits from the presented approach. In the evaluation it will be shown that\nmodeling uncertainty allows for improving the performance of a given model\npurely at test time without any further training steps. The evaluation\nconsiders several applications in the field of computer vision, including\nobject classification and detection as well as scene attribute recognition.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 14:24:08 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 12:25:35 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Grzeszick", "Rene", ""], ["Sudholt", "Sebastian", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1609.07986", "submitter": "Nicolas Brodu", "authors": "Nicolas Brodu", "title": "Super-resolving multiresolution images with band-independant geometry of\n  multispectral pixels", "comments": "Source code with a ready-to-use script for super-resolving Sentinel-2\n  data is available at http://nicolas.brodu.net/recherche/superres/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new resolution enhancement method is presented for multispectral and\nmulti-resolution images, such as these provided by the Sentinel-2 satellites.\nStarting from the highest resolution bands, band-dependent information\n(reflectance) is separated from information that is common to all bands\n(geometry of scene elements). This model is then applied to unmix\nlow-resolution bands, preserving their reflectance, while propagating\nband-independent information to preserve the sub-pixel details. A reference\nimplementation is provided, with an application example for super-resolving\nSentinel-2 data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 14:28:49 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 15:49:21 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 10:29:55 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Brodu", "Nicolas", ""]]}, {"id": "1609.08004", "submitter": "Jose Rodrigues Jr", "authors": "Bruno Machado, Jonatan Orue, Mauro Arruda, Cleidimar Santos, Diogo\n  Sarath, Wesley Goncalves, Gercina Silva, Hemerson Pistori, Antonia Roel, Jose\n  Rodrigues-Jr", "title": "BioLeaf: a professional mobile application to measure foliar damage\n  caused by insect herbivory", "comments": null, "journal-ref": "Computers and Electronics in Agriculture 129: 1. 44-55 (2016)", "doi": "10.1016/j.compag.2016.09.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soybean is one of the ten greatest crops in the world, answering for\nbillion-dollar businesses every year. This crop suffers from insect herbivory\nthat costs millions from producers. Hence, constant monitoring of the crop\nfoliar damage is necessary to guide the application of insecticides. However,\ncurrent methods to measure foliar damage are expensive and dependent on\nlaboratory facilities, in some cases, depending on complex devices. To cope\nwith these shortcomings, we introduce an image processing methodology to\nmeasure the foliar damage in soybean leaves. We developed a non-destructive\nimaging method based on two techniques, Otsu segmentation and Bezier curves, to\nestimate the foliar loss in leaves with or without border damage. We\ninstantiate our methodology in a mobile application named BioLeaf, which is\nfreely distributed for smartphone users. We experimented with real-world leaves\ncollected from a soybean crop in Brazil. Our results demonstrated that BioLeaf\nachieves foliar damage quantification with precision comparable to that of\nhuman specialists. With these results, our proposal might assist soybean\nproducers, reducing the time to measure foliar damage, reducing analytical\ncosts, and defining a commodity application that is applicable not only to soy,\nbut also to different crops such as cotton, bean, potato, coffee, and\nvegetables.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 14:59:50 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 01:02:57 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Machado", "Bruno", ""], ["Orue", "Jonatan", ""], ["Arruda", "Mauro", ""], ["Santos", "Cleidimar", ""], ["Sarath", "Diogo", ""], ["Goncalves", "Wesley", ""], ["Silva", "Gercina", ""], ["Pistori", "Hemerson", ""], ["Roel", "Antonia", ""], ["Rodrigues-Jr", "Jose", ""]]}, {"id": "1609.08035", "submitter": "Jayajit Das", "authors": "Sayak Mukherjee, David Stewart, William Stewart, Lewis L. Lanier,\n  Jayajit Das", "title": "Connecting the dots across time: Reconstruction of single cell signaling\n  trajectories using time-stamped data", "comments": "revised version, accepted for publication in Royal Society Open\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cond-mat.stat-mech cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single cell responses are shaped by the geometry of signaling kinetic\ntrajectories carved in a multidimensional space spanned by signaling protein\nabundances. It is however challenging to assay large number (>3) of signaling\nspecies in live-cell imaging which makes it difficult to probe single cell\nsignaling kinetic trajectories in large dimensions. Flow and mass cytometry\ntechniques can measure a large number (4 - >40) of signaling species but are\nunable to track single cells. Thus cytometry experiments provide detailed time\nstamped snapshots of single cell signaling kinetics. Is it possible to use the\ntime stamped cytometry data to reconstruct single cell signaling trajectories?\nBorrowing concepts of conserved and slow variables from non-equilibrium\nstatistical physics we develop an approach to reconstruct signaling\ntrajectories using snapshot data by creating new variables that remain\ninvariant or vary slowly during the signaling kinetics. We apply this approach\nto reconstruct trajectories using snapshot data obtained from in silico\nsimulations and live-cell imaging measurements. The use of invariants and slow\nvariables to reconstruct trajectories provides a radically different way to\ntrack object using snapshot data. The approach is likely to have implications\nfor solving matching problems in a wide range of disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 15:56:12 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 14:33:02 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Mukherjee", "Sayak", ""], ["Stewart", "David", ""], ["Stewart", "William", ""], ["Lanier", "Lewis L.", ""], ["Das", "Jayajit", ""]]}, {"id": "1609.08078", "submitter": "Chiwoo Park", "authors": "Garret Vo and Chiwoo Park", "title": "Robust Regression For Image Binarization Under Heavy Noises and\n  Nonuniform Background", "comments": null, "journal-ref": "Pattern Recognition Volume 81, September 2018, Pages 224-239", "doi": "10.1016/j.patcog.2018.04.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust regression approach for image binarization under\nsignificant background variations and observation noises. The work is motivated\nby the need of identifying foreground regions in noisy microscopic image or\ndegraded document images, where significant background variation and severe\nnoise make an image binarization challenging. The proposed method first\nestimates the background of an input image, subtracts the estimated background\nfrom the input image, and apply a global thresholding to the subtracted outcome\nfor achieving a binary image of foregrounds. A robust regression approach was\nproposed to estimate the background intensity surface with minimal effects of\nforeground intensities and noises, and a global threshold selector was proposed\non the basis of a model selection criterion in a sparse regression. The\nproposed approach was validated using 26 test images and the corresponding\nground truths, and the outcomes of the proposed work were compared with those\nfrom nine existing image binarization methods. The approach was also combined\nwith three state-of-the-art morphological segmentation methods to show how the\nproposed approach can improve their image segmentation outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 17:07:49 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 17:30:08 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 16:06:59 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 20:35:51 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Vo", "Garret", ""], ["Park", "Chiwoo", ""]]}, {"id": "1609.08080", "submitter": "Malcolm Reynolds", "authors": "Malcolm Reynolds, Tom S. F. Haines and Gabriel J. Brostow", "title": "Swipe Mosaics from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A panoramic image mosaic is an attractive visualization for viewing many\noverlapping photos, but its images must be both captured and processed\ncorrectly to produce an acceptable composite. We propose Swipe Mosaics, an\ninteractive visualization that places the individual video frames on a 2D\nplanar map that represents the layout of the physical scene. Compared to\ntraditional panoramic mosaics, our capture is easier because the user can both\ntranslate the camera center and film moving subjects. Processing and display\ndegrade gracefully if the footage lacks distinct, overlapping, non-repeating\ntexture. Our proposed visual odometry algorithm produces a distribution over\n(x,y) translations for image pairs. Inferring a distribution of possible camera\nmotions allows us to better cope with parallax, lack of texture, dynamic\nscenes, and other phenomena that hurt deterministic reconstruction techniques.\nRobustness is obtained by training on synthetic scenes with known camera\nmotions. We show that Swipe Mosaics are easy to generate, support a wide range\nof difficult scenes, and are useful for documenting a scene for closer\ninspection.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 17:14:53 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Reynolds", "Malcolm", ""], ["Haines", "Tom S. F.", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1609.08124", "submitter": "Atousa Torabi Atousa Torabi", "authors": "Atousa Torabi, Niket Tandon, Leonid Sigal", "title": "Learning Language-Visual Embedding for Movie Understanding with\n  Natural-Language", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a joint language-visual embedding has a number of very appealing\nproperties and can result in variety of practical application, including\nnatural language image/video annotation and search. In this work, we study\nthree different joint language-visual neural network model architectures. We\nevaluate our models on large scale LSMDC16 movie dataset for two tasks: 1)\nStandard Ranking for video annotation and retrieval 2) Our proposed movie\nmultiple-choice test. This test facilitate automatic evaluation of\nvisual-language models for natural language video annotation based on human\nactivities. In addition to original Audio Description (AD) captions, provided\nas part of LSMDC16, we collected and will make available a) manually generated\nre-phrasings of those captions obtained using Amazon MTurk b) automatically\ngenerated human activity elements in \"Predicate + Object\" (PO) phrases based on\n\"Knowlywood\", an activity knowledge mining model. Our best model archives\nRecall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset\nof 1000 samples. For multiple-choice test, our best model achieve accuracy\n58.11% over whole LSMDC16 public test-set.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 19:14:12 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Torabi", "Atousa", ""], ["Tandon", "Niket", ""], ["Sigal", "Leonid", ""]]}, {"id": "1609.08209", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Ivan Koptelov and German Novikov and Timur Khanipov", "title": "Automatic Construction of a Recurrent Neural Network based Classifier\n  for Vehicle Passage Detection", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are extensively used for time-series\nmodeling and prediction. We propose an approach for automatic construction of a\nbinary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for\ndetection of a vehicle passage through a checkpoint. As an input to the\nclassifier we use multidimensional signals of various sensors that are\ninstalled on the checkpoint. Obtained results demonstrate that the previous\napproach to handcrafting a classifier, consisting of a set of deterministic\nrules, can be successfully replaced by an automatic RNN training on an\nappropriately labelled data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 22:11:05 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Koptelov", "Ivan", ""], ["Novikov", "German", ""], ["Khanipov", "Timur", ""]]}, {"id": "1609.08221", "submitter": "Rui Liu", "authors": "Rui Liu, Hossein Nejati, Seyed Hamid Safavi, Ngai-Man Cheung", "title": "Simultaneous Low-rank Component and Graph Estimation for\n  High-dimensional Graph Signals: Application to Brain Imaging", "comments": "Accepted by ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to uncover the intrinsic low-rank component of a\nhigh-dimensional, graph-smooth and grossly-corrupted dataset, under the\nsituations that the underlying graph is unknown. Based on a model with a\nlow-rank component plus a sparse perturbation, and an initial graph estimation,\nour proposed algorithm simultaneously learns the low-rank component and refines\nthe graph. Our evaluations using synthetic and real brain imaging data in\nunsupervised and supervised classification tasks demonstrate encouraging\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 23:24:27 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 03:23:43 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Liu", "Rui", ""], ["Nejati", "Hossein", ""], ["Safavi", "Seyed Hamid", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1609.08267", "submitter": "Robert Maier", "authors": "Maksym Dzitsiuk, J\\\"urgen Sturm, Robert Maier, Lingni Ma and Daniel\n  Cremers", "title": "De-noising, Stabilizing and Completing 3D Reconstructions On-the-go\n  using Plane Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating 3D maps on robots and other mobile devices has become a reality in\nrecent years. Online 3D reconstruction enables many exciting applications in\nrobotics and AR/VR gaming. However, the reconstructions are noisy and generally\nincomplete. Moreover, during onine reconstruction, the surface changes with\nevery newly integrated depth image which poses a significant challenge for\nphysics engines and path planning algorithms. This paper presents a novel, fast\nand robust method for obtaining and using information about planar surfaces,\nsuch as walls, floors, and ceilings as a stage in 3D reconstruction based on\nSigned Distance Fields. Our algorithm recovers clean and accurate surfaces,\nreduces the movement of individual mesh vertices caused by noise during online\nreconstruction and fills in the occluded and unobserved regions. We implemented\nand evaluated two different strategies to generate plane candidates and two\nstrategies for merging them. Our implementation is optimized to run in\nreal-time on mobile devices such as the Tango tablet. In an extensive set of\nexperiments, we validated that our approach works well in a large number of\nnatural environments despite the presence of significant amount of occlusion,\nclutter and noise, which occur frequently. We further show that plane fitting\nenables in many cases a meaningful semantic segmentation of real-world scenes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 05:59:12 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 01:45:24 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Dzitsiuk", "Maksym", ""], ["Sturm", "J\u00fcrgen", ""], ["Maier", "Robert", ""], ["Ma", "Lingni", ""], ["Cremers", "Daniel", ""]]}, {"id": "1609.08291", "submitter": "Yusuke Uchida", "authors": "Yusuke Uchida, Shigeyuki Sakazawa, Shin'ichi Satoh", "title": "Image Retrieval with Fisher Vectors of Binary Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Fisher vector representation of local features has attracted\nmuch attention because of its effectiveness in both image classification and\nimage retrieval. Another trend in the area of image retrieval is the use of\nbinary features such as ORB, FREAK, and BRISK. Considering the significant\nperformance improvement for accuracy in both image classification and retrieval\nby the Fisher vector of continuous feature descriptors, if the Fisher vector\nwere also to be applied to binary features, we would receive similar benefits\nin binary feature based image retrieval and classification. In this paper, we\nderive the closed-form approximation of the Fisher vector of binary features\nmodeled by the Bernoulli mixture model. We also propose accelerating the Fisher\nvector by using the approximate value of posterior probability. Experiments\nshow that the Fisher vector representation significantly improves the accuracy\nof image retrieval compared with a bag of binary words approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 07:33:58 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Uchida", "Yusuke", ""], ["Sakazawa", "Shigeyuki", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1609.08364", "submitter": "Mohamed Elawady", "authors": "Ibrahim Sadek, Mohamed Elawady, Viktor Stefanovski", "title": "Automated Breast Lesion Segmentation in Ultrasound Images", "comments": "Course Work in 'Medical Image Analysis' Module, VIBOT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this project is to segment different breast ultrasound\nimages to find out lesion area by discarding the low contrast regions as well\nas the inherent speckle noise. The proposed method consists of three stages\n(removing noise, segmentation, classification) in order to extract the correct\nlesion. We used normalized cuts approach to segment ultrasound images into\nregions of interest where we can possibly finds the lesion, and then K-means\nclassifier is applied to decide finally the location of the lesion. For every\noriginal image, an annotated ground-truth image is given to perform comparison\nwith the obtained experimental results, providing accurate evaluation measures.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 11:51:16 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Sadek", "Ibrahim", ""], ["Elawady", "Mohamed", ""], ["Stefanovski", "Viktor", ""]]}, {"id": "1609.08387", "submitter": "Jinming Duan", "authors": "Jinming Duan, Wil OC Ward, Luke Sibbett, Zhenkuan Pan, Li Bai", "title": "Tensor Based Second Order Variational Model for Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second order total variation (SOTV) models have advantages for image\nreconstruction over their first order counterparts including their ability to\nremove the staircase artefact in the reconstructed image, but they tend to blur\nthe reconstructed image. To overcome this drawback, we introduce a new Tensor\nWeighted Second Order (TWSO) model for image reconstruction. Specifically, we\ndevelop a novel regulariser for the SOTV model that uses the Frobenius norm of\nthe product of the SOTV Hessian matrix and the anisotropic tensor. We then\nadapt the alternating direction method of multipliers (ADMM) to solve the\nproposed model by breaking down the original problem into several subproblems.\nAll the subproblems have closed-forms and can thus be solved efficiently. The\nproposed method is compared with a range of state-of-the-art approaches such as\ntensor-based anisotropic diffusion, total generalised variation, Euler's\nelastica, etc. Numerical experimental results of the method on both synthetic\nand real images from the Berkeley database BSDS500 demonstrate that the\nproposed method eliminates both the staircase and blurring effects and\noutperforms the existing approaches for image inpainting and denoising\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 12:51:09 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Duan", "Jinming", ""], ["Ward", "Wil OC", ""], ["Sibbett", "Luke", ""], ["Pan", "Zhenkuan", ""], ["Bai", "Li", ""]]}, {"id": "1609.08393", "submitter": "Veronique Eglin", "authors": "St\\'ephane Bres (imagine), V\\'eronique Eglin (imagine), Vincent\n  Poulain (LOCEAN)", "title": "Semi Automatic Color Segmentation of Document Pages", "comments": null, "journal-ref": "International Workshop on Document Analysis System, Apr 2016,\n  Santorini, France", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  -This paper presents a semi automatic method used to segment color documents\ninto different uniform color plans. The practical application is dedicated to\nadministrative documents segmentation. In these documents, like in many other\ncases, color has a semantic meaning: it is then possible to identify some\nspecific regions like manual annotations, rubber stamps or colored\nhighlighting. A first step of user-controlled learning of the desired color\nplans is made on few sample documents. An automatic process can then be\nperformed on the much bigger set as a batch. Our experiments show very\ninteresting results in with a very competitive processing time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:04:44 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Bres", "St\u00e9phane", "", "imagine"], ["Eglin", "V\u00e9ronique", "", "imagine"], ["Poulain", "Vincent", "", "LOCEAN"]]}, {"id": "1609.08399", "submitter": "Mohamed Moustafa", "authors": "Eman Ahmed, Mohamed Moustafa", "title": "House price estimation from visual and textual features", "comments": "NCTA 2016. Final paper is on SCITEPRESS digital library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing automatic house price estimation systems rely only on some\ntextual data like its neighborhood area and the number of rooms. The final\nprice is estimated by a human agent who visits the house and assesses it\nvisually. In this paper, we propose extracting visual features from house\nphotographs and combining them with the house's textual information. The\ncombined features are fed to a fully connected multilayer Neural Network (NN)\nthat estimates the house price as its single output. To train and evaluate our\nnetwork, we have collected the first houses dataset (to our knowledge) that\ncombines both images and textual attributes. The dataset is composed of 535\nsample houses from the state of California, USA. Our experiments showed that\nadding the visual features increased the R-value by a factor of 3 and decreased\nthe Mean Square Error (MSE) by one order of magnitude compared with\ntextual-only features. Additionally, when trained on the benchmark textual-only\nfeatures housing dataset, our proposed NN still outperformed the existing model\npublished results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:15:31 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Ahmed", "Eman", ""], ["Moustafa", "Mohamed", ""]]}, {"id": "1609.08417", "submitter": "Ru-Ze Liang", "authors": "Yanyan Geng, Ru-Ze Liang, Weizhi Li, Jingbin Wang, Gaoyuan Liang,\n  Chenhao Xu, Jing-Yan Wang", "title": "Learning convolutional neural network to maximize Pos@Top performance\n  measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the machine learning problems, the performance measure is used to evaluate\nthe machine learning models. Recently, the number positive data points ranked\nat the top positions (Pos@Top) has been a popular performance measure in the\nmachine learning community. In this paper, we propose to learn a convolutional\nneural network (CNN) model to maximize the Pos@Top performance measure. The CNN\nmodel is used to represent the multi-instance data point, and a classifier\nfunction is used to predict the label from the its CNN representation. We\npropose to minimize the loss function of Pos@Top over a training set to learn\nthe filters of CNN and the classifier parameter. The classifier parameter\nvector is solved by the Lagrange multiplier method, and the filters are updated\nby the gradient descent method alternately in an iterative algorithm.\nExperiments over benchmark data sets show that the proposed method outperforms\nthe state-of-the-art Pos@Top maximization methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:27:40 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 04:04:49 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 02:55:45 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Geng", "Yanyan", ""], ["Liang", "Ru-Ze", ""], ["Li", "Weizhi", ""], ["Wang", "Jingbin", ""], ["Liang", "Gaoyuan", ""], ["Xu", "Chenhao", ""], ["Wang", "Jing-Yan", ""]]}, {"id": "1609.08436", "submitter": "Kangru Wang", "authors": "Kangru Wang, Lei Qu, Lili Chen, Yuzhang Gu, DongChen zhu, Xiaolin\n  Zhang", "title": "Non-flat Ground Detection Based on A Local Descriptor", "comments": "9 pages, submitted to IEICE Transactions on Information and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of road and free space remains challenging for non-flat plane,\nespecially with the varying latitudinal and longitudinal slope or in the case\nof multi-ground plane. In this paper, we propose a framework of the ground\nplane detection with stereo vision. The main contribution of this paper is a\nnewly proposed descriptor which is implemented in the disparity image to obtain\na disparity texture image. The ground plane regions can be distinguished from\ntheir surroundings effectively in the disparity texture image. Because the\ndescriptor is implemented in the local area of the image, it can address well\nthe problem of non-flat plane. And we also present a complete framework to\ndetect the ground plane regions base on the disparity texture image with\nconvolutional neural network architecture.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:41:04 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 16:16:37 GMT"}, {"version": "v3", "created": "Sat, 28 Jan 2017 06:22:12 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 11:49:17 GMT"}, {"version": "v5", "created": "Tue, 7 Mar 2017 12:47:26 GMT"}, {"version": "v6", "created": "Wed, 19 Apr 2017 16:20:42 GMT"}, {"version": "v7", "created": "Sat, 22 Apr 2017 12:01:35 GMT"}, {"version": "v8", "created": "Tue, 5 Jun 2018 07:20:42 GMT"}, {"version": "v9", "created": "Wed, 6 Jun 2018 00:54:05 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Wang", "Kangru", ""], ["Qu", "Lei", ""], ["Chen", "Lili", ""], ["Gu", "Yuzhang", ""], ["zhu", "DongChen", ""], ["Zhang", "Xiaolin", ""]]}, {"id": "1609.08438", "submitter": "Raz Z. Nossek", "authors": "Raz Z. Nossek and Guy Gilboa", "title": "Flows Generating Nonlinear Eigenfunctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear variational methods have become very powerful tools for many image\nprocessing tasks. Recently a new line of research has emerged, dealing with\nnonlinear eigenfunctions induced by convex functionals. This has provided new\ninsights and better theoretical understanding of convex regularization and\nintroduced new processing methods. However, the theory of nonlinear eigenvalue\nproblems is still at its infancy. We present a new flow that can generate\nnonlinear eigenfunctions of the form $T(u)=\\lambda u$, where $T(u)$ is a\nnonlinear operator and $\\lambda \\in \\mathbb{R} $ is the eigenvalue. We develop\nthe theory where $T(u)$ is a subgradient element of a regularizing\none-homogeneous functional, such as total-variation (TV) or\ntotal-generalized-variation (TGV). We introduce two flows: a forward flow and\nan inverse flow; for which the steady state solution is a nonlinear\neigenfunction. The forward flow monotonically smooths the solution (with\nrespect to the regularizer) and simultaneously increases the $L^2$ norm. The\ninverse flow has the opposite characteristics. For both flows, the steady state\ndepends on the initial condition, thus different initial conditions yield\ndifferent eigenfunctions. This enables a deeper investigation into the space of\nnonlinear eigenfunctions, allowing to produce numerically diverse examples,\nwhich may be unknown yet. In addition we suggest an indicator to measure the\naffinity of a function to an eigenfunction and relate it to\npseudo-eigenfunctions in the linear case.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:42:13 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Nossek", "Raz Z.", ""], ["Gilboa", "Guy", ""]]}, {"id": "1609.08475", "submitter": "Ester Hait", "authors": "Ester Hait and Guy Gilboa", "title": "Blind Facial Image Quality Enhancement using Non-Rigid Semantic Patches", "comments": "Please see the updated published version: Hait, Ester, and Guy\n  Gilboa. Blind Facial Image Quality Enhancement using Non-Rigid Semantic\n  Patches. IEEE Transactions on Image Processing 26.6 (2017): 2705", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to combine semantic data and registration algorithms to solve\nvarious image processing problems such as denoising, super-resolution and\ncolor-correction. It is shown how such new techniques can achieve significant\nquality enhancement, both visually and quantitatively, in the case of facial\nimage enhancement. Our model assumes prior high quality data of the person to\nbe processed, but no knowledge of the degradation model. We try to overcome the\nclassical processing limits by using semantically-aware patches, with adaptive\nsize and location regions of coherent structure and context, as building\nblocks. The method is demonstrated on the problem of cellular photography\nenhancement of dark facial images for different identities, expressions and\nposes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 14:29:33 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 07:39:56 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Hait", "Ester", ""], ["Gilboa", "Guy", ""]]}, {"id": "1609.08661", "submitter": "Antonia Creswell", "authors": "Antonia Creswell and Anil A. Bharath", "title": "Task Specific Adversarial Cost Function", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost function used to train a generative model should fit the purpose of\nthe model. If the model is intended for tasks such as generating perceptually\ncorrect samples, it is beneficial to maximise the likelihood of a sample drawn\nfrom the model, Q, coming from the same distribution as the training data, P.\nThis is equivalent to minimising the Kullback-Leibler (KL) distance, KL[Q||P].\nHowever, if the model is intended for tasks such as retrieval or classification\nit is beneficial to maximise the likelihood that a sample drawn from the\ntraining data is captured by the model, equivalent to minimising KL[P||Q]. The\ncost function used in adversarial training optimises the Jensen-Shannon entropy\nwhich can be seen as an even interpolation between KL[Q||P] and KL[P||Q]. Here,\nwe propose an alternative adversarial cost function which allows easy tuning of\nthe model for either task. Our task specific cost function is evaluated on a\ndataset of hand-written characters in the following tasks: Generation,\nretrieval and one-shot learning.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 20:47:42 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Creswell", "Antonia", ""], ["Bharath", "Anil A.", ""]]}, {"id": "1609.08669", "submitter": "Matthew Thorpe", "authors": "Matthew Thorpe, Serim Park, Soheil Kolouri, Gustavo K. Rohde, Dejan\n  Slep\\v{c}ev", "title": "A Transportation $L^p$ Distance for Signal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transport based distances, such as the Wasserstein distance and earth mover's\ndistance, have been shown to be an effective tool in signal and image analysis.\nThe success of transport based distances is in part due to their Lagrangian\nnature which allows it to capture the important variations in many signal\nclasses. However these distances require the signal to be nonnegative and\nnormalized. Furthermore, the signals are considered as measures and compared by\nredistributing (transporting) them, which does not directly take into account\nthe signal intensity. Here we study a transport-based distance, called the\n$TL^p$ distance, that combines Lagrangian and intensity modelling and is\ndirectly applicable to general, non-positive and multi-channelled signals. The\nframework allows the application of existing numerical methods. We give an\noverview of the basic properties of this distance and applications to\nclassification, with multi-channelled, non-positive one and two-dimensional\nsignals, and color transfer.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 21:08:22 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Thorpe", "Matthew", ""], ["Park", "Serim", ""], ["Kolouri", "Soheil", ""], ["Rohde", "Gustavo K.", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1609.08675", "submitter": "Sami Abu-El-Haija", "authors": "Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George\n  Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan", "title": "YouTube-8M: A Large-Scale Video Classification Benchmark", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advancements in Computer Vision are attributed to large datasets.\nOpen-source software packages for Machine Learning and inexpensive commodity\nhardware have reduced the barrier of entry for exploring novel approaches at\nscale. It is possible to train models over millions of examples within a few\ndays. Although large-scale datasets exist for image understanding, such as\nImageNet, there are no comparable size video classification datasets.\n  In this paper, we introduce YouTube-8M, the largest multi-label video\nclassification dataset, composed of ~8 million videos (500K hours of video),\nannotated with a vocabulary of 4800 visual entities. To get the videos and\ntheir labels, we used a YouTube video annotation system, which labels videos\nwith their main topics. While the labels are machine-generated, they have\nhigh-precision and are derived from a variety of human-based signals including\nmetadata and query click signals. We filtered the video labels (Knowledge Graph\nentities) using both automated and manual curation strategies, including asking\nhuman raters if the labels are visually recognizable. Then, we decoded each\nvideo at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to\nextract the hidden representation immediately prior to the classification\nlayer. Finally, we compressed the frame features and make both the features and\nvideo-level labels available for download.\n  We trained various (modest) classification models on the dataset, evaluated\nthem using popular evaluation metrics, and report them as baselines. Despite\nthe size of the dataset, some of our models train to convergence in less than a\nday on a single machine using TensorFlow. We plan to release code for training\na TensorFlow model and for computing metrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 21:21:49 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Abu-El-Haija", "Sami", ""], ["Kothari", "Nisarg", ""], ["Lee", "Joonseok", ""], ["Natsev", "Paul", ""], ["Toderici", "George", ""], ["Varadarajan", "Balakrishnan", ""], ["Vijayanarasimhan", "Sudheendra", ""]]}, {"id": "1609.08677", "submitter": "Zhao Kang", "authors": "Chong Peng, Zhao Kang, Qiang Chen", "title": "A Fast Factorization-based Approach to Robust PCA", "comments": "ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) has been widely used for\nrecovering low-rank matrices in many data mining and machine learning problems.\nIt separates a data matrix into a low-rank part and a sparse part. The convex\napproach has been well studied in the literature. However, state-of-the-art\nalgorithms for the convex approach usually have relatively high complexity due\nto the need of solving (partial) singular value decompositions of large\nmatrices. A non-convex approach, AltProj, has also been proposed with lighter\ncomplexity and better scalability. Given the true rank $r$ of the underlying\nlow rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\\times n$ is\nthe size of data matrix. In this paper, we propose a novel factorization-based\nmodel of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound\nof the true rank. Our method does not need the precise value of the true rank.\nFrom extensive experiments, we observe that AltProj can work only when $r$ is\nprecisely known in advance; however, when the needed rank parameter $r$ is\nspecified to a value different from the true rank, AltProj cannot fully\nseparate the two parts while our method succeeds. Even when both work, our\nmethod is about 4 times faster than AltProj. Our method can be used as a\nlight-weight, scalable tool for RPCA in the absence of the precise value of the\ntrue rank.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 21:32:16 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Peng", "Chong", ""], ["Kang", "Zhao", ""], ["Chen", "Qiang", ""]]}, {"id": "1609.08685", "submitter": "Soren Pirk", "authors": "S\\\"oren Pirk, Vojtech Krs, Kaimo Hu, Suren Deepak Rajasekaran, Hao\n  Kang, Bedrich Benes, Yusuke Yoshiyasu, Leonidas J. Guibas", "title": "Understanding and Exploiting Object Interaction Landscapes", "comments": "14 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions play a key role in understanding objects and scenes, for both\nvirtual and real world agents. We introduce a new general representation for\nproximal interactions among physical objects that is agnostic to the type of\nobjects or interaction involved. The representation is based on tracking\nparticles on one of the participating objects and then observing them with\nsensors appropriately placed in the interaction volume or on the interaction\nsurfaces. We show how to factorize these interaction descriptors and project\nthem into a particular participating object so as to obtain a new functional\ndescriptor for that object, its interaction landscape, capturing its observed\nuse in a spatio-temporal framework. Interaction landscapes are independent of\nthe particular interaction and capture subtle dynamic effects in how objects\nmove and behave when in functional use. Our method relates objects based on\ntheir function, establishes correspondences between shapes based on functional\nkey points and regions, and retrieves peer and partner objects with respect to\nan interaction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 22:00:56 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 20:13:56 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Pirk", "S\u00f6ren", ""], ["Krs", "Vojtech", ""], ["Hu", "Kaimo", ""], ["Rajasekaran", "Suren Deepak", ""], ["Kang", "Hao", ""], ["Benes", "Bedrich", ""], ["Yoshiyasu", "Yusuke", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1609.08740", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Jianmin Li, Jinma Guo, and Bo Zhang", "title": "Scalable Discrete Supervised Hash Learning with Asymmetric Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, and it has received a broad attention due to its low storage cost and\nfast retrieval speed. However, the existing limitations make the present\nalgorithms difficult to deal with large-scale datasets: (1) discrete\nconstraints are involved in the learning of the hash function; (2) pairwise or\ntriplet similarity is adopted to generate efficient hashcodes, resulting both\ntime and space complexity are greater than O(n^2). To address these issues, we\npropose a novel discrete supervised hash learning framework which can be\nscalable to large-scale datasets. First, the discrete learning procedure is\ndecomposed into a binary classifier learning scheme and binary codes learning\nscheme, which makes the learning procedure more efficient. Second, we adopt the\nAsymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based\nBatch Coordinate Descent method, such that the time and space complexity is\nreduced to O(n). The proposed framework also provides a flexible paradigm to\nincorporate with arbitrary hash function, including deep neural networks and\nkernel methods. Experiments on large-scale datasets demonstrate that the\nproposed method is superior or comparable with state-of-the-art hashing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 02:37:23 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Zhang", "Shifeng", ""], ["Li", "Jianmin", ""], ["Guo", "Jinma", ""], ["Zhang", "Bo", ""]]}, {"id": "1609.08758", "submitter": "Mayu Otani", "authors": "Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkil\\\"a, Naokazu\n  Yokoya", "title": "Video Summarization using Deep Semantic Features", "comments": "16 pages, the 13th Asian Conference on Computer Vision (ACCV'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a video summarization technique for an Internet video to\nprovide a quick way to overview its content. This is a challenging problem\nbecause finding important or informative parts of the original video requires\nto understand its content. Furthermore the content of Internet videos is very\ndiverse, ranging from home videos to documentaries, which makes video\nsummarization much more tough as prior knowledge is almost not available. To\ntackle this problem, we propose to use deep video features that can encode\nvarious levels of content semantics, including objects, actions, and scenes,\nimproving the efficiency of standard video summarization techniques. For this,\nwe design a deep neural network that maps videos as well as descriptions to a\ncommon semantic space and jointly trained it with associated pairs of videos\nand descriptions. To generate a video summary, we extract the deep features\nfrom each segment of the original video and apply a clustering-based\nsummarization technique to them. We evaluate our video summaries using the\nSumMe dataset as well as baseline approaches. The results demonstrated the\nadvantages of incorporating our deep semantic features in a video summarization\ntechnique.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 03:41:49 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""], ["Rahtu", "Esa", ""], ["Heikkil\u00e4", "Janne", ""], ["Yokoya", "Naokazu", ""]]}, {"id": "1609.08764", "submitter": "Sebastien Wong", "authors": "Sebastien C. Wong, Adam Gatt, Victor Stamatescu and Mark D. McDonnell", "title": "Understanding data augmentation for classification: when to warp?", "comments": "6 pages, 6 figures, DICTA 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the benefit of augmenting data with\nsynthetically created samples when training a machine learning classifier. Two\napproaches for creating additional training samples are data warping, which\ngenerates additional samples through transformations applied in the data-space,\nand synthetic over-sampling, which creates additional samples in feature-space.\nWe experimentally evaluate the benefits of data augmentation for a\nconvolutional backpropagation-trained neural network, a convolutional support\nvector machine and a convolutional extreme learning machine classifier, using\nthe standard MNIST handwritten digit dataset. We found that while it is\npossible to perform generic augmentation in feature-space, if plausible\ntransforms for the data are known then augmentation in data-space provides a\ngreater benefit for improving performance and reducing overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 04:37:32 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 11:08:19 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Wong", "Sebastien C.", ""], ["Gatt", "Adam", ""], ["Stamatescu", "Victor", ""], ["McDonnell", "Mark D.", ""]]}, {"id": "1609.08864", "submitter": "Mrutyunjaya Panda", "authors": "Mrutyunjaya Panda (Utkal University, Vani Vihar, Bhubaneswar, India)", "title": "Towards the effectiveness of Deep Convolutional Neural Network based\n  Fast Random Forest Classifier", "comments": "11 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is considered to be a quite young in the area of machine\nlearning research, found its effectiveness in dealing complex yet high\ndimensional dataset that includes but limited to images, text and speech etc.\nwith multiple levels of representation and abstraction. As there are a plethora\nof research on these datasets by various researchers , a win over them needs\nlots of attention. Careful setting of Deep learning parameters is of paramount\nimportance in order to avoid the overfitting unlike conventional methods with\nlimited parameter settings. Deep Convolutional neural network (DCNN) with\nmultiple layers of compositions and appropriate settings might be is an\nefficient machine learning method that can outperform the conventional methods\nin a great way. However, due to its slow adoption in learning, there are also\nalways a chance of overfitting during feature selection process, which can be\naddressed by employing a regularization method called dropout. Fast Random\nForest (FRF) is a powerful ensemble classifier especially when the datasets are\nnoisy and when the number of attributes is large in comparison to the number of\ninstances, as is the case of Bioinformatics datasets. Several publicly\navailable Bioinformatics dataset, Handwritten digits recognition and Image\nsegmentation dataset are considered for evaluation of the proposed approach.\nThe excellent performance obtained by the proposed DCNN based feature selection\nwith FRF classifier on high dimensional datasets makes it a fast and accurate\nclassifier in comparison the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 11:35:17 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Panda", "Mrutyunjaya", "", "Utkal University, Vani Vihar, Bhubaneswar, India"]]}, {"id": "1609.08938", "submitter": "Allison Del Giorno", "authors": "Allison Del Giorno, J. Andrew Bagnell, Martial Hebert", "title": "A Discriminative Framework for Anomaly Detection in Large Videos", "comments": "14 pages without references, 16 pages with. 7 figures. Accepted to\n  ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an anomaly detection setting in which training sequences are\nunavailable and anomalies are scored independently of temporal ordering.\nCurrent algorithms in anomaly detection are based on the classical density\nestimation approach of learning high-dimensional models and finding\nlow-probability events. These algorithms are sensitive to the order in which\nanomalies appear and require either training data or early context assumptions\nthat do not hold for longer, more complex videos. By defining anomalies as\nexamples that can be distinguished from other examples in the same video, our\ndefinition inspires a shift in approaches from classical density estimation to\nsimple discriminative learning. Our contributions include a novel framework for\nanomaly detection that is (1) independent of temporal ordering of anomalies,\nand (2) unsupervised, requiring no separate training sequences. We show that\nour algorithm can achieve state-of-the-art results even when we adjust the\nsetting by removing training sequences from standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 14:48:32 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Del Giorno", "Allison", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1609.08965", "submitter": "Michael Edwards", "authors": "Michael Edwards, Xianghua Xie", "title": "Graph Based Convolutional Neural Network", "comments": "11 pages, accepted into BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefit of localized features within the regular domain has given rise to\nthe use of Convolutional Neural Networks (CNNs) in machine learning, with great\nproficiency in the image classification. The use of CNNs becomes problematic\nwithin the irregular spatial domain due to design and convolution of a kernel\nfilter being non-trivial. One solution to this problem is to utilize graph\nsignal processing techniques and the convolution theorem to perform\nconvolutions on the graph of the irregular domain to obtain feature map\nresponses to learnt filters. We propose graph convolution and pooling operators\nanalogous to those in the regular domain. We also provide gradient calculations\non the input data and spectral filters, which allow for the deep learning of an\nirregular spatial domain problem. Signal filters take the form of spectral\nmultipliers, applying convolution in the graph spectral domain. Applying smooth\nmultipliers results in localized convolutions in the spatial domain, with\nsmoother multipliers providing sharper feature maps. Algebraic Multigrid is\npresented as a graph pooling method, reducing the resolution of the graph\nthrough agglomeration of nodes between layers of the network. Evaluation of\nperformance on the MNIST digit classification problem in both the regular and\nirregular domain is presented, with comparison drawn to standard CNN. The\nproposed graph CNN provides a deep learning method for the irregular domains\npresent in the machine learning community, obtaining 94.23% on the regular\ngrid, and 94.96% on a spatially irregular subsampled MNIST.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 15:32:59 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Edwards", "Michael", ""], ["Xie", "Xianghua", ""]]}, {"id": "1609.09018", "submitter": "Tobi Baumgartner", "authors": "Tobi Baumgartner and Jack Culpepper", "title": "Deep Architectures for Face Attributes", "comments": "11 pages, 2 figures, accepted in \"Workshop on Facial Informatics in\n  conjunction with ACCV '16\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a deep convolutional neural network to perform identity\nclassification using a new dataset of public figures annotated with age,\ngender, ethnicity and emotion labels, and then fine-tune it for attribute\nclassification. An optimal sharing pattern of computational resources within\nthis network is determined by experiment, requiring only 1 G flops to produce\nall predictions. Rather than fine-tune by relearning weights in one additional\nlayer after the penultimate layer of the identity network, we try several\ndifferent depths for each attribute. We find that prediction of age and emotion\nis improved by fine-tuning from earlier layers onward, presumably because\ndeeper layers are progressively invariant to non-identity related changes in\nthe input.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 17:57:46 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Baumgartner", "Tobi", ""], ["Culpepper", "Jack", ""]]}, {"id": "1609.09025", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto and Abhinav Gupta", "title": "Learning to Push by Grasping: Using multiple tasks for effective\n  learning", "comments": "Under review at the International Conference on Robotics and\n  Automation (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end learning frameworks are gaining prevalence in the field\nof robot control. These frameworks input states/images and directly predict the\ntorques or the action parameters. However, these approaches are often critiqued\ndue to their huge data requirements for learning a task. The argument of the\ndifficulty in scalability to multiple tasks is well founded, since training\nthese tasks often require hundreds or thousands of examples. But do end-to-end\napproaches need to learn a unique model for every task? Intuitively, it seems\nthat sharing across tasks should help since all tasks require some common\nunderstanding of the environment. In this paper, we attempt to take the next\nstep in data-driven end-to-end learning frameworks: move from the realm of\ntask-specific models to joint learning of multiple robot tasks. In an\nastonishing result we show that models with multi-task learning tend to perform\nbetter than task-specific models trained with same amounts of data. For\nexample, a deep-network learned with 2.5K grasp and 2.5K push examples performs\nbetter on grasping than a network trained on 5K grasp examples.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 18:13:02 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1609.09058", "submitter": "Ruiqi Zhao", "authors": "Ruiqi Zhao, Yan Wang and Aleix Martinez", "title": "A Simple, Fast and Highly-Accurate Algorithm to Recover 3D Shape from 2D\n  Landmarks on a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional shape reconstruction of 2D landmark points on a single\nimage is a hallmark of human vision, but is a task that has been proven\ndifficult for computer vision algorithms. We define a feed-forward deep neural\nnetwork algorithm that can reconstruct 3D shapes from 2D landmark points almost\nperfectly (i.e., with extremely small reconstruction errors), even when these\n2D landmarks are from a single image. Our experimental results show an\nimprovement of up to two-fold over state-of-the-art computer vision algorithms;\n3D shape reconstruction of human faces is given at a reconstruction error <\n.004, cars at .0022, human bodies at .022, and highly-deformable flags at an\nerror of .0004. Our algorithm was also a top performer at the 2016 3D Face\nAlignment in the Wild Challenge competition (done in conjunction with the\nEuropean Conference on Computer Vision, ECCV) that required the reconstruction\nof 3D face shape from a single image. The derived algorithm can be trained in a\ncouple hours and testing runs at more than 1, 000 frames/s on an i7 desktop. We\nalso present an innovative data augmentation approach that allows us to train\nthe system efficiently with small number of samples. And the system is robust\nto noise (e.g., imprecise landmark points) and missing data (e.g., occluded or\nundetected landmark points).\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 19:58:37 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Zhao", "Ruiqi", ""], ["Wang", "Yan", ""], ["Martinez", "Aleix", ""]]}, {"id": "1609.09143", "submitter": "Giovanni Montana", "authors": "Petros-Pavlos Ypsilantis, Giovanni Montana", "title": "Recurrent Convolutional Networks for Pulmonary Nodule Detection in CT\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) generates a stack of cross-sectional images covering\na region of the body. The visual assessment of these images for the\nidentification of potential abnormalities is a challenging and time consuming\ntask due to the large amount of information that needs to be processed. In this\narticle we propose a deep artificial neural network architecture, ReCTnet, for\nthe fully-automated detection of pulmonary nodules in CT scans. The\narchitecture learns to distinguish nodules and normal structures at the pixel\nlevel and generates three-dimensional probability maps highlighting areas that\nare likely to harbour the objects of interest. Convolutional and recurrent\nlayers are combined to learn expressive image representations exploiting the\nspatial dependencies across axial slices. We demonstrate that leveraging\nintra-slice dependencies substantially increases the sensitivity to detect\npulmonary nodules without inflating the false positive rate. On the publicly\navailable LIDC/IDRI dataset consisting of 1,018 annotated CT scans, ReCTnet\nreaches a detection sensitivity of 90.5% with an average of 4.5 false positives\nper scan. Comparisons with a competing multi-channel convolutional neural\nnetwork for multi-slice segmentation and other published methodologies using\nthe same dataset provide evidence that ReCTnet offers significant performance\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 22:32:24 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 12:30:14 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Ypsilantis", "Petros-Pavlos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1609.09156", "submitter": "Minyoung Kim", "authors": "Minyoung Kim, Stefano Alletto, Luca Rigazio", "title": "Similarity Mapping with Enhanced Siamese Network for Multi-Object\n  Tracking", "comments": "1) accepted as a poster presentation at WiML (Women in Machine\n  Learning) workshop 2016, colocated with NIPS 2016 in Barcelona, Spain, 2)\n  accepted as a poster presentation at MLITS (Machine Learning for Intelligent\n  Transportation Systems) Workshop held in conjunction with the NIPS 2016 in\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking has recently become an important area of computer\nvision, especially for Advanced Driver Assistance Systems (ADAS). Despite\ngrowing attention, achieving high performance tracking is still challenging,\nwith state-of-the- art systems resulting in high complexity with a large number\nof hyper parameters. In this paper, we focus on reducing overall system\ncomplexity and the number hyper parameters that need to be tuned to a specific\nenvironment. We introduce a novel tracking system based on similarity mapping\nby Enhanced Siamese Neural Network (ESNN), which accounts for both appearance\nand geometric information, and is trainable end-to-end. Our system achieves\ncompetitive performance in both speed and accuracy on MOT16 challenge, compared\nto known state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 23:52:50 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 00:51:57 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Kim", "Minyoung", ""], ["Alletto", "Stefano", ""], ["Rigazio", "Luca", ""]]}, {"id": "1609.09178", "submitter": "Wenbin Li", "authors": "Wenbin Li, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, Yinghuan Shi", "title": "OPML: A One-Pass Closed-Form Solution for Online Metric Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve a low computational cost when performing online metric learning\nfor large-scale data, we present a one-pass closed-form solution namely OPML in\nthis paper. Typically, the proposed OPML first adopts a one-pass triplet\nconstruction strategy, which aims to use only a very small number of triplets\nto approximate the representation ability of whole original triplets obtained\nby batch-manner methods. Then, OPML employs a closed-form solution to update\nthe metric for new coming samples, which leads to a low space (i.e., $O(d)$)\nand time (i.e., $O(d^2)$) complexity, where $d$ is the feature dimensionality.\nIn addition, an extension of OPML (namely COPML) is further proposed to enhance\nthe robustness when in real case the first several samples come from the same\nclass (i.e., cold start problem). In the experiments, we have systematically\nevaluated our methods (OPML and COPML) on three typical tasks, including UCI\ndata classification, face verification, and abnormal event detection in videos,\nwhich aims to fully evaluate the proposed methods on different sample number,\ndifferent feature dimensionalities and different feature extraction ways (i.e.,\nhand-crafted and deeply-learned). The results show that OPML and COPML can\nobtain the promising performance with a very low computational cost. Also, the\neffectiveness of COPML under the cold start setting is experimentally verified.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 02:18:06 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Li", "Wenbin", ""], ["Gao", "Yang", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""]]}, {"id": "1609.09199", "submitter": "Yael Yankelevsky", "authors": "Yael Yankelevsky and Michael Elad", "title": "Structure-Aware Classification using Supervised Dictionary Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952992", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a supervised dictionary learning algorithm that\naims to preserve the local geometry in both dimensions of the data. A\ngraph-based regularization explicitly takes into account the local manifold\nstructure of the data points. A second graph regularization gives similar\ntreatment to the feature domain and helps in learning a more robust dictionary.\nBoth graphs can be constructed from the training data or learned and adapted\nalong the dictionary learning process. The combination of these two terms\npromotes the discriminative power of the learned sparse representations and\nleads to improved classification accuracy. The proposed method was evaluated on\nseveral different datasets, representing both single-label and multi-label\nclassification problems, and demonstrated better performance compared with\nother dictionary based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 04:30:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yankelevsky", "Yael", ""], ["Elad", "Michael", ""]]}, {"id": "1609.09220", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Hossein Mousavi, Moin Nabi, Mohammad Rastegari,\n  Carlo Regazzoni", "title": "CNN-aware Binary Map for General Semantic Segmentation", "comments": "ICIP 2016 Best Paper / Student Paper Finalist", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel method for general semantic segmentation\nthat can benefit from general semantics of Convolutional Neural Network (CNN).\nOur segmentation proposes visually and semantically coherent image segments. We\nuse binary encoding of CNN features to overcome the difficulty of the\nclustering on the high-dimensional CNN feature space. These binary codes are\nvery robust against noise and non-semantic changes in the image. These binary\nencoding can be embedded into the CNN as an extra layer at the end of the\nnetwork. This results in real-time segmentation. To the best of our knowledge\nour method is the first attempt on general semantic image segmentation using\nCNN. All the previous papers were limited to few number of category of the\nimages (e.g. PASCAL VOC). Experiments show that our segmentation algorithm\noutperform the state-of-the-art non-semantic segmentation methods by large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 06:46:27 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Mousavi", "Hossein", ""], ["Nabi", "Moin", ""], ["Rastegari", "Mohammad", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1609.09227", "submitter": "Srinivasa Chakravarthy", "authors": "Manali Naik, V. Srinivasa Chakravarthy", "title": "A comparative study of complexity of handwritten Bharati characters with\n  that of major Indian scripts", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Bharati, a simple, novel script that can represent the characters\nof a majority of contemporary Indian scripts. The shapes/motifs of Bharati\ncharacters are drawn from some of the simplest characters of existing Indian\nscripts. Bharati characters are designed such that they strictly reflect the\nunderlying phonetic organization, thereby attributing to the script qualities\nof simplicity, familiarity, ease of acquisition and use. Thus, employing\nBharati script as a common script for a majority of Indian languages can\nameliorate several existing communication bottlenecks in India. We perform a\ncomplexity analysis of handwritten Bharati script and compare its complexity\nwith that of 9 major Indian scripts. The measures of complexity are derived\nfrom a theory of handwritten characters based on Catastrophe theory. Bharati\nscript is shown to be simpler than the 9 major Indian scripts in most measures\nof complexity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 07:06:46 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Naik", "Manali", ""], ["Chakravarthy", "V. Srinivasa", ""]]}, {"id": "1609.09240", "submitter": "Biel Moy\\`a", "authors": "Gabriel Moy\\`a-Alcover, Ahmed Elgammal, Antoni Jaume-i-Cap\\'o and\n  Javier Varona", "title": "Modelling depth for nonparametric foreground segmentation using RGBD\n  devices", "comments": "Accepted in Pattern Recognition Letters. Will update the info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting changes in a scene and segmenting the foreground\nfrom background is still challenging, despite previous work. Moreover, new RGBD\ncapturing devices include depth cues, which could be incorporated to improve\nforeground segmentation. In this work, we present a new nonparametric approach\nwhere a unified model mixes the device multiple information cues. In order to\nunify all the device channel cues, a new probabilistic depth data model is also\nproposed where we show how handle the inaccurate data to improve foreground\nsegmentation. A new RGBD video dataset is presented in order to introduce a new\nstandard for comparison purposes of this kind of algorithms. Results show that\nthe proposed approach can handle several practical situations and obtain good\nresults in all cases.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 07:54:31 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Moy\u00e0-Alcover", "Gabriel", ""], ["Elgammal", "Ahmed", ""], ["Jaume-i-Cap\u00f3", "Antoni", ""], ["Varona", "Javier", ""]]}, {"id": "1609.09251", "submitter": "Minh Ha Quang", "authors": "H\\`a Quang Minh, Marco San Biagio, Loris Bazzani, and Vittorio Murino", "title": "Kernel Methods on Approximate Infinite-Dimensional Covariance Operators\n  for Image Classification", "comments": "18 double-column pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for visual object recognition using\ninfinite-dimensional covariance operators of input features in the paradigm of\nkernel methods on infinite-dimensional Riemannian manifolds. Our formulation\nprovides in particular a rich representation of image features by exploiting\ntheir non-linear correlations. Theoretically, we provide a finite-dimensional\napproximation of the Log-Hilbert-Schmidt (Log-HS) distance between covariance\noperators that is scalable to large datasets, while maintaining an effective\ndiscriminating capability. This allows us to efficiently approximate any\ncontinuous shift-invariant kernel defined using the Log-HS distance. At the\nsame time, we prove that the Log-HS inner product between covariance operators\nis only approximable by its finite-dimensional counterpart in a very limited\nscenario. Consequently, kernels defined using the Log-HS inner product, such as\npolynomial kernels, are not scalable in the same way as shift-invariant\nkernels. Computationally, we apply the approximate Log-HS distance formulation\nto covariance operators of both handcrafted and convolutional features,\nexploiting both the expressiveness of these features and the power of the\ncovariance representation. Empirically, we tested our framework on the task of\nimage classification on twelve challenging datasets. In almost all cases, the\nresults obtained outperform other state of the art methods, demonstrating the\ncompetitiveness and potential of our framework.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 08:26:28 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Minh", "H\u00e0 Quang", ""], ["Biagio", "Marco San", ""], ["Bazzani", "Loris", ""], ["Murino", "Vittorio", ""]]}, {"id": "1609.09267", "submitter": "Andrea Romanoni", "authors": "Gheorghii Postica and Andrea Romanoni and Matteo Matteucci", "title": "Robust Moving Objects Detection in Lidar Data Exploiting Visual Cues", "comments": "6 pages, to appear in IROS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting moving objects in dynamic scenes from sequences of lidar scans is\nan important task in object tracking, mapping, localization, and navigation.\nMany works focus on changes detection in previously observed scenes, while a\nvery limited amount of literature addresses moving objects detection. The\nstate-of-the-art method exploits Dempster-Shafer Theory to evaluate the\noccupancy of a lidar scan and to discriminate points belonging to the static\nscene from moving ones. In this paper we improve both speed and accuracy of\nthis method by discretizing the occupancy representation, and by removing false\npositives through visual cues. Many false positives lying on the ground plane\nare also removed thanks to a novel ground plane removal algorithm. Efficiency\nis improved through an octree indexing strategy. Experimental evaluation\nagainst the KITTI public dataset shows the effectiveness of our approach, both\nqualitatively and quantitatively with respect to the state- of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 09:29:46 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Postica", "Gheorghii", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1609.09270", "submitter": "Bjorn Stenger", "authors": "Jiu Xu, Bjorn Stenger, Tommi Kerola, Tony Tung", "title": "Pano2CAD: Room Layout From A Single Panorama Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of estimating the geometry of a room and the 3D\npose of objects from a single 360-degree panorama image. Assuming Manhattan\nWorld geometry, we formulate the task as a Bayesian inference problem in which\nwe estimate positions and orientations of walls and objects. The method\ncombines surface normal estimation, 2D object detection and 3D object pose\nestimation. Quantitative results are presented on a dataset of synthetically\ngenerated 3D rooms containing objects, as well as on a subset of hand-labeled\nimages from the public SUN360 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 09:35:29 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 08:33:25 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Xu", "Jiu", ""], ["Stenger", "Bjorn", ""], ["Kerola", "Tommi", ""], ["Tung", "Tony", ""]]}, {"id": "1609.09296", "submitter": "Alejandro Linares-Barranco A. Linares-Barranco", "authors": "R. Tapiador, A. Rios-Navarro, A. Linares-Barranco, Minkyu Kim, Deepak\n  Kadetotad, Jae-sun Seo", "title": "Comprehensive Evaluation of OpenCL-based Convolutional Neural Network\n  Accelerators in Xilinx and Altera FPGAs", "comments": "6 pages, 6 figures. Robotic and Technology of Computers Lab report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has significantly advanced the state of the art in artificial\nintelligence, gaining wide popularity from both industry and academia. Special\ninterest is around Convolutional Neural Networks (CNN), which take inspiration\nfrom the hierarchical structure of the visual cortex, to form deep layers of\nconvolutional operations, along with fully connected classifiers. Hardware\nimplementations of these deep CNN architectures are challenged with memory\nbottlenecks that require many convolution and fully-connected layers demanding\nlarge amount of communication for parallel computation. Multi-core CPU based\nsolutions have demonstrated their inadequacy for this problem due to the memory\nwall and low parallelism. Many-core GPU architectures show superior performance\nbut they consume high power and also have memory constraints due to\ninconsistencies between cache and main memory. FPGA design solutions are also\nactively being explored, which allow implementing the memory hierarchy using\nembedded BlockRAM. This boosts the parallel use of shared memory elements\nbetween multiple processing units, avoiding data replicability and\ninconsistencies. This makes FPGAs potentially powerful solutions for real-time\nclassification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design\nframework from GPU for FPGA designs as a pseudo-automatic development solution.\nIn this paper, a comprehensive evaluation and comparison of Altera and Xilinx\nOpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,\ntemporal performance and the OpenCL architecture for CNNs are discussed. Xilinx\ndemonstrates faster synthesis, better FPGA resource utilization and more\ncompact boards. Altera provides multi-platforms tools, mature design community\nand better execution times.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 11:03:21 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Tapiador", "R.", ""], ["Rios-Navarro", "A.", ""], ["Linares-Barranco", "A.", ""], ["Kim", "Minkyu", ""], ["Kadetotad", "Deepak", ""], ["Seo", "Jae-sun", ""]]}, {"id": "1609.09365", "submitter": "Julie Dequaire", "authors": "Julie Dequaire, Dushyant Rao, Peter Ondruska, Dominic Wang and Ingmar\n  Posner", "title": "Deep Tracking on the Move: Learning to Track the World from a Moving\n  Vehicle using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end approach for tracking static and dynamic\nobjects for an autonomous vehicle driving through crowded urban environments.\nUnlike traditional approaches to tracking, this method is learned end-to-end,\nand is able to directly predict a full unoccluded occupancy grid map from raw\nlaser input data. Inspired by the recently presented DeepTracking approach\n[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the\ntemporal evolution of the state of the environment, and propose to use Spatial\nTransformer modules to exploit estimates of the egomotion of the vehicle. Our\nresults demonstrate the ability to track a range of objects, including cars,\nbuses, pedestrians, and cyclists through occlusion, from both moving and\nstationary platforms, using a single learned model. Experimental results\ndemonstrate that the model can also predict the future states of objects from\ncurrent inputs, with greater accuracy than previous work.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 14:39:10 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 21:35:15 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 14:31:32 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Dequaire", "Julie", ""], ["Rao", "Dushyant", ""], ["Ondruska", "Peter", ""], ["Wang", "Dominic", ""], ["Posner", "Ingmar", ""]]}, {"id": "1609.09408", "submitter": "Yang Lu", "authors": "Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu and Ying Nian Wu", "title": "Cooperative Training of Descriptor and Generator Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the cooperative training of two generative models for\nimage modeling and synthesis. Both models are parametrized by convolutional\nneural networks (ConvNets). The first model is a deep energy-based model, whose\nenergy function is defined by a bottom-up ConvNet, which maps the observed\nimage to the energy. We call it the descriptor network. The second model is a\ngenerator network, which is a non-linear version of factor analysis. It is\ndefined by a top-down ConvNet, which maps the latent factors to the observed\nimage. The maximum likelihood learning algorithms of both models involve MCMC\nsampling such as Langevin dynamics. We observe that the two learning algorithms\ncan be seamlessly interwoven into a cooperative learning algorithm that can\ntrain both models simultaneously. Specifically, within each iteration of the\ncooperative learning algorithm, the generator model generates initial\nsynthesized examples to initialize a finite-step MCMC that samples and trains\nthe energy-based descriptor model. After that, the generator model learns from\nhow the MCMC changes its synthesized examples. That is, the descriptor model\nteaches the generator model by MCMC, so that the generator model accumulates\nthe MCMC transitions and reproduces them by direct ancestral sampling. We call\nthis scheme MCMC teaching. We show that the cooperative algorithm can learn\nhighly realistic generative models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 16:14:45 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 17:32:46 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 16:42:24 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Xie", "Jianwen", ""], ["Lu", "Yang", ""], ["Gao", "Ruiqi", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1609.09432", "submitter": "Hejia Zhang", "authors": "Hejia Zhang, Po-Hsuan Chen, Janice Chen, Xia Zhu, Javier S. Turek,\n  Theodore L. Willke, Uri Hasson, Peter J. Ramadge", "title": "A Searchlight Factor Model Approach for Locating Shared Information in\n  Multi-Subject fMRI Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in joint multi-subject fMRI analysis. The\nchallenge of such analysis comes from inherent anatomical and functional\nvariability across subjects. One approach to resolving this is a shared\nresponse factor model. This assumes a shared and time synchronized stimulus\nacross subjects. Such a model can often identify shared information, but it may\nnot be able to pinpoint with high resolution the spatial location of this\ninformation. In this work, we examine a searchlight based shared response model\nto identify shared information in small contiguous regions (searchlights)\nacross the whole brain. Validation using classification tasks demonstrates that\nwe can pinpoint informative local regions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:20:23 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Zhang", "Hejia", ""], ["Chen", "Po-Hsuan", ""], ["Chen", "Janice", ""], ["Zhu", "Xia", ""], ["Turek", "Javier S.", ""], ["Willke", "Theodore L.", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1609.09444", "submitter": "Arnab Ghosh", "authors": "Arnab Ghosh and Viveka Kulharia and Amitabha Mukerjee and Vinay\n  Namboodiri and Mohit Bansal", "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation", "comments": "To Appear in AAAI-17 and NIPS Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding, predicting, and generating object motions and transformations\nis a core problem in artificial intelligence. Modeling sequences of evolving\nimages may provide better representations and models of motion and may\nultimately be used for forecasting, simulation, or video generation.\nDiagrammatic Abstract Reasoning is an avenue in which diagrams evolve in\ncomplex patterns and one needs to infer the underlying pattern sequence and\ngenerate the next image in the sequence. For this, we develop a novel\nContextual Generative Adversarial Network based on Recurrent Neural Networks\n(Context-RNN-GANs), where both the generator and the discriminator modules are\nbased on contextual history (modeled as RNNs) and the adversarial discriminator\nguides the generator to produce realistic images for the particular time step\nin the image sequence. We evaluate the Context-RNN-GAN model (and its variants)\non a novel dataset of Diagrammatic Abstract Reasoning, where it performs\ncompetitively with 10th-grade human performance but there is still scope for\ninteresting improvements as compared to college-grade human performance. We\nalso evaluate our model on a standard video next-frame prediction task,\nachieving improved performance over comparable state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:56:32 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 13:14:09 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Mukerjee", "Amitabha", ""], ["Namboodiri", "Vinay", ""], ["Bansal", "Mohit", ""]]}, {"id": "1609.09451", "submitter": "Anguelos Nicolaou", "authors": "Anguelos Nicolaou, Liwicki Marcus", "title": "Redefining Binarization and the Visual Archetype", "comments": "Short paper presented at the 12th IEEE workshop on Document Analysis\n  Systems (DAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although binarization is considered passe, it still remains a highly popular\nresearch topic. In this paper we propose a rethinking of what binarization is.\nWe introduce the notion of the visual archetype as the ideal form of any one\ndocument. Binarization can be defined as the restoration of the visual\narchetype for a class of images. This definition broadens the scope of what\nbinarization means but also suggests ground-truth should focus on the\nforeground.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 18:15:16 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Nicolaou", "Anguelos", ""], ["Marcus", "Liwicki", ""]]}, {"id": "1609.09468", "submitter": "Madhava Krishna K", "authors": "J. Krishna Murthy, G.V. Sai Krishna, Falak Chhaya and K. Madhava\n  Krishna", "title": "Reconstructing Vechicles from a Single Image: Shape Priors for Road\n  Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for reconstructing vehicles from a single (RGB) image,\nin the context of autonomous driving. Though the problem appears to be\nill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles\nproject to an image can be used to reason about the reverse process, i.e., how\nshapes (back-)project from 2D to 3D. We encode this knowledge in \\emph{shape\npriors}, which are learnt over a small keypoint-annotated dataset. We then\nformulate a shape-aware adjustment problem that uses the learnt shape priors to\nrecover the 3D pose and shape of a query object from an image. For shape\nrepresentation and inference, we leverage recent successes of Convolutional\nNeural Networks (CNNs) for the task of object and keypoint localization, and\ntrain a novel cascaded fully-convolutional architecture to localize vehicle\n\\emph{keypoints} in images. The shape-aware adjustment then robustly recovers\nshape (3D locations of the detected keypoints) while simultaneously filling in\noccluded keypoints. To tackle estimation errors incurred due to erroneously\ndetected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS)\nscheme for robust optimization, and as a by-product characterize noise models\nfor each predicted keypoint. We evaluate our approach on autonomous driving\nbenchmarks, and present superior results to existing monocular, as well as\nstereo approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:17:38 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Murthy", "J. Krishna", ""], ["Krishna", "G. V. Sai", ""], ["Chhaya", "Falak", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1609.09475", "submitter": "Andy Zeng", "authors": "Andy Zeng, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker Jr.,\n  Alberto Rodriguez and Jianxiong Xiao", "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the\n  Amazon Picking Challenge", "comments": "To appear at the International Conference on Robotics and Automation\n  (ICRA) 2017. Project webpage: http://apc.cs.princeton.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot warehouse automation has attracted significant interest in recent\nyears, perhaps most visibly in the Amazon Picking Challenge (APC). A fully\nautonomous warehouse pick-and-place system requires robust vision that reliably\nrecognizes and locates objects amid cluttered environments, self-occlusions,\nsensor noise, and a large variety of objects. In this paper we present an\napproach that leverages multi-view RGB-D data and self-supervised, data-driven\nlearning to overcome those difficulties. The approach was part of the\nMIT-Princeton Team system that took 3rd- and 4th- place in the stowing and\npicking tasks, respectively at APC 2016. In the proposed approach, we segment\nand label multiple views of a scene with a fully convolutional neural network,\nand then fit pre-scanned 3D object models to the resulting segmentation to get\nthe 6D object pose. Training a deep neural network for segmentation typically\nrequires a large amount of training data. We propose a self-supervised method\nto generate a large labeled dataset without tedious manual segmentation. We\ndemonstrate that our system can reliably estimate the 6D pose of objects under\na variety of scenarios. All code, data, and benchmarks are available at\nhttp://apc.cs.princeton.edu/\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:39:13 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 00:24:29 GMT"}, {"version": "v3", "created": "Sun, 7 May 2017 20:12:55 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Zeng", "Andy", ""], ["Yu", "Kuan-Ting", ""], ["Song", "Shuran", ""], ["Suo", "Daniel", ""], ["Walker", "Ed", "Jr."], ["Rodriguez", "Alberto", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1609.09525", "submitter": "Yoann Isaac", "authors": "Yoann Isaac, Quentin Barth\\'elemy, C\\'edric Gouy-Pailler, Mich\\`ele\n  Sebag, Jamal Atif", "title": "Multi-dimensional signal approximation with sparse structured priors\n  using split Bregman iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the structurally-constrained sparse decomposition of\nmulti-dimensional signals onto overcomplete families of vectors, called\ndictionaries. The contribution of the paper is threefold. Firstly, a generic\nspatio-temporal regularization term is designed and used together with the\nstandard $\\ell_1$ regularization term to enforce a sparse decomposition\npreserving the spatio-temporal structure of the signal. Secondly, an\noptimization algorithm based on the split Bregman approach is proposed to\nhandle the associated optimization problem, and its convergence is analyzed.\nOur well-founded approach yields same accuracy as the other algorithms at the\nstate-of-the-art, with significant gains in terms of convergence speed.\nThirdly, the empirical validation of the approach on artificial and real-world\nproblems demonstrates the generality and effectiveness of the method. On\nartificial problems, the proposed regularization subsumes the Total Variation\nminimization and recovers the expected decomposition. On the real-world problem\nof electro-encephalography brainwave decomposition, the approach outperforms\nsimilar approaches in terms of P300 evoked potentials detection, using\nstructured spatial priors to guide the decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 20:50:16 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Isaac", "Yoann", ""], ["Barth\u00e9lemy", "Quentin", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Sebag", "Mich\u00e8le", ""], ["Atif", "Jamal", ""]]}, {"id": "1609.09545", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face\n  Alignment in the Wild (3DFAW) Challenge", "comments": "Winner of 3D Face Alignment in the Wild (3DFAW) Challenge, ECCV 2016", "journal-ref": null, "doi": "10.1007/978-3-319-48881-3_43", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to the 1st 3D Face Alignment in the Wild\n(3DFAW) Challenge. Our method builds upon the idea of convolutional part\nheatmap regression [1], extending it for 3D face alignment. Our method\ndecomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z\n(depth) estimation. At the first stage, our method estimates the X,Y\ncoordinates of the facial landmarks by producing a set of 2D heatmaps, one for\neach landmark, using convolutional part heatmap regression. Then, these\nheatmaps, alongside the input RGB image, are used as input to a very deep\nsubnetwork trained via residual learning for regressing the Z coordinate. Our\nmethod ranked 1st in the 3DFAW Challenge, surpassing the second best result by\nmore than 22%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 23:07:01 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1609.09582", "submitter": "Varun Adibhatla", "authors": "Varun Adibhatla (ARGO Labs), Shi Fan (NYU Center for Data Science),\n  Krystof Litomisky (ARGO Labs), Patrick Atwater (ARGO Labs)", "title": "Digitizing Municipal Street Inspections Using Computer Vision", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"People want an authority to tell them how to value things. But they chose\nthis authority not based on facts or results. They chose it because it seems\nauthoritative and familiar.\" - The Big Short\n  The pavement condition index is one such a familiar measure used by many US\ncities to measure street quality and justify billions of dollars spent every\nyear on street repair. These billion-dollar decisions are based on evaluation\ncriteria that are subjective and not representative. In this paper, we build\nupon our initial submission to D4GX 2015 that approaches this problem of\ninformation asymmetry in municipal decision-making.\n  We describe a process to identify street-defects using computer vision\ntechniques on data collected using the Street Quality Identification Device\n(SQUID). A User Interface to host a large quantity of image data towards\ndigitizing the street inspection process and enabling actionable intelligence\nfor a core public service is also described. This approach of combining device,\ndata and decision-making around street repair enables cities make targeted\ndecisions about street repair and could lead to an anticipatory response which\ncan result in significant cost savings. Lastly, we share lessons learnt from\nthe deployment of SQUID in the city of Syracuse, NY.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 03:36:03 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Adibhatla", "Varun", "", "ARGO Labs"], ["Fan", "Shi", "", "NYU Center for Data Science"], ["Litomisky", "Krystof", "", "ARGO Labs"], ["Atwater", "Patrick", "", "ARGO Labs"]]}, {"id": "1609.09642", "submitter": "Aaron Jackson", "authors": "Aaron Jackson, Michel Valstar, Georgios Tzimiropoulos", "title": "A CNN Cascade for Landmark Guided Semantic Part Segmentation", "comments": "accepted to Geometry Meets Deep Learning ECCV 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a CNN cascade for semantic part segmentation guided by\npose-specific information encoded in terms of a set of landmarks (or\nkeypoints). There is large amount of prior work on each of these tasks\nseparately, yet, to the best of our knowledge, this is the first time in\nliterature that the interplay between pose estimation and semantic part\nsegmentation is investigated. To address this limitation of prior work, in this\npaper, we propose a CNN cascade of tasks that firstly performs landmark\nlocalisation and then uses this information as input for guiding semantic part\nsegmentation. We applied our architecture to the problem of facial part\nsegmentation and report large performance improvement over the standard\nunguided network on the most challenging face datasets. Testing code and models\nwill be published online at http://cs.nott.ac.uk/~psxasj/.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 09:11:13 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Jackson", "Aaron", ""], ["Valstar", "Michel", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1609.09671", "submitter": "Roberto DiCecco", "authors": "Roberto DiCecco, Griffin Lacey, Jasmina Vasiljevic, Paul Chow, Graham\n  Taylor and Shawki Areibi", "title": "Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have gained significant traction in the\nfield of machine learning, particularly due to their high accuracy in visual\nrecognition. Recent works have pushed the performance of GPU implementations of\nCNNs to significantly improve their classification and training times. With\nthese improvements, many frameworks have become available for implementing CNNs\non both CPUs and GPUs, with no support for FPGA implementations. In this work\nwe present a modified version of the popular CNN framework Caffe, with FPGA\nsupport. This allows for classification using CNN models and specialized FPGA\nimplementations with the flexibility of reprogramming the device when\nnecessary, seamless memory transactions between host and device, simple-to-use\ntest benches, and the ability to create pipelined layer implementations. To\nvalidate the framework, we use the Xilinx SDAccel environment to implement an\nFPGA-based Winograd convolution engine and show that the FPGA layer can be used\nalongside other layers running on a host processor to run several popular CNNs\n(AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework\nachieves 50 GFLOPS across 3x3 convolutions in the benchmarks. This is achieved\nwithin a practical framework, which will aid in future development of\nFPGA-based CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 11:13:21 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["DiCecco", "Roberto", ""], ["Lacey", "Griffin", ""], ["Vasiljevic", "Jasmina", ""], ["Chow", "Paul", ""], ["Taylor", "Graham", ""], ["Areibi", "Shawki", ""]]}, {"id": "1609.09698", "submitter": "Markus Oberweger", "authors": "Markus Oberweger, Paul Wohlhart, Vincent Lepetit", "title": "Training a Feedback Loop for Hand Pose Estimation", "comments": "Presented at ICCV 2015 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an entirely data-driven approach to estimating the 3D pose of a\nhand given a depth image. We show that we can correct the mistakes made by a\nConvolutional Neural Network trained to predict an estimate of the 3D pose by\nusing a feedback loop. The components of this feedback loop are also Deep\nNetworks, optimized using training data. They remove the need for fitting a 3D\nmodel to the input data, which requires both a carefully designed fitting\nfunction and algorithm. We show that our approach outperforms state-of-the-art\nmethods, and is efficient as our implementation runs at over 400 fps on a\nsingle GPU.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 12:35:26 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Oberweger", "Markus", ""], ["Wohlhart", "Paul", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1609.09713", "submitter": "Fabio Maria Carlucci", "authors": "Fabio Maria Carlucci, Paolo Russo and Barbara Caputo", "title": "A deep representation for depth images from synthetic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) trained on large scale RGB databases\nhave become the secret sauce in the majority of recent approaches for object\ncategorization from RGB-D data. Thanks to colorization techniques, these\nmethods exploit the filters learned from 2D images to extract meaningful\nrepresentations in 2.5D. Still, the perceptual signature of these two kind of\nimages is very different, with the first usually strongly characterized by\ntextures, and the second mostly by silhouettes of objects. Ideally, one would\nlike to have two CNNs, one for RGB and one for depth, each trained on a\nsuitable data collection, able to capture the perceptual properties of each\nchannel for the task at hand. This has not been possible so far, due to the\nlack of a suitable depth database. This paper addresses this issue, proposing\nto opt for synthetically generated images rather than collecting by hand a 2.5D\nlarge scale database. While being clearly a proxy for real data, synthetic\nimages allow to trade quality for quantity, making it possible to generate a\nvirtually infinite amount of data. We show that the filters learned from such\ndata collection, using the very same architecture typically used on visual\ndata, learns very different filters, resulting in depth features (a) able to\nbetter characterize the different facets of depth images, and (b) complementary\nwith respect to those derived from CNNs pre-trained on 2D datasets. Experiments\non two publicly available databases show the power of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 13:10:20 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Carlucci", "Fabio Maria", ""], ["Russo", "Paolo", ""], ["Caputo", "Barbara", ""]]}, {"id": "1609.09850", "submitter": "Yao Tang", "authors": "Yao Tang, Fei Gao, Jufu Feng", "title": "Latent fingerprint minutia extraction using fully convolutional network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minutiae play a major role in fingerprint identification. Extracting reliable\nminutiae is difficult for latent fingerprints which are usually of poor\nquality. As the limitation of traditional handcrafted features, a fully\nconvolutional network (FCN) is utilized to learn features directly from data to\novercome complex background noises. Raw fingerprints are mapped to a\ncorrespondingly-sized minutia-score map with a fixed stride. And thus a large\nnumber of minutiae will be extracted through a given threshold. Then small\nregions centering at these minutia points are entered into a convolutional\nneural network (CNN) to reclassify these minutiae and calculate their\norientations. The CNN shares convolutional layers with the fully convolutional\nnetwork to speed up. 0.45 second is used on average to detect one fingerprint\non a GPU. On the NIST SD27 database, we achieve 53\\% recall rate and 53\\%\nprecise rate that outperform many other algorithms. Our trained model is also\nvisualized to show that we have successfully extracted features preserving\nridge information of a latent fingerprint.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 18:29:41 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 12:15:54 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Tang", "Yao", ""], ["Gao", "Fei", ""], ["Feng", "Jufu", ""]]}]