[{"id": "1410.0095", "submitter": "Xu Wang", "authors": "Xu Wang, Konstantinos Slavakis, Gilad Lerman", "title": "Riemannian Multi-Manifold Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates a novel framework for segmenting a dataset in a\nRiemannian manifold $M$ into clusters lying around low-dimensional submanifolds\nof $M$. Important examples of $M$, for which the proposed clustering algorithm\nis computationally efficient, are the sphere, the set of positive definite\nmatrices, and the Grassmannian. The clustering problem with these examples of\n$M$ is already useful for numerous application domains such as action\nidentification in video sequences, dynamic texture clustering, brain fiber\nsegmentation in medical imaging, and clustering of deformed images. The\nproposed clustering algorithm constructs a data-affinity matrix by thoroughly\nexploiting the intrinsic geometry and then applies spectral clustering. The\nintrinsic local geometry is encoded by local sparse coding and more importantly\nby directional information of local tangent spaces and geodesics. Theoretical\nguarantees are established for a simplified variant of the algorithm even when\nthe clusters intersect. To avoid complication, these guarantees assume that the\nunderlying submanifolds are geodesic. Extensive validation on synthetic and\nreal data demonstrates the resiliency of the proposed method against deviations\nfrom the theoretical model as well as its superior performance over\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 02:37:12 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Wang", "Xu", ""], ["Slavakis", "Konstantinos", ""], ["Lerman", "Gilad", ""]]}, {"id": "1410.0117", "submitter": "Atul Kanaujia", "authors": "Atul Kanaujia", "title": "Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape\n  Estimation from Monocular Image Sequences", "comments": "13 Pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused\non acquiring behavioral information of the targets and their activities.\nContinuous evolution of intelligence being gathered of the human centric\nactivities has put increased focus on the humans, especially inferring their\ninnate characteristics - size, shapes and physiology. These bio-signatures\nextracted from the surveillance sensors can be used to deduce age, ethnicity,\ngender and actions, and further characterize human actions in unseen scenarios.\nHowever, recovery of pose and shape of humans in such monocular videos is\ninherently an ill-posed problem, marked by frequent depth and view based\nambiguities due to self-occlusion, foreshortening and misalignment. The\nlikelihood function often yields a highly multimodal posterior that is\ndifficult to propagate even using the most advanced particle filtering(PF)\nalgorithms. Motivated by the recent success of the discriminative approaches to\nefficiently predict 3D poses directly from the 2D images, we present several\nprincipled approaches to integrate predictive cues using learned regression\nmodels to sustain multimodality of the posterior during tracking. Additionally,\nthese learned priors can be actively adapted to the test data using a\nlikelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D\nhuman shape model to each frame independently for inferring anthropometric\nbio-signatures. The proposed system is fully automated, robust to noisy test\ndata and has ability to swiftly recover from tracking failures even after\nconfronting with significant errors. We evaluate the system on a large number\nof monocular human motion sequences.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 06:19:05 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 05:04:16 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Kanaujia", "Atul", ""]]}, {"id": "1410.0210", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "A Multi-World Approach to Question Answering about Real-World Scenes\n  based on Uncertain Input", "comments": "Published in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for automatically answering questions about images by\nbringing together recent advances from natural language processing and computer\nvision. We combine discrete reasoning with uncertain predictions by a\nmulti-world approach that represents uncertainty about the perceived world in a\nbayesian framework. Our approach can handle human questions of high complexity\nabout realistic scenes and replies with range of answer like counts, object\nclasses, instances and lists of them. The system is directly trained from\nquestion-answer pairs. We establish a first benchmark for this task that can be\nseen as a modern attempt at a visual turing test.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 12:59:16 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 16:29:44 GMT"}, {"version": "v3", "created": "Tue, 11 Nov 2014 12:13:18 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 17:39:10 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1410.0226", "submitter": "Xiaohao Cai", "authors": "Juheon Lee, Xiaohao Cai, Carola-Bibiane Schonlieb, David Coomes", "title": "Non-parametric Image Registration of Airborne LiDAR, Hyperspectral and\n  Photographic Imagery of Forests", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TGRS.2015.2431692", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much current interest in using multi-sensor airborne remote sensing\nto monitor the structure and biodiversity of forests. This paper addresses the\napplication of non-parametric image registration techniques to precisely align\nimages obtained from multimodal imaging, which is critical for the successful\nidentification of individual trees using object recognition approaches.\nNon-parametric image registration, in particular the technique of optimizing\none objective function containing data fidelity and regularization terms,\nprovides flexible algorithms for image registration. Using a survey of\nwoodlands in southern Spain as an example, we show that non-parametric image\nregistration can be successful at fusing datasets when there is little prior\nknowledge about how the datasets are interrelated (i.e. in the absence of\nground control points). The validity of non-parametric registration methods in\nairborne remote sensing is demonstrated by a series of experiments. Precise\ndata fusion is a prerequisite to accurate recognition of objects within\nairborne imagery, so non-parametric image registration could make a valuable\ncontribution to the analysis pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 11:21:57 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Lee", "Juheon", ""], ["Cai", "Xiaohao", ""], ["Schonlieb", "Carola-Bibiane", ""], ["Coomes", "David", ""]]}, {"id": "1410.0243", "submitter": "Aleksandra Pizurica", "authors": "Aleksandra Pizurica", "title": "Pattern Encoding on the Poincare Sphere", "comments": "26 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convenient graphical tool for encoding visual patterns\n(such as image patches and image atoms) as point constellations in a space\nspanned by perceptual features and with a clear geometrical interpretation.\nGeneral theory and a practical pattern encoding scheme are presented, inspired\nby encoding polarization states of a light wave on the Poincare sphere. This\nnew pattern encoding scheme can be useful for many applications in image\nprocessing and computer vision. Here, three possible applications are\nillustrated, in clustering perceptually similar patterns, visualizing\nproperties of learned dictionaries of image atoms and generating new\ndictionaries of image atoms from spherical codes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 14:41:55 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Pizurica", "Aleksandra", ""]]}, {"id": "1410.0311", "submitter": "Subhadip Mukherjee", "authors": "Subhadip Mukherjee, Rupam Basu, and Chandra Sekhar Seelamantula", "title": "$\\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous\n  Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a dictionary learning algorithm by minimizing the $\\ell_1$\ndistortion metric on the data term, which is known to be robust for\nnon-Gaussian noise contamination. The proposed algorithm exploits the idea of\niterative minimization of weighted $\\ell_2$ error. We refer to this algorithm\nas $\\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparse\ncoefficients are simultaneously updated to minimize the $\\ell_1$ objective,\nresulting in noise-robustness. We demonstrate through experiments that the\n$\\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with the\nK-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.,\nboth in Gaussian and non-Gaussian noise conditions. We also show that, for\nfixed values of sparsity, number of dictionary atoms, and data-dimension, the\n$\\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when the\ntraining set available is small. We apply the proposed algorithm for denoising\nnatural images corrupted by additive Gaussian and Laplacian noise. The images\ndenoised using $\\ell_1$-K-SVD are observed to have slightly higher peak\nsignal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but the\nimprovement in structural similarity index (SSIM) is significant (approximately\n$0.1$) for lower values of input PSNR, indicating the efficacy of the $\\ell_1$\nmetric.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 07:23:04 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:38:37 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["Basu", "Rupam", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1410.0371", "submitter": "Ankit Chaudhary", "authors": "J. L. Raheja, B. Ajay, Ankit Chaudhary", "title": "Real Time Fabric Defect Detection System on an Embedded DSP Platform", "comments": "Optik Elsevier 2013", "journal-ref": null, "doi": "10.1016/j.ijleo.2013.03.038", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial fabric productions, automated real time systems are needed to\nfind out the minor defects. It will save the cost by not transporting defected\nproducts and also would help in making compmay image of quality fabrics by\nsending out only undefected products. A real time fabric defect detection\nsystem (FDDS), implementd on an embedded DSP platform is presented here.\nTextural features of fabric image are extracted based on gray level\nco-occurrence matrix (GLCM). A sliding window technique is used for defect\ndetection where window moves over the whole image computing a textural energy\nfrom the GLCM of the fabric image. The energy values are compared to a\nreference and the deviations beyond a threshold are reported as defects and\nalso visually represented by a window. The implementation is carried out on a\nTI TMS320DM642 platform and programmed using code composer studio software. The\nreal time output of this implementation was shown on a monitor.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 02:49:07 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Raheja", "J. L.", ""], ["Ajay", "B.", ""], ["Chaudhary", "Ankit", ""]]}, {"id": "1410.0389", "submitter": "Novi Quadrianto", "authors": "Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert", "title": "Learning to Transfer Privileged Information", "comments": "20 pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning framework called learning using privileged\ninformation (LUPI) to the computer vision field. We focus on the prototypical\ncomputer vision problem of teaching computers to recognize objects in images.\nWe want the computers to be able to learn faster at the expense of providing\nextra information during training time. As additional information about the\nimage data, we look at several scenarios that have been studied in computer\nvision before: attributes, bounding boxes and image tags. The information is\nprivileged as it is available at training time but not at test time. We explore\ntwo maximum-margin techniques that are able to make use of this additional\nsource of information, for binary and multiclass object classification. We\ninterpret these methods as learning easiness and hardness of the objects in the\nprivileged space and then transferring this knowledge to train a better\nclassifier in the original space. We provide a thorough analysis and comparison\nof information transfer from privileged to the original data spaces for both\nLUPI methods. Our experiments show that incorporating privileged information\ncan improve the classification accuracy. Finally, we conduct user studies to\nunderstand which samples are easy and which are hard for human learning, and\nexplore how this information is related to easy and hard samples when learning\na classifier.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 21:29:45 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Sharmanska", "Viktoriia", ""], ["Quadrianto", "Novi", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1410.0478", "submitter": "Subhadip Basu", "authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram\n  Sarkar, Mahantapas Kundu, Mita Nasipuri", "title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set", "comments": null, "journal-ref": "2009 International Conference on Artificial Intelligence and\n  Pattern Recognition, At Orlando, Florida pp. 380-386", "doi": "10.13140/2.1.3689.4089", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 08:26:38 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Das", "Nibaran", ""], ["Pramanik", "Sandip", ""], ["Basu", "Subhadip", ""], ["Saha", "Punam Kumar", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1410.0582", "submitter": "Hugh Kennedy Dr.", "authors": "Hugh L. Kennedy", "title": "Multidimensional Digital Smoothing Filters for Target Detection", "comments": "With galley proof fixes", "journal-ref": "Signal Processing, Volume 114, September 2015, Pages 251-264", "doi": "10.1016/j.sigpro.2015.03.005.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive, causal and non-causal, multidimensional digital filters, with\ninfinite impulse responses and maximally flat magnitude and delay responses in\nthe low-frequency region, are designed to negate correlated clutter and\ninterference in the background and to accumulate power due to dim targets in\nthe foreground of a surveillance sensor. Expressions relating mean\nimpulse-response duration, frequency selectivity and group delay, to low-order\nlinear-difference-equation coefficients are derived using discrete Laguerre\npolynomials and discounted least-squares regression, then verified through\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 15:06:35 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 10:29:06 GMT"}, {"version": "v3", "created": "Sat, 21 Feb 2015 04:00:30 GMT"}, {"version": "v4", "created": "Sun, 8 Mar 2015 10:11:35 GMT"}, {"version": "v5", "created": "Wed, 1 Apr 2015 06:18:11 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Kennedy", "Hugh L.", ""]]}, {"id": "1410.0719", "submitter": "Laurent Jacques", "authors": "L. Jacques, C. De Vleeschouwer, Y. Boursier, P. Sudhakar, C. De Mol,\n  A. Pizurica, S. Anthoine, P. Vandergheynst, P. Frossard, C. Bilen, S. Kitic,\n  N. Bertin, R. Gribonval, N. Boumal, B. Mishra, P.-A. Absil, R. Sepulchre, S.\n  Bundervoet, C. Schretter, A. Dooms, P. Schelkens, O. Chabiron, F. Malgouyres,\n  J.-Y. Tourneret, N. Dobigeon, P. Chainais, C. Richard, B. Cornelis, I.\n  Daubechies, D. Dunson, M. Dankova, P. Rajmic, K. Degraux, V. Cambareri, B.\n  Geelen, G. Lafruit, G. Setti, J.-F. Determe, J. Louveaux, F. Horlin, A.\n  Dr\\'emeau, P. Heas, C. Herzet, V. Duval, G. Peyr\\'e, A. Fawzi, M. Davies, N.\n  Gillis, S. A. Vavasis, C. Soussen, L. Le Magoarou, J. Liang, J. Fadili, A.\n  Liutkus, D. Martina, S. Gigan, L. Daudet, M. Maggioni, S. Minsker, N. Strawn,\n  C. Mory, F. Ngole, J.-L. Starck, I. Loris, S. Vaiter, M. Golbabaee, D.\n  Vukobratovic", "title": "Proceedings of the second \"international Traveling Workshop on\n  Interactions between Sparse models and Technology\" (iTWIST'14)", "comments": "69 pages, 24 extended abstracts, iTWIST'14 website:\n  http://sites.google.com/site/itwist14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implicit objective of the biennial \"international - Traveling Workshop on\nInteractions between Sparse models and Technology\" (iTWIST) is to foster\ncollaboration between international scientific teams by disseminating ideas\nthrough both specific oral/poster presentations and free discussions. For its\nsecond edition, the iTWIST workshop took place in the medieval and picturesque\ntown of Namur in Belgium, from Wednesday August 27th till Friday August 29th,\n2014. The workshop was conveniently located in \"The Arsenal\" building within\nwalking distance of both hotels and town center. iTWIST'14 has gathered about\n70 international participants and has featured 9 invited talks, 10 oral\npresentations, and 14 posters on the following themes, all related to the\ntheory, application and generalization of the \"sparsity paradigm\":\nSparsity-driven data sensing and processing; Union of low dimensional\nsubspaces; Beyond linear and convex inverse problem; Matrix/manifold/graph\nsensing/processing; Blind inverse problems and dictionary learning; Sparsity\nand computational neuroscience; Information theory, geometry and randomness;\nComplexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?;\nSparse machine learning and inference.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 21:40:08 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 07:55:35 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Jacques", "L.", ""], ["De Vleeschouwer", "C.", ""], ["Boursier", "Y.", ""], ["Sudhakar", "P.", ""], ["De Mol", "C.", ""], ["Pizurica", "A.", ""], ["Anthoine", "S.", ""], ["Vandergheynst", "P.", ""], ["Frossard", "P.", ""], ["Bilen", "C.", ""], ["Kitic", "S.", ""], ["Bertin", "N.", ""], ["Gribonval", "R.", ""], ["Boumal", "N.", ""], ["Mishra", "B.", ""], ["Absil", "P. -A.", ""], ["Sepulchre", "R.", ""], ["Bundervoet", "S.", ""], ["Schretter", "C.", ""], ["Dooms", "A.", ""], ["Schelkens", "P.", ""], ["Chabiron", "O.", ""], ["Malgouyres", "F.", ""], ["Tourneret", "J. -Y.", ""], ["Dobigeon", "N.", ""], ["Chainais", "P.", ""], ["Richard", "C.", ""], ["Cornelis", "B.", ""], ["Daubechies", "I.", ""], ["Dunson", "D.", ""], ["Dankova", "M.", ""], ["Rajmic", "P.", ""], ["Degraux", "K.", ""], ["Cambareri", "V.", ""], ["Geelen", "B.", ""], ["Lafruit", "G.", ""], ["Setti", "G.", ""], ["Determe", "J. -F.", ""], ["Louveaux", "J.", ""], ["Horlin", "F.", ""], ["Dr\u00e9meau", "A.", ""], ["Heas", "P.", ""], ["Herzet", "C.", ""], ["Duval", "V.", ""], ["Peyr\u00e9", "G.", ""], ["Fawzi", "A.", ""], ["Davies", "M.", ""], ["Gillis", "N.", ""], ["Vavasis", "S. A.", ""], ["Soussen", "C.", ""], ["Magoarou", "L. Le", ""], ["Liang", "J.", ""], ["Fadili", "J.", ""], ["Liutkus", "A.", ""], ["Martina", "D.", ""], ["Gigan", "S.", ""], ["Daudet", "L.", ""], ["Maggioni", "M.", ""], ["Minsker", "S.", ""], ["Strawn", "N.", ""], ["Mory", "C.", ""], ["Ngole", "F.", ""], ["Starck", "J. -L.", ""], ["Loris", "I.", ""], ["Vaiter", "S.", ""], ["Golbabaee", "M.", ""], ["Vukobratovic", "D.", ""]]}, {"id": "1410.0736", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis\n  DeCoste, Wei Di, Yizhou Yu", "title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\n  Visual Recognition", "comments": "Add new results on ImageNet using VGG-16-layer building block net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification, visual separability between different object\ncategories is highly uneven, and some categories are more difficult to\ndistinguish than others. Such difficult categories demand more dedicated\nclassifiers. However, existing deep convolutional neural networks (CNN) are\ntrained as flat N-way classifiers, and few efforts have been made to leverage\nthe hierarchical structure of categories. In this paper, we introduce\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\nwhile distinguishing difficult classes using fine category classifiers. During\nHD-CNN training, component-wise pretraining is followed by global finetuning\nwith a multinomial logistic loss regularized by a coarse category consistency\nterm. In addition, conditional executions of fine category classifiers and\nlayer parameter compression make HD-CNNs scalable for large-scale visual\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\nCNNs by 2.65%, 3.1% and 1.1%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 01:17:20 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:51:51 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 03:11:49 GMT"}, {"version": "v4", "created": "Sat, 16 May 2015 03:36:32 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Piramuthu", "Robinson", ""], ["Jagadeesh", "Vignesh", ""], ["DeCoste", "Dennis", ""], ["Di", "Wei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1410.0745", "submitter": "Robinson Piramuthu Robinson Piramuthu", "authors": "Qiaosong Wang, Vignesh Jagadeesh, Bryan Ressler, Robinson Piramuthu", "title": "Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer\n  Depth Camera and Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in consumer depth sensors have created many opportunities for\nhuman body measurement and modeling. Estimation of 3D body shape is\nparticularly useful for fashion e-commerce applications such as virtual try-on\nor fit personalization. In this paper, we propose a method for capturing\naccurate human body shape and anthropometrics from a single consumer grade\ndepth sensor. We first generate a large dataset of synthetic 3D human body\nmodels using real-world body size distributions. Next, we estimate key body\nmeasurements from a single monocular depth image. We combine body measurement\nestimates with local geometry features around key joint positions to form a\nrobust multi-dimensional feature vector. This allows us to conduct a fast\nnearest-neighbor search to every sample in the dataset and return the closest\none. Compared to existing methods, our approach is able to predict accurate\nfull body parameters from a partial view using measurement parameters learned\nfrom the synthetic dataset. Furthermore, our system is capable of generating 3D\nhuman mesh models in real-time, which is significantly faster than methods\nwhich attempt to model shape and pose deformations. To validate the efficiency\nand applicability of our system, we collected a dataset that contains frontal\nand back scans of 83 clothed people with ground truth height and weight.\nExperiments on real-world dataset show that the proposed method can achieve\nreal-time performance with competing results achieving an average error of 1.9\ncm in estimated measurements.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 02:33:08 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 20:30:32 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Wang", "Qiaosong", ""], ["Jagadeesh", "Vignesh", ""], ["Ressler", "Bryan", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1410.0818", "submitter": "Junhua Li", "authors": "Junhua Li, Zbigniew Struzik, Liqing Zhang, Andrzej Cichocki", "title": "Feature Learning from Incomplete EEG with Denoising Autoencoder", "comments": "The paper was accepted for publication by Neurocomputing", "journal-ref": "Neurocomputing, 2015, 165: 23-31", "doi": "10.1016/j.neucom.2014.08.092", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An alternative pathway for the human brain to communicate with the outside\nworld is by means of a brain computer interface (BCI). A BCI can decode\nelectroencephalogram (EEG) signals of brain activities, and then send a command\nor an intent to an external interactive device, such as a wheelchair. The\neffectiveness of the BCI depends on the performance in decoding the EEG.\nUsually, the EEG is contaminated by different kinds of artefacts (e.g.,\nelectromyogram (EMG), background activity), which leads to a low decoding\nperformance. A number of filtering methods can be utilized to remove or weaken\nthe effects of artefacts, but they generally fail when the EEG contains extreme\nartefacts. In such cases, the most common approach is to discard the whole data\nsegment containing extreme artefacts. This causes the fatal drawback that the\nBCI cannot output decoding results during that time. In order to solve this\nproblem, we employ the Lomb-Scargle periodogram to estimate the spectral power\nfrom incomplete EEG (after removing only parts contaminated by artefacts), and\nDenoising Autoencoder (DAE) for learning. The proposed method is evaluated with\nmotor imagery EEG data. The results show that our method can successfully\ndecode incomplete EEG to good effect.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 11:12:47 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Li", "Junhua", ""], ["Struzik", "Zbigniew", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1410.0868", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Zhihua Zhang, Xiaobing Feng", "title": "Group Orbit Optimization: A Unified Approach to Data Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study an optimization problem over a matrix\ngroup orbit that we call \\emph{Group Orbit Optimization} (GOO). We prove that\nGOO can be used to induce matrix decomposition techniques such as singular\nvalue decomposition (SVD), LU decomposition, QR decomposition, Schur\ndecomposition and Cholesky decomposition, etc. This gives rise to a unified\nframework for matrix decomposition and allows us to bridge these matrix\ndecomposition methods. Moreover, we generalize GOO for tensor decomposition. As\na concrete application of GOO, we devise a new data decomposition method over a\nspecial linear group to normalize point cloud data. Experiment results show\nthat our normalization method is able to obtain recovery well from distortions\nlike shearing, rotation and squeezing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:43:01 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Zhou", "Shuchang", ""], ["Zhang", "Zhihua", ""], ["Feng", "Xiaobing", ""]]}, {"id": "1410.0925", "submitter": "Carl Yuheng Ren", "authors": "Victor Adrian Prisacariu, Olaf K\\\"ahler, Ming Ming Cheng, Carl Yuheng\n  Ren, Julien Valentin, Philip H.S. Torr, Ian D. Reid, David W. Murray", "title": "A Framework for the Volumetric Integration of Depth Images", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric models have become a popular representation for 3D scenes in\nrecent years. One of the breakthroughs leading to their popularity was\nKinectFusion, where the focus is on 3D reconstruction using RGB-D sensors.\nHowever, monocular SLAM has since also been tackled with very similar\napproaches. Representing the reconstruction volumetrically as a truncated\nsigned distance function leads to most of the simplicity and efficiency that\ncan be achieved with GPU implementations of these systems. However, this\nrepresentation is also memory-intensive and limits the applicability to small\nscale reconstructions. Several avenues have been explored for overcoming this\nlimitation. With the aim of summarizing them and providing for a fast and\nflexible 3D reconstruction pipeline, we propose a new, unifying framework\ncalled InfiniTAM. The core idea is that individual steps like camera tracking,\nscene representation and integration of new data can easily be replaced and\nadapted to the needs of the user. Along with the framework we also provide a\nset of components for scalable reconstruction: two implementations of camera\ntrackers, based on RGB data and on depth data, two representations of the 3D\nvolumetric data, a dense volume and one based on hashes of subblocks, and an\noptional module for swapping subblocks in and out of the typically limited GPU\nmemory.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 17:42:14 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 13:17:52 GMT"}, {"version": "v3", "created": "Thu, 23 Oct 2014 20:48:42 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Prisacariu", "Victor Adrian", ""], ["K\u00e4hler", "Olaf", ""], ["Cheng", "Ming Ming", ""], ["Ren", "Carl Yuheng", ""], ["Valentin", "Julien", ""], ["Torr", "Philip H. S.", ""], ["Reid", "Ian D.", ""], ["Murray", "David W.", ""]]}, {"id": "1410.0969", "submitter": "Abdul Kadir", "authors": "Abdul Kadir", "title": "A Model of Plant Identification System Using GLCM, Lacunarity And Shen\n  Features", "comments": "10 pages", "journal-ref": "Research Journal of Pharmaceutical, Biological and Chemical\n  Sciences, Vol 5(2), 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many approaches have been introduced by several researchers to\nidentify plants. Now, applications of texture, shape, color and vein features\nare common practices. However, there are many possibilities of methods can be\ndeveloped to improve the performance of such identification systems. Therefore,\nseveral experiments had been conducted in this research. As a result, a new\nnovel approach by using combination of Gray-Level Co-occurrence Matrix,\nlacunarity and Shen features and a Bayesian classifier gives a better result\ncompared to other plant identification systems. For comparison, this research\nused two kinds of several datasets that were usually used for testing the\nperformance of each plant identification system. The results show that the\nsystem gives an accuracy rate of 97.19% when using the Flavia dataset and\n95.00% when using the Foliage dataset and outperforms other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 00:49:05 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Kadir", "Abdul", ""]]}, {"id": "1410.1035", "submitter": "Rahul Rama Varior Mr.", "authors": "Rahul Rama Varior, Gang Wang and Jiwen Lu", "title": "Learning Invariant Color Features for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching people across multiple camera views known as person\nre-identification, is a challenging problem due to the change in visual\nappearance caused by varying lighting conditions. The perceived color of the\nsubject appears to be different with respect to illumination. Previous works\nuse color as it is or address these challenges by designing color spaces\nfocusing on a specific cue. In this paper, we propose a data driven approach\nfor learning color patterns from pixels sampled from images across two camera\nviews. The intuition behind this work is that, even though pixel values of same\ncolor would be different across views, they should be encoded with the same\nvalues. We model color feature generation as a learning problem by jointly\nlearning a linear transformation and a dictionary to encode pixel values. We\nalso analyze different photometric invariant color spaces. Using color as the\nonly cue, we compare our approach with all the photometric invariant color\nspaces and show superior performance over all of them. Combining with other\nlearned low-level and high-level features, we obtain promising results in\nViPER, Person Re-ID 2011 and CAVIAR4REID datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 10:27:51 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 10:32:36 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Varior", "Rahul Rama", ""], ["Wang", "Gang", ""], ["Lu", "Jiwen", ""]]}, {"id": "1410.1037", "submitter": "Dacheng Tao", "authors": "Nannan Wang, Xinbo Gao, Dacheng Tao, Xuelong Li", "title": "Facial Feature Point Detection: A Comprehensive Survey", "comments": "32 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive survey of facial feature point detection\nwith the assistance of abundant manually labeled images. Facial feature point\ndetection favors many applications such as face recognition, animation,\ntracking, hallucination, expression analysis and 3D face modeling. Existing\nmethods can be categorized into the following four groups: constrained local\nmodel (CLM)-based, active appearance model (AAM)-based, regression-based, and\nother methods. CLM-based methods consist of a shape model and a number of local\nexperts, each of which is utilized to detect a facial feature point. AAM-based\nmethods fit a shape model to an image by minimizing texture synthesis errors.\nRegression-based methods directly learn a mapping function from facial image\nappearance to facial feature points. Besides the above three major categories\nof methods, there are also minor categories of methods which we classify into\nother methods: graphical model-based methods, joint face alignment methods,\nindependent facial feature point detectors, and deep learning-based methods.\nThough significant progress has been made, facial feature point detection is\nlimited in its success by wild and real-world conditions: variations across\nposes, expressions, illuminations, and occlusions. A comparative illustration\nand analysis of representative methods provide us a holistic understanding and\ndeep insight into facial feature point detection, which also motivates us to\nexplore promising future directions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 11:04:50 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Wang", "Nannan", ""], ["Gao", "Xinbo", ""], ["Tao", "Dacheng", ""], ["Li", "Xuelong", ""]]}, {"id": "1410.1090", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille", "title": "Explain Images with Multimodal Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel sentence descriptions to explain the content of images. It\ndirectly models the probability distribution of generating a word given\nprevious words and the image. Image descriptions are generated by sampling from\nthis distribution. The model consists of two sub-networks: a deep recurrent\nneural network for sentences and a deep convolutional network for images. These\ntwo sub-networks interact with each other in a multimodal layer to form the\nwhole m-RNN model. The effectiveness of our model is validated on three\nbenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model\noutperforms the state-of-the-art generative method. In addition, the m-RNN\nmodel can be applied to retrieval tasks for retrieving images or sentences, and\nachieves significant performance improvement over the state-of-the-art methods\nwhich directly optimize the ranking objective function for retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 20:24:34 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1410.1267", "submitter": "Alex James Dr", "authors": "Akshay Kumar Maan, Dinesh Sasi Kumar, Sherin Sugathan, and Alex\n  Pappachen James", "title": "Memristive Threshold Logic Circuit Design of Fast Moving Object\n  Detection", "comments": "To be published in IEEE Transactions on Very Large Scale Integration\n  (VLSI) Systems", "journal-ref": null, "doi": "10.1109/TVLSI.2014.2359801", "report-no": null, "categories": "cs.CV cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Real-time detection of moving objects involves memorisation of features in\nthe template image and their comparison with those in the test image. At high\nsampling rates, such techniques face the problems of high algorithmic\ncomplexity and component delays. We present a new resistive switching based\nthreshold logic cell which encodes the pixels of a template image. The cell\ncomprises a voltage divider circuit that programs the resistances of the\nmemristors arranged in a single node threshold logic network and the output is\nencoded as a binary value using a CMOS inverter gate. When a test image is\napplied to the template-programmed cell, a mismatch in the respective pixels is\nseen as a change in the output voltage of the cell. The proposed cell when\ncompared with CMOS equivalent implementation shows improved performance in\narea, leakage power, power dissipation and delay.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 06:52:57 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Maan", "Akshay Kumar", ""], ["Kumar", "Dinesh Sasi", ""], ["Sugathan", "Sherin", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1410.1606", "submitter": "Xiang Xiang", "authors": "Xiang Xiang, Minh Dao, Gregory D. Hager, Trac D. Tran", "title": "Hierarchical Sparse and Collaborative Low-Rank Representation for\n  Emotion Recognition", "comments": "5 pages, 5 figures; accepted to IEEE ICASSP 2015; programs available\n  at https://github.com/eglxiang/icassp15_emotion/", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a Collaborative-Hierarchical Sparse and Low-Rank\n(C-HiSLR) model that is natural for recognizing human emotion in visual data.\nPrevious attempts require explicit expression components, which are often\nunavailable and difficult to recover. Instead, our model exploits the lowrank\nproperty over expressive facial frames and rescue inexact sparse\nrepresentations by incorporating group sparsity. For the CK+ dataset, C-HiSLR\non raw expressive faces performs as competitive as the Sparse Representation\nbased Classification (SRC) applied on manually prepared emotions. C-HiSLR\nperforms even better than SRC in terms of true positive rate.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 03:45:20 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 18:02:33 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Xiang", "Xiang", ""], ["Dao", "Minh", ""], ["Hager", "Gregory D.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1410.1699", "submitter": "Martin Storath", "authors": "Andreas Weinmann, Laurent Demaret, Martin Storath", "title": "Mumford-Shah and Potts Regularization for Manifold-Valued Data with\n  Applications to DTI and Q-Ball Imaging", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-015-0628-2", "report-no": null, "categories": "math.NA cs.CV math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mumford-Shah and Potts functionals are powerful variational models for\nregularization which are widely used in signal and image processing; typical\napplications are edge-preserving denoising and segmentation. Being both\nnon-smooth and non-convex, they are computationally challenging even for scalar\ndata. For manifold-valued data, the problem becomes even more involved since\ntypical features of vector spaces are not available. In this paper, we propose\nalgorithms for Mumford-Shah and for Potts regularization of manifold-valued\nsignals and images. For the univariate problems, we derive solvers based on\ndynamic programming combined with (convex) optimization techniques for\nmanifold-valued data. For the class of Cartan-Hadamard manifolds (which\nincludes the data space in diffusion tensor imaging), we show that our\nalgorithms compute global minimizers for any starting point. For the\nmultivariate Mumford-Shah and Potts problems (for image regularization) we\npropose a splitting into suitable subproblems which we can solve exactly using\nthe techniques developed for the corresponding univariate problems. Our method\ndoes not require any a priori restrictions on the edge set and we do not have\nto discretize the data space. We apply our method to diffusion tensor imaging\n(DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentation\nof the corpus callosum.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 11:59:14 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Weinmann", "Andreas", ""], ["Demaret", "Laurent", ""], ["Storath", "Martin", ""]]}, {"id": "1410.1980", "submitter": "David Menotti", "authors": "David Menotti, Giovani Chiachia, Allan Pinto, William Robson Schwartz,\n  Helio Pedrini, Alexandre Xavier Falcao and Anderson Rocha", "title": "Deep Representations for Iris, Face, and Fingerprint Spoofing Detection", "comments": "Pre-print of article that will appear in the IEEE Transactions on\n  Information Forenseics and Security (T.IFS), Special Issue on Biometric\n  Spoofing and Countermeasures, vol 10, n. 4, April 2015", "journal-ref": null, "doi": "10.1109/TIFS.2015.2398817", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics systems have significantly improved person identification and\nauthentication, playing an important role in personal, national, and global\nsecurity. However, these systems might be deceived (or \"spoofed\") and, despite\nthe recent advances in spoofing detection, current solutions often rely on\ndomain knowledge, specific biometric reading systems, and attack types. We\nassume a very limited knowledge about biometric spoofing at the sensor to\nderive outstanding spoofing detection systems for iris, face, and fingerprint\nmodalities based on two deep learning approaches. The first approach consists\nof learning suitable convolutional network architectures for each domain, while\nthe second approach focuses on learning the weights of the network via\nback-propagation. We consider nine biometric spoofing benchmarks --- each one\ncontaining real and fake samples of a given biometric modality and attack type\n--- and learn deep representations for each benchmark by combining and\ncontrasting the two learning approaches. This strategy not only provides better\ncomprehension of how these approaches interplay, but also creates systems that\nexceed the best known results in eight out of the nine benchmarks. The results\nstrongly indicate that spoofing detection systems based on convolutional\nnetworks can be robust to attacks already known and possibly adapted, with\nlittle effort, to image-based attacks that are yet to come.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 04:42:01 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 20:11:26 GMT"}, {"version": "v3", "created": "Thu, 29 Jan 2015 21:42:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Menotti", "David", ""], ["Chiachia", "Giovani", ""], ["Pinto", "Allan", ""], ["Schwartz", "William Robson", ""], ["Pedrini", "Helio", ""], ["Falcao", "Alexandre Xavier", ""], ["Rocha", "Anderson", ""]]}, {"id": "1410.2167", "submitter": "Luigi Nardi", "authors": "Luigi Nardi, Bruno Bodin, M. Zeeshan Zia, John Mawer, Andy Nisbet,\n  Paul H. J. Kelly, Andrew J. Davison, Mikel Luj\\'an, Michael F. P. O'Boyle,\n  Graham Riley, Nigel Topham, Steve Furber", "title": "Introducing SLAMBench, a performance and accuracy benchmarking\n  methodology for SLAM", "comments": "8 pages, ICRA 2015 conference paper", "journal-ref": "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7140009\n  IEEE Xplore 2015", "doi": "10.1109/ICRA.2015.7140009", "report-no": null, "categories": "cs.RO cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time dense computer vision and SLAM offer great potential for a new\nlevel of scene modelling, tracking and real environmental interaction for many\ntypes of robot, but their high computational requirements mean that use on mass\nmarket embedded platforms is challenging. Meanwhile, trends in low-cost,\nlow-power processing are towards massive parallelism and heterogeneity, making\nit difficult for robotics and vision researchers to implement their algorithms\nin a performance-portable way. In this paper we introduce SLAMBench, a\npublicly-available software framework which represents a starting point for\nquantitative, comparable and validatable experimental research to investigate\ntrade-offs in performance, accuracy and energy consumption of a dense RGB-D\nSLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP,\nOpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D\nsequences with trajectory and scene ground truth for reliable accuracy\ncomparison of different implementation and algorithms. We present an analysis\nand breakdown of the constituent algorithmic elements of KinectFusion, and\nexperimentally investigate their execution time on a variety of multicore and\nGPUaccelerated platforms. For a popular embedded platform, we also present an\nanalysis of energy efficiency for different configuration alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 15:34:43 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 16:28:27 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Nardi", "Luigi", ""], ["Bodin", "Bruno", ""], ["Zia", "M. Zeeshan", ""], ["Mawer", "John", ""], ["Nisbet", "Andy", ""], ["Kelly", "Paul H. J.", ""], ["Davison", "Andrew J.", ""], ["Luj\u00e1n", "Mikel", ""], ["O'Boyle", "Michael F. P.", ""], ["Riley", "Graham", ""], ["Topham", "Nigel", ""], ["Furber", "Steve", ""]]}, {"id": "1410.2173", "submitter": "Khairul Azha A Aziz", "authors": "K.A.A. Aziz and S.S. Abdullah", "title": "Face Detection Using Radial Basis Functions Neural Networks With Fixed\n  Spread", "comments": "6 pages, The Second International Conference on Control,\n  Instrumentation and Mechatronic Engineering (CIM09) Malacca, Malaysia, June\n  2-3, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presented a face detection system using Radial Basis Function\nNeural Networks With Fixed Spread Value. Face detection is the first step in\nface recognition system. The purpose is to localize and extract the face region\nfrom the background that will be fed into the face recognition system for\nidentification. General preprocessing approach was used for normalizing the\nimage and Radial Basis Function (RBF) Neural Network was used to distinguish\nbetween face and non-face. RBF Neural Networks offer several advantages\ncompared to other neural network architecture such as they can be trained using\nfast two stages training algorithm and the network possesses the property of\nbest approximation. The output of the network can be optimized by setting\nsuitable value of center and spread of the RBF. In this paper, fixed spread\nvalue will be used. The Radial Basis Function Neural Network (RBFNN) used to\ndistinguish faces and non-faces and the evaluation of the system will be the\nperformance of detection, False Acceptance Rate (FAR), False Rejection Rate\n(FRR) and the discriminative properties.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 08:30:34 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Aziz", "K. A. A.", ""], ["Abdullah", "S. S.", ""]]}, {"id": "1410.2175", "submitter": "Suman Shrestha", "authors": "Suman Shrestha", "title": "Image Denoising using New Adaptive Based Median Filters", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is a major issue while transferring images through all kinds of\nelectronic communication. One of the most common noise in electronic\ncommunication is an impulse noise which is caused by unstable voltage. In this\npaper, the comparison of known image denoising techniques is discussed and a\nnew technique using the decision based approach has been used for the removal\nof impulse noise. All these methods can primarily preserve image details while\nsuppressing impulsive noise. The principle of these techniques is at first\nintroduced and then analysed with various simulation results using MATLAB. Most\nof the previously known techniques are applicable for the denoising of images\ncorrupted with less noise density. Here a new decision based technique has been\npresented which shows better performances than those already being used. The\ncomparisons are made based on visual appreciation and further quantitatively by\nMean Square error (MSE) and Peak Signal to Noise Ratio (PSNR) of different\nfiltered images..\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 12:57:21 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Shrestha", "Suman", ""]]}, {"id": "1410.2188", "submitter": "Luming Zhang Luming", "authors": "Yuxin Hu, Luming Zhang", "title": "An Aerial Image Recognition Framework using Discrimination and\n  Redundancy Quality Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial image categorization plays an indispensable role in remote sensing and\nartificial intelligence. In this paper, we propose a new aerial image\ncategorization framework, focusing on organizing the local patches of each\naerial image into multiple discriminative subgraphs. The subgraphs reflect both\nthe geometric property and the color distribution of an aerial image. First,\neach aerial image is decomposed into a collection of regions in terms of their\ncolor intensities. Thereby region connected graph (RCG), which models the\nconnection between the spatial neighboring regions, is constructed to encode\nthe spatial context of an aerial image. Second, a subgraph mining technique is\nadopted to discover the frequent structures in the RCGs constructed from the\ntraining aerial images. Thereafter, a set of refined structures are selected\namong the frequent ones toward being highly discriminative and low redundant.\nLastly, given a new aerial image, its sub-RCGs corresponding to the refined\nstructures are extracted. They are further quantized into a discriminative\nvector for SVM classification. Thorough experimental results validate the\neffectiveness of the proposed method. In addition, the visualized mined\nsubgraphs show that the discriminative topologies of each aerial image are\ndiscovered.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 16:40:02 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Hu", "Yuxin", ""], ["Zhang", "Luming", ""]]}, {"id": "1410.2381", "submitter": "Mohamed Sayedelahl Dr", "authors": "R. M. Farouk, S. Badr and M. Sayed Elahl", "title": "Recognition of cDNA microarray image Using Feedforward artificial neural\n  network", "comments": "17 pages, 7 figures and 23 References", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 5, No. 5, September 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complementary DNA (cDNA) sequence is considered to be the magic biometric\ntechnique for personal identification. In this paper, we present a new method\nfor cDNA recognition based on the artificial neural network (ANN). Microarray\nimaging is used for the concurrent identification of thousands of genes. We\nhave segmented the location of the spots in a cDNA microarray. Thus, a precise\nlocalization and segmenting of a spot are essential to obtain a more accurate\nintensity measurement, leading to a more precise expression measurement of a\ngene. The segmented cDNA microarray image is resized and it is used as an input\nfor the proposed artificial neural network. For matching and recognition, we\nhave trained the artificial neural network. Recognition results are given for\nthe galleries of cDNA sequences . The numerical results show that, the proposed\nmatching technique is an effective in the cDNA sequences process. We also\ncompare our results with previous results and find out that, the proposed\ntechnique is an effective matching performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 08:37:21 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Farouk", "R. M.", ""], ["Badr", "S.", ""], ["Elahl", "M. Sayed", ""]]}, {"id": "1410.2386", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Guoxu Zhou, Liqing Zhang, Andrzej Cichocki, and Shun-ichi\n  Amari", "title": "Bayesian Robust Tensor Factorization for Incomplete Multiway Data", "comments": "in IEEE Transactions on Neural Networks and Learning Systems, 2015", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2423694", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model for robust tensor factorization in the presence\nof both missing data and outliers. The objective is to explicitly infer the\nunderlying low-CP-rank tensor capturing the global information and a sparse\ntensor capturing the local information (also considered as outliers), thus\nproviding the robust predictive distribution over missing entries. The\nlow-CP-rank tensor is modeled by multilinear interactions between multiple\nlatent factors on which the column sparsity is enforced by a hierarchical\nprior, while the sparse tensor is modeled by a hierarchical view of Student-$t$\ndistribution that associates an individual hyperparameter with each element\nindependently. For model learning, we develop an efficient closed-form\nvariational inference under a fully Bayesian treatment, which can effectively\nprevent the overfitting problem and scales linearly with data size. In contrast\nto existing related works, our method can perform model selection automatically\nand implicitly without need of tuning parameters. More specifically, it can\ndiscover the groundtruth of CP rank and automatically adapt the sparsity\ninducing priors to various types of outliers. In addition, the tradeoff between\nthe low-rank approximation and the sparse representation can be optimized in\nthe sense of maximum model evidence. The extensive experiments and comparisons\nwith many state-of-the-art algorithms on both synthetic and real-world datasets\ndemonstrate the superiorities of our method from several perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 08:50:31 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 05:36:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhou", "Guoxu", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""], ["Amari", "Shun-ichi", ""]]}, {"id": "1410.2474", "submitter": "Haythem Ghazouani", "authors": "Haythem Ghazouani", "title": "Genetic Stereo Matching Algorithm with Fuzzy Fitness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a genetic stereo matching algorithm with fuzzy evaluation\nfunction. The proposed algorithm presents a new encoding scheme in which a\nchromosome is represented by a disparity matrix. Evolution is controlled by a\nfuzzy fitness function able to deal with noise and uncertain camera\nmeasurements, and uses classical evolutionary operators. The result of the\nalgorithm is accurate dense disparity maps obtained in a reasonable\ncomputational time suitable for real-time applications as shown in experimental\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:08:46 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Ghazouani", "Haythem", ""]]}, {"id": "1410.2488", "submitter": "Ahmed Elgammal", "authors": "Emily L. Spratt and Ahmed Elgammal", "title": "Computational Beauty: Aesthetic Judgment at the Intersection of Art and\n  Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.hist-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In part one of the Critique of Judgment, Immanuel Kant wrote that \"the\njudgment of taste...is not a cognitive judgment, and so not logical, but is\naesthetic.\"\\cite{Kant} While the condition of aesthetic discernment has long\nbeen the subject of philosophical discourse, the role of the arbiters of that\njudgment has more often been assumed than questioned. The art historian,\ncritic, connoisseur, and curator have long held the esteemed position of the\naesthetic judge, their training, instinct, and eye part of the inimitable\nsubjective processes that Kant described as occurring upon artistic evaluation.\nAlthough the concept of intangible knowledge in regard to aesthetic theory has\nbeen much explored, little discussion has arisen in response to the development\nof new types of artificial intelligence as a challenge to the seemingly\nineffable abilities of the human observer. This paper examines the developments\nin the field of computer vision analysis of paintings from canonical movements\nwith the history of Western art and the reaction of art historians to the\napplication of this technology in the field. Through an investigation of the\nethical consequences of this innovative technology, the unquestioned authority\nof the art expert is challenged and the subjective nature of aesthetic judgment\nis brought to philosophical scrutiny once again.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 01:31:58 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Spratt", "Emily L.", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1410.2535", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau, Daniel Clark, Spela Ivekovic, Chee Sing Lee, Jose\n  Franco", "title": "A unified approach for multi-object triangulation, tracking and camera\n  calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object triangulation, 3-D object tracking, feature correspondence, and camera\ncalibration are key problems for estimation from camera networks. This paper\naddresses these problems within a unified Bayesian framework for joint\nmulti-object tracking and sensor registration. Given that using standard\nfiltering approaches for state estimation from cameras is problematic, an\nalternative parametrisation is exploited, called disparity space. The disparity\nspace-based approach for triangulation and object tracking is shown to be more\neffective than non-linear versions of the Kalman filter and particle filtering\nfor non-rectified cameras. The approach for feature correspondence is based on\nthe Probability Hypothesis Density (PHD) filter, and hence inherits the ability\nto update without explicit measurement association, to initiate new targets,\nand to discriminate between target and clutter. The PHD filtering approach then\nforms the basis of a camera calibration method from static or moving objects.\nResults are shown on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 17:13:29 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Clark", "Daniel", ""], ["Ivekovic", "Spela", ""], ["Lee", "Chee Sing", ""], ["Franco", "Jose", ""]]}, {"id": "1410.2663", "submitter": "Florian Yger", "authors": "Florian Yger", "title": "Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet\n  marginals", "comments": "9 pages, 4 Figues, 2 Tables, Challenge IEEE-ISBI : Bone Texture\n  Characterization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short memo aims at explaining our approach for the challenge IEEE-ISBI\non Bone Texture Characterization. In this work, we focus on the use of\ncovariance matrices and wavelet marginals in an SVM classifier.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 02:48:08 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Yger", "Florian", ""]]}, {"id": "1410.2959", "submitter": "Mohammed  Javed", "authors": "Mohammed Javed, P. Nagabhushan, B.B. Chaudhuri", "title": "Direct Processing of Document Images in Compressed Domain", "comments": "2014 Fourth IDRBT Doctoral Colloquium, December 11-12, 2014\n  Hyderabad, India", "journal-ref": null, "doi": "10.13140/2.1.2523.9042", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in the volume of Big data of this digital era, fax\ndocuments, invoices, receipts, etc are traditionally subjected to compression\nfor the efficiency of data storage and transfer. However, in order to process\nthese documents, they need to undergo the stage of decompression which indents\nadditional computing resources. This limitation induces the motivation to\nresearch on the possibility of directly processing of compressed images. In\nthis research paper, we summarize the research work carried out to perform\ndifferent operations straight from run-length compressed documents without\ngoing through the stage of decompression. The different operations demonstrated\nare feature extraction; text-line, word and character segmentation; document\nblock segmentation; and font size detection, all carried out in the compressed\nversion of the document. Feature extraction methods demonstrate how to extract\nthe conventionally defined features such as projection profile, run-histogram\nand entropy, directly from the compressed document data. Document segmentation\ninvolves the extraction of compressed segments of text-lines, words and\ncharacters using the vertical and horizontal projection profile features.\nFurther an attempt is made to segment randomly a block of interest from the\ncompressed document and subsequently facilitate absolute and relative\ncharacterization of the segmented block which finds real time applications in\nautomatic processing of Bank Cheques, Challans, etc, in compressed domain.\nFinally an application to detect font size at text line level is also\ninvestigated. All the proposed algorithms are validated experimentally with\nsufficient data set of compressed documents.\n", "versions": [{"version": "v1", "created": "Sat, 11 Oct 2014 07:16:41 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 04:21:22 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Javed", "Mohammed", ""], ["Nagabhushan", "P.", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "1410.3080", "submitter": "Xin Yuan", "authors": "Xin Yuan, Patrick Llull, David J. Brady, and Lawrence Carin", "title": "Tree-Structure Bayesian Compressive Sensing for Video", "comments": "5 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian compressive sensing framework is developed for video\nreconstruction based on the color coded aperture compressive temporal imaging\n(CACTI) system. By exploiting the three dimension (3D) tree structure of the\nwavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesian\ncompressive sensing inversion algorithm is derived to reconstruct (up to 22)\ncolor video frames from a single monochromatic compressive measurement. Both\nsimulated and real datasets are adopted to verify the performance of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 12 Oct 2014 11:43:37 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Yuan", "Xin", ""], ["Llull", "Patrick", ""], ["Brady", "David J.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1410.3349", "submitter": "Chinh Dang", "authors": "Chinh Dang and Hayder Radha", "title": "Single Image Super Resolution via Manifold Approximation", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution remains an important research topic to overcome the\nlimitations of physical acquisition systems, and to support the development of\nhigh resolution displays. Previous example-based super-resolution approaches\nmainly focus on analyzing the co-occurrence properties of low resolution and\nhigh-resolution patches. Recently, we proposed a novel single image\nsuper-resolution approach based on linear manifold approximation of the\nhigh-resolution image-patch space [1]. The image super-resolution problem is\nthen formulated as an optimization problem of searching for the best matched\nhigh resolution patch in the manifold for a given low-resolution patch. We\ndeveloped a novel technique based on the l1 norm sparse graph to learn a set of\nlow dimensional affine spaces or tangent subspaces of the high-resolution patch\nmanifold. The optimization problem is then solved based on the learned set of\ntangent subspaces. In this paper, we build on our recent work as follows.\nFirst, we consider and analyze each tangent subspace as one point in a\nGrassmann manifold, which helps to compute geodesic pairwise distances among\nthese tangent subspaces. Second, we develop a min-max algorithm to select an\noptimal subset of tangent subspaces. This optimal subset reduces the\ncomputational cost while still preserving the quality of the reconstructed\nhigh-resolution image. Third, and to further achieve lower computational\ncomplexity, we perform hierarchical clustering on the optimal subset based on\nGrassmann manifold distances. Finally, we analytically prove the validity of\nthe proposed Grassmann-distance based clustering. A comparison of the obtained\nresults with other state-of-the-art methods clearly indicates the viability of\nthe new proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:28:34 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 18:39:36 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Dang", "Chinh", ""], ["Radha", "Hayder", ""]]}, {"id": "1410.3426", "submitter": "Roberto Cavoretto", "authors": "R. Cavoretto, A. De Rossi, H. Qiao, B. Quatember, W. Recheis, M. Mayr", "title": "Computing Topology Preservation of RBF Transformations for\n  Landmark-Based Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image registration, a proper transformation should be topology preserving.\nEspecially for landmark-based image registration, if the displacement of one\nlandmark is larger enough than those of neighbourhood landmarks, topology\nviolation will be occurred. This paper aim to analyse the topology preservation\nof some Radial Basis Functions (RBFs) which are used to model deformations in\nimage registration. Mat\\'{e}rn functions are quite common in the statistic\nliterature (see, e.g. \\cite{Matern86,Stein99}). In this paper, we use them to\nsolve the landmark-based image registration problem. We present the topology\npreservation properties of RBFs in one landmark and four landmarks model\nrespectively. Numerical results of three kinds of Mat\\'{e}rn transformations\nare compared with results of Gaussian, Wendland's, and Wu's functions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 18:39:48 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Cavoretto", "R.", ""], ["De Rossi", "A.", ""], ["Qiao", "H.", ""], ["Quatember", "B.", ""], ["Recheis", "W.", ""], ["Mayr", "M.", ""]]}, {"id": "1410.3462", "submitter": "Xirong Li", "authors": "Xirong Li", "title": "Tag Relevance Fusion for Social Image Retrieval", "comments": null, "journal-ref": null, "doi": "10.1007/s00530-014-0430-9", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the subjective nature of social tagging, measuring the relevance of\nsocial tags with respect to the visual content is crucial for retrieving the\nincreasing amounts of social-networked images. Witnessing the limit of a single\nmeasurement of tag relevance, we introduce in this paper tag relevance fusion\nas an extension to methods for tag relevance estimation. We present a\nsystematic study, covering tag relevance fusion in early and late stages, and\nin supervised and unsupervised settings. Experiments on a large present-day\nbenchmark set show that tag relevance fusion leads to better image retrieval.\nMoreover, unsupervised tag relevance fusion is found to be practically as\neffective as supervised tag relevance fusion, but without the need of any\ntraining efforts. This finding suggests the potential of tag relevance fusion\nfor real-world deployment.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 13:27:55 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Li", "Xirong", ""]]}, {"id": "1410.3699", "submitter": "Rita Ammanouil", "authors": "Rita Ammanouil, Andr\\'e Ferrari, C\\'edric Richard", "title": "A graph Laplacian regularization for hyperspectral data unmixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a graph Laplacian regularization in the hyperspectral\nunmixing formulation. The proposed regularization relies upon the construction\nof a graph representation of the hyperspectral image. Each node in the graph\nrepresents a pixel's spectrum, and edges connect spectrally and spatially\nsimilar pixels. The proposed graph framework promotes smoothness in the\nestimated abundance maps and collaborative estimation between homogeneous areas\nof the image. The resulting convex optimization problem is solved using the\nAlternating Direction Method of Multipliers (ADMM). A special attention is\ngiven to the computational complexity of the algorithm, and Graph-cut methods\nare proposed in order to reduce the computational burden. Finally, simulations\nconducted on synthetic data illustrate the effectiveness of the graph Laplacian\nregularization with respect to other classical regularizations for\nhyperspectral unmixing.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 14:11:43 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Ammanouil", "Rita", ""], ["Ferrari", "Andr\u00e9", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1410.3726", "submitter": "Chee Seng Chan", "authors": "Chern Hong Lim, Anhar Risnumawan and Chee Seng Chan", "title": "Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene\n  Understanding", "comments": "Accepted in IEEE Transactions on Fuzzy Systems", "journal-ref": "IEEE Transactions on Fuzzy Systems, vol. 22(6), pp. 1541 - 1556,\n  2014", "doi": "10.1109/TFUZZ.2014.2298233", "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity or uncertainty is a pervasive element of many real world decision\nmaking processes. Variation in decisions is a norm in this situation when the\nsame problem is posed to different subjects. Psychological and metaphysical\nresearch had proven that decision making by human is subjective. It is\ninfluenced by many factors such as experience, age, background, etc. Scene\nunderstanding is one of the computer vision problems that fall into this\ncategory. Conventional methods relax this problem by assuming scene images are\nmutually exclusive; and therefore, focus on developing different approaches to\nperform the binary classification tasks. In this paper, we show that scene\nimages are non-mutually exclusive, and propose the Fuzzy Qualitative Rank\nClassifier (FQRC) to tackle the aforementioned problems. The proposed FQRC\nprovides a ranking interpretation instead of binary decision. Evaluations in\nterm of qualitative and quantitative using large numbers and challenging public\nscene datasets have shown the effectiveness of our proposed method in modeling\nthe non-mutually exclusive scene images.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 15:19:43 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Lim", "Chern Hong", ""], ["Risnumawan", "Anhar", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3744", "submitter": "Chee Seng Chan", "authors": "Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso and Paolo Remagnino", "title": "Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking", "comments": "Accepted in Information Sciences, new abrupt motion (MAMo) dataset is\n  introduced", "journal-ref": "Information Sciences, vol. 283, pp. 267-287, 2014", "doi": "10.1016/j.ins.2014.01.003", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional tracking solutions are not feasible in handling abrupt motion as\nthey are based on smooth motion assumption or an accurate motion model. Abrupt\nmotion is not subject to motion continuity and smoothness. To assuage this, we\ndeem tracking as an optimisation problem and propose a novel abrupt motion\ntracker that based on swarm intelligence - the SwaTrack. Unlike existing\nswarm-based filtering methods, we first of all introduce an optimised\nswarm-based sampling strategy to tradeoff between the exploration and\nexploitation of the search space in search for the optimal proposal\ndistribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allow\non the fly tuning of the best mean and variance of the distribution for\nsampling. Such innovating idea of combining these strategies in an ingenious\nway in the PSO framework to handle the abrupt motion, which so far no existing\nworks are found. Experimental results in both quantitative and qualitative had\nshown the effectiveness of the proposed method in tracking abrupt motions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:06:13 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Lim", "Mei Kuan", ""], ["Chan", "Chee Seng", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1410.3748", "submitter": "Chee Seng Chan", "authors": "Wai Lam Hoo and Chee Seng Chan", "title": "Zero-Shot Object Recognition System based on Topic Model", "comments": "To appear in IEEE Transactions on Human-Machine Systems", "journal-ref": null, "doi": "10.1109/THMS.2014.2358649", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition systems usually require fully complete manually labeled\ntraining data to train the classifier. In this paper, we study the problem of\nobject recognition where the training samples are missing during the classifier\nlearning stage, a task also known as zero-shot learning. We propose a novel\nzero-shot learning strategy that utilizes the topic model and hierarchical\nclass concept. Our proposed method advanced where cumbersome human annotation\nstage (i.e. attribute-based classification) is eliminated. We achieve\ncomparable performance with state-of-the-art algorithms in four public\ndatasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), and\nAnimals with Attributes (49.65%) when unseen classes exist in the\nclassification task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:11:43 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Hoo", "Wai Lam", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3751", "submitter": "Chee Seng Chan", "authors": "Wei Ren Tan, Chee Seng Chan, Pratheepan Yogarajah and Joan Condell", "title": "A Fusion Approach for Efficient Human Skin Detection", "comments": "Accepted in IEEE Transactions on Industrial Informatics, vol. 8(1),\n  pp. 138-147, new skin detection + ground truth (Pratheepan) dataset", "journal-ref": "IEEE Transactions on Industrial Informatics, vol. 8(1), pp.\n  138-147, 2012", "doi": "10.1109/TII.2011.2172451", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable human skin detection method that is adaptable to different human\nskin colours and illu- mination conditions is essential for better human skin\nsegmentation. Even though different human skin colour detection solutions have\nbeen successfully applied, they are prone to false skin detection and are not\nable to cope with the variety of human skin colours across different ethnic.\nMoreover, existing methods require high computational cost. In this paper, we\npropose a novel human skin de- tection approach that combines a smoothed 2D\nhistogram and Gaussian model, for automatic human skin detection in colour\nimage(s). In our approach an eye detector is used to refine the skin model for\na specific person. The proposed approach reduces computational costs as no\ntraining is required; and it improves the accuracy of skin detection despite\nwide variation in ethnicity and illumination. To the best of our knowledge,\nthis is the first method to employ fusion strategy for this purpose.\nQualitative and quantitative results on three standard public datasets and a\ncomparison with state-of-the-art methods have shown the effectiveness and\nrobustness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:12:58 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Tan", "Wei Ren", ""], ["Chan", "Chee Seng", ""], ["Yogarajah", "Pratheepan", ""], ["Condell", "Joan", ""]]}, {"id": "1410.3752", "submitter": "Chee Seng Chan", "authors": "Wai Lam Hoo, Tae-Kyun Kim, Yuru Pei and Chee Seng Chan", "title": "Enhanced Random Forest with Image/Patch-Level Learning for Image\n  Understanding", "comments": "Accepted in ICPR 2014 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image understanding is an important research domain in the computer vision\ndue to its wide real-world applications. For an image understanding framework\nthat uses the Bag-of-Words model representation, the visual codebook is an\nessential part. Random forest (RF) as a tree-structure discriminative codebook\nhas been a popular choice. However, the performance of the RF can be degraded\nif the local patch labels are poorly assigned. In this paper, we tackle this\nproblem by a novel way to update the RF codebook learning for a more\ndiscriminative codebook with the introduction of the soft class labels,\nestimated from the pLSA model based on a feedback scheme. The feedback scheme\nis performed on both the image and patch levels respectively, which is in\ncontrast to the state- of-the-art RF codebook learning that focused on either\nimage or patch level only. Experiments on 15-Scene and C-Pascal datasets had\nshown the effectiveness of the proposed method in image understanding task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:13:45 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Hoo", "Wai Lam", ""], ["Kim", "Tae-Kyun", ""], ["Pei", "Yuru", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3756", "submitter": "Chee Seng Chan", "authors": "Mei Kuan Lim, Ven Jyn Kok, Chen Change Loy and Chee Seng Chan", "title": "Crowd Saliency Detection via Global Similarity Structure", "comments": "Accepted in ICPR 2014 (Oral). Mei Kuan Lim and Ven Jyn Kok share\n  equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for CCTV operators to overlook inter- esting events taking place\nwithin the crowd due to large number of people in the crowded scene (i.e.\nmarathon, rally). Thus, there is a dire need to automate the detection of\nsalient crowd regions acquiring immediate attention for a more effective and\nproactive surveillance. This paper proposes a novel framework to identify and\nlocalize salient regions in a crowd scene, by transforming low-level features\nextracted from crowd motion field into a global similarity structure. The\nglobal similarity structure representation allows the discovery of the\nintrinsic manifold of the motion dynamics, which could not be captured by the\nlow-level representation. Ranking is then performed on the global similarity\nstructure to identify a set of extrema. The proposed approach is unsupervised\nso learning stage is eliminated. Experimental results on public datasets\ndemonstrates the effectiveness of exploiting such extrema in identifying\nsalient regions in various crowd scenarios that exhibit crowding, local\nirregular motion, and unique motion areas such as sources and sinks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:24:24 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Lim", "Mei Kuan", ""], ["Kok", "Ven Jyn", ""], ["Loy", "Chen Change", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3905", "submitter": "Xiankai Lu", "authors": "Xiankai Lu, Zheng Fang, Tao Xu, Haiting Zhang, Hongya Tuo", "title": "Efficient Image Categorization with Sparse Fisher Vector", "comments": "5pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In object recognition, Fisher vector (FV) representation is one of the\nstate-of-art image representations ways at the expense of dense, high\ndimensional features and increased computation time. A simplification of FV is\nattractive, so we propose Sparse Fisher vector (SFV). By incorporating locality\nstrategy, we can accelerate the Fisher coding step in image categorization\nwhich is implemented from a collective of local descriptors. Combining with\npooling step, we explore the relationship between coding step and pooling step\nto give a theoretical explanation about SFV. Experiments on benchmark datasets\nhave shown that SFV leads to a speedup of several-fold of magnitude compares\nwith FV, while maintaining the categorization performance. In addition, we\ndemonstrate how SFV preserves the consistence in representation of similar\nlocal features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 02:00:29 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Lu", "Xiankai", ""], ["Fang", "Zheng", ""], ["Xu", "Tao", ""], ["Zhang", "Haiting", ""], ["Tuo", "Hongya", ""]]}, {"id": "1410.3910", "submitter": "Xiankai Lu", "authors": "Wenya Zhu, Xiankai Lu, Tao Xu, Ziyi Zhao", "title": "High Order Structure Descriptors for Scene Images", "comments": "5 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Structure information is ubiquitous in natural scene images and it plays an\nimportant role in scene representation. In this paper, third order structure\nstatistics (TOSS) and fourth order structure statistics (FOSS) are exploited to\nencode higher order structure information. Afterwards, based on the radial and\nnormal slice of TOSS and FOSS, we propose the high order structure feature:\nthird order structure feature (TOSF) and fourth order structure feature (FOSF).\nIt is well known that scene images are well characterized by particular\narrangements of their local structures, we divide the scene image into the\nnon-overlapping sub-regions and compute the proposed higher order structural\nfeatures among them. Then a scene classification is performed by using SVM\nclassifier with these higher order structure features. The experimental results\nshow that higher order structure statistics can deliver image structure\ninformation well and its spatial envelope has strong discriminative ability.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 02:42:09 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Zhu", "Wenya", ""], ["Lu", "Xiankai", ""], ["Xu", "Tao", ""], ["Zhao", "Ziyi", ""]]}, {"id": "1410.3932", "submitter": "Mei Kuan Lim", "authors": "Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino", "title": "Detection of Salient Regions in Crowded Scenes", "comments": "Accepted in Electronics Letters Vol. 5, Issue 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing number of cameras and a handful of human operators to monitor\nthe video inputs from hundreds of cameras leave the system ill equipped to\nfulfil the task of detecting anomalies. Thus, there is a dire need to\nautomatically detect regions that require immediate attention for a more\neffective and proactive surveillance. We propose a framework that utilises the\ntemporal variations in the flow field of a crowd scene to automatically detect\nsalient regions, while eliminating the need to have prior knowledge of the\nscene or training. We deem the flow fields to be a dynamic system and adopt the\nstability theory of dynamical systems, to determine the motion dynamics within\na given area. In the context of this work, salient regions refer to areas with\nhigh motion dynamics, where points in a particular region are unstable.\nExperimental results on public, crowd scenes have shown the effectiveness of\nthe proposed method in detecting salient regions which correspond to unstable\nflow, occlusions, bottlenecks, entries and exits.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 05:24:16 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Lim", "Mei Kuan", ""], ["Chan", "Chee Seng", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1410.3970", "submitter": "Haythem Ghazouani", "authors": "Haythem Ghazouani", "title": "Shape and Color Object Tracking for Real-Time Robotic Navigation", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a real-time approach for single-colored ball detection\nand tracking. The approach consists of two main phases. In a first offline\ncalibration phase, the intrinsic parameters of the camera and the radial\ndistortion are estimated, and a classification of colors is learned from a\nsample image of colored balls. The second phase consists of four main steps:\n(1) color segmentation of the input image into several regions based on the\noffline classification, (2) robust estimation of the circle parameters (3)\nrefinement of the circle parameters, and (4) ball tracking. The experimental\nresults showed that the approach presents a good compromise between suitability\nfor real-time navigation and robustness to occlusions, background congestion\nand colors interference in the scene.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 08:42:25 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Ghazouani", "Haythem", ""]]}, {"id": "1410.4012", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, S. Dey, K. Mukherjee, T. S. Jana", "title": "Online interpretation of numeric sign language using 2-d skeletal model", "comments": null, "journal-ref": "Proc. of Intl. Conf. on Communication Devices and Intelligent\n  Systems (CODIS), pp. 570-573, Jan-2004, Kolkata", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesturing is one of the natural modes of human communication. Signs produced\nby gestures can have a basic meaning coupled with additional information that\nis layered over the basic meaning of the sign. Sign language is an important\nexample of communicative gestures that are highly structured and well accepted\nacross the world as a communication medium for deaf and dumb. In this paper, an\nonline recognition scheme is proposed to interpret the standard numeric sign\nlanguage comprising of 10 basic hand symbols. A web camera is used to capture\nthe real time hand movements as input to the system. The basic meaning of the\nhand gesture is extracted from the input data frame by analysing the shape of\nthe hand, considering its orientation, movement and location to be fixed. The\ninput hand shape is processed to identify the palm structure, fingertips and\ntheir relative positions and the presence of the extended thumb. A\n2-dimensional skeletal model is generated from the acquired shape information\nto represent and subsequently interpret the basic meaning of the hand gesture.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 11:18:55 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Basu", "Subhadip", ""], ["Dey", "S.", ""], ["Mukherjee", "K.", ""], ["Jana", "T. S.", ""]]}, {"id": "1410.4013", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, Mahantapas Kundu, Mita Nasipuri and Dipak Kumar Basu", "title": "A two-pass fuzzy-geno approach to pattern classification", "comments": null, "journal-ref": "Proc. of International Conference on Computer Processing of\n  Bangla, pp. 130-134, Feb-2006, Dhaka", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presents an extension of the fuzzy approach to 2-D shape recognition\n[1] through refinement of initial or coarse classification decisions under a\ntwo pass approach. In this approach, an unknown pattern is classified by\nrefining possible classification decisions obtained through coarse\nclassification of the same. To build a fuzzy model of a pattern class\nhorizontal and vertical fuzzy partitions on the sample images of the class are\noptimized using genetic algorithm. To make coarse classification decisions\nabout an unknown pattern, the fuzzy representation of the pattern is compared\nwith models of all pattern classes through a specially designed similarity\nmeasure. Coarse classification decisions are refined in the second pass to\nobtain the final classification decision of the unknown pattern. To do so,\noptimized horizontal and vertical fuzzy partitions are again created on certain\nregions of the image frame, specific to each group of similar type of pattern\nclasses. It is observed through experiments that the technique improves the\noverall recognition rate from 86.2%, in the first pass, to 90.4% after the\nsecond pass, with 500 training samples of handwritten digits.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 11:19:33 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Basu", "Subhadip", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1410.4017", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, S. Chakraborty, K. Mukherjee, S. K. Pandit", "title": "Online Tracking of Skin Colour Regions Against a Complex Background", "comments": null, "journal-ref": "Proc. of IEEE INDICON, pp. 184-186, Dec-2004, Kharagpur", "doi": "10.1109/INDICO.2004.1497734", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online tracking of human activity against a complex background is a\nchallenging task for many applications. In this paper, we have developed a\nrobust technique for localizing skin colour regions from unconstrained image\nframes. A simple and fast segmentation algorithm is used to train a multiplayer\nperceptron (MLP) for detection of skin colours. Stepper motors are synchronized\nwith the MLP to track the movement of the skin colour regions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 11:24:55 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Basu", "Subhadip", ""], ["Chakraborty", "S.", ""], ["Mukherjee", "K.", ""], ["Pandit", "S. K.", ""]]}, {"id": "1410.4341", "submitter": "Manasij Venkatesh", "authors": "Manasij Venkatesh, Vikas Majjagi, and Deepu Vijayasenan", "title": "Implicit segmentation of Kannada characters in offline handwriting\n  recognition using hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for classification of handwritten Kannada characters\nusing Hidden Markov Models (HMMs). Kannada script is agglutinative, where\nsimple shapes are concatenated horizontally to form a character. This results\nin a large number of characters making the task of classification difficult.\nCharacter segmentation plays a significant role in reducing the number of\nclasses. Explicit segmentation techniques suffer when overlapping shapes are\npresent, which is common in the case of handwritten text. We use HMMs to take\nadvantage of the agglutinative nature of Kannada script, which allows us to\nperform implicit segmentation of characters along with recognition. All the\nexperiments are performed on the Chars74k dataset that consists of 657\nhandwritten characters collected across multiple users. Gradient-based features\nare extracted from individual characters and are used to train character HMMs.\nThe use of implicit segmentation technique at the character level resulted in\nan improvement of around 10%. This system also outperformed an existing system\ntested on the same dataset by around 16%. Analysis based on learning curves\nshowed that increasing the training data could result in better accuracy.\nAccordingly, we collected additional data and obtained an improvement of 4%\nwith 6 additional samples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 09:09:45 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Venkatesh", "Manasij", ""], ["Majjagi", "Vikas", ""], ["Vijayasenan", "Deepu", ""]]}, {"id": "1410.4393", "submitter": "Christopher Herbon", "authors": "Christopher Herbon", "title": "The HAWKwood Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a database consisting of wood pile images, which can be used as a\nbenchmark to evaluate the performance of wood pile detection and surveying\nalgorithms. We distinguish six database cate- gories which can be used for\ndifferent types of algorithms. Images of real and synthetic scenes are\nprovided, which consist of 7655 images divided into 354 data sets. Depending on\nthe category the data sets either include ground truth data or forestry\nspecific measurements with which algorithms may be compared.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 12:25:50 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 07:07:30 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Herbon", "Christopher", ""]]}, {"id": "1410.4441", "submitter": "Ariyan Zarei", "authors": "Ariyan Zarei", "title": "Improve CAPTCHA's Security Using Gaussian Blur Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing security for webservers against unwanted and automated\nregistrations has become a big concern. To prevent these kinds of false\nregistrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based\nor visual CAPTCHAs are very common. Actually visual CAPTCHA is an image\ncontaining a sequence of characters. So far most of visual CAPTCHAs, in order\nto resist against OCR programs, use some common implementations such as\nwrapping the characters, random placement and rotations of characters, etc. In\nthis paper we applied Gaussian Blur filter, which is an image transformation,\nto visual CAPTCHAs to reduce their readability by OCR programs. We concluded\nthat this technique made CAPTCHAs almost unreadable for OCR programs but, their\nreadability by human users still remained high.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 14:17:21 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Zarei", "Ariyan", ""]]}, {"id": "1410.4470", "submitter": "Raviteja Vemulapalli", "authors": "Raviteja Vemulapalli, Vinay Praneeth Boda, and Rama Chellappa", "title": "MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, automatic selection or combination of kernels (or\nfeatures) based on multiple kernel learning (MKL) approaches has been receiving\nsignificant attention from various research communities. Though MKL has been\nextensively studied in the context of support vector machines (SVM), it is\nrelatively less explored for ratio-trace problems. In this paper, we show that\nMKL can be formulated as a convex optimization problem for a general class of\nratio-trace problems that encompasses many popular algorithms used in various\ncomputer vision applications. We also provide an optimization procedure that is\nguaranteed to converge to the global optimum of the proposed optimization\nproblem. We experimentally demonstrate that the proposed MKL approach, which we\nrefer to as MKL-RT, can be successfully used to select features for\ndiscriminative dimensionality reduction and cross-modal retrieval. We also show\nthat the proposed convex MKL-RT approach performs better than the recently\nproposed non-convex MKL-DR approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 15:51:50 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 06:12:37 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Vemulapalli", "Raviteja", ""], ["Boda", "Vinay Praneeth", ""], ["Chellappa", "Rama", ""]]}, {"id": "1410.4485", "submitter": "Miguel \\'Angel Bautista Martin", "authors": "Miguel \\'Angel Bautista, Antonio Hern\\'andez-Vela, Sergio Escalera,\n  Laura Igual, Oriol Pujol, Josep Moya, Ver\\'onica Violant, Mar\\'ia Teresa\n  Anguera", "title": "A Gesture Recognition System for Detecting Behavioral Patterns of ADHD", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application of gesture recognition using an extension of\nDynamic Time Warping (DTW) to recognize behavioural patterns of Attention\nDeficit Hyperactivity Disorder (ADHD). We propose an extension of DTW using\none-class classifiers in order to be able to encode the variability of a\ngesture category, and thus, perform an alignment between a gesture sample and a\ngesture class. We model the set of gesture samples of a certain gesture\ncategory using either GMMs or an approximation of Convex Hulls. Thus, we add a\ntheoretical contribution to classical warping path in DTW by including local\nmodeling of intra-class gesture variability. This methodology is applied in a\nclinical context, detecting a group of ADHD behavioural patterns defined by\nexperts in psychology/psychiatry, to provide support to clinicians in the\ndiagnose procedure. The proposed methodology is tested on a novel multi-modal\ndataset (RGB plus Depth) of ADHD children recordings with behavioural patterns.\nWe obtain satisfying results when compared to standard state-of-the-art\napproaches in the DTW context.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 16:25:29 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 10:25:13 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Bautista", "Miguel \u00c1ngel", ""], ["Hern\u00e1ndez-Vela", "Antonio", ""], ["Escalera", "Sergio", ""], ["Igual", "Laura", ""], ["Pujol", "Oriol", ""], ["Moya", "Josep", ""], ["Violant", "Ver\u00f3nica", ""], ["Anguera", "Mar\u00eda Teresa", ""]]}, {"id": "1410.4521", "submitter": "Michael Maire", "authors": "Michael Maire and Stella X. Yu and Pietro Perona", "title": "Reconstructive Sparse Code Transfer for Contour Detection and Semantic\n  Labeling", "comments": "to appear in Asian Conference on Computer Vision (ACCV), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame the task of predicting a semantic labeling as a sparse\nreconstruction procedure that applies a target-specific learned transfer\nfunction to a generic deep sparse code representation of an image. This\nstrategy partitions training into two distinct stages. First, in an\nunsupervised manner, we learn a set of generic dictionaries optimized for\nsparse coding of image patches. We train a multilayer representation via\nrecursive sparse dictionary learning on pooled codes output by earlier layers.\nSecond, we encode all training images with the generic dictionaries and learn a\ntransfer function that optimizes reconstruction of patches extracted from\nannotated ground-truth given the sparse codes of their corresponding image\npatches. At test time, we encode a novel image using the generic dictionaries\nand then reconstruct using the transfer function. The output reconstruction is\na semantic labeling of the test image.\n  Applying this strategy to the task of contour detection, we demonstrate\nperformance competitive with state-of-the-art systems. Unlike almost all prior\nwork, our approach obviates the need for any form of hand-designed features or\nfilters. To illustrate general applicability, we also show initial results on\nsemantic part labeling of human faces.\n  The effectiveness of our approach opens new avenues for research on deep\nsparse representations. Our classifiers utilize this representation in a novel\nmanner. Rather than acting on nodes in the deepest layer, they attach to nodes\nalong a slice through multiple layers of the network in order to make\npredictions about local patches. Our flexible combination of a generatively\nlearned sparse representation with discriminatively trained transfer\nclassifiers extends the notion of sparse reconstruction to encompass arbitrary\nsemantic labeling tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 18:09:58 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Maire", "Michael", ""], ["Yu", "Stella X.", ""], ["Perona", "Pietro", ""]]}, {"id": "1410.4627", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Hamed Pirsiavash, Aude Oliva, Antonio Torralba", "title": "Learning visual biases from human imagination", "comments": "To appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the human visual system can recognize many concepts under\nchallenging conditions, it still has some biases. In this paper, we investigate\nwhether we can extract these biases and transfer them into a machine\nrecognition system. We introduce a novel method that, inspired by well-known\ntools in human psychophysics, estimates the biases that the human visual system\nmight use for recognition, but in computer vision feature spaces. Our\nexperiments are surprising, and suggest that classifiers from the human visual\nsystem can be transferred into a machine with some success. Since these\nclassifiers seem to capture favorable biases in the human visual system, we\nfurther present an SVM formulation that constrains the orientation of the SVM\nhyperplane to agree with the bias from human visual system. Our results suggest\nthat transferring this human bias into machines may help object recognition\nsystems generalize across datasets and perform better when very little training\ndata is available.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 03:47:12 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 14:14:22 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1410.4650", "submitter": "Yilun Wang", "authors": "Yilun Wang and Junjie Zheng and Sheng Zhang and Xujun Duan and Huafu\n  Chen", "title": "Randomized Structural Sparsity via Constrained Block Subsampling for\n  Improved Sensitivity of Discriminative Voxel Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider voxel selection for functional Magnetic Resonance\nImaging (fMRI) brain data with the aim of finding a more complete set of\nprobably correlated discriminative voxels, thus improving interpretation of the\ndiscovered potential biomarkers. The main difficulty in doing this is an\nextremely high dimensional voxel space and few training samples, resulting in\nunreliable feature selection. In order to deal with the difficulty, stability\nselection has received a great deal of attention lately, especially due to its\nfinite sample control of false discoveries and transparent principle for\nchoosing a proper amount of regularization. However, it fails to make explicit\nuse of the correlation property or structural information of these\ndiscriminative features and leads to large false negative rates. In other\nwords, many relevant but probably correlated discriminative voxels are missed.\nThus, we propose a new variant on stability selection \"randomized structural\nsparsity\", which incorporates the idea of structural sparsity. Numerical\nexperiments demonstrate that our method can be superior in controlling for\nfalse negatives while also keeping the control of false positives inherited\nfrom stability selection.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 07:02:47 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 05:47:29 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Yilun", ""], ["Zheng", "Junjie", ""], ["Zhang", "Sheng", ""], ["Duan", "Xujun", ""], ["Chen", "Huafu", ""]]}, {"id": "1410.4673", "submitter": "Zhiding Yu", "authors": "Weiyang Liu, Zhiding Yu, Lijia Lu, Yandong Wen, Hui Li and Yuexian Zou", "title": "KCRC-LCD: Discriminative Kernel Collaborative Representation with\n  Locality Constrained Dictionary for Visual Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the image classification problem via kernel collaborative\nrepresentation classification with locality constrained dictionary (KCRC-LCD).\nSpecifically, we propose a kernel collaborative representation classification\n(KCRC) approach in which kernel method is used to improve the discrimination\nability of collaborative representation classification (CRC). We then measure\nthe similarities between the query and atoms in the global dictionary in order\nto construct a locality constrained dictionary (LCD) for KCRC. In addition, we\ndiscuss several similarity measure approaches in LCD and further present a\nsimple yet effective unified similarity measure whose superiority is validated\nin experiments. There are several appealing aspects associated with LCD. First,\nLCD can be nicely incorporated under the framework of KCRC. The LCD similarity\nmeasure can be kernelized under KCRC, which theoretically links CRC and LCD\nunder the kernel method. Second, KCRC-LCD becomes more scalable to both the\ntraining set size and the feature dimension. Example shows that KCRC is able to\nperfectly classify data with certain distribution, while conventional CRC fails\ncompletely. Comprehensive experiments on many public datasets also show that\nKCRC-LCD is a robust discriminative classifier with both excellent performance\nand good scalability, being comparable or outperforming many other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 09:40:20 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Liu", "Weiyang", ""], ["Yu", "Zhiding", ""], ["Lu", "Lijia", ""], ["Wen", "Yandong", ""], ["Li", "Hui", ""], ["Zou", "Yuexian", ""]]}, {"id": "1410.4688", "submitter": "Ibrahim Abdelaziz", "authors": "Ibrahim Abdelaziz and Sherif Abdou and Hassanin Al-Barhamtoshy", "title": "Large Vocabulary Arabic Online Handwriting Recognition System", "comments": "Preprint submitted to Pattern Analysis and Applications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic handwriting is a consonantal and cursive writing. The analysis of\nArabic script is further complicated due to obligatory dots/strokes that are\nplaced above or below most letters and usually written delayed in order. Due to\nambiguities and diversities of writing styles, recognition systems are\ngenerally based on a set of possible words called lexicon. When the lexicon is\nsmall, recognition accuracy is more important as the recognition time is\nminimal. On the other hand, recognition speed as well as the accuracy are both\ncritical when handling large lexicons. Arabic is rich in morphology and syntax\nwhich makes its lexicon large. Therefore, a practical online handwriting\nrecognition system should be able to handle a large lexicon with reasonable\nperformance in terms of both accuracy and time. In this paper, we introduce a\nfully-fledged Hidden Markov Model (HMM) based system for Arabic online\nhandwriting recognition that provides solutions for most of the difficulties\ninherent in recognizing the Arabic script. A new preprocessing technique for\nhandling the delayed strokes is introduced. We use advanced modeling techniques\nfor building our recognition system from the training data to provide more\ndetailed representation for the differences between the writing units, minimize\nthe variances between writers in the training data and have a better\nrepresentation for the features space. System results are enhanced using an\nadditional post-processing step with a higher order language model and\ncross-word HMM models. The system performance is evaluated using two different\ndatabases covering small and large lexicons. Our system outperforms the\nstate-of-art systems for the small lexicon database. Furthermore, it shows\npromising results (accuracy and time) when supporting large lexicon with the\npossibility for adapting the models for specific writers to get even better\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 11:09:35 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 14:31:48 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2015 09:47:28 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Abdelaziz", "Ibrahim", ""], ["Abdou", "Sherif", ""], ["Al-Barhamtoshy", "Hassanin", ""]]}, {"id": "1410.4871", "submitter": "Herwig Wendt", "authors": "S\\'ebastien Combrexelle, Herwig Wendt, Nicolas Dobigeon, Jean-Yves\n  Tourneret, Steve McLaughlin and Patrice Abry", "title": "Bayesian estimation of the multifractality parameter for image texture\n  using a Whittle approximation", "comments": null, "journal-ref": "IEEE T. Image Proces., vol. 24, no. 8, pp. 2540-2551, Aug. 2015", "doi": "10.1109/TIP.2015.2426021", "report-no": null, "categories": "physics.data-an cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture characterization is a central element in many image processing\napplications. Multifractal analysis is a useful signal and image processing\ntool, yet, the accurate estimation of multifractal parameters for image texture\nremains a challenge. This is due in the main to the fact that current\nestimation procedures consist of performing linear regressions across frequency\nscales of the two-dimensional (2D) dyadic wavelet transform, for which only a\nfew such scales are computable for images. The strongly non-Gaussian nature of\nmultifractal processes, combined with their complicated dependence structure,\nmakes it difficult to develop suitable models for parameter estimation. Here,\nwe propose a Bayesian procedure that addresses the difficulties in the\nestimation of the multifractality parameter. The originality of the procedure\nis threefold: The construction of a generic semi-parametric statistical model\nfor the logarithm of wavelet leaders; the formulation of Bayesian estimators\nthat are associated with this model and the set of parameter values admitted by\nmultifractal theory; the exploitation of a suitable Whittle approximation\nwithin the Bayesian model which enables the otherwise infeasible evaluation of\nthe posterior distribution associated with the model. Performance is assessed\nnumerically for several 2D multifractal processes, for several image sizes and\na large range of process parameters. The procedure yields significant benefits\nover current benchmark estimators in terms of estimation performance and\nability to discriminate between the two most commonly used classes of\nmultifractal process models. The gains in performance are particularly\npronounced for small image sizes, notably enabling for the first time the\nanalysis of image patches as small as 64x64 pixels.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 21:32:28 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 09:13:28 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Combrexelle", "S\u00e9bastien", ""], ["Wendt", "Herwig", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["McLaughlin", "Steve", ""], ["Abry", "Patrice", ""]]}, {"id": "1410.5058", "submitter": "Syed Zulqarnain Gilani", "authors": "Syed Zulqarnain Gilani, Ajmal Mian, Faisal Shafait and Ian Reid", "title": "Dense 3D Face Correspondence", "comments": "24 Pages, 12 Figures, 6 Tables and 3 Algorithms", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  40(7), 2017", "doi": "10.1109/TPAMI.2017.2725279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that automatically establishes dense correspondences\nbetween a large number of 3D faces. Starting from automatically detected sparse\ncorrespondences on the outer boundary of 3D faces, the algorithm triangulates\nexisting correspondences and expands them iteratively by matching points of\ndistinctive surface curvature along the triangle edges. After exhausting\nkeypoint matches, further correspondences are established by generating evenly\ndistributed points within triangles by evolving level set geodesic curves from\nthe centroids of large triangles. A deformable model (K3DM) is constructed from\nthe dense corresponded faces and an algorithm is proposed for morphing the K3DM\nto fit unseen faces. This algorithm iterates between rigid alignment of an\nunseen face followed by regularized morphing of the deformable model. We have\nextensively evaluated the proposed algorithms on synthetic data and real 3D\nfaces from the FRGCv2, Bosphorus, BU3DFE and UND Ear databases using\nquantitative and qualitative benchmarks. Our algorithm achieved dense\ncorrespondences with a mean localisation error of 1.28mm on synthetic faces and\ndetected $14$ anthropometric landmarks on unseen real faces from the FRGCv2\ndatabase with 3mm precision. Furthermore, our deformable model fitting\nalgorithm achieved 98.5% face recognition accuracy on the FRGCv2 and 98.6% on\nBosphorus database. Our dense model is also able to generalize to unseen\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 10:50:08 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 06:32:44 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Gilani", "Syed Zulqarnain", ""], ["Mian", "Ajmal", ""], ["Shafait", "Faisal", ""], ["Reid", "Ian", ""]]}, {"id": "1410.5224", "submitter": "Albert Gordo", "authors": "Albert Gordo", "title": "Supervised mid-level features for word image representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of learning word image representations:\ngiven the cropped image of a word, we are interested in finding a descriptive,\nrobust, and compact fixed-length representation. Machine learning techniques\ncan then be supplied with these representations to produce models useful for\nword retrieval or recognition tasks. Although many works have focused on the\nmachine learning aspect once a global representation has been produced, little\nwork has been devoted to the construction of those base image representations:\nmost works use standard coding and aggregation techniques directly on top of\nstandard computer vision features such as SIFT or HOG.\n  We propose to learn local mid-level features suitable for building word image\nrepresentations. These features are learnt by leveraging character bounding box\nannotations on a small set of training images. However, contrary to other\napproaches that use character bounding box information, our approach does not\nrely on detecting the individual characters explicitly at testing time. Our\nlocal mid-level features can then be aggregated to produce a global word image\nsignature. When pairing these features with the recent word attributes\nframework of Almaz\\'an et al., we obtain results comparable with or better than\nthe state-of-the-art on matching and recognition tasks using global descriptors\nof only 96 dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 10:38:56 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 16:57:00 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Gordo", "Albert", ""]]}, {"id": "1410.5263", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Guido Del Vescovo, Antonello Rizzi, Fabio Massimo\n  Frattale Mascioli", "title": "Building pattern recognition applications with the SPARE library", "comments": "Home page: https://sourceforge.net/p/libspare/home/Spare/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the SPARE C++ library, an open source software tool\nconceived to build pattern recognition and soft computing systems. The library\nfollows the requirement of the generality: most of the implemented algorithms\nare able to process user-defined input data types transparently, such as\nlabeled graphs and sequences of objects, as well as standard numeric vectors.\nHere we present a high-level picture of the SPARE library characteristics,\nfocusing instead on the specific practical possibility of constructing pattern\nrecognition systems for different input data types. In particular, as a proof\nof concept, we discuss two application instances involving clustering of\nreal-valued multidimensional sequences and classification of labeled graphs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 13:18:33 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 15:56:36 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Livi", "Lorenzo", ""], ["Del Vescovo", "Guido", ""], ["Rizzi", "Antonello", ""], ["Mascioli", "Fabio Massimo Frattale", ""]]}, {"id": "1410.5358", "submitter": "Claudio Cusano", "authors": "Claudio Cusano, Paolo Napoletano, Raimondo Schettini", "title": "Remote sensing image classification exploiting multiple kernel learning", "comments": "Accepted for publication on the IEEE Geoscience and Remote Sensing\n  letters", "journal-ref": null, "doi": "10.1109/LGRS.2015.2476365", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a strategy for land use classification which exploits Multiple\nKernel Learning (MKL) to automatically determine a suitable combination of a\nset of features without requiring any heuristic knowledge about the\nclassification task. We present a novel procedure that allows MKL to achieve\ngood performance in the case of small training sets. Experimental results on\npublicly available datasets demonstrate the feasibility of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 17:15:50 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 13:17:27 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2015 09:25:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Cusano", "Claudio", ""], ["Napoletano", "Paolo", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1410.5524", "submitter": "Jie Feng", "authors": "Jie Feng, Wei Liu, Yan Wang", "title": "Learning to Rank Binary Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary codes have been widely used in vision problems as a compact feature\nrepresentation to achieve both space and time advantages. Various methods have\nbeen proposed to learn data-dependent hash functions which map a feature vector\nto a binary code. However, considerable data information is inevitably lost\nduring the binarization step which also causes ambiguity in measuring sample\nsimilarity using Hamming distance. Besides, the learned hash functions cannot\nbe changed after training, which makes them incapable of adapting to new data\noutside the training data set. To address both issues, in this paper we propose\na flexible bitwise weight learning framework based on the binary codes obtained\nby state-of-the-art hashing methods, and incorporate the learned weights into\nthe weighted Hamming distance computation. We then formulate the proposed\nframework as a ranking problem and leverage the Ranking SVM model to offline\ntackle the weight learning. The framework is further extended to an online mode\nwhich updates the weights at each time new data comes, thereby making it\nscalable to large and dynamic data sets. Extensive experimental results\ndemonstrate significant performance gains of using binary codes with bitwise\nweighting in image retrieval tasks. It is appealing that the online weight\nlearning leads to comparable accuracy with its offline counterpart, which thus\nmakes our approach practical for realistic applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 03:09:17 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Feng", "Jie", ""], ["Liu", "Wei", ""], ["Wang", "Yan", ""]]}, {"id": "1410.5600", "submitter": "Ramviyas Nattanmai Parasuraman", "authors": "Ramviyas Parasuraman", "title": "Mobility Enhancement for Elderly", "comments": "Masters thesis, Indian Institute of Technology Delhi", "journal-ref": null, "doi": null, "report-no": "2008JID2945", "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss of Mobility is a common handicap to senior citizens. It denies them the\nease of movement they would like to have like outdoor visits, movement in\nhospitals, social outgoings, but more seriously in the day to day in-house\nroutine functions necessary for living etc. Trying to overcome this handicap by\nmeans of servant or domestic help and simple wheel chairs is not only costly in\nthe long run, but forces the senior citizen to be at the mercy of sincerity of\ndomestic helps and also the consequent loss of dignity. In order to give a\ndignified life, the mobility obtained must be at the complete discretion, will\nand control of the senior citizen. This can be provided only by a reasonably\nsophisticated and versatile wheel chair, giving enhanced ability of vision,\nhearing through man-machine interface, and sensor aided navigation and control.\nMore often than not senior people have poor vision which makes it difficult for\nthem to maker visual judgement and so calls for the use of Artificial\nIntelligence in visual image analysis and guided navigation systems.\n  In this project, we deal with two important enhancement features for mobility\nenhancement, Audio command and Vision aided obstacle detection and navigation.\nWe have implemented speech recognition algorithm using template of stored words\nfor identifying the voice command given by the user. This frees the user of an\nagile hand to operate joystick or mouse control. Also, we have developed a new\nappearance based obstacle detection system using stereo-vision cameras which\nestimates the distance of nearest obstacle to the wheel chair and takes\nnecessary action. This helps user in making better judgement of route and\nnavigate obstacles. The main challenge in this project is how to navigate in an\nunknown/unfamiliar environment by avoiding obstacles.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 09:59:33 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Parasuraman", "Ramviyas", ""]]}, {"id": "1410.5605", "submitter": "Paolo Napoletano", "authors": "Paolo Napoletano, Giuseppe Boccignone, Francesco Tisato", "title": "Attentive monitoring of multiple video streams driven by a Bayesian\n  foraging strategy", "comments": "Accepted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2431438", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we shall consider the problem of deploying attention to subsets\nof the video streams for collating the most relevant data and information of\ninterest related to a given task. We formalize this monitoring problem as a\nforaging problem. We propose a probabilistic framework to model observer's\nattentive behavior as the behavior of a forager. The forager, moment to moment,\nfocuses its attention on the most informative stream/camera, detects\ninteresting objects or activities, or switches to a more profitable stream. The\napproach proposed here is suitable to be exploited for multi-stream video\nsummarization. Meanwhile, it can serve as a preliminary step for more\nsophisticated video surveillance, e.g. activity and behavior analysis.\nExperimental results achieved on the UCR Videoweb Activities Dataset, a\npublicly available dataset, are presented to illustrate the utility of the\nproposed technique.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 10:13:51 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 11:21:47 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2015 13:02:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Napoletano", "Paolo", ""], ["Boccignone", "Giuseppe", ""], ["Tisato", "Francesco", ""]]}, {"id": "1410.5861", "submitter": "Ran Xu", "authors": "Ran Xu and Gang Chen and Caiming Xiong and Wei Chen and Jason J. Corso", "title": "Compositional Structure Learning for Action Understanding", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of the action understanding literature has predominately been\nclassification, how- ever, there are many applications demanding richer action\nunderstanding such as mobile robotics and video search, with solutions to\nclassification, localization and detection. In this paper, we propose a\ncompositional model that leverages a new mid-level representation called\ncompositional trajectories and a locally articulated spatiotemporal deformable\nparts model (LALSDPM) for fully action understanding. Our methods is\nadvantageous in capturing the variable structure of dynamic human activity over\na long range. First, the compositional trajectories capture long-ranging,\nfrequently co-occurring groups of trajectories in space time and represent them\nin discriminative hierarchies, where human motion is largely separated from\ncamera motion; second, LASTDPM learns a structured model with multi-layer\ndeformable parts to capture multiple levels of articulated motion. We implement\nour methods and demonstrate state of the art performance on all three problems:\naction detection, localization, and recognition.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 21:25:45 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Xu", "Ran", ""], ["Chen", "Gang", ""], ["Xiong", "Caiming", ""], ["Chen", "Wei", ""], ["Corso", "Jason J.", ""]]}, {"id": "1410.5894", "submitter": "Raad Hadi", "authors": "Raad Ahmed Hadi, Ghazali Sulong and Loay Edwar George", "title": "Vehicle Detection and Tracking Techniques: A Concise Review", "comments": null, "journal-ref": null, "doi": "10.5121/sipij.2013.5101", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection and tracking applications play an important role for\ncivilian and military applications such as in highway traffic surveillance\ncontrol, management and urban traffic planning. Vehicle detection process on\nroad are used for vehicle tracking, counts, average speed of each individual\nvehicle, traffic analysis and vehicle categorizing objectives and may be\nimplemented under different environments changes. In this review, we present a\nconcise overview of image processing methods and analysis tools which used in\nbuilding these previous mentioned applications that involved developing traffic\nsurveillance systems. More precisely and in contrast with other reviews, we\nclassified the processing methods under three categories for more clarification\nto explain the traffic systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 01:52:53 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Hadi", "Raad Ahmed", ""], ["Sulong", "Ghazali", ""], ["George", "Loay Edwar", ""]]}, {"id": "1410.5926", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang, Zejian Yuan, Ming-Ming Cheng, Yihong Gong, Nanning\n  Zheng, Jingdong Wang", "title": "Salient Object Detection: A Discriminative Regional Feature Integration\n  Approach", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-016-0977-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection has been attracting a lot of interest, and recently\nvarious heuristic computational models have been designed. In this paper, we\nformulate saliency map computation as a regression problem. Our method, which\nis based on multi-level image segmentation, utilizes the supervised learning\napproach to map the regional feature vector to a saliency score. Saliency\nscores across multiple levels are finally fused to produce the saliency map.\nThe contributions lie in two-fold. One is that we propose a discriminate\nregional feature integration approach for salient object detection. Compared\nwith existing heuristic models, our proposed method is able to automatically\nintegrate high-dimensional regional saliency features and choose discriminative\nones. The other is that by investigating standard generic region properties as\nwell as two widely studied concepts for salient object detection, i.e.,\nregional contrast and backgroundness, our approach significantly outperforms\nstate-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate\nthat our method runs as fast as most existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 07:05:38 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Jiang", "Huaizu", ""], ["Yuan", "Zejian", ""], ["Cheng", "Ming-Ming", ""], ["Gong", "Yihong", ""], ["Zheng", "Nanning", ""], ["Wang", "Jingdong", ""]]}, {"id": "1410.6126", "submitter": "German Ros", "authors": "German Ros and Jose Alvarez and Julio Guerrero", "title": "Motion Estimation via Robust Decomposition with Constrained Rank", "comments": "Submitted to IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of outlier detection for robust motion\nestimation by using modern sparse-low-rank decompositions, i.e., Robust\nPCA-like methods, to impose global rank constraints. Robust decompositions have\nshown to be good at splitting a corrupted matrix into an uncorrupted low-rank\nmatrix and a sparse matrix, containing outliers. However, this process only\nworks when matrices have relatively low rank with respect to their ambient\nspace, a property not met in motion estimation problems. As a solution, we\npropose to exploit the partial information present in the decomposition to\ndecide which matches are outliers. We provide evidences showing that even when\nit is not possible to recover an uncorrupted low-rank matrix, the resulting\ninformation can be exploited for outlier detection. To this end we propose the\nRobust Decomposition with Constrained Rank (RD-CR), a proximal gradient based\nmethod that enforces the rank constraints inherent to motion estimation. We\nalso present a general framework to perform robust estimation for stereo Visual\nOdometry, based on our RD-CR and a simple but effective compressed optimization\nmethod that achieves high performance. Our evaluation on synthetic data and on\nthe KITTI dataset demonstrates the applicability of our approach in complex\nscenarios and it yields state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 18:15:27 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Ros", "German", ""], ["Alvarez", "Jose", ""], ["Guerrero", "Julio", ""]]}, {"id": "1410.6264", "submitter": "Alessandro Perina", "authors": "Alessandro Perina and Nebojsa Jojic", "title": "Capturing spatial interdependence in image features: the counting grid,\n  an epitomic representation for bags of features", "comments": "The counting grid code is available at www.alessandroperina.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent scene recognition research images or large image regions are often\nrepresented as disorganized \"bags\" of features which can then be analyzed using\nmodels originally developed to capture co-variation of word counts in text.\nHowever, image feature counts are likely to be constrained in different ways\nthan word counts in text. For example, as a camera pans upwards from a building\nentrance over its first few floors and then further up into the sky Fig. 1,\nsome feature counts in the image drop while others rise -- only to drop again\ngiving way to features found more often at higher elevations. The space of all\npossible feature count combinations is constrained both by the properties of\nthe larger scene and the size and the location of the window into it. To\ncapture such variation, in this paper we propose the use of the counting grid\nmodel. This generative model is based on a grid of feature counts, considerably\nlarger than any of the modeled images, and considerably smaller than the real\nestate needed to tile the images next to each other tightly. Each modeled image\nis assumed to have a representative window in the grid in which the feature\ncounts mimic the feature distribution in the image. We provide a learning\nprocedure that jointly maps all images in the training set to the counting grid\nand estimates the appropriate local counts in it. Experimentally, we\ndemonstrate that the resulting representation captures the space of feature\ncount combinations more accurately than the traditional models, not only when\nthe input images come from a panning camera, but even when modeling images of\ndifferent scenes from the same category.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 07:04:22 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Perina", "Alessandro", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1410.6313", "submitter": "Junhua Li", "authors": "Junhua Li, Chao Li, and Andrzej Cichocki", "title": "Canonical Polyadic Decomposition with Auxiliary Information for Brain\n  Computer Interface", "comments": null, "journal-ref": "IEEE journal of biomedical and health informatics, 2015, 21(1):\n  263-271", "doi": "10.1109/JBHI.2015.2491645", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physiological signals are often organized in the form of multiple dimensions\n(e.g., channel, time, task, and 3D voxel), so it is better to preserve original\norganization structure when processing. Unlike vector-based methods that\ndestroy data structure, Canonical Polyadic Decomposition (CPD) aims to process\nphysiological signals in the form of multi-way array, which considers\nrelationships between dimensions and preserves structure information contained\nby the physiological signal. Nowadays, CPD is utilized as an unsupervised\nmethod for feature extraction in a classification problem. After that, a\nclassifier, such as support vector machine, is required to classify those\nfeatures. In this manner, classification task is achieved in two isolated\nsteps. We proposed supervised Canonical Polyadic Decomposition by directly\nincorporating auxiliary label information during decomposition, by which a\nclassification task can be achieved without an extra step of classifier\ntraining. The proposed method merges the decomposition and classifier learning\ntogether, so it reduces procedure of classification task compared with that of\nrespective decomposition and classification. In order to evaluate the\nperformance of the proposed method, three different kinds of signals, synthetic\nsignal, EEG signal, and MEG signal, were used. The results based on evaluations\nof synthetic and real signals demonstrated that the proposed method is\neffective and efficient.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 10:14:05 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 14:59:37 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Li", "Junhua", ""], ["Li", "Chao", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1410.6333", "submitter": "Yves van Gennip", "authors": "Yves van Gennip, Prashant Athavale, J\\'er\\^ome Gilles, Rustum Choksi", "title": "A Regularization Approach to Blind Deblurring and Denoising of QR\n  Barcodes", "comments": "14 pages, 19 figures (with a total of 57 subfigures), 1 table; v3:\n  previously missing reference [35] added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QR bar codes are prototypical images for which part of the image is a priori\nknown (required patterns). Open source bar code readers, such as ZBar, are\nreadily available. We exploit both these facts to provide and assess purely\nregularization-based methods for blind deblurring of QR bar codes in the\npresence of noise.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 12:04:31 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 11:10:32 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 14:51:09 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["van Gennip", "Yves", ""], ["Athavale", "Prashant", ""], ["Gilles", "J\u00e9r\u00f4me", ""], ["Choksi", "Rustum", ""]]}, {"id": "1410.6447", "submitter": "Ji Zhao", "authors": "Ji Zhao, Deyu Meng, Jiayi Ma", "title": "Density-Based Region Search with Arbitrary Shape for Object Localization", "comments": null, "journal-ref": null, "doi": "10.1049/iet-cvi.2014.0442", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region search is widely used for object localization. Typically, the region\nsearch methods project the score of a classifier into an image plane, and then\nsearch the region with the maximal score. The recently proposed region search\nmethods, such as efficient subwindow search and efficient region search, %which\nlocalize objects from the score distribution on an image are much more\nefficient than sliding window search. However, for some classifiers and tasks,\nthe projected scores are nearly all positive, and hence maximizing the score of\na region results in localizing nearly the entire images as objects, which is\nmeaningless.\n  In this paper, we observe that the large scores are mainly concentrated on or\naround objects. Based on this observation, we propose a method, named level set\nmaximum-weight connected subgraph (LS-MWCS), which localizes objects with\narbitrary shapes by searching regions with the densest score rather than the\nmaximal score. The region density can be controlled by a parameter flexibly.\nAnd we prove an important property of the proposed LS-MWCS, which guarantees\nthat the region with the densest score can be searched. Moreover, the LS-MWCS\ncan be efficiently optimized by belief propagation. The method is evaluated on\nthe problem of weakly-supervised object localization, and the quantitative\nresults demonstrate the superiorities of our LS-MWCS compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 18:41:11 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Zhao", "Ji", ""], ["Meng", "Deyu", ""], ["Ma", "Jiayi", ""]]}, {"id": "1410.6472", "submitter": "Ange Mika\\\"el Mousse", "authors": "Mika\\\"el A. Mousse, Eug\\`ene C. Ezin and Cina Motamed", "title": "Foreground-Background Segmentation Based on Codebook and Edge Detector", "comments": "to appear in the 10th International Conference on Signal Image\n  Technology & Internet Based Systems, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background modeling techniques are used for moving object detection in video.\nMany algorithms exist in the field of object detection with different purposes.\nIn this paper, we propose an improvement of moving object detection based on\ncodebook segmentation. We associate the original codebook algorithm with an\nedge detection algorithm. Our goal is to prove the efficiency of using an edge\ndetection algorithm with a background modeling algorithm. Throughout our study,\nwe compared the quality of the moving object detection when codebook\nsegmentation algorithm is associated with some standard edge detectors. In each\ncase, we use frame-based metrics for the evaluation of the detection. The\ndifferent results are presented and analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 19:48:11 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Mousse", "Mika\u00ebl A.", ""], ["Ezin", "Eug\u00e8ne C.", ""], ["Motamed", "Cina", ""]]}, {"id": "1410.6532", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuting Chen, Venkatesh Saligrama", "title": "A Novel Visual Word Co-occurrence Model for Person Re-identification", "comments": "Accepted at ECCV Workshop on Visual Surveillance and\n  Re-Identification, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to maintain the identity of an individual in\ndiverse locations through different non-overlapping camera views. The problem\nis fundamentally challenging due to appearance variations resulting from\ndiffering poses, illumination and configurations of camera views. To deal with\nthese difficulties, we propose a novel visual word co-occurrence model. We\nfirst map each pixel of an image to a visual word using a codebook, which is\nlearned in an unsupervised manner. The appearance transformation between camera\nviews is encoded by a co-occurrence matrix of visual word joint distributions\nin probe and gallery images. Our appearance model naturally accounts for\nspatial similarities and variations caused by pose, illumination &\nconfiguration change across camera views. Linear SVMs are then trained as\nclassifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campus\nbenchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on the\nCumulative Match Characteristic (CMC) curves, and beats the state-of-the-art\nresults by 10.44% and 22.27%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 01:04:37 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Zhang", "Ziming", ""], ["Chen", "Yuting", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1410.6736", "submitter": "Sheng Huang", "authors": "Sheng Huang and Ahmed Elgammal and Dan Yang", "title": "On The Effect of Hyperedge Weights On Hypergraph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph is a powerful representation in several computer vision, machine\nlearning and pattern recognition problems. In the last decade, many researchers\nhave been keen to develop different hypergraph models. In contrast, no much\nattention has been paid to the design of hyperedge weights. However, many\nstudies on pairwise graphs show that the choice of edge weight can\nsignificantly influence the performances of such graph algorithms. We argue\nthat this also applies to hypegraphs. In this paper, we empirically discuss the\ninfluence of hyperedge weight on hypegraph learning via proposing three novel\nhyperedge weights from the perspectives of geometry, multivariate statistical\nanalysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE,\nSheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar to\ngraph learning, several representative hyperedge weighting schemes can be\nconcluded by our experimental studies. Moreover, the experiments also\ndemonstrate that the combinations of such weighting schemes and conventional\nhypergraph models can get very promising classification and clustering\nperformances in comparison with some recent state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 16:52:41 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Huang", "Sheng", ""], ["Elgammal", "Ahmed", ""], ["Yang", "Dan", ""]]}, {"id": "1410.6751", "submitter": "Christoph Riedl", "authors": "Christoph Riedl, Richard Zanibbi, Marti A. Hearst, Siyu Zhu, Michael\n  Menietti, Jason Crusan, Ivan Metelsky, Karim R. Lakhani", "title": "Detecting Figures and Part Labels in Patents: Competition-Based\n  Development of Image Processing Algorithms", "comments": null, "journal-ref": null, "doi": "10.1007/s10032-016-0260-8", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the findings of a month-long online competition in which\nparticipants developed algorithms for augmenting the digital version of patent\ndocuments published by the United States Patent and Trademark Office (USPTO).\nThe goal was to detect figures and part labels in U.S. patent drawing pages.\nThe challenge drew 232 teams of two, of which 70 teams (30%) submitted\nsolutions. Collectively, teams submitted 1,797 solutions that were compiled on\nthe competition servers. Participants reported spending an average of 63 hours\ndeveloping their solutions, resulting in a total of 5,591 hours of development\ntime. A manually labeled dataset of 306 patents was used for training, online\nsystem tests, and evaluation. The design and performance of the top-5 systems\nare presented, along with a system developed after the competition which\nillustrates that winning teams produced near state-of-the-art results under\nstrict time and computation constraints. For the 1st place system, the harmonic\nmean of recall and precision (f-measure) was 88.57% for figure region\ndetection, 78.81% for figure regions with correctly recognized figure titles,\nand 70.98% for part label detection and character recognition. Data and\nsoftware from the competition are available through the online UCI Machine\nLearning repository to inspire follow-on work by the image processing\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 17:45:36 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 10:54:17 GMT"}, {"version": "v3", "created": "Tue, 11 Nov 2014 14:33:11 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Riedl", "Christoph", ""], ["Zanibbi", "Richard", ""], ["Hearst", "Marti A.", ""], ["Zhu", "Siyu", ""], ["Menietti", "Michael", ""], ["Crusan", "Jason", ""], ["Metelsky", "Ivan", ""], ["Lakhani", "Karim R.", ""]]}, {"id": "1410.6909", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sunil Kumar Kopparapu and Lajish V. L", "title": "A Framework for On-Line Devanagari Handwritten Character Recognition", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge in on-line handwritten character recognition in Indian\nlan- guage is the large size of the character set, larger similarity between\ndifferent characters in the script and the huge variation in writing style. In\nthis paper we propose a framework for on-line handwitten script recognition\ntaking cues from speech signal processing literature. The framework is based on\nidentify- ing strokes, which in turn lead to recognition of handwritten on-line\ncharacters rather that the conventional character identification. Though the\nframework is described for Devanagari script, the framework is general and can\nbe applied to any language.\n  The proposed platform consists of pre-processing, feature extraction, recog-\nnition and post processing like the conventional character recognition but ap-\nplied to strokes. The on-line Devanagari character recognition reduces to one\nof recognizing one of 69 primitives and recognition of a character is performed\nby recognizing a sequence of such primitives. We further show the impact of\nnoise removal on on-line raw data which is usually noisy. The use of Fuzzy\nDirec- tional Features to enhance the accuracy of stroke recognition is also\ndescribed. The recognition results are compared with commonly used directional\nfeatures in literature using several classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 10:13:02 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Kopparapu", "Sunil Kumar", ""], ["L", "Lajish V.", ""]]}, {"id": "1410.6996", "submitter": "Musa Maharramov", "authors": "Musa Maharramov and Biondo Biondi", "title": "Improved depth imaging by constrained full-waveform inversion", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "SEP 155", "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formulation of full-wavefield inversion (FWI) as a constrained\noptimization problem, and describe a computationally efficient technique for\nsolving constrained full-wavefield inversion (CFWI). The technique is based on\nusing a total-variation regularization method, with the regularization weighted\nin favor of constraining deeper subsurface model sections. The method helps to\npromote \"edge-preserving\" blocky model inversion where fitting the seismic data\nalone fails to adequately constrain the model. The method is demonstrated on\nsynthetic datasets with added noise, and is shown to enhance the sharpness of\nthe inverted model and correctly reposition mispositioned reflectors by better\nconstraining the velocity model at depth.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 08:02:01 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Maharramov", "Musa", ""], ["Biondi", "Biondo", ""]]}, {"id": "1410.7100", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou", "title": "Estimating the intrinsic dimension in fMRI space via dataset fractal\n  analysis - Counting the `cpu cores' of the human brain", "comments": "27 pages, 10 figures, 2 tables, 47 references", "journal-ref": null, "doi": null, "report-no": "HG/AI.1014.27v1 (draft/preprint)", "categories": "cs.AI cs.CV q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool\nfor localizing and analyzing brain activity. This study focuses on one very\nimportant aspect of the functional properties of human brain, specifically the\nestimation of the level of parallelism when performing complex cognitive tasks.\nUsing fMRI as the main modality, the human brain activity is investigated\nthrough a purely data-driven signal processing and dimensionality analysis\napproach. Specifically, the fMRI signal is treated as a multi-dimensional data\nspace and its intrinsic `complexity' is studied via dataset fractal analysis\nand blind-source separation (BSS) methods. One simulated and two real fMRI\ndatasets are used in combination with Independent Component Analysis (ICA) and\nfractal analysis for estimating the intrinsic (true) dimensionality, in order\nto provide data-driven experimental evidence on the number of independent brain\nprocesses that run in parallel when visual or visuo-motor tasks are performed.\nAlthough this number is can not be defined as a strict threshold but rather as\na continuous range, when a specific activation level is defined, a\ncorresponding number of parallel processes or the casual equivalent of `cpu\ncores' can be detected in normal human brain activity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 00:25:24 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Georgiou", "Harris V.", ""]]}, {"id": "1410.7164", "submitter": "Manasij Venkatesh", "authors": "Manasij Venkatesh and Chandra Sekhar Seelamantula", "title": "Directional Bilateral Filters", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178236", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a bilateral filter with a locally controlled domain kernel for\ndirectional edge-preserving smoothing. Traditional bilateral filters use a\nrange kernel, which is responsible for edge preservation, and a fixed domain\nkernel that performs smoothing. Our intuition is that orientation and\nanisotropy of image structures should be incorporated into the domain kernel\nwhile smoothing. For this purpose, we employ an oriented Gaussian domain kernel\nlocally controlled by a structure tensor. The oriented domain kernel combined\nwith a range kernel forms the directional bilateral filter. The two kernels\nassist each other in effectively suppressing the influence of the outliers\nwhile smoothing. To find the optimal parameters of the directional bilateral\nfilter, we propose the use of Stein's unbiased risk estimate (SURE). We test\nthe capabilities of the kernels separately as well as together, first on\nsynthetic images, and then on real endoscopic images. The directional bilateral\nfilter has better denoising performance than the Gaussian bilateral filter at\nvarious noise levels in terms of peak signal-to-noise ratio (PSNR).\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 09:38:48 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Venkatesh", "Manasij", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1410.7211", "submitter": "Daniel Castro", "authors": "Daniel Castro, Paulo F\\'elix, Jes\\'us Presedo", "title": "A method for context-based adaptive QRS clustering in real-time", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": "10.1109/JBHI.2014.2361659", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous follow-up of heart condition through long-term electrocardiogram\nmonitoring is an invaluable tool for diagnosing some cardiac arrhythmias. In\nsuch context, providing tools for fast locating alterations of normal\nconduction patterns is mandatory and still remains an open issue. This work\npresents a real-time method for adaptive clustering QRS complexes from\nmultilead ECG signals that provides the set of QRS morphologies that appear\nduring an ECG recording. The method processes the QRS complexes sequentially,\ngrouping them into a dynamic set of clusters based on the information content\nof the temporal context. The clusters are represented by templates which evolve\nover time and adapt to the QRS morphology changes. Rules to create, merge and\nremove clusters are defined along with techniques for noise detection in order\nto avoid their proliferation. To cope with beat misalignment, Derivative\nDynamic Time Warping is used. The proposed method has been validated against\nthe MIT-BIH Arrhythmia Database and the AHA ECG Database showing a global\npurity of 98.56% and 99.56%, respectively. Results show that our proposal not\nonly provides better results than previous offline solutions but also fulfills\nreal-time requirements.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 12:52:57 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Castro", "Daniel", ""], ["F\u00e9lix", "Paulo", ""], ["Presedo", "Jes\u00fas", ""]]}, {"id": "1410.7252", "submitter": "Abhimanyu Sarin Mr.", "authors": "Abhimanyu Sarin, Dr. Jagadish Nayak", "title": "Iris Biometric System using a hybrid approach", "comments": "11 pages, 11 pictures, Computer Science & Information Technology-CSCP\n  2014", "journal-ref": null, "doi": "10.5121/csit.2014.4914", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris Recognition Systems are ocular- based biometric devices used primarily\nfor security reasons. The complexity and the randomness of the Iris, amongst\nvarious other factors, ensure that this biometric system is inarguably an exact\nand reliable method of identification. The algorithm is responsible for\nautomatic localization and segmentation of boundaries using circular Hough\nTransform, noise reductions, image enhancement and feature extraction across\nnumerous distinct images present in the database. This paper delves into the\nvarious kinds of techniques required to approximate the pupillary and limbic\nboundaries of the enrolled iris image, captured using a suitable image\nacquisition device and perform feature extraction on the normalized iris image\nwith the help of Haar Wavelets to encode the input data into a binary string\nformat. These techniques were validated using images from the CASIA database,\nand various other procedures were also tried and tested.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 14:28:59 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Sarin", "Abhimanyu", ""], ["Nayak", "Dr. Jagadish", ""]]}, {"id": "1410.7265", "submitter": "Balint Antal", "authors": "Balint Antal, Bence Remenyik, Andras Hajdu", "title": "An Unsupervised Ensemble-based Markov Random Field Approach to\n  Microscope Cell Image Segmentation", "comments": null, "journal-ref": "Proceeingds of the 10th International Conference on Signal\n  Processing and Multimedia Applications (SIGMAP 2013), Reykjavik, Iceland,\n  2013, pp. 94-99", "doi": "10.5220/0004612900940099", "report-no": null, "categories": "cs.CV cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to the unsupervised segmentation of\nimages using Markov Random Field. The proposed approach is based on the idea of\nBit Plane Slicing. We use the planes as initial labellings for an ensemble of\nsegmentations. With pixelwise voting, a robust segmentation approach can be\nachieved, which we demonstrate on microscope cell images. We tested our\napproach on a publicly available database, where it proven to be competitive\nwith other methods and manual segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 14:58:28 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Antal", "Balint", ""], ["Remenyik", "Bence", ""], ["Hajdu", "Andras", ""]]}, {"id": "1410.7328", "submitter": "Paul Vitanyi", "authors": "P.M.B. Vitanyi (CWI and University of Amsterdam)", "title": "Exact Expression For Information Distance", "comments": "6 pages LaTeX. added material and corrected it", "journal-ref": "IEEE Trans. Inform. Theory, 63:8(2017), 4725-4728", "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.CV cs.DM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information distance can be defined not only between two strings but also in\na finite multiset of strings of cardinality greater than two. We give an\nelementary proof for expressing the information distance in terms of plain\nKolmogorov complexity. It is exact since for each cardinality of the multiset\nthe lower bound for some multiset equals the upper bound for all multisets up\nto a constant additive term.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 17:46:57 GMT"}, {"version": "v10", "created": "Tue, 11 Jul 2017 17:07:32 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 17:53:43 GMT"}, {"version": "v3", "created": "Wed, 29 Oct 2014 17:25:56 GMT"}, {"version": "v4", "created": "Thu, 30 Oct 2014 18:19:10 GMT"}, {"version": "v5", "created": "Fri, 31 Oct 2014 16:31:08 GMT"}, {"version": "v6", "created": "Mon, 9 Feb 2015 16:49:52 GMT"}, {"version": "v7", "created": "Mon, 1 Jun 2015 16:37:02 GMT"}, {"version": "v8", "created": "Tue, 2 Jun 2015 15:36:38 GMT"}, {"version": "v9", "created": "Fri, 2 Oct 2015 16:05:46 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Vitanyi", "P. M. B.", "", "CWI and University of Amsterdam"]]}, {"id": "1410.7376", "submitter": "Nicholas Rhinehart", "authors": "Nicholas Rhinehart, Jiaji Zhou, Martial Hebert, J. Andrew Bagnell", "title": "Visual Chunking: A List Prediction Framework for Region-Based Object\n  Detection", "comments": "to appear at ICRA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider detecting objects in an image by iteratively selecting from a set\nof arbitrarily shaped candidate regions. Our generic approach, which we term\nvisual chunking, reasons about the locations of multiple object instances in an\nimage while expressively describing object boundaries. We design an\noptimization criterion for measuring the performance of a list of such\ndetections as a natural extension to a common per-instance metric. We present\nan efficient algorithm with provable performance for building a high-quality\nlist of detections from any candidate set of region-based proposals. We also\ndevelop a simple class-specific algorithm to generate a candidate region\ninstance in near-linear time in the number of low-level superpixels that\noutperforms other region generating methods. In order to make predictions on\nnovel images at testing time without access to ground truth, we develop\nlearning approaches to emulate these algorithms' behaviors. We demonstrate that\nour new approach outperforms sophisticated baselines on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 19:54:41 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 21:20:12 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Rhinehart", "Nicholas", ""], ["Zhou", "Jiaji", ""], ["Hebert", "Martial", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1410.7429", "submitter": "Yunjin Chen", "authors": "Yunjin Chen", "title": "Higher-order MRFs based image super resolution: why not MAP?", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trainable filter-based higher-order Markov Random Fields (MRFs) model - the\nso called Fields of Experts (FoE), has proved a highly effective image prior\nmodel for many classic image restoration problems. Generally, two options are\navailable to incorporate the learned FoE prior in the inference procedure: (1)\nsampling-based minimum mean square error (MMSE) estimate, and (2) energy\nminimization-based maximum a posteriori (MAP) estimate. This letter is devoted\nto the FoE prior based single image super resolution (SR) problem, and we\nsuggest to make use of the MAP estimate for inference based on two facts: (I)\nIt is well-known that the MAP inference has a remarkable advantage of high\ncomputational efficiency, while the sampling-based MMSE estimate is very time\nconsuming. (II) Practical SR experiment results demonstrate that the MAP\nestimate works equally well compared to the MMSE estimate with exactly the same\nFoE prior model. Moreover, it can lead to even further improvements by\nincorporating our discriminatively trained FoE prior model. In summary, we hold\nthat for higher-order natural image prior based SR problem, it is better to\nemploy the MAP estimate for inference.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 21:05:48 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 12:37:11 GMT"}, {"version": "v3", "created": "Wed, 14 Jan 2015 10:43:36 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2015 12:34:17 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Chen", "Yunjin", ""]]}, {"id": "1410.7452", "submitter": "Varun Jampani", "authors": "Varun Jampani, S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli and\n  John Winn", "title": "Consensus Message Passing for Layered Graphical Models", "comments": "Appearing in Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Generative models provide a powerful framework for probabilistic reasoning.\nHowever, in many domains their use has been hampered by the practical\ndifficulties of inference. This is particularly the case in computer vision,\nwhere models of the imaging process tend to be large, loopy and layered. For\nthis reason bottom-up conditional models have traditionally dominated in such\ndomains. We find that widely-used, general-purpose message passing inference\nalgorithms such as Expectation Propagation (EP) and Variational Message Passing\n(VMP) fail on the simplest of vision models. With these models in mind, we\nintroduce a modification to message passing that learns to exploit their\nlayered structure by passing 'consensus' messages that guide inference towards\ngood solutions. Experiments on a variety of problems show that the proposed\ntechnique leads to significantly more accurate inference results, not only when\ncompared to standard EP and VMP, but also when compared to competitive\nbottom-up conditional models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:40:52 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 21:36:36 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Jampani", "Varun", ""], ["Eslami", "S. M. Ali", ""], ["Tarlow", "Daniel", ""], ["Kohli", "Pushmeet", ""], ["Winn", "John", ""]]}, {"id": "1410.7454", "submitter": "Neeraj Dhungel", "authors": "Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley", "title": "Deep Structured learning for mass segmentation from Mammograms", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present a novel method for the segmentation of breast\nmasses from mammograms exploring structured and deep learning. Specifically,\nusing structured support vector machine (SSVM), we formulate a model that\ncombines different types of potential functions, including one that classifies\nimage regions using deep learning. Our main goal with this work is to show the\naccuracy and efficiency improvements that these relatively new techniques can\nprovide for the segmentation of breast masses from mammograms. We also propose\nan easily reproducible quantitative analysis to as- sess the performance of\nbreast mass segmentation methodologies based on widely accepted accuracy and\nrunning time measurements on public datasets, which will facilitate further\ncomparisons for this segmentation problem. In particular, we use two publicly\navailable datasets (DDSM-BCRP and INbreast) and propose the computa- tion of\nthe running time taken for the methodology to produce a mass segmentation given\nan input image and the use of the Dice index to quantitatively measure the\nsegmentation accuracy. For both databases, we show that our proposed\nmethodology produces competitive results in terms of accuracy and running time.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:44:26 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 01:21:39 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Dhungel", "Neeraj", ""], ["Carneiro", "Gustavo", ""], ["Bradley", "Andrew P.", ""]]}, {"id": "1410.7484", "submitter": "Tianfei Zhou", "authors": "Tianfei Zhou and Yao Lu and Feng Lv and Huijun Di and Qingjie Zhao and\n  Jian Zhang", "title": "Abrupt Motion Tracking via Nearest Neighbor Field Driven Stochastic\n  Sampling", "comments": "submitted to Elsevier Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2015.03.024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic sampling based trackers have shown good performance for abrupt\nmotion tracking so that they have gained popularity in recent years. However,\nconventional methods tend to use a two-stage sampling paradigm, in which the\nsearch space needs to be uniformly explored with an inefficient preliminary\nsampling phase. In this paper, we propose a novel sampling-based method in the\nBayesian filtering framework to address the problem. Within the framework,\nnearest neighbor field estimation is utilized to compute the importance\nproposal probabilities, which guide the Markov chain search towards promising\nregions and thus enhance the sampling efficiency; given the motion priors, a\nsmoothing stochastic sampling Monte Carlo algorithm is proposed to approximate\nthe posterior distribution through a smoothing weight-updating scheme.\nMoreover, to track the abrupt and the smooth motions simultaneously, we develop\nan abrupt-motion detection scheme which can discover the presence of abrupt\nmotions during online tracking. Extensive experiments on challenging image\nsequences demonstrate the effectiveness and the robustness of our algorithm in\nhandling the abrupt motions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 01:38:23 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 05:47:13 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Zhou", "Tianfei", ""], ["Lu", "Yao", ""], ["Lv", "Feng", ""], ["Di", "Huijun", ""], ["Zhao", "Qingjie", ""], ["Zhang", "Jian", ""]]}, {"id": "1410.7580", "submitter": "Linchao Bao", "authors": "Linchao Bao and Qingxiong Yang", "title": "Robust Piecewise-Constant Smoothing: M-Smoother Revisited", "comments": "11 pages, 9 figures, update url links", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust estimator, namely M-smoother, for piecewise-constant smoothing is\nrevisited in this paper. Starting from its generalized formulation, we propose\na numerical scheme/framework for solving it via a series of weighted-average\nfiltering (e.g., box filtering, Gaussian filtering, bilateral filtering, and\nguided filtering). Because of the equivalence between M-smoother and\nlocal-histogram-based filters (such as median filter and mode filter), the\nproposed framework enables fast approximation of histogram filters via a number\nof box filtering or Gaussian filtering. In addition, high-quality\npiecewise-constant smoothing can be achieved via a number of bilateral\nfiltering or guided filtering integrated in the proposed framework. Experiments\non depth map denoising show the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 10:57:41 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 08:10:13 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1410.7613", "submitter": "Shaopeng  Ma", "authors": "Xian Wang, Shaopeng Ma", "title": "A Short Image Series Based Scheme for Time Series Digital Image\n  Correlation", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new scheme for digital image correlation, i.e., short time series DIC\n(STS-DIC) is proposed. Instead of processing the original deformed speckle\nimages individually, STS-DIC combines several adjacent deformed speckle images\nfrom a short time series and then processes the averaged image, for which\ndeformation continuity over time is introduced. The deformation of several\nadjacent images is assumed to be linear in time and a new spatial-temporal\ndisplacement representation method with eight unknowns is presented based on\nthe subset-based representation method. Then, the model of STS-DIC is created\nand a solving scheme is developed based on the Newton-Raphson iteration. The\nproposed method is verified for numerical and experimental cases. The results\nshow that the proposed STS-DIC greatly improves the accuracy of traditional\nDIC, both under simple and complicated deformation conditions, while retaining\nacceptable actual computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 13:33:55 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wang", "Xian", ""], ["Ma", "Shaopeng", ""]]}, {"id": "1410.7632", "submitter": "Martin Barczyk", "authors": "Silv\\`ere Bonnabel, Martin Barczyk and Fran\\c{c}ois Goulette", "title": "On the Covariance of ICP-based Scan-matching Techniques", "comments": "Accepted at 2016 American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating the covariance of\nroto-translations computed by the Iterative Closest Point (ICP) algorithm. The\nproblem is relevant for localization of mobile robots and vehicles equipped\nwith depth-sensing cameras (e.g., Kinect) or Lidar (e.g., Velodyne). The\nclosed-form formulas for covariance proposed in previous literature generally\nbuild upon the fact that the solution to ICP is obtained by minimizing a linear\nleast-squares problem. In this paper, we show this approach needs caution\nbecause the rematching step of the algorithm is not explicitly accounted for,\nand applying it to the point-to-point version of ICP leads to completely\nerroneous covariances. We then provide a formal mathematical proof why the\napproach is valid in the point-to-plane version of ICP, which validates the\nintuition and experimental results of practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 00:14:05 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 05:52:02 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 23:41:00 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Bonnabel", "Silv\u00e8re", ""], ["Barczyk", "Martin", ""], ["Goulette", "Fran\u00e7ois", ""]]}, {"id": "1410.7679", "submitter": "Fred Ngol\\`e", "authors": "Fred Maurice Ngol\\`e Mboula, Jean-Luc Starck, Samuel Ronayette, Koryo\n  Okumura, J\\'er\\^ome Amiaux", "title": "Super-resolution method using sparse regularization for point-spread\n  function recovery", "comments": null, "journal-ref": null, "doi": "10.1051/0004-6361/201424167", "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale spatial surveys, such as the forthcoming ESA Euclid mission,\nimages may be undersampled due to the optical sensors sizes. Therefore, one may\nconsider using a super-resolution (SR) method to recover aliased frequencies,\nprior to further analysis. This is particularly relevant for point-source\nimages, which provide direct measurements of the instrument point-spread\nfunction (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse,\nwhich is an SR algorithm using a sparse analysis prior. We show that such a\nprior provides significant improvements over existing methods, especially on\nlow SNR PSFs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 06:49:00 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Mboula", "Fred Maurice Ngol\u00e8", ""], ["Starck", "Jean-Luc", ""], ["Ronayette", "Samuel", ""], ["Okumura", "Koryo", ""], ["Amiaux", "J\u00e9r\u00f4me", ""]]}, {"id": "1410.7730", "submitter": "Yasel Garc\\'es Su\\'arez", "authors": "Yasel Garc\\'es, Esley Torres, Osvaldo Pereira, Roberto Rodr\\'iguez", "title": "New similarity index based on entropy and group theory", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new similarity index for images considering the\nentropy function and group theory. This index considers an algebraic group of\nimages, it is defined by an inner law that provides a novel approach for the\nsubtraction of images. Through an equivalence relationship in the field of\nimages, we prove the existence of the quotient group, on which the new\nsimilarity index is defined. We also present the main properties of the new\nindex, and the immediate application thereof as a stopping criterion of the\n\"Mean Shift Iterative Algorithm\".\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 18:55:37 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Garc\u00e9s", "Yasel", ""], ["Torres", "Esley", ""], ["Pereira", "Osvaldo", ""], ["Rodr\u00edguez", "Roberto", ""]]}, {"id": "1410.7762", "submitter": "Reza Moazzezi", "authors": "Reza Moazzezi", "title": "A hierarchical framework for object recognition", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in the presence of background clutter and distractors is a\ncentral problem both in neuroscience and in machine learning. However, the\nperformance level of the models that are inspired by cortical mechanisms,\nincluding deep networks such as convolutional neural networks and deep belief\nnetworks, is shown to significantly decrease in the presence of noise and\nbackground objects [19, 24]. Here we develop a computational framework that is\nhierarchical, relies heavily on key properties of the visual cortex including\nmid-level feature selectivity in visual area V4 and Inferotemporal cortex (IT)\n[4, 9, 12, 18], high degrees of selectivity and invariance in IT [13, 17, 18]\nand the prior knowledge that is built into cortical circuits (such as the\nemergence of edge detector neurons in primary visual cortex before the onset of\nthe visual experience) [1, 21], and addresses the problem of object recognition\nin the presence of background noise and distractors. Our approach is\nspecifically designed to address large deformations, allows flexible\ncommunication between different layers of representation and learns highly\nselective filters from a small number of training examples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 19:44:55 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Moazzezi", "Reza", ""]]}, {"id": "1410.7795", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik\n  Maharatna, Fabio Apicella, Federico Sicca", "title": "Classification of Autism Spectrum Disorder Using Supervised Learning of\n  Brain Connectivity Measures Extracted from Synchrostates", "comments": "27 pages, 17 figures", "journal-ref": "Journal of Neural Engineering, Volume 11, Number 4, pp. 046019,\n  August 2014", "doi": "10.1088/1741-2560/11/4/046019", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. The paper investigates the presence of autism using the functional\nbrain connectivity measures derived from electro-encephalogram (EEG) of\nchildren during face perception tasks. Approach. Phase synchronized patterns\nfrom 128-channel EEG signals are obtained for typical children and children\nwith autism spectrum disorder (ASD). The phase synchronized states or\nsynchrostates temporally switch amongst themselves as an underlying process for\nthe completion of a particular cognitive task. We used 12 subjects in each\ngroup (ASD and typical) for analyzing their EEG while processing fearful, happy\nand neutral faces. The minimal and maximally occurring synchrostates for each\nsubject are chosen for extraction of brain connectivity features, which are\nused for classification between these two groups of subjects. Among different\nsupervised learning techniques, we here explored the discriminant analysis and\nsupport vector machine both with polynomial kernels for the classification\ntask. Main results. The leave one out cross-validation of the classification\nalgorithm gives 94.7% accuracy as the best performance with corresponding\nsensitivity and specificity values as 85.7% and 100% respectively.\nSignificance. The proposed method gives high classification accuracies and\noutperforms other contemporary research results. The effectiveness of the\nproposed method for classification of autistic and typical children suggests\nthe possibility of using it on a larger population to validate it for clinical\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 17:16:04 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Oprescu", "Ioana-Anastasia", ""], ["Maharatna", "Koushik", ""], ["Apicella", "Fabio", ""], ["Sicca", "Federico", ""]]}, {"id": "1410.7876", "submitter": "Minh Dao", "authors": "Minh Dao, Nam H. Nguyen, Nasser M. Nasrabadi, and Trac D. Tran", "title": "Collaborative Multi-sensor Classification via Sparsity-based\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general collaborative sparse representation\nframework for multi-sensor classification, which takes into account the\ncorrelations as well as complementary information between heterogeneous sensors\nsimultaneously while considering joint sparsity within each sensor's\nobservations. We also robustify our models to deal with the presence of sparse\nnoise and low-rank interference signals. Specifically, we demonstrate that\nincorporating the noise or interference signal as a low-rank component in our\nmodels is essential in a multi-sensor classification problem when multiple\nco-located sources/sensors simultaneously record the same physical event. We\nfurther extend our frameworks to kernelized models which rely on sparsely\nrepresenting a test sample in terms of all the training samples in a feature\nspace induced by a kernel function. A fast and efficient algorithm based on\nalternative direction method is proposed where its convergence to an optimal\nsolution is guaranteed. Extensive experiments are conducted on several real\nmulti-sensor data sets and results are compared with the conventional\nclassifiers to verify the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:25:44 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 16:46:36 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Dao", "Minh", ""], ["Nguyen", "Nam H.", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1410.7922", "submitter": "Mikhail Mozerov G", "authors": "Mikhail G. Mozerov", "title": "Extended Dynamic Programming and Fast Multidimensional Search Algorithm\n  for Energy Minization in Stereo and Motion", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel extended dynamic programming approach for energy\nminimization (EDP) to solve the correspondence problem for stereo and motion. A\nsignificant speedup is achieved using a recursive minimum search strategy\n(RMS). The mentioned speedup is particularly important if the disparity space\nis 2D as well as 3D. The proposed RMS can also be applied in the well-known\ndynamic programming (DP) approach for stereo and motion. In this case, the\ngeneral 2D problem of the global discrete energy minimization is reduced to\nseveral mutually independent sub-problems of the one-dimensional minimization.\nThe EDP method is used when the approximation of the general 2D discrete energy\nminimization problem is considered. Then the RMS algorithm is an essential part\nof the EDP method. Using the EDP algorithm we obtain a lower energy bound than\nthe graph cuts (GC) expansion technique on stereo and motion problems. The\nproposed calculation scheme possesses natural parallelism and can be realized\non graphics processing unit (GPU) platforms, and can be potentially restricted\nfurther by the number of scanlines in the image plane. Furthermore, the RMS and\nEDP methods can be used in any optimization problem where the objective\nfunction meets specific conditions in the smoothness term.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 10:31:27 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Mozerov", "Mikhail G.", ""]]}, {"id": "1410.8027", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Towards a Visual Turing Challenge", "comments": "Published in the NIPS 2014 Workshop on Learning Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As language and visual understanding by machines progresses rapidly, we are\nobserving an increasing interest in holistic architectures that tightly\ninterlink both modalities in a joint learning and inference process. This trend\nhas allowed the community to progress towards more challenging and open tasks\nand refueled the hope at achieving the old AI dream of building machines that\ncould pass a turing test in open domains. In order to steadily make progress\ntowards this goal, we realize that quantifying performance becomes increasingly\ndifficult. Therefore we ask how we can precisely define such challenges and how\nwe can evaluate different algorithms on this open tasks? In this paper, we\nsummarize and discuss such challenges as well as try to give answers where\nappropriate options are available in the literature. We exemplify some of the\nsolutions on a recently presented dataset of question-answering task based on\nreal-world indoor images that establishes a visual turing challenge. Finally,\nwe argue despite the success of unique ground-truth annotation, we likely have\nto step away from carefully curated dataset and rather rely on 'social\nconsensus' as the main driving force to create suitable benchmarks. Providing\ncoverage in this inherently ambiguous output space is an emerging challenge\nthat we face in order to make quantifiable progress in this area.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 15:38:29 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 12:09:53 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 18:03:56 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1410.8151", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Giorgos Tolias, Philippe-Henri Gosselin, Herv\\'e J\\'egou", "title": "A comparison of dense region detectors for image search and fine-grained\n  classification", "comments": "Accepted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2423557", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a pipeline for image classification or search based on coding\napproaches like Bag of Words or Fisher vectors. In this context, the most\ncommon approach is to extract the image patches regularly in a dense manner on\nseveral scales. This paper proposes and evaluates alternative choices to\nextract patches densely. Beyond simple strategies derived from regular interest\nregion detectors, we propose approaches based on super-pixels, edges, and a\nbank of Zernike filters used as detectors. The different approaches are\nevaluated on recent image retrieval and fine-grain classification benchmarks.\nOur results show that the regular dense detector is outperformed by other\nmethods in most situations, leading us to improve the state of the art in\ncomparable setups on standard retrieval and fined-grain benchmarks. As a\nbyproduct of our study, we show that existing methods for blob and super-pixel\nextraction achieve high accuracy if the patches are extracted along the edges\nand not around the detected regions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:18:41 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 12:56:39 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 18:36:17 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Gosselin", "Philippe-Henri", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1410.8546", "submitter": "Florian Bernard", "authors": "Florian Bernard, Johan Thunberg, Peter Gemmar, Frank Hertel, Andreas\n  Husch, Jorge Goncalves", "title": "A Solution for Multi-Alignment by Transformation Synchronisation", "comments": "Accepted for CVPR 2015 (please cite CVPR version)", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298828", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alignment of a set of objects by means of transformations plays an\nimportant role in computer vision. Whilst the case for only two objects can be\nsolved globally, when multiple objects are considered usually iterative methods\nare used. In practice the iterative methods perform well if the relative\ntransformations between any pair of objects are free of noise. However, if only\nnoisy relative transformations are available (e.g. due to missing data or wrong\ncorrespondences) the iterative methods may fail.\n  Based on the observation that the underlying noise-free transformations can\nbe retrieved from the null space of a matrix that can directly be obtained from\npairwise alignments, this paper presents a novel method for the synchronisation\nof pairwise transformations such that they are transitively consistent.\n  Simulations demonstrate that for noisy transformations, a large proportion of\nmissing data and even for wrong correspondence assignments the method delivers\nencouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 20:29:08 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 16:19:45 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Bernard", "Florian", ""], ["Thunberg", "Johan", ""], ["Gemmar", "Peter", ""], ["Hertel", "Frank", ""], ["Husch", "Andreas", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1410.8576", "submitter": "Balint Antal", "authors": "Balint Antal, Andras Hajdu", "title": "An ensemble-based system for automatic screening of diabetic retinopathy", "comments": null, "journal-ref": "Knowledge-Based Systems, Elsevier, Volume 60, April 2014, Pages\n  20-27", "doi": "10.1016/j.knosys.2013.12.023", "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an ensemble-based method for the screening of diabetic\nretinopathy (DR) is proposed. This approach is based on features extracted from\nthe output of several retinal image processing algorithms, such as image-level\n(quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms,\nexudates) and anatomical (macula, optic disc) components. The actual decision\nabout the presence of the disease is then made by an ensemble of machine\nlearning classifiers. We have tested our approach on the publicly available\nMessidor database, where 90% sensitivity, 91% specificity and 90% accuracy and\n0.989 AUC are achieved in a disease/no-disease setting. These results are\nhighly competitive in this field and suggest that retinal image processing is a\nvalid approach for automatic DR screening.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:14:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Antal", "Balint", ""], ["Hajdu", "Andras", ""]]}, {"id": "1410.8577", "submitter": "Balint Antal", "authors": "Balint Antal, Andras Hajdu", "title": "An Ensemble-based System for Microaneurysm Detection and Diabetic\n  Retinopathy Grading", "comments": null, "journal-ref": "IEEE Transactions on Biomedical Engineering, vol.59, no.6, pp.\n  1720-1726, June 2012", "doi": "10.1109/TBME.2012.2193126", "report-no": null, "categories": "cs.CV cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable microaneurysm detection in digital fundus images is still an open\nissue in medical image processing. We propose an ensemble-based framework to\nimprove microaneurysm detection. Unlike the well-known approach of considering\nthe output of multiple classifiers, we propose a combination of internal\ncomponents of microaneurysm detectors, namely preprocessing methods and\ncandidate extractors. We have evaluated our approach for microaneurysm\ndetection in an online competition, where this algorithm is currently ranked as\nfirst and also on two other databases. Since microaneurysm detection is\ndecisive in diabetic retinopathy grading, we also tested the proposed method\nfor this task on the publicly available Messidor database, where a promising\nAUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification\nbased on the presence or absence of the microaneurysms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:21:02 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Antal", "Balint", ""], ["Hajdu", "Andras", ""]]}, {"id": "1410.8586", "submitter": "Tao Chen", "authors": "Tao Chen, Damian Borth, Trevor Darrell and Shih-Fu Chang", "title": "DeepSentiBank: Visual Sentiment Concept Classification with Deep\n  Convolutional Neural Networks", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a visual sentiment concept classification method based\non deep convolutional neural networks (CNNs). The visual sentiment concepts are\nadjective noun pairs (ANPs) automatically discovered from the tags of web\nphotos, and can be utilized as effective statistical cues for detecting\nemotions depicted in the images. Nearly one million Flickr images tagged with\nthese ANPs are downloaded to train the classifiers of the concepts. We adopt\nthe popular model of deep convolutional neural networks which recently shows\ngreat performance improvement on classifying large-scale web-based image\ndataset such as ImageNet. Our deep CNNs model is trained based on Caffe, a\nnewly developed deep learning framework. To deal with the biased training data\nwhich only contains images with strong sentiment and to prevent overfitting, we\ninitialize the model with the model weights trained from ImageNet. Performance\nevaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called\nDeepSentiBank) is significantly improved in both annotation accuracy and\nretrieval performance, compared to its predecessors which mainly use binary SVM\nclassification models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:57:12 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Chen", "Tao", ""], ["Borth", "Damian", ""], ["Darrell", "Trevor", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1410.8618", "submitter": "Chen Jie", "authors": "Jie Chen and Haixian Zhang and Hua Mao and Yongsheng Sang and Zhang Yi", "title": "Symmetric low-rank representation for subspace clustering", "comments": "13 pages", "journal-ref": null, "doi": "10.1016/j.neucom.2015.08.077", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a symmetric low-rank representation (SLRR) method for subspace\nclustering, which assumes that a data set is approximately drawn from the union\nof multiple subspaces. The proposed technique can reveal the membership of\nmultiple subspaces through the self-expressiveness property of the data. In\nparticular, the SLRR method considers a collaborative representation combined\nwith low-rank matrix recovery techniques as a low-rank representation to learn\na symmetric low-rank representation, which preserves the subspace structures of\nhigh-dimensional data. In contrast to performing iterative singular value\ndecomposition in some existing low-rank representation based algorithms, the\nsymmetric low-rank representation in the SLRR method can be calculated as a\nclosed form solution by solving the symmetric low-rank optimization problem. By\nmaking use of the angular information of the principal directions of the\nsymmetric low-rank representation, an affinity graph matrix is constructed for\nspectral clustering. Extensive experimental results show that it outperforms\nstate-of-the-art subspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 02:14:03 GMT"}, {"version": "v2", "created": "Thu, 21 May 2015 14:04:13 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Chen", "Jie", ""], ["Zhang", "Haixian", ""], ["Mao", "Hua", ""], ["Sang", "Yongsheng", ""], ["Yi", "Zhang", ""]]}, {"id": "1410.8623", "submitter": "David Budden", "authors": "Shannon Fenn, Alexandre Mendes and David Budden", "title": "Addressing the non-functional requirements of computer vision systems: A\n  case study", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision plays a major role in the robotics industry, where vision\ndata is frequently used for navigation and high-level decision making. Although\nthere is significant research in algorithms and functional requirements, there\nis a comparative lack of emphasis on how best to map these abstract concepts\nonto an appropriate software architecture.\n  In this study, we distinguish between the functional and non-functional\nrequirements of a computer vision system. Using a RoboCup humanoid robot system\nas a case study, we propose and develop a software architecture that fulfills\nthe latter criteria.\n  The modifiability of the proposed architecture is demonstrated by detailing a\nnumber of feature detection algorithms and emphasizing which aspects of the\nunderlying framework were modified to support their integration. To demonstrate\nportability, we port our vision system (designed for an application-specific\nDARwIn-OP humanoid robot) to a general-purpose, Raspberry Pi computer. We\nevaluate performance on both platforms and compare them to a vision system\noptimised for functional requirements only.\n  The architecture and implementation presented in this study provide a highly\ngeneralisable framework for computer vision system design that is of particular\nbenefit in research and development, competition and other environments in\nwhich rapid system evolution is necessary.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 03:05:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Fenn", "Shannon", ""], ["Mendes", "Alexandre", ""], ["Budden", "David", ""]]}]