[{"id": "1007.0085", "submitter": "Nitin Bhatia", "authors": "Nitin Bhatia and Vandana", "title": "Survey of Nearest Neighbor Techniques", "comments": "4 pages, 1 table", "journal-ref": "IJCSIS vol. 8, No. 2, 2010 pp. 302-305", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nearest neighbor (NN) technique is very simple, highly efficient and\neffective in the field of pattern recognition, text categorization, object\nrecognition etc. Its simplicity is its main advantage, but the disadvantages\ncan't be ignored even. The memory requirement and computation complexity also\nmatter. Many techniques are developed to overcome these limitations. NN\ntechniques are broadly classified into structure less and structure based\ntechniques. In this paper, we present the survey of such techniques. Weighted\nkNN, Model based kNN, Condensed NN, Reduced NN, Generalized NN are structure\nless techniques whereas k-d tree, ball tree, Principal Axis Tree, Nearest\nFeature Line, Tunable NN, Orthogonal Search Tree are structure based algorithms\ndeveloped on the basis of kNN. The structure less method overcome memory\nlimitation and structure based techniques reduce the computational complexity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 06:53:50 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Bhatia", "Nitin", ""], ["Vandana", "", ""]]}, {"id": "1007.0210", "submitter": "Sergei Gepshtein", "authors": "Sergei Gepshtein and Ivan Tyukin", "title": "Uncertainty of visual measurement and efficient allocation of sensory\n  resources", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the reasoning underlying two approaches to combination of sensory\nuncertainties. First approach is noncommittal, making no assumptions about\nproperties of uncertainty or parameters of stimulation. Then we explain the\nrelationship between this approach and the one commonly used in modeling\n\"higher level\" aspects of sensory systems, such as in visual cue integration,\nwhere assumptions are made about properties of stimulation. The two approaches\nfollow similar logic, except in one case maximal uncertainty is minimized, and\nin the other minimal certainty is maximized. Then we demonstrate how optimal\nsolutions are found to the problem of resource allocation under uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 16:37:34 GMT"}, {"version": "v2", "created": "Sat, 3 May 2014 00:57:19 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Gepshtein", "Sergei", ""], ["Tyukin", "Ivan", ""]]}, {"id": "1007.0313", "submitter": "Duc Phu Chau", "authors": "Duc Phu Chau (INRIA Sophia Antipolis), Francois Bremond (INRIA Sophia\n  Antipolis), Etienne Corvee (INRIA Sophia Antipolis), Monique Thonnat (INRIA\n  Sophia Antipolis)", "title": "Repairing People Trajectories Based on Point Clustering", "comments": null, "journal-ref": "The International Conference on Computer Vision Theory and\n  Applications (VISAPP), Lisboa : Portugal (2009)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for improving any object tracking algorithm\nbased on machine learning. During the training phase, important trajectory\nfeatures are extracted which are then used to calculate a confidence value of\ntrajectory. The positions at which objects are usually lost and found are\nclustered in order to construct the set of 'lost zones' and 'found zones' in\nthe scene. Using these zones, we construct a triplet set of zones i.e. three\nzones: In/Out zone (zone where an object can enter or exit the scene), 'lost\nzone' and 'found zone'. Thanks to these triplets, during the testing phase, we\ncan repair the erroneous trajectories according to which triplet they are most\nlikely to belong to. The advantage of our approach over the existing state of\nthe art approaches is that (i) this method does not depend on a predefined\ncontextual scene, (ii) we exploit the semantic of the scene and (iii) we have\nproposed a method to filter out noisy trajectories based on their confidence\nvalue.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2010 07:50:03 GMT"}], "update_date": "2010-07-21", "authors_parsed": [["Chau", "Duc Phu", "", "INRIA Sophia Antipolis"], ["Bremond", "Francois", "", "INRIA Sophia\n  Antipolis"], ["Corvee", "Etienne", "", "INRIA Sophia Antipolis"], ["Thonnat", "Monique", "", "INRIA\n  Sophia Antipolis"]]}, {"id": "1007.0547", "submitter": "Nitin Bhatia", "authors": "Chandan Singh and Nitin Bhatia", "title": "A Fast Decision Technique for Hierarchical Hough Transform for Line\n  Detection", "comments": "7 pages, published at IEEE conference on Signal and Image Processing\n  - 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques have been proposed to speedup the performance of classic\nHough Transform. These techniques are primarily based on converting the voting\nprocedure to a hierarchy based voting method. These methods use approximate\ndecision-making process. In this paper, we propose a fast decision making\nprocess that enhances the speed and reduces the space requirements.\nExperimental results demonstrate that the proposed algorithm is much faster\nthan a similar Fast Hough Transform.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 12:19:16 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Singh", "Chandan", ""], ["Bhatia", "Nitin", ""]]}, {"id": "1007.0618", "submitter": "Debotosh Bhattacharjee", "authors": "Santanu Halder, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar\n  Basu, and Mahantapas Kundu", "title": "Face Synthesis (FASY) System for Determining the Characteristics of a\n  Face Image", "comments": null, "journal-ref": "RAIT 2009", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at determining the characteristics of a face image by\nextracting its components. The FASY (FAce SYnthesis) System is a Face Database\nRetrieval and new Face generation System that is under development. One of its\nmain features is the generation of the requested face when it is not found in\nthe existing database, which allows a continuous growing of the database also.\nTo generate the new face image, we need to store the face components in the\ndatabase. So we have designed a new technique to extract the face components by\na sophisticated method. After extraction of the facial feature points we have\nanalyzed the components to determine their characteristics. After extraction\nand analysis we have stored the components along with their characteristics\ninto the face database for later use during the face construction.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 05:24:30 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Halder", "Santanu", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1007.0620", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Mita Nasipuri, Dipak\n  Kumar Basu, and Mahantapas Kundu", "title": "Quotient Based Multiresolution Image Fusion of Thermal and Visual Images\n  Using Daubechies Wavelet Transform for Human Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the multiresolution level-1 and level-2 Quotient\nbased Fusion of thermal and visual images. In the proposed system, the method-1\nnamely \"Decompose then Quotient Fuse Level-1\" and the method-2 namely\n\"Decompose-Reconstruct then Quotient Fuse Level-2\" both work on wavelet\ntransformations of the visual and thermal face images. The wavelet transform is\nwell-suited to manage different image resolution and allows the image\ndecomposition in different kinds of coefficients, while preserving the image\ninformation without any loss. This approach is based on a definition of an\nillumination invariant signature image which enables an analytic generation of\nthe image space with varying illumination. The quotient fused images are passed\nthrough Principal Component Analysis (PCA) for dimension reduction and then\nthose images are classified using a multi-layer perceptron (MLP). The\nperformances of both the methods have been evaluated using OTCBVS and IRIS\ndatabases. All the different classes have been tested separately, among them\nthe maximum recognition result is 100%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 05:56:44 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1007.0621", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Mita Nasipuri, Dipak\n  Kumar Basu, and Mahantapas Kundu", "title": "Fusion of Daubechies Wavelet Coefficients for Human Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper fusion of visual and thermal images in wavelet transformed\ndomain has been presented. Here, Daubechies wavelet transform, called as D2,\ncoefficients from visual and corresponding coefficients computed in the same\nmanner from thermal images are combined to get fused coefficients. After\ndecomposition up to fifth level (Level 5) fusion of coefficients is done.\nInverse Daubechies wavelet transform of those coefficients gives us fused face\nimages. The main advantage of using wavelet transform is that it is well-suited\nto manage different image resolution and allows the image decomposition in\ndifferent kinds of coefficients, while preserving the image information. Fused\nimages thus found are passed through Principal Component Analysis (PCA) for\nreduction of dimensions and then those reduced fused images are classified\nusing a multi-layer perceptron. For experiments IRIS Thermal/Visual Face\nDatabase was used. Experimental results show that the performance of the\napproach presented here achieves maximum success rate of 100% in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 06:04:43 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1007.0626", "submitter": "Debotosh Bhattacharjee", "authors": "M. K. Bhowmik, Debotosh Bhattacharjee, M. Nasipuri, D. K. Basu, and M.\n  Kundu", "title": "Fusion of Wavelet Coefficients from Visual and Thermal Face Images for\n  Human Face Recognition - A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a comparative study on fusion of visual and thermal\nimages using different wavelet transformations. Here, coefficients of discrete\nwavelet transforms from both visual and thermal images are computed separately\nand combined. Next, inverse discrete wavelet transformation is taken in order\nto obtain fused face image. Both Haar and Daubechies (db2) wavelet transforms\nhave been used to compare recognition results. For experiments IRIS\nThermal/Visual Face Database was used. Experimental results using Haar and\nDaubechies wavelets show that the performance of the approach presented here\nachieves maximum success rate of 100% in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 07:33:45 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "M. K.", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "M.", ""], ["Basu", "D. K.", ""], ["Kundu", "M.", ""]]}, {"id": "1007.0627", "submitter": "Debotosh Bhattacharjee", "authors": "M.K. Bhowmik, Debotosh Bhattacharjee, M. Nasipuri, D. K. Basu, and M.\n  Kundu", "title": "A Parallel Framework for Multilayer Perceptron for Human Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have already shown their success in face\nrecognition and similar complex pattern recognition tasks. However, a major\ndisadvantage of the technique is that it is extremely slow during training for\nlarger classes and hence not suitable for real-time complex problems such as\npattern recognition. This is an attempt to develop a parallel framework for the\ntraining algorithm of a perceptron. In this paper, two general architectures\nfor a Multilayer Perceptron (MLP) have been demonstrated. The first\narchitecture is All-Class-in-One-Network (ACON) where all the classes are\nplaced in a single network and the second one is One-Class-in-One-Network\n(OCON) where an individual single network is responsible for each and every\nclass. Capabilities of these two architectures were compared and verified in\nsolving human face recognition, which is a complex pattern recognition task\nwhere several factors affect the recognition performance like pose variations,\nfacial expression changes, occlusions, and most importantly illumination\nchanges. Both the structures were implemented and tested for face recognition\npurpose and experimental results show that the OCON structure performs better\nthan the generally used ACON ones in term of training convergence speed of the\nnetwork. Unlike the conventional sequential approach of training the neural\nnetworks, the OCON technique may be implemented by training all the classes of\nthe face images simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 07:40:56 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "M. K.", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "M.", ""], ["Basu", "D. K.", ""], ["Kundu", "M.", ""]]}, {"id": "1007.0628", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Mita Nasipuri, Dipak\n  Kumar Basu, and Mahantapas Kundu", "title": "Image Pixel Fusion for Human Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a technique for fusion of optical and thermal face\nimages based on image pixel fusion approach. Out of several factors, which\naffect face recognition performance in case of visual images, illumination\nchanges are a significant factor that needs to be addressed. Thermal images are\nbetter in handling illumination conditions but not very consistent in capturing\ntexture details of the faces. Other factors like sunglasses, beard, moustache\netc also play active role in adding complicacies to the recognition process.\nFusion of thermal and visual images is a solution to overcome the drawbacks\npresent in the individual thermal and visual face images. Here fused images are\nprojected into an eigenspace and the projected images are classified using a\nradial basis function (RBF) neural network and also by a multi-layer perceptron\n(MLP). In the experiments Object Tracking and Classification Beyond Visible\nSpectrum (OTCBVS) database benchmark for thermal and visual face images have\nbeen used. Comparison of experimental results show that the proposed approach\nperforms significantly well in recognizing face images with a success rate of\n96% and 95.07% for RBF Neural Network and MLP respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 07:45:48 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1007.0631", "submitter": "Debotosh Bhattacharjee", "authors": "M.K. Bhowmik, Debotosh Bhattacharjee, M. Nasipuri, D. K. Basu and M.\n  Kundu", "title": "Classification of Fused Images using Radial Basis Function Neural\n  Network for Human Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here an efficient fusion technique for automatic face recognition has been\npresented. Fusion of visual and thermal images has been done to take the\nadvantages of thermal images as well as visual images. By employing fusion a\nnew image can be obtained, which provides the most detailed, reliable, and\ndiscriminating information. In this method fused images are generated using\nvisual and thermal face images in the first step. In the second step, fused\nimages are projected into eigenspace and finally classified using a radial\nbasis function neural network. In the experiments Object Tracking and\nClassification Beyond Visible Spectrum (OTCBVS) database benchmark for thermal\nand visual face images have been used. Experimental results show that the\nproposed approach performs well in recognizing unknown individuals with a\nmaximum success rate of 96%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 07:57:42 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "M. K.", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "M.", ""], ["Basu", "D. K.", ""], ["Kundu", "M.", ""]]}, {"id": "1007.0633", "submitter": "Debotosh Bhattacharjee", "authors": "Debotosh Bhattacharjee, Mrinal Kanti Bhowmik, Mita Nasipuri, Dipak\n  Kumar Basu, and Mahantapas Kundu", "title": "Classification of fused face images using multilayer perceptron neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a concept of image pixel fusion of visual and thermal\nfaces, which can significantly improve the overall performance of a face\nrecognition system. Several factors affect face recognition performance\nincluding pose variations, facial expression changes, occlusions, and most\nimportantly illumination changes. So, image pixel fusion of thermal and visual\nimages is a solution to overcome the drawbacks present in the individual\nthermal and visual face images. Fused images are projected into eigenspace and\nfinally classified using a multi-layer perceptron. In the experiments we have\nused Object Tracking and Classification Beyond Visible Spectrum (OTCBVS)\ndatabase benchmark thermal and visual face images. Experimental results show\nthat the proposed approach significantly improves the verification and\nidentification performance and the success rate is 95.07%. The main objective\nof employing fusion is to produce a fused image that provides the most detailed\nand reliable information. Fusion of multiple images together produces a more\nefficient representation of the image.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 08:01:11 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhattacharjee", "Debotosh", ""], ["Bhowmik", "Mrinal Kanti", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1007.0636", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Mita Nasipuri,\n  Mahantapas Kundu, and Dipak Kumar Basu", "title": "Classification of Log-Polar-Visual Eigenfaces using Multilayer\n  Perceptron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a simple novel approach to tackle the challenges of\nscaling and rotation of face images in face recognition. The proposed approach\nregisters the training and testing visual face images by log-polar\ntransformation, which is capable to handle complicacies introduced by scaling\nand rotation. Log-polar images are projected into eigenspace and finally\nclassified using an improved multi-layer perceptron. In the experiments we have\nused ORL face database and Object Tracking and Classification Beyond Visible\nSpectrum (OTCBVS) database for visual face images. Experimental results show\nthat the proposed approach significantly improves the recognition performances\nfrom visual to log-polar-visual face images. In case of ORL face database,\nrecognition rate for visual face images is 89.5% and that is increased to 97.5%\nfor log-polar-visual face images whereas for OTCBVS face database recognition\nrate for visual images is 87.84% and 96.36% for log-polar-visual face images.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 08:05:14 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1007.0638", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Mita Nasipuri, Dipak\n  Kumar Basu, and Mahantapas Kundu", "title": "Human Face Recognition using Line Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate a novel approach to handle the challenges of face\nrecognition, which includes rotation, scale, occlusion, illumination etc. Here,\nwe have used thermal face images as those are capable to minimize the affect of\nillumination changes and occlusion due to moustache, beards, adornments etc.\nThe proposed approach registers the training and testing thermal face images in\npolar coordinate, which is capable to handle complicacies introduced by scaling\nand rotation. Line features are extracted from thermal polar images and feature\nvectors are constructed using these line. Feature vectors thus obtained passes\nthrough principal component analysis (PCA) for the dimensionality reduction of\nfeature vectors. Finally, the images projected into eigenspace are classified\nusing a multi-layer perceptron. In the experiments we have used Object Tracking\nand Classification Beyond Visible Spectrum (OTCBVS) database. Experimental\nresults show that the proposed approach significantly improves the verification\nand identification performance and the success rate is 99.25%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 08:10:30 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1007.1016", "submitter": "Oleg Pianykh", "authors": "Oleg S. Pianykh", "title": "Bilateral filters: what they can and cannot do", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear bilateral filters (BF) deliver a fine blend of computational\nsimplicity and blur-free denoising. However, little is known about their\nnature, noise-suppressing properties, and optimal choices of filter parameters.\nOur study is meant to fill this gap-explaining the underlying mechanism of\nbilateral filtering and providing the methodology for optimal filter selection.\nPractical application to CT image denoising is discussed to illustrate our\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2010 23:25:39 GMT"}], "update_date": "2010-07-08", "authors_parsed": [["Pianykh", "Oleg S.", ""]]}, {"id": "1007.1048", "submitter": "Dhamodaran Sasikala", "authors": "D.Sasikala and R.Neelaveni", "title": "Registration of Brain Images using Fast Walsh Hadamard Transform", "comments": "10 pages, 37 figures, 12 tables", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 8, No. 2, May 2010", "doi": "10.5120/1745-2053", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A lot of image registration techniques have been developed with great\nsignificance for data analysis in medicine, astrophotography, satellite imaging\nand few other areas. This work proposes a method for medical image registration\nusing Fast Walsh Hadamard transform. This algorithm registers images of the\nsame or different modalities. Each image bit is lengthened in terms of Fast\nWalsh Hadamard basis functions. Each basis function is a notion of determining\nvarious aspects of local structure, e.g., horizontal edge, corner, etc. These\ncoefficients are normalized and used as numerals in a chosen number system\nwhich allows one to form a unique number for each type of local structure. The\nexperimental results show that Fast Walsh Hadamard transform accomplished\nbetter results than the conventional Walsh transform in the time domain. Also\nFast Walsh Hadamard transform is more reliable in medical image registration\nconsuming less time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 04:49:16 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Sasikala", "D.", ""], ["Neelaveni", "R.", ""]]}, {"id": "1007.1398", "submitter": "Raphael Sznitman", "authors": "Raphael Sznitman, Manaswi Gupta, Gregory D. Hager, Paulo E. Arratia,\n  and Josue Sznitman", "title": "Multi-environment model estimation for motility analysis of\n  Caenorhabditis Elegans", "comments": "21 pages, 8 figures, accepted in: PLoS ONE (2010)", "journal-ref": null, "doi": "10.1371/journal.pone.0011631", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nematode Caenorhabditis elegans is a well-known model organism used to\ninvestigate fundamental questions in biology. Motility assays of this small\nroundworm are designed to study the relationships between genes and behavior.\nCommonly, motility analysis is used to classify nematode movements and\ncharacterize them quantitatively. Over the past years, C. elegans' motility has\nbeen studied across a wide range of environments, including crawling on\nsubstrates, swimming in fluids, and locomoting through microfluidic substrates.\nHowever, each environment often requires customized image processing tools\nrelying on heuristic parameter tuning. In the present study, we propose a novel\nMulti-Environment Model Estimation (MEME) framework for automated image\nsegmentation that is versatile across various environments. The MEME platform\nis constructed around the concept of Mixture of Gaussian (MOG) models, where\nstatistical models for both the background environment and the nematode\nappearance are explicitly learned and used to accurately segment a target\nnematode. Our method is designed to simplify the burden often imposed on users;\nhere, only a single image which includes a nematode in its environment must be\nprovided for model learning. In addition, our platform enables the extraction\nof nematode `skeletons' for straightforward motility quantification. We test\nour algorithm on various locomotive environments and compare performances with\nan intensity-based thresholding method. Overall, MEME outperforms the\nthreshold-based approach for the overwhelming majority of cases examined.\nUltimately, MEME provides researchers with an attractive platform for C.\nelegans' segmentation and `skeletonizing' across a wide range of motility\nassays.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 15:10:05 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Sznitman", "Raphael", ""], ["Gupta", "Manaswi", ""], ["Hager", "Gregory D.", ""], ["Arratia", "Paulo E.", ""], ["Sznitman", "Josue", ""]]}, {"id": "1007.1432", "submitter": "Edward Rosten", "authors": "Edward Rosten, Gerhard Reitmayr, Tom Drummond", "title": "Improved RANSAC performance using simple, iterative minimal-set solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RANSAC is a popular technique for estimating model parameters in the presence\nof outliers. The best speed is achieved when the minimum possible number of\npoints is used to estimate hypotheses for the model. Many useful problems can\nbe represented using polynomial constraints (for instance, the determinant of a\nfundamental matrix must be zero) and so have a number of solutions which are\nconsistent with a minimal set. A considerable amount of effort has been\nexpended on finding the constraints of such problems, and these often require\nthe solution of systems of polynomial equations. We show that better\nperformance can be achieved by using a simple optimization based approach on\nminimal sets. For a given minimal set, the optimization approach is not\nguaranteed to converge to the correct solution. However, when used within\nRANSAC the greater speed and numerical stability results in better performance\noverall, and much simpler algorithms. We also show that by selecting more than\nthe minimal number of points and using robust optimization can yield better\nresults for very noisy by reducing the number of trials required. The increased\nspeed of our method demonstrated with experiments on essential matrix\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 18:12:49 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Rosten", "Edward", ""], ["Reitmayr", "Gerhard", ""], ["Drummond", "Tom", ""]]}, {"id": "1007.1708", "submitter": "Kadirvelu SivaKumar", "authors": "Lim Huey Charn, Liyana Nuraini Rasid, Shahrel A. Suandi", "title": "A Study on the Effectiveness of Different Patch Size and Shape for Eyes\n  and Mouth Detection", "comments": "9 Pages", "journal-ref": "(IJCSE) International Journal on Computer Science and Engineering,\n  Vol. 2, No. 3, May 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template matching is one of the simplest methods used for eyes and mouth\ndetection. However, it can be modified and extended to become a powerful tool.\nSince the patch itself plays a significant role in optimizing detection\nperformance, a study on the influence of patch size and shape is carried out.\nThe optimum patch size and shape is determined using the proposed method.\nUsually, template matching is also combined with other methods in order to\nimprove detection accuracy. Thus, in this paper, the effectiveness of two image\nprocessing methods i.e. grayscale and Haar wavelet transform, when used with\ntemplate matching are analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2010 09:01:48 GMT"}], "update_date": "2010-07-13", "authors_parsed": [["Charn", "Lim Huey", ""], ["Rasid", "Liyana Nuraini", ""], ["Suandi", "Shahrel A.", ""]]}, {"id": "1007.2442", "submitter": "Hod Lipson", "authors": "Kyle Johnson, Clayton Chang, and Hod Lipson", "title": "Neural Network Based Reconstruction of a 3D Object from a 2D Wireframe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for constructing a 3D representation from a 2D\nwireframe drawing. A drawing is simply a parallel projection of a 3D object\nonto a 2D surface; humans are able to recreate mental 3D models from 2D\nrepresentations very easily, yet the process is very difficult to emulate\ncomputationally. We hypothesize that our ability to perform this construction\nrelies on the angles in the 2D scene, among other geometric properties. Being\nable to reproduce this reconstruction process automatically would allow for\nefficient and robust 3D sketch interfaces. Our research focuses on the\nrelationship between 2D geometry observable in the sketch and 3D geometry\nderived from a potential 3D construction. We present a fully automated system\nthat constructs 3D representations from 2D wireframes using a neural network in\nconjunction with a genetic search algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 22:01:26 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Johnson", "Kyle", ""], ["Chang", "Clayton", ""], ["Lipson", "Hod", ""]]}, {"id": "1007.2958", "submitter": "Hoang Trinh", "authors": "Hoang Trinh", "title": "A Machine Learning Approach to Recovery of Scene Geometry from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D structure of the scene from images yields useful\ninformation for tasks such as shape and scene recognition, object detection, or\nmotion planning and object grasping in robotics. In this thesis, we introduce a\ngeneral machine learning approach called unsupervised CRF learning based on\nmaximizing the conditional likelihood. We apply our approach to computer vision\nsystems that recover the 3-D scene geometry from images. We focus on recovering\n3D geometry from single images, stereo pairs and video sequences. Building\nthese systems requires algorithms for doing inference as well as learning the\nparameters of conditional Markov random fields (MRF). Our system is trained\nunsupervisedly without using ground-truth labeled data. We employ a\nslanted-plane stereo vision model in which we use a fixed over-segmentation to\nsegment the left image into coherent regions called superpixels, then assign a\ndisparity plane for each superpixel. Plane parameters are estimated by solving\nan MRF labelling problem, through minimizing an energy fuction. We demonstrate\nthe use of our unsupervised CRF learning algorithm for a parameterized\nslanted-plane stereo vision model involving shape from texture cues. Our stereo\nmodel with texture cues, only by unsupervised training, outperforms the results\nin related work on the same stereo dataset. In this thesis, we also formulate\nstructure and motion estimation as an energy minimization problem, in which the\nmodel is an extension of our slanted-plane stereo vision model that also\nhandles surface velocity. Velocity estimation is achieved by solving an MRF\nlabeling problem using Loopy BP. Performance analysis is done using our novel\nevaluation metrics based on the notion of view prediction error. Experiments on\nroad-driving stereo sequences show encouraging results.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2010 19:59:11 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Trinh", "Hoang", ""]]}, {"id": "1007.3753", "submitter": "Allen Yang", "authors": "Allen Y. Yang, Zihan Zhou, Arvind Ganesh, S. Shankar Sastry, and Yi Ma", "title": "Fast L1-Minimization Algorithms For Robust Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L1-minimization refers to finding the minimum L1-norm solution to an\nunderdetermined linear system b=Ax. Under certain conditions as described in\ncompressive sensing theory, the minimum L1-norm solution is also the sparsest\nsolution. In this paper, our study addresses the speed and scalability of its\nalgorithms. In particular, we focus on the numerical implementation of a\nsparsity-based classification framework in robust face recognition, where\nsparse representation is sought to recover human identities from very\nhigh-dimensional facial images that may be corrupted by illumination, facial\ndisguise, and pose variation. Although the underlying numerical problem is a\nlinear program, traditional algorithms are known to suffer poor scalability for\nlarge-scale applications. We investigate a new solution based on a classical\nconvex optimization framework, known as Augmented Lagrangian Methods (ALM). The\nnew convex solvers provide a viable solution to real-world, time-critical\napplications such as face recognition. We conduct extensive experiments to\nvalidate and compare the performance of the ALM algorithms against several\npopular L1-minimization solvers, including interior-point method, Homotopy,\nFISTA, SESOP-PCD, approximate message passing (AMP) and TFOCS. To aid peer\nevaluation, the code for all the algorithms has been made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 20:26:26 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2010 10:15:09 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2012 05:22:51 GMT"}, {"version": "v4", "created": "Sun, 26 Aug 2012 23:17:25 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Yang", "Allen Y.", ""], ["Zhou", "Zihan", ""], ["Ganesh", "Arvind", ""], ["Sastry", "S. Shankar", ""], ["Ma", "Yi", ""]]}, {"id": "1007.3772", "submitter": "Stephen O'Hara", "authors": "Stephen O'Hara", "title": "Video Event Recognition for Surveillance Applications (VERSA)", "comments": "Master's Thesis, University of Nebraska at Omaha, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VERSA provides a general-purpose framework for defining and recognizing\nevents in live or recorded surveillance video streams. The approach for event\nrecognition in VERSA is using a declarative logic language to define the\nspatial and temporal relationships that characterize a given event or activity.\nDoing so requires the definition of certain fundamental spatial and temporal\nrelationships and a high-level syntax for specifying frame templates and query\nparameters. Although the handling of uncertainty in the current VERSA\nimplementation is simplistic, the language and architecture is amenable to\nextending using Fuzzy Logic or similar approaches. VERSA's high-level\narchitecture is designed to work in XML-based, services- oriented environments.\nVERSA can be thought of as subscribing to the XML annotations streamed by a\nlower-level video analytics service that provides basic entity detection,\nlabeling, and tracking. One or many VERSA Event Monitors could thus analyze\nvideo streams and provide alerts when certain events are detected.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 22:47:00 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["O'Hara", "Stephen", ""]]}, {"id": "1007.3881", "submitter": "Vasil Kolev", "authors": "Vasil Kolev", "title": "Orthogonal multifilters image processing of astronomical images from\n  scanned photographic plates", "comments": "6 pages, The ACM proceedings of CompSysTech 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper orthogonal multifilters for astronomical image processing are\npresented. We obtained new orthogonal multifilters based on the orthogonal\nwavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as\na more powerful multiscale analysis tool. It adds several degrees of freedom in\nmultifilter design and makes it possible to have several useful properties such\nas symmetry, orthogonality, short support, and a higher number of vanishing\nmoments simultaneously. Multifilter decomposition of scanned photographic\nplates with astronomical images is made.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 13:22:50 GMT"}, {"version": "v2", "created": "Sat, 16 Oct 2010 17:01:02 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Kolev", "Vasil", ""]]}, {"id": "1007.3926", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Phalguni Gupta, Jamuna Kanta Sing", "title": "Ear Identification by Fusion of Segmented Slice Regions using Invariant\n  Features: An Experimental Manifold with Dual Fusion Approach", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes a robust ear identification system which is developed by\nfusing SIFT features of color segmented slice regions of an ear. The proposed\near identification method makes use of Gaussian mixture model (GMM) to build\near model with mixture of Gaussian using vector quantization algorithm and K-L\ndivergence is applied to the GMM framework for recording the color similarity\nin the specified ranges by comparing color similarity between a pair of\nreference ear and probe ear. SIFT features are then detected and extracted from\neach color slice region as a part of invariant feature extraction. The\nextracted keypoints are then fused separately by the two fusion approaches,\nnamely concatenation and the Dempster-Shafer theory. Finally, the fusion\napproaches generate two independent augmented feature vectors which are used\nfor identification of individuals separately. The proposed identification\ntechnique is tested on IIT Kanpur ear database of 400 individuals and is found\nto achieve 98.25% accuracy for identification while top 5 matched criteria is\nset for each subject.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 14:09:33 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1007.4531", "submitter": "Barak Fishbain", "authors": "Barak Fishbain, Dorit S. Hochbaum, Stefan Mueller", "title": "Competitive Analysis of Minimum-Cut Maximum Flow Algorithms in Vision\n  Problems", "comments": null, "journal-ref": "Journal of Real-Time Image Processing, March 2016, Vol. 11, Issue\n  3, pp 589-609. (Online April 2013.)", "doi": "10.1007/s11554-013-0344-3.", "report-no": null, "categories": "cs.CV cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in image acquisition and storage technology underline the need\nfor algorithms that are capable of solving large scale image processing and\ncomputer-vision problems. The minimum cut problem plays an important role in\nprocessing many of these imaging problems such as, image and video\nsegmentation, stereo vision, multi-view reconstruction and surface fitting.\nWhile several min-cut/max-flow algorithms can be found in the literature, their\nperformance in practice has been studied primarily outside the scope of\ncomputer vision. We present here the results of a comprehensive computational\nstudy, in terms of execution times and memory utilization, of four recently\npublished algorithms, which optimally solve the {\\em s-t} cut and maximum flow\nproblems: (i) Goldberg's and Tarjan's {\\em Push-Relabel}; (ii) Hochbaum's {\\em\npseudoflow}; (iii) Boykov's and Kolmogorov's {\\em augmenting paths}; and (iv)\nGoldberg's {\\em partial augment-relabel}. Our results demonstrate that the {\\em\nHochbaum's pseudoflow} algorithm, is faster and utilizes less memory than the\nother algorithms on all problem instances investigated.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2010 18:58:32 GMT"}, {"version": "v2", "created": "Mon, 18 Oct 2010 21:31:15 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Fishbain", "Barak", ""], ["Hochbaum", "Dorit S.", ""], ["Mueller", "Stefan", ""]]}, {"id": "1007.5129", "submitter": "Secretary  Ijaia", "authors": "Mohammed J. Islam, Majid Ahmadi and Maher A. Sid-Ahmed (University of\n  Windsor, Canada)", "title": "An Efficient Automatic Mass Classification Method In Digitized\n  Mammograms Using Artificial Neural Network", "comments": "13 pages, 10 figures", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  1.3 (2010) 1-13", "doi": "10.5121/ijaia.2010.1301", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we present an efficient computer aided mass classification\nmethod in digitized mammograms using Artificial Neural Network (ANN), which\nperforms benign-malignant classification on region of interest (ROI) that\ncontains mass. One of the major mammographic characteristics for mass\nclassification is texture. ANN exploits this important factor to classify the\nmass into benign or malignant. The statistical textural features used in\ncharacterizing the masses are mean, standard deviation, entropy, skewness,\nkurtosis and uniformity. The main aim of the method is to increase the\neffectiveness and efficiency of the classification process in an objective\nmanner to reduce the numbers of false-positive of malignancies. Three layers\nartificial neural network (ANN) with seven features was proposed for\nclassifying the marked regions into benign and malignant and 90.91% sensitivity\nand 83.87% specificity is achieved that is very much promising compare to the\nradiologist's sensitivity 75%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2010 07:19:58 GMT"}], "update_date": "2010-07-30", "authors_parsed": [["Islam", "Mohammed J.", "", "University of\n  Windsor, Canada"], ["Ahmadi", "Majid", "", "University of\n  Windsor, Canada"], ["Sid-Ahmed", "Maher A.", "", "University of\n  Windsor, Canada"]]}]