[{"id": "1704.00033", "submitter": "Xingyu Lin", "authors": "Xingyu Lin, Hao Wang, Zhihao Li, Yimeng Zhang, Alan Yuille, Tai Sing\n  Lee", "title": "Transfer of View-manifold Learning to Similarity Perception of Novel\n  Objects", "comments": "Accepted to ICLR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model of perceptual similarity judgment based on re-training a\ndeep convolution neural network (DCNN) that learns to associate different views\nof each 3D object to capture the notion of object persistence and continuity in\nour visual experience. The re-training process effectively performs distance\nmetric learning under the object persistency constraints, to modify the\nview-manifold of object representations. It reduces the effective distance\nbetween the representations of different views of the same object without\ncompromising the distance between those of the views of different objects,\nresulting in the untangling of the view-manifolds between individual objects\nwithin the same category and across categories. This untangling enables the\nmodel to discriminate and recognize objects within the same category,\nindependent of viewpoints. We found that this ability is not limited to the\ntrained objects, but transfers to novel objects in both trained and untrained\ncategories, as well as to a variety of completely novel artificial synthetic\nobjects. This transfer in learning suggests the modification of distance\nmetrics in view- manifolds is more general and abstract, likely at the levels\nof parts, and independent of the specific objects or categories experienced\nduring training. Interestingly, the resulting transformation of feature\nrepresentation in the deep networks is found to significantly better match\nhuman perceptual similarity judgment than AlexNet, suggesting that object\npersistence could be an important constraint in the development of perceptual\nsimilarity judgment in biological neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 19:50:06 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Lin", "Xingyu", ""], ["Wang", "Hao", ""], ["Li", "Zhihao", ""], ["Zhang", "Yimeng", ""], ["Yuille", "Alan", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1704.00036", "submitter": "Xiao Yang", "authors": "Xu Han, Xiao Yang, Stephen Aylward, Roland Kwitt, Marc Niethammer", "title": "Efficient Registration of Pathological Images: A Joint\n  PCA/Image-Reconstruction Approach", "comments": "Accepted as a conference paper for ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration involving one or more images containing pathologies is\nchallenging, as standard image similarity measures and spatial transforms\ncannot account for common changes due to pathologies. Low-rank/Sparse (LRS)\ndecomposition removes pathologies prior to registration; however, LRS is\nmemory-demanding and slow, which limits its use on larger data sets.\nAdditionally, LRS blurs normal tissue regions, which may degrade registration\nperformance. This paper proposes an efficient alternative to LRS: (1) normal\ntissue appearance is captured by principal component analysis (PCA) and (2)\nblurring is avoided by an integrated model for pathology removal and image\nreconstruction. Results on synthetic and BRATS 2015 data demonstrate its\nutility.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 19:57:12 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Han", "Xu", ""], ["Yang", "Xiao", ""], ["Aylward", "Stephen", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""]]}, {"id": "1704.00077", "submitter": "Hieu Le", "authors": "Hieu Le, Vu Nguyen, Chen-Ping Yu, Dimitris Samaras", "title": "Geodesic Distance Histogram Feature for Video Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-54181-5_18", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a geodesic-distance-based feature that encodes global\ninformation for improved video segmentation algorithms. The feature is a joint\nhistogram of intensity and geodesic distances, where the geodesic distances are\ncomputed as the shortest paths between superpixels via their boundaries. We\nalso incorporate adaptive voting weights and spatial pyramid configurations to\ninclude spatial information into the geodesic histogram feature and show that\nthis further improves results. The feature is generic and can be used as part\nof various algorithms. In experiments, we test the geodesic histogram feature\nby incorporating it into two existing video segmentation frameworks. This leads\nto significantly better performance in 3D video segmentation benchmarks on two\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 22:39:32 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Le", "Hieu", ""], ["Nguyen", "Vu", ""], ["Yu", "Chen-Ping", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1704.00083", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba, Shin Ishii", "title": "Efficient Asymmetric Co-Tracking using Uncertainty Sampling", "comments": "Submitted to IEEE ICSIPA'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive tracking-by-detection approaches are popular for tracking arbitrary\nobjects. They treat the tracking problem as a classification task and use\nonline learning techniques to update the object model. However, these\napproaches are heavily invested in the efficiency and effectiveness of their\ndetectors. Evaluating a massive number of samples for each frame (e.g.,\nobtained by a sliding window) forces the detector to trade the accuracy in\nfavor of speed. Furthermore, misclassification of borderline samples in the\ndetector introduce accumulating errors in tracking. In this study, we propose a\nco-tracking based on the efficient cooperation of two detectors: a rapid\nadaptive exemplar-based detector and another more sophisticated but slower\ndetector with a long-term memory. The sampling labeling and co-learning of the\ndetectors are conducted by an uncertainty sampling unit, which improves the\nspeed and accuracy of the system. We also introduce a budgeting mechanism which\nprevents the unbounded growth in the number of examples in the first detector\nto maintain its rapid response. Experiments demonstrate the efficiency and\neffectiveness of the proposed tracker against its baselines and its superior\nperformance against state-of-the-art trackers on various benchmark videos.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 23:28:20 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Mirzaei", "Maryam Sadat", ""], ["Oba", "Shigeyuki", ""], ["Ishii", "Shin", ""]]}, {"id": "1704.00085", "submitter": "Cheng Peng", "authors": "Cheng Peng and Volkan Isler", "title": "View Selection with Geometric Uncertainty Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating positions of world points from features observed in images is a\nkey problem in 3D reconstruction, image mosaicking,simultaneous localization\nand mapping and structure from motion. We consider a special instance in which\nthere is a dominant ground plane $\\mathcal{G}$ viewed from a parallel viewing\nplane $\\mathcal{S}$ above it. Such instances commonly arise, for example, in\naerial photography. Consider a world point $g \\in \\mathcal{G}$ and its worst\ncase reconstruction uncertainty $\\varepsilon(g,\\mathcal{S})$ obtained by\nmerging \\emph{all} possible views of $g$ chosen from $\\mathcal{S}$. We first\nshow that one can pick two views $s_p$ and $s_q$ such that the uncertainty\n$\\varepsilon(g,\\{s_p,s_q\\})$ obtained using only these two views is almost as\ngood as (i.e. within a small constant factor of) $\\varepsilon(g,\\mathcal{S})$.\nNext, we extend the result to the entire ground plane $\\mathcal{G}$ and show\nthat one can pick a small subset of $\\mathcal{S'} \\subseteq \\mathcal{S}$ (which\ngrows only linearly with the area of $\\mathcal{G}$) and still obtain a constant\nfactor approximation, for every point $g \\in \\mathcal{G}$, to the minimum worst\ncase estimate obtained by merging all views in $\\mathcal{S}$. Finally, we\npresent a multi-resolution view selection method which extends our techniques\nto non-planar scenes. We show that the method can produce rich and accurate\ndense reconstructions with a small number of views. Our results provide a view\nselection mechanism with provable performance guarantees which can drastically\nincrease the speed of scene reconstruction algorithms. In addition to\ntheoretical results, we demonstrate their effectiveness in an application where\naerial imagery is used for monitoring farms and orchards.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 23:33:06 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 23:46:34 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Peng", "Cheng", ""], ["Isler", "Volkan", ""]]}, {"id": "1704.00090", "submitter": "Marc-Andr\\'e Gardner", "authors": "Marc-Andr\\'e Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen,\n  Emiliano Gambaretto, Christian Gagn\\'e, Jean-Fran\\c{c}ois Lalonde", "title": "Learning to Predict Indoor Illumination from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic method to infer high dynamic range illumination from\na single, limited field-of-view, low dynamic range photograph of an indoor\nscene. In contrast to previous work that relies on specialized image capture,\nuser input, and/or simple scene models, we train an end-to-end deep neural\nnetwork that directly regresses a limited field-of-view photo to HDR\nillumination, without strong assumptions on scene geometry, material\nproperties, or lighting. We show that this can be accomplished in a three step\nprocess: 1) we train a robust lighting classifier to automatically annotate the\nlocation of light sources in a large dataset of LDR environment maps, 2) we use\nthese annotations to train a deep neural network that predicts the location of\nlights in a scene from a single limited field-of-view photo, and 3) we\nfine-tune this network using a small dataset of HDR environment maps to predict\nlight intensities. This allows us to automatically recover high-quality HDR\nillumination estimates that significantly outperform previous state-of-the-art\nmethods. Consequently, using our illumination estimates for applications like\n3D object insertion, we can achieve results that are photo-realistic, which is\nvalidated via a perceptual user study.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 00:50:12 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 19:20:01 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 08:32:24 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Gardner", "Marc-Andr\u00e9", ""], ["Sunkavalli", "Kalyan", ""], ["Yumer", "Ersin", ""], ["Shen", "Xiaohui", ""], ["Gambaretto", "Emiliano", ""], ["Gagn\u00e9", "Christian", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1704.00098", "submitter": "Shan Su", "authors": "Shan Su, Jianbo Shi, Hyun Soo Park", "title": "Customizing First Person Image Through Desired Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a problem of inverse visual path planning: creating a\nvisual scene from a first person action. Our conjecture is that the spatial\narrangement of a first person visual scene is deployed to afford an action, and\ntherefore, the action can be inversely used to synthesize a new scene such that\nthe action is feasible. As a proof-of-concept, we focus on linking visual\nexperiences induced by walking.\n  A key innovation of this paper is a concept of ActionTunnel---a 3D virtual\ntunnel along the future trajectory encoding what the wearer will visually\nexperience as moving into the scene. This connects two distinctive first person\nimages through similar walking paths. Our method takes a first person image\nwith a user defined future trajectory and outputs a new image that can afford\nthe future motion. The image is created by combining present and future\nActionTunnels in 3D where the missing pixels in adjoining area are computed by\na generative adversarial network. Our work can provide a travel across\ndifferent first person experiences in diverse real world scenes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 01:55:28 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Su", "Shan", ""], ["Shi", "Jianbo", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1704.00103", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Theerasit Issaranon, David Forsyth", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to produce a network where current methods such as\nDeepFool have great difficulty producing adversarial samples. Our construction\nsuggests some insights into how deep networks work. We provide a reasonable\nanalyses that our construction is difficult to defeat, and show experimentally\nthat our method is hard to defeat with both Type I and Type II attacks using\nseveral standard networks and datasets. This SafetyNet architecture is used to\nan important and novel application SceneProof, which can reliably detect\nwhether an image is a picture of a real scene or not. SceneProof applies to\nimages captured with depth maps (RGBD images) and checks if a pair of image and\ndepth map is consistent. It relies on the relative difficulty of producing\nnaturalistic depth maps for images in post processing. We demonstrate that our\nSafetyNet is robust to adversarial examples built from currently known\nattacking approaches.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 02:12:40 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 05:59:38 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Lu", "Jiajun", ""], ["Issaranon", "Theerasit", ""], ["Forsyth", "David", ""]]}, {"id": "1704.00112", "submitter": "Yixin Zhu", "authors": "Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin,\n  Lap-Fai Yu, Demetri Terzopoulos, Song-Chun Zhu", "title": "Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel\n  Ground Truth using Stochastic Grammars", "comments": "Accepted in IJCV 2018", "journal-ref": null, "doi": "10.1007/s11263-018-1103-5", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a systematic learning-based approach to the generation of massive\nquantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D\nimages thereof, with associated ground truth information, for the purposes of\ntraining, benchmarking, and diagnosing learning-based computer vision and\nrobotics algorithms. In particular, we devise a learning-based pipeline of\nalgorithms capable of automatically generating and rendering a potentially\ninfinite variety of indoor scenes by using a stochastic grammar, represented as\nan attributed Spatial And-Or Graph, in conjunction with state-of-the-art\nphysics-based rendering. Our pipeline is capable of synthesizing scene layouts\nwith high diversity, and it is configurable inasmuch as it enables the precise\ncustomization and control of important attributes of the generated scenes. It\nrenders photorealistic RGB images of the generated scenes while automatically\nsynthesizing detailed, per-pixel ground truth data, including visible surface\ndepth and normal, object identity, and material information (detailed to object\nparts), as well as environments (e.g., illuminations and camera viewpoints). We\ndemonstrate the value of our synthesized dataset, by improving performance in\ncertain machine-learning-based scene understanding tasks--depth and surface\nnormal prediction, semantic segmentation, reconstruction, etc.--and by\nproviding benchmarks for and diagnostics of trained models by modifying object\nattributes and scene properties in a controllable manner.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 03:05:29 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 00:50:58 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 15:24:55 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Jiang", "Chenfanfu", ""], ["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Huang", "Siyuan", ""], ["Lin", "Jenny", ""], ["Yu", "Lap-Fai", ""], ["Terzopoulos", "Demetri", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1704.00138", "submitter": "Peng Tang", "authors": "Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu", "title": "Multiple Instance Detection Network with Online Instance Classifier\n  Refinement", "comments": "Accepted by CVPR 2017, IEEE Conference on Computer Vision and Pattern\n  Recognition 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of late, weakly supervised object detection is with great importance in\nobject recognition. Based on deep learning, weakly supervised detectors have\nachieved many promising results. However, compared with fully supervised\ndetection, it is more challenging to train deep network based detectors in a\nweakly supervised manner. Here we formulate weakly supervised detection as a\nMultiple Instance Learning (MIL) problem, where instance classifiers (object\ndetectors) are put into the network as hidden nodes. We propose a novel online\ninstance classifier refinement algorithm to integrate MIL and the instance\nclassifier refinement procedure into a single deep network, and train the\nnetwork end-to-end with only image-level supervision, i.e., without object\nlocation information. More precisely, instance labels inferred from weak\nsupervision are propagated to their spatially overlapped instances to refine\ninstance classifier online. The iterative instance classifier refinement\nprocedure is implemented using multiple streams in deep network, where each\nstream supervises its latter stream. Weakly supervised object detection\nexperiments are carried out on the challenging PASCAL VOC 2007 and 2012\nbenchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the\nprevious state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 08:32:40 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tang", "Peng", ""], ["Wang", "Xinggang", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""]]}, {"id": "1704.00159", "submitter": "Xiao Sun", "authors": "Xiao Sun, Jiaxiang Shang, Shuang Liang, Yichen Wei", "title": "Compositional Human Pose Regression", "comments": "Accepted by International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression based methods are not performing as well as detection based\nmethods for human pose estimation. A central problem is that the structural\ninformation in the pose is not well exploited in the previous regression\nmethods. In this work, we propose a structure-aware regression approach. It\nadopts a reparameterized pose representation using bones instead of joints. It\nexploits the joint connection structure to define a compositional loss function\nthat encodes the long range interactions in the pose. It is simple, effective,\nand general for both 2D and 3D pose estimation in a unified setting.\nComprehensive evaluation validates the effectiveness of our approach. It\nsignificantly advances the state-of-the-art on Human3.6M and is competitive\nwith state-of-the-art results on MPII.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 11:59:41 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 03:03:11 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 03:31:39 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Sun", "Xiao", ""], ["Shang", "Jiaxiang", ""], ["Liang", "Shuang", ""], ["Wei", "Yichen", ""]]}, {"id": "1704.00180", "submitter": "Manoel Horta Ribeiro", "authors": "Manoel Horta Ribeiro, Bruno Teixeira, Ant\\^onio Ot\\'avio Fernandes,\n  Wagner Meira Jr., Erickson R. Nascimento", "title": "Complexity-Aware Assignment of Latent Values in Discriminative Models\n  for Accurate Gesture Recognition", "comments": "Conference paper published at 2016 29th SIBGRAPI, Conference on\n  Graphics, Patterns and Images (SIBGRAPI). 8 pages, 7 figures", "journal-ref": null, "doi": "10.1109/SIBGRAPI.2016.059", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the state-of-the-art algorithms for gesture recognition are based on\nConditional Random Fields (CRFs). Successful approaches, such as the\nLatent-Dynamic CRFs, extend the CRF by incorporating latent variables, whose\nvalues are mapped to the values of the labels. In this paper we propose a novel\nmethodology to set the latent values according to the gesture complexity. We\nuse an heuristic that iterates through the samples associated with each label\nvalue, stimating their complexity. We then use it to assign the latent values\nto the label values. We evaluate our method on the task of recognizing human\ngestures from video streams. The experiments were performed in binary datasets,\ngenerated by grouping different labels. Our results demonstrate that our\napproach outperforms the arbitrary one in many cases, increasing the accuracy\nby up to 10%.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 15:15:38 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Ribeiro", "Manoel Horta", ""], ["Teixeira", "Bruno", ""], ["Fernandes", "Ant\u00f4nio Ot\u00e1vio", ""], ["Meira", "Wagner", "Jr."], ["Nascimento", "Erickson R.", ""]]}, {"id": "1704.00248", "submitter": "Shuang Ma", "authors": "Shuang Ma, Jing Liu and Chang Wen Chen", "title": "A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural\n  Network for Photo Aesthetic Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have recently been shown to generate\npromising results for aesthetics assessment. However, the performance of these\ndeep CNN methods is often compromised by the constraint that the neural network\nonly takes the fixed-size input. To accommodate this requirement, input images\nneed to be transformed via cropping, warping, or padding, which often alter\nimage composition, reduce image resolution, or cause image distortion. Thus the\naesthetics of the original images is impaired because of potential loss of fine\ngrained details and holistic image layout. However, such fine grained details\nand holistic image layout is critical for evaluating an image's aesthetics. In\nthis paper, we present an Adaptive Layout-Aware Multi-Patch Convolutional\nNeural Network (A-Lamp CNN) architecture for photo aesthetic assessment. This\nnovel scheme is able to accept arbitrary sized images, and learn from both\nfined grained details and holistic image layout simultaneously. To enable\ntraining on these hybrid inputs, we extend the method by developing a dedicated\ndouble-subnet neural network structure, i.e. a Multi-Patch subnet and a\nLayout-Aware subnet. We further construct an aggregation layer to effectively\ncombine the hybrid features from these two subnets. Extensive experiments on\nthe large-scale aesthetics assessment benchmark (AVA) demonstrate significant\nperformance improvement over the state-of-the-art in photo aesthetic\nassessment.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 03:12:18 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Ma", "Shuang", ""], ["Liu", "Jing", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1704.00260", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Kevin Shih, Saurabh Singh, and Derek Hoiem", "title": "Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks", "comments": "Accepted in ICCV 2017. The arxiv version has an extra analysis on\n  correlation with human attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 08:01:30 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 05:34:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gupta", "Tanmay", ""], ["Shih", "Kevin", ""], ["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""]]}, {"id": "1704.00275", "submitter": "Giovanni Poggi", "authors": "G. Chierchia and D. Cozzolino and G. Poggi and L. Verdoliva", "title": "SAR image despeckling through convolutional neural networks", "comments": "Accepted at 2017 IEEE International Geoscience and Remote Sensing\n  Symposium, Fort Worth, Texas, July 23-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the use of discriminative model learning through\nConvolutional Neural Networks (CNNs) for SAR image despeckling. The network\nuses a residual learning strategy, hence it does not recover the filtered\nimage, but the speckle component, which is then subtracted from the noisy one.\nTraining is carried out by considering a large multitemporal SAR image and its\nmultilook version, in order to approximate a clean image. Experimental results,\nboth on synthetic and real SAR data, show the method to achieve better\nperformance with respect to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 09:44:05 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 15:53:02 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Chierchia", "G.", ""], ["Cozzolino", "D.", ""], ["Poggi", "G.", ""], ["Verdoliva", "L.", ""]]}, {"id": "1704.00280", "submitter": "Marius Cordts", "authors": "Marius Cordts, Timo Rehfeld, Lukas Schneider, David Pfeiffer, Markus\n  Enzweiler, Stefan Roth, Marc Pollefeys, Uwe Franke", "title": "The Stixel world: A medium-level representation of traffic scenes", "comments": "Accepted for publication in Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2017.01.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in advanced driver assistance systems and the race towards\nautonomous vehicles is mainly driven by two factors: (1) increasingly\nsophisticated algorithms that interpret the environment around the vehicle and\nreact accordingly, and (2) the continuous improvements of sensor technology\nitself. In terms of cameras, these improvements typically include higher\nspatial resolution, which as a consequence requires more data to be processed.\nThe trend to add multiple cameras to cover the entire surrounding of the\nvehicle is not conducive in that matter. At the same time, an increasing number\nof special purpose algorithms need access to the sensor input data to correctly\ninterpret the various complex situations that can occur, particularly in urban\ntraffic.\n  By observing those trends, it becomes clear that a key challenge for vision\narchitectures in intelligent vehicles is to share computational resources. We\nbelieve this challenge should be faced by introducing a representation of the\nsensory data that provides compressed and structured access to all relevant\nvisual content of the scene. The Stixel World discussed in this paper is such a\nrepresentation. It is a medium-level model of the environment that is\nspecifically designed to compress information about obstacles by leveraging the\ntypical layout of outdoor traffic scenes. It has proven useful for a multitude\nof automotive vision applications, including object detection, tracking,\nsegmentation, and mapping.\n  In this paper, we summarize the ideas behind the model and generalize it to\ntake into account multiple dense input streams: the image itself, stereo depth\nmaps, and semantic class probability maps that can be generated, e.g., by CNNs.\nOur generalization is embedded into a novel mathematical formulation for the\nStixel model. We further sketch how the free parameters of the model can be\nlearned using structured SVMs.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 10:38:49 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Cordts", "Marius", ""], ["Rehfeld", "Timo", ""], ["Schneider", "Lukas", ""], ["Pfeiffer", "David", ""], ["Enzweiler", "Markus", ""], ["Roth", "Stefan", ""], ["Pollefeys", "Marc", ""], ["Franke", "Uwe", ""]]}, {"id": "1704.00299", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Shigeyuki Oba, Shin Ishii", "title": "Efficient Version-Space Reduction for Visual Tracking", "comments": "CRV'17 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrminative trackers, employ a classification approach to separate the\ntarget from its background. To cope with variations of the target shape and\nappearance, the classifier is updated online with different samples of the\ntarget and the background. Sample selection, labeling and updating the\nclassifier is prone to various sources of errors that drift the tracker. We\nintroduce the use of an efficient version space shrinking strategy to reduce\nthe labeling errors and enhance its sampling strategy by measuring the\nuncertainty of the tracker about the samples. The proposed tracker, utilize an\nensemble of classifiers that represents different hypotheses about the target,\ndiversify them using boosting to provide a larger and more consistent coverage\nof the version-space and tune the classifiers' weights in voting. The proposed\nsystem adjusts the model update rate by promoting the co-training of the\nshort-memory ensemble with a long-memory oracle. The proposed tracker\noutperformed state-of-the-art trackers on different sequences bearing various\ntracking challenges.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 14:00:48 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Oba", "Shigeyuki", ""], ["Ishii", "Shin", ""]]}, {"id": "1704.00326", "submitter": "Alessandro Lameiras Koerich", "authors": "Fabio Dittrich and Luiz E. S. de Oliveira and Alceu S. Britto Jr. and\n  Alessandro L. Koerich", "title": "People Counting in Crowded and Outdoor Scenes using a Hybrid\n  Multi-Camera Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two novel approaches for people counting in crowded and\nopen environments that combine the information gathered by multiple views.\nMultiple camera are used to expand the field of view as well as to mitigate the\nproblem of occlusion that commonly affects the performance of counting methods\nusing single cameras. The first approach is regarded as a direct approach and\nit attempts to segment and count each individual in the crowd. For such an aim,\ntwo head detectors trained with head images are employed: one based on support\nvector machines and another based on Adaboost perceptron. The second approach,\nregarded as an indirect approach employs learning algorithms and statistical\nanalysis on the whole crowd to achieve counting. For such an aim, corner points\nare extracted from groups of people in a foreground image and computed by a\nlearning algorithm which estimates the number of people in the scene. Both\napproaches count the number of people on the scene and not only on a given\nimage or video frame of the scene. The experimental results obtained on the\nbenchmark PETS2009 video dataset show that proposed indirect method surpasses\nother methods with improvements of up to 46.7% and provides accurate counting\nresults for the crowded scenes. On the other hand, the direct method shows high\nerror rates due to the fact that the latter has much more complex problems to\nsolve, such as segmentation of heads.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 16:38:04 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 12:51:51 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Dittrich", "Fabio", ""], ["de Oliveira", "Luiz E. S.", ""], ["Britto", "Alceu S.", "Jr."], ["Koerich", "Alessandro L.", ""]]}, {"id": "1704.00330", "submitter": "Kun He Prof.", "authors": "Kun He, Jingbo Wang, Haochuan Li, Yao Shu, Mengxiao Zhang, Man Zhu,\n  Liwei Wang, John E. Hopcroft", "title": "Randomness in Deconvolutional Networks for Visual Representation", "comments": "15 pages, 10 Figures, submitted to a 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toward a deeper understanding on the inner work of deep neural networks, we\ninvestigate CNN (convolutional neural network) using DCN (deconvolutional\nnetwork) and randomization technique, and gain new insights for the intrinsic\nproperty of this network architecture. For the random representations of an\nuntrained CNN, we train the corresponding DCN to reconstruct the input images.\nCompared with the image inversion on pre-trained CNN, our training converges\nfaster and the yielding network exhibits higher quality for image\nreconstruction. It indicates there is rich information encoded in the random\nfeatures; the pre-trained CNN may discard information irrelevant for\nclassification and encode relevant features in a way favorable for\nclassification but harder for reconstruction. We further explore the property\nof the overall random CNN-DCN architecture. Surprisingly, images can be\ninverted with satisfactory quality. Extensive empirical evidence as well as\ntheoretical analysis are provided.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 17:13:55 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 09:04:21 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 07:19:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["He", "Kun", ""], ["Wang", "Jingbo", ""], ["Li", "Haochuan", ""], ["Shu", "Yao", ""], ["Zhang", "Mengxiao", ""], ["Zhu", "Man", ""], ["Wang", "Liwei", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1704.00331", "submitter": "Claudius Zelenka", "authors": "Claudius Zelenka and Reinhard Koch", "title": "Restoration of Images with Wavefront Aberrations", "comments": "To appear in the proceedings of the 23rd International Conference on\n  Pattern Recognition (ICPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution deals with image restoration in optical systems with\ncoherent illumination, which is an important topic in astronomy, coherent\nmicroscopy and radar imaging. Such optical systems suffer from wavefront\ndistortions, which are caused by imperfect imaging components and conditions.\nKnown image restoration algorithms work well for incoherent imaging, they fail\nin case of coherent images. In this paper a novel wavefront correction\nalgorithm is presented, which allows image restoration under coherent\nconditions. In most coherent imaging systems, especially in astronomy, the\nwavefront deformation is known. Using this information, the proposed algorithm\nallows a high quality restoration even in case of severe wavefront distortions.\nWe present two versions of this algorithm, which are an evolution of the\nGerchberg-Saxton and the Hybrid-Input-Output algorithm. The algorithm is\nverified on simulated and real microscopic images.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 17:19:02 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Zelenka", "Claudius", ""], ["Koch", "Reinhard", ""]]}, {"id": "1704.00337", "submitter": "Yvain Qu\\'eau", "authors": "Yvain Qu\\'eau, Jean M\\'elou, Jean-Denis Durou and Daniel Cremers", "title": "Dense Multi-view 3D-reconstruction Without Dense Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variational method for multi-view shape-from-shading under\nnatural illumination. The key idea is to couple PDE-based solutions for\nsingle-image based shape-from-shading problems across multiple images and\nmultiple color channels by means of a variational formulation. Rather than\nalternatingly solving the individual SFS problems and optimizing the\nconsistency across images and channels which is known to lead to suboptimal\nresults, we propose an efficient solution of the coupled problem by means of an\nADMM algorithm. In numerous experiments on both simulated and real imagery, we\ndemonstrate that the proposed fusion of multiple-view reconstruction and\nshape-from-shading provides highly accurate dense reconstructions without the\nneed to compute dense correspondences. With the proposed variational\nintegration across multiple views shape-from-shading techniques become\napplicable to challenging real-world reconstruction problems, giving rise to\nhighly detailed geometry even in areas of smooth brightness variation and\nlacking texture.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 17:56:47 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Qu\u00e9au", "Yvain", ""], ["M\u00e9lou", "Jean", ""], ["Durou", "Jean-Denis", ""], ["Cremers", "Daniel", ""]]}, {"id": "1704.00389", "submitter": "Yi Zhu", "authors": "Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann", "title": "Hidden Two-Stream Convolutional Networks for Action Recognition", "comments": "Accepted at ACCV 2018, camera ready. Code available at\n  https://github.com/bryanyzhu/Hidden-Two-Stream", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. State-of-the-art action recognition\napproaches rely on traditional optical flow estimation methods to pre-compute\nmotion information for CNNs. Such a two-stage approach is computationally\nexpensive, storage demanding, and not end-to-end trainable. In this paper, we\npresent a novel CNN architecture that implicitly captures motion information\nbetween adjacent frames. We name our approach hidden two-stream CNNs because it\nonly takes raw video frames as input and directly predicts action classes\nwithout explicitly computing optical flow. Our end-to-end approach is 10x\nfaster than its two-stage baseline. Experimental results on four challenging\naction recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show\nthat our approach significantly outperforms the previous best real-time\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 23:39:51 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 21:48:54 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 03:53:21 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 04:55:03 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Lan", "Zhenzhong", ""], ["Newsam", "Shawn", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1704.00390", "submitter": "Alex Kendall", "authors": "Alex Kendall and Roberto Cipolla", "title": "Geometric Loss Functions for Camera Pose Regression with Deep Learning", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown to be effective for robust and real-time monocular\nimage relocalisation. In particular, PoseNet is a deep convolutional neural\nnetwork which learns to regress the 6-DOF camera pose from a single image. It\nlearns to localize using high level features and is robust to difficult\nlighting, motion blur and unknown camera intrinsics, where point based SIFT\nregistration fails. However, it was trained using a naive loss function, with\nhyper-parameters which require expensive tuning. In this paper, we give the\nproblem a more fundamental theoretical treatment. We explore a number of novel\nloss functions for learning camera pose which are based on geometry and scene\nreprojection error. Additionally we show how to automatically learn an optimal\nweighting to simultaneously regress position and orientation. By leveraging\ngeometry, we demonstrate that our technique significantly improves PoseNet's\nperformance across datasets ranging from indoor rooms to a small city.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 23:58:22 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 13:45:48 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Kendall", "Alex", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1704.00406", "submitter": "Le Hou", "authors": "Le Hou, Vu Nguyen, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, Tianhao\n  Zhao, Joel H. Saltz", "title": "Sparse Autoencoder for Unsupervised Nucleus Detection and Representation\n  in Histopathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathology images are crucial to the study of complex diseases such as\ncancer. The histologic characteristics of nuclei play a key role in disease\ndiagnosis, prognosis and analysis. In this work, we propose a sparse\nConvolutional Autoencoder (CAE) for fully unsupervised, simultaneous nucleus\ndetection and feature extraction in histopathology tissue images. Our CAE\ndetects and encodes nuclei in image patches in tissue images into sparse\nfeature maps that encode both the location and appearance of nuclei. Our CAE is\nthe first unsupervised detection network for computer vision applications. The\npretrained nucleus detection and feature extraction modules in our CAE can be\nfine-tuned for supervised learning in an end-to-end fashion. We evaluate our\nmethod on four datasets and reduce the errors of state-of-the-art methods up to\n42%. We are able to achieve comparable performance with only 5% of the\nfully-supervised annotation cost.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 02:12:28 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 16:37:46 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Hou", "Le", ""], ["Nguyen", "Vu", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin M.", ""], ["Gao", "Yi", ""], ["Zhao", "Tianhao", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1704.00438", "submitter": "Jian Zhao", "authors": "Lin Xiong, Jayashree Karlekar, Jian Zhao, Yi Cheng, Yan Xu, Jiashi\n  Feng, Sugiri Pranata, Shengmei Shen", "title": "A Good Practice Towards Top Performance of Face Recognition: Transferred\n  Deep Feature Fusion", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unconstrained face recognition performance evaluations have traditionally\nfocused on Labeled Faces in the Wild (LFW) dataset for imagery and the\nYouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular\nprogress in this field has resulted in saturation on verification and\nidentification accuracies for those benchmark datasets. In this paper, we\npropose a unified learning framework named Transferred Deep Feature Fusion\n(TDFF) targeting at the new IARPA Janus Benchmark A (IJB-A) face recognition\ndataset released by NIST face challenge. The IJB-A dataset includes real-world\nunconstrained faces from 500 subjects with full pose and illumination\nvariations which are much harder than the LFW and YTF datasets. Inspired by\ntransfer learning, we train two advanced deep convolutional neural networks\n(DCNN) with two different large datasets in source domain, respectively. By\nexploring the complementarity of two distinct DCNNs, deep feature fusion is\nutilized after feature extraction in target domain. Then, template specific\nlinear SVMs is adopted to enhance the discrimination of framework. Finally,\nmultiple matching scores corresponding different templates are merged as the\nfinal results. This simple unified framework exhibits excellent performance on\nIJB-A dataset. Based on the proposed approach, we have submitted our IJB-A\nresults to National Institute of Standards and Technology (NIST) for official\nevaluation. Moreover, by introducing new data and advanced neural architecture,\nour method outperforms the state-of-the-art by a wide margin on IJB-A dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 06:11:43 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 05:37:41 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Xiong", "Lin", ""], ["Karlekar", "Jayashree", ""], ["Zhao", "Jian", ""], ["Cheng", "Yi", ""], ["Xu", "Yan", ""], ["Feng", "Jiashi", ""], ["Pranata", "Sugiri", ""], ["Shen", "Shengmei", ""]]}, {"id": "1704.00447", "submitter": "Kerstin Hammernik", "authors": "Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht,\n  Daniel K Sodickson, Thomas Pock and Florian Knoll", "title": "Learning a Variational Network for Reconstruction of Accelerated MRI\n  Data", "comments": "Submitted to Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 06:49:46 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Hammernik", "Kerstin", ""], ["Klatzer", "Teresa", ""], ["Kobler", "Erich", ""], ["Recht", "Michael P", ""], ["Sodickson", "Daniel K", ""], ["Pock", "Thomas", ""], ["Knoll", "Florian", ""]]}, {"id": "1704.00454", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "Clustering in Hilbert simplex geometry", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering categorical distributions in the probability simplex is a\nfundamental task met in many applications dealing with normalized histograms.\nTraditionally, the differential-geometric structures of the probability simplex\nhave been used either by (i) setting the Riemannian metric tensor to the Fisher\ninformation matrix of the categorical distributions, or (ii) defining the\ndualistic information-geometric structure induced by a smooth dissimilarity\nmeasure, the Kullback-Leibler divergence. In this work, we introduce for this\nclustering task a novel computationally-friendly framework for modeling the\nprobability simplex termed {\\em Hilbert simplex geometry}. In the Hilbert\nsimplex geometry, the distance function is described by a polytope. We discuss\nthe pros and cons of those different statistical modelings, and benchmark\nexperimentally these geometries for center-based $k$-means and $k$-center\nclusterings. We show that Hilbert metric in the probability simplex satisfies\nthe property of information monotonicity. Furthermore, since a canonical\nHilbert metric distance can be defined on any bounded convex subset of the\nEuclidean space, we also consider Hilbert's projective geometry of the\nelliptope of correlation matrices and study its clustering performances.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 07:23:37 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 15:30:07 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 03:12:09 GMT"}, {"version": "v4", "created": "Mon, 26 Nov 2018 05:03:27 GMT"}, {"version": "v5", "created": "Wed, 12 Dec 2018 07:46:11 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1704.00492", "submitter": "Dimitrios Tzionas", "authors": "Dimitrios Tzionas and Juergen Gall", "title": "A Comparison of Directional Distances for Hand Pose Estimation", "comments": "German Conference on Pattern Recognition (GCPR) 2013,\n  http://files.is.tue.mpg.de/dtzionas/GCPR_2013.html", "journal-ref": null, "doi": "10.1007/978-3-642-40602-7_14", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarking methods for 3d hand tracking is still an open problem due to the\ndifficulty of acquiring ground truth data. We introduce a new dataset and\nbenchmarking protocol that is insensitive to the accumulative error of other\nprotocols. To this end, we create testing frame pairs of increasing difficulty\nand measure the pose estimation error separately for each of them. This\napproach gives new insights and allows to accurately study the performance of\neach feature or method without employing a full tracking pipeline. Following\nthis protocol, we evaluate various directional distances in the context of\nsilhouette-based 3d hand tracking, expressed as special cases of a generalized\nChamfer distance form. An appropriate parameter setup is proposed for each of\nthem, and a comparative study reveals the best performing method in this\ncontext.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 09:31:01 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tzionas", "Dimitrios", ""], ["Gall", "Juergen", ""]]}, {"id": "1704.00498", "submitter": "Malte Nissen", "authors": "Malte St{\\ae}r Nissen, Oswin Krause, Kristian Almstrup, S{\\o}ren\n  Kj{\\ae}rulff, Torben Trindk{\\ae}r Nielsen, Mads Nielsen", "title": "Convolutional neural networks for segmentation and object detection of\n  human semen", "comments": "Submitted for Scandinavian Conference on Image Analysis 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare a set of convolutional neural network (CNN) architectures for the\ntask of segmenting and detecting human sperm cells in an image taken from a\nsemen sample. In contrast to previous work, samples are not stained or washed\nto allow for full sperm quality analysis, making analysis harder due to\nclutter. Our results indicate that training on full images is superior to\ntraining on patches when class-skew is properly handled. Full image training\nincluding up-sampling during training proves to be beneficial in deep CNNs for\npixel wise accuracy and detection performance. Predicted sperm cells are found\nby using connected components on the CNN predictions. We investigate\noptimization of a threshold parameter on the size of detected components. Our\nbest network achieves 93.87% precision and 91.89% recall on our test dataset\nafter thresholding outperforming a classical mage analysis approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 09:40:56 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Nissen", "Malte St\u00e6r", ""], ["Krause", "Oswin", ""], ["Almstrup", "Kristian", ""], ["Kj\u00e6rulff", "S\u00f8ren", ""], ["Nielsen", "Torben Trindk\u00e6r", ""], ["Nielsen", "Mads", ""]]}, {"id": "1704.00509", "submitter": "Yan Zhang", "authors": "Yan Zhang and Mete Ozay and Shuohao Li and Takayuki Okatani", "title": "Truncating Wide Networks using Binary Tree Architectures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent study shows that a wide deep network can obtain accuracy comparable to\na deeper but narrower network. Compared to narrower and deeper networks, wide\nnetworks employ relatively less number of layers and have various important\nbenefits, such that they have less running time on parallel computing devices,\nand they are less affected by gradient vanishing problems. However, the\nparameter size of a wide network can be very large due to use of large width of\neach layer in the network. In order to keep the benefits of wide networks\nmeanwhile improve the parameter size and accuracy trade-off of wide networks,\nwe propose a binary tree architecture to truncate architecture of wide networks\nby reducing the width of the networks. More precisely, in the proposed\narchitecture, the width is continuously reduced from lower layers to higher\nlayers in order to increase the expressive capacity of network with a less\nincrease on parameter size. Also, to ease the gradient vanishing problem,\nfeatures obtained at different layers are concatenated to form the output of\nour architecture. By employing the proposed architecture on a baseline wide\nnetwork, we can construct and train a new network with same depth but\nconsiderably less number of parameters. In our experimental analyses, we\nobserve that the proposed architecture enables us to obtain better parameter\nsize and accuracy trade-off compared to baseline networks using various\nbenchmark image classification datasets. The results show that our model can\ndecrease the classification error of baseline from 20.43% to 19.22% on\nCifar-100 using only 28% of parameters that baseline has. Code is available at\nhttps://github.com/ZhangVision/bitnet.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:11:10 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Zhang", "Yan", ""], ["Ozay", "Mete", ""], ["Li", "Shuohao", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1704.00515", "submitter": "Dimitrios Tzionas", "authors": "Dimitrios Tzionas and Abhilash Srikantha and Pablo Aponte and Juergen\n  Gall", "title": "Capturing Hand Motion with an RGB-D Sensor, Fusing a Generative Model\n  with Salient Points", "comments": "German Conference on Pattern Recognition (GCPR) 2014,\n  http://files.is.tue.mpg.de/dtzionas/GCPR_2014.html", "journal-ref": null, "doi": "10.1007/978-3-319-11752-2_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand motion capture has been an active research topic in recent years,\nfollowing the success of full-body pose tracking. Despite similarities, hand\ntracking proves to be more challenging, characterized by a higher\ndimensionality, severe occlusions and self-similarity between fingers. For this\nreason, most approaches rely on strong assumptions, like hands in isolation or\nexpensive multi-camera systems, that limit the practical use. In this work, we\npropose a framework for hand tracking that can capture the motion of two\ninteracting hands using only a single, inexpensive RGB-D camera. Our approach\ncombines a generative model with collision detection and discriminatively\nlearned salient points. We quantitatively evaluate our approach on 14 new\nsequences with challenging interactions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:26:45 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tzionas", "Dimitrios", ""], ["Srikantha", "Abhilash", ""], ["Aponte", "Pablo", ""], ["Gall", "Juergen", ""]]}, {"id": "1704.00524", "submitter": "Byeongyong Ahn", "authors": "Byeongyong Ahn, and Nam Ik Cho", "title": "Block-Matching Convolutional Neural Network for Image Denoising", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main streams in up-to-date image denoising algorithms:\nnon-local self similarity (NSS) prior based methods and convolutional neural\nnetwork (CNN) based methods. The NSS based methods are favorable on images with\nregular and repetitive patterns while the CNN based methods perform better on\nirregular structures. In this paper, we propose a block-matching convolutional\nneural network (BMCNN) method that combines NSS prior and CNN. Initially,\nsimilar local patches in the input image are integrated into a 3D block. In\norder to prevent the noise from messing up the block matching, we first apply\nan existing denoising algorithm on the noisy image. The denoised image is\nemployed as a pilot signal for the block matching, and then denoising function\nfor the block is learned by a CNN structure. Experimental results show that the\nproposed BMCNN algorithm achieves state-of-the-art performance. In detail,\nBMCNN can restore both repetitive and irregular structures.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 11:01:47 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Ahn", "Byeongyong", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1704.00529", "submitter": "Dimitrios Tzionas", "authors": "Dimitrios Tzionas and Juergen Gall", "title": "3D Object Reconstruction from Hand-Object Interactions", "comments": "International Conference on Computer Vision (ICCV) 2015,\n  http://files.is.tue.mpg.de/dtzionas/In-Hand-Scanning", "journal-ref": null, "doi": "10.1109/ICCV.2015.90", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have enabled 3d object reconstruction approaches using a\nsingle off-the-shelf RGB-D camera. Although these approaches are successful for\na wide range of object classes, they rely on stable and distinctive geometric\nor texture features. Many objects like mechanical parts, toys, household or\ndecorative articles, however, are textureless and characterized by minimalistic\nshapes that are simple and symmetric. Existing in-hand scanning systems and 3d\nreconstruction techniques fail for such symmetric objects in the absence of\nhighly distinctive features. In this work, we show that extracting 3d hand\nmotion for in-hand scanning effectively facilitates the reconstruction of even\nfeatureless and highly symmetric objects and we present an approach that fuses\nthe rich additional information of hands into a 3d reconstruction pipeline,\nsignificantly contributing to the state-of-the-art of in-hand scanning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 11:19:03 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tzionas", "Dimitrios", ""], ["Gall", "Juergen", ""]]}, {"id": "1704.00570", "submitter": "Yunjie Ke", "authors": "Lijie Fan, Yunjie Ke", "title": "Spatiotemporal Networks for Video Emotion Recognition", "comments": "The reason is that the article is just an experimental report without\n  being reviewed carefully. There are some fatal drawbacks in this article and\n  it may not be suitable for being published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our experiment adapts several popular deep learning methods as well as some\ntraditional methods on the problem of video emotion recognition. In our\nexperiment, we use the CNN-LSTM architecture for visual information extraction\nand classification and utilize traditional methods such as for audio feature\nclassification. For multimodal fusion, we use the traditional Support Vector\nMachine. Our experiment yields a good result on the AFEW 6.0 Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:21:38 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 01:32:11 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:39:34 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Fan", "Lijie", ""], ["Ke", "Yunjie", ""]]}, {"id": "1704.00616", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, Gabriel L. Oliveira, Nima Sedaghat, and\n  Thomas Brox", "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection", "comments": "10 pages, 7 figures, ICCV 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 14:29:40 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:40:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["Oliveira", "Gabriel L.", ""], ["Sedaghat", "Nima", ""], ["Brox", "Thomas", ""]]}, {"id": "1704.00642", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings, Thomas B. Berrett and Richard J. Samworth", "title": "Local nearest neighbour classification with applications to\n  semi-supervised learning", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new asymptotic expansion for the global excess risk of a\nlocal-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon\nthe test point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the decision boundary of the optimal\nBayes classifier, but we also show that if these conditions are not satisfied,\nthen the dominant contribution may arise from the tails of the marginal\ndistribution of the features. Moreover, we prove that, provided the\n$d$-dimensional marginal distribution of the features has a finite $\\rho$th\nmoment for some $\\rho > 4$ (as well as other regularity conditions), a local\nchoice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n> 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. Our worst-case rates are complemented by a minimax lower\nbound, which reveals that the local, semi-supervised $k$-nearest neighbour\nclassifier attains the minimax optimal rate over our classes for the excess\nrisk, up to a subpolynomial factor in $n$. These theoretical improvements over\nthe standard $k$-nearest neighbour classifier are also illustrated through a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:34:11 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 11:16:30 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 10:49:46 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1704.00648", "submitter": "Eirikur Agustsson", "authors": "Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli,\n  Radu Timofte, Luca Benini and Luc Van Gool", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to learn compressible representations in deep\narchitectures with an end-to-end training strategy. Our method is based on a\nsoft (continuous) relaxation of quantization and entropy, which we anneal to\ntheir discrete counterparts throughout training. We showcase this method for\ntwo challenging applications: Image compression and neural network compression.\nWhile these tasks have typically been approached with different methods, our\nsoft-to-hard quantization approach gives results competitive with the\nstate-of-the-art for both.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:39:56 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 09:18:22 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Mentzer", "Fabian", ""], ["Tschannen", "Michael", ""], ["Cavigelli", "Lukas", ""], ["Timofte", "Radu", ""], ["Benini", "Luca", ""], ["Van Gool", "Luc", ""]]}, {"id": "1704.00675", "submitter": "Jordi Pont-Tuset", "authors": "Jordi Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo\n  Arbel\\'aez and Alex Sorkine-Hornung and Luc Van Gool", "title": "The 2017 DAVIS Challenge on Video Object Segmentation", "comments": "Challenge website: http://davischallenge.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the 2017 DAVIS Challenge on Video Object Segmentation, a public\ndataset, benchmark, and competition specifically designed for the task of video\nobject segmentation. Following the footsteps of other successful initiatives,\nsuch as ILSVRC and PASCAL VOC, which established the avenue of research in the\nfields of scene classification and semantic segmentation, the DAVIS Challenge\ncomprises a dataset, an evaluation methodology, and a public competition with a\ndedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on\nthe recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which\nhas fostered the development of several novel state-of-the-art video object\nsegmentation techniques. In this paper we describe the scope of the benchmark,\nhighlight the main characteristics of the dataset, define the evaluation\nmetrics of the competition, and present a detailed analysis of the results of\nthe participants to the challenge.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 16:44:46 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 18:07:57 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 17:50:08 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Pont-Tuset", "Jordi", ""], ["Perazzi", "Federico", ""], ["Caelles", "Sergi", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Sorkine-Hornung", "Alex", ""], ["Van Gool", "Luc", ""]]}, {"id": "1704.00705", "submitter": "Christian Schulz", "authors": "Orlando Moreira, Merten Popp, Christian Schulz", "title": "Graph Partitioning with Acyclicity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are widely used to model execution dependencies in applications. In\nparticular, the NP-complete problem of partitioning a graph under constraints\nreceives enormous attention by researchers because of its applicability in\nmultiprocessor scheduling. We identified the additional constraint of acyclic\ndependencies between blocks when mapping computer vision and imaging\napplications to a heterogeneous embedded multiprocessor. Existing algorithms\nand heuristics do not address this requirement and deliver results that are not\napplicable for our use-case. In this work, we show that this more constrained\nversion of the graph partitioning problem is NP-complete and present heuristics\nthat achieve a close approximation of the optimal solution found by an\nexhaustive search for small problem instances and much better scalability for\nlarger instances. In addition, we can show a positive impact on the schedule of\na real imaging application that improves communication volume and execution\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:45:10 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Moreira", "Orlando", ""], ["Popp", "Merten", ""], ["Schulz", "Christian", ""]]}, {"id": "1704.00710", "submitter": "Christian H\\\"ane", "authors": "Christian H\\\"ane, Shubham Tulsiani, Jitendra Malik", "title": "Hierarchical Surface Prediction for 3D Object Reconstruction", "comments": "3DV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks have shown promising results for 3D\ngeometry prediction. They can make predictions from very little input data such\nas a single color image. A major limitation of such approaches is that they\nonly predict a coarse resolution voxel grid, which does not capture the surface\nof the objects well. We propose a general framework, called hierarchical\nsurface prediction (HSP), which facilitates prediction of high resolution voxel\ngrids. The main insight is that it is sufficient to predict high resolution\nvoxels around the predicted surfaces. The exterior and interior of the objects\ncan be represented with coarse resolution voxels. Our approach is not dependent\non a specific input type. We show results for geometry prediction from color\nimages, depth images and shape completion from partial voxel grids. Our\nanalysis shows that our high resolution predictions are more accurate than low\nresolution predictions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:52:51 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 19:11:08 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["H\u00e4ne", "Christian", ""], ["Tulsiani", "Shubham", ""], ["Malik", "Jitendra", ""]]}, {"id": "1704.00717", "submitter": "Viraj Prabhu", "authors": "Arjun Chandrasekaran, Deshraj Yadav, Prithvijit Chattopadhyay, Viraj\n  Prabhu, Devi Parikh", "title": "It Takes Two to Tango: Towards Theory of AI's Mind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory of Mind is the ability to attribute mental states (beliefs, intents,\nknowledge, perspectives, etc.) to others and recognize that these mental states\nmay differ from one's own. Theory of Mind is critical to effective\ncommunication and to teams demonstrating higher collective performance. To\neffectively leverage the progress in Artificial Intelligence (AI) to make our\nlives more productive, it is important for humans and AI to work well together\nin a team. Traditionally, there has been much emphasis on research to make AI\nmore accurate, and (to a lesser extent) on having it better understand human\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\nmore human-like and having it develop a theory of our minds. In this work, we\nargue that for human-AI teams to be effective, humans must also develop a\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\nand quirks. We instantiate these ideas within the domain of Visual Question\nAnswering (VQA). We find that using just a few examples (50), lay people can be\ntrained to better predict responses and oncoming failures of a complex VQA\nmodel. We further evaluate the role existing explanation (or interpretability)\nmodalities play in helping humans build ToAIM. Explainable AI has received\nconsiderable scientific and popular attention in recent times. Surprisingly, we\nfind that having access to the model's internal states - its confidence in its\ntop-k predictions, explicit or implicit attention maps which highlight regions\nin the image (and words in the question) the model is looking at (and listening\nto) while answering a question about an image - do not help people better\npredict its behavior.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:58:07 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 17:55:50 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Yadav", "Deshraj", ""], ["Chattopadhyay", "Prithvijit", ""], ["Prabhu", "Viraj", ""], ["Parikh", "Devi", ""]]}, {"id": "1704.00758", "submitter": "Waqas Sultani", "authors": "Waqas Sultani, Dong Zhang and Mubarak Shah", "title": "Unsupervised Action Proposal Ranking through Proposal Recombination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, action proposal methods have played an important role in action\nrecognition tasks, as they reduce the search space dramatically. Most\nunsupervised action proposal methods tend to generate hundreds of action\nproposals which include many noisy, inconsistent, and unranked action\nproposals, while supervised action proposal methods take advantage of\npredefined object detectors (e.g., human detector) to refine and score the\naction proposals, but they require thousands of manual annotations to train.\n  Given the action proposals in a video, the goal of the proposed work is to\ngenerate a few better action proposals that are ranked properly. In our\napproach, we first divide action proposal into sub-proposal and then use\nDynamic Programming based graph optimization scheme to select the optimal\ncombinations of sub-proposals from different proposals and assign each new\nproposal a score. We propose a new unsupervised image-based actioness detector\nthat leverages web images and employs it as one of the node scores in our graph\nformulation. Moreover, we capture motion information by estimating the number\nof motion contours within each action proposal patch. The proposed method is an\nunsupervised method that neither needs bounding box annotations nor video level\nlabels, which is desirable with the current explosion of large-scale action\ndatasets. Our approach is generic and does not depend on a specific action\nproposal method. We evaluate our approach on several publicly available trimmed\nand un-trimmed datasets and obtain better performance compared to several\nproposal ranking methods. In addition, we demonstrate that properly ranked\nproposals produce significantly better action detection as compared to\nstate-of-the-art proposal based methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 18:43:20 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Sultani", "Waqas", ""], ["Zhang", "Dong", ""], ["Shah", "Mubarak", ""]]}, {"id": "1704.00763", "submitter": "Kan Chen", "authors": "Kan Chen, Trung Bui, Fang Chen, Zhaowen Wang, Ram Nevatia", "title": "AMC: Attention guided Multi-modal Correlation Learning for Image Search", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a user's query, traditional image search systems rank images according\nto its relevance to a single modality (e.g., image content or surrounding\ntext). Nowadays, an increasing number of images on the Internet are available\nwith associated meta data in rich modalities (e.g., titles, keywords, tags,\netc.), which can be exploited for better similarity measure with queries. In\nthis paper, we leverage visual and textual modalities for image search by\nlearning their correlation with input query. According to the intent of query,\nattention mechanism can be introduced to adaptively balance the importance of\ndifferent modalities. We propose a novel Attention guided Multi-modal\nCorrelation (AMC) learning method which consists of a jointly learned hierarchy\nof intra and inter-attention networks. Conditioned on query's intent,\nintra-attention networks (i.e., visual intra-attention network and language\nintra-attention network) attend on informative parts within each modality; a\nmulti-modal inter-attention network promotes the importance of the most\nquery-relevant modalities. In experiments, we evaluate AMC models on the search\nlogs from two real world image search engines and show a significant boost on\nthe ranking of user-clicked images in search results. Additionally, we extend\nAMC models to caption ranking task on COCO dataset and achieve competitive\nresults compared with recent state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 18:57:42 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Chen", "Kan", ""], ["Bui", "Trung", ""], ["Chen", "Fang", ""], ["Wang", "Zhaowen", ""], ["Nevatia", "Ram", ""]]}, {"id": "1704.00829", "submitter": "Emiliano Diaz", "authors": "Emiliano Diaz", "title": "Online deforestation detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deforestation detection using satellite images can make an important\ncontribution to forest management. Current approaches can be broadly divided\ninto those that compare two images taken at similar periods of the year and\nthose that monitor changes by using multiple images taken during the growing\nseason. The CMFDA algorithm described in Zhu et al. (2012) is an algorithm that\nbuilds on the latter category by implementing a year-long, continuous,\ntime-series based approach to monitoring images. This algorithm was developed\nfor 30m resolution, 16-day frequency reflectance data from the Landsat\nsatellite. In this work we adapt the algorithm to 1km, 16-day frequency\nreflectance data from the modis sensor aboard the Terra satellite. The CMFDA\nalgorithm is composed of two submodels which are fitted on a pixel-by-pixel\nbasis. The first estimates the amount of surface reflectance as a function of\nthe day of the year. The second estimates the occurrence of a deforestation\nevent by comparing the last few predicted and real reflectance values. For this\ncomparison, the reflectance observations for six different bands are first\ncombined into a forest index. Real and predicted values of the forest index are\nthen compared and high absolute differences for consecutive observation dates\nare flagged as deforestation events. Our adapted algorithm also uses the two\nmodel framework. However, since the modis 13A2 dataset used, includes\nreflectance data for different spectral bands than those included in the\nLandsat dataset, we cannot construct the forest index. Instead we propose two\ncontrasting approaches: a multivariate and an index approach similar to that of\nCMFDA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 22:40:48 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Diaz", "Emiliano", ""]]}, {"id": "1704.00834", "submitter": "Siyang Qin", "authors": "Siyang Qin and Roberto Manduchi", "title": "Cascaded Segmentation-Detection Networks for Word-Level Text Spotting", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm for word-level text spotting that is able to\naccurately and reliably determine the bounding regions of individual words of\ntext \"in the wild\". Our system is formed by the cascade of two convolutional\nneural networks. The first network is fully convolutional and is in charge of\ndetecting areas containing text. This results in a very reliable but possibly\ninaccurate segmentation of the input image. The second network (inspired by the\npopular YOLO architecture) analyzes each segment produced in the first stage,\nand predicts oriented rectangular regions containing individual words. No\npost-processing (e.g. text line grouping) is necessary. With execution time of\n450 ms for a 1000-by-560 image on a Titan X GPU, our system achieves the\nhighest score to date among published algorithms on the ICDAR 2015 Incidental\nScene Text dataset benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 23:55:13 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Qin", "Siyang", ""], ["Manduchi", "Roberto", ""]]}, {"id": "1704.00848", "submitter": "Daniel Haehn", "authors": "Daniel Haehn, Verena Kaynig, James Tompkin, Jeff W. Lichtman,\n  Hanspeter Pfister", "title": "Guided Proofreading of Automatic Segmentations for Connectomics", "comments": "Supplemental material available at\n  http://rhoana.org/guidedproofreading/supplemental.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic cell image segmentation methods in connectomics produce merge and\nsplit errors, which require correction through proofreading. Previous research\nhas identified the visual search for these errors as the bottleneck in\ninteractive proofreading. To aid error correction, we develop two classifiers\nthat automatically recommend candidate merges and splits to the user. These\nclassifiers use a convolutional neural network (CNN) that has been trained with\nerrors in automatic segmentations against expert-labeled ground truth. Our\nclassifiers detect potentially-erroneous regions by considering a large context\nregion around a segmentation boundary. Corrections can then be performed by a\nuser with yes/no decisions, which reduces variation of information 7.5x faster\nthan previous proofreading methods. We also present a fully-automatic mode that\nuses a probability threshold to make merge/split decisions. Extensive\nexperiments using the automatic approach and comparing performance of novice\nand expert users demonstrate that our method performs favorably against\nstate-of-the-art proofreading methods on different connectomics datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 01:46:46 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Haehn", "Daniel", ""], ["Kaynig", "Verena", ""], ["Tompkin", "James", ""], ["Lichtman", "Jeff W.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1704.00860", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do and Dang-Khoa Le Tan and Trung T. Pham and Ngai-Man\n  Cheung", "title": "Simultaneous Feature Aggregating and Hashing for Large-scale Image\n  Search", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most state-of-the-art hashing-based visual search systems, local image\ndescriptors of an image are first aggregated as a single feature vector. This\nfeature vector is then subjected to a hashing function that produces a binary\nhash code. In previous work, the aggregating and the hashing processes are\ndesigned independently. In this paper, we propose a novel framework where\nfeature aggregating and hashing are designed simultaneously and optimized\njointly. Specifically, our joint optimization produces aggregated\nrepresentations that can be better reconstructed by some binary codes. This\nleads to more discriminative binary hash codes and improved retrieval accuracy.\nIn addition, we also propose a fast version of the recently-proposed Binary\nAutoencoder to be used in our proposed framework. We perform extensive\nretrieval experiments on several benchmark datasets with both SIFT and\nconvolutional features. Our results suggest that the proposed framework\nachieves significant improvements over the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 03:04:30 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Tan", "Dang-Khoa Le", ""], ["Pham", "Trung T.", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1704.00887", "submitter": "Alireza Khosravian", "authors": "Alireza Khosravian, Tat-Jun Chin, Ian Reid", "title": "A Branch-and-Bound Algorithm for Checkerboard Extraction in Camera-Laser\n  Calibration", "comments": "To appear in IEEE Conference on Robotics and Automation 2017", "journal-ref": "Proceedings of IEEE Conference on Robotics and Automation, 2017", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of camera-to-laser-scanner calibration using a\ncheckerboard and multiple image-laser scan pairs. Distinguishing which laser\npoints measure the checkerboard and which lie on the background is essential to\nany such system. We formulate the checkerboard extraction as a combinatorial\noptimization problem with a clear cut objective function. We propose a\nbranch-and-bound technique that deterministically and globally optimizes the\nobjective. Unlike what is available in the literature, the proposed method is\nnot heuristic and does not require assumptions such as constraints on the\nbackground or relying on discontinuity of the range measurements to partition\nthe data into line segments. The proposed approach is generic and can be\napplied to both 3D or 2D laser scanners as well as the cases where multiple\ncheckerboards are present. We demonstrate the effectiveness of the proposed\napproach by providing numerical simulations as well as experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 06:28:30 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Khosravian", "Alireza", ""], ["Chin", "Tat-Jun", ""], ["Reid", "Ian", ""]]}, {"id": "1704.00979", "submitter": "Artem Sevastopolsky", "authors": "Artem Sevastopolsky", "title": "Optic Disc and Cup Segmentation Methods for Glaucoma Detection with\n  Modification of U-Net Convolutional Neural Network", "comments": "accepted for publication in \"Pattern Recognition and Image Analysis:\n  Advances in Mathematical Theory and Applications\" journal, ISSN 1054-6618", "journal-ref": null, "doi": "10.1134/S1054661817030269", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is the second leading cause of blindness all over the world, with\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\ntime, glaucoma causes irreversible damage to the optic nerve leading to\nblindness. The optic nerve head examination, which involves measurement of\ncup-to-disc ratio, is considered one of the most valuable methods of structural\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\nof optic disc and optic cup on eye fundus images and can be performed by modern\ncomputer vision algorithms. This work presents universal approach for automatic\noptic disc and cup segmentation, which is based on deep learning, namely,\nmodification of U-Net convolutional neural network. Our experiments include\ncomparison with the best known methods on publicly available databases\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\nour method achieves quality comparable to current state-of-the-art methods,\noutperforming them in terms of the prediction time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 12:28:12 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Sevastopolsky", "Artem", ""]]}, {"id": "1704.01047", "submitter": "Gernot Riegler", "authors": "Gernot Riegler, Ali Osman Ulusoy, Horst Bischof, Andreas Geiger", "title": "OctNetFusion: Learning Depth Fusion from Data", "comments": "3DV 2017, https://github.com/griegler/octnetfusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a learning based approach to depth fusion, i.e.,\ndense 3D reconstruction from multiple depth images. The most common approach to\ndepth fusion is based on averaging truncated signed distance functions, which\nwas originally proposed by Curless and Levoy in 1996. While this method is\nsimple and provides great results, it is not able to reconstruct (partially)\noccluded surfaces and requires a large number frames to filter out sensor noise\nand outliers. Motivated by the availability of large 3D model repositories and\nrecent advances in deep learning, we present a novel 3D CNN architecture that\nlearns to predict an implicit surface representation from the input depth maps.\nOur learning based method significantly outperforms the traditional volumetric\nfusion approach in terms of noise reduction and outlier suppression. By\nlearning the structure of real world 3D objects and scenes, our approach is\nfurther able to reconstruct occluded regions and to fill in gaps in the\nreconstruction. We demonstrate that our learning based approach outperforms\nboth vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric\nfusion. Further, we demonstrate state-of-the-art 3D shape completion results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:02:05 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 08:08:45 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 20:58:32 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Riegler", "Gernot", ""], ["Ulusoy", "Ali Osman", ""], ["Bischof", "Horst", ""], ["Geiger", "Andreas", ""]]}, {"id": "1704.01069", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee and Sungmin Eum and Heesung Kwon", "title": "ME R-CNN: Multi-Expert R-CNN for Object Detection", "comments": "Submit to CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Multi-Expert Region-based CNN (ME R-CNN) which is equipped with\nmultiple experts and built on top of the R-CNN framework known to be one of the\nstate-of-the-art object detection methods. ME R-CNN focuses in better capturing\nthe appearance variations caused by different shapes, poses, and viewing\nangles. The proposed approach consists of three experts each responsible for\nobjects with particular shapes: horizontally elongated, square-like, and\nvertically elongated. On top of using selective search which provides a\ncompact, yet effective set of region of interests (RoIs) for object detection,\nwe augmented the set by also employing the exhaustive search for training only.\nIncorporating the exhaustive search can provide complementary advantages: i) it\ncaptures the multitude of neighboring RoIs missed by the selective search, and\nthus ii) provide significantly larger amount of training examples. We show that\nthe ME R-CNN architecture provides considerable performance increase over the\nbaselines on PASCAL VOC 07, 12, and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:35:31 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 15:13:19 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lee", "Hyungtae", ""], ["Eum", "Sungmin", ""], ["Kwon", "Heesung", ""]]}, {"id": "1704.01085", "submitter": "Caner Hazirbas", "authors": "Caner Hazirbas, Sebastian Georg Soyer, Maximilian Christian Staab,\n  Laura Leal-Taix\\'e, Daniel Cremers", "title": "Deep Depth From Focus", "comments": "accepted to Asian Conference on Computer Vision (ACCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth from focus (DFF) is one of the classical ill-posed inverse problems in\ncomputer vision. Most approaches recover the depth at each pixel based on the\nfocal setting which exhibits maximal sharpness. Yet, it is not obvious how to\nreliably estimate the sharpness level, particularly in low-textured areas. In\nthis paper, we propose `Deep Depth From Focus (DDFF)' as the first end-to-end\nlearning approach to this problem. One of the main challenges we face is the\nhunger for data of deep neural networks. In order to obtain a significant\namount of focal stacks with corresponding groundtruth depth, we propose to\nleverage a light-field camera with a co-calibrated RGB-D sensor. This allows us\nto digitally create focal stacks of varying sizes. Compared to existing\nbenchmarks our dataset is 25 times larger, enabling the use of machine learning\nfor this inverse problem. We compare our results with state-of-the-art DFF\nmethods and we also analyze the effect of several key deep architectural\ncomponents. These experiments show that our proposed method `DDFFNet' achieves\nstate-of-the-art performance in all scenes, reducing depth error by more than\n75% compared to the classical DFF methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 16:15:54 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 14:44:56 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 15:00:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hazirbas", "Caner", ""], ["Soyer", "Sebastian Georg", ""], ["Staab", "Maximilian Christian", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Cremers", "Daniel", ""]]}, {"id": "1704.01088", "submitter": "Shuoxin Ma", "authors": "Shuoxin Ma, Tan Wang", "title": "sWSI: A Low-cost and Commercial-quality Whole Slide Imaging System on\n  Android and iOS Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, scalable Whole Slide Imaging (sWSI), a novel high-throughput,\ncost-effective and robust whole slide imaging system on both Android and iOS\nplatforms is introduced and analyzed. With sWSI, most mainstream smartphone\nconnected to a optical eyepiece of any manually controlled microscope can be\nautomatically controlled to capture sequences of mega-pixel fields of views\nthat are synthesized into giga-pixel virtual slides. Remote servers carry out\nthe majority of computation asynchronously to support clients running at\nsatisfying frame rates without sacrificing image quality nor robustness. A\ntypical 15x15mm sample can be digitized in 30 seconds with 4X or in 3 minutes\nwith 10X object magnification, costing under $1. The virtual slide quality is\nconsidered comparable to existing high-end scanners thus satisfying for\nclinical usage by surveyed pathologies. The scan procedure with features such\nas supporting magnification up to 100x, recoding z-stacks,\nspecimen-type-neutral and giving real-time feedback, is deemed\nwork-flow-friendly and reliable.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 06:15:25 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Ma", "Shuoxin", ""], ["Wang", "Tan", ""]]}, {"id": "1704.01133", "submitter": "Dong Ki Kim", "authors": "Dong-Ki Kim and Matthew R. Walter", "title": "Satellite Image-based Localization via Learned Embeddings", "comments": "To be published in IEEE International Conference on Robotics and\n  Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vision-based method that localizes a ground vehicle using\npublicly available satellite imagery as the only prior knowledge of the\nenvironment. Our approach takes as input a sequence of ground-level images\nacquired by the vehicle as it navigates, and outputs an estimate of the\nvehicle's pose relative to a georeferenced satellite image. We overcome the\nsignificant viewpoint and appearance variations between the images through a\nneural multi-view model that learns location-discriminative embeddings in which\nground-level images are matched with their corresponding satellite view of the\nscene. We use this learned function as an observation model in a filtering\nframework to maintain a distribution over the vehicle's pose. We evaluate our\nmethod on different benchmark datasets and demonstrate its ability localize\nground-level images in environments novel relative to training, despite the\nchallenges of significant viewpoint and appearance variations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:03:55 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Kim", "Dong-Ki", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1704.01137", "submitter": "Swagath Venkataramani", "authors": "Sanjay Ganapathy, Swagath Venkataramani, Balaraman Ravindran, Anand\n  Raghunathan", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety\nof machine learning tasks and are deployed in increasing numbers of products\nand services. However, the computational requirements of training and\nevaluating large-scale DNNs are growing at a much faster pace than the\ncapabilities of the underlying hardware platforms that they are executed upon.\nIn this work, we propose Dynamic Variable Effort Deep Neural Networks\n(DyVEDeep) to reduce the computational requirements of DNNs during inference.\nPrevious efforts propose specialized hardware implementations for DNNs,\nstatically prune the network, or compress the weights. Complementary to these\napproaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in\nthe inputs to DNNs to improve their compute efficiency with comparable\nclassification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms\nthat, in the course of processing an input, identify how critical a group of\ncomputations are to classify the input. DyVEDeep dynamically focuses its\ncompute effort only on the critical computa- tions, while skipping or\napproximating the rest. We propose 3 effort knobs that operate at different\nlevels of granularity viz. neuron, feature and layer levels. We build DyVEDeep\nversions for 5 popular image recognition benchmarks - one for CIFAR-10 and four\nfor ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across\nall benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar\noperations, which translates to 1.8x-2.3x performance improvement over a\nCaffe-based implementation, with < 0.5% loss in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:14:02 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Ganapathy", "Sanjay", ""], ["Venkataramani", "Swagath", ""], ["Ravindran", "Balaraman", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1704.01152", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Maxwell Collins and Matthew Brown and Serge\n  Belongie", "title": "Pose2Instance: Harnessing Keypoints for Person Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human keypoints are a well-studied representation of people.We explore how to\nuse keypoint models to improve instance-level person segmentation. The main\nidea is to harness the notion of a distance transform of oracle provided\nkeypoints or estimated keypoint heatmaps as a prior for person instance\nsegmentation task within a deep neural network. For training and evaluation, we\nconsider all those images from COCO where both instance segmentation and human\nkeypoints annotations are available. We first show how oracle keypoints can\nboost the performance of existing human segmentation model during inference\nwithout any training. Next, we propose a framework to directly learn a deep\ninstance segmentation model conditioned on human pose. Experimental results\nshow that at various Intersection Over Union (IOU) thresholds, in a constrained\nenvironment with oracle keypoints, the instance segmentation accuracy achieves\n10% to 12% relative improvements over a strong baseline of oracle bounding\nboxes. In a more realistic environment, without the oracle keypoints, the\nproposed deep person instance segmentation model conditioned on human pose\nachieves 3.8% to 10.5% relative improvements comparing with its strongest\nbaseline of a deep network trained only for segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:41:47 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Tripathi", "Subarna", ""], ["Collins", "Maxwell", ""], ["Brown", "Matthew", ""], ["Belongie", "Serge", ""]]}, {"id": "1704.01155", "submitter": "Weilin Xu", "authors": "Weilin Xu, David Evans, Yanjun Qi", "title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural\n  Networks", "comments": "To appear in Network and Distributed Systems Security Symposium\n  (NDSS) 2018", "journal-ref": null, "doi": "10.14722/ndss.2018.23198", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks (DNNs) have achieved great success in many\ntasks, they can often be fooled by \\emph{adversarial examples} that are\ngenerated by adding small but purposeful distortions to natural examples.\nPrevious studies to defend against adversarial examples mostly focused on\nrefining the DNN models, but have either shown limited success or required\nexpensive computation. We propose a new strategy, \\emph{feature squeezing},\nthat can be used to harden DNN models by detecting adversarial examples.\nFeature squeezing reduces the search space available to an adversary by\ncoalescing samples that correspond to many different feature vectors in the\noriginal space into a single sample. By comparing a DNN model's prediction on\nthe original input with that on squeezed inputs, feature squeezing detects\nadversarial examples with high accuracy and few false positives. This paper\nexplores two feature squeezing methods: reducing the color bit depth of each\npixel and spatial smoothing. These simple strategies are inexpensive and\ncomplementary to other defenses, and can be combined in a joint detection\nframework to achieve high detection rates against state-of-the-art attacks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:56:53 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 23:45:08 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Xu", "Weilin", ""], ["Evans", "David", ""], ["Qi", "Yanjun", ""]]}, {"id": "1704.01194", "submitter": "Harshala Gammulle Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Two Stream LSTM: A Deep Fusion Framework for Human Action Recognition", "comments": "Published as a conference paper at WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of human action recognition from video\nsequences. Inspired by the exemplary results obtained via automatic feature\nlearning and deep learning approaches in computer vision, we focus our\nattention towards learning salient spatial features via a convolutional neural\nnetwork (CNN) and then map their temporal relationship with the aid of\nLong-Short-Term-Memory (LSTM) networks. Our contribution in this paper is a\ndeep fusion framework that more effectively exploits spatial features from CNNs\nwith temporal features from LSTM models. We also extensively evaluate their\nstrengths and weaknesses. We find that by combining both the sets of features,\nthe fully connected features effectively act as an attention mechanism to\ndirect the LSTM to interesting parts of the convolutional feature sequence. The\nsignificance of our fusion method is its simplicity and effectiveness compared\nto other state-of-the-art methods. The evaluation results demonstrate that this\nhierarchical multi stream fusion method has higher performance compared to\nsingle stream mapping methods allowing it to achieve high accuracy\noutperforming current state-of-the-art methods in three widely used databases:\nUCF11, UCFSports, jHMDB.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 21:32:04 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1704.01222", "submitter": "Roman Klokov", "authors": "Roman Klokov, Victor Lempitsky", "title": "Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point\n  Cloud Models", "comments": "Spotlight at ICCV'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep learning architecture (called Kd-network) that is\ndesigned for 3D model recognition tasks and works with unstructured point\nclouds. The new architecture performs multiplicative transformations and share\nparameters of these transformations according to the subdivisions of the point\nclouds imposed onto them by Kd-trees. Unlike the currently dominant\nconvolutional architectures that usually require rasterization on uniform\ntwo-dimensional or three-dimensional grids, Kd-networks do not rely on such\ngrids in any way and therefore avoid poor scaling behaviour. In a series of\nexperiments with popular shape recognition benchmarks, Kd-networks demonstrate\ncompetitive performance in a number of shape recognition tasks such as shape\nclassification, shape retrieval and shape part segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 23:52:40 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 08:51:12 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Klokov", "Roman", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1704.01235", "submitter": "Parag Chandakkar", "authors": "Parag S. Chandakkar and Baoxin Li", "title": "Joint Regression and Ranking for Image Enhancement", "comments": "WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on automated image enhancement has gained momentum in recent years,\npartially due to the need for easy-to-use tools for enhancing pictures captured\nby ubiquitous cameras on mobile devices. Many of the existing leading methods\nemploy machine-learning-based techniques, by which some enhancement parameters\nfor a given image are found by relating the image to the training images with\nknown enhancement parameters. While knowing the structure of the parameter\nspace can facilitate search for the optimal solution, none of the existing\nmethods has explicitly modeled and learned that structure. This paper presents\nan end-to-end, novel joint regression and ranking approach to model the\ninteraction between desired enhancement parameters and images to be processed,\nemploying a Gaussian process (GP). GP allows searching for ideal parameters\nusing only the image features. The model naturally leads to a ranking technique\nfor comparing images in the induced feature space. Comparative evaluation using\nthe ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on\nan additional data-set were used to demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 01:28:04 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chandakkar", "Parag S.", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01246", "submitter": "Chuyang Ye", "authors": "Chuyang Ye", "title": "Estimation of Tissue Microstructure Using a Deep Network Inspired by a\n  Sparse Reconstruction Framework", "comments": "12 pages, 5 figures. Accepted by IPMI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) provides a unique tool for\nnoninvasively probing the microstructure of the neuronal tissue. The NODDI\nmodel has been a popular approach to the estimation of tissue microstructure in\nmany neuroscience studies. It represents the diffusion signals with three types\nof diffusion in tissue: intra-cellular, extra-cellular, and cerebrospinal fluid\ncompartments. However, the original NODDI method uses a computationally\nexpensive procedure to fit the model and could require a large number of\ndiffusion gradients for accurate microstructure estimation, which may be\nimpractical for clinical use. Therefore, efforts have been devoted to efficient\nand accurate NODDI microstructure estimation with a reduced number of diffusion\ngradients. In this work, we propose a deep network based approach to the NODDI\nmicrostructure estimation, which is named Microstructure Estimation using a\nDeep Network (MEDN). Motivated by the AMICO algorithm which accelerates the\ncomputation of NODDI parameters, we formulate the microstructure estimation\nproblem in a dictionary-based framework. The proposed network comprises two\ncascaded stages. The first stage resembles the solution to a dictionary-based\nsparse reconstruction problem and the second stage computes the final\nmicrostructure using the output of the first stage. The weights in the two\nstages are jointly learned from training data, which is obtained from training\ndMRI scans with diffusion gradients that densely sample the q-space. The\nproposed method was applied to brain dMRI scans, where two shells each with 30\ngradient directions (60 diffusion gradients in total) were used. Estimation\naccuracy with respect to the gold standard was measured and the results\ndemonstrate that MEDN outperforms the competing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 02:37:44 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Ye", "Chuyang", ""]]}, {"id": "1704.01248", "submitter": "Parag Chandakkar", "authors": "Parag S. Chandakkar, Vijetha Gattupalli and Baoxin Li", "title": "A Computational Approach to Relative Aesthetics", "comments": "ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational visual aesthetics has recently become an active research area.\nExisting state-of-art methods formulate this as a binary classification task\nwhere a given image is predicted to be beautiful or not. In many applications\nsuch as image retrieval and enhancement, it is more important to rank images\nbased on their aesthetic quality instead of binary-categorizing them.\nFurthermore, in such applications, it may be possible that all images belong to\nthe same category. Hence determining the aesthetic ranking of the images is\nmore appropriate. To this end, we formulate a novel problem of ranking images\nwith respect to their aesthetic quality. We construct a new dataset of image\npairs with relative labels by carefully selecting images from the popular AVA\ndataset. Unlike in aesthetics classification, there is no single threshold\nwhich would determine the ranking order of the images across our entire\ndataset. We propose a deep neural network based approach that is trained on\nimage pairs by incorporating principles from relative learning. Results show\nthat such relative training procedure allows our network to rank the images\nwith a higher accuracy than a state-of-art network trained on the same set of\nimages using binary labels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 02:49:30 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chandakkar", "Parag S.", ""], ["Gattupalli", "Vijetha", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01249", "submitter": "Parag Chandakkar", "authors": "Parag S. Chandakkar and Baoxin Li", "title": "A Structured Approach to Predicting Image Enhancement Parameters", "comments": "WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networking on mobile devices has become a commonplace of everyday\nlife. In addition, photo capturing process has become trivial due to the\nadvances in mobile imaging. Hence people capture a lot of photos everyday and\nthey want them to be visually-attractive. This has given rise to automated,\none-touch enhancement tools. However, the inability of those tools to provide\npersonalized and content-adaptive enhancement has paved way for machine-learned\nmethods to do the same. The existing typical machine-learned methods\nheuristically (e.g. kNN-search) predict the enhancement parameters for a new\nimage by relating the image to a set of similar training images. These\nheuristic methods need constant interaction with the training images which\nmakes the parameter prediction sub-optimal and computationally expensive at\ntest time which is undesired. This paper presents a novel approach to\npredicting the enhancement parameters given a new image using only its\nfeatures, without using any training images. We propose to model the\ninteraction between the image features and its corresponding enhancement\nparameters using the matrix factorization (MF) principles. We also propose a\nway to integrate the image features in the MF formulation. We show that our\napproach outperforms heuristic approaches as well as recent approaches in MF\nand structured prediction on synthetic as well as real-world data of image\nenhancement.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 03:04:28 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chandakkar", "Parag S.", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01250", "submitter": "Parag Chandakkar", "authors": "Parag S. Chandakkar, Qiongjie Tian and Baoxin Li", "title": "Relative Learning from Web Images for Content-adaptive Enhancement", "comments": "ICME 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized and content-adaptive image enhancement can find many\napplications in the age of social media and mobile computing. This paper\npresents a relative-learning-based approach, which, unlike previous methods,\ndoes not require matching original and enhanced images for training. This\nallows the use of massive online photo collections to train a ranking model for\nimproved enhancement. We first propose a multi-level ranking model, which is\nlearned from only relatively-labeled inputs that are automatically crawled.\nThen we design a novel parameter sampling scheme under this model to generate\nthe desired enhancement parameters for a new image. For evaluation, we first\nverify the effectiveness and the generalization abilities of our approach,\nusing images that have been enhanced/labeled by experts. Then we carry out\nsubjective tests, which show that users prefer images enhanced by our approach\nover other existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 03:13:01 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chandakkar", "Parag S.", ""], ["Tian", "Qiongjie", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01256", "submitter": "Parag Chandakkar", "authors": "Parag S. Chandakkar, Yilin Wang and Baoxin Li", "title": "Improving Vision-based Self-positioning in Intelligent Transportation\n  Systems via Integrated Lane and Vehicle Detection", "comments": "WACV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic congestion is a widespread problem. Dynamic traffic routing systems\nand congestion pricing are getting importance in recent research. Lane\nprediction and vehicle density estimation is an important component of such\nsystems. We introduce a novel problem of vehicle self-positioning which\ninvolves predicting the number of lanes on the road and vehicle's position in\nthose lanes using videos captured by a dashboard camera. We propose an\nintegrated closed-loop approach where we use the presence of vehicles to aid\nthe task of self-positioning and vice-versa. To incorporate multiple factors\nand high-level semantic knowledge into the solution, we formulate this problem\nas a Bayesian framework. In the framework, the number of lanes, the vehicle's\nposition in those lanes and the presence of other vehicles are considered as\nparameters. We also propose a bounding box selection scheme to reduce the\nnumber of false detections and increase the computational efficiency. We show\nthat the number of box proposals decreases by a factor of 6 using the selection\napproach. It also results in large reduction in the number of false detections.\nThe entire approach is tested on real-world videos and is found to give\nacceptable results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 03:38:08 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chandakkar", "Parag S.", ""], ["Wang", "Yilin", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01262", "submitter": "Parag Chandakkar", "authors": "Parag S. Chandakkar and Baoxin Li", "title": "Investigating Human Factors in Image Forgery Detection", "comments": "ACM MM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's age of internet and social media, one can find an enormous volume\nof forged images on-line. These images have been used in the past to convey\nfalsified information and achieve harmful intentions. The spread and the effect\nof the social media only makes this problem more severe. While creating forged\nimages has become easier due to software advancements, there is no automated\nalgorithm which can reliably detect forgery.\n  Image forgery detection can be seen as a subset of image understanding\nproblem. Human performance is still the gold-standard for these type of\nproblems when compared to existing state-of-art automated algorithms. We\nconduct a subjective evaluation test with the aid of eye-tracker to investigate\ninto human factors associated with this problem. We compare the performance of\nan automated algorithm and humans for forgery detection problem. We also\ndevelop an algorithm which uses the data from the evaluation test to predict\nthe difficulty-level of an image (the difficulty-level of an image here denotes\nhow difficult it is for humans to detect forgery in an image. Terms such as\n\"Easy/difficult image\" will be used in the same context). The experimental\nresults presented in this paper should facilitate development of better\nalgorithms in the future.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 04:07:07 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chandakkar", "Parag S.", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01264", "submitter": "Parag Chandakkar", "authors": "Ragav Venkatesan, Parag S. Chandakkar and Baoxin Li", "title": "Classification of Diabetic Retinopathy Images Using Multi-Class\n  Multiple-Instance Learning Based on Color Correlogram Features", "comments": "EMBS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All people with diabetes have the risk of developing diabetic retinopathy\n(DR), a vision-threatening complication. Early detection and timely treatment\ncan reduce the occurrence of blindness due to DR. Computer-aided diagnosis has\nthe potential benefit of improving the accuracy and speed in DR detection. This\nstudy is concerned with automatic classification of images with microaneurysm\n(MA) and neovascularization (NV), two important DR clinical findings. Together\nwith normal images, this presents a 3-class classification problem. We propose\na modified color auto-correlogram feature (AutoCC) with low dimensionality that\nis spectrally tuned towards DR images. Recognizing the fact that the images\nwith or without MA or NV are generally different only in small, localized\nregions, we propose to employ a multi-class, multiple-instance learning\nframework for performing the classification task using the proposed feature.\nExtensive experiments including comparison with a few state-of-art image\nclassification approaches have been performed and the results suggest that the\nproposed approach is promising as it outperforms other methods by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 04:20:13 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Chandakkar", "Parag S.", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01266", "submitter": "Parag Chandakkar", "authors": "Archana Paladugu, Parag S. Chandakkar, Peng Zhang and Baoxin Li", "title": "Supporting Navigation of Outdoor Shopping Complexes for\n  Visually-impaired Users through Multi-modal Data Fusion", "comments": "ICME 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor shopping complexes (OSC) are extremely difficult for people with\nvisual impairment to navigate. Existing GPS devices are mostly designed for\nroadside navigation and seldom transition well into an OSC-like setting. We\nreport our study on the challenges faced by a blind person in navigating OSC\nthrough developing a new mobile application named iExplore. We first report an\nexploratory study aiming at deriving specific design principles for building\nthis system by learning the unique challenges of the problem. Then we present a\nmethodology that can be used to derive the necessary information for the\ndevelopment of iExplore, followed by experimental validation of the technology\nby a group of visually impaired users in a local outdoor shopping center. User\nfeedback and other experiments suggest that iExplore, while at its very initial\nphase, has the potential of filling a practical gap in existing assistive\ntechnologies for the visually impaired.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 04:24:41 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Paladugu", "Archana", ""], ["Chandakkar", "Parag S.", ""], ["Zhang", "Peng", ""], ["Li", "Baoxin", ""]]}, {"id": "1704.01285", "submitter": "Vijay Kumar  B G Dr", "authors": "Ben Harwood, Vijay Kumar B G, Gustavo Carneiro, Ian Reid, Tom Drummond", "title": "Smart Mining for Deep Metric Learning", "comments": "*Vijay Kumar B G and Ben Harwood contributed equally to this work.\n  Accepted in IEEE International Conference on Computer Vision, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve deep metric learning problems and producing feature embeddings,\ncurrent methodologies will commonly use a triplet model to minimise the\nrelative distance between samples from the same class and maximise the relative\ndistance between samples from different classes. Though successful, the\ntraining convergence of this triplet model can be compromised by the fact that\nthe vast majority of the training samples will produce gradients with\nmagnitudes that are close to zero. This issue has motivated the development of\nmethods that explore the global structure of the embedding and other methods\nthat explore hard negative/positive mining. The effectiveness of such mining\nmethods is often associated with intractable computational requirements. In\nthis paper, we propose a novel deep metric learning method that combines the\ntriplet model and the global structure of the embedding space. We rely on a\nsmart mining procedure that produces effective training samples for a low\ncomputational cost. In addition, we propose an adaptive controller that\nautomatically adjusts the smart mining hyper-parameters and speeds up the\nconvergence of the training process. We show empirically that our proposed\nmethod allows for fast and more accurate training of triplet ConvNets than\nother competing mining methods. Additionally, we show that our method achieves\nnew state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 06:58:56 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 02:51:15 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 05:27:22 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Harwood", "Ben", ""], ["G", "Vijay Kumar B", ""], ["Carneiro", "Gustavo", ""], ["Reid", "Ian", ""], ["Drummond", "Tom", ""]]}, {"id": "1704.01297", "submitter": "Sawon  Pratiher", "authors": "S Pratiher, S Chatterjee, R Bose", "title": "Automated Diagnosis of Epilepsy Employing Multifractal Detrended\n  Fluctuation Analysis Based Features", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV nlin.AO nlin.CD q-bio.QM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution reports an application of MultiFractal Detrended\nFluctuation Analysis, MFDFA based novel feature extraction technique for\nautomated detection of epilepsy. In fractal geometry, Multifractal Detrended\nFluctuation Analysis MFDFA is a popular technique to examine the\nself-similarity of a nonlinear, chaotic and noisy time series. In the present\nresearch work, EEG signals representing healthy, interictal (seizure free) and\nictal activities (seizure) are acquired from an existing available database.\nThe acquired EEG signals of different states are at first analyzed using MFDFA.\nTo requisite the time series singularity quantification at local and global\nscales, a novel set of fourteen different features. Suitable feature ranking\nemploying students t-test has been done to select the most statistically\nsignificant features which are henceforth being used as inputs to a support\nvector machines (SVM) classifier for the classification of different EEG\nsignals. Eight different classification problems have been presented in this\npaper and it has been observed that the overall classification accuracy using\nMFDFA based features are reasonably satisfactory for all classification\nproblems. The performance of the proposed method are also found to be quite\ncommensurable and in some cases even better when compared with the results\npublished in existing literature studied on the similar data set.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 08:00:14 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Pratiher", "S", ""], ["Chatterjee", "S", ""], ["Bose", "R", ""]]}, {"id": "1704.01344", "submitter": "Ziwei Liu", "authors": "Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via\n  Deep Layer Cascade", "comments": "To appear in CVPR 2017 as a spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep layer cascade (LC) method to improve the accuracy and\nspeed of semantic segmentation. Unlike the conventional model cascade (MC) that\nis composed of multiple independent models, LC treats a single deep model as a\ncascade of several sub-models. Earlier sub-models are trained to handle easy\nand confident regions, and they progressively feed-forward harder regions to\nthe next sub-model for processing. Convolutions are only calculated on these\nregions to reduce computations. The proposed method possesses several\nadvantages. First, LC classifies most of the easy regions in the shallow stage\nand makes deeper stage focuses on a few hard regions. Such an adaptive and\n'difficulty-aware' learning improves segmentation performance. Second, LC\naccelerates both training and testing of deep network thanks to early decisions\nin the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable\nframework, allowing joint learning of all sub-models. We evaluate our method on\nPASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and\nfast speed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 09:58:51 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Liu", "Ziwei", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1704.01358", "submitter": "Harkirat Behl", "authors": "Harkirat Singh Behl, Michael Sapienza, Gurkirt Singh, Suman Saha,\n  Fabio Cuzzolin, Philip H. S. Torr", "title": "Incremental Tube Construction for Human Action Detection", "comments": "British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art action detection systems are tailored for offline\nbatch-processing applications. However, for online applications like\nhuman-robot interaction, current systems fall short, either because they only\ndetect one action per video, or because they assume that the entire video is\navailable ahead of time. In this work, we introduce a real-time and online\njoint-labelling and association algorithm for action detection that can\nincrementally construct space-time action tubes on the most challenging action\nvideos in which different action categories occur concurrently. In contrast to\nprevious methods, we solve the detection-window association and action\nlabelling problems jointly in a single pass. We demonstrate superior online\nassociation accuracy and speed (2.2ms per frame) as compared to the current\nstate-of-the-art offline systems. We further demonstrate that the entire action\ndetection pipeline can easily be made to work effectively in real-time using\nour action tube construction algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 10:37:42 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 18:20:21 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Behl", "Harkirat Singh", ""], ["Sapienza", "Michael", ""], ["Singh", "Gurkirt", ""], ["Saha", "Suman", ""], ["Cuzzolin", "Fabio", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1704.01372", "submitter": "Jiqing Wu", "authors": "Jiqing Wu, Radu Timofte, Zhiwu Huang, Luc Van Gool", "title": "On the Relation between Color Image Denoising and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amount of image denoising literature focuses on single channel images\nand often experimentally validates the proposed methods on tens of images at\nmost. In this paper, we investigate the interaction between denoising and\nclassification on large scale dataset. Inspired by classification models, we\npropose a novel deep learning architecture for color (multichannel) image\ndenoising and report on thousands of images from ImageNet dataset as well as\ncommonly used imagery. We study the importance of (sufficient) training data,\nhow semantic class information can be traded for improved denoising results. As\na result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on\naverage over state-of-the art methods on large scale dataset. We conclude that\nit is beneficial to incorporate in classification models. On the other hand, we\nalso study how noise affect classification performance. In the end, we come to\na number of interesting conclusions, some being counter-intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 11:28:25 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Wu", "Jiqing", ""], ["Timofte", "Radu", ""], ["Huang", "Zhiwu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1704.01426", "submitter": "Daniele Pannone", "authors": "Danilo Avola, Gian Luca Foresti, Niki Martinel, Daniele Pannone and\n  Claudio Piciarelli", "title": "The UMCD Dataset", "comments": "3 pages, 5 figures", "journal-ref": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2018", "doi": "10.1109/TSMC.2018.2804766", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the technological improvements of low-cost small-scale\nUnmanned Aerial Vehicles (UAVs) are promoting an ever-increasing use of them in\ndifferent tasks. In particular, the use of small-scale UAVs is useful in all\nthese low-altitude tasks in which common UAVs cannot be adopted, such as\nrecurrent comprehensive view of wide environments, frequent monitoring of\nmilitary areas, real-time classification of static and moving entities (e.g.,\npeople, cars, etc.). These tasks can be supported by mosaicking and change\ndetection algorithms achieved at low-altitude. Currently, public datasets for\ntesting these algorithms are not available. This paper presents the UMCD\ndataset, the first collection of geo-referenced video sequences acquired at\nlow-altitude for mosaicking and change detection purposes. Five reference\nscenarios are also reported.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 13:49:27 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Avola", "Danilo", ""], ["Foresti", "Gian Luca", ""], ["Martinel", "Niki", ""], ["Pannone", "Daniele", ""], ["Piciarelli", "Claudio", ""]]}, {"id": "1704.01429", "submitter": "Zhiyuan Zha", "authors": "Qiong Wang, Xinggan Zhang, Yu Wu, Lan Tang and Zhiyuan Zha", "title": "Non-Convex Weighted Lp Minimization based Group Sparse Representation\n  Framework for Image Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2731791", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonlocal image representation or group sparsity has attracted considerable\ninterest in various low-level vision tasks and has led to several\nstate-of-the-art image denoising techniques, such as BM3D, LSSC. In the past,\nconvex optimization with sparsity-promoting convex regularization was usually\nregarded as a standard scheme for estimating sparse signals in noise. However,\nusing convex regularization can not still obtain the correct sparsity solution\nunder some practical problems including image inverse problems. In this paper\nwe propose a non-convex weighted $\\ell_p$ minimization based group sparse\nrepresentation (GSR) framework for image denoising. To make the proposed scheme\ntractable and robust, the generalized soft-thresholding (GST) algorithm is\nadopted to solve the non-convex $\\ell_p$ minimization problem. In addition, to\nimprove the accuracy of the nonlocal similar patches selection, an adaptive\npatch search (APS) scheme is proposed. Experimental results have demonstrated\nthat the proposed approach not only outperforms many state-of-the-art denoising\nmethods such as BM3D and WNNM, but also results in a competitive speed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 13:53:25 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 12:01:43 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 01:43:29 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wang", "Qiong", ""], ["Zhang", "Xinggan", ""], ["Wu", "Yu", ""], ["Tang", "Lan", ""], ["Zha", "Zhiyuan", ""]]}, {"id": "1704.01464", "submitter": "Ahmed ElSayed", "authors": "Ahmed ElSayed, Ausif Mahmood, Tarek Sobh", "title": "Effect of Super Resolution on High Dimensional Features for Unsupervised\n  Face Recognition in the Wild", "comments": null, "journal-ref": null, "doi": "10.1109/AIPR.2017.8457967", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the face recognition algorithms use query faces captured from\nuncontrolled, in the wild, environment. Often caused by the cameras limited\ncapabilities, it is common for these captured facial images to be blurred or\nlow resolution. Super resolution algorithms are therefore crucial in improving\nthe resolution of such images especially when the image size is small requiring\nenlargement. This paper aims to demonstrate the effect of one of the\nstate-of-the-art algorithms in the field of image super resolution. To\ndemonstrate the functionality of the algorithm, various before and after 3D\nface alignment cases are provided using the images from the Labeled Faces in\nthe Wild (lfw). Resulting images are subject to testing on a closed set face\nrecognition protocol using unsupervised algorithms with high dimension\nextracted features. The inclusion of super resolution algorithm resulted in\nsignificant improved recognition rate over recently reported results obtained\nfrom unsupervised algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 19:58:27 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 18:10:59 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["ElSayed", "Ahmed", ""], ["Mahmood", "Ausif", ""], ["Sobh", "Tarek", ""]]}, {"id": "1704.01466", "submitter": "Anurag Sahoo", "authors": "Anurag Sahoo, Vishal Kaushal, Khoshrav Doctor, Suyash Shetty, Rishabh\n  Iyer, Ganesh Ramakrishnan", "title": "A Unified Multi-Faceted Video Summarization System", "comments": "18 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses automatic summarization and search in visual data\ncomprising of videos, live streams and image collections in a unified manner.\nIn particular, we propose a framework for multi-faceted summarization which\nextracts key-frames (image summaries), skims (video summaries) and entity\nsummaries (summarization at the level of entities like objects, scenes, humans\nand faces in the video). The user can either view these as extractive\nsummarization, or query focused summarization. Our approach first pre-processes\nthe video or image collection once, to extract all important visual features,\nfollowing which we provide an interactive mechanism to the user to summarize\nthe video based on their choice. We investigate several diversity, coverage and\nrepresentation models for all these problems, and argue the utility of these\ndifferent mod- els depending on the application. While most of the prior work\non submodular summarization approaches has focused on combining several models\nand learning weighted mixtures, we focus on the explain-ability of different\nthe diversity, coverage and representation models and their scalability. Most\nimportantly, we also show that we can summarize hours of video data in a few\nseconds, and our system allows the user to generate summaries of various\nlengths and types interactively on the fly.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 07:29:34 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Sahoo", "Anurag", ""], ["Kaushal", "Vishal", ""], ["Doctor", "Khoshrav", ""], ["Shetty", "Suyash", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1704.01472", "submitter": "Min Xian", "authors": "Min Xian, Yingtao Zhang, H.D. Cheng, Fei Xu, Boyu Zhang, Jianrui Ding", "title": "Automatic Breast Ultrasound Image Segmentation: A Survey", "comments": "40 pages, 6 tables, 180 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the leading causes of cancer death among women\nworldwide. In clinical routine, automatic breast ultrasound (BUS) image\nsegmentation is very challenging and essential for cancer diagnosis and\ntreatment planning. Many BUS segmentation approaches have been studied in the\nlast two decades, and have been proved to be effective on private datasets.\nCurrently, the advancement of BUS image segmentation seems to meet its\nbottleneck. The improvement of the performance is increasingly challenging, and\nonly few new approaches were published in the last several years. It is the\ntime to look at the field by reviewing previous approaches comprehensively and\nto investigate the future directions. In this paper, we study the basic ideas,\ntheories, pros and cons of the approaches, group them into categories, and\nextensively review each category in depth by discussing the principles,\napplication issues, and advantages/disadvantages.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:23:26 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 23:19:21 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Xian", "Min", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Xu", "Fei", ""], ["Zhang", "Boyu", ""], ["Ding", "Jianrui", ""]]}, {"id": "1704.01474", "submitter": "Kai Chen", "authors": "Kai Chen and Mathias Seuret", "title": "Convolutional Neural Networks for Page Segmentation of Historical\n  Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Convolutional Neural Network (CNN) based page\nsegmentation method for handwritten historical document images. We consider\npage segmentation as a pixel labeling problem, i.e., each pixel is classified\nas one of the predefined classes. Traditional methods in this area rely on\ncarefully hand-crafted features or large amounts of prior knowledge. In\ncontrast, we propose to learn features from raw image pixels using a CNN. While\nmany researchers focus on developing deep CNN architectures to solve different\nproblems, we train a simple CNN with only one convolution layer. We show that\nthe simple architecture achieves competitive results against other deep\narchitectures on different public datasets. Experiments also demonstrate the\neffectiveness and superiority of the proposed method compared to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 15:12:25 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 10:16:49 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Chen", "Kai", ""], ["Seuret", "Mathias", ""]]}, {"id": "1704.01502", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Jianguo Li and Zhou Su and Minjun Li and Yurong Chen\n  and Yu-Gang Jiang and Xiangyang Xue", "title": "Weakly Supervised Dense Video Captioning", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a novel and challenging vision task, dense video\ncaptioning, which aims to automatically describe a video clip with multiple\ninformative and diverse caption sentences. The proposed method is trained\nwithout explicit annotation of fine-grained sentence to video region-sequence\ncorrespondence, but is only based on weak video-level sentence annotations. It\ndiffers from existing video captioning systems in three technical aspects.\nFirst, we propose lexical fully convolutional neural networks (Lexical-FCN)\nwith weakly supervised multi-instance multi-label learning to weakly link video\nregions with lexical labels. Second, we introduce a novel submodular\nmaximization scheme to generate multiple informative and diverse\nregion-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is\nadopted to weakly associate sentences to region-sequences in the training\nphase. Third, a sequence-to-sequence learning based language model is trained\nwith the weakly supervised information obtained through the association\nprocess. We show that the proposed method can not only produce informative and\ndiverse dense captions, but also outperform state-of-the-art single video\ncaptioning methods by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:06:09 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Li", "Jianguo", ""], ["Su", "Zhou", ""], ["Li", "Minjun", ""], ["Chen", "Yurong", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1704.01510", "submitter": "Martin Weigert", "authors": "Martin Weigert, Loic Royer, Florian Jug, Gene Myers", "title": "Isotropic reconstruction of 3D fluorescence microscopy images using\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy images usually show severe anisotropy in axial versus\nlateral resolution. This hampers downstream processing, i.e. the automatic\nextraction of quantitative biological data. While deconvolution methods and\nother techniques to address this problem exist, they are either time consuming\nto apply or limited in their ability to remove anisotropy. We propose a method\nto recover isotropic resolution from readily acquired anisotropic data. We\nachieve this using a convolutional neural network that is trained end-to-end\nfrom the same anisotropic body of data we later apply the network to. The\nnetwork effectively learns to restore the full isotropic resolution by\nrestoring the image under a trained, sample specific image prior. We apply our\nmethod to $3$ synthetic and $3$ real datasets and show that our results improve\non results from deconvolution and state-of-the-art super-resolution techniques.\nFinally, we demonstrate that a standard 3D segmentation pipeline performs on\nthe output of our network with comparable accuracy as on the full isotropic\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:20:36 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Weigert", "Martin", ""], ["Royer", "Loic", ""], ["Jug", "Florian", ""], ["Myers", "Gene", ""]]}, {"id": "1704.01518", "submitter": "Anna Rohrbach", "authors": "Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon Oh, Bernt\n  Schiele", "title": "Generating Descriptions with Grounded and Co-Referenced People", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to generate descriptions of images or videos received major\ninterest both in the Computer Vision and Natural Language Processing\ncommunities. While a few works have proposed to learn a grounding during the\ngeneration process in an unsupervised way (via an attention mechanism), it\nremains unclear how good the quality of the grounding is and whether it\nbenefits the description quality. In this work we propose a movie description\nmodel which learns to generate description and jointly ground (localize) the\nmentioned characters as well as do visual co-reference resolution between pairs\nof consecutive sentences/clips. We also propose to use weak localization\nsupervision through character mentions provided in movie descriptions to learn\nthe character grounding. At training time, we first learn how to localize\ncharacters by relating their visual appearance to mentions in the descriptions\nvia a semi-supervised approach. We then provide this (noisy) supervision into\nour description model which greatly improves its performance. Our proposed\ndescription model improves over prior work w.r.t. generated description quality\nand additionally provides grounding and local co-reference resolution. We\nevaluate it on the MPII Movie Description dataset using automatic and human\nevaluation measures and using our newly collected grounding and co-reference\ndata for characters.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:36:13 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Tang", "Siyu", ""], ["Oh", "Seong Joon", ""], ["Schiele", "Bernt", ""]]}, {"id": "1704.01664", "submitter": "Cheng Ju", "authors": "Cheng Ju and Aur\\'elien Bibaut and Mark J. van der Laan", "title": "The Relative Performance of Ensemble Methods with Deep Convolutional\n  Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have been successfully applied to a variety of\nmachine learning tasks, including image recognition, semantic segmentation, and\nmachine translation. However, few studies fully investigated ensembles of\nartificial neural networks. In this work, we investigated multiple widely used\nensemble methods, including unweighted averaging, majority voting, the Bayes\nOptimal Classifier, and the (discrete) Super Learner, for image recognition\ntasks, with deep neural networks as candidate algorithms. We designed several\nexperiments, with the candidate algorithms being the same network structure\nwith different model checkpoints within a single training process, networks\nwith same structure but trained multiple times stochastically, and networks\nwith different structure. In addition, we further studied the over-confidence\nphenomenon of the neural networks, as well as its impact on the ensemble\nmethods. Across all of our experiments, the Super Learner achieved best\nperformance among all the ensemble methods in this study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 23:04:43 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Ju", "Cheng", ""], ["Bibaut", "Aur\u00e9lien", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1704.01705", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, Rama\n  Chellappa", "title": "Generate To Adapt: Aligning Domains using Generative Adversarial\n  Networks", "comments": "Accepted as spotlight talk at CVPR 2018. Code available here:\n  https://github.com/yogeshbalaji/Generate_To_Adapt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation is an actively researched problem in Computer Vision. In\nthis work, we propose an approach that leverages unsupervised data to bring the\nsource and target distributions closer in a learned joint feature space. We\naccomplish this by inducing a symbiotic relationship between the learned\nembedding and a generative adversarial network. This is in contrast to methods\nwhich use the adversarial framework for realistic data generation and\nretraining deep models with such data. We demonstrate the strength and\ngenerality of our approach by performing experiments on three different tasks\nwith varying levels of difficulty: (1) Digit classification (MNIST, SVHN and\nUSPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain\nadaptation from synthetic to real data. Our method achieves state-of-the art\nperformance in most experimental settings and by far the only GAN-based method\nthat has been shown to work well across different datasets such as OFFICE and\nDIGITS.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 04:52:48 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 05:11:11 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 21:54:10 GMT"}, {"version": "v4", "created": "Thu, 12 Apr 2018 18:02:56 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Balaji", "Yogesh", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1704.01716", "submitter": "Jue Wang", "authors": "Jue Wang, Anoop Cherian, Fatih Porikli, Stephen Gould", "title": "Action Representation Using Classifier Decision Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most popular deep learning based models for action recognition are designed\nto generate separate predictions within their short temporal windows, which are\noften aggregated by heuristic means to assign an action label to the full video\nsegment. Given that not all frames from a video characterize the underlying\naction, pooling schemes that impose equal importance to all frames might be\nunfavorable. In an attempt towards tackling this challenge, we propose a novel\npooling scheme, dubbed SVM pooling, based on the notion that among the bag of\nfeatures generated by a CNN on all temporal windows, there is at least one\nfeature that characterizes the action. To this end, we learn a decision\nhyperplane that separates this unknown yet useful feature from the rest.\nApplying multiple instance learning in an SVM setup, we use the parameters of\nthis separating hyperplane as a descriptor for the video. Since these\nparameters are directly related to the support vectors in a max-margin\nframework, they serve as robust representations for pooling of the CNN\nfeatures. We devise a joint optimization objective and an efficient solver that\nlearns these hyperplanes per video and the corresponding action classifiers\nover the hyperplanes. Showcased experiments on the standard HMDB and UCF101\ndatasets demonstrate state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 06:00:14 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Wang", "Jue", ""], ["Cherian", "Anoop", ""], ["Porikli", "Fatih", ""], ["Gould", "Stephen", ""]]}, {"id": "1704.01719", "submitter": "Weihua Chen", "authors": "Weihua Chen, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang", "title": "Beyond triplet loss: a deep quadruplet network for person\n  re-identification", "comments": "accepted to CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is an important task in wide area video\nsurveillance which focuses on identifying people across different cameras.\nRecently, deep learning networks with a triplet loss become a common framework\nfor person ReID. However, the triplet loss pays main attentions on obtaining\ncorrect orders on the training set. It still suffers from a weaker\ngeneralization capability from the training set to the testing set, thus\nresulting in inferior performance. In this paper, we design a quadruplet loss,\nwhich can lead to the model output with a larger inter-class variation and a\nsmaller intra-class variation compared to the triplet loss. As a result, our\nmodel has a better generalization ability and can achieve a higher performance\non the testing set. In particular, a quadruplet deep network using a\nmargin-based online hard negative mining is proposed based on the quadruplet\nloss for the person ReID. In extensive experiments, the proposed network\noutperforms most of the state-of-the-art algorithms on representative datasets\nwhich clearly demonstrates the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 06:09:55 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Chen", "Weihua", ""], ["Chen", "Xiaotang", ""], ["Zhang", "Jianguo", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1704.01740", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Xiangteng He, and Junjie Zhao", "title": "Object-Part Attention Model for Fine-grained Image Classification", "comments": "14 pages, submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2017.2774041", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification is to recognize hundreds of subcategories\nbelonging to the same basic-level category, such as 200 subcategories belonging\nto the bird, which is highly challenging due to large variance in the same\nsubcategory and small variance among different subcategories. Existing methods\ngenerally first locate the objects or parts and then discriminate which\nsubcategory the image belongs to. However, they mainly have two limitations:\n(1) Relying on object or part annotations which are heavily labor consuming.\n(2) Ignoring the spatial relationships between the object and its parts as well\nas among these parts, both of which are significantly helpful for finding\ndiscriminative parts. Therefore, this paper proposes the object-part attention\nmodel (OPAM) for weakly supervised fine-grained image classification, and the\nmain novelties are: (1) Object-part attention model integrates two level\nattentions: object-level attention localizes objects of images, and part-level\nattention selects discriminative parts of object. Both are jointly employed to\nlearn multi-view and multi-scale features to enhance their mutual promotions.\n(2) Object-part spatial constraint model combines two spatial constraints:\nobject spatial constraint ensures selected parts highly representative, and\npart spatial constraint eliminates redundancy and enhances discrimination of\nselected parts. Both are jointly employed to exploit the subtle and local\ndifferences for distinguishing the subcategories. Importantly, neither object\nnor part annotations are used in our proposed approach, which avoids the heavy\nlabor consumption of labeling. Comparing with more than 10 state-of-the-art\nmethods on 4 widely-used datasets, our OPAM approach achieves the best\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 08:05:04 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 02:27:29 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Peng", "Yuxin", ""], ["He", "Xiangteng", ""], ["Zhao", "Junjie", ""]]}, {"id": "1704.01745", "submitter": "Xavier Alameda-Pineda", "authors": "Aliaksandr Siarohin, Gloria Zen, Cveta Majtanovic, Xavier\n  Alameda-Pineda, Elisa Ricci and Nicu Sebe", "title": "How to Make an Image More Memorable? A Deep Style Transfer Approach", "comments": "Accepted at ACM ICMR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that it is possible to automatically predict\nintrinsic image properties like memorability. In this paper, we take a step\nforward addressing the question: \"Can we make an image more memorable?\".\nMethods for automatically increasing image memorability would have an impact in\nmany application fields like education, gaming or advertising. Our work is\ninspired by the popular editing-by-applying-filters paradigm adopted in photo\nediting applications, like Instagram and Prisma. In this context, the problem\nof increasing image memorability maps to that of retrieving \"memorabilizing\"\nfilters or style \"seeds\". Still, users generally have to go through most of the\navailable filters before finding the desired solution, thus turning the editing\nprocess into a resource and time consuming task. In this work, we show that it\nis possible to automatically retrieve the best style seeds for a given image,\nthus remarkably reducing the number of human attempts needed to find a good\nmatch. Our approach leverages from recent advances in the field of image\nsynthesis and adopts a deep architecture for generating a memorable picture\nfrom a given input image and a style seed. Importantly, to automatically select\nthe best style a novel learning-based solution, also relying on deep models, is\nproposed. Our experimental evaluation, conducted on publicly available\nbenchmarks, demonstrates the effectiveness of the proposed approach for\ngenerating memorable images through automatic style seed selection\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 08:25:19 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Zen", "Gloria", ""], ["Majtanovic", "Cveta", ""], ["Alameda-Pineda", "Xavier", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1704.01754", "submitter": "Tuan N.A. Hoang", "authors": "Tuan Hoang, Thanh-Toan Do, Dang-Khoa Le Tan and Ngai-Man Cheung", "title": "Enhance Feature Discrimination for Unsupervised Hashing", "comments": "Accepted to ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to improve unsupervised hashing. Specifically,\nwe propose a very efficient embedding method: Gaussian Mixture Model embedding\n(Gemb). The proposed method, using Gaussian Mixture Model, embeds feature\nvector into a low-dimensional vector and, simultaneously, enhances the\ndiscriminative property of features before passing them into hashing. Our\nexperiment shows that the proposed method boosts the hashing performance of\nmany state-of-the-art, e.g. Binary Autoencoder (BA) [1], Iterative Quantization\n(ITQ) [2], in standard evaluation metrics for the three main benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 08:58:39 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 03:30:42 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Hoang", "Tuan", ""], ["Do", "Thanh-Toan", ""], ["Tan", "Dang-Khoa Le", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1704.01811", "submitter": "Margret Keuper", "authors": "Margret Keuper", "title": "Higher-Order Minimum Cost Lifted Multicuts for Motion Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art motion segmentation algorithms draw their potential\nfrom modeling motion differences of local entities such as point trajectories\nin terms of pairwise potentials in graphical models. Inference in instances of\nminimum cost multicut problems defined on such graphs al- lows to optimize the\nnumber of the resulting segments along with the segment assignment. However,\npairwise potentials limit the discriminative power of the employed motion\nmodels to translational differences. More complex models such as Euclidean or\naffine transformations call for higher-order potentials and a tractable\ninference in the resulting higher-order graphical models. In this paper, we (1)\nintroduce a generalization of the minimum cost lifted multicut problem to\nhypergraphs, and (2) propose a simple primal feasible heuristic that allows for\na reasonably efficient inference in instances of higher-order lifted multicut\nproblem instances defined on point trajectory hypergraphs for motion\nsegmentation. The resulting motion segmentations improve over the\nstate-of-the-art on the FBMS-59 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 12:55:11 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 00:53:40 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Keuper", "Margret", ""]]}, {"id": "1704.01880", "submitter": "Amit Kumar", "authors": "Amit Kumar, Rama Chellappa", "title": "A Convolution Tree with Deconvolution Branches: Exploiting Geometric\n  Relationships for Single Shot Keypoint Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Convolution Networks (DCNNs) have been applied to the task of\nface alignment and have shown potential for learning improved feature\nrepresentations. Although deeper layers can capture abstract concepts like\npose, it is difficult to capture the geometric relationships among the\nkeypoints in DCNNs. In this paper, we propose a novel convolution-deconvolution\nnetwork for facial keypoint detection. Our model predicts the 2D locations of\nthe keypoints and their individual visibility along with 3D head pose, while\nexploiting the spatial relationships among different keypoints. Different from\nexisting approaches of modeling these relationships, we propose learnable\ntransform functions which captures the relationships between keypoints at\nfeature level. However, due to extensive variations in pose, not all of these\nrelationships act at once, and hence we propose, a pose-based routing function\nwhich implicitly models the active relationships. Both transform functions and\nthe routing function are implemented through convolutions in a multi-task\nframework. Our approach presents a single-shot keypoint detection method,\nmaking it different from many existing cascade regression-based methods. We\nalso show that learning these relationships significantly improve the accuracy\nof keypoint detections for in-the-wild face images from challenging datasets\nsuch as AFW and AFLW.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 15:08:59 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Kumar", "Amit", ""], ["Chellappa", "Rama", ""]]}, {"id": "1704.01897", "submitter": "Wei-Shi Zheng", "authors": "Long-Kai Huang, Qiang Yang, Wei-Shi Zheng", "title": "Online Hashing", "comments": "To appear in IEEE Transactions on Neural Networks and Learning\n  Systems (DOI: 10.1109/TNNLS.2017.2689242)", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2689242", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although hash function learning algorithms have achieved great success in\nrecent years, most existing hash models are off-line, which are not suitable\nfor processing sequential or online data. To address this problem, this work\nproposes an online hash model to accommodate data coming in stream for online\nlearning. Specifically, a new loss function is proposed to measure the\nsimilarity loss between a pair of data samples in hamming space. Then, a\nstructured hash model is derived and optimized in a passive-aggressive way.\nTheoretical analysis on the upper bound of the cumulative loss for the proposed\nonline hash model is provided. Furthermore, we extend our online hashing from a\nsingle-model to a multi-model online hashing that trains multiple models so as\nto retain diverse online hashing models in order to avoid biased update. The\ncompetitive efficiency and effectiveness of the proposed online hash models are\nverified through extensive experiments on several large-scale datasets as\ncompared to related hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 15:44:29 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Huang", "Long-Kai", ""], ["Yang", "Qiang", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1704.01920", "submitter": "Rahaf Aljundi", "authors": "Amal Rannen Triki, Rahaf Aljundi, Mathew B. Blaschko and Tinne\n  Tuytelaars", "title": "Encoder Based Lifelong Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICCV.2017.148", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new lifelong learning solution where a single model\nis trained for a sequence of tasks. The main challenge that vision systems face\nin this context is catastrophic forgetting: as they tend to adapt to the most\nrecently seen task, they lose performance on the tasks that were learned\npreviously. Our method aims at preserving the knowledge of the previous tasks\nwhile learning a new one by using autoencoders. For each task, an\nunder-complete autoencoder is learned, capturing the features that are crucial\nfor its achievement. When a new task is presented to the system, we prevent the\nreconstructions of the features with these autoencoders from changing, which\nhas the effect of preserving the information on which the previous tasks are\nmainly relying. At the same time, the features are given space to adjust to the\nmost recent environment as only their projection into a low dimension\nsubmanifold is controlled. The proposed system is evaluated on image\nclassification tasks and shows a reduction of forgetting over the\nstate-of-the-art\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 16:37:15 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Triki", "Amal Rannen", ""], ["Aljundi", "Rahaf", ""], ["Blaschko", "Mathew B.", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1704.01925", "submitter": "Kai Cao", "authors": "Kai Cao and Anil K. Jain", "title": "Automated Latent Fingerprint Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent fingerprints are one of the most important and widely used evidence in\nlaw enforcement and forensic agencies worldwide. Yet, NIST evaluations show\nthat the performance of state-of-the-art latent recognition systems is far from\nsatisfactory. An automated latent fingerprint recognition system with high\naccuracy is essential to compare latents found at crime scenes to a large\ncollection of reference prints to generate a candidate list of possible mates.\nIn this paper, we propose an automated latent fingerprint recognition algorithm\nthat utilizes Convolutional Neural Networks (ConvNets) for ridge flow\nestimation and minutiae descriptor extraction, and extract complementary\ntemplates (two minutiae templates and one texture template) to represent the\nlatent. The comparison scores between the latent and a reference print based on\nthe three templates are fused to retrieve a short candidate list from the\nreference database. Experimental results show that the rank-1 identification\naccuracies (query latent is matched with its true mate in the reference\ndatabase) are 64.7% for the NIST SD27 and 75.3% for the WVU latent databases,\nagainst a reference database of 100K rolled prints. These results are the best\namong published papers on latent recognition and competitive with the\nperformance (66.7% and 70.8% rank-1 accuracies on NIST SD27 and WVU DB,\nrespectively) of a leading COTS latent Automated Fingerprint Identification\nSystem (AFIS). By score-level (rank-level) fusion of our system with the\ncommercial off-the-shelf (COTS) latent AFIS, the overall rank-1 identification\nperformance can be improved from 64.7% and 75.3% to 73.3% (74.4%) and 76.6%\n(78.4%) on NIST SD27 and WVU latent databases, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 16:47:16 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1704.01926", "submitter": "Sergi Caelles", "authors": "Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Luc Van Gool", "title": "Semantically-Guided Video Object Segmentation", "comments": "This paper has been incorporated in the following T-PAMI publication:\n  arXiv:1709.06031", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of semi-supervised video object segmentation,\nthat is, segmenting an object in a sequence given its mask in the first frame.\nOne of the main challenges in this scenario is the change of appearance of the\nobjects of interest. Their semantics, on the other hand, do not vary. This\npaper investigates how to take advantage of such invariance via the\nintroduction of a semantic prior that guides the appearance model.\nSpecifically, given the segmentation mask of the first frame of a sequence, we\nestimate the semantics of the object of interest, and propagate that knowledge\nthroughout the sequence to improve the results based on an appearance model. We\npresent Semantically-Guided Video Object Segmentation (SGV), which improves\nresults over previous state of the art on two different datasets using a\nvariety of evaluation metrics, while running in half a second per frame.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 16:47:56 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 20:30:31 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Caelles", "Sergi", ""], ["Chen", "Yuhua", ""], ["Pont-Tuset", "Jordi", ""], ["Van Gool", "Luc", ""]]}, {"id": "1704.02071", "submitter": "Xiaoyong Shen", "authors": "Xiaoyong Shen, Ying-Cong Chen, Xin Tao, Jiaya Jia", "title": "Convolutional Neural Pyramid for Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a principled convolutional neural pyramid (CNP) framework for\ngeneral low-level vision and image processing tasks. It is based on the\nessential finding that many applications require large receptive fields for\nstructure understanding. But corresponding neural networks for regression\neither stack many layers or apply large kernels to achieve it, which is\ncomputationally very costly. Our pyramid structure can greatly enlarge the\nfield while not sacrificing computation efficiency. Extra benefit includes\nadaptive network depth and progressive upsampling for quasi-realtime testing on\nVGA-size input. Our method profits a broad set of applications, such as\ndepth/RGB image restoration, completion, noise/artifact removal, edge\nrefinement, image filtering, image enhancement and colorization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 02:15:42 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Shen", "Xiaoyong", ""], ["Chen", "Ying-Cong", ""], ["Tao", "Xin", ""], ["Jia", "Jiaya", ""]]}, {"id": "1704.02081", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Elnaz Barshan, and Alexander Wong", "title": "Evolution in Groups: A deeper look at synaptic cluster driven evolution\n  of deep neural networks", "comments": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:1609.01360", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising paradigm for achieving highly efficient deep neural networks is\nthe idea of evolutionary deep intelligence, which mimics biological evolution\nprocesses to progressively synthesize more efficient networks. A crucial design\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\nsimulate heredity and determine the architectures of offspring networks. In\nthis study, we take a deeper look at the notion of synaptic cluster-driven\nevolution of deep neural networks which guides the evolution process towards\nthe formation of a highly sparse set of synaptic clusters in offspring\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\nprobabilistic encoding of synaptic traits considers not only individual\nsynaptic properties but also inter-synaptic relationships within a deep neural\nnetwork. This process results in highly sparse offspring networks which are\nparticularly tailored for parallel computational devices such as GPUs and deep\nneural network accelerator chips. Comprehensive experimental results using four\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\nDetectNet) on two different tasks (object categorization and object detection)\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\nencoding scheme synthesizes networks that can achieve state-of-the-art\nperformance with significantly smaller number of synapses than that of the\noriginal ancestor network. ($\\sim$125-fold decrease in synapses for MNIST).\nFurthermore, the improved cluster efficiency in the generated offspring\nnetworks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold\ndecrease in clusters for KITTI) is particularly useful for accelerated\nperformance on parallel computing hardware architectures such as those in GPUs\nand deep neural network accelerator chips.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 03:28:02 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Barshan", "Elnaz", ""], ["Wong", "Alexander", ""]]}, {"id": "1704.02083", "submitter": "Li Sulimowicz Mrs.", "authors": "Li Sulimowicz, Ishfaq Ahmad", "title": "\"RAPID\" Regions-of-Interest Detection In Big Histopathological Images", "comments": "6 pages, 5 figures, ICME conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer volume and size of histopathological images (e.g.,10^6 MPixel)\nunderscores the need for faster and more accurate Regions-of-interest (ROI)\ndetection algorithms. In this paper, we propose such an algorithm, which has\nfour main components that help achieve greater accuracy and faster speed:\nFirst, while using coarse-to-fine topology preserving segmentation as the\nbaseline, the proposed algorithm uses a superpixel regularity optimization\nscheme for avoiding irregular and extremely small superpixels. Second, the\nproposed technique employs a prediction strategy to focus only on important\nsuperpixels at finer image levels. Third, the algorithm reuses the information\ngained from the coarsest image level at other finer image levels. Both the\nsecond and the third components drastically lower the complexity. Fourth, the\nalgorithm employs a highly effective parallelization scheme using adap- tive\ndata partitioning, which gains high speedup. Experimental results, conducted on\nthe BSD500 [1] and 500 whole-slide histological images from the National Lung\nScreening Trial (NLST)1 dataset, confirm that the proposed algorithm gained 13\ntimes speedup compared with the baseline, and around 160 times compared with\nSLIC [11], without losing accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 03:34:40 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Sulimowicz", "Li", ""], ["Ahmad", "Ishfaq", ""]]}, {"id": "1704.02088", "submitter": "Xian-Ling Mao", "authors": "Dan Wang, Heyan Huang, Chi Lu, Bo-Si Feng, Liqiang Nie, Guihua Wen,\n  Xian-Ling Mao", "title": "Supervised Deep Hashing for Hierarchical Labeled Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, hashing methods have been widely used in large-scale image\nretrieval. However, most existing hashing methods did not consider the\nhierarchical relation of labels, which means that they ignored the rich\ninformation stored in the hierarchy. Moreover, most of previous works treat\neach bit in a hash code equally, which does not meet the scenario of\nhierarchical labeled data. In this paper, we propose a novel deep hashing\nmethod, called supervised hierarchical deep hashing (SHDH), to perform hash\ncode learning for hierarchical labeled data. Specifically, we define a novel\nsimilarity formula for hierarchical labeled data by weighting each layer, and\ndesign a deep convolutional neural network to obtain a hash code for each data\npoint. Extensive experiments on several real-world public datasets show that\nthe proposed method outperforms the state-of-the-art baselines in the image\nretrieval task.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 05:03:37 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 04:21:44 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 07:53:35 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Wang", "Dan", ""], ["Huang", "Heyan", ""], ["Lu", "Chi", ""], ["Feng", "Bo-Si", ""], ["Nie", "Liqiang", ""], ["Wen", "Guihua", ""], ["Mao", "Xian-Ling", ""]]}, {"id": "1704.02112", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Basura Fernando, Mehrtash Harandi, Stephen Gould", "title": "Generalized Rank Pooling for Activity Recognition", "comments": "Accepted at IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular deep models for action recognition split video sequences into\nshort sub-sequences consisting of a few frames; frame-based features are then\npooled for recognizing the activity. Usually, this pooling step discards the\ntemporal order of the frames, which could otherwise be used for better\nrecognition. Towards this end, we propose a novel pooling method, generalized\nrank pooling (GRP), that takes as input, features from the intermediate layers\nof a CNN that is trained on tiny sub-sequences, and produces as output the\nparameters of a subspace which (i) provides a low-rank approximation to the\nfeatures and (ii) preserves their temporal order. We propose to use these\nparameters as a compact representation for the video sequence, which is then\nused in a classification setup. We formulate an objective for computing this\nsubspace as a Riemannian optimization problem on the Grassmann manifold, and\npropose an efficient conjugate gradient scheme for solving it. Experiments on\nseveral activity recognition datasets show that our scheme leads to\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 07:10:53 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 03:54:30 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 04:22:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Cherian", "Anoop", ""], ["Fernando", "Basura", ""], ["Harandi", "Mehrtash", ""], ["Gould", "Stephen", ""]]}, {"id": "1704.02117", "submitter": "Upal Mahbub", "authors": "Upal Mahbub, Sayantan Sarkar, and Rama Chellappa", "title": "Partial Face Detection in the Mobile Domain", "comments": "18 pages, 22 figures, 3 tables, submitted to IEEE Transactions on\n  Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic face detection algorithms do not perform well in the mobile domain\ndue to significant presence of occluded and partially visible faces. One\npromising technique to handle the challenge of partial faces is to design face\ndetectors based on facial segments. In this paper two different approaches of\nfacial segment-based face detection are discussed, namely, proposal-based\ndetection and detection by end-to-end regression. Methods that follow the first\napproach rely on generating face proposals that contain facial segment\ninformation. The three detectors following this approach, namely Facial\nSegment-based Face Detector (FSFD), SegFace and DeepSegFace, discussed in this\npaper, perform binary classification on each proposal based on features learned\nfrom facial segments. The process of proposal generation, however, needs to be\nhandled separately, which can be very time consuming, and is not truly\nnecessary given the nature of the active authentication problem. Hence a novel\nalgorithm, Deep Regression-based User Image Detector (DRUID) is proposed, which\nshifts from the classification to the regression paradigm, thus obviating the\nneed for proposal generation. DRUID has an unique network architecture with\ncustomized loss functions, is trained using a relatively small amount of data\nby utilizing a novel data augmentation scheme and is fast since it outputs the\nbounding boxes of a face and its segments in a single pass. Being robust to\nocclusion by design, the facial segment-based face detection methods,\nespecially DRUID show superior performance over other state-of-the-art face\ndetectors in terms of precision-recall and ROC curve on two mobile face\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 07:43:11 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Mahbub", "Upal", ""], ["Sarkar", "Sayantan", ""], ["Chellappa", "Rama", ""]]}, {"id": "1704.02157", "submitter": "Dan Xu", "authors": "Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe", "title": "Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular\n  Depth Estimation", "comments": "Accepted as a spotlight paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of depth estimation from a single still\nimage. Inspired by recent works on multi- scale convolutional neural networks\n(CNN), we propose a deep model which fuses complementary information derived\nfrom multiple CNN side outputs. Different from previous methods, the\nintegration is obtained by means of continuous Conditional Random Fields\n(CRFs). In particular, we propose two different variations, one based on a\ncascade of multiple CRFs, the other on a unified graphical model. By designing\na novel CNN implementation of mean-field updates for continuous CRFs, we show\nthat both proposed models can be regarded as sequential deep networks and that\ntraining can be performed end-to-end. Through extensive experimental evaluation\nwe demonstrate the effective- ness of the proposed approach and establish new\nstate of the art results on publicly available datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:39:01 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Xu", "Dan", ""], ["Ricci", "Elisa", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Sebe", "Nicu", ""]]}, {"id": "1704.02161", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Sailesh Conjeti, Sri Phani Krishna Karri, Debdoot\n  Sheet, Amin Katouzian, Christian Wachinger, Nassir Navab", "title": "ReLayNet: Retinal Layer and Fluid Segmentation of Macular Optical\n  Coherence Tomography using Fully Convolutional Network", "comments": "Accepted for Publication at Biomedical Optics Express", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is used for non-invasive diagnosis of\ndiabetic macular edema assessing the retinal layers. In this paper, we propose\na new fully convolutional deep architecture, termed ReLayNet, for end-to-end\nsegmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses\na contracting path of convolutional blocks (encoders) to learn a hierarchy of\ncontextual features, followed by an expansive path of convolutional blocks\n(decoders) for semantic segmentation. ReLayNet is trained to optimize a joint\nloss function comprising of weighted logistic regression and Dice overlap loss.\nThe framework is validated on a publicly available benchmark dataset with\ncomparisons against five state-of-the-art segmentation methods including two\ndeep learning based approaches to substantiate its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:50:05 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 10:14:41 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Conjeti", "Sailesh", ""], ["Karri", "Sri Phani Krishna", ""], ["Sheet", "Debdoot", ""], ["Katouzian", "Amin", ""], ["Wachinger", "Christian", ""], ["Navab", "Nassir", ""]]}, {"id": "1704.02163", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos, \\'Alvaro Peris, Francisco Casacuberta, Sergi Soler,\n  Petia Radeva", "title": "Egocentric Video Description based on Temporally-Linked Sequences", "comments": "19 pages, 10 figures, 3 tables. Submitted to Journal of Visual\n  Communication and Image Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric vision consists in acquiring images along the day from a first\nperson point-of-view using wearable cameras. The automatic analysis of this\ninformation allows to discover daily patterns for improving the quality of life\nof the user. A natural topic that arises in egocentric vision is storytelling,\nthat is, how to understand and tell the story relying behind the pictures. In\nthis paper, we tackle storytelling as an egocentric sequences description\nproblem. We propose a novel methodology that exploits information from\ntemporally neighboring events, matching precisely the nature of egocentric\nsequences. Furthermore, we present a new method for multimodal data fusion\nconsisting on a multi-input attention recurrent network. We also publish the\nfirst dataset for egocentric image sequences description, consisting of 1,339\nevents with 3,991 descriptions, from 55 days acquired by 11 people.\nFurthermore, we prove that our proposal outperforms classical attentional\nencoder-decoder methods for video description.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:56:32 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 08:30:49 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 09:55:39 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Peris", "\u00c1lvaro", ""], ["Casacuberta", "Francisco", ""], ["Soler", "Sergi", ""], ["Radeva", "Petia", ""]]}, {"id": "1704.02166", "submitter": "Yanwei  Fu", "authors": "Weidong Yin, Yanwei Fu, Leonid Sigal and Xiangyang Xue", "title": "Semi-Latent GAN: Learning to generate and modify facial images from\n  attributes", "comments": "10 pages, submitted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating and manipulating human facial images using high-level attributal\ncontrols are important and interesting problems. The models proposed in\nprevious work can solve one of these two problems (generation or manipulation),\nbut not both coherently. This paper proposes a novel model that learns how to\nboth generate and modify the facial image from high-level semantic attributes.\nOur key idea is to formulate a Semi-Latent Facial Attribute Space (SL-FAS) to\nsystematically learn relationship between user-defined and latent attributes,\nas well as between those attributes and RGB imagery. As part of this newly\nformulated space, we propose a new model --- SL-GAN which is a specific form of\nGenerative Adversarial Network. Finally, we present an iterative training\nalgorithm for SL-GAN. The experiments on recent CelebA and CASIA-WebFace\ndatasets validate the effectiveness of our proposed framework. We will also\nmake data, pre-trained models and code available.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 10:04:06 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Yin", "Weidong", ""], ["Fu", "Yanwei", ""], ["Sigal", "Leonid", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1704.02199", "submitter": "Yuta Matsuzaki Mr", "authors": "Yuta Matsuzaki, Kazushige Okayasu, Takaaki Imanari, Naomichi\n  Kobayashi, Yoshihiro Kanehara, Ryousuke Takasawa, Akio Nakamura, Hirokatsu\n  Kataoka", "title": "Could you guess an interesting movie from the posters?: An evaluation of\n  vision-based features on movie poster database", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to estimate the Winner of world-wide film festival from\nthe exhibited movie poster. The task is an extremely challenging because the\nestimation must be done with only an exhibited movie poster, without any film\nratings and box-office takings. In order to tackle this problem, we have\ncreated a new database which is consist of all movie posters included in the\nfour biggest film festivals. The movie poster database (MPDB) contains historic\nmovies over 80 years which are nominated a movie award at each year. We apply a\ncouple of feature types, namely hand-craft, mid-level and deep feature to\nextract various information from a movie poster. Our experiments showed\nsuggestive knowledge, for example, the Academy award estimation can be better\nrate with a color feature and a facial emotion feature generally performs good\nrate on the MPDB. The paper may suggest a possibility of modeling human taste\nfor a movie recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 12:17:38 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Matsuzaki", "Yuta", ""], ["Okayasu", "Kazushige", ""], ["Imanari", "Takaaki", ""], ["Kobayashi", "Naomichi", ""], ["Kanehara", "Yoshihiro", ""], ["Takasawa", "Ryousuke", ""], ["Nakamura", "Akio", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1704.02201", "submitter": "Franziska Mueller", "authors": "Franziska Mueller, Dushyant Mehta, Oleksandr Sotnychenko, Srinath\n  Sridhar, Dan Casas, Christian Theobalt", "title": "Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor", "comments": "Accepted at the International Conference on Computer Vision (ICCV)\n  2017", "journal-ref": null, "doi": "10.1109/ICCV.2017.131", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for real-time, robust and accurate hand pose\nestimation from moving egocentric RGB-D cameras in cluttered real environments.\nExisting methods typically fail for hand-object interactions in cluttered\nscenes imaged from egocentric viewpoints, common for virtual or augmented\nreality applications. Our approach uses two subsequently applied Convolutional\nNeural Networks (CNNs) to localize the hand and regress 3D joint locations.\nHand localization is achieved by using a CNN to estimate the 2D position of the\nhand center in the input, even in the presence of clutter and occlusions. The\nlocalized hand position, together with the corresponding input depth value, is\nused to generate a normalized cropped image that is fed into a second CNN to\nregress relative 3D hand joint locations in real time. For added accuracy,\nrobustness and temporal stability, we refine the pose estimates using a\nkinematic pose tracking energy. To train the CNNs, we introduce a new\nphotorealistic dataset that uses a merged reality approach to capture and\nsynthesize large amounts of annotated data of natural hand interaction in\ncluttered scenes. Through quantitative and qualitative evaluation, we show that\nour method is robust to self-occlusion and occlusions by objects, particularly\nin moving egocentric perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 12:23:03 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 14:05:06 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Mueller", "Franziska", ""], ["Mehta", "Dushyant", ""], ["Sotnychenko", "Oleksandr", ""], ["Sridhar", "Srinath", ""], ["Casas", "Dan", ""], ["Theobalt", "Christian", ""]]}, {"id": "1704.02203", "submitter": "Ryo Yonetani", "authors": "Ryo Yonetani, Vishnu Naresh Boddeti, Kris M. Kitani, Yoichi Sato", "title": "Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic\n  Encryption", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a privacy-preserving framework for learning visual classifiers by\nleveraging distributed private image data. This framework is designed to\naggregate multiple classifiers updated locally using private data and to ensure\nthat no private information about the data is exposed during and after its\nlearning procedure. We utilize a homomorphic cryptosystem that can aggregate\nthe local classifiers while they are encrypted and thus kept secret. To\novercome the high computational cost of homomorphic encryption of\nhigh-dimensional classifiers, we (1) impose sparsity constraints on local\nclassifier updates and (2) propose a novel efficient encryption scheme named\ndoubly-permuted homomorphic encryption (DPHE) which is tailored to sparse\nhigh-dimensional data. DPHE (i) decomposes sparse data into its constituent\nnon-zero values and their corresponding support indices, (ii) applies\nhomomorphic encryption only to the non-zero values, and (iii) employs double\npermutations on the support indices to make them secret. Our experimental\nevaluation on several public datasets shows that the proposed approach achieves\ncomparable performance against state-of-the-art visual recognition methods\nwhile preserving privacy and significantly outperforms other privacy-preserving\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 12:26:51 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 15:39:23 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Yonetani", "Ryo", ""], ["Boddeti", "Vishnu Naresh", ""], ["Kitani", "Kris M.", ""], ["Sato", "Yoichi", ""]]}, {"id": "1704.02205", "submitter": "Xin Tao", "authors": "Xiaoyong Shen, Hongyun Gao, Xin Tao, Chao Zhou, Jiaya Jia", "title": "High-Quality Correspondence and Segmentation Estimation for Dual-Lens\n  Smart-Phone Portraits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating correspondence between two images and extracting the foreground\nobject are two challenges in computer vision. With dual-lens smart phones, such\nas iPhone 7Plus and Huawei P9, coming into the market, two images of slightly\ndifferent views provide us new information to unify the two topics. We propose\na joint method to tackle them simultaneously via a joint fully connected\nconditional random field (CRF) framework. The regional correspondence is used\nto handle textureless regions in matching and make our CRF system\ncomputationally efficient. Our method is evaluated over 2,000 new image pairs,\nand produces promising results on challenging portrait images.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 12:36:42 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Shen", "Xiaoyong", ""], ["Gao", "Hongyun", ""], ["Tao", "Xin", ""], ["Zhou", "Chao", ""], ["Jia", "Jiaya", ""]]}, {"id": "1704.02206", "submitter": "Oggi Rudovic", "authors": "Dieu Linh Tran, Robert Walecki, Ognjen Rudovic, Stefanos\n  Eleftheriadis, Bj{\\o}rn Schuller and Maja Pantic", "title": "DeepCoder: Semi-parametric Variational Autoencoders for Automatic Facial\n  Action Coding", "comments": "ICCV 2017 - accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face exhibits an inherent hierarchy in its representations (i.e.,\nholistic facial expressions can be encoded via a set of facial action units\n(AUs) and their intensity). Variational (deep) auto-encoders (VAE) have shown\ngreat results in unsupervised extraction of hierarchical latent representations\nfrom large amounts of image data, while being robust to noise and other\nundesired artifacts. Potentially, this makes VAEs a suitable approach for\nlearning facial features for AU intensity estimation. Yet, most existing\nVAE-based methods apply classifiers learned separately from the encoded\nfeatures. By contrast, the non-parametric (probabilistic) approaches, such as\nGaussian Processes (GPs), typically outperform their parametric counterparts,\nbut cannot deal easily with large amounts of data. To this end, we propose a\nnovel VAE semi-parametric modeling framework, named DeepCoder, which combines\nthe modeling power of parametric (convolutional) and nonparametric (ordinal\nGPs) VAEs, for joint learning of (1) latent representations at multiple levels\nin a task hierarchy1, and (2) classification of multiple ordinal outputs. We\nshow on benchmark datasets for AU intensity estimation that the proposed\nDeepCoder outperforms the state-of-the-art approaches, and related VAEs and\ndeep learning models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 16:23:56 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 04:26:08 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Tran", "Dieu Linh", ""], ["Walecki", "Robert", ""], ["Rudovic", "Ognjen", ""], ["Eleftheriadis", "Stefanos", ""], ["Schuller", "Bj\u00f8rn", ""], ["Pantic", "Maja", ""]]}, {"id": "1704.02218", "submitter": "Hamed R. Tavakoli", "authors": "Hamed R. Tavakoli, Jorma Laaksonen, and Esa Rahtu", "title": "Investigating Natural Image Pleasantness Recognition using Deep Features\n  and Eye Tracking for Loosely Controlled Human-computer Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits recognition of natural image pleasantness by employing\ndeep convolutional neural networks and affordable eye trackers. There exist\nseveral approaches to recognize image pleasantness: (1) computer vision, and\n(2) psychophysical signals. For natural images, computer vision approaches have\nnot been as successful as for abstract paintings and is lagging behind the\npsychophysical signals like eye movements. Despite better results, the\nscalability of eye movements is adversely affected by the sensor cost. While\nthe introduction of affordable sensors have helped the scalability issue by\nmaking the sensors more accessible, the application of such sensors in a\nloosely controlled human-computer interaction setup is not yet studied for\naffective image tagging. On the other hand, deep convolutional neural networks\nhave boosted the performance of vision-based techniques significantly in recent\nyears. To investigate the current status in regard to affective image tagging,\nwe (1) introduce a new eye movement dataset using an affordable eye tracker,\n(2) study the use of deep neural networks for pleasantness recognition, (3)\ninvestigate the gap between deep features and eye movements. To meet these\nends, we record eye movements in a less controlled setup, akin to daily\nhuman-computer interaction. We assess features from eye movements, visual\nfeatures, and their combination. Our results show that (1) recognizing natural\nimage pleasantness from eye movement under less restricted setup is difficult\nand previously used techniques are prone to fail, and (2) visual class\ncategories are strong cues for predicting pleasantness, due to their\ncorrelation with emotions, necessitating careful study of this phenomenon. This\nlatter finding is alerting as some deep learning approaches may fit to the\nclass category bias.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:16:17 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Tavakoli", "Hamed R.", ""], ["Laaksonen", "Jorma", ""], ["Rahtu", "Esa", ""]]}, {"id": "1704.02224", "submitter": "Xiaoming Deng", "authors": "Xiaoming Deng, Shuo Yang, Yinda Zhang, Ping Tan, Liang Chang, Hongan\n  Wang", "title": "Hand3D: Hand Pose Estimation using 3D Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D neural network architecture for 3D hand pose estimation\nfrom a single depth image. Different from previous works that mostly run on 2D\ndepth image domain and require intermediate or post process to bring in the\nsupervision from 3D space, we convert the depth map to a 3D volumetric\nrepresentation, and feed it into a 3D convolutional neural network(CNN) to\ndirectly produce the pose in 3D requiring no further process. Our system does\nnot require the ground truth reference point for initialization, and our\nnetwork architecture naturally integrates both local feature and global context\nin 3D space. To increase the coverage of the hand pose space of the training\ndata, we render synthetic depth image by transferring hand pose from existing\nreal image datasets. We evaluation our algorithm on two public benchmarks and\nachieve the state-of-the-art performance. The synthetic hand pose dataset will\nbe available.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:27:48 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Deng", "Xiaoming", ""], ["Yang", "Shuo", ""], ["Zhang", "Yinda", ""], ["Tan", "Ping", ""], ["Chang", "Liang", ""], ["Wang", "Hongan", ""]]}, {"id": "1704.02231", "submitter": "Maedeh Aghaei", "authors": "Maedeh Aghaei, Federico Parezzan, Mariella Dimiccoli, Petia Radeva,\n  Marco Cristani", "title": "Clothing and People - A Social Signal Processing Perspective", "comments": "To appear in the 12th IEEE International Conference on Automatic Face\n  and Gesture Recognition (FG 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our society and century, clothing is not anymore used only as a means for\nbody protection. Our paper builds upon the evidence, studied within the social\nsciences, that clothing brings a clear communicative message in terms of social\nsignals, influencing the impression and behaviour of others towards a person.\nIn fact, clothing correlates with personality traits, both in terms of\nself-assessment and assessments that unacquainted people give to an individual.\nThe consequences of these facts are important: the influence of clothing on the\ndecision making of individuals has been investigated in the literature, showing\nthat it represents a discriminative factor to differentiate among diverse\ngroups of people. Unfortunately, this has been observed after cumbersome and\nexpensive manual annotations, on very restricted populations, limiting the\nscope of the resulting claims. With this position paper, we want to sketch the\nmain steps of the very first systematic analysis, driven by social signal\nprocessing techniques, of the relationship between clothing and social signals,\nboth sent and perceived. Thanks to human parsing technologies, which exhibit\nhigh robustness owing to deep learning architectures, we are now capable to\nisolate visual patterns characterising a large types of garments. These\nalgorithms will be used to capture statistical relations on a large corpus of\nevidence to confirm the sociological findings and to go beyond the state of the\nart.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:45:52 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Aghaei", "Maedeh", ""], ["Parezzan", "Federico", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""], ["Cristani", "Marco", ""]]}, {"id": "1704.02249", "submitter": "Steffen Wolf", "authors": "Steffen Wolf, Lukas Schott, Ullrich K\\\"othe and Fred Hamprecht", "title": "Learned Watershed: End-to-End Learning of Seeded Segmentation", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned boundary maps are known to outperform hand- crafted ones as a basis\nfor the watershed algorithm. We show, for the first time, how to train\nwatershed computation jointly with boundary map prediction. The estimator for\nthe merging priorities is cast as a neural network that is con- volutional\n(over space) and recurrent (over iterations). The latter allows learning of\ncomplex shape priors. The method gives the best known seeded segmentation\nresults on the CREMI segmentation challenge.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 14:40:15 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 08:10:24 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Wolf", "Steffen", ""], ["Schott", "Lukas", ""], ["K\u00f6the", "Ullrich", ""], ["Hamprecht", "Fred", ""]]}, {"id": "1704.02268", "submitter": "Artsiom Sanakoyeu", "authors": "Miguel A Bautista, Artsiom Sanakoyeu, Bj\\\"orn Ommer", "title": "Deep Unsupervised Similarity Learning using Partially Ordered Sets", "comments": "Accepted for publication at IEEE Computer Vision and Pattern\n  Recognition 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 15:44:51 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 09:09:48 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 12:39:41 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Bautista", "Miguel A", ""], ["Sanakoyeu", "Artsiom", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1704.02304", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "It Takes (Only) Two: Adversarial Generator-Encoder Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:38:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:09:41 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 15:05:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1704.02348", "submitter": "Jana Lipkova", "authors": "Jana Lipkov\\'a, Markus Rempfler, Patrick Christ, John Lowengrub,\n  Bjoern H. Menze", "title": "Automated Unsupervised Segmentation of Liver Lesions in CT scans via\n  Cahn-Hilliard Phase Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of liver lesions is crucial for detection, diagnosis and\nmonitoring progression of liver cancer. However, design of accurate automated\nmethods remains challenging due to high noise in CT scans, low contrast between\nliver and lesions, as well as large lesion variability. We propose a 3D\nautomatic, unsupervised method for liver lesions segmentation using a phase\nseparation approach. It is assumed that liver is a mixture of two phases:\nhealthy liver and lesions, represented by different image intensities polluted\nby noise. The Cahn-Hilliard equation is used to remove the noise and separate\nthe mixture into two distinct phases with well-defined interfaces. This\nsimplifies the lesion detection and segmentation task drastically and enables\nto segment liver lesions by thresholding the Cahn-Hilliard solution. The method\nwas tested on 3Dircadb and LITS dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 18:59:13 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Lipkov\u00e1", "Jana", ""], ["Rempfler", "Markus", ""], ["Christ", "Patrick", ""], ["Lowengrub", "John", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1704.02356", "submitter": "Philip Saponaro Philip Saponaro", "authors": "P. Saponaro, W. Treible, A. Kolagunda, S. Rhein, J. Caplan, C.\n  Kambhamettu, R. Wisser", "title": "Three-Dimensional Segmentation of Vesicular Networks of Fungal Hyphae in\n  Macroscopic Microscopy Image Stacks", "comments": "This is submitted to ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the extraction and quantification of features from\nthree-dimensional (3-D) image stacks is a critical task for advancing computer\nvision research. The union of 3-D image acquisition and analysis enables the\nquantification of biological resistance of a plant tissue to fungal infection\nthrough the analysis of attributes such as fungal penetration depth, fungal\nmass, and branching of the fungal network of connected cells. From an image\nprocessing perspective, these tasks reduce to segmentation of vessel-like\nstructures and the extraction of features from their skeletonization. In order\nto sample multiple infection events for analysis, we have developed an approach\nwe refer to as macroscopic microscopy. However, macroscopic microscopy produces\nhigh-resolution image stacks that pose challenges to routine approaches and are\ndifficult for a human to annotate to obtain ground truth data. We present a\nsynthetic hyphal network generator, a comparison of several vessel segmentation\nmethods, and a minimum spanning tree method for connecting small gaps resulting\nfrom imperfections in imaging or incomplete skeletonization of hyphal networks.\nQualitative results are shown for real microscopic data. We believe the\ncomparison of vessel detectors on macroscopic microscopy data, the synthetic\nvessel generator, and the gap closing technique are beneficial to the image\nprocessing community.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 19:47:02 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Saponaro", "P.", ""], ["Treible", "W.", ""], ["Kolagunda", "A.", ""], ["Rhein", "S.", ""], ["Caplan", "J.", ""], ["Kambhamettu", "C.", ""], ["Wisser", "R.", ""]]}, {"id": "1704.02386", "submitter": "Anurag Arnab", "authors": "Anurag Arnab and Philip H.S Torr", "title": "Pixelwise Instance Segmentation with a Dynamically Instantiated Network", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation and object detection research have recently achieved\nrapid progress. However, the former task has no notion of different instances\nof the same object, and the latter operates at a coarse, bounding-box level. We\npropose an Instance Segmentation system that produces a segmentation map where\neach pixel is assigned an object class and instance identity label. Most\napproaches adapt object detectors to produce segments instead of boxes. In\ncontrast, our method is based on an initial semantic segmentation module, which\nfeeds into an instance subnetwork. This subnetwork uses the initial\ncategory-level segmentation, along with cues from the output of an object\ndetector, within an end-to-end CRF to predict instances. This part of our model\nis dynamically instantiated to produce a variable number of instances per\nimage. Our end-to-end approach requires no post-processing and considers the\nimage holistically, instead of processing independent proposals. Therefore,\nunlike some related work, a pixel cannot belong to multiple instances.\nFurthermore, far more precise segmentations are achieved, as shown by our\nstate-of-the-art results (particularly at high IoU thresholds) on the Pascal\nVOC and Cityscapes datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 22:14:09 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Arnab", "Anurag", ""], ["Torr", "Philip H. S", ""]]}, {"id": "1704.02393", "submitter": "Kyle Genova", "authors": "Kyle Genova, Manolis Savva, Angel X. Chang, Thomas Funkhouser", "title": "Learning Where to Look: Data-Driven Viewpoint Set Selection for 3D\n  Scenes", "comments": "ICCV submission, combined main paper and supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of rendered images, whether from completely synthetic datasets or\nfrom 3D reconstructions, is increasingly prevalent in vision tasks. However,\nlittle attention has been given to how the selection of viewpoints affects the\nperformance of rendered training sets. In this paper, we propose a data-driven\napproach to view set selection. Given a set of example images, we extract\nstatistics describing their contents and generate a set of views matching the\ndistribution of those statistics. Motivated by semantic segmentation tasks, we\nmodel the spatial distribution of each semantic object category within an image\nview volume. We provide a search algorithm that generates a sampling of likely\ncandidate views according to the example distribution, and a set selection\nalgorithm that chooses a subset of the candidates that jointly cover the\nexample distribution. Results of experiments with these algorithms on SUNCG\nindicate that they are indeed able to produce view distributions similar to an\nexample set from NYUDv2 according to the earth mover's distance. Furthermore,\nthe selected views improve performance on semantic segmentation compared to\nalternative view selection algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 22:40:46 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Genova", "Kyle", ""], ["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1704.02402", "submitter": "Yuhang Wu", "authors": "Yuhang Wu, Shishir K. Shah, Ioannis A. Kakadiaris", "title": "GoDP: Globally optimized dual pathway system for facial landmark\n  localization in-the-wild", "comments": "Accepted by Image and Vision Computing in December, 2017", "journal-ref": "Image and Vision Computing, 2018", "doi": "10.1016/j.imavis.2017.12.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark localization is a fundamental module for pose-invariant face\nrecognition. The most common approach for facial landmark detection is cascaded\nregression, which is composed of two steps: feature extraction and facial shape\nregression. Recent methods employ deep convolutional networks to extract robust\nfeatures for each step, while the whole system could be regarded as a deep\ncascaded regression architecture. In this work, instead of employing a deep\nregression network, a Globally Optimized Dual-Pathway (GoDP) deep architecture\nis proposed to identify the target pixels through solving a cascaded pixel\nlabeling problem without resorting to high-level inference models or complex\nstacked architecture. The proposed end-to-end system relies on distance-aware\nsoftmax functions and dual-pathway proposal-refinement architecture. Results\nshow that it outperforms the state-of-the-art cascaded regression-based methods\non multiple in-the-wild face alignment databases. The model achieves 1.84\nnormalized mean error (NME) on the AFLW database, which outperforms 3DDFA by\n61.8%. Experiments on face identification demonstrate that GoDP, coupled with\nDPM-headhunter, is able to improve rank-1 identification rate by 44.2% compared\nto Dlib toolbox on a challenging database.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 23:39:29 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 15:14:18 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wu", "Yuhang", ""], ["Shah", "Shishir K.", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1704.02422", "submitter": "Jo Schlemper", "authors": "Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony Price, Daniel\n  Rueckert", "title": "A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image\n  Reconstruction", "comments": "To be published in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by recent advances in deep learning, we propose a framework for\nreconstructing dynamic sequences of 2D cardiac magnetic resonance (MR) images\nfrom undersampled data using a deep cascade of convolutional neural networks\n(CNNs) to accelerate the data acquisition process. In particular, we address\nthe case where data is acquired using aggressive Cartesian undersampling.\nFirstly, we show that when each 2D image frame is reconstructed independently,\nthe proposed method outperforms state-of-the-art 2D compressed sensing\napproaches such as dictionary learning-based MR image reconstruction, in terms\nof reconstruction error and reconstruction speed. Secondly, when reconstructing\nthe frames of the sequences jointly, we demonstrate that CNNs can learn\nspatio-temporal correlations efficiently by combining convolution and data\nsharing approaches. We show that the proposed method consistently outperforms\nstate-of-the-art methods and is capable of preserving anatomical structure more\nfaithfully up to 11-fold undersampling. Moreover, reconstruction is very fast:\neach complete dynamic sequence can be reconstructed in less than 10s and, for\nthe 2D case, each image frame can be reconstructed in 23ms, enabling real-time\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 02:13:48 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 11:28:10 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Schlemper", "Jo", ""], ["Caballero", "Jose", ""], ["Hajnal", "Joseph V.", ""], ["Price", "Anthony", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1704.02431", "submitter": "Dan Xu", "authors": "Dan Xu, Wanli Ouyang, Elisa Ricci, Xiaogang Wang, Nicu Sebe", "title": "Learning Cross-Modal Deep Representations for Robust Pedestrian\n  Detection", "comments": "Accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for detecting pedestrians under adverse\nillumination conditions. Our approach relies on a novel cross-modality learning\nframework and it is based on two main phases. First, given a multimodal\ndataset, a deep convolutional network is employed to learn a non-linear\nmapping, modeling the relations between RGB and thermal data. Then, the learned\nfeature representations are transferred to a second deep network, which\nreceives as input an RGB image and outputs the detection results. In this way,\nfeatures which are both discriminative and robust to bad illumination\nconditions are learned. Importantly, at test time, only the second pipeline is\nconsidered and no thermal data are required. Our extensive evaluation\ndemonstrates that the proposed approach outperforms the state-of- the-art on\nthe challenging KAIST multispectral pedestrian dataset and it is competitive\nwith previous methods on the popular Caltech dataset.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 03:12:23 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 21:50:22 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Xu", "Dan", ""], ["Ouyang", "Wanli", ""], ["Ricci", "Elisa", ""], ["Wang", "Xiaogang", ""], ["Sebe", "Nicu", ""]]}, {"id": "1704.02446", "submitter": "Feng Qian", "authors": "Feng Qian, Miao Yin, Ming-Jun Su, Yaojun Wang, Guangmin Hu", "title": "Seismic facies recognition based on prestack data using deep\n  convolutional autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prestack seismic data carries much useful information that can help us find\nmore complex atypical reservoirs. Therefore, we are increasingly inclined to\nuse prestack seismic data for seis- mic facies recognition. However, due to the\ninclusion of ex- cessive redundancy, effective feature extraction from prestack\nseismic data becomes critical. In this paper, we consider seis- mic facies\nrecognition based on prestack data as an image clus- tering problem in computer\nvision (CV) by thinking of each prestack seismic gather as a picture. We\npropose a convo- lutional autoencoder (CAE) network for deep feature learn- ing\nfrom prestack seismic data, which is more effective than principal component\nanalysis (PCA) in redundancy removing and valid information extraction. Then,\nusing conventional classification or clustering techniques (e.g. K-means or\nself- organizing maps) on the extracted features, we can achieve seismic facies\nrecognition. We applied our method to the prestack data from physical model and\nLZB region. The re- sult shows that our approach is superior to the\nconventionals.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 06:17:34 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Qian", "Feng", ""], ["Yin", "Miao", ""], ["Su", "Ming-Jun", ""], ["Wang", "Yaojun", ""], ["Hu", "Guangmin", ""]]}, {"id": "1704.02447", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, Yichen Wei", "title": "Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised\n  Approach", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of 3D human pose estimation in the wild.\nThis task is challenging due to lack of training data, as existing datasets are\neither in the wild images with 2D pose or in the lab images with 3D pose.\n  We propose a weakly-supervised transfer learning method that uses mixed 2D\nand 3D labels in a unified deep neutral network that presents two-stage\ncascaded structure. Our network augments a state-of-the-art 2D pose estimation\nsub-network with a 3D depth regression sub-network. Unlike previous two stage\napproaches that train the two sub-networks sequentially and separately, our\ntraining is end-to-end and fully exploits the correlation between the 2D pose\nand depth estimation sub-tasks. The deep features are better learnt through\nshared representations. In doing so, the 3D pose labels in controlled lab\nenvironments are transferred to in the wild images. In addition, we introduce a\n3D geometric constraint to regularize the 3D pose prediction, which is\neffective in the absence of ground truth depth labels. Our method achieves\ncompetitive results on both 2D and 3D benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 06:21:48 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 15:01:30 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Zhou", "Xingyi", ""], ["Huang", "Qixing", ""], ["Sun", "Xiao", ""], ["Xue", "Xiangyang", ""], ["Wei", "Yichen", ""]]}, {"id": "1704.02450", "submitter": "Xiang Wu", "authors": "Xiang Wu, Lingxiao Song, Ran He and Tieniu Tan", "title": "Coupled Deep Learning for Heterogeneous Face Recognition", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face matching is a challenge issue in face recognition due to\nlarge domain difference as well as insufficient pairwise images in different\nmodalities during training. This paper proposes a coupled deep learning (CDL)\napproach for the heterogeneous face matching. CDL seeks a shared feature space\nin which the heterogeneous face matching problem can be approximately treated\nas a homogeneous face matching problem. The objective function of CDL mainly\nincludes two parts. The first part contains a trace norm and a block-diagonal\nprior as relevance constraints, which not only make unpaired images from\nmultiple modalities be clustered and correlated, but also regularize the\nparameters to alleviate overfitting. An approximate variational formulation is\nintroduced to deal with the difficulties of optimizing low-rank constraint\ndirectly. The second part contains a cross modal ranking among triplet domain\nspecific images to maximize the margin for different identities and increase\ndata for a small amount of training samples. Besides, an alternating\nminimization method is employed to iteratively update the parameters of CDL.\nExperimental results show that CDL achieves better performance on the\nchallenging CASIA NIR-VIS 2.0 face recognition database, the IIIT-D Sketch\ndatabase, the CUHK Face Sketch (CUFS), and the CUHK Face Sketch FERET (CUFSF),\nwhich significantly outperforms state-of-the-art heterogeneous face recognition\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 07:10:45 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 08:48:23 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Wu", "Xiang", ""], ["Song", "Lingxiao", ""], ["He", "Ran", ""], ["Tan", "Tieniu", ""]]}, {"id": "1704.02455", "submitter": "Mohammad Reza Khosravi", "authors": "Mohammad Reza Khosravi, Habib Rostami, Gholam Reza Ahmadi, Suleiman\n  Mansouri, Ahmad Keshavarz", "title": "A New Pseudo-color Technique Based on Intensity Information Protection\n  for Passive Sensor Imagery", "comments": null, "journal-ref": "International Journal of Electronics Communication and Computer\n  Engineering, vol. 6, no. 3, pp. 324-329 (2015)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remote sensing image processing is so important in geo-sciences. Images which\nare obtained by different types of sensors might initially be unrecognizable.\nTo make an acceptable visual perception in the images, some pre-processing\nsteps (for removing noises and etc) are preformed which they affect the\nanalysis of images. There are different types of processing according to the\ntypes of remote sensing images. The method that we are going to introduce in\nthis paper is to use virtual colors to colorize the gray-scale images of\nsatellite sensors. This approach helps us to have a better analysis on a sample\nsingle-band image which has been taken by Landsat-8 (OLI) sensor (as a\nmulti-band sensor with natural color bands, its images' natural color can be\ncompared to synthetic color by our approach). A good feature of this method is\nthe original image reversibility in order to keep the suitable resolution of\noutput images.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 08:16:56 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Khosravi", "Mohammad Reza", ""], ["Rostami", "Habib", ""], ["Ahmadi", "Gholam Reza", ""], ["Mansouri", "Suleiman", ""], ["Keshavarz", "Ahmad", ""]]}, {"id": "1704.02463", "submitter": "Guillermo Garcia-Hernando", "authors": "Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, Tae-Kyun Kim", "title": "First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose\n  Annotations", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the use of 3D hand poses to recognize first-person\ndynamic hand actions interacting with 3D objects. Towards this goal, we\ncollected RGB-D video sequences comprised of more than 100K frames of 45 daily\nhand action categories, involving 26 different objects in several hand\nconfigurations. To obtain hand pose annotations, we used our own mo-cap system\nthat automatically infers the 3D location of each of the 21 joints of a hand\nmodel via 6 magnetic sensors and inverse kinematics. Additionally, we recorded\nthe 6D object poses and provide 3D object models for a subset of hand-object\ninteraction sequences. To the best of our knowledge, this is the first\nbenchmark that enables the study of first-person hand actions with the use of\n3D hand poses. We present an extensive experimental evaluation of RGB-D and\npose-based action recognition by 18 baselines/state-of-the-art approaches. The\nimpact of using appearance features, poses, and their combinations are\nmeasured, and the different training/testing protocols are evaluated. Finally,\nwe assess how ready the 3D hand pose estimation field is when hands are\nseverely occluded by objects in egocentric views and its influence on action\nrecognition. From the results, we see clear benefits of using hand pose as a\ncue for action recognition compared to other data modalities. Our dataset and\nexperiments can be of interest to communities of 3D hand pose estimation, 6D\nobject pose, and robotics as well as action recognition.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 09:45:12 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 13:02:01 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Garcia-Hernando", "Guillermo", ""], ["Yuan", "Shanxin", ""], ["Baek", "Seungryul", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1704.02470", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, Luc\n  Van Gool", "title": "DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a rapid rise in the quality of built-in smartphone cameras, their\nphysical limitations - small sensor size, compact lenses and the lack of\nspecific hardware, - impede them to achieve the quality results of DSLR\ncameras. In this work we present an end-to-end deep learning approach that\nbridges this gap by translating ordinary photos into DSLR-quality images. We\npropose learning the translation function using a residual convolutional neural\nnetwork that improves both color rendition and image sharpness. Since the\nstandard mean squared loss is not well suited for measuring perceptual image\nquality, we introduce a composite perceptual error function that combines\ncontent, color and texture losses. The first two losses are defined\nanalytically, while the texture loss is learned in an adversarial fashion. We\nalso present DPED, a large-scale dataset that consists of real photos captured\nfrom three different phones and one high-end reflex camera. Our quantitative\nand qualitative assessments reveal that the enhanced image quality is\ncomparable to that of DSLR-taken photos, while the methodology is generalized\nto any type of digital camera.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 10:27:36 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 19:46:11 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ignatov", "Andrey", ""], ["Kobyshev", "Nikolay", ""], ["Timofte", "Radu", ""], ["Vanhoey", "Kenneth", ""], ["Van Gool", "Luc", ""]]}, {"id": "1704.02492", "submitter": "Lu Tian", "authors": "Lu Tian, Shengjin Wang", "title": "Metric Learning in Codebook Generation of Bag-of-Words for Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is generally divided into two part: first how to\nrepresent a pedestrian by discriminative visual descriptors and second how to\ncompare them by suitable distance metrics. Conventional methods isolate these\ntwo parts, the first part usually unsupervised and the second part supervised.\nThe Bag-of-Words (BoW) model is a widely used image representing descriptor in\npart one. Its codebook is simply generated by clustering visual features in\nEuclidian space. In this paper, we propose to use part two metric learning\ntechniques in the codebook generation phase of BoW. In particular, the proposed\ncodebook is clustered under Mahalanobis distance which is learned supervised.\nExtensive experiments prove that our proposed method is effective. With several\nlow level features extracted on superpixel and fused together, our method\noutperforms state-of-the-art on person re-identification benchmarks including\nVIPeR, PRID450S, and Market1501.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 13:11:12 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 16:21:33 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Tian", "Lu", ""], ["Wang", "Shengjin", ""]]}, {"id": "1704.02510", "submitter": "Zili Yi", "authors": "Zili Yi, Hao Zhang, Ping Tan, Minglun Gong", "title": "DualGAN: Unsupervised Dual Learning for Image-to-Image Translation", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Generative Adversarial Networks (GANs) for cross-domain\nimage-to-image translation have made much progress recently. Depending on the\ntask complexity, thousands to millions of labeled image pairs are needed to\ntrain a conditional GAN. However, human labeling is expensive, even\nimpractical, and large quantities of data may not always be available. Inspired\nby dual learning from natural language translation, we develop a novel dual-GAN\nmechanism, which enables image translators to be trained from two sets of\nunlabeled images from two domains. In our architecture, the primal GAN learns\nto translate images from domain U to those in domain V, while the dual GAN\nlearns to invert the task. The closed loop made by the primal and dual tasks\nallows images from either domain to be translated and then reconstructed. Hence\na loss function that accounts for the reconstruction error of images can be\nused to train the translators. Experiments on multiple image translation tasks\nwith unlabeled data show considerable performance gain of DualGAN over a single\nGAN. For some tasks, DualGAN can even achieve comparable or slightly better\nresults than conditional GAN trained on fully labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 16:13:52 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 00:18:08 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 06:43:40 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 20:42:00 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Yi", "Zili", ""], ["Zhang", "Hao", ""], ["Tan", "Ping", ""], ["Gong", "Minglun", ""]]}, {"id": "1704.02516", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan, Ambar Pal, Gaurav Sharma and Anurag Mittal", "title": "An Empirical Evaluation of Visual Question Answering for Novel Objects", "comments": "11 pages, 4 figures, accepted in CVPR 2017 (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of answering questions about images in the harder\nsetting, where the test questions and corresponding images contain novel\nobjects, which were not queried about in the training data. Such setting is\ninevitable in real world-owing to the heavy tailed distribution of the visual\ncategories, there would be some objects which would not be annotated in the\ntrain set. We show that the performance of two popular existing methods drop\nsignificantly (up to 28%) when evaluated on novel objects cf. known objects. We\npropose methods which use large existing external corpora of (i) unlabeled\ntext, i.e. books, and (ii) images tagged with classes, to achieve novel object\nbased visual question answering. We do systematic empirical studies, for both\nan oracle case where the novel objects are known textually, as well as a fully\nautomatic case without any explicit knowledge of the novel objects, but with\nthe minimal assumption that the novel objects are semantically related to the\nexisting objects in training. The proposed methods for novel object based\nvisual question answering are modular and can potentially be used with many\nvisual question answering architectures. We show consistent improvements with\nthe two popular architectures and give qualitative analysis of the cases where\nthe model does well and of those where it fails to bring improvements.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 17:51:46 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Pal", "Ambar", ""], ["Sharma", "Gaurav", ""], ["Mittal", "Anurag", ""]]}, {"id": "1704.02518", "submitter": "Lorenzo Seidenari", "authors": "Leonardo Galteri, Lorenzo Seidenari, Marco Bertini, Alberto Del Bimbo", "title": "Deep Generative Adversarial Compression Artifact Removal", "comments": "ICCV 2017 Camera Ready + Acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression artifacts arise in images whenever a lossy compression algorithm\nis applied. These artifacts eliminate details present in the original image, or\nadd noise and small structures; because of these effects they make images less\npleasant for the human eye, and may also lead to decreased performance of\ncomputer vision algorithms such as object detectors. To eliminate such\nartifacts, when decompressing an image, it is required to recover the original\nimage from a disturbed version. To this end, we present a feed-forward fully\nconvolutional residual network model trained using a generative adversarial\nframework. To provide a baseline, we show that our model can be also trained\noptimizing the Structural Similarity (SSIM), which is a better loss with\nrespect to the simpler Mean Squared Error (MSE). Our GAN is able to produce\nimages with more photorealistic details than MSE or SSIM based networks.\nMoreover we show that our approach can be used as a pre-processing step for\nobject detection in case images are degraded by compression to a point that\nstate-of-the art detectors fail. In this task, our GAN method obtains better\nperformance than MSE or SSIM trained networks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 18:15:58 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 15:56:46 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 18:38:21 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Galteri", "Leonardo", ""], ["Seidenari", "Lorenzo", ""], ["Bertini", "Marco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1704.02567", "submitter": "Xin Chen", "authors": "Xin Chen and Emma Marriott and Yuling Yan", "title": "Motion Saliency Based Automatic Delineation of Glottis Contour in\n  High-speed Digital Images", "comments": "4 pages 2 figures", "journal-ref": null, "doi": "10.1109/ICIEA.2017.8282998", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, high-speed videoendoscopy (HSV) has significantly aided the\ndiagnosis of voice pathologies and furthered the understanding the voice\nproduction in recent years. As the first step of these studies, automatic\nsegmentation of glottal images till presents a major challenge for this\ntechnique. In this paper, we propose an improved Saliency Network that\nautomatically delineates the contour of the glottis from HSV image sequences.\nOur proposed additional saliency measure, Motion Saliency (MS), improves upon\nthe original Saliency Network by using the velocities of defined edges. In our\nresults and analysis, we demonstrate the effectiveness of our approach and\ndiscuss its potential applications for computer-aided assessment of voice\npathologies and understanding voice production.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 06:27:27 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Chen", "Xin", ""], ["Marriott", "Emma", ""], ["Yan", "Yuling", ""]]}, {"id": "1704.02581", "submitter": "Hongsong Wang", "authors": "Hongsong Wang, Liang Wang", "title": "Modeling Temporal Dynamics and Spatial Configurations of Actions Using\n  Two-Stream Recurrent Neural Networks", "comments": "Accepted to IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, skeleton based action recognition gains more popularity due to\ncost-effective depth sensors coupled with real-time skeleton estimation\nalgorithms. Traditional approaches based on handcrafted features are limited to\nrepresent the complexity of motion patterns. Recent methods that use Recurrent\nNeural Networks (RNN) to handle raw skeletons only focus on the contextual\ndependency in the temporal domain and neglect the spatial configurations of\narticulated skeletons. In this paper, we propose a novel two-stream RNN\narchitecture to model both temporal dynamics and spatial configurations for\nskeleton based action recognition. We explore two different structures for the\ntemporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed\naccording to human body kinematics. We also propose two effective methods to\nmodel the spatial structure by converting the spatial graph into a sequence of\njoints. To improve generalization of our model, we further exploit 3D\ntransformation based data augmentation techniques including rotation and\nscaling transformation to transform the 3D coordinates of skeletons during\ntraining. Experiments on 3D action recognition benchmark datasets show that our\nmethod brings a considerable improvement for a variety of actions, i.e.,\ngeneric actions, interaction activities and gestures.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 10:09:55 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 09:08:19 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Wang", "Hongsong", ""], ["Wang", "Liang", ""]]}, {"id": "1704.02602", "submitter": "Ferda Ofli", "authors": "Dat Tien Nguyen, Firoj Alam, Ferda Ofli, Muhammad Imran", "title": "Automatic Image Filtering on Social Networks Using Deep Learning and\n  Perceptual Hashing During Crises", "comments": "Accepted for publication in the 14th International Conference on\n  Information Systems For Crisis Response and Management (ISCRAM), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of social media platforms, especially during disasters,\ncreates unique opportunities for humanitarian organizations to gain situational\nawareness and launch relief operations accordingly. In addition to the textual\ncontent, people post overwhelming amounts of imagery data on social networks\nwithin minutes of a disaster hit. Studies point to the importance of this\nonline imagery content for emergency response. Despite recent advances in the\ncomputer vision field, automatic processing of the crisis-related social media\nimagery data remains a challenging task. It is because a majority of which\nconsists of redundant and irrelevant content. In this paper, we present an\nimage processing pipeline that comprises de-duplication and relevancy filtering\nmechanisms to collect and filter social media image content in real-time during\na crisis event. Results obtained from extensive experiments on real-world\ncrisis datasets demonstrate the significance of the proposed pipeline for\noptimal utilization of both human and machine computing resources.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 13:34:27 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Alam", "Firoj", ""], ["Ofli", "Ferda", ""], ["Imran", "Muhammad", ""]]}, {"id": "1704.02612", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, Tae-Kyun Kim", "title": "BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a large-scale hand pose dataset, collected using a\nnovel capture method. Existing datasets are either generated synthetically or\ncaptured using depth sensors: synthetic datasets exhibit a certain level of\nappearance difference from real depth images, and real datasets are limited in\nquantity and coverage, mainly due to the difficulty to annotate them. We\npropose a tracking system with six 6D magnetic sensors and inverse kinematics\nto automatically obtain 21-joints hand pose annotations of depth maps captured\nwith minimal restriction on the range of motion. The capture protocol aims to\nfully cover the natural hand pose space. As shown in embedding plots, the new\ndataset exhibits a significantly wider and denser range of hand poses compared\nto existing benchmarks. Current state-of-the-art methods are evaluated on the\ndataset, and we demonstrate significant improvements in cross-benchmark\nperformance. We also show significant improvements in egocentric hand pose\nestimation with a CNN trained on the new dataset.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 15:00:31 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 17:05:42 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Yuan", "Shanxin", ""], ["Ye", "Qi", ""], ["Stenger", "Bjorn", ""], ["Jain", "Siddhant", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1704.02672", "submitter": "Kaveh Fathian", "authors": "Kaveh Fathian, J. Pablo Ramirez-Paredes, Emily A. Doucette, J. Willard\n  Curtis, Nicholas R. Gans", "title": "Quaternion Based Camera Pose Estimation From Matched Feature Points", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2018.2792142", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel solution to the camera pose estimation problem, where\nrotation and translation of a camera between two views are estimated from\nmatched feature points in the images. The camera pose estimation problem is\ntraditionally solved via algorithms that are based on the essential matrix or\nthe Euclidean homography. With six or more feature points in general positions\nin the space, essential matrix based algorithms can recover a unique solution.\nHowever, such algorithms fail when points are on critical surfaces (e.g.,\ncoplanar points) and homography should be used instead. By formulating the\nproblem in quaternions and decoupling the rotation and translation estimation,\nour proposed algorithm works for all point configurations. Using both simulated\nand real world images, we compare the estimation accuracy of our algorithm with\nsome of the most commonly used algorithms. Our method is shown to be more\nrobust to noise and outliers. For the benefit of community, we have made the\nimplementation of our algorithm available online and free.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 23:29:55 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 18:41:05 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Fathian", "Kaveh", ""], ["Ramirez-Paredes", "J. Pablo", ""], ["Doucette", "Emily A.", ""], ["Curtis", "J. Willard", ""], ["Gans", "Nicholas R.", ""]]}, {"id": "1704.02685", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Peyton Greenside, Anshul Kundaje", "title": "Learning Important Features Through Propagating Activation Differences", "comments": "Updated to include changes present in the ICML camera-ready paper,\n  and other small corrections", "journal-ref": "PMLR 70:3145-3153, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purported \"black box\" nature of neural networks is a barrier to adoption\nin applications where interpretability is essential. Here we present DeepLIFT\n(Deep Learning Important FeaTures), a method for decomposing the output\nprediction of a neural network on a specific input by backpropagating the\ncontributions of all neurons in the network to every feature of the input.\nDeepLIFT compares the activation of each neuron to its 'reference activation'\nand assigns contribution scores according to the difference. By optionally\ngiving separate consideration to positive and negative contributions, DeepLIFT\ncan also reveal dependencies which are missed by other approaches. Scores can\nbe computed efficiently in a single backward pass. We apply DeepLIFT to models\ntrained on MNIST and simulated genomic data, and show significant advantages\nover gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:\nbit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:\nhttp://goo.gl/RM8jvH.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 02:23:57 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 22:13:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Greenside", "Peyton", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1704.02694", "submitter": "Rodney LaLonde Iii", "authors": "Rodney LaLonde, Dong Zhang, Mubarak Shah", "title": "ClusterNet: Detecting Small Objects in Large Scenes by Exploiting\n  Spatio-Temporal Information", "comments": "Main paper is 8 pages. Supplemental section contains a walk-through\n  of our method (using a qualitative example) and qualitative results for WPAFB\n  2009 dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in wide area motion imagery (WAMI) has drawn the attention\nof the computer vision research community for a number of years. WAMI proposes\na number of unique challenges including extremely small object sizes, both\nsparse and densely-packed objects, and extremely large search spaces (large\nvideo frames). Nearly all state-of-the-art methods in WAMI object detection\nreport that appearance-based classifiers fail in this challenging data and\ninstead rely almost entirely on motion information in the form of background\nsubtraction or frame-differencing. In this work, we experimentally verify the\nfailure of appearance-based classifiers in WAMI, such as Faster R-CNN and a\nheatmap-based fully convolutional neural network (CNN), and propose a novel\ntwo-stage spatio-temporal CNN which effectively and efficiently combines both\nappearance and motion information to significantly surpass the state-of-the-art\nin WAMI object detection. To reduce the large search space, the first stage\n(ClusterNet) takes in a set of extremely large video frames, combines the\nmotion and appearance information within the convolutional architecture, and\nproposes regions of objects of interest (ROOBI). These ROOBI can contain from\none to clusters of several hundred objects due to the large video frame size\nand varying object density in WAMI. The second stage (FoveaNet) then estimates\nthe centroid location of all objects in that given ROOBI simultaneously via\nheatmap estimation. The proposed method exceeds state-of-the-art results on the\nWPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped\nobjects, as well as being the first proposed method in wide area motion imagery\nto detect completely stationary objects.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 03:44:13 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 21:14:19 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["LaLonde", "Rodney", ""], ["Zhang", "Dong", ""], ["Shah", "Mubarak", ""]]}, {"id": "1704.02703", "submitter": "Lei Bi", "authors": "Lei Bi, Jinman Kim, Ashnil Kumar, Dagan Feng", "title": "Automatic Liver Lesion Detection using Cascaded Deep Residual Networks", "comments": "Submission for 2017 ISBI LiTS Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of liver lesions is a fundamental requirement towards\nthe creation of computer aided diagnosis (CAD) and decision support systems\n(CDS). Traditional segmentation approaches depend heavily upon hand-crafted\nfeatures and a priori knowledge of the user. As such, these methods are\ndifficult to adopt within a clinical environment. Recently, deep learning\nmethods based on fully convolutional networks (FCNs) have been successful in\nmany segmentation problems primarily because they leverage a large labelled\ndataset to hierarchically learn the features that best correspond to the\nshallow visual appearance as well as the deep semantics of the areas to be\nsegmented. However, FCNs based on a 16 layer VGGNet architecture have limited\ncapacity to add additional layers. Therefore, it is challenging to learn more\ndiscriminative features among different classes for FCNs. In this study, we\novercome these limitations using deep residual networks (ResNet) to segment\nliver lesions. ResNet contain skip connections between convolutional layers,\nwhich solved the problem of the training degradation of training accuracy in\nvery deep networks and thereby enables the use of additional layers for\nlearning more discriminative features. In addition, we achieve more precise\nboundary definitions through a novel cascaded ResNet architecture with\nmulti-scale fusion to gradually learn and infer the boundaries of both the\nliver and the liver lesions. Our proposed method achieved 4th place in the ISBI\n2017 Liver Tumor Segmentation Challenge by the submission deadline.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 04:05:50 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 02:58:40 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Bi", "Lei", ""], ["Kim", "Jinman", ""], ["Kumar", "Ashnil", ""], ["Feng", "Dagan", ""]]}, {"id": "1704.02712", "submitter": "Zheng Xu", "authors": "Zheng Xu, Mario A. T. Figueiredo, Xiaoming Yuan, Christoph Studer, and\n  Tom Goldstein", "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern computer vision and machine learning applications rely on solving\ndifficult optimization problems that involve non-differentiable objective\nfunctions and constraints. The alternating direction method of multipliers\n(ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a\ngeneralization of ADMM that often achieves better performance, but its\nefficiency depends strongly on algorithm parameters that must be chosen by an\nexpert user. We propose an adaptive method that automatically tunes the key\nalgorithm parameters to achieve optimal performance without user oversight.\nInspired by recent work on adaptivity, the proposed adaptive relaxed ADMM\n(ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A\ndetailed convergence analysis of ARADMM is provided, and numerical results on\nseveral applications demonstrate fast practical convergence.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 05:07:38 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Xu", "Zheng", ""], ["Figueiredo", "Mario A. T.", ""], ["Yuan", "Xiaoming", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "1704.02729", "submitter": "Rodrigo Santa Cruz", "authors": "Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian and Stephen Gould", "title": "DeepPermNet: Visual Permutation Learning", "comments": "Accepted in IEEE International Conference on Computer Vision and\n  Pattern Recognition CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principled approach to uncover the structure of visual data by\nsolving a novel deep learning task coined visual permutation learning. The goal\nof this task is to find the permutation that recovers the structure of data\nfrom shuffled versions of it. In the case of natural images, this task boils\ndown to recovering the original image from patches shuffled by an unknown\npermutation matrix. Unfortunately, permutation matrices are discrete, thereby\nposing difficulties for gradient-based methods. To this end, we resort to a\ncontinuous approximation of these matrices using doubly-stochastic matrices\nwhich we generate from standard CNN predictions using Sinkhorn iterations.\nUnrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet,\nan end-to-end CNN model for this task. The utility of DeepPermNet is\ndemonstrated on two challenging computer vision problems, namely, (i) relative\nattributes learning and (ii) self-supervised representation learning. Our\nresults show state-of-the-art performance on the Public Figures and OSR\nbenchmarks for (i) and on the classification and segmentation tasks on the\nPASCAL VOC dataset for (ii).\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 06:59:00 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Cruz", "Rodrigo Santa", ""], ["Fernando", "Basura", ""], ["Cherian", "Anoop", ""], ["Gould", "Stephen", ""]]}, {"id": "1704.02738", "submitter": "Xin Tao", "authors": "Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, Jiaya Jia", "title": "Detail-revealing Deep Video Super-resolution", "comments": "9 pages, submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous CNN-based video super-resolution approaches need to align multiple\nframes to the reference. In this paper, we show that proper frame alignment and\nmotion compensation is crucial for achieving high quality results. We\naccordingly propose a `sub-pixel motion compensation' (SPMC) layer in a CNN\nframework. Analysis and experiments show the suitability of this layer in video\nSR. The final end-to-end, scalable CNN framework effectively incorporates the\nSPMC layer and fuses multiple frames to reveal image details. Our\nimplementation can generate visually and quantitatively high-quality results,\nsuperior to current state-of-the-arts, without the need of parameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 07:28:27 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Tao", "Xin", ""], ["Gao", "Hongyun", ""], ["Liao", "Renjie", ""], ["Wang", "Jue", ""], ["Jia", "Jiaya", ""]]}, {"id": "1704.02781", "submitter": "Laura Leal-Taix\\'e", "authors": "Laura Leal-Taix\\'e and Anton Milan and Konrad Schindler and Daniel\n  Cremers and Ian Reid and Stefan Roth", "title": "Tracking the Trackers: An Analysis of the State of the Art in Multiple\n  Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for research. We present a\nbenchmark for Multiple Object Tracking launched in the late 2014, with the goal\nof creating a framework for the standardized evaluation of multiple object\ntracking methods. This paper collects the two releases of the benchmark made so\nfar, and provides an in-depth analysis of almost 50 state-of-the-art trackers\nthat were tested on over 11000 frames. We show the current trends and\nweaknesses of multiple people tracking methods, and provide pointers of what\nresearchers should be focusing on to push the field forward.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:44:39 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Leal-Taix\u00e9", "Laura", ""], ["Milan", "Anton", ""], ["Schindler", "Konrad", ""], ["Cremers", "Daniel", ""], ["Reid", "Ian", ""], ["Roth", "Stefan", ""]]}, {"id": "1704.02787", "submitter": "Spyridon Thermos", "authors": "Spyridon Thermos, Georgios Th. Papadopoulos, Petros Daras, Gerasimos\n  Potamianos", "title": "Deep Affordance-grounded Sensorimotor Object Recognition", "comments": "9 pages, 7 figures, dataset link included, accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-established by cognitive neuroscience that human perception of\nobjects constitutes a complex process, where object appearance information is\ncombined with evidence about the so-called object \"affordances\", namely the\ntypes of actions that humans typically perform when interacting with them. This\nfact has recently motivated the \"sensorimotor\" approach to the challenging task\nof automatic object recognition, where both information sources are fused to\nimprove robustness. In this work, the aforementioned paradigm is adopted,\nsurpassing current limitations of sensorimotor object recognition research.\nSpecifically, the deep learning paradigm is introduced to the problem for the\nfirst time, developing a number of novel neuro-biologically and\nneuro-physiologically inspired architectures that utilize state-of-the-art\nneural networks for fusing the available information sources in multiple ways.\nThe proposed methods are evaluated using a large RGB-D corpus, which is\nspecifically collected for the task of sensorimotor object recognition and is\nmade publicly available. Experimental results demonstrate the utility of\naffordance information to object recognition, achieving an up to 29% relative\nerror reduction by its inclusion.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:06:53 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Thermos", "Spyridon", ""], ["Papadopoulos", "Georgios Th.", ""], ["Daras", "Petros", ""], ["Potamianos", "Gerasimos", ""]]}, {"id": "1704.02792", "submitter": "Yuxin Peng", "authors": "Xiangteng He and Yuxin Peng", "title": "Fine-graind Image Classification via Combining Vision and Language", "comments": "9 pages, to appear in CVPR 2017", "journal-ref": null, "doi": "10.1109/CVPR.2017.775", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification is a challenging task due to the large\nintra-class variance and small inter-class variance, aiming at recognizing\nhundreds of sub-categories belonging to the same basic-level category. Most\nexisting fine-grained image classification methods generally learn part\ndetection models to obtain the semantic parts for better classification\naccuracy. Despite achieving promising results, these methods mainly have two\nlimitations: (1) not all the parts which obtained through the part detection\nmodels are beneficial and indispensable for classification, and (2)\nfine-grained image classification requires more detailed visual descriptions\nwhich could not be provided by the part locations or attribute annotations. For\naddressing the above two limitations, this paper proposes the two-stream model\ncombining vision and language (CVL) for learning latent semantic\nrepresentations. The vision stream learns deep representations from the\noriginal visual information via deep convolutional neural network. The language\nstream utilizes the natural language descriptions which could point out the\ndiscriminative parts or characteristics for each image, and provides a flexible\nand compact way of encoding the salient visual aspects for distinguishing\nsub-categories. Since the two streams are complementary, combining the two\nstreams can further achieves better classification accuracy. Comparing with 12\nstate-of-the-art methods on the widely used CUB-200-2011 dataset for\nfine-grained image classification, the experimental results demonstrate our CVL\napproach achieves the best performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:34:06 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 03:01:38 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["He", "Xiangteng", ""], ["Peng", "Yuxin", ""]]}, {"id": "1704.02809", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Mariella Dimiccoli, Marc Bola\\~nos, Maedeh Aghaei,\n  and Petia Radeva", "title": "R-Clustering for Egocentric Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new method for egocentric video temporal\nsegmentation based on integrating a statistical mean change detector and\nagglomerative clustering(AC) within an energy-minimization framework. Given the\ntendency of most AC methods to oversegment video sequences when clustering\ntheir frames, we combine the clustering with a concept drift detection\ntechnique (ADWIN) that has rigorous guarantee of performances. ADWIN serves as\na statistical upper bound for the clustering-based video segmentation. We\nintegrate both techniques in an energy-minimization framework that serves to\ndisambiguate the decision of both techniques and to complete the segmentation\ntaking into account the temporal continuity of video frames descriptors. We\npresent experiments over egocentric sets of more than 13.000 images acquired\nwith different wearable cameras, showing that our method outperforms\nstate-of-the-art clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 11:36:01 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Talavera", "Estefania", ""], ["Dimiccoli", "Mariella", ""], ["Bola\u00f1os", "Marc", ""], ["Aghaei", "Maedeh", ""], ["Radeva", "Petia", ""]]}, {"id": "1704.02827", "submitter": "Partha Ghosh", "authors": "Partha Ghosh, Jie Song, Emre Aksan, Otmar Hilliges", "title": "Learning Human Motion Models for Long-term Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture for the learning of predictive spatio-temporal\nmotion models from data alone. Our approach, dubbed the Dropout Autoencoder\nLSTM, is capable of synthesizing natural looking motion sequences over long\ntime horizons without catastrophic drift or motion degradation. The model\nconsists of two components, a 3-layer recurrent neural network to model\ntemporal aspects and a novel auto-encoder that is trained to implicitly recover\nthe spatial structure of the human skeleton via randomly removing information\nabout joints during training time. This Dropout Autoencoder (D-AE) is then used\nto filter each predicted pose of the LSTM, reducing accumulation of error and\nhence drift over time. Furthermore, we propose new evaluation protocols to\nassess the quality of synthetic motion sequences even for which no ground truth\ndata exists. The proposed protocols can be used to assess generated sequences\nof arbitrary length. Finally, we evaluate our proposed method on two of the\nlargest motion-capture datasets available to date and show that our model\noutperforms the state-of-the-art on a variety of actions, including cyclic and\nacyclic motion, and that it can produce natural looking sequences over longer\ntime horizons than previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 12:22:22 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 11:51:17 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Ghosh", "Partha", ""], ["Song", "Jie", ""], ["Aksan", "Emre", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1704.02895", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic and Bryan\n  Russell", "title": "ActionVLAD: Learning spatio-temporal aggregation for action\n  classification", "comments": "Accepted to CVPR 2017. Project page:\n  https://rohitgirdhar.github.io/ActionVLAD/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new video representation for action\nclassification that aggregates local convolutional features across the entire\nspatio-temporal extent of the video. We do so by integrating state-of-the-art\ntwo-stream networks with learnable spatio-temporal feature aggregation. The\nresulting architecture is end-to-end trainable for whole-video classification.\nWe investigate different strategies for pooling across space and time and\ncombining signals from the different streams. We find that: (i) it is important\nto pool jointly across space and time, but (ii) appearance and motion streams\nare best aggregated into their own separate representations. Finally, we show\nthat our representation outperforms the two-stream base architecture by a large\nmargin (13% relative) as well as out-performs other baselines with comparable\nbase architectures on HMDB51, UCF101, and Charades video classification\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:09:41 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Girdhar", "Rohit", ""], ["Ramanan", "Deva", ""], ["Gupta", "Abhinav", ""], ["Sivic", "Josef", ""], ["Russell", "Bryan", ""]]}, {"id": "1704.02899", "submitter": "Roy R. Lederman", "authors": "Roy R. Lederman, Amit Singer", "title": "Continuously heterogeneous hyper-objects in cryo-EM and 3-D movies of\n  many temporal dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle cryo-electron microscopy (EM) is an increasingly popular\nmethod for determining the 3-D structure of macromolecules from noisy 2-D\nimages of single macromolecules whose orientations and positions are random and\nunknown. One of the great opportunities in cryo-EM is to recover the structure\nof macromolecules in heterogeneous samples, where multiple types or multiple\nconformations are mixed together. Indeed, in recent years, many tools have been\nintroduced for the analysis of multiple discrete classes of molecules mixed\ntogether in a cryo-EM experiment. However, many interesting structures have a\ncontinuum of conformations which do not fit discrete models nicely; the\nanalysis of such continuously heterogeneous models has remained a more elusive\ngoal. In this manuscript, we propose to represent heterogeneous molecules and\nsimilar structures as higher dimensional objects. We generalize the basic\noperations used in many existing reconstruction algorithms, making our approach\ngeneric in the sense that, in principle, existing algorithms can be adapted to\nreconstruct those higher dimensional objects. As proof of concept, we present a\nprototype of a new algorithm which we use to solve simulated reconstruction\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:15:25 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Lederman", "Roy R.", ""], ["Singer", "Amit", ""]]}, {"id": "1704.02901", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Nikos Komodakis", "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on\n  Graphs", "comments": "Accepted to CVPR 2017; extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems can be formulated as prediction on graph-structured\ndata. In this work, we generalize the convolution operator from regular grids\nto arbitrary graphs while avoiding the spectral domain, which allows us to\nhandle graphs of varying size and connectivity. To move beyond a simple\ndiffusion, filter weights are conditioned on the specific edge labels in the\nneighborhood of a vertex. Together with the proper choice of graph coarsening,\nwe explore constructing deep neural networks for graph classification. In\nparticular, we demonstrate the generality of our formulation in point cloud\nclassification, where we set the new state of the art, and on a graph\nclassification dataset, where we outperform other deep learning approaches. The\nsource code is available at https://github.com/mys007/ecc\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:18:54 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 18:05:11 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 09:31:17 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1704.02906", "submitter": "Viveka Kulharia", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri and Philip H. S.\n  Torr and Puneet K. Dokania", "title": "Multi-Agent Diverse Generative Adversarial Networks", "comments": "This is an updated version of our CVPR'18 paper with the same title.\n  In this version, we also introduce MAD-GAN-Sim in Appendix B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MAD-GAN, an intuitive generalization to the Generative Adversarial\nNetworks (GANs) and its conditional variants to address the well known problem\nof mode collapse. First, MAD-GAN is a multi-agent GAN architecture\nincorporating multiple generators and one discriminator. Second, to enforce\nthat different generators capture diverse high probability modes, the\ndiscriminator of MAD-GAN is designed such that along with finding the real and\nfake samples, it is also required to identify the generator that generated the\ngiven fake sample. Intuitively, to succeed in this task, the discriminator must\nlearn to push different generators towards different identifiable modes. We\nperform extensive experiments on synthetic and real datasets and compare\nMAD-GAN with different variants of GAN. We show high quality diverse sample\ngenerations for challenging tasks such as image-to-image translation and face\ngeneration. In addition, we also show that MAD-GAN is able to disentangle\ndifferent modalities when trained using highly challenging diverse-class\ndataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the\nend, we show its efficacy on the unsupervised feature representation task. In\nAppendix, we introduce a similarity based competing objective (MAD-GAN-Sim)\nwhich encourages different generators to generate diverse samples based on a\nuser defined similarity metric. We show its performance on the image-to-image\ntranslation, and also show its effectiveness on the unsupervised feature\nrepresentation task.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 23:29:16 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 16:21:52 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""], ["Torr", "Philip H. S.", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "1704.02923", "submitter": "Sandro Pezzelle", "authors": "Ionut Sorodoc, Sandro Pezzelle, Aur\\'elie Herbelot, Mariella\n  Dimiccoli, Raffaella Bernardi", "title": "Pay Attention to Those Sets! Learning Quantification from Images", "comments": "Submitted to Journal Paper, 28 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major advances have recently been made in merging language and vision\nrepresentations. But most tasks considered so far have confined themselves to\nthe processing of objects and lexicalised relations amongst objects (content\nwords). We know, however, that humans (even pre-school children) can abstract\nover raw data to perform certain types of higher-level reasoning, expressed in\nnatural language by function words. A case in point is given by their ability\nto learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From\nformal semantics and cognitive linguistics, we know that quantifiers are\nrelations over sets which, as a simplification, we can see as proportions. For\ninstance, in 'most fish are red', most encodes the proportion of fish which are\nred fish. In this paper, we study how well current language and vision\nstrategies model such relations. We show that state-of-the-art attention\nmechanisms coupled with a traditional linguistic formalisation of quantifiers\ngives best performance on the task. Additionally, we provide insights on the\nrole of 'gist' representations in quantification. A 'logical' strategy to\ntackle the task would be to first obtain a numerosity estimation for the two\ninvolved sets and then compare their cardinalities. We however argue that\nprecisely identifying the composition of the sets is not only beyond current\nstate-of-the-art models but perhaps even detrimental to a task that is most\nefficiently performed by refining the approximate numerosity estimator of the\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 16:03:31 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Sorodoc", "Ionut", ""], ["Pezzelle", "Sandro", ""], ["Herbelot", "Aur\u00e9lie", ""], ["Dimiccoli", "Mariella", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1704.02930", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Erik Rodner, Christoph K\\\"ading, Joachim Denzler", "title": "Fast Learning and Prediction for Object Detection using Whitened CNN\n  Features", "comments": "Technical Report about the possibilities introduced with ARTOS v2,\n  originally created March 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine features extracted from pre-trained convolutional neural networks\n(CNNs) with the fast, linear Exemplar-LDA classifier to get the advantages of\nboth: the high detection performance of CNNs, automatic feature engineering,\nfast model learning from few training samples and efficient sliding-window\ndetection. The Adaptive Real-Time Object Detection System (ARTOS) has been\nrefactored broadly to be used in combination with Caffe for the experimental\nstudies reported in this work.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 16:15:00 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 15:35:43 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Rodner", "Erik", ""], ["K\u00e4ding", "Christoph", ""], ["Denzler", "Joachim", ""]]}, {"id": "1704.02956", "submitter": "Weifeng Chen", "authors": "Weifeng Chen, Donglai Xiang, Jia Deng", "title": "Surface Normals in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of single-image depth estimation for images in the wild.\nWe collect human annotated surface normals and use them to train a neural\nnetwork that directly predicts pixel-wise depth. We propose two novel loss\nfunctions for training with surface normal annotations. Experiments on NYU\nDepth and our own dataset demonstrate that our approach can significantly\nimprove the quality of depth estimation in the wild.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:13:00 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Chen", "Weifeng", ""], ["Xiang", "Donglai", ""], ["Deng", "Jia", ""]]}, {"id": "1704.02965", "submitter": "Adrian Albert", "authors": "Adrian Albert and Jasleen Kaur and Marta Gonzalez", "title": "Using convolutional networks and satellite imagery to identify patterns\n  in urban environments at a large scale", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban planning applications (energy audits, investment, etc.) require an\nunderstanding of built infrastructure and its environment, i.e., both\nlow-level, physical features (amount of vegetation, building area and geometry\netc.), as well as higher-level concepts such as land use classes (which encode\nexpert understanding of socio-economic end uses). This kind of data is\nexpensive and labor-intensive to obtain, which limits its availability\n(particularly in developing countries). We analyze patterns in land use in\nurban neighborhoods using large-scale satellite imagery data (which is\navailable worldwide from third-party providers) and state-of-the-art computer\nvision techniques based on deep convolutional neural networks. For supervision,\ngiven the limited availability of standard benchmarks for remote-sensing data,\nwe obtain ground truth land use class labels carefully sampled from open-source\nsurveys, in particular the Urban Atlas land classification dataset of $20$ land\nuse classes across $~300$ European cities. We use this data to train and\ncompare deep architectures which have recently shown good performance on\nstandard computer vision tasks (image classification and segmentation),\nincluding on geospatial data. Furthermore, we show that the deep\nrepresentations extracted from satellite imagery of urban environments can be\nused to compare neighborhoods across several cities. We make our dataset\navailable for other machine learning researchers to use for remote-sensing\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:42:37 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 20:09:46 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Albert", "Adrian", ""], ["Kaur", "Jasleen", ""], ["Gonzalez", "Marta", ""]]}, {"id": "1704.02966", "submitter": "Samuel Rota Bul\\`o", "authors": "Samuel Rota Bul\\`o, Gerhard Neuhold, Peter Kontschieder", "title": "Loss Max-Pooling for Semantic Image Segmentation", "comments": "accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel loss max-pooling concept for handling imbalanced\ntraining data distributions, applicable as alternative loss layer in the\ncontext of deep neural networks for semantic image segmentation. Most\nreal-world semantic segmentation datasets exhibit long tail distributions with\nfew object categories comprising the majority of data and consequently biasing\nthe classifiers towards them. Our method adaptively re-weights the\ncontributions of each pixel based on their observed losses, targeting\nunder-performing classification results as often encountered for\nunder-represented object classes. Our approach goes beyond conventional\ncost-sensitive learning attempts through adaptive considerations that allow us\nto indirectly address both, inter- and intra-class imbalances. We provide a\ntheoretical justification of our approach, complementary to experimental\nanalyses on benchmark datasets. In our experiments on the Cityscapes and Pascal\nVOC 2012 segmentation datasets we find consistently improved results,\ndemonstrating the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:44:33 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Bul\u00f2", "Samuel Rota", ""], ["Neuhold", "Gerhard", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1704.02998", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Larry S. Davis, Leonid Sigal", "title": "Weakly-Supervised Spatial Context Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the power of spatial context as a self-supervisory signal for\nlearning visual representations. In particular, we propose spatial context\nnetworks that learn to predict a representation of one image patch from another\nimage patch, within the same image, conditioned on their real-valued relative\nspatial offset. Unlike auto-encoders, that aim to encode and reconstruct\noriginal image patches, our network aims to encode and reconstruct intermediate\nrepresentations of the spatially offset patches. As such, the network learns a\nspatially conditioned contextual representation. By testing performance with\nvarious patch selection mechanisms we show that focusing on object-centric\npatches is important, and that using object proposal as a patch selection\nmechanism leads to the highest improvement in performance. Further, unlike\nauto-encoders, context encoders [21], or other forms of unsupervised feature\nlearning, we illustrate that contextual supervision (with pre-trained model\ninitialization) can improve on existing pre-trained model performance. We build\nour spatial context networks on top of standard VGG_19 and CNN_M architectures\nand, among other things, show that we can achieve improvements (with no\nadditional explicit supervision) over the original ImageNet pre-trained VGG_19\nand CNN_M models in object categorization and detection on VOC2007.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:15:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:07:41 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Wu", "Zuxuan", ""], ["Davis", "Larry S.", ""], ["Sigal", "Leonid", ""]]}, {"id": "1704.03039", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Nuno Vasconcelos", "title": "Semantically Consistent Regularization for Zero-Shot Recognition", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of semantics in zero-shot learning is considered. The effectiveness\nof previous approaches is analyzed according to the form of supervision\nprovided. While some learn semantics independently, others only supervise the\nsemantic subspace explained by training classes. Thus, the former is able to\nconstrain the whole space but lacks the ability to model semantic correlations.\nThe latter addresses this issue but leaves part of the semantic space\nunsupervised. This complementarity is exploited in a new convolutional neural\nnetwork (CNN) framework, which proposes the use of semantics as constraints for\nrecognition.Although a CNN trained for classification has no transfer ability,\nthis can be encouraged by learning an hidden semantic layer together with a\nsemantic code for classification. Two forms of semantic constraints are then\nintroduced. The first is a loss-based regularizer that introduces a\ngeneralization constraint on each semantic predictor. The second is a codeword\nregularizer that favors semantic-to-class mappings consistent with prior\nsemantic knowledge while allowing these to be learned from data. Significant\nimprovements over the state-of-the-art are achieved on several datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:59:33 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Morgado", "Pedro", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1704.03057", "submitter": "Samet Hicsonmez", "authors": "Samet Hicsonmez, Nermin Samet, Fadime Sener, Pinar Duygulu", "title": "DRAW: Deep networks for Recognizing styles of Artists Who illustrate\n  children's books", "comments": "ACM ICMR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated from a young boy's capability to recognize an\nillustrator's style in a totally different context. In the book \"We are All\nBorn Free\" [1], composed of selected rights from the Universal Declaration of\nHuman Rights interpreted by different illustrators, the boy was surprised to\nsee a picture similar to the ones in the \"Winnie the Witch\" series drawn by\nKorky Paul (Figure 1). The style was noticeable in other characters of the same\nillustrator in different books as well. The capability of a child to easily\nspot the style was shown to be valid for other illustrators such as Axel\nScheffler and Debi Gliori. The boy's enthusiasm let us to start the journey to\nexplore the capabilities of machines to recognize the style of illustrators.\n  We collected pages from children's books to construct a new illustrations\ndataset consisting of about 6500 pages from 24 artists. We exploited deep\nnetworks for categorizing illustrators and with around 94% classification\nperformance our method over-performed the traditional methods by more than 10%.\nGoing beyond categorization we explored transferring style. The classification\nperformance on the transferred images has shown the ability of our system to\ncapture the style. Furthermore, we discovered representative illustrations and\ndiscriminative stylistic elements.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 21:03:51 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Hicsonmez", "Samet", ""], ["Samet", "Nermin", ""], ["Sener", "Fadime", ""], ["Duygulu", "Pinar", ""]]}, {"id": "1704.03058", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Sinisa Todorovic, Song-Chun Zhu", "title": "CERN: Confidence-Energy Recurrent Network for Group Activity Recognition", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 21:08:39 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Shu", "Tianmin", ""], ["Todorovic", "Sinisa", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1704.03067", "submitter": "Wei Li", "authors": "Wei Li, Farnaz Abitahi, Zhigang Zhu", "title": "Action Unit Detection with Region Adaptation, Multi-labeling Learning\n  and Optimal Temporal Fusing", "comments": "The paper is accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action Unit (AU) detection becomes essential for facial analysis. Many\nproposed approaches face challenging problems in dealing with the alignments of\ndifferent face regions, in the effective fusion of temporal information, and in\ntraining a model for multiple AU labels. To better address these problems, we\npropose a deep learning framework for AU detection with region of interest\n(ROI) adaptation, integrated multi-label learning, and optimal LSTM-based\ntemporal fusing. First, ROI cropping nets (ROI Nets) are designed to make sure\nspecifically interested regions of faces are learned independently; each\nsub-region has a local convolutional neural network (CNN) - an ROI Net, whose\nconvolutional filters will only be trained for the corresponding region.\nSecond, multi-label learning is employed to integrate the outputs of those\nindividual ROI cropping nets, which learns the inter-relationships of various\nAUs and acquires global features across sub-regions for AU detection. Finally,\nthe optimal selection of multiple LSTM layers to form the best LSTM Net is\ncarried out to best fuse temporal features, in order to make the AU prediction\nthe most accurate. The proposed approach is evaluated on two popular AU\ndetection datasets, BP4D and DISFA, outperforming the state of the art\nsignificantly, with an average improvement of around 13% on BP4D and 25% on\nDISFA, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 21:58:56 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Li", "Wei", ""], ["Abitahi", "Farnaz", ""], ["Zhu", "Zhigang", ""]]}, {"id": "1704.03069", "submitter": "Dario Prandi", "authors": "Dario Prandi and Jean-Paul Gauthier", "title": "A semidiscrete version of the Citti-Petitot-Sarti model as a plausible\n  model for anthropomorphic image reconstruction and pattern recognition", "comments": "123 pages, revised version", "journal-ref": null, "doi": "10.1007/978-3-319-78482-3", "report-no": null, "categories": "cs.CV math.AP math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his beautiful book [66], Jean Petitot proposes a sub-Riemannian model for\nthe primary visual cortex of mammals. This model is neurophysiologically\njustified. Further developments of this theory lead to efficient algorithms for\nimage reconstruction, based upon the consideration of an associated\nhypoelliptic diffusion. The sub-Riemannian model of Petitot and Citti-Sarti (or\ncertain of its improvements) is a left-invariant structure over the group\n$SE(2)$ of rototranslations of the plane. Here, we propose a semi-discrete\nversion of this theory, leading to a left-invariant structure over the group\n$SE(2,N)$, restricting to a finite number of rotations. This apparently very\nsimple group is in fact quite atypical: it is maximally almost periodic, which\nleads to much simpler harmonic analysis compared to $SE(2).$ Based upon this\nsemi-discrete model, we improve on previous image-reconstruction algorithms and\nwe develop a pattern-recognition theory that leads also to very efficient\nalgorithms in practice.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 22:15:42 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 09:05:13 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 14:53:55 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Prandi", "Dario", ""], ["Gauthier", "Jean-Paul", ""]]}, {"id": "1704.03079", "submitter": "Asit Mishra", "authors": "Asit Mishra, Jeffrey J Cook, Eriko Nurvitadhi and Debbie Marr", "title": "WRPN: Training and Inference using Wide Reduced-Precision Networks", "comments": "Under submission to CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For computer vision applications, prior works have shown the efficacy of\nreducing the numeric precision of model parameters (network weights) in deep\nneural networks but also that reducing the precision of activations hurts model\naccuracy much more than reducing the precision of model parameters. We study\nschemes to train networks from scratch using reduced-precision activations\nwithout hurting the model accuracy. We reduce the precision of activation maps\n(along with model parameters) using a novel quantization scheme and increase\nthe number of filter maps in a layer, and find that this scheme compensates or\nsurpasses the accuracy of the baseline full-precision network. As a result, one\ncan significantly reduce the dynamic memory footprint, memory bandwidth,\ncomputational energy and speed up the training and inference process with\nappropriate hardware support. We call our scheme WRPN - wide reduced-precision\nnetworks. We report results using our proposed schemes and show that our\nresults are better than previously reported accuracies on ILSVRC-12 dataset\nwhile being computationally less expensive compared to previously reported\nreduced-precision networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 22:54:38 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mishra", "Asit", ""], ["Cook", "Jeffrey J", ""], ["Nurvitadhi", "Eriko", ""], ["Marr", "Debbie", ""]]}, {"id": "1704.03114", "submitter": "Bo Dai", "authors": "Bo Dai, Yuqi Zhang, Dahua Lin", "title": "Detecting Visual Relationships with Deep Relational Networks", "comments": "To be appeared in CVPR 2017 as an oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationships among objects play a crucial role in image understanding.\nDespite the great success of deep learning techniques in recognizing individual\nobjects, reasoning about the relationships among objects remains a challenging\ntask. Previous methods often treat this as a classification problem,\nconsidering each type of relationship (e.g. \"ride\") or each distinct visual\nphrase (e.g. \"person-ride-horse\") as a category. Such approaches are faced with\nsignificant difficulties caused by the high diversity of visual appearance for\neach kind of relationships or the large number of distinct visual phrases. We\npropose an integrated framework to tackle this problem. At the heart of this\nframework is the Deep Relational Network, a novel formulation designed\nspecifically for exploiting the statistical dependencies between objects and\ntheir relationships. On two large datasets, the proposed method achieves\nsubstantial improvement over state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 02:11:20 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 08:18:58 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Dai", "Bo", ""], ["Zhang", "Yuqi", ""], ["Lin", "Dahua", ""]]}, {"id": "1704.03116", "submitter": "Jose Dolz", "authors": "Jose Dolz and Ismail Ben Ayed and Christian Desrosiers", "title": "DOPE: Distributed Optimization for Pairwise Energies", "comments": "Accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate an Alternating Direction Method of Mul-tipliers (ADMM) that\nsystematically distributes the computations of any technique for optimizing\npairwise functions, including non-submodular potentials. Such discrete\nfunctions are very useful in segmentation and a breadth of other vision\nproblems. Our method decomposes the problem into a large set of small\nsub-problems, each involving a sub-region of the image domain, which can be\nsolved in parallel. We achieve consistency between the sub-problems through a\nnovel constraint that can be used for a large class of pair-wise functions. We\ngive an iterative numerical solution that alternates between solving the\nsub-problems and updating consistency variables, until convergence. We report\ncomprehensive experiments, which demonstrate the benefit of our general\ndistributed solution in the case of the popular serial algorithm of Boykov and\nKolmogorov (BK algorithm) and, also, in the context of non-submodular\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 02:21:13 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1704.03135", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Yale Song, Jiebo Luo", "title": "Improving Pairwise Ranking for Multi-label Image Classification", "comments": "cvpr 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank has recently emerged as an attractive technique to train\ndeep convolutional neural networks for various computer vision tasks. Pairwise\nranking, in particular, has been successful in multi-label image\nclassification, achieving state-of-the-art results on various benchmarks.\nHowever, most existing approaches use the hinge loss to train their models,\nwhich is non-smooth and thus is difficult to optimize especially with deep\nnetworks. Furthermore, they employ simple heuristics, such as top-k or\nthresholding, to determine which labels to include in the output from a ranked\nlist of labels, which limits their use in the real-world setting. In this work,\nwe propose two techniques to improve pairwise ranking based multi-label image\nclassification: (1) we propose a novel loss function for pairwise ranking,\nwhich is smooth everywhere and thus is easier to optimize; and (2) we\nincorporate a label decision module into the model, estimating the optimal\nconfidence thresholds for each visual concept. We provide theoretical analyses\nof our loss function in the Bayes consistency and risk minimization framework,\nand show its benefit over existing pairwise ranking formulations. We\ndemonstrate the effectiveness of our approach on three large-scale datasets,\nVOC2007, NUS-WIDE and MS-COCO, achieving the best reported results in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:04:54 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 05:29:01 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 02:35:58 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Li", "Yuncheng", ""], ["Song", "Yale", ""], ["Luo", "Jiebo", ""]]}, {"id": "1704.03140", "submitter": "Chun Pong Lau", "authors": "Chun Pong Lau, Yu Hin Lai, Lok Ming Lui", "title": "Restoration of Atmospheric Turbulence-distorted Images via RPCA and\n  Quasiconformal Maps", "comments": "21 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problem of restoring a high-quality image from an observed\nimage sequence strongly distorted by atmospheric turbulence. A novel algorithm\nis proposed in this paper to reduce geometric distortion as well as\nspace-and-time-varying blur due to strong turbulence. By considering a suitable\nenergy functional, our algorithm first obtains a sharp reference image and a\nsubsampled image sequence containing sharp and mildly distorted image frames\nwith respect to the reference image. The subsampled image sequence is then\nstabilized by applying the Robust Principal Component Analysis (RPCA) on the\ndeformation fields between image frames and warping the image frames by a\nquasiconformal map associated with the low-rank part of the deformation matrix.\nAfter image frames are registered to the reference image, the low-rank part of\nthem are deblurred via a blind deconvolution, and the deblurred frames are then\nfused with the enhanced sparse part. Experiments have been carried out on both\nsynthetic and real turbulence-distorted video. Results demonstrate that our\nmethod is effective in alleviating distortions and blur, restoring image\ndetails and enhancing visual quality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:24:44 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 03:40:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Lau", "Chun Pong", ""], ["Lai", "Yu Hin", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1704.03152", "submitter": "Xitong Yang", "authors": "Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar\n  A. Bernal and Jiebo Luo", "title": "Deep Multimodal Representation Learning from Temporal Data", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Learning has been successfully applied to multimodal\nlearning problems, with the aim of learning useful joint representations in\ndata fusion applications. When the available modalities consist of time series\ndata such as video, audio and sensor signals, it becomes imperative to consider\ntheir temporal structure during the fusion process. In this paper, we propose\nthe Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion\nmodel for fusing multiple input modalities that are inherently temporal in\nnature. Key features of our proposed model include: (i) simultaneous learning\nof the joint representation and temporal dependencies between modalities, (ii)\nuse of multiple loss terms in the objective function, including a maximum\ncorrelation loss term to enhance learning of cross-modal information, and (iii)\nthe use of an attention model to dynamically adjust the contribution of\ndifferent input modalities to the joint representation. We validate our model\nvia experimentation on two different tasks: video- and sensor-based activity\nclassification, and audio-visual speech recognition. We empirically analyze the\ncontributions of different components of the proposed CorrRNN model, and\ndemonstrate its robustness, effectiveness and state-of-the-art performance on\nmultiple datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 05:47:42 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Yang", "Xitong", ""], ["Ramesh", "Palghat", ""], ["Chitta", "Radha", ""], ["Madhvanath", "Sriganesh", ""], ["Bernal", "Edgar A.", ""], ["Luo", "Jiebo", ""]]}, {"id": "1704.03155", "submitter": "Xinyu Zhou", "authors": "Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He,\n  Jiajun Liang", "title": "EAST: An Efficient and Accurate Scene Text Detector", "comments": "Accepted to CVPR 2017, fix equation (3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches for scene text detection have already achieved promising\nperformances across various benchmarks. However, they usually fall short when\ndealing with challenging scenarios, even when equipped with deep neural network\nmodels, because the overall performance is determined by the interplay of\nmultiple stages and components in the pipelines. In this work, we propose a\nsimple yet powerful pipeline that yields fast and accurate text detection in\nnatural scenes. The pipeline directly predicts words or text lines of arbitrary\norientations and quadrilateral shapes in full images, eliminating unnecessary\nintermediate steps (e.g., candidate aggregation and word partitioning), with a\nsingle neural network. The simplicity of our pipeline allows concentrating\nefforts on designing loss functions and neural network architecture.\nExperiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500\ndemonstrate that the proposed algorithm significantly outperforms\nstate-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR\n2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps\nat 720p resolution.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 06:04:12 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 08:10:52 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhou", "Xinyu", ""], ["Yao", "Cong", ""], ["Wen", "He", ""], ["Wang", "Yuzhi", ""], ["Zhou", "Shuchang", ""], ["He", "Weiran", ""], ["Liang", "Jiajun", ""]]}, {"id": "1704.03162", "submitter": "Vahid Kazemi", "authors": "Vahid Kazemi, Ali Elqursh", "title": "Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new baseline for visual question answering task. Given\nan image and a question in natural language, our model produces accurate\nanswers according to the content of the image. Our model, while being\narchitecturally simple and relatively small in terms of trainable parameters,\nsets a new state of the art on both unbalanced and balanced VQA benchmark. On\nVQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the\ntest-standard set without using additional data, an improvement of 0.4% over\nstate of the art, and on newly released VQA 2.0, our model scores 59.7% on\nvalidation set outperforming best previously reported results by 0.5%. The\nresults presented in this paper are especially interesting because very similar\nmodels have been tried before but significantly lower performance were\nreported. In light of the new results we hope to see more meaningful research\non visual question answering in the future.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 06:22:57 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 05:53:56 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Kazemi", "Vahid", ""], ["Elqursh", "Ali", ""]]}, {"id": "1704.03173", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu", "title": "Mining Object Parts from CNNs via Active Question-Answering", "comments": "Published in CVPR 2017", "journal-ref": "Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu,\n  \"Mining Object Parts from CNNs via Active Question-Answering\" in CVPR 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a convolutional neural network (CNN) that is pre-trained for object\nclassification, this paper proposes to use active question-answering to\nsemanticize neural patterns in conv-layers of the CNN and mine part concepts.\nFor each part concept, we mine neural patterns in the pre-trained CNN, which\nare related to the target part, and use these patterns to construct an And-Or\ngraph (AOG) to represent a four-layer semantic hierarchy of the part. As an\ninterpretable model, the AOG associates different CNN units with different\nexplicit object parts. We use an active human-computer communication to\nincrementally grow such an AOG on the pre-trained CNN as follows. We allow the\ncomputer to actively identify objects, whose neural patterns cannot be\nexplained by the current AOG. Then, the computer asks human about the\nunexplained objects, and uses the answers to automatically discover certain CNN\npatterns corresponding to the missing knowledge. We incrementally grow the AOG\nto encode new knowledge discovered during the active-learning process. In\nexperiments, our method exhibits high learning efficiency. Our method uses\nabout 1/6-1/3 of the part annotations for training, but achieves similar or\nbetter part-localization performance than fast-RCNN methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 07:27:33 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zhang", "Quanshi", ""], ["Cao", "Ruiming", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1704.03217", "submitter": "Yuanwei Li", "authors": "Yuanwei Li", "title": "Pyramidal Gradient Matching for Optical Flow Estimation", "comments": "This work was finished in August 2016 and then submitted to IEEE PAMI\n  in August 17,2016 and submitted to IEEE TIP in April 9,2017 after revising", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initializing optical flow field by either sparse descriptor matching or dense\npatch matches has been proved to be particularly useful for capturing large\ndisplacements. In this paper, we present a pyramidal gradient matching approach\nthat can provide dense matches for highly accurate and efficient optical flow\nestimation. A novel contribution of our method is that image gradient is used\nto describe image patches and proved to be able to produce robust matching.\nTherefore, our method is more efficient than methods that adopt special\nfeatures (like SIFT) or patch distance metric. Moreover, we find that image\ngradient is scalable for optical flow estimation, which means we can use\ndifferent levels of gradient feature (for example, full gradients or only\ndirection information of gradients) to obtain different complexity without\ndramatic changes in accuracy. Another contribution is that we uncover the\nsecrets of limited PatchMatch through a thorough analysis and design a\npyramidal matching framework based these secrets. Our pyramidal matching\nframework is aimed at robust gradient matching and effective to grow inliers\nand reject outliers. In this framework, we present some special enhancements\nfor outlier filtering in gradient matching. By initializing EpicFlow with our\nmatches, experimental results show that our method is efficient and robust\n(ranking 1st on both clean pass and final pass of MPI Sintel dataset among\npublished methods).\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 09:38:52 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Li", "Yuanwei", ""]]}, {"id": "1704.03225", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Reconstruction of three-dimensional porous media using generative\n  adversarial neural networks", "comments": "21 pages, 20 figures", "journal-ref": "Phys. Rev. E 96, 043309 (2017)", "doi": "10.1103/PhysRevE.96.043309", "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci physics.flu-dyn physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate the variability of multi-phase flow properties of porous media at\nthe pore scale, it is necessary to acquire a number of representative samples\nof the void-solid structure. While modern x-ray computer tomography has made it\npossible to extract three-dimensional images of the pore space, assessment of\nthe variability in the inherent material properties is often experimentally not\nfeasible. We present a novel method to reconstruct the solid-void structure of\nporous media by applying a generative neural network that allows an implicit\ndescription of the probability distribution represented by three-dimensional\nimage datasets. We show, by using an adversarial learning approach for neural\nnetworks, that this method of unsupervised learning is able to generate\nrepresentative samples of porous media that honor their statistics. We\nsuccessfully compare measures of pore morphology, such as the Euler\ncharacteristic, two-point statistics and directional single-phase permeability\nof synthetic realizations with the calculated properties of a bead pack, Berea\nsandstone, and Ketton limestone. Results show that GANs can be used to\nreconstruct high-resolution three-dimensional images of porous media at\ndifferent scales that are representative of the morphology of the images used\nto train the neural network. The fully convolutional nature of the trained\nneural network allows the generation of large samples while maintaining\ncomputational efficiency. Compared to classical stochastic methods of image\nreconstruction, the implicit representation of the learned data distribution\ncan be stored and reused to generate multiple realizations of the pore\nstructure very rapidly.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 09:55:55 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1704.03264", "submitter": "Kai Zhang", "authors": "Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang", "title": "Learning Deep CNN Denoiser Prior for Image Restoration", "comments": "Accepted to CVPR 2017. Code: https://github.com/cszn/ircnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based optimization methods and discriminative learning methods have\nbeen the two dominant strategies for solving various inverse problems in\nlow-level vision. Typically, those two kinds of methods have their respective\nmerits and drawbacks, e.g., model-based optimization methods are flexible for\nhandling different inverse problems but are usually time-consuming with\nsophisticated priors for the purpose of good performance; in the meanwhile,\ndiscriminative learning methods have fast testing speed but their application\nrange is greatly restricted by the specialized task. Recent works have revealed\nthat, with the aid of variable splitting techniques, denoiser prior can be\nplugged in as a modular part of model-based optimization methods to solve other\ninverse problems (e.g., deblurring). Such an integration induces considerable\nadvantage when the denoiser is obtained via discriminative learning. However,\nthe study of integration with fast discriminative denoiser prior is still\nlacking. To this end, this paper aims to train a set of fast and effective CNN\n(convolutional neural network) denoisers and integrate them into model-based\noptimization method to solve other inverse problems. Experimental results\ndemonstrate that the learned set of denoisers not only achieve promising\nGaussian denoising results but also can be used as prior to deliver good\nperformance for various low-level vision applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 12:30:46 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Gu", "Shuhang", ""], ["Zhang", "Lei", ""]]}, {"id": "1704.03273", "submitter": "Yuchao Dai Dr.", "authors": "Liyuan Pan, Yuchao Dai, Miaomiao Liu and Fatih Porikli", "title": "Simultaneous Stereo Video Deblurring and Scene Flow Estimation", "comments": "Accepted to IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos for outdoor scene often show unpleasant blur effects due to the large\nrelative motion between the camera and the dynamic objects and large depth\nvariations. Existing works typically focus monocular video deblurring. In this\npaper, we propose a novel approach to deblurring from stereo videos. In\nparticular, we exploit the piece-wise planar assumption about the scene and\nleverage the scene flow information to deblur the image. Unlike the existing\napproach [31] which used a pre-computed scene flow, we propose a single\nframework to jointly estimate the scene flow and deblur the image, where the\nmotion cues from scene flow estimation and blur information could reinforce\neach other, and produce superior results than the conventional scene flow\nestimation or stereo deblurring methods. We evaluate our method extensively on\ntwo available datasets and achieve significant improvement in flow estimation\nand removing the blur effect over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 13:04:15 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Pan", "Liyuan", ""], ["Dai", "Yuchao", ""], ["Liu", "Miaomiao", ""], ["Porikli", "Fatih", ""]]}, {"id": "1704.03285", "submitter": "Tae Hyun Kim", "authors": "Tae Hyun Kim, Kyoung Mu Lee, Bernhard Sch\\\"olkopf, Michael Hirsch", "title": "Online Video Deblurring via Dynamic Temporal Blending Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art video deblurring methods are capable of removing non-uniform\nblur caused by unwanted camera shake and/or object motion in dynamic scenes.\nHowever, most existing methods are based on batch processing and thus need\naccess to all recorded frames, rendering them computationally demanding and\ntime consuming and thus limiting their practical use. In contrast, we propose\nan online (sequential) video deblurring method based on a spatio-temporal\nrecurrent network that allows for real-time performance. In particular, we\nintroduce a novel architecture which extends the receptive field while keeping\nthe overall size of the network small to enable fast execution. In doing so,\nour network is able to remove even large blur caused by strong camera shake\nand/or fast moving objects. Furthermore, we propose a novel network layer that\nenforces temporal consistency between consecutive frames by dynamic temporal\nblending which compares and adaptively (at test time) shares features obtained\nat different time steps. We show the superiority of the proposed method in an\nextensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 13:41:50 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Kim", "Tae Hyun", ""], ["Lee", "Kyoung Mu", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1704.03295", "submitter": "Pim Moeskops", "authors": "Pim Moeskops, Max A. Viergever, Adri\\\"enne M. Mendrik, Linda S. de\n  Vries, Manon J.N.L. Benders, Ivana I\\v{s}gum", "title": "Automatic segmentation of MR brain images with a convolutional neural\n  network", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 35(5), 1252-1261 (2016)", "doi": "10.1109/TMI.2016.2548501", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation in MR brain images is important for quantitative\nanalysis in large-scale studies with images acquired at all ages.\n  This paper presents a method for the automatic segmentation of MR brain\nimages into a number of tissue classes using a convolutional neural network. To\nensure that the method obtains accurate segmentation details as well as spatial\nconsistency, the network uses multiple patch sizes and multiple convolution\nkernel sizes to acquire multi-scale information about each voxel. The method is\nnot dependent on explicit features, but learns to recognise the information\nthat is important for the classification based on training data. The method\nrequires a single anatomical MR image only.\n  The segmentation method is applied to five different data sets: coronal\nT2-weighted images of preterm infants acquired at 30 weeks postmenstrual age\n(PMA) and 40 weeks PMA, axial T2- weighted images of preterm infants acquired\nat 40 weeks PMA, axial T1-weighted images of ageing adults acquired at an\naverage age of 70 years, and T1-weighted images of young adults acquired at an\naverage age of 23 years. The method obtained the following average Dice\ncoefficients over all segmented tissue classes for each data set, respectively:\n0.87, 0.82, 0.84, 0.86 and 0.91.\n  The results demonstrate that the method obtains accurate segmentations in all\nfive sets, and hence demonstrates its robustness to differences in age and\nacquisition protocol.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:15:07 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Moeskops", "Pim", ""], ["Viergever", "Max A.", ""], ["Mendrik", "Adri\u00ebnne M.", ""], ["de Vries", "Linda S.", ""], ["Benders", "Manon J. N. L.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1704.03296", "submitter": "Ruth Fong", "authors": "Ruth Fong and Andrea Vedaldi", "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation", "comments": "Final camera-ready paper published at ICCV 2017 (Supplementary\n  materials:\n  http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Fong_Interpretable_Explanations_of_ICCV_2017_supplemental.pdf)", "journal-ref": "Proceedings of the 2017 IEEE International Conference on Computer\n  Vision (ICCV)", "doi": "10.1109/ICCV.2017.371", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:15:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 13:53:21 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 16:03:33 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1704.03373", "submitter": "Yu Liu", "authors": "Yu Liu, Junjie Yan, Wanli Ouyang", "title": "Quality Aware Network for Set to Set Recognition", "comments": "Accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets on the problem of set to set recognition, which learns the\nmetric between two image sets. Images in each set belong to the same identity.\nSince images in a set can be complementary, they hopefully lead to higher\naccuracy in practical applications. However, the quality of each sample cannot\nbe guaranteed, and samples with poor quality will hurt the metric. In this\npaper, the quality aware network (QAN) is proposed to confront this problem,\nwhere the quality of each sample can be automatically learned although such\ninformation is not explicitly provided in the training stage. The network has\ntwo branches, where the first branch extracts appearance feature embedding for\neach sample and the other branch predicts quality score for each sample.\nFeatures and quality scores of all samples in a set are then aggregated to\ngenerate the final feature embedding. We show that the two branches can be\ntrained in an end-to-end manner given only the set-level identity annotation.\nAnalysis on gradient spread of this mechanism indicates that the quality\nlearned by the network is beneficial to set-to-set recognition and simplifies\nthe distribution that the network needs to fit. Experiments on both face\nverification and person re-identification show advantages of the proposed QAN.\nThe source code and network structure can be downloaded at\nhttps://github.com/sciencefans/Quality-Aware-Network.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:47:41 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Liu", "Yu", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1704.03375", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Reconstruction of~3-D Rigid Smooth Curves Moving Free when Two Traceable\n  Points Only are Available", "comments": null, "journal-ref": "Preliminaru version of the paper M.A. K{\\l}opotek: Reconstruction\n  of 3-D rigid smooth curves moving free when two traceable points only are\n  available. Machine Graphics \\& Vision 1(1992)1-2, pp. 392-405", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends previous research in that sense that for orthogonal\nprojections of rigid smooth (true-3D) curves moving totally free it reduces the\nnumber of required traceable points to two only (the best results known so far\nto the author are 3 points from free motion and 2 for motion restricted to\nrotation around a fixed direction and and 2 for motion restricted to influence\nof a homogeneous force field). The method used is exploitation of information\non tangential projections. It discusses also possibility of simplification of\nreconstruction of flat curves moving free for prospective projections.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:48:56 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1704.03379", "submitter": "Pim Moeskops", "authors": "Pim Moeskops, Jelmer M. Wolterink, Bas H.M. van der Velden, Kenneth\n  G.A. Gilhuijs, Tim Leiner, Max A. Viergever, Ivana I\\v{s}gum", "title": "Deep Learning for Multi-Task Medical Image Segmentation in Multiple\n  Modalities", "comments": null, "journal-ref": "Moeskops, P., Wolterink, J.M., van der Velden, B.H.M., Gilhuijs,\n  K.G.A., Leiner, T., Viergever, M.A., I\\v{s}gum, I. Deep learning for\n  multi-task medical image segmentation in multiple modalities. In: MICCAI\n  2016, pp. 478-486", "doi": "10.1007/978-3-319-46723-8_55", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of medical images is an important task for many\nclinical applications. In practice, a wide range of anatomical structures are\nvisualised using different imaging modalities. In this paper, we investigate\nwhether a single convolutional neural network (CNN) can be trained to perform\ndifferent segmentation tasks.\n  A single CNN is trained to segment six tissues in MR brain images, the\npectoral muscle in MR breast images, and the coronary arteries in cardiac CTA.\nThe CNN therefore learns to identify the imaging modality, the visualised\nanatomical structures, and the tissue classes.\n  For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this\ncombined training procedure resulted in a segmentation performance equivalent\nto that of a CNN trained specifically for that task, demonstrating the high\ncapacity of CNN architectures. Hence, a single system could be used in clinical\npractice to automatically perform diverse segmentation tasks without\ntask-specific training.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:52:34 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Moeskops", "Pim", ""], ["Wolterink", "Jelmer M.", ""], ["van der Velden", "Bas H. M.", ""], ["Gilhuijs", "Kenneth G. A.", ""], ["Leiner", "Tim", ""], ["Viergever", "Max A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1704.03414", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta", "title": "A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection", "comments": "CVPR 2017 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we learn an object detector that is invariant to occlusions and\ndeformations? Our current solution is to use a data-driven strategy -- collect\nlarge-scale datasets which have object instances under different conditions.\nThe hope is that the final classifier can use these examples to learn\ninvariances. But is it really possible to see all the occlusions in a dataset?\nWe argue that like categories, occlusions and object deformations also follow a\nlong-tail. Some occlusions and deformations are so rare that they hardly\nhappen; yet we want to learn a model invariant to such occurrences. In this\npaper, we propose an alternative solution. We propose to learn an adversarial\nnetwork that generates examples with occlusions and deformations. The goal of\nthe adversary is to generate examples that are difficult for the object\ndetector to classify. In our framework both the original detector and adversary\nare learned in a joint manner. Our experimental results indicate a 2.3% mAP\nboost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge\ncompared to the Fast-RCNN pipeline. We also release the code for this paper.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:57:52 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Wang", "Xiaolong", ""], ["Shrivastava", "Abhinav", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1704.03432", "submitter": "Yu-Wei Chao", "authors": "Yu-Wei Chao, Jimei Yang, Brian Price, Scott Cohen, Jia Deng", "title": "Forecasting Human Dynamics from Static Images", "comments": "Accepted in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first study on forecasting human dynamics from static\nimages. The problem is to input a single RGB image and generate a sequence of\nupcoming human body poses in 3D. To address the problem, we propose the 3D Pose\nForecasting Network (3D-PFNet). Our 3D-PFNet integrates recent advances on\nsingle-image human pose estimation and sequence prediction, and converts the 2D\npredictions into 3D space. We train our 3D-PFNet using a three-step training\nstrategy to leverage a diverse source of training data, including image and\nvideo based human pose datasets and 3D motion capture (MoCap) data. We\ndemonstrate competitive performance of our 3D-PFNet on 2D pose forecasting and\n3D pose recovery through quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 17:20:06 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Chao", "Yu-Wei", ""], ["Yang", "Jimei", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Deng", "Jia", ""]]}, {"id": "1704.03443", "submitter": "Majid Mohammadi", "authors": "Majid Mohammadi, Wout Hofman, Yaohua Tan and S. Hamid Mousavi", "title": "Solving the L1 regularized least square problem via a box-constrained\n  smooth minimization", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an equivalent smooth minimization for the L1 regularized least\nsquare problem is proposed. The proposed problem is a convex box-constrained\nsmooth minimization which allows applying fast optimization methods to find its\nsolution. Further, it is investigated that the property \"the dual of dual is\nprimal\" holds for the L1 regularized least square problem. A solver for the\nsmooth problem is proposed, and its affinity to the proximal gradient is shown.\nFinally, the experiments on L1 and total variation regularized problems are\nperformed, and the corresponding results are reported.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 17:41:24 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 08:04:06 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Mohammadi", "Majid", ""], ["Hofman", "Wout", ""], ["Tan", "Yaohua", ""], ["Mousavi", "S. Hamid", ""]]}, {"id": "1704.03470", "submitter": "Liwei Wang", "authors": "Liwei Wang, Yin Li, Jing Huang, Svetlana Lazebnik", "title": "Learning Two-Branch Neural Networks for Image-Text Matching Tasks", "comments": "accepted version in TPAMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-language matching tasks have recently attracted a lot of attention in\nthe computer vision field. These tasks include image-sentence matching, i.e.,\ngiven an image query, retrieving relevant sentences and vice versa, and\nregion-phrase matching or visual grounding, i.e., matching a phrase to relevant\nregions. This paper investigates two-branch neural networks for learning the\nsimilarity between these two data modalities. We propose two network structures\nthat produce different output representations. The first one, referred to as an\nembedding network, learns an explicit shared latent embedding space with a\nmaximum-margin ranking loss and novel neighborhood constraints. Compared to\nstandard triplet sampling, we perform improved neighborhood sampling that takes\nneighborhood information into consideration while constructing mini-batches.\nThe second network structure, referred to as a similarity network, fuses the\ntwo branches via element-wise product and is trained with regression loss to\ndirectly predict a similarity score. Extensive experiments show that our\nnetworks achieve high accuracies for phrase localization on the Flickr30K\nEntities dataset and for bi-directional image-sentence retrieval on Flickr30K\nand MSCOCO datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:00:25 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 02:33:17 GMT"}, {"version": "v3", "created": "Thu, 28 Dec 2017 09:18:06 GMT"}, {"version": "v4", "created": "Tue, 1 May 2018 18:21:59 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Wang", "Liwei", ""], ["Li", "Yin", ""], ["Huang", "Jing", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1704.03488", "submitter": "Tim Meinhardt", "authors": "Tim Meinhardt, Michael Moeller, Caner Hazirbas and Daniel Cremers", "title": "Learning Proximal Operators: Using Denoising Networks for Regularizing\n  Inverse Imaging Problems", "comments": null, "journal-ref": null, "doi": "10.1109/ICCV.2017.198", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While variational methods have been among the most powerful tools for solving\nlinear inverse problems in imaging, deep (convolutional) neural networks have\nrecently taken the lead in many challenging benchmarks. A remaining drawback of\ndeep learning approaches is their requirement for an expensive retraining\nwhenever the specific problem, the noise level, noise type, or desired measure\nof fidelity changes. On the contrary, variational methods have a plug-and-play\nnature as they usually consist of separate data fidelity and regularization\nterms.\n  In this paper we study the possibility of replacing the proximal operator of\nthe regularization used in many convex energy minimization algorithms by a\ndenoising neural network. The latter therefore serves as an implicit natural\nimage prior, while the data term can still be chosen independently. Using a\nfixed denoising neural network in exemplary problems of image deconvolution\nwith different blur kernels and image demosaicking, we obtain state-of-the-art\nreconstruction results. These indicate the high generalizability of our\napproach and a reduction of the need for problem-specific training.\nAdditionally, we discuss novel results on the analysis of possible optimization\nalgorithms to incorporate the network into, as well as the choices of algorithm\nparameters and their relation to the noise level the neural network is trained\non.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:33:51 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 11:22:33 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Meinhardt", "Tim", ""], ["Moeller", "Michael", ""], ["Hazirbas", "Caner", ""], ["Cremers", "Daniel", ""]]}, {"id": "1704.03489", "submitter": "Keisuke Tateno", "authors": "Keisuke Tateno, Federico Tombari, Iro Laina, Nassir Navab", "title": "CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction", "comments": "10 pages, 6 figures, IEEE Computer Society Conference on Computer\n  Vision and Pattern Recognition (CVPR), Hawaii, USA, June, 2017. The first two\n  authors contribute equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the recent advances in depth prediction from Convolutional Neural\nNetworks (CNNs), this paper investigates how predicted depth maps from a deep\nneural network can be deployed for accurate and dense monocular reconstruction.\nWe propose a method where CNN-predicted dense depth maps are naturally fused\ntogether with depth measurements obtained from direct monocular SLAM. Our\nfusion scheme privileges depth prediction in image locations where monocular\nSLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa.\nWe demonstrate the use of depth prediction for estimating the absolute scale of\nthe reconstruction, hence overcoming one of the major limitations of monocular\nSLAM. Finally, we propose a framework to efficiently fuse semantic labels,\nobtained from a single frame, with dense SLAM, yielding semantically coherent\nscene reconstruction from a single view. Evaluation results on two benchmark\ndatasets show the robustness and accuracy of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:37:11 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Tateno", "Keisuke", ""], ["Tombari", "Federico", ""], ["Laina", "Iro", ""], ["Navab", "Nassir", ""]]}, {"id": "1704.03493", "submitter": "Ziyu Zhang", "authors": "Unnat Jain, Ziyu Zhang, Alexander Schwing", "title": "Creativity: Generating Diverse Questions using Variational Autoencoders", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating diverse questions for given images is an important task for\ncomputational education, entertainment and AI assistants. Different from many\nconventional prediction techniques is the need for algorithms to generate a\ndiverse set of plausible questions, which we refer to as \"creativity\". In this\npaper we propose a creative algorithm for visual question generation which\ncombines the advantages of variational autoencoders with long short-term memory\nnetworks. We demonstrate that our framework is able to generate a large set of\nvarying questions given a single input image.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:46:58 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Jain", "Unnat", ""], ["Zhang", "Ziyu", ""], ["Schwing", "Alexander", ""]]}, {"id": "1704.03503", "submitter": "Yi Zhu", "authors": "Yi Zhu, Shawn Newsam, Zaikun Xu", "title": "UC Merced Submission to the ActivityNet Challenge 2016", "comments": "Notebook paper for ActivityNet 2016 challenge, untrimmed video\n  classification track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper describes our system for the untrimmed classification\ntask in the ActivityNet challenge 2016. We investigate multiple\nstate-of-the-art approaches for action recognition in long, untrimmed videos.\nWe exploit hand-crafted motion boundary histogram features as well feature\nactivations from deep networks such as VGG16, GoogLeNet, and C3D. These\nfeatures are separately fed to linear, one-versus-rest support vector machine\nclassifiers to produce confidence scores for each action class. These\npredictions are then fused along with the softmax scores of the recent\nultra-deep ResNet-101 using weighted averaging.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 19:11:36 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""], ["Xu", "Zaikun", ""]]}, {"id": "1704.03527", "submitter": "Nhien-An Le-Khac", "authors": "V-H Cao, K-X Chu, Nhien-An Le-Khac, M-T Kechadi, Debra F. Laefer, Linh\n  Truong-Hong", "title": "Toward a new approach for massive LiDAR data processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laser scanning (also known as Light Detection And Ranging) has been widely\napplied in various application. As part of that, aerial laser scanning (ALS)\nhas been used to collect topographic data points for a large area, which\ntriggers to million points to be acquired. Furthermore, today, with integrating\nfull wareform (FWF) technology during ALS data acquisition, all return\ninformation of laser pulse is stored. Thus, ALS data are to be massive and\ncomplexity since the FWF of each laser pulse can be stored up to 256 samples\nand density of ALS data is also increasing significantly. Processing LiDAR data\ndemands heavy operations and the traditional approaches require significant\nhardware and running time. On the other hand, researchers have recently\nproposed parallel approaches for analysing LiDAR data. These approaches are\nnormally based on parallel architecture of target systems such as multi-core\nprocessors, GPU, etc. However, there is still missing efficient\napproaches/tools supporting the analysis of LiDAR data due to the lack of a\ndeep study on both library tools and algorithms used in processing this data.\nIn this paper, we present a comparative study of software libraries and\nalgorithms to optimise the processing of LiDAR data. We also propose new method\nto improve this process with experiments on large LiDAR data. Finally, we\ndiscuss on a parallel solution of our approach where we integrate parallel\ncomputing in processing LiDAR data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 20:33:28 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cao", "V-H", ""], ["Chu", "K-X", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-T", ""], ["Laefer", "Debra F.", ""], ["Truong-Hong", "Linh", ""]]}, {"id": "1704.03530", "submitter": "Nhien-An Le-Khac", "authors": "Nhien-An Le-Khac, M-Tahar Kechadi, Bo Wu, C. Chen", "title": "Feature Selection Parallel Technique for Remotely Sensed Imagery\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing research focusing on feature selection has long attracted the\nattention of the remote sensing community because feature selection is a\nprerequisite for image processing and various applications. Different feature\nselection methods have been proposed to improve the classification accuracy.\nThey vary from basic search techniques to clonal selections, and various\noptimal criteria have been investigated. Recently, methods using\ndependence-based measures have attracted much attention due to their ability to\ndeal with very high dimensional datasets. However, these methods are based on\nCramers V test, which has performance issues with large datasets. In this\npaper, we propose a parallel approach to improve their performance. We evaluate\nour approach on hyper-spectral and high spatial resolution images and compare\nit to the proposed methods with a centralized version as preliminary results.\nThe results are very promising.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 20:44:10 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""], ["Wu", "Bo", ""], ["Chen", "C.", ""]]}, {"id": "1704.03533", "submitter": "Samaneh Azadi", "authors": "Samaneh Azadi, Jiashi Feng, and Trevor Darrell", "title": "Learning Detection with Diverse Proposals", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To predict a set of diverse and informative proposals with enriched\nrepresentations, this paper introduces a differentiable Determinantal Point\nProcess (DPP) layer that is able to augment the object detection architectures.\nMost modern object detection architectures, such as Faster R-CNN, learn to\nlocalize objects by minimizing deviations from the ground-truth but ignore\ncorrelation between multiple proposals and object categories. Non-Maximum\nSuppression (NMS) as a widely used proposal pruning scheme ignores label- and\ninstance-level relations between object candidates resulting in multi-labeled\ndetections. In the multi-class case, NMS selects boxes with the largest\nprediction scores ignoring the semantic relation between categories of\npotential election. In contrast, our trainable DPP layer, allowing for Learning\nDetection with Diverse Proposals (LDDP), considers both label-level contextual\ninformation and spatial layout relationships between proposals without\nincreasing the number of parameters of the network, and thus improves location\nand category specifications of final detected bounding boxes substantially\nduring both training and inference schemes. Furthermore, we show that LDDP\nkeeps it superiority over Faster R-CNN even if the number of proposals\ngenerated by LDPP is only ~30% as many as those for Faster R-CNN.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 20:51:40 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Azadi", "Samaneh", ""], ["Feng", "Jiashi", ""], ["Darrell", "Trevor", ""]]}, {"id": "1704.03549", "submitter": "Zbigniew Wojna", "authors": "Zbigniew Wojna, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\n  Yeqing Li, Julian Ibarz", "title": "Attention-based Extraction of Structured Information from Street View\n  Imagery", "comments": "Updated references, added link to the source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network model - based on CNNs, RNNs and a novel attention\nmechanism - which achieves 84.2% accuracy on the challenging French Street Name\nSigns (FSNS) dataset, significantly outperforming the previous state of the art\n(Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler\nand more general than the previous approach. To demonstrate the generality of\nour model, we show that it also performs well on an even more challenging\ndataset derived from Google Street View, in which the goal is to extract\nbusiness names from store fronts. Finally, we study the speed/accuracy tradeoff\nthat results from using CNN feature extractors of different depths.\nSurprisingly, we find that deeper is not always better (in terms of accuracy,\nas well as speed). Our resulting model is simple, accurate and fast, allowing\nit to be used at scale on a variety of challenging real-world text extraction\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 21:55:19 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 03:14:24 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 00:14:33 GMT"}, {"version": "v4", "created": "Sun, 20 Aug 2017 11:34:31 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Wojna", "Zbigniew", ""], ["Gorban", "Alex", ""], ["Lee", "Dar-Shyang", ""], ["Murphy", "Kevin", ""], ["Yu", "Qian", ""], ["Li", "Yeqing", ""], ["Ibarz", "Julian", ""]]}, {"id": "1704.03557", "submitter": "Andreas K\\\"olsch", "authors": "Muhammad Zeshan Afzal, Andreas K\\\"olsch, Sheraz Ahmed, Marcus Liwicki", "title": "Cutting the Error by Half: Investigation of Very Deep CNN and Advanced\n  Training Strategies for Document Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/ICDAR.2017.149", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an exhaustive investigation of recent Deep Learning architectures,\nalgorithms, and strategies for the task of document image classification to\nfinally reduce the error by more than half. Existing approaches, such as the\nDeepDocClassifier, apply standard Convolutional Network architectures with\ntransfer learning from the object recognition domain. The contribution of the\npaper is threefold: First, it investigates recently introduced very deep neural\nnetwork architectures (GoogLeNet, VGG, ResNet) using transfer learning (from\nreal images). Second, it proposes transfer learning from a huge set of document\nimages, i.e. 400,000 documents. Third, it analyzes the impact of the amount of\ntraining data (document images) and other parameters to the classification\nabilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP\ndataset. We achieve an accuracy of 91.13% for the Tobacco-3482 dataset while\nearlier approaches reach only 77.6%. Thus, a relative error reduction of more\nthan 60% is achieved. For the large dataset RVL-CDIP, an accuracy of 90.97% is\nachieved, corresponding to a relative error reduction of 11.5%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 22:35:58 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Afzal", "Muhammad Zeshan", ""], ["K\u00f6lsch", "Andreas", ""], ["Ahmed", "Sheraz", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1704.03568", "submitter": "Christopher Funk", "authors": "Christopher Funk and Yanxi Liu", "title": "Beyond Planar Symmetry: Modeling human perception of reflection and\n  rotation symmetries in the wild", "comments": "To appear in the International Conference on Computer Vision (ICCV)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans take advantage of real world symmetries for various tasks, yet\ncapturing their superb symmetry perception mechanism with a computational model\nremains elusive. Motivated by a new study demonstrating the extremely high\ninter-person accuracy of human perceived symmetries in the wild, we have\nconstructed the first deep-learning neural network for reflection and rotation\nsymmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common\nObject in COntext) dataset with nearly 11K consistent symmetry-labels from more\nthan 400 human observers. We employ novel methods to convert discrete human\nlabels into symmetry heatmaps, capture symmetry densely in an image and\nquantitatively evaluate Sym-NET against multiple existing computer vision\nalgorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO\nphotos, Sym-NET significantly outperforms all other competitors. Beyond\nmathematically well-defined symmetries on a plane, Sym-NET demonstrates\nabilities to identify viewpoint-varied 3D symmetries, partially occluded\nsymmetrical objects, and symmetries at a semantic level.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 23:25:25 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 17:11:05 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Funk", "Christopher", ""], ["Liu", "Yanxi", ""]]}, {"id": "1704.03593", "submitter": "Kha Gia Quach", "authors": "Ngan Le, Kha Gia Quach, Khoa Luu, Marios Savvides, Chenchen Zhu", "title": "Reformulating Level Sets as Deep Recurrent Neural Network Approach to\n  Semantic Segmentation", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Level Set (LS) has been a widely used method in medical\nsegmentation. However, it is limited when dealing with multi-instance objects\nin the real world. In addition, its segmentation results are quite sensitive to\ninitial settings and highly depend on the number of iterations. To address\nthese issues and boost the classic variational LS methods to a new level of the\nlearnable deep learning approaches, we propose a novel definition of contour\nevolution named Recurrent Level Set (RLS)} to employ Gated Recurrent Unit under\nthe energy minimization of a variational LS functional. The curve deformation\nprocess in RLS is formed as a hidden state evolution procedure and updated by\nminimizing an energy functional composed of fitting forces and contour length.\nBy sharing the convolutional features in a fully end-to-end trainable\nframework, we extend RLS to Contextual RLS (CRLS) to address semantic\nsegmentation in the wild. The experimental results have shown that our proposed\nRLS improves both computational time and segmentation accuracy against the\nclassic variations LS-based method, whereas the fully end-to-end system CRLS\nachieves competitive performance compared to the state-of-the-art semantic\nsegmentation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 01:51:52 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Le", "Ngan", ""], ["Quach", "Kha Gia", ""], ["Luu", "Khoa", ""], ["Savvides", "Marios", ""], ["Zhu", "Chenchen", ""]]}, {"id": "1704.03594", "submitter": "Chi Nhan Duong", "authors": "T. Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Marios\n  Savvides, Dipan Pal", "title": "Deep Contextual Recurrent Residual Networks for Scene Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designed as extremely deep architectures, deep residual networks which\nprovide a rich visual representation and offer robust convergence behaviors\nhave recently achieved exceptional performance in numerous computer vision\nproblems. Being directly applied to a scene labeling problem, however, they\nwere limited to capture long-range contextual dependence, which is a critical\naspect. To address this issue, we propose a novel approach, Contextual\nRecurrent Residual Networks (CRRN) which is able to simultaneously handle rich\nvisual representation learning and long-range context modeling within a fully\nend-to-end deep network. Furthermore, our proposed end-to-end CRRN is\ncompletely trained from scratch, without using any pre-trained models in\ncontrast to most existing methods usually fine-tuned from the state-of-the-art\npre-trained models, e.g. VGG-16, ResNet, etc. The experiments are conducted on\nfour challenging scene labeling datasets, i.e. SiftFlow, CamVid, Stanford\nbackground and SUN datasets, and compared against various state-of-the-art\nscene labeling methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 01:52:06 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Le", "T. Hoang Ngan", ""], ["Duong", "Chi Nhan", ""], ["Han", "Ligong", ""], ["Luu", "Khoa", ""], ["Savvides", "Marios", ""], ["Pal", "Dipan", ""]]}, {"id": "1704.03604", "submitter": "Guanbin Li", "authors": "Guanbin Li, Yuan Xie, Liang Lin, Yizhou Yu", "title": "Instance-Level Salient Object Segmentation", "comments": "To appear in CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image saliency detection has recently witnessed rapid progress due to deep\nconvolutional neural networks. However, none of the existing methods is able to\nidentify object instances in the detected salient regions. In this paper, we\npresent a salient instance segmentation method that produces a saliency mask\nwith distinct object instance labels for an input image. Our method consists of\nthree steps, estimating saliency map, detecting salient object contours and\nidentifying salient object instances. For the first two steps, we propose a\nmultiscale saliency refinement network, which generates high-quality salient\nregion masks and salient object contours. Once integrated with multiscale\ncombinatorial grouping and a MAP-based subset optimization framework, our\nmethod can generate very promising salient object instance segmentation\nresults. To promote further research and evaluation of salient instance\nsegmentation, we also construct a new database of 1000 images and their\npixelwise salient instance annotations. Experimental results demonstrate that\nour proposed method is capable of achieving state-of-the-art performance on all\npublic benchmarks for salient region detection as well as on our new dataset\nfor salient instance segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 03:05:27 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Li", "Guanbin", ""], ["Xie", "Yuan", ""], ["Lin", "Liang", ""], ["Yu", "Yizhou", ""]]}, {"id": "1704.03607", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah and Rainer Stiefelhagen", "title": "Automatic Discovery, Association Estimation and Learning of Semantic\n  Attributes for a Thousand Categories", "comments": "Accepted as a conference paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute-based recognition models, due to their impressive performance and\ntheir ability to generalize well on novel categories, have been widely adopted\nfor many computer vision applications. However, usually both the attribute\nvocabulary and the class-attribute associations have to be provided manually by\ndomain experts or large number of annotators. This is very costly and not\nnecessarily optimal regarding recognition performance, and most importantly, it\nlimits the applicability of attribute-based models to large scale data sets. To\ntackle this problem, we propose an end-to-end unsupervised attribute learning\napproach. We utilize online text corpora to automatically discover a salient\nand discriminative vocabulary that correlates well with the human concept of\nsemantic attributes. Moreover, we propose a deep convolutional model to\noptimize class-attribute associations with a linguistic prior that accounts for\nnoise and missing data in text. In a thorough evaluation on ImageNet, we\ndemonstrate that our model is able to efficiently discover and learn semantic\nattributes at a large scale. Furthermore, we demonstrate that our model\noutperforms the state-of-the-art in zero-shot learning on three data sets:\nImageNet, Animals with Attributes and aPascal/aYahoo. Finally, we enable\nattribute-based learning on ImageNet and will share the attributes and\nassociations for future research.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 03:34:19 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1704.03615", "submitter": "Achal Dave", "authors": "Achal Dave, Olga Russakovsky, Deva Ramanan", "title": "Predictive-Corrective Networks for Action Detection", "comments": "Accepted to CVPR 2017. [v2]: Updated Multi-LSTM mAP on MultiTHUMOS\n  (should be 29.7, was initially reported as 29.6). [Project URL]:\n  http://www.achaldave.com/projects/predictive-corrective/", "journal-ref": null, "doi": "10.1109/CVPR.2017.223", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep feature learning has revolutionized techniques for static-image\nunderstanding, the same does not quite hold for video processing. Architectures\nand optimization techniques used for video are largely based off those for\nstatic images, potentially underutilizing rich video information. In this work,\nwe rethink both the underlying network architecture and the stochastic learning\nparadigm for temporal data. To do so, we draw inspiration from classic theory\non linear dynamic systems for modeling time series. By extending such models to\ninclude nonlinear mappings, we derive a series of novel recurrent neural\nnetworks that sequentially make top-down predictions about the future and then\ncorrect those predictions with bottom-up observations. Predictive-corrective\nnetworks have a number of desirable properties: (1) they can adaptively focus\ncomputation on \"surprising\" frames where predictions require large corrections,\n(2) they simplify learning in that only \"residual-like\" corrective terms need\nto be learned over time and (3) they naturally decorrelate an input data stream\nin a hierarchical fashion, producing a more reliable signal for learning at\neach layer of a network. We provide an extensive analysis of our lightweight\nand interpretable framework, and demonstrate that our model is competitive with\nthe two-stream network on three challenging datasets without the need for\ncomputationally expensive optical flow.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 04:20:35 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 05:13:43 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Dave", "Achal", ""], ["Russakovsky", "Olga", ""], ["Ramanan", "Deva", ""]]}, {"id": "1704.03660", "submitter": "Weidi Xie", "authors": "Davis M. Vigneault, Weidi Xie, David A. Bluemke, and J. Alison Noble", "title": "Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline\n  Optimization", "comments": "Accepted to Functional Imaging and Modeling of the Heart (FIMH) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature tracking Cardiac Magnetic Resonance (CMR) has recently emerged as an\narea of interest for quantification of regional cardiac function from balanced,\nsteady state free precession (SSFP) cine sequences. However, currently\navailable techniques lack full automation, limiting reproducibility. We propose\na fully automated technique whereby a CMR image sequence is first segmented\nwith a deep, fully convolutional neural network (CNN) architecture, and\nquadratic basis splines are fitted simultaneously across all cardiac frames\nusing least squares optimization. Experiments are performed using data from 42\npatients with hypertrophic cardiomyopathy (HCM) and 21 healthy control\nsubjects. In terms of segmentation, we compared state-of-the-art CNN\nframeworks, U-Net and dilated convolution architectures, with and without\ntemporal context, using cross validation with three folds. Performance relative\nto expert manual segmentation was similar across all networks: pixel accuracy\nwas ~97%, intersection-over-union (IoU) across all classes was ~87%, and IoU\nacross foreground classes only was ~85%. Endocardial left ventricular\ncircumferential strain calculated from the proposed pipeline was significantly\ndifferent in control and disease subjects (-25.3% vs -29.1%, p = 0.006), in\nagreement with the current clinical literature.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 08:43:35 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Vigneault", "Davis M.", ""], ["Xie", "Weidi", ""], ["Bluemke", "David A.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1704.03669", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Tim Leiner and Max A. Viergever and Ivana\n  I\\v{s}gum", "title": "Dilated Convolutional Neural Networks for Cardiovascular MR Segmentation\n  in Congenital Heart Disease", "comments": null, "journal-ref": "RAMBO 2016, HVSMR 2016. LNCS 10129. pp. 95-102", "doi": "10.1007/978-3-319-52280-7_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic method using dilated convolutional neural networks\n(CNNs) for segmentation of the myocardium and blood pool in cardiovascular MR\n(CMR) of patients with congenital heart disease (CHD).\n  Ten training and ten test CMR scans cropped to an ROI around the heart were\nprovided in the MICCAI 2016 HVSMR challenge. A dilated CNN with a receptive\nfield of 131x131 voxels was trained for myocardium and blood pool segmentation\nin axial, sagittal and coronal image slices. Performance was evaluated within\nthe HVSMR challenge.\n  Automatic segmentation of the test scans resulted in Dice indices of\n0.80$\\pm$0.06 and 0.93$\\pm$0.02, average distances to boundaries of\n0.96$\\pm$0.31 and 0.89$\\pm$0.24 mm, and Hausdorff distances of 6.13$\\pm$3.76\nand 7.07$\\pm$3.01 mm for the myocardium and blood pool, respectively.\nSegmentation took 41.5$\\pm$14.7 s per scan.\n  In conclusion, dilated CNNs trained on a small set of CMR images of CHD\npatients showing large anatomical variability provide accurate myocardium and\nblood pool segmentations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 09:28:40 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["Viergever", "Max A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1704.03706", "submitter": "Mikko Lauri", "authors": "Mikko Lauri and Simone Frintrop", "title": "Object proposal generation applying the distance dependent Chinese\n  restaurant process", "comments": "To appear at Scandinavian Conference on Image Analysis (SCIA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application domains such as robotics, it is useful to represent the\nuncertainty related to the robot's belief about the state of its environment.\nAlgorithms that only yield a single \"best guess\" as a result are not\nsufficient. In this paper, we propose object proposal generation based on\nnon-parametric Bayesian inference that allows quantification of the likelihood\nof the proposals. We apply Markov chain Monte Carlo to draw samples of image\nsegmentations via the distance dependent Chinese restaurant process. Our method\nachieves state-of-the-art performance on an indoor object discovery data set,\nwhile additionally providing a likelihood term for each proposal. We show that\nthe likelihood term can effectively be used to rank proposals according to\ntheir quality.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 11:15:32 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Lauri", "Mikko", ""], ["Frintrop", "Simone", ""]]}, {"id": "1704.03724", "submitter": "Rolf P. W\\\"urtz", "authors": "Thomas Walther and Rolf P. W\\\"urtz", "title": "Unsupervised Construction of Human Body Models Using Principles of\n  Organic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of a generalizable model of the visual appearance of\nhumans from video data is of major importance for computing systems interacting\nnaturally with their users and others. We propose a step towards automatic\nbehavior understanding by integrating principles of Organic Computing into the\nposture estimation cycle, thereby relegating the need for human intervention\nwhile simultaneously raising the level of system autonomy. The system extracts\ncoherent motion from moving upper bodies and autonomously decides about limbs\nand their possible spatial relationships. The models from many videos are\nintegrated into meta-models, which show good generalization to different\nindividuals, backgrounds, and attire. These models allow robust interpretation\nof single video frames without temporal continuity and posture mimicking by an\nandroid robot.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 12:32:20 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Walther", "Thomas", ""], ["W\u00fcrtz", "Rolf P.", ""]]}, {"id": "1704.03743", "submitter": "Giles Tetteh", "authors": "Giles Tetteh, Markus Rempfler, Bjoern H. Menze, Claus Zimmer", "title": "Deep-FExt: Deep Feature Extraction for Vessel Segmentation and\n  Centerline Prediction", "comments": "9 pages", "journal-ref": "Wang Q., Shi Y., Suk HI., Suzuki K. (eds) Machine Learning in\n  Medical Imaging. MLMI 2017. Lecture Notes in Computer Science, vol 10541.\n  Springer, Cham", "doi": "10.1007/978-3-319-67389-9_40", "report-no": "pp 344-352", "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a very crucial task in image and pixel (voxel)\nclassification and regression in biomedical image modeling. In this work we\npresent a machine learning based feature extraction scheme based on inception\nmodels for pixel classification tasks. We extract features under multi-scale\nand multi-layer schemes through convolutional operators. Layers of Fully\nConvolutional Network are later stacked on this feature extraction layers and\ntrained end-to-end for the purpose of classification. We test our model on the\nDRIVE and STARE public data sets for the purpose of segmentation and centerline\ndetection and it out performs most existing hand crafted or deterministic\nfeature schemes found in literature. We achieve an average maximum Dice of 0.85\non the DRIVE data set which out performs the scores from the second human\nannotator of this data set. We also achieve an average maximum Dice of 0.85 and\nkappa of 0.84 on the STARE data set. Though these datasets are mainly 2-D we\nalso propose ways of extending this feature extraction scheme to handle 3-D\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:10:20 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Tetteh", "Giles", ""], ["Rempfler", "Markus", ""], ["Menze", "Bjoern H.", ""], ["Zimmer", "Claus", ""]]}, {"id": "1704.03755", "submitter": "Ronan Sicre", "authors": "Ronan Sicre, Yannis Avrithis, Ewa Kijak, Frederic Jurie", "title": "Unsupervised part learning for visual recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based image classification aims at representing categories by small sets\nof learned discriminative parts, upon which an image representation is built.\nConsidered as a promising avenue a decade ago, this direction has been\nneglected since the advent of deep neural networks. In this context, this paper\nbrings two contributions: first, it shows that despite the recent success of\nend-to-end holistic models, explicit part learning can boosts classification\nperformance. Second, this work proceeds one step further than recent part-based\nmodels (PBM), focusing on how to learn parts without using any labeled data.\nInstead of learning a set of parts per class, as generally done in the PBM\nliterature, the proposed approach both constructs a partition of a given set of\nimages into visually similar groups, and subsequently learn a set of\ndiscriminative parts per group in a fully unsupervised fashion. This strategy\nopens the door to the use of PBM in new applications for which the notion of\nimage categories is irrelevant, such as instance-based image retrieval, for\nexample. We experimentally show that our learned parts can help building\nefficient image representations, for classification as well as for indexing\ntasks, resulting in performance superior to holistic state-of-the art Deep\nConvolutional Neural Networks (DCNN) encoding.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:35:03 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Sicre", "Ronan", ""], ["Avrithis", "Yannis", ""], ["Kijak", "Ewa", ""], ["Jurie", "Frederic", ""]]}, {"id": "1704.03801", "submitter": "Deepti Ameta", "authors": "Deepti Ameta", "title": "Ensemble classifier approach in breast cancer detection and malignancy\n  grading- A review", "comments": "10 pages,1 figure,5 tables", "journal-ref": "International Journal of Managing Public Sector Information and\n  Communication Technologies (IJMPICT) Vol. 8, No. 1, March 2017", "doi": "10.5121/ijmpict.2017.8102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosed cases of Breast cancer is increasing annually and unfortunately\ngetting converted into a high mortality rate. Cancer, at the early stages, is\nhard to detect because the malicious cells show similar properties (density) as\nshown by the non-malicious cells. The mortality ratio could have been minimized\nif the breast cancer could have been detected in its early stages. But the\ncurrent systems have not been able to achieve a fully automatic system which is\nnot just capable of detecting the breast cancer but also can detect the stage\nof it. Estimation of malignancy grading is important in diagnosing the degree\nof growth of malicious cells as well as in selecting a proper therapy for the\npatient. Therefore, a complete and efficient clinical decision support system\nis proposed which is capable of achieving breast cancer malignancy grading\nscheme very efficiently. The system is based on Image processing and machine\nlearning domains. Classification Imbalance problem, a machine learning problem,\noccurs when instances of one class is much higher than the instances of the\nother class resulting in an inefficient classification of samples and hence a\nbad decision support system. Therefore EUSBoost, ensemble based classifier is\nproposed which is efficient and is able to outperform other classifiers as it\ntakes the benefits of both-boosting algorithm with Random Undersampling\ntechniques. Also comparison of EUSBoost with other techniques is shown in the\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 13:31:31 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Ameta", "Deepti", ""]]}, {"id": "1704.03805", "submitter": "Yibo Hu", "authors": "Yibo Hu, Xiang Wu, Ran He", "title": "Attention-Set based Metric Learning for Video Face Recognition", "comments": "modify for ACPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has made great progress with the development of deep\nlearning. However, video face recognition (VFR) is still an ongoing task due to\nvarious illumination, low-resolution, pose variations and motion blur. Most\nexisting CNN-based VFR methods only obtain a feature vector from a single image\nand simply aggregate the features in a video, which less consider the\ncorrelations of face images in one video. In this paper, we propose a novel\nAttention-Set based Metric Learning (ASML) method to measure the statistical\ncharacteristics of image sets. It is a promising and generalized extension of\nMaximum Mean Discrepancy with memory attention weighting. First, we define an\neffective distance metric on image sets, which explicitly minimizes the\nintra-set distance and maximizes the inter-set distance simultaneously. Second,\ninspired by Neural Turing Machine, a Memory Attention Weighting is proposed to\nadapt set-aware global contents. Then ASML is naturally integrated into CNNs,\nresulting in an end-to-end learning scheme. Our method achieves\nstate-of-the-art performance for the task of video face recognition on the\nthree widely used benchmarks including YouTubeFace, YouTube Celebrities and\nCelebrity-1000.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 15:54:24 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 09:38:24 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 09:25:35 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Hu", "Yibo", ""], ["Wu", "Xiang", ""], ["He", "Ran", ""]]}, {"id": "1704.03822", "submitter": "Wenzhen Yuan", "authors": "Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, Edward Adelson", "title": "Connecting Look and Feel: Associating the visual and tactile properties\n  of physical materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For machines to interact with the physical world, they must understand the\nphysical properties of objects and materials they encounter. We use fabrics as\nan example of a deformable material with a rich set of mechanical properties. A\nthin flexible fabric, when draped, tends to look different from a heavy stiff\nfabric. It also feels different when touched. Using a collection of 118 fabric\nsample, we captured color and depth images of draped fabrics along with tactile\ndata from a high resolution touch sensor. We then sought to associate the\ninformation from vision and touch by jointly training CNNs across the three\nmodalities. Through the CNN, each input, regardless of the modality, generates\nan embedding vector that records the fabric's physical property. By comparing\nthe embeddings, our system is able to look at a fabric image and predict how it\nwill feel, and vice versa. We also show that a system jointly trained on vision\nand touch data can outperform a similar system trained only on visual data when\ntested purely with visual inputs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 16:28:14 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Yuan", "Wenzhen", ""], ["Wang", "Shaoxiong", ""], ["Dong", "Siyuan", ""], ["Adelson", "Edward", ""]]}, {"id": "1704.03847", "submitter": "Nikolay Savinov", "authors": "Timo Hackel, Nikolay Savinov, Lubor Ladicky, Jan D. Wegner, Konrad\n  Schindler, Marc Pollefeys", "title": "Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark", "comments": "Accepted to ISPRS Annals. The benchmark website is available at\n  http://www.semantic3d.net/ . The baseline code is available at\n  https://github.com/nsavinov/semantic3dnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new 3D point cloud classification benchmark data set\nwith over four billion manually labelled points, meant as input for data-hungry\n(deep) learning methods. We also discuss first submissions to the benchmark\nthat use deep convolutional neural networks (CNNs) as a work horse, which\nalready show remarkable performance improvements over state-of-the-art. CNNs\nhave become the de-facto standard for many tasks in computer vision and machine\nlearning like semantic segmentation or object detection in images, but have no\nyet led to a true breakthrough for 3D point cloud labelling tasks due to lack\nof training data. With the massive data set presented in this paper, we aim at\nclosing this data gap to help unleash the full potential of deep learning\nmethods for 3D labelling tasks. Our semantic3D.net data set consists of dense\npoint clouds acquired with static terrestrial laser scanners. It contains 8\nsemantic classes and covers a wide range of urban outdoor scenes: churches,\nstreets, railroad tracks, squares, villages, soccer fields and castles. We\ndescribe our labelling interface and show that our data set provides more dense\nand complete point clouds with much higher overall number of labelled points\ncompared to those already available to the research community. We further\nprovide baseline method descriptions and comparison between methods submitted\nto our online system. We hope semantic3D.net will pave the way for deep\nlearning methods in 3D point cloud labelling to learn richer, more general 3D\nrepresentations, and first submissions after only a few months indicate that\nthis might indeed be the case.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:12:57 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Hackel", "Timo", ""], ["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Wegner", "Jan D.", ""], ["Schindler", "Konrad", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1704.03886", "submitter": "Omar Elgendy", "authors": "Omar A. Elgendy and Stanley H. Chan", "title": "Optimal Threshold Design for Quanta Image Sensor", "comments": "12 pages main paper, and 8 pages supplementary", "journal-ref": null, "doi": "10.1109/TCI.2017.2781185", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quanta Image Sensor (QIS) is a binary imaging device envisioned to be the\nnext generation image sensor after CCD and CMOS. Equipped with a massive number\nof single photon detectors, the sensor has a threshold $q$ above which the\nnumber of arriving photons will trigger a binary response \"1\", or \"0\"\notherwise. Existing methods in the device literature typically assume that\n$q=1$ uniformly. We argue that a spatially varying threshold can significantly\nimprove the signal-to-noise ratio of the reconstructed image. In this paper, we\npresent an optimal threshold design framework. We make two contributions.\nFirst, we derive a set of oracle results to theoretically inform the maximally\nachievable performance. We show that the oracle threshold should match exactly\nwith the underlying pixel intensity. Second, we show that around the oracle\nthreshold there exists a set of thresholds that give asymptotically unbiased\nreconstructions. The asymptotic unbiasedness has a phase transition behavior\nwhich allows us to develop a practical threshold update scheme using a\nbisection method. Experimentally, the new threshold design method achieves\nbetter rate of convergence than existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 18:19:28 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 20:03:08 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Elgendy", "Omar A.", ""], ["Chan", "Stanley H.", ""]]}, {"id": "1704.03895", "submitter": "Siddha Ganju", "authors": "Siddha Ganju and Olga Russakovsky and Abhinav Gupta", "title": "What's in a Question: Using Visual Questions as a Form of Supervision", "comments": "CVPR 2017 Spotlight paper and supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collecting fully annotated image datasets is challenging and expensive. Many\ntypes of weak supervision have been explored: weak manual annotations, web\nsearch results, temporal continuity, ambient sound and others. We focus on one\nparticular unexplored mode: visual questions that are asked about images. The\nkey observation that inspires our work is that the question itself provides\nuseful information about the image (even without the answer being available).\nFor instance, the question \"what is the breed of the dog?\" informs the AI that\nthe animal in the scene is a dog and that there is only one dog present. We\nmake three contributions: (1) providing an extensive qualitative and\nquantitative analysis of the information contained in human visual questions,\n(2) proposing two simple but surprisingly effective modifications to the\nstandard visual question answering models that allow them to make use of weak\nsupervision in the form of unanswered questions associated with images and (3)\ndemonstrating that a simple data augmentation strategy inspired by our insights\nresults in a 7.1% improvement on the standard VQA benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 18:48:15 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ganju", "Siddha", ""], ["Russakovsky", "Olga", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1704.03899", "submitter": "Xiaoyu Wang", "authors": "Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Li-Jia Li", "title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a challenging problem owing to the complexity in\nunderstanding the image content and diverse ways of describing it in natural\nlanguage. Recent advances in deep neural networks have substantially improved\nthe performance of this task. Most state-of-the-art approaches follow an\nencoder-decoder framework, which generates captions using a sequential\nrecurrent prediction model. However, in this paper, we introduce a novel\ndecision-making framework for image captioning. We utilize a \"policy network\"\nand a \"value network\" to collaboratively generate captions. The policy network\nserves as a local guidance by providing the confidence of predicting the next\nword according to the current state. Additionally, the value network serves as\na global and lookahead guidance by evaluating all possible extensions of the\ncurrent state. In essence, it adjusts the goal of predicting the correct words\ntowards the goal of generating captions similar to the ground truth captions.\nWe train both networks using an actor-critic reinforcement learning model, with\na novel reward defined by visual-semantic embedding. Extensive experiments and\nanalyses on the Microsoft COCO dataset show that the proposed framework\noutperforms state-of-the-art approaches across different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 18:55:03 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ren", "Zhou", ""], ["Wang", "Xiaoyu", ""], ["Zhang", "Ning", ""], ["Lv", "Xutao", ""], ["Li", "Li-Jia", ""]]}, {"id": "1704.03915", "submitter": "Wei-Sheng Lai", "authors": "Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang", "title": "Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution", "comments": "This work is accepted in CVPR 2017. The code and datasets are\n  available on http://vllab.ucmerced.edu/wlai24/LapSRN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently demonstrated high-quality\nreconstruction for single-image super-resolution. In this paper, we propose the\nLaplacian Pyramid Super-Resolution Network (LapSRN) to progressively\nreconstruct the sub-band residuals of high-resolution images. At each pyramid\nlevel, our model takes coarse-resolution feature maps as input, predicts the\nhigh-frequency residuals, and uses transposed convolutions for upsampling to\nthe finer level. Our method does not require the bicubic interpolation as the\npre-processing step and thus dramatically reduces the computational complexity.\nWe train the proposed LapSRN with deep supervision using a robust Charbonnier\nloss function and achieve high-quality reconstruction. Furthermore, our network\ngenerates multi-scale predictions in one feed-forward pass through the\nprogressive reconstruction, thereby facilitates resource-aware applications.\nExtensive quantitative and qualitative evaluations on benchmark datasets show\nthat the proposed algorithm performs favorably against the state-of-the-art\nmethods in terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:04:06 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 22:47:12 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lai", "Wei-Sheng", ""], ["Huang", "Jia-Bin", ""], ["Ahuja", "Narendra", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1704.03925", "submitter": "Chong You", "authors": "Chong You and Daniel P. Robinson and Ren\\'e Vidal", "title": "Provable Self-Representation Based Outlier Detection in a Union of\n  Subspaces", "comments": "16 pages. CVPR 2017 spotlight oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision tasks involve processing large amounts of data\ncontaminated by outliers, which need to be detected and rejected. While outlier\ndetection methods based on robust statistics have existed for decades, only\nrecently have methods based on sparse and low-rank representation been\ndeveloped along with guarantees of correct outlier detection when the inliers\nlie in one or more low-dimensional subspaces. This paper proposes a new outlier\ndetection method that combines tools from sparse representation with random\nwalks on a graph. By exploiting the property that data points can be expressed\nas sparse linear combinations of each other, we obtain an asymmetric affinity\nmatrix among data points, which we use to construct a weighted directed graph.\nBy defining a suitable Markov Chain from this graph, we establish a connection\nbetween inliers/outliers and essential/inessential states of the Markov chain,\nwhich allows us to detect outliers by using random walks. We provide a\ntheoretical analysis that justifies the correctness of our method under\ngeometric and connectivity assumptions. Experimental results on image databases\ndemonstrate its superiority with respect to state-of-the-art sparse and\nlow-rank outlier detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:45:48 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["You", "Chong", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1704.03944", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, Honglak\n  Lee", "title": "Discriminative Bimodal Networks for Visual Localization and Detection\n  with Natural Language Queries", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating image regions with text queries has been recently explored as a\nnew way to bridge visual and linguistic representations. A few pioneering\napproaches have been proposed based on recurrent neural language models trained\ngeneratively (e.g., generating captions), but achieving somewhat limited\nlocalization accuracy. To better address natural-language-based visual entity\nlocalization, we propose a discriminative approach. We formulate a\ndiscriminative bimodal neural network (DBNet), which can be trained by a\nclassifier with extensive use of negative samples. Our training objective\nencourages better localization on single images, incorporates text phrases in a\nbroad range, and properly pairs image regions with text phrases into positive\nand negative examples. Experiments on the Visual Genome dataset demonstrate the\nproposed DBNet significantly outperforms previous state-of-the-art methods both\nfor localization on single images and for detection on multiple images. We we\nalso establish an evaluation protocol for natural-language visual detection.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 22:09:36 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 07:22:14 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Zhang", "Yuting", ""], ["Yuan", "Luyao", ""], ["Guo", "Yijie", ""], ["He", "Zhiyuan", ""], ["Huang", "I-An", ""], ["Lee", "Honglak", ""]]}, {"id": "1704.03946", "submitter": "Giorgos Tolias", "authors": "Giorgos Tolias and Ond\\v{r}ej Chum", "title": "Asymmetric Feature Maps with Application to Sketch Based Retrieval", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel concept of asymmetric feature maps (AFM), which allows to\nevaluate multiple kernels between a query and database entries without\nincreasing the memory requirements. To demonstrate the advantages of the AFM\nmethod, we derive a short vector image representation that, due to asymmetric\nfeature maps, supports efficient scale and translation invariant sketch-based\nimage retrieval. Unlike most of the short-code based retrieval systems, the\nproposed method provides the query localization in the retrieved image. The\nefficiency of the search is boosted by approximating a 2D translation search\nvia trigonometric polynomial of scores by 1D projections. The projections are a\nspecial case of AFM. An order of magnitude speed-up is achieved compared to\ntraditional trigonometric polynomials. The results are boosted by an\nimage-based average query expansion, exceeding significantly the state of the\nart on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 22:20:06 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Tolias", "Giorgos", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1704.03952", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Yurong You, Ziyan Wang, Cewu Lu", "title": "Virtual to Real Reinforcement Learning for Autonomous Driving", "comments": null, "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC) 2017\n  (Spotlight)", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is considered as a promising direction for driving\npolicy learning. However, training autonomous driving vehicle with\nreinforcement learning in real environment involves non-affordable\ntrial-and-error. It is more desirable to first train in a virtual environment\nand then transfer to the real environment. In this paper, we propose a novel\nrealistic translation network to make model trained in virtual environment be\nworkable in real world. The proposed network can convert non-realistic virtual\nimage input into a realistic one with similar scene structure. Given realistic\nframes as input, driving policy trained by reinforcement learning can nicely\nadapt to real world driving. Experiments show that our proposed virtual to real\n(VR) reinforcement learning (RL) works pretty well. To our knowledge, this is\nthe first successful case of driving policy trained by reinforcement learning\nthat can adapt to real world driving data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 00:03:40 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 08:09:40 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 16:56:54 GMT"}, {"version": "v4", "created": "Tue, 26 Sep 2017 17:22:04 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Pan", "Xinlei", ""], ["You", "Yurong", ""], ["Wang", "Ziyan", ""], ["Lu", "Cewu", ""]]}, {"id": "1704.03958", "submitter": "Stephen Tierney", "authors": "Stephen Tierney, Yi Guo, Junbin Gao", "title": "Efficient Sparse Subspace Clustering by Nearest Neighbour Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Subspace Clustering (SSC) has been used extensively for subspace\nidentification tasks due to its theoretical guarantees and relative ease of\nimplementation. However SSC has quadratic computation and memory requirements\nwith respect to the number of input data points. This burden has prohibited\nSSCs use for all but the smallest datasets. To overcome this we propose a new\nmethod, k-SSC, that screens out a large number of data points to both reduce\nSSC to linear memory and computational requirements. We provide theoretical\nanalysis for the bounds of success for k-SSC. Our experiments show that k-SSC\nexceeds theoretical expectations and outperforms existing SSC approximations by\nmaintaining the classification performance of SSC. Furthermore in the spirit of\nreproducible research we have publicly released the source code for k-SSC\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 00:48:32 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Tierney", "Stephen", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""]]}, {"id": "1704.03963", "submitter": "Stephen Tierney", "authors": "Stephen Tierney, Junbin Gao, Yi Guo, Zheng Zhang", "title": "Tractable Clustering of Data on the Curve Manifold", "comments": "arXiv admin note: substantial text overlap with arXiv:1601.00732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning it is common to interpret each data point as a vector in\nEuclidean space. However the data may actually be functional i.e.\\ each data\npoint is a function of some variable such as time and the function is\ndiscretely sampled. The naive treatment of functional data as traditional\nmultivariate data can lead to poor performance since the algorithms are\nignoring the correlation in the curvature of each function. In this paper we\npropose a tractable method to cluster functional data or curves by adapting the\nEuclidean Low-Rank Representation (LRR) to the curve manifold. Experimental\nevaluation on synthetic and real data reveals that this method massively\noutperforms prior clustering methods in both speed and accuracy when clustering\nfunctional data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 01:18:41 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Tierney", "Stephen", ""], ["Gao", "Junbin", ""], ["Guo", "Yi", ""], ["Zhang", "Zheng", ""]]}, {"id": "1704.03966", "submitter": "Stephen Tierney", "authors": "Stephen Tierney, Yi Guo, Junbin Gao", "title": "Collaborative Low-Rank Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Collaborative Low-Rank Subspace Clustering. Given\nmultiple observations of a phenomenon we learn a unified representation matrix.\nThis unified matrix incorporates the features from all the observations, thus\nincreasing the discriminative power compared with learning the representation\nmatrix on each observation separately. Experimental evaluation shows that our\nmethod outperforms subspace clustering on separate observations and the state\nof the art collaborative learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 01:41:30 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Tierney", "Stephen", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""]]}, {"id": "1704.03971", "submitter": "Sitao Xiang", "authors": "Sitao Xiang, Hao Li", "title": "On the Effects of Batch and Weight Normalization in Generative\n  Adversarial Networks", "comments": "v3 rejected by NIPS 2017, updated and re-submitted to CVPR 2018. v4:\n  add experiments with ResNet and like to new code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are highly effective unsupervised\nlearning frameworks that can generate very sharp data, even for data such as\nimages with complex, highly multimodal distributions. However GANs are known to\nbe very hard to train, suffering from problems such as mode collapse and\ndisturbing visual artifacts. Batch normalization (BN) techniques have been\nintroduced to address the training. Though BN accelerates the training in the\nbeginning, our experiments show that the use of BN can be unstable and\nnegatively impact the quality of the trained model. The evaluation of BN and\nnumerous other recent schemes for improving GAN training is hindered by the\nlack of an effective objective quality measure for GAN models. To address these\nissues, we first introduce a weight normalization (WN) approach for GAN\ntraining that significantly improves the stability, efficiency and the quality\nof the generated samples. To allow a methodical evaluation, we introduce\nsquared Euclidean reconstruction error on a test set as a new objective\nmeasure, to assess training performance in terms of speed, stability, and\nquality of generated samples. Our experiments with a standard DCGAN\narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)\nindicate that training using WN is generally superior to BN for GANs, achieving\n10% lower mean squared loss for reconstruction and significantly better\nqualitative results than BN. We further demonstrate the stability of WN on a\n21-layer ResNet trained with the CelebA data set. The code for this paper is\navailable at https://github.com/stormraiser/gan-weightnorm-resnet\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:15:28 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 02:28:03 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 07:16:59 GMT"}, {"version": "v4", "created": "Mon, 4 Dec 2017 01:56:42 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Xiang", "Sitao", ""], ["Li", "Hao", ""]]}, {"id": "1704.03986", "submitter": "Ju Yong Chang", "authors": "Ju Yong Chang and Kyoung Mu Lee", "title": "2D-3D Pose Consistency-based Conditional Random Fields for 3D Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the 3D human pose estimation problem in a single RGB\nimage by proposing a conditional random field (CRF) model over 2D poses, in\nwhich the 3D pose is obtained as a byproduct of the inference process. The\nunary term of the proposed CRF model is defined based on a powerful heat-map\nregression network, which has been proposed for 2D human pose estimation. This\nstudy also presents a regression network for lifting the 2D pose to 3D pose and\nproposes the prior term based on the consistency between the estimated 3D pose\nand the 2D pose. To obtain the approximate solution of the proposed CRF model,\nthe N-best strategy is adopted. The proposed inference algorithm can be viewed\nas sequential processes of bottom-up generation of 2D and 3D pose proposals\nfrom the input 2D image based on deep networks and top-down verification of\nsuch proposals by checking their consistencies. To evaluate the proposed\nmethod, we use two large-scale datasets: Human3.6M and HumanEva. Experimental\nresults show that the proposed method achieves the state-of-the-art 3D human\npose estimation performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 03:46:57 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 05:31:47 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1704.04023", "submitter": "Maheen Rashid", "authors": "Maheen Rashid, Xiuye Gu, Yong Jae Lee", "title": "Interspecies Knowledge Transfer for Facial Keypoint Detection", "comments": "CVPR 2017 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for localizing facial keypoints on animals by\ntransferring knowledge gained from human faces. Instead of directly finetuning\na network trained to detect keypoints on human faces to animal faces (which is\nsub-optimal since human and animal faces can look quite different), we propose\nto first adapt the animal images to the pre-trained human detection network by\ncorrecting for the differences in animal and human face shape. We first find\nthe nearest human neighbors for each animal image using an unsupervised shape\nmatching method. We use these matches to train a thin plate spline warping\nnetwork to warp each animal face to look more human-like. The warping network\nis then jointly finetuned with a pre-trained human facial keypoint detection\nnetwork using an animal dataset. We demonstrate state-of-the-art results on\nboth horse and sheep facial keypoint detection, and significant improvement\nover simple finetuning, especially when training data is scarce. Additionally,\nwe present a new dataset with 3717 images with horse face and facial keypoint\nannotations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 07:52:21 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Rashid", "Maheen", ""], ["Gu", "Xiuye", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1704.04037", "submitter": "Xin Tao", "authors": "Xin Tao, Chao Zhou, Xiaoyong Shen, Jue Wang, Jiaya Jia", "title": "Zero-order Reverse Filtering", "comments": "9 pages, submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an unconventional but practically meaningful\nreversibility problem of commonly used image filters. We broadly define filters\nas operations to smooth images or to produce layers via global or local\nalgorithms. And we raise the intriguingly problem if they are reservable to the\nstatus before filtering. To answer it, we present a novel strategy to\nunderstand general filter via contraction mappings on a metric space. A very\nsimple yet effective zero-order algorithm is proposed. It is able to\npractically reverse most filters with low computational cost. We present quite\na few experiments in the paper and supplementary file to thoroughly verify its\nperformance. This method can also be generalized to solve other inverse\nproblems and enables new applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:01:43 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Tao", "Xin", ""], ["Zhou", "Chao", ""], ["Shen", "Xiaoyong", ""], ["Wang", "Jue", ""], ["Jia", "Jiaya", ""]]}, {"id": "1704.04054", "submitter": "Ge Gao", "authors": "Ge Gao, Mikko Lauri, Jianwei Zhang and Simone Frintrop", "title": "Saliency-guided Adaptive Seeding for Supervoxel Segmentation", "comments": "6 pages, accepted to IROS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new saliency-guided method for generating supervoxels in 3D\nspace. Rather than using an evenly distributed spatial seeding procedure, our\nmethod uses visual saliency to guide the process of supervoxel generation. This\nresults in densely distributed, small, and precise supervoxels in salient\nregions which often contain objects, and larger supervoxels in less salient\nregions that often correspond to background. Our approach largely improves the\nquality of the resulting supervoxel segmentation in terms of boundary recall\nand under-segmentation error on publicly available benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:43:09 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 20:02:15 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Gao", "Ge", ""], ["Lauri", "Mikko", ""], ["Zhang", "Jianwei", ""], ["Frintrop", "Simone", ""]]}, {"id": "1704.04055", "submitter": "Dino Ienco", "authors": "Dino Ienco, Raffaele Gaetano, Claire Dupaquier and Pierre Maurel", "title": "Land Cover Classification via Multi-temporal Spatial Data by Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2017.2728698", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, modern earth observation programs produce huge volumes of satellite\nimages time series (SITS) that can be useful to monitor geographical areas\nthrough time. How to efficiently analyze such kind of information is still an\nopen question in the remote sensing field. Recently, deep learning methods\nproved suitable to deal with remote sensing data mainly for scene\nclassification (i.e. Convolutional Neural Networks - CNNs - on single images)\nwhile only very few studies exist involving temporal deep learning approaches\n(i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.\nIn this letter we evaluate the ability of Recurrent Neural Networks, in\nparticular the Long-Short Term Memory (LSTM) model, to perform land cover\nclassification considering multi-temporal spatial data derived from a time\nseries of satellite images. We carried out experiments on two different\ndatasets considering both pixel-based and object-based classification. The\nobtained results show that Recurrent Neural Networks are competitive compared\nto state-of-the-art classifiers, and may outperform classical approaches in\npresence of low represented and/or highly mixed classes. We also show that\nusing the alternative feature representation generated by LSTM can improve the\nperformances of standard classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:47:12 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ienco", "Dino", ""], ["Gaetano", "Raffaele", ""], ["Dupaquier", "Claire", ""], ["Maurel", "Pierre", ""]]}, {"id": "1704.04057", "submitter": "Qiang Wang", "authors": "Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, Weiming Hu", "title": "DCFNet: Discriminant Correlation Filters Network for Visual Tracking", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminant Correlation Filters (DCF) based methods now become a kind of\ndominant approach to online object tracking. The features used in these\nmethods, however, are either based on hand-crafted features like HoGs, or\nconvolutional features trained independently from other tasks like image\nclassification. In this work, we present an end-to-end lightweight network\narchitecture, namely DCFNet, to learn the convolutional features and perform\nthe correlation tracking process simultaneously. Specifically, we treat DCF as\na special correlation filter layer added in a Siamese network, and carefully\nderive the backpropagation through it by defining the network output as the\nprobability heatmap of object location. Since the derivation is still carried\nout in Fourier frequency domain, the efficiency property of DCF is preserved.\nThis enables our tracker to run at more than 60 FPS during test time, while\nachieving a significant accuracy gain compared with KCF using HoGs. Extensive\nevaluations on OTB-2013, OTB-2015, and VOT2015 benchmarks demonstrate that the\nproposed DCFNet tracker is competitive with several state-of-the-art trackers,\nwhile being more compact and much faster.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 10:08:14 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Wang", "Qiang", ""], ["Gao", "Jin", ""], ["Xing", "Junliang", ""], ["Zhang", "Mengdan", ""], ["Hu", "Weiming", ""]]}, {"id": "1704.04081", "submitter": "Vinay Namboodiri", "authors": "Prabuddha Chakraborty and Vinay P. Namboodiri", "title": "Learning to Estimate Pose by Watching Videos", "comments": "11 pages, 8 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a technique for obtaining coarse pose estimation of\nhumans in an image that does not require any manual supervision. While a\ngeneral unsupervised technique would fail to estimate human pose, we suggest\nthat sufficient information about coarse pose can be obtained by observing\nhuman motion in multiple frames. Specifically, we consider obtaining surrogate\nsupervision through videos as a means for obtaining motion based grouping cues.\nWe supplement the method using a basic object detector that detects persons.\nWith just these components we obtain a rough estimate of the human pose.\n  With these samples for training, we train a fully convolutional neural\nnetwork (FCNN)[20] to obtain accurate dense blob based pose estimation. We show\nthat the results obtained are close to the ground-truth and to the results\nobtained using a fully supervised convolutional pose estimation method [31] as\nevaluated on a challenging dataset [15]. This is further validated by\nevaluating the obtained poses using a pose based action recognition method [5].\nIn this setting we outperform the results as obtained using the baseline method\nthat uses a fully supervised pose estimation algorithm and is competitive with\na new baseline created using convolutional pose estimation with full\nsupervision.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 11:54:53 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Chakraborty", "Prabuddha", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1704.04086", "submitter": "Rui Huang", "authors": "Rui Huang, Shu Zhang, Tianyu Li, Ran He", "title": "Beyond Face Rotation: Global and Local Perception GAN for Photorealistic\n  and Identity Preserving Frontal View Synthesis", "comments": "accepted at ICCV 2017, main paper & supplementary material, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic frontal view synthesis from a single face image has a wide\nrange of applications in the field of face recognition. Although data-driven\ndeep learning methods have been proposed to address this problem by seeking\nsolutions from ample face data, this problem is still challenging because it is\nintrinsically ill-posed. This paper proposes a Two-Pathway Generative\nAdversarial Network (TP-GAN) for photorealistic frontal view synthesis by\nsimultaneously perceiving global structures and local details. Four landmark\nlocated patch networks are proposed to attend to local textures in addition to\nthe commonly used global encoder-decoder network. Except for the novel\narchitecture, we make this ill-posed problem well constrained by introducing a\ncombination of adversarial loss, symmetry loss and identity preserving loss.\nThe combined loss function leverages both frontal face distribution and\npre-trained discriminative deep face models to guide an identity preserving\ninference of frontal views from profiles. Different from previous deep learning\nmethods that mainly rely on intermediate features for recognition, our method\ndirectly leverages the synthesized identity preserving image for downstream\ntasks like face recognition and attribution estimation. Experimental results\ndemonstrate that our method not only presents compelling perceptual results but\nalso outperforms state-of-the-art results on large pose face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 12:18:13 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 03:44:37 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Huang", "Rui", ""], ["Zhang", "Shu", ""], ["Li", "Tianyu", ""], ["He", "Ran", ""]]}, {"id": "1704.04097", "submitter": "Juan Francisco Mar\\'in", "authors": "Alejandro Cartas, Juan Mar\\'in, Petia Radeva and Mariella Dimiccoli", "title": "Recognizing Activities of Daily Living from Egocentric Images", "comments": "To appear in the Proceedings of IbPRIA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Activities of Daily Living (ADLs) has a large number of health\napplications, such as characterize lifestyle for habit improvement, nursing and\nrehabilitation services. Wearable cameras can daily gather large amounts of\nimage data that provide rich visual information about ADLs than using other\nwearable sensors. In this paper, we explore the classification of ADLs from\nimages captured by low temporal resolution wearable camera (2fpm) by using a\nConvolutional Neural Networks (CNN) approach. We show that the classification\naccuracy of a CNN largely improves when its output is combined, through a\nrandom decision forest, with contextual information from a fully connected\nlayer. The proposed method was tested on a subset of the NTCIR-12 egocentric\ndataset, consisting of 18,674 images and achieved an overall accuracy of 86%\nactivity recognition on 21 classes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 12:49:28 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Cartas", "Alejandro", ""], ["Mar\u00edn", "Juan", ""], ["Radeva", "Petia", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1704.04126", "submitter": "Crist\\'ov\\~ao Cruz", "authors": "Crist\\'ov\\~ao Cruz, Rakesh Mehta, Vladimir Katkovnik, Karen Egiazarian", "title": "Single Image Super-Resolution based on Wiener Filter in Similarity\n  Domain", "comments": "Paper accepted for publication on IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2017.2779265", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super resolution (SISR) is an ill-posed problem aiming at\nestimating a plausible high resolution (HR) image from a single low resolution\n(LR) image. Current state-of-the-art SISR methods are patch-based. They use\neither external data or internal self-similarity to learn a prior for a HR\nimage. External data based methods utilize large number of patches from the\ntraining data, while self-similarity based approaches leverage one or more\nsimilar patches from the input image. In this paper we propose a\nself-similarity based approach that is able to use large groups of similar\npatches extracted from the input image to solve the SISR problem. We introduce\na novel prior leading to collaborative filtering of patch groups in 1D\nsimilarity domain and couple it with an iterative back-projection framework.\nThe performance of the proposed algorithm is evaluated on a number of SISR\nbenchmark datasets. Without using any external data, the proposed approach\noutperforms the current non-CNN based methods on the tested datasets for\nvarious scaling factors. On certain datasets, the gain is over 1 dB, when\ncompared to the recent method A+. For high sampling rate (x4) the proposed\nmethod performs similarly to very recent state-of-the-art deep convolutional\nnetwork based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:38:36 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 06:31:45 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 14:08:09 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Cruz", "Crist\u00f3v\u00e3o", ""], ["Mehta", "Rakesh", ""], ["Katkovnik", "Vladimir", ""], ["Egiazarian", "Karen", ""]]}, {"id": "1704.04131", "submitter": "Zhixin Shu", "authors": "Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli\n  Shechtman, Dimitris Samaras", "title": "Neural Face Editing with Intrinsic Image Disentangling", "comments": "CVPR 2017 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional face editing methods often require a number of sophisticated and\ntask specific algorithms to be applied one after the other --- a process that\nis tedious, fragile, and computationally intensive. In this paper, we propose\nan end-to-end generative adversarial network that infers a face-specific\ndisentangled representation of intrinsic face properties, including shape (i.e.\nnormals), albedo, and lighting, and an alpha matte. We show that this network\ncan be trained on \"in-the-wild\" images by incorporating an in-network\nphysically-based image formation module and appropriate loss functions. Our\ndisentangling latent representation allows for semantically relevant edits,\nwhere one aspect of facial appearance can be manipulated while keeping\northogonal properties fixed, and we demonstrate its use for a number of facial\nediting applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:42:20 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Shu", "Zhixin", ""], ["Yumer", "Ersin", ""], ["Hadap", "Sunil", ""], ["Sunkavalli", "Kalyan", ""], ["Shechtman", "Eli", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1704.04133", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Alexander Wong, Graham W. Taylor", "title": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR)\n  Approach to Understanding Deep Neural Networks", "comments": "Accepted at Computer Vision and Patter Recognition Workshop (CVPR-W)\n  on Explainable Computer Vision, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an\napproach to visualize and understand the decisions made by deep neural networks\n(DNNs) given a specific input. CLEAR facilitates the visualization of attentive\nregions and levels of interest of DNNs during the decision-making process. It\nalso enables the visualization of the most dominant classes associated with\nthese attentive regions of interest. As such, CLEAR can mitigate some of the\nshortcomings of heatmap-based methods associated with decision ambiguity, and\nallows for better insights into the decision-making process of DNNs.\nQuantitative and qualitative experiments across three different datasets\ndemonstrate the efficacy of CLEAR for gaining a better understanding of the\ninner workings of DNNs during the decision-making process.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:44:33 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 18:38:06 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Kumar", "Devinder", ""], ["Wong", "Alexander", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1704.04141", "submitter": "Xin Sun", "authors": "Junyu Dong and Lina Wang and Jun Liu and Xin Sun", "title": "A Procedural Texture Generation Framework Based on Semantic Descriptions", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural textures are normally generated from mathematical models with\nparameters carefully selected by experienced users. However, for naive users,\nthe intuitive way to obtain a desired texture is to provide semantic\ndescriptions such as \"regular,\" \"lacelike,\" and \"repetitive\" and then a\nprocedural model with proper parameters will be automatically suggested to\ngenerate the corresponding textures. By contrast, it is less practical for\nusers to learn mathematical models and tune parameters based on multiple\nexaminations of large numbers of generated textures. In this study, we propose\na novel framework that generates procedural textures according to user-defined\nsemantic descriptions, and we establish a mapping between procedural models and\nsemantic texture descriptions. First, based on a vocabulary of semantic\nattributes collected from psychophysical experiments, a multi-label learning\nmethod is employed to annotate a large number of textures with semantic\nattributes to form a semantic procedural texture dataset. Then, we derive a low\ndimensional semantic space in which the semantic descriptions can be separated\nfrom one other. Finally, given a set of semantic descriptions, the diverse\nproperties of the samples in the semantic space can lead the framework to find\nan appropriate generation model that uses appropriate parameters to produce a\ndesired texture. The experimental results show that the proposed framework is\neffective and that the generated textures closely correlate with the input\nsemantic descriptions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 14:02:38 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Dong", "Junyu", ""], ["Wang", "Lina", ""], ["Liu", "Jun", ""], ["Sun", "Xin", ""]]}, {"id": "1704.04186", "submitter": "Silvia-Laura Pintea", "authors": "Yichao Zhang, Silvia L. Pintea, and Jan C. van Gemert", "title": "Video Acceleration Magnification", "comments": "Accepted paper at CVPR 2017. Project webpage:\n  http://acceleration-magnification.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to amplify or reduce subtle image changes over time is useful in\ncontexts such as video editing, medical video analysis, product quality control\nand sports. In these contexts there is often large motion present which\nseverely distorts current video amplification methods that magnify change\nlinearly. In this work we propose a method to cope with large motions while\nstill magnifying small changes. We make the following two observations: i)\nlarge motions are linear on the temporal scale of the small changes; ii) small\nchanges deviate from this linearity. We ignore linear motion and propose to\nmagnify acceleration. Our method is pure Eulerian and does not require any\noptical flow, temporal alignment or region annotations. We link temporal\nsecond-order derivative filtering to spatial acceleration magnification. We\napply our method to moving objects where we show motion magnification and color\nmagnification. We provide quantitative as well as qualitative evidence for our\nmethod while comparing to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 15:55:19 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 19:53:52 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zhang", "Yichao", ""], ["Pintea", "Silvia L.", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "1704.04224", "submitter": "Xinlei Chen", "authors": "Xinlei Chen, Abhinav Gupta", "title": "Spatial Memory for Context Reasoning in Object Detection", "comments": "Draft submitted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling instance-level context and object-object relationships is extremely\nchallenging. It requires reasoning about bounding boxes of different classes,\nlocations \\etc. Above all, instance-level spatial reasoning inherently requires\nmodeling conditional distributions on previous detections. Unfortunately, our\ncurrent object detection systems do not have any {\\bf memory} to remember what\nto condition on! The state-of-the-art object detectors still detect all object\nin parallel followed by non-maximal suppression (NMS). While memory has been\nused for tasks such as captioning, they mostly use image-level memory cells\nwithout capturing the spatial layout. On the other hand, modeling object-object\nrelationships requires {\\bf spatial} reasoning -- not only do we need a memory\nto store the spatial layout, but also a effective reasoning module to extract\nspatial patterns. This paper presents a conceptually simple yet powerful\nsolution -- Spatial Memory Network (SMN), to model the instance-level context\nefficiently and effectively. Our spatial memory essentially assembles object\ninstances back into a pseudo \"image\" representation that is easy to be fed into\nanother ConvNet for object-object context reasoning. This leads to a new\nsequential reasoning architecture where image and memory are processed in\nparallel to obtain detections which update the memory again. We show our SMN\ndirection is promising as it provides 2.2\\% improvement over baseline Faster\nRCNN on the COCO dataset so far.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 17:47:03 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Chen", "Xinlei", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1704.04232", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh and Yong Jae Lee", "title": "Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised\n  Object and Action Localization", "comments": "Camera-Ready Version (ICCV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose `Hide-and-Seek', a weakly-supervised framework that aims to\nimprove object localization in images and action localization in videos. Most\nexisting weakly-supervised methods localize only the most discriminative parts\nof an object rather than all relevant parts, which leads to suboptimal\nperformance. Our key idea is to hide patches in a training image randomly,\nforcing the network to seek other relevant parts when the most discriminative\npart is hidden. Our approach only needs to modify the input image and can work\nwith any network designed for object localization. During testing, we do not\nneed to hide any patches. Our Hide-and-Seek approach obtains superior\nperformance compared to previous methods for weakly-supervised object\nlocalization on the ILSVRC dataset. We also demonstrate that our framework can\nbe easily extended to weakly-supervised action localization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 17:59:31 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 10:43:54 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1704.04235", "submitter": "Xiaofang Wang", "authors": "Lingkun Luo, Xiaofang Wang, Shiqiang Hu, Chao Wang, Yuxing Tang,\n  Liming Chen", "title": "Close Yet Distinctive Domain Adaptation", "comments": "11pages, 3 figures, ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is transfer learning which aims to generalize a learning\nmodel across training and testing data with different distributions. Most\nprevious research tackle this problem in seeking a shared feature\nrepresentation between source and target domains while reducing the mismatch of\ntheir data distributions. In this paper, we propose a close yet discriminative\ndomain adaptation method, namely CDDA, which generates a latent feature\nrepresentation with two interesting properties. First, the discrepancy between\nthe source and target domain, measured in terms of both marginal and\nconditional probability distribution via Maximum Mean Discrepancy is minimized\nso as to attract two domains close to each other. More importantly, we also\ndesign a repulsive force term, which maximizes the distances between each label\ndependent sub-domain to all others so as to drag different class dependent\nsub-domains far away from each other and thereby increase the discriminative\npower of the adapted domain. Moreover, given the fact that the underlying data\nmanifold could have complex geometric structure, we further propose the\nconstraints of label smoothness and geometric structure consistency for label\npropagation. Extensive experiments are conducted on 36 cross-domain image\nclassification tasks over four public datasets. The comprehensive results show\nthat the proposed method consistently outperforms the state-of-the-art methods\nwith significant margins.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 08:30:21 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Luo", "Lingkun", ""], ["Wang", "Xiaofang", ""], ["Hu", "Shiqiang", ""], ["Wang", "Chao", ""], ["Tang", "Yuxing", ""], ["Chen", "Liming", ""]]}, {"id": "1704.04251", "submitter": "Sandipan Banerjee", "authors": "Sandipan Banerjee, James Sweet, Christopher Sweet, Marya Lieberman", "title": "Visual Recognition of Paper Analytical Device Images for Detection of\n  Falsified Pharmaceuticals", "comments": "in Proc. IEEE Winter Conference on Applications of Computer Vision\n  (WACV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falsification of medicines is a big problem in many developing countries,\nwhere technological infrastructure is inadequate to detect these harmful\nproducts. We have developed a set of inexpensive paper cards, called Paper\nAnalytical Devices (PADs), which can efficiently classify drugs based on their\nchemical composition, as a potential solution to the problem. These cards have\ndifferent reagents embedded in them which produce a set of distinctive color\ndescriptors upon reacting with the chemical compounds that constitute\npharmaceutical dosage forms. If a falsified version of the medicine lacks the\nactive ingredient or includes substitute fillers, the difference in color is\nperceivable by humans. However, reading the cards with accuracy takes training\nand practice, which may hamper their scaling and implementation in low resource\nsettings. To deal with this, we have developed an automatic visual recognition\nsystem to read the results from the PAD images. At first, the optimal set of\nreagents was found by running singular value decomposition on the intensity\nvalues of the color tones in the card images. A dataset of cards embedded with\nthese reagents is produced to generate the most distinctive results for a set\nof 26 different active pharmaceutical ingredients (APIs) and excipients. Then,\nwe train two popular convolutional neural network (CNN) models, with the card\nimages. We also extract some \"hand-crafted\" features from the images and train\na nearest neighbor classifier and a non-linear support vector machine with\nthem. On testing, higher-level features performed much better in accurately\nclassifying the PAD images, with the CNN models reaching the highest average\naccuracy of over 94\\%.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 18:35:33 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Sweet", "James", ""], ["Sweet", "Christopher", ""], ["Lieberman", "Marya", ""]]}, {"id": "1704.04281", "submitter": "Qi Liu", "authors": "Qi Liu, Yawen Zhang, Qin Lv, Li Shang", "title": "Applying High-Resolution Visible Imagery to Satellite Melt Pond Fraction\n  Retrieval: A Neural Network Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During summer, melt ponds have a significant influence on Arctic sea-ice\nalbedo. The melt pond fraction (MPF) also has the ability to forecast the\nArctic sea-ice in a certain period. It is important to retrieve accurate melt\npond fraction (MPF) from satellite data for Arctic research. This paper\nproposes a satellite MPF retrieval model based on the multi-layer neural\nnetwork, named MPF-NN. Our model uses multi-spectral satellite data as model\ninput and MPF information from multi-site and multi-period visible imagery as\nprior knowledge for modeling. It can effectively model melt ponds evolution of\ndifferent regions and periods over the Arctic. Evaluation results show that the\nMPF retrieved from MODIS data using the proposed model has an RMSE of 3.91% and\na correlation coefficient of 0.73. The seasonal distribution of MPF is also\nconsistent with previous results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 21:21:57 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Liu", "Qi", ""], ["Zhang", "Yawen", ""], ["Lv", "Qin", ""], ["Shang", "Li", ""]]}, {"id": "1704.04296", "submitter": "Jesse Lieman-Sifry", "authors": "Jesse Lieman-Sifry, Matthieu Le, Felix Lau, Sean Sall, Daniel Golden", "title": "FastVentricle: Cardiac Segmentation with ENet", "comments": "11 pages, 6 figures, Accepted to Functional Imaging and Modeling of\n  the Heart (FIMH) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac Magnetic Resonance (CMR) imaging is commonly used to assess cardiac\nstructure and function. One disadvantage of CMR is that post-processing of\nexams is tedious. Without automation, precise assessment of cardiac function\nvia CMR typically requires an annotator to spend tens of minutes per case\nmanually contouring ventricular structures. Automatic contouring can lower the\nrequired time per patient by generating contour suggestions that can be lightly\nmodified by the annotator. Fully convolutional networks (FCNs), a variant of\nconvolutional neural networks, have been used to rapidly advance the\nstate-of-the-art in automated segmentation, which makes FCNs a natural choice\nfor ventricular segmentation. However, FCNs are limited by their computational\ncost, which increases the monetary cost and degrades the user experience of\nproduction systems. To combat this shortcoming, we have developed the\nFastVentricle architecture, an FCN architecture for ventricular segmentation\nbased on the recently developed ENet architecture. FastVentricle is 4x faster\nand runs with 6x less memory than the previous state-of-the-art ventricular\nsegmentation architecture while still maintaining excellent clinical accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 22:57:54 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Lieman-Sifry", "Jesse", ""], ["Le", "Matthieu", ""], ["Lau", "Felix", ""], ["Sall", "Sean", ""], ["Golden", "Daniel", ""]]}, {"id": "1704.04313", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Philippe Degen, Luca Benini", "title": "CBinfer: Change-Based Inference for Convolutional Neural Networks on\n  Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.PF eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting per-frame features using convolutional neural networks for\nreal-time processing of video data is currently mainly performed on powerful\nGPU-accelerated workstations and compute clusters. However, there are many\napplications such as smart surveillance cameras that require or would benefit\nfrom on-site processing. To this end, we propose and evaluate a novel algorithm\nfor change-based evaluation of CNNs for video data recorded with a static\ncamera setting, exploiting the spatio-temporal sparsity of pixel changes. We\nachieve an average speed-up of 8.6x over a cuDNN baseline on a realistic\nbenchmark with a negligible accuracy loss of less than 0.1% and no retraining\nof the network. The resulting energy efficiency is 10x higher than that of\nper-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1\nplatform.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 00:36:55 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 09:27:14 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Degen", "Philippe", ""], ["Benini", "Luca", ""]]}, {"id": "1704.04326", "submitter": "Daniel Crispell", "authors": "Daniel Crispell, Octavian Biris, Nate Crosswhite, Jeffrey Byrne,\n  Joseph L. Mundy", "title": "Dataset Augmentation for Pose and Lighting Invariant Face Recognition", "comments": "Appeared in 2016 IEEE Applied Imagery Pattern Recognition Workshop\n  (AIPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of modern face recognition systems is a function of the\ndataset on which they are trained. Most datasets are largely biased toward\n\"near-frontal\" views with benign lighting conditions, negatively effecting\nrecognition performance on images that do not meet these criteria. The proposed\napproach demonstrates how a baseline training set can be augmented to increase\npose and lighting variability using semi-synthetic images with simulated pose\nand lighting conditions. The semi-synthetic images are generated using a fast\nand robust 3-d shape estimation and rendering pipeline which includes the full\nhead and background. Various methods of incorporating the semi-synthetic\nrenderings into the training procedure of a state of the art deep neural\nnetwork-based recognition system without modifying the structure of the network\nitself are investigated. Quantitative results are presented on the challenging\nIJB-A identification dataset using a state of the art recognition pipeline as a\nbaseline.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 01:56:35 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Crispell", "Daniel", ""], ["Biris", "Octavian", ""], ["Crosswhite", "Nate", ""], ["Byrne", "Jeffrey", ""], ["Mundy", "Joseph L.", ""]]}, {"id": "1704.04360", "submitter": "Gil Ben-Artzi", "authors": "Gil Ben-Artzi", "title": "Camera Calibration by Global Constraints on the Motion of Silhouettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of epipolar geometry using the motion of silhouettes.\nSuch methods match epipolar lines or frontier points across views, which are\nthen used as the set of putative correspondences. We introduce an approach that\nimproves by two orders of magnitude the performance over state-of-the-art\nmethods, by significantly reducing the number of outliers in the putative\nmatching. We model the frontier points' correspondence problem as constrained\nflow optimization, requiring small differences between their coordinates over\nconsecutive frames. Our approach is formulated as a Linear Integer Program and\nwe show that due to the nature of our problem, it can be solved efficiently in\nan iterative manner. Our method was validated on four standard datasets\nproviding accurate calibrations across very different viewpoints.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 06:09:27 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Ben-Artzi", "Gil", ""]]}, {"id": "1704.04394", "submitter": "Namhoon Lee", "authors": "Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B. Choy, Philip H.\n  S. Torr and Manmohan Chandraker", "title": "DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting\n  Agents", "comments": "Accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Deep Stochastic IOC RNN Encoderdecoder framework, DESIRE, for\nthe task of future predictions of multiple interacting agents in dynamic\nscenes. DESIRE effectively predicts future locations of objects in multiple\nscenes by 1) accounting for the multi-modal nature of the future prediction\n(i.e., given the same context, future may vary), 2) foreseeing the potential\nfuture outcomes and make a strategic prediction based on that, and 3) reasoning\nnot only from the past motion history, but also from the scene context as well\nas the interactions among the agents. DESIRE achieves these in a single\nend-to-end trainable neural network model, while being computationally\nefficient. The model first obtains a diverse set of hypothetical future\nprediction samples employing a conditional variational autoencoder, which are\nranked and refined by the following RNN scoring-regression module. Samples are\nscored by accounting for accumulated future rewards, which enables better\nlong-term strategic decisions similar to IOC frameworks. An RNN scene context\nfusion module jointly captures past motion histories, the semantic scene\ncontext and interactions among multiple agents. A feedback mechanism iterates\nover the ranking and refinement to further boost the prediction accuracy. We\nevaluate our model on two publicly available datasets: KITTI and Stanford Drone\nDataset. Our experiments show that the proposed model significantly improves\nthe prediction accuracy compared to other baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 11:15:44 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Lee", "Namhoon", ""], ["Choi", "Wongun", ""], ["Vernaza", "Paul", ""], ["Choy", "Christopher B.", ""], ["Torr", "Philip H. S.", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1704.04422", "submitter": "Rui Chen", "authors": "Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao", "title": "Learning a collaborative multiscale dictionary based on robust empirical\n  mode decomposition", "comments": "to be published in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is a challenge topic in many image processing areas. The\nbasic goal is to learn a sparse representation from an overcomplete basis set.\nDue to combining the advantages of generic multiscale representations with\nlearning based adaptivity, multiscale dictionary representation approaches have\nthe power in capturing structural characteristics of natural images. However,\nexisting multiscale learning approaches still suffer from three main\nweaknesses: inadaptability to diverse scales of image data, sensitivity to\nnoise and outliers, difficulty to determine optimal dictionary structure. In\nthis paper, we present a novel multiscale dictionary learning paradigm for\nsparse image representations based on an improved empirical mode decomposition.\nThis powerful data-driven analysis tool for multi-dimensional signal can fully\nadaptively decompose the image into multiscale oscillating components according\nto intrinsic modes of data self. This treatment can obtain a robust and\neffective sparse representation, and meanwhile generates a raw base dictionary\nat multiple geometric scales and spatial frequency bands. This dictionary is\nrefined by selecting optimal oscillating atoms based on frequency clustering.\nIn order to further enhance sparsity and generalization, a tolerance dictionary\nis learned using a coherence regularized model. A fast proximal scheme is\ndeveloped to optimize this model. The multiscale dictionary is considered as\nthe product of oscillating dictionary and tolerance dictionary. Experimental\nresults demonstrate that the proposed learning approach has the superior\nperformance in sparse image representations as compared with several competing\nmethods. We also show the promising results in image denoising application.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 02:31:20 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Chen", "Rui", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""], ["Gao", "Wen", ""]]}, {"id": "1704.04428", "submitter": "Aravind Vasudevan", "authors": "Aravind Vasudevan, Andrew Anderson and David Gregg", "title": "Parallel Multi Channel Convolution using General Matrix Multiplication", "comments": "Camera ready version to be published at ASAP 2017 - The 28th Annual\n  IEEE International Conference on Application-specific Systems, Architectures\n  and Processors. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have emerged as one of the most\nsuccessful machine learning technologies for image and video processing. The\nmost computationally intensive parts of CNNs are the convolutional layers,\nwhich convolve multi-channel images with multiple kernels. A common approach to\nimplementing convolutional layers is to expand the image into a column matrix\n(im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using\nan existing parallel General Matrix Multiplication (GEMM) library. This im2col\nconversion greatly increases the memory footprint of the input matrix and\nreduces data locality.\n  In this paper we propose a new approach to MCMK convolution that is based on\nGeneral Matrix Multiplication (GEMM), but not on im2col. Our algorithm\neliminates the need for data replication on the input thereby enabling us to\napply the convolution kernels on the input images directly. We have implemented\nseveral variants of our algorithm on a CPU processor and an embedded ARM\nprocessor. On the CPU, our algorithm is faster than im2col in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 17:09:43 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 12:43:10 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Vasudevan", "Aravind", ""], ["Anderson", "Andrew", ""], ["Gregg", "David", ""]]}, {"id": "1704.04429", "submitter": "Xiao-Yang Liu", "authors": "Ming-Jun Su, Jingbo Chang, Feng Qian, Guangmin Hu, Xiao-Yang Liu", "title": "3D seismic data denoising using two-dimensional sparse coding scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic data denoising is vital to geophysical applications and the\ntransform-based function method is one of the most widely used techniques.\nHowever, it is challenging to design a suit- able sparse representation to\nexpress a transform-based func- tion group due to the complexity of seismic\ndata. In this paper, we apply a seismic data denoising method based on\nlearning- type overcomplete dictionaries which uses two-dimensional sparse\ncoding (2DSC). First, we model the input seismic data and dictionaries as\nthird-order tensors and introduce tensor- linear combinations for data\napproximation. Second, we ap- ply learning-type overcomplete dictionary, i.e.,\noptimal sparse data representation is achieved through learning and training.\nThird, we exploit the alternating minimization algorithm to solve the\noptimization problem of seismic denoising. Finally we evaluate its denoising\nperformance on synthetic seismic data and land data survey. Experiment results\nshow that the two-dimensional sparse coding scheme reduces computational costs\nand enhances the signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 06:13:00 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Su", "Ming-Jun", ""], ["Chang", "Jingbo", ""], ["Qian", "Feng", ""], ["Hu", "Guangmin", ""], ["Liu", "Xiao-Yang", ""]]}, {"id": "1704.04481", "submitter": "Robert Walecki Mr", "authors": "Robert Walecki, Ognjen (Oggi) Rudovic, Vladimir Pavlovic, Bj\\\"orn\n  Schuller and Maja Pantic", "title": "Deep Structured Learning for Facial Action Unit Intensity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of automated estimation of facial expression intensity.\nThis involves estimation of multiple output variables (facial action units ---\nAUs) that are structurally dependent. Their structure arises from statistically\ninduced co-occurrence patterns of AU intensity levels. Modeling this structure\nis critical for improving the estimation performance; however, this performance\nis bounded by the quality of the input features extracted from face images. The\ngoal of this paper is to model these structures and estimate complex feature\nrepresentations simultaneously by combining conditional random field (CRF)\nencoded AU dependencies with deep learning. To this end, we propose a novel\nCopula CNN deep learning approach for modeling multivariate ordinal variables.\nOur model accounts for $ordinal$ structure in output variables and their\n$non$-$linear$ dependencies via copula functions modeled as cliques of a CRF.\nThese are jointly optimized with deep CNN feature encoding layers using a newly\nintroduced balanced batch iterative training algorithm. We demonstrate the\neffectiveness of our approach on the task of AU intensity estimation on two\nbenchmark datasets. We show that joint learning of the deep features and the\ntarget output structure results in significant performance gains compared to\nexisting deep structured models for analysis of facial expressions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 16:51:40 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Walecki", "Robert", "", "Oggi"], ["Ognjen", "", "", "Oggi"], ["Rudovic", "", ""], ["Pavlovic", "Vladimir", ""], ["Schuller", "Bj\u00f6rn", ""], ["Pantic", "Maja", ""]]}, {"id": "1704.04497", "submitter": "Yunseok Jang", "authors": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim", "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering", "comments": "Accepted paper at CVPR 2017 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and language understanding has emerged as a subject undergoing intense\nstudy in Artificial Intelligence. Among many tasks in this line of research,\nvisual question answering (VQA) has been one of the most successful ones, where\nthe goal is to learn a model that understands visual content at region-level\ndetails and finds their associations with pairs of questions and answers in the\nnatural language form. Despite the rapid progress in the past few years, most\nexisting work in VQA have focused primarily on images. In this paper, we focus\non extending VQA to the video domain and contribute to the literature in three\nimportant ways. First, we propose three new tasks designed specifically for\nvideo VQA, which require spatio-temporal reasoning from videos to answer\nquestions correctly. Next, we introduce a new large-scale dataset for video VQA\nnamed TGIF-QA that extends existing VQA work with our new tasks. Finally, we\npropose a dual-LSTM based approach with both spatial and temporal attention,\nand show its effectiveness over conventional VQA techniques through empirical\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 17:57:01 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 20:56:25 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 04:46:42 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Jang", "Yunseok", ""], ["Song", "Yale", ""], ["Yu", "Youngjae", ""], ["Kim", "Youngjin", ""], ["Kim", "Gunhee", ""]]}, {"id": "1704.04503", "submitter": "Bharat Singh", "authors": "Navaneeth Bodla, Bharat Singh, Rama Chellappa, Larry S. Davis", "title": "Soft-NMS -- Improving Object Detection With One Line of Code", "comments": "ICCV 2017 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-maximum suppression is an integral part of the object detection pipeline.\nFirst, it sorts all detection boxes on the basis of their scores. The detection\nbox M with the maximum score is selected and all other detection boxes with a\nsignificant overlap (using a pre-defined threshold) with M are suppressed. This\nprocess is recursively applied on the remaining boxes. As per the design of the\nalgorithm, if an object lies within the predefined overlap threshold, it leads\nto a miss. To this end, we propose Soft-NMS, an algorithm which decays the\ndetection scores of all other objects as a continuous function of their overlap\nwith M. Hence, no object is eliminated in this process. Soft-NMS obtains\nconsistent improvements for the coco-style mAP metric on standard datasets like\nPASCAL VOC 2007 (1.7% for both R-FCN and Faster-RCNN) and MS-COCO (1.3% for\nR-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any\nadditional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves\nstate-of-the-art in object detection from 39.8% to 40.9% with a single model.\nFurther, the computational complexity of Soft-NMS is the same as traditional\nNMS and hence it can be efficiently implemented. Since Soft-NMS does not\nrequire any extra training and is simple to implement, it can be easily\nintegrated into any object detection pipeline. Code for Soft-NMS is publicly\navailable on GitHub (http://bit.ly/2nJLNMu).\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 18:00:03 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 17:49:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Bodla", "Navaneeth", ""], ["Singh", "Bharat", ""], ["Chellappa", "Rama", ""], ["Davis", "Larry S.", ""]]}, {"id": "1704.04511", "submitter": "Arvind Balachandrasekaran", "authors": "Arvind Balachandrasekaran, Vincent Magnotta and Mathews Jacob", "title": "Recovery of damped exponentials using structured low rank matrix\n  completion", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a structured low rank matrix completion algorithm to recover a\nseries of images from their under-sampled measurements, where the signal along\nthe parameter dimension at every pixel is described by a linear combination of\nexponentials. We exploit the exponential behavior of the signal at every pixel,\nalong with the spatial smoothness of the exponential parameters to derive an\nannihilation relation in the Fourier domain. This relation translates to a\nlow-rank property on a structured matrix constructed from the Fourier samples.\nWe enforce the low rank property of the structured matrix as a regularization\nprior to recover the images. Since the direct use of current low rank matrix\nrecovery schemes to this problem is associated with high computational\ncomplexity and memory demand, we adopt an iterative re-weighted least squares\n(IRLS) algorithm, which facilitates the exploitation of the convolutional\nstructure of the matrix. Novel approximations involving two dimensional Fast\nFourier Transforms (FFT) are introduced to drastically reduce the memory demand\nand computational complexity, which facilitates the extension of structured low\nrank methods to large scale three dimensional problems. We demonstrate our\nalgorithm in the MR parameter mapping setting and show improvement over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 18:35:25 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 14:46:30 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Balachandrasekaran", "Arvind", ""], ["Magnotta", "Vincent", ""], ["Jacob", "Mathews", ""]]}, {"id": "1704.04516", "submitter": "Tae Soo Kim", "authors": "Tae Soo Kim, Austin Reiter", "title": "Interpretable 3D Human Action Analysis with Temporal Convolutional\n  Networks", "comments": "8 pages, 5 figures, BNMW CVPR 2017 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discriminative power of modern deep learning models for 3D human action\nrecognition is growing ever so potent. In conjunction with the recent\nresurgence of 3D human action representation with 3D skeletons, the quality and\nthe pace of recent progress have been significant. However, the inner workings\nof state-of-the-art learning based methods in 3D human action recognition still\nremain mostly black-box. In this work, we propose to use a new class of models\nknown as Temporal Convolutional Neural Networks (TCN) for 3D human action\nrecognition. Compared to popular LSTM-based Recurrent Neural Network models,\ngiven interpretable input such as 3D skeletons, TCN provides us a way to\nexplicitly learn readily interpretable spatio-temporal representations for 3D\nhuman action recognition. We provide our strategy in re-designing the TCN with\ninterpretability in mind and how such characteristics of the model is leveraged\nto construct a powerful 3D activity recognition method. Through this work, we\nwish to take a step towards a spatio-temporal model that is easier to\nunderstand, explain and interpret. The resulting model, Res-TCN, achieves\nstate-of-the-art results on the largest 3D human action recognition dataset,\nNTU-RGBD.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 19:00:36 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kim", "Tae Soo", ""], ["Reiter", "Austin", ""]]}, {"id": "1704.04517", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "ShapeWorld - A new test methodology for multimodal language\n  understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for evaluating multimodal deep learning models\nwith respect to their language understanding and generalization abilities. In\nthis approach, artificial data is automatically generated according to the\nexperimenter's specifications. The content of the data, both during training\nand evaluation, can be controlled in detail, which enables tasks to be created\nthat require true generalization abilities, in particular the combination of\npreviously introduced concepts in novel ways. We demonstrate the potential of\nour methodology by evaluating various visual question answering models on four\ndifferent tasks, and show how our framework gives us detailed insights into\ntheir capabilities and limitations. By open-sourcing our framework, we hope to\nstimulate progress in the field of multimodal language understanding.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 19:01:51 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1704.04587", "submitter": "Markus Haltmeier", "authors": "Stephan Antholzer, Markus Haltmeier, and Johannes Schwab", "title": "Deep Learning for Photoacoustic Tomography from Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of fast and accurate image reconstruction algorithms is a\ncentral aspect of computed tomography. In this paper, we investigate this issue\nfor the sparse data problem in photoacoustic tomography (PAT). We develop a\ndirect and highly efficient reconstruction algorithm based on deep learning. In\nour approach image reconstruction is performed with a deep convolutional neural\nnetwork (CNN), whose weights are adjusted prior to the actual image\nreconstruction based on a set of training data. The proposed reconstruction\napproach can be interpreted as a network that uses the PAT filtered\nbackprojection algorithm for the first layer, followed by the U-net\narchitecture for the remaining layers. Actual image reconstruction with deep\nlearning consists in one evaluation of the trained CNN, which does not require\ntime consuming solution of the forward and adjoint problems. At the same time,\nour numerical results demonstrate that the proposed deep learning approach\nreconstructs images with a quality comparable to state of the art iterative\napproaches for PAT from sparse data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 05:33:32 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 06:22:48 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 13:45:40 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Antholzer", "Stephan", ""], ["Haltmeier", "Markus", ""], ["Schwab", "Johannes", ""]]}, {"id": "1704.04610", "submitter": "Raj Gupta", "authors": "Raj Kumar Gupta and Alex Yong-Sang Chia and Deepu Rajan and Huang\n  Zhiyong", "title": "A learning-based approach for automatic image and video colorization", "comments": "Computer Graphics International - 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a color transfer algorithm to colorize a broad\nrange of gray images without any user intervention. The algorithm uses a\nmachine learning-based approach to automatically colorize grayscale images. The\nalgorithm uses the superpixel representation of the reference color images to\nlearn the relationship between different image features and their corresponding\ncolor values. We use this learned information to predict the color value of\neach grayscale image superpixel. As compared to processing individual image\npixels, our use of superpixels helps us to achieve a much higher degree of\nspatial consistency as well as speeds up the colorization process. The\npredicted color values of the gray-scale image superpixels are used to provide\na 'micro-scribble' at the centroid of the superpixels. These color scribbles\nare refined by using a voting based approach. To generate the final\ncolorization result, we use an optimization-based approach to smoothly spread\nthe color scribble across all pixels within a superpixel. Experimental results\non a broad range of images and the comparison with existing state-of-the-art\ncolorization methods demonstrate the greater effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 09:21:57 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Gupta", "Raj Kumar", ""], ["Chia", "Alex Yong-Sang", ""], ["Rajan", "Deepu", ""], ["Zhiyong", "Huang", ""]]}, {"id": "1704.04613", "submitter": "Yang Mingkun", "authors": "Xiang Bai, Mingkun Yang, Pengyuan Lyu, Yongchao Xu and Jiebo Luo", "title": "Integrating Scene Text and Visual Appearance for Fine-Grained Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text in natural images contains rich semantics that are often highly relevant\nto objects or scene. In this paper, we focus on the problem of fully exploiting\nscene text for visual understanding. The main idea is combining word\nrepresentations and deep visual features into a globally trainable deep\nconvolutional neural network. First, the recognized words are obtained by a\nscene text reading system. Then, we combine the word embedding of the\nrecognized words and the deep visual features into a single representation,\nwhich is optimized by a convolutional neural network for fine-grained image\nclassification. In our framework, the attention mechanism is adopted to reveal\nthe relevance between each recognized word and the given image, which further\nenhances the recognition performance. We have performed experiments on two\ndatasets: Con-Text dataset and Drink Bottle dataset, that are proposed for\nfine-grained classification of business places and drink bottles, respectively.\nThe experimental results consistently demonstrate that the proposed method\ncombining textual and visual cues significantly outperforms classification with\nonly visual representations. Moreover, we have shown that the learned\nrepresentation improves the retrieval performance on the drink bottle images by\na large margin, making it potentially useful in product search.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 09:44:08 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 01:27:20 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Bai", "Xiang", ""], ["Yang", "Mingkun", ""], ["Lyu", "Pengyuan", ""], ["Xu", "Yongchao", ""], ["Luo", "Jiebo", ""]]}, {"id": "1704.04650", "submitter": "Christian Igel", "authors": "Jan Kremer, Kristoffer Stensbo-Smidt, Fabian Gieseke, Kim Steenstrup\n  Pedersen, Christian Igel", "title": "Big Universe, Big Data: Machine Learning and Image Analysis for\n  Astronomy", "comments": null, "journal-ref": "IEEE Intelligent Systems, vol. 32, no. , pp. 16-22, Mar.-Apr. 2017", "doi": "10.1109/MIS.2017.40", "report-no": null, "categories": "astro-ph.IM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrophysics and cosmology are rich with data. The advent of wide-area\ndigital cameras on large aperture telescopes has led to ever more ambitious\nsurveys of the sky. Data volumes of entire surveys a decade ago can now be\nacquired in a single night and real-time analysis is often desired. Thus,\nmodern astronomy requires big data know-how, in particular it demands highly\nefficient machine learning and image analysis algorithms. But scalability is\nnot the only challenge: Astronomy applications touch several current machine\nlearning research questions, such as learning from biased data and dealing with\nlabel and measurement noise. We argue that this makes astronomy a great domain\nfor computer science research, as it pushes the boundaries of data analysis. In\nthe following, we will present this exciting application area for data\nscientists. We will focus on exemplary results, discuss main challenges, and\nhighlight some recent methodological advancements in machine learning and image\nanalysis triggered by astronomical applications.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 15:32:13 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kremer", "Jan", ""], ["Stensbo-Smidt", "Kristoffer", ""], ["Gieseke", "Fabian", ""], ["Pedersen", "Kim Steenstrup", ""], ["Igel", "Christian", ""]]}, {"id": "1704.04671", "submitter": "Jonathan Stroud", "authors": "Zehuan Yuan, Jonathan C. Stroud, Tong Lu, Jia Deng", "title": "Temporal Action Localization by Structured Maximal Sums", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of temporal action localization in videos. We pose\naction localization as a structured prediction over arbitrary-length temporal\nwindows, where each window is scored as the sum of frame-wise classification\nscores. Additionally, our model classifies the start, middle, and end of each\naction as separate components, allowing our system to explicitly model each\naction's temporal evolution and take advantage of informative temporal\ndependencies present in this structure. In this framework, we localize actions\nby searching for the structured maximal sum, a problem for which we develop a\nnovel, provably-efficient algorithmic solution. The frame-wise classification\nscores are computed using features from a deep Convolutional Neural Network\n(CNN), which are trained end-to-end to directly optimize for a novel structured\nobjective. We evaluate our system on the THUMOS 14 action detection benchmark\nand achieve competitive performance.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 18:10:21 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Yuan", "Zehuan", ""], ["Stroud", "Jonathan C.", ""], ["Lu", "Tong", ""], ["Deng", "Jia", ""]]}, {"id": "1704.04689", "submitter": "Amir Mazaheri", "authors": "Amir Mazaheri, Dong Zhang, Mubarak Shah", "title": "Video Fill In the Blank using LR/RL LSTMs with Spatial-Temporal\n  Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video and a description sentence with one missing word (we call it\nthe \"source sentence\"), Video-Fill-In-the-Blank (VFIB) problem is to find the\nmissing word automatically. The contextual information of the sentence, as well\nas visual cues from the video, are important to infer the missing word\naccurately. Since the source sentence is broken into two fragments: the\nsentence's left fragment (before the blank) and the sentence's right fragment\n(after the blank), traditional Recurrent Neural Networks cannot encode this\nstructure accurately because of many possible variations of the missing word in\nterms of the location and type of the word in the source sentence. For example,\na missing word can be the first word or be in the middle of the sentence and it\ncan be a verb or an adjective. In this paper, we propose a framework to tackle\nthe textual encoding: Two separate LSTMs (the LR and RL LSTMs) are employed to\nencode the left and right sentence fragments and a novel structure is\nintroduced to combine each fragment with an \"external memory\" corresponding the\nopposite fragments. For the visual encoding, end-to-end spatial and temporal\nattention models are employed to select discriminative visual representations\nto find the missing word. In the experiments, we demonstrate the superior\nperformance of the proposed method on challenging VFIB problem. Furthermore, we\nintroduce an extended and more generalized version of VFIB, which is not\nlimited to a single blank. Our experiments indicate the generalization\ncapability of our method in dealing with such more realistic scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 21:13:41 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Mazaheri", "Amir", ""], ["Zhang", "Dong", ""], ["Shah", "Mubarak", ""]]}, {"id": "1704.04706", "submitter": "Lukas On Arnold", "authors": "Lukas On Arnold (for the SoLid collaboration)", "title": "Trigger for the SoLid Reactor Antineutrino Experiment", "comments": "Poster presented at NuPhys2016 (London, 12-14 December 2016). 8\n  pages, LaTeX, 6 png figures, 1 pdf figure", "journal-ref": null, "doi": null, "report-no": "NuPhys2016-Arnold", "categories": "physics.ins-det cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SoLid, located at SCK-CEN in Mol, Belgium, is a reactor antineutrino\nexperiment at a very short baseline of 5.5 -- 10m aiming at the search for\nsterile neutrinos and for high precision measurement of the neutrino energy\nspectrum of Uranium-235. It uses a novel approach using Lithium-6 sheets and\nPVT cubes as scintillators for tagging the Inverse Beta-Decay products (neutron\nand positron). Being located overground and close to the BR2 research reactor,\nthe experiment faces a large amount of backgrounds. Efficient real-time\nbackground and noise rejection is essential in order to increase the\nsignal-background ratio for precise oscillation measurement and decrease data\nproduction to a rate which can be handled by the online software. Therefore, a\nreliable distinction between the neutrons and background signals is crucial.\nThis can be performed online with a dedicated firmware trigger. A peak counting\nalgorithm and an algorithm measuring time over threshold have been identified\nas performing well both in terms of efficiency and fake rate, and have been\nimplemented onto FPGA.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 01:27:22 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 19:12:56 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Arnold", "Lukas On", "", "for the SoLid collaboration"]]}, {"id": "1704.04749", "submitter": "David Novotn\\'y", "authors": "David Novotny, Diane Larlus, Andrea Vedaldi", "title": "AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive\n  Features For Semantic Matching", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress of deep learning in recent years,\nstate-of-the-art semantic matching methods still rely on legacy features such\nas SIFT or HoG. We argue that the strong invariance properties that are key to\nthe success of recent deep architectures on the classification task make them\nunfit for dense correspondence tasks, unless a large amount of supervision is\nused. In this work, we propose a deep network, termed AnchorNet, that produces\nimage representations that are well-suited for semantic matching. It relies on\na set of filters whose response is geometrically consistent across different\nobject instances, even in the presence of strong intra-class, scale, or\nviewpoint variations. Trained only with weak image-level labels, the final\nrepresentation successfully captures information about the object structure and\nimproves results of state-of-the-art semantic matching methods such as the\ndeformable spatial pyramid or the proposal flow methods. We show positive\nresults on the cross-instance matching task where different instances of the\nsame object category are matched as well as on a new cross-category semantic\nmatching task aligning pairs of instances each from a different object class.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 11:07:02 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Novotny", "David", ""], ["Larlus", "Diane", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1704.04793", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas\n  Daniilidis", "title": "Harvesting Multiple Views for Marker-less 3D Human Pose Annotations", "comments": "CVPR 2017 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances with Convolutional Networks (ConvNets) have shifted the\nbottleneck for many computer vision tasks to annotated data collection. In this\npaper, we present a geometry-driven approach to automatically collect\nannotations for human pose prediction tasks. Starting from a generic ConvNet\nfor 2D human pose, and assuming a multi-view setup, we describe an automatic\nway to collect accurate 3D human pose annotations. We capitalize on constraints\noffered by the 3D geometry of the camera setup and the 3D structure of the\nhuman body to probabilistically combine per view 2D ConvNet predictions into a\nglobally optimal 3D pose. This 3D pose is used as the basis for harvesting\nannotations. The benefit of the annotations produced automatically with our\napproach is demonstrated in two challenging settings: (i) fine-tuning a generic\nConvNet-based 2D pose predictor to capture the discriminative aspects of a\nsubject's appearance (i.e.,\"personalization\"), and (ii) training a ConvNet from\nscratch for single view 3D human pose prediction without leveraging 3D pose\ngroundtruth. The proposed multi-view pose estimator achieves state-of-the-art\nresults on standard benchmarks, demonstrating the effectiveness of our method\nin exploiting the available multi-view information.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 16:19:19 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Zhou", "Xiaowei", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1704.04805", "submitter": "Tinsae Gebrechristos Dulecha", "authors": "Tinsae G.Dulecha", "title": "Replicator Equation: Applications Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The replicator equation is a simple model of evolution that leads to stable\nform of Nash Equilibrium, Evolutionary Stable Strategy (ESS). It has been\nstudied in connection with Evolutionary Game Theory and was originally\ndeveloped for symmetric games. Beyond its first emphasis in biological use,\nevolutionary game theory has been expanded well beyond in social studies for\nbehavioral analysis, in machine learning, computer vision and others. Its\nseveral applications in the fields of machine learning and computer vision has\ndrawn my attention which is the reason to write this extended abstract\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 18:20:44 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 17:22:47 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Dulecha", "Tinsae G.", ""]]}, {"id": "1704.04825", "submitter": "Wenxiang Cong", "authors": "Wenxiang Cong, Ge Wang, Qingsong Yang, Jiang Hsieh, Jia Li, Rongjie\n  Lai", "title": "CT Image Reconstruction in a Low Dimensional Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization methods are commonly used in X-ray CT image reconstruction.\nDifferent regularization methods reflect the characterization of different\nprior knowledge of images. In a recent work, a new regularization method called\na low-dimensional manifold model (LDMM) is investigated to characterize the\nlow-dimensional patch manifold structure of natural images, where the manifold\ndimensionality characterizes structural information of an image. In this paper,\nwe propose a CT image reconstruction method based on the prior knowledge of the\nlow-dimensional manifold of CT image. Using the clinical raw projection data\nfrom GE clinic, we conduct comparisons for the CT image reconstruction among\nthe proposed method, the simultaneous algebraic reconstruction technique (SART)\nwith the total variation (TV) regularization, and the filtered back projection\n(FBP) method. Results show that the proposed method can successfully recover\nstructural details of an imaging object, and achieve higher spatial and\ncontrast resolution of the reconstructed image than counterparts of FBP and\nSART with TV.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 22:02:27 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Cong", "Wenxiang", ""], ["Wang", "Ge", ""], ["Yang", "Qingsong", ""], ["Hsieh", "Jiang", ""], ["Li", "Jia", ""], ["Lai", "Rongjie", ""]]}, {"id": "1704.04861", "submitter": "Menglong Zhu", "authors": "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\n  Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 03:57:34 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Howard", "Andrew G.", ""], ["Zhu", "Menglong", ""], ["Chen", "Bo", ""], ["Kalenichenko", "Dmitry", ""], ["Wang", "Weijun", ""], ["Weyand", "Tobias", ""], ["Andreetto", "Marco", ""], ["Adam", "Hartwig", ""]]}, {"id": "1704.04865", "submitter": "Felix Juefei-Xu", "authors": "Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides", "title": "Gang of GANs: Generative Adversarial Networks with Maximum Margin\n  Ranking", "comments": "16 pages. 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional generative adversarial networks (GAN) and many of its variants\nare trained by minimizing the KL or JS-divergence loss that measures how close\nthe generated data distribution is from the true data distribution. A recent\nadvance called the WGAN based on Wasserstein distance can improve on the KL and\nJS-divergence based GANs, and alleviate the gradient vanishing, instability,\nand mode collapse issues that are common in the GAN training. In this work, we\naim at improving on the WGAN by first generalizing its discriminator loss to a\nmargin-based one, which leads to a better discriminator, and in turn a better\ngenerator, and then carrying out a progressive training paradigm involving\nmultiple GANs to contribute to the maximum margin ranking loss so that the GAN\nat later stages will improve upon early stages. We call this method Gang of\nGANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce\nthe gap between the true data distribution and the generated data distribution\nby at least half in an optimally trained WGAN. We have also proposed a new way\nof measuring GAN quality which is based on image completion tasks. We have\nevaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10,\nand 50K-SSFF, and have seen both visual and quantitative improvement over\nbaseline WGAN.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 04:42:56 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Juefei-Xu", "Felix", ""], ["Boddeti", "Vishnu Naresh", ""], ["Savvides", "Marios", ""]]}, {"id": "1704.04877", "submitter": "Anand S. Sengupta", "authors": "Amit Reza, Anand S. Sengupta", "title": "Least square ellipsoid fitting using iterative orthogonal\n  transformations", "comments": "Submitted to Applied Mathematics and Computation (Elsevier)", "journal-ref": null, "doi": "10.1016/j.amc.2017.07.025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generalised method for ellipsoid fitting against a minimum set\nof data points. The proposed method is numerically stable and applies to a wide\nrange of ellipsoidal shapes, including highly elongated and arbitrarily\noriented ellipsoids. This new method also provides for the retrieval of\nrotational angle and length of semi-axes of the fitted ellipsoids accurately.\nWe demonstrate the efficacy of this algorithm on simulated data sets and also\nindicate its potential use in gravitational wave data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 05:44:05 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 09:16:59 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 13:08:03 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Reza", "Amit", ""], ["Sengupta", "Anand S.", ""]]}, {"id": "1704.04886", "submitter": "Bo Zhao", "authors": "Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, Jiashi Feng", "title": "Multi-View Image Generation from a Single-View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a challenging problem -- how to generate multi-view\ncloth images from only a single view input. To generate realistic-looking\nimages with different views from the input, we propose a new image generation\nmodel termed VariGANs that combines the strengths of the variational inference\nand the Generative Adversarial Networks (GANs). Our proposed VariGANs model\ngenerates the target image in a coarse-to-fine manner instead of a single pass\nwhich suffers from severe artifacts. It first performs variational inference to\nmodel global appearance of the object (e.g., shape and color) and produce a\ncoarse image with a different view. Conditioned on the generated low resolution\nimages, it then proceeds to perform adversarial learning to fill details and\ngenerate images of consistent details with the input. Extensive experiments\nconducted on two clothing datasets, MVC and DeepFashion, have demonstrated that\nimages of a novel view generated by our model are more plausible than those\ngenerated by existing approaches, in terms of more consistent global appearance\nas well as richer and sharper details.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 06:54:34 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 06:42:24 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 03:55:09 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 02:36:32 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhao", "Bo", ""], ["Wu", "Xiao", ""], ["Cheng", "Zhi-Qi", ""], ["Liu", "Hao", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""]]}, {"id": "1704.04952", "submitter": "Suman Saha", "authors": "Suman Saha, Gurkirt Singh, Fabio Cuzzolin", "title": "AMTnet: Action-Micro-Tube Regression by End-to-end Trainable Deep\n  Architecture", "comments": "Update to version in ICCV 2017 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dominant approaches to action detection can only provide sub-optimal\nsolutions to the problem, as they rely on seeking frame-level detections, to\nlater compose them into \"action tubes\" in a post-processing step. With this\npaper we radically depart from current practice, and take a first step towards\nthe design and implementation of a deep network architecture able to classify\nand regress whole video subsets, so providing a truly optimal solution of the\naction detection problem. In this work, in particular, we propose a novel deep\nnet framework able to regress and classify 3D region proposals spanning two\nsuccessive video frames, whose core is an evolution of classical region\nproposal networks (RPNs). As such, our 3D-RPN net is able to effectively encode\nthe temporal aspect of actions by purely exploiting appearance, as opposed to\nmethods which heavily rely on expensive flow maps. The proposed model is\nend-to-end trainable and can be jointly optimised for action localisation and\nclassification in a single step. At test time the network predicts\n\"micro-tubes\" encompassing two successive frames, which are linked up into\ncomplete action tubes via a new algorithm which exploits the temporal encoding\nlearned by the network and cuts computation time by 50%. Promising results on\nthe J-HMDB-21 and UCF-101 action detection datasets show that our model does\noutperform the state-of-the-art when relying purely on appearance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:04:46 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 14:18:44 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Saha", "Suman", ""], ["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1704.05020", "submitter": "Pengfei Dou", "authors": "Pengfei Dou, Shishir K. Shah, Ioannis A. Kakadiaris", "title": "End-to-end 3D face reconstruction with deep neural networks", "comments": "Accepted to CVPR17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D facial shape reconstruction from a single 2D facial image has\nbeen an active research area due to its wide applications. Inspired by the\nsuccess of deep neural networks (DNN), we propose a DNN-based approach for\nEnd-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different\nfrom recent works that reconstruct and refine the 3D face in an iterative\nmanner using both an RGB image and an initial 3D facial shape rendering, our\nDNN model is end-to-end, and thus the complicated 3D rendering process can be\navoided. Moreover, we integrate in the DNN architecture two components, namely\na multi-task loss function and a fusion convolutional neural network (CNN) to\nimprove facial expression reconstruction. With the multi-task loss function, 3D\nface reconstruction is divided into neutral 3D facial shape reconstruction and\nexpressive 3D facial shape reconstruction. The neutral 3D facial shape is\nclass-specific. Therefore, higher layer features are useful. In comparison, the\nexpressive 3D facial shape favors lower or intermediate layer features. With\nthe fusion-CNN, features from different intermediate layers are fused and\ntransformed for predicting the 3D expressive facial shape. Through extensive\nexperiments, we demonstrate the superiority of our end-to-end framework in\nimproving the accuracy of 3D face reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:31:12 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Dou", "Pengfei", ""], ["Shah", "Shishir K.", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1704.05051", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao and Radha Poovendran", "title": "Google's Cloud Vision API Is Not Robust To Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google has recently introduced the Cloud Vision API for image analysis.\nAccording to the demonstration website, the API \"quickly classifies images into\nthousands of categories, detects individual objects and faces within images,\nand finds and reads printed words contained within images.\" It can be also used\nto \"detect different types of inappropriate content from adult to violent\ncontent.\"\n  In this paper, we evaluate the robustness of Google Cloud Vision API to input\nperturbation. In particular, we show that by adding sufficient noise to the\nimage, the API generates completely different outputs for the noisy image,\nwhile a human observer would perceive its original content. We show that the\nattack is consistently successful, by performing extensive experiments on\ndifferent image types, including natural images, images containing faces and\nimages with texts. For instance, using images from ImageNet dataset, we found\nthat adding an average of 14.25% impulse noise is enough to deceive the API.\nOur findings indicate the vulnerability of the API in adversarial environments.\nFor example, an adversary can bypass an image filtering system by adding noise\nto inappropriate images. We then show that when a noise filter is applied on\ninput images, the API generates mostly the same outputs for restored images as\nfor original images. This observation suggests that cloud vision API can\nreadily benefit from noise filtering, without the need for updating image\nanalysis algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 09:47:46 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 05:31:16 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Poovendran", "Radha", ""]]}, {"id": "1704.05122", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "A Gabor Filter Texture Analysis Approach for Histopathological Brain\n  Tumor Subtype Discrimination", "comments": "14 pages,4 figures, 2 tables", "journal-ref": "ISESCO Journal of Science and Technology, vol. 12, no. 22, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meningioma brain tumour discrimination is challenging as many histological\npatterns are mixed between the different subtypes. In clinical practice,\ndominant patterns are investigated for signs of specific meningioma pathology;\nhowever the simple observation could result in inter- and intra-observer\nvariation due to the complexity of the histopathological patterns. Also\nemploying a computerised feature extraction approach applied at a single\nresolution scale might not suffice in accurately delineating the mixture of\nhistopathological patterns. In this work we propose a novel multiresolution\nfeature extraction approach for characterising the textural properties of the\ndifferent pathological patterns (i.e. mainly cell nuclei shape, orientation and\nspatial arrangement within the cytoplasm). The pattern textural properties are\ncharacterised at various scales and orientations for an improved separability\nbetween the different extracted features. The Gabor filter energy output of\neach magnitude response was combined with four other fixed-resolution texture\nsignatures (2 model-based and 2 statistical-based) with and without cell nuclei\nsegmentation. The highest classification accuracy of 95% was reported when\ncombining the Gabor filters energy and the meningioma subimage fractal\nsignature as a feature vector without performing any prior cell nuceli\nsegmentation. This indicates that characterising the cell-nuclei\nself-similarity properties via Gabor filters can assists in achieving an\nimproved meningioma subtype classification, which can assist in overcoming\nvariations in reported diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:06:09 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1704.05165", "submitter": "Brent Griffin", "authors": "Brent A. Griffin and Jason J. Corso", "title": "Video Object Segmentation using Supervoxel-Based Gerrymandering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixels operate locally. Superpixels have some potential to collect\ninformation across many pixels; supervoxels have more potential by implicitly\noperating across time. In this paper, we explore this well established notion\nthoroughly analyzing how supervoxels can be used in place of and in conjunction\nwith other means of aggregating information across space-time. Focusing on the\nproblem of strictly unsupervised video object segmentation, we devise a method\ncalled supervoxel gerrymandering that links masks of foregroundness and\nbackgroundness via local and non-local consensus measures. We pose and answer a\nseries of critical questions about the ability of supervoxels to adequately\nsway local voting; the questions regard type and scale of supervoxels as well\nas local versus non-local consensus, and the questions are posed in a general\nway so as to impact the broader knowledge of the use of supervoxels in video\nunderstanding. We work with the DAVIS dataset and find that our analysis yields\nan unsupervised method that outperforms all other known unsupervised methods\nand even many supervised ones.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 01:11:35 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Griffin", "Brent A.", ""], ["Corso", "Jason J.", ""]]}, {"id": "1704.05188", "submitter": "Zequn Jie", "authors": "Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng and Wei Liu", "title": "Deep Self-Taught Learning for Weakly Supervised Object Localization", "comments": "Accepted as spotlight paper by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing weakly supervised localization (WSL) approaches learn detectors\nby finding positive bounding boxes based on features learned with image-level\nsupervision. However, those features do not contain spatial location related\ninformation and usually provide poor-quality positive samples for training a\ndetector. To overcome this issue, we propose a deep self-taught learning\napproach, which makes the detector learn the object-level features reliable for\nacquiring tight positive samples and afterwards re-train itself based on them.\nConsequently, the detector progressively improves its detection ability and\nlocalizes more informative positive samples. To implement such self-taught\nlearning, we propose a seed sample acquisition method via image-to-object\ntransferring and dense subgraph discovery to find reliable positive samples for\ninitializing the detector. An online supportive sample harvesting scheme is\nfurther proposed to dynamically select the most confident tight positive\nsamples and train the detector in a mutual boosting way. To prevent the\ndetector from being trapped in poor optima due to overfitting, we propose a new\nrelative improvement of predicted CNN scores for guiding the self-taught\nlearning process. Extensive experiments on PASCAL 2007 and 2012 show that our\napproach outperforms the state-of-the-arts, strongly validating its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 03:30:28 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 06:23:53 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jie", "Zequn", ""], ["Wei", "Yunchao", ""], ["Jin", "Xiaojie", ""], ["Feng", "Jiashi", ""], ["Liu", "Wei", ""]]}, {"id": "1704.05231", "submitter": "Dongbo Min", "authors": "Suhyuk Um, Jaeyoon Kim, and Dongbo Min (Senior Member, IEEE)", "title": "Fast 2-D Complex Gabor Filter with Kernel Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2-D complex Gabor filtering has found numerous applications in the fields of\ncomputer vision and image processing. Especially, in some applications, it is\noften needed to compute 2-D complex Gabor filter bank consisting of the 2-D\ncomplex Gabor filtering outputs at multiple orientations and frequencies.\nAlthough several approaches for fast 2-D complex Gabor filtering have been\nproposed, they primarily focus on reducing the runtime of performing the 2-D\ncomplex Gabor filtering once at specific orientation and frequency. To obtain\nthe 2-D complex Gabor filter bank output, existing methods are repeatedly\napplied with respect to multiple orientations and frequencies. In this paper,\nwe propose a novel approach that efficiently computes the 2-D complex Gabor\nfilter bank by reducing the computational redundancy that arises when\nperforming the Gabor filtering at multiple orientations and frequencies. The\nproposed method first decomposes the Gabor basis kernels to allow a fast\nconvolution with the Gaussian kernel in a separable manner. This enables\nreducing the runtime of the 2-D complex Gabor filter bank by reusing\nintermediate results of the 2-D complex Gabor filtering computed at a specific\norientation. Furthermore, we extend this idea into 2-D localized sliding\ndiscrete Fourier transform (SDFT) using the Gaussian kernel in the DFT\ncomputation, which lends a spatial localization ability as in the 2-D complex\nGabor filter. Experimental results demonstrate that our method runs faster than\nstate-of-the-arts methods for fast 2-D complex Gabor filtering, while\nmaintaining similar filtering quality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 08:34:33 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Um", "Suhyuk", "", "Senior Member, IEEE"], ["Kim", "Jaeyoon", "", "Senior Member, IEEE"], ["Min", "Dongbo", "", "Senior Member, IEEE"]]}, {"id": "1704.05239", "submitter": "Ruoteng Li", "authors": "Ruoteng Li, Robby T. Tan, Loong-Fah Cheong", "title": "Robust Optical Flow Estimation in Rainy Scenes", "comments": "9 pages, CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation in the rainy scenes is challenging due to background\ndegradation introduced by rain streaks and rain accumulation effects in the\nscene. Rain accumulation effect refers to poor visibility of remote objects due\nto the intense rainfall. Most existing optical flow methods are erroneous when\napplied to rain sequences because the conventional brightness constancy\nconstraint (BCC) and gradient constancy constraint (GCC) generally break down\nin this situation. Based on the observation that the RGB color channels receive\nraindrop radiance equally, we introduce a residue channel as a new data\nconstraint to reduce the effect of rain streaks. To handle rain accumulation,\nour method decomposes the image into a piecewise-smooth background layer and a\nhigh-frequency detail layer. It also enforces the BCC on the background layer\nonly. Results on both synthetic dataset and real images show that our algorithm\noutperforms existing methods on different types of rain sequences. To our\nknowledge, this is the first optical flow method specifically dealing with\nrain.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 09:04:02 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 15:39:08 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Li", "Ruoteng", ""], ["Tan", "Robby T.", ""], ["Cheong", "Loong-Fah", ""]]}, {"id": "1704.05240", "submitter": "Sergiy Vorobyov A.", "authors": "Rui Gao, Sergiy A. Vorobyov, Hong Zhao", "title": "Image Fusion With Cosparse Analysis Operator", "comments": "12 pages, 4 figures, 1 table, Submitted to IEEE Signal Processing\n  Letters on December 2016", "journal-ref": "IEEE Signal Processing Letters, vol. 24, no. 7, pp. 943-947, July\n  2017", "doi": "10.1109/LSP.2017.2696055", "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the image fusion problem, where multiple images captured\nwith different focus distances are to be combined into a higher quality\nall-in-focus image. Most current approaches for image fusion strongly rely on\nthe unrealistic noise-free assumption used during the image acquisition, and\nthen yield limited robustness in fusion processing. In our approach, we\nformulate the multi-focus image fusion problem in terms of an analysis sparse\nmodel, and simultaneously perform the restoration and fusion of multi-focus\nimages. Based on this model, we propose an analysis operator learning, and\ndefine a novel fusion function to generate an all-in-focus image. Experimental\nevaluations confirm the effectiveness of the proposed fusion approach both\nvisually and quantitatively, and show that our approach outperforms\nstate-of-the-art fusion methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 09:05:09 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Gao", "Rui", ""], ["Vorobyov", "Sergiy A.", ""], ["Zhao", "Hong", ""]]}, {"id": "1704.05267", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "A Comment on \"Analysis of Video Image Sequences Using Point and Line\n  Correspondences\"", "comments": null, "journal-ref": "preliminary version of: M.A. K{\\l}opotek: A comment on \"Analysis\n  of video image sequences using point and line correspondences\". Pattern\n  Recognition 28(1995)2, pp. 283-292", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we would like to deny the results of Wang et al. raising two\nfundamental claims:\n  * A line does not contribute anything to recognition of motion parameters\nfrom two images\n  * Four traceable points are not sufficient to recover motion parameters from\ntwo perspective\n  To be constructive, however, we show that four traceable points are\nsufficient to recover motion parameters from two frames under orthogonal\nprojection and that five points are sufficient to simplify the solution of the\ntwo-frame problem under orthogonal projection to solving a linear equation\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 10:56:14 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1704.05310", "submitter": "Armand Joulin", "authors": "Piotr Bojanowski, Armand Joulin", "title": "Unsupervised Learning by Predicting Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks provide visual features that perform remarkably\nwell in many computer vision applications. However, training these networks\nrequires significant amounts of supervision. This paper introduces a generic\nframework to train deep networks, end-to-end, with no supervision. We propose\nto fix a set of target representations, called Noise As Targets (NAT), and to\nconstrain the deep features to align to them. This domain agnostic approach\navoids the standard unsupervised learning issues of trivial solutions and\ncollapsing of features. Thanks to a stochastic batch reassignment strategy and\na separable square loss function, it scales to millions of images. The proposed\napproach produces representations that perform on par with state-of-the-art\nunsupervised methods on ImageNet and Pascal VOC.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 12:51:47 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""]]}, {"id": "1704.05368", "submitter": "Jan Egger", "authors": "Jan Egger, Dieter Schmalstieg, Xiaojun Chen, Wolfram G. Zoller,\n  Alexander Hann", "title": "Interactive Outlining of Pancreatic Cancer Liver Metastases in\n  Ultrasound Images", "comments": "15 pages, 16 figures, 2 tables, 58 references", "journal-ref": "Sci. Rep. 7, 892, 2017", "doi": "10.1038/s41598-017-00940-z", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is the most commonly used liver imaging modality worldwide.\nDue to its low cost, it is increasingly used in the follow-up of cancer\npatients with metastases localized in the liver. In this contribution, we\npresent the results of an interactive segmentation approach for liver\nmetastases in US acquisitions. A (semi-) automatic segmentation is still very\nchallenging because of the low image quality and the low contrast between the\nmetastasis and the surrounding liver tissue. Thus, the state of the art in\nclinical practice is still manual measurement and outlining of the metastases\nin the US images. We tackle the problem by providing an interactive\nsegmentation approach providing real-time feedback of the segmentation results.\nThe approach has been evaluated with typical US acquisitions from the clinical\nroutine, and the datasets consisted of pancreatic cancer metastases. Even for\ndifficult cases, satisfying segmentations results could be achieved because of\nthe interactive real-time behavior of the approach. In total, 40 clinical\nimages have been evaluated with our method by comparing the results against\nmanual ground truth segmentations. This evaluation yielded to an average Dice\nScore of 85% and an average Hausdorff Distance of 13 pixels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 14:45:20 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Egger", "Jan", ""], ["Schmalstieg", "Dieter", ""], ["Chen", "Xiaojun", ""], ["Zoller", "Wolfram G.", ""], ["Hann", "Alexander", ""]]}, {"id": "1704.05409", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo and Simone Melzi", "title": "Ranking to Learn: Feature Ranking and Selection via Eigenvector\n  Centrality", "comments": "Preprint version - Lecture Notes in Computer Science - Springer 2017", "journal-ref": "New Frontiers in Mining Complex Patterns, Fifth International\n  workshop, nfMCP2016. Lecture Notes in Computer Science - Springer", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era where accumulating data is easy and storing it inexpensive, feature\nselection plays a central role in helping to reduce the high-dimensionality of\nhuge amounts of otherwise meaningless data. In this paper, we propose a\ngraph-based method for feature selection that ranks features by identifying the\nmost important ones into arbitrary set of cues. Mapping the problem on an\naffinity graph-where features are the nodes-the solution is given by assessing\nthe importance of nodes through some indicators of centrality, in particular,\nthe Eigen-vector Centrality (EC). The gist of EC is to estimate the importance\nof a feature as a function of the importance of its neighbors. Ranking central\nnodes individuates candidate features, which turn out to be effective from a\nclassification point of view, as proved by a thoroughly experimental section.\nOur approach has been tested on 7 diverse datasets from recent literature\n(e.g., biological data and object recognition, among others), and compared\nagainst filter, embedded and wrappers methods. The results are remarkable in\nterms of accuracy, stability and low execution time.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:21:05 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Roffo", "Giorgio", ""], ["Melzi", "Simone", ""]]}, {"id": "1704.05416", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Ren Ng, Ravi Ramamoorthi", "title": "Light Field Blind Motion Deblurring", "comments": "To be presented at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deblurring light fields of general 3D scenes captured\nunder 3D camera motion and present both theoretical and practical\ncontributions. By analyzing the motion-blurred light field in the primal and\nFourier domains, we develop intuition into the effects of camera motion on the\nlight field, show the advantages of capturing a 4D light field instead of a\nconventional 2D image for motion deblurring, and derive simple methods of\nmotion deblurring in certain cases. We then present an algorithm to blindly\ndeblur light fields of general scenes without any estimation of scene geometry,\nand demonstrate that we can recover both the sharp light field and the 3D\ncamera motion path of real and synthetically-blurred light fields.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:40:59 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Ng", "Ren", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1704.05519", "submitter": "Joel Janai", "authors": "Joel Janai, Fatma G\\\"uney, Aseem Behl and Andreas Geiger", "title": "Computer Vision for Autonomous Vehicles: Problems, Datasets and State of\n  the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed enormous progress in AI-related fields such as\ncomputer vision, machine learning, and autonomous vehicles. As with any rapidly\ngrowing field, it becomes increasingly difficult to stay up-to-date or enter\nthe field as a beginner. While several survey papers on particular sub-problems\nhave appeared, no comprehensive survey on problems, datasets, and methods in\ncomputer vision for autonomous vehicles has been published. This book attempts\nto narrow this gap by providing a survey on the state-of-the-art datasets and\ntechniques. Our survey includes both the historically most relevant literature\nas well as the current state of the art on several specific topics, including\nrecognition, reconstruction, motion estimation, tracking, scene understanding,\nand end-to-end learning for autonomous driving. Towards this goal, we analyze\nthe performance of the state of the art on several challenging benchmarking\ndatasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open\nproblems and current research challenges. To ease accessibility and accommodate\nmissing references, we also provide a website that allows navigating topics as\nwell as methods and provides additional information.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 20:33:50 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 16:09:40 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 19:16:56 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Janai", "Joel", ""], ["G\u00fcney", "Fatma", ""], ["Behl", "Aseem", ""], ["Geiger", "Andreas", ""]]}, {"id": "1704.05526", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate\n  Saenko", "title": "Learning to Reason: End-to-End Module Networks for Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language questions are inherently compositional, and many are most\neasily answered by reasoning about their decomposition into modular\nsub-problems. For example, to answer \"is there an equal number of balls and\nboxes?\" we can look for balls, look for boxes, count them, and compare the\nresults. The recently proposed Neural Module Network (NMN) architecture\nimplements this approach to question answering by parsing questions into\nlinguistic substructures and assembling question-specific deep networks from\nsmaller modules that each solve one subtask. However, existing NMN\nimplementations rely on brittle off-the-shelf parsers, and are restricted to\nthe module configurations proposed by these parsers rather than learning them\nfrom data. In this paper, we propose End-to-End Module Networks (N2NMNs), which\nlearn to reason by directly predicting instance-specific network layouts\nwithout the aid of a parser. Our model learns to generate network structures\n(by imitating expert demonstrations) while simultaneously learning network\nparameters (using the downstream task loss). Experimental results on the new\nCLEVR dataset targeted at compositional question answering show that N2NMNs\nachieve an error reduction of nearly 50% relative to state-of-the-art\nattentional approaches, while discovering interpretable network architectures\nspecialized for each question.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 20:57:32 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 03:22:40 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 22:22:59 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Hu", "Ronghang", ""], ["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1704.05548", "submitter": "Lluis Castrejon", "authors": "Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, Sanja Fidler", "title": "Annotating Object Instances with a Polygon-RNN", "comments": null, "journal-ref": "CVPR 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for semi-automatic annotation of object instances.\nWhile most current methods treat object segmentation as a pixel-labeling\nproblem, we here cast it as a polygon prediction task, mimicking how most\ncurrent datasets have been annotated. In particular, our approach takes as\ninput an image crop and sequentially produces vertices of the polygon outlining\nthe object. This allows a human annotator to interfere at any time and correct\na vertex if needed, producing as accurate segmentation as desired by the\nannotator. We show that our approach speeds up the annotation process by a\nfactor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement\nin IoU with original ground-truth, matching the typical agreement between human\nannotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We\nfurther show generalization capabilities of our approach to unseen datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 22:17:28 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Castrejon", "Lluis", ""], ["Kundu", "Kaustav", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1704.05564", "submitter": "Zhuo Hui", "authors": "Zhuo Hui and Kalyan Sunkavalli and Sunil Hadap and Aswin C.\n  Sankaranarayanan", "title": "Illuminant Spectra-based Source Separation Using Flash Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world lighting often consists of multiple illuminants with different\nspectra. Separating and manipulating these illuminants in post-process is a\nchallenging problem that requires either significant manual input or calibrated\nscene geometry and lighting. In this work, we leverage a flash/no-flash image\npair to analyze and edit scene illuminants based on their spectral differences.\nWe derive a novel physics-based relationship between color variations in the\nobserved flash/no-flash intensities and the spectra and surface shading\ncorresponding to individual scene illuminants. Our technique uses this\nconstraint to automatically separate an image into constituent images lit by\neach illuminant. This separation can be used to support applications like white\nbalancing, lighting editing, and RGB photometric stereo, where we demonstrate\nresults that outperform state-of-the-art techniques on a wide range of images.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 00:09:12 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 00:37:26 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Hui", "Zhuo", ""], ["Sunkavalli", "Kalyan", ""], ["Hadap", "Sunil", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1704.05588", "submitter": "Dhiraj Gandhi", "authors": "Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta", "title": "Learning to Fly by Crashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid\nobstacles? One approach is to use a small dataset collected by human experts:\nhowever, high capacity learning algorithms tend to overfit when trained with\nlittle data. An alternative is to use simulation. But the gap between\nsimulation and real world remains large especially for perception problems. The\nreason most research avoids using large-scale real data is the fear of crashes!\nIn this paper, we propose to bite the bullet and collect a dataset of crashes\nitself! We build a drone whose sole purpose is to crash into objects: it\nsamples naive trajectories and crashes into random objects. We crash our drone\n11,500 times to create one of the biggest UAV crash dataset. This dataset\ncaptures the different ways in which a UAV can crash. We use all this negative\nflying data in conjunction with positive data sampled from the same\ntrajectories to learn a simple yet powerful policy for UAV navigation. We show\nthat this simple self-supervised model is quite effective in navigating the UAV\neven in extremely cluttered environments with dynamic obstacles including\nhumans. For supplementary video see: https://youtu.be/u151hJaGKUo\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:20:20 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 00:13:19 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Gandhi", "Dhiraj", ""], ["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1704.05591", "submitter": "Hamed Sadeghi", "authors": "Hamed Sadeghi, Shahrokh Valaee and Shahram Shirani", "title": "OCRAPOSE II: An OCR-based indoor positioning system using mobile phone\n  images", "comments": "14 pages, 22 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an OCR (optical character recognition)-based\nlocalization system called OCRAPOSE II, which is applicable in a number of\nindoor scenarios including office buildings, parkings, airports, grocery\nstores, etc. In these scenarios, characters (i.e. texts or numbers) can be used\nas suitable distinctive landmarks for localization. The proposed system takes\nadvantage of OCR to read these characters in the query still images and\nprovides a rough location estimate using a floor plan. Then, it finds depth and\nangle-of-view of the query using the information provided by the OCR engine in\norder to refine the location estimate. We derive novel formulas for the query\nangle-of-view and depth estimation using image line segments and the OCR box\ninformation. We demonstrate the applicability and effectiveness of the proposed\nsystem through experiments in indoor scenarios. It is shown that our system\ndemonstrates better performance compared to the state-of-the-art benchmarks in\nterms of location recognition rate and average localization error specially\nunder sparse database condition.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:43:23 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Sadeghi", "Hamed", ""], ["Valaee", "Shahrokh", ""], ["Shirani", "Shahram", ""]]}, {"id": "1704.05596", "submitter": "Zhen Wang", "authors": "Zhen Wang, Yuan-Hai Shao, Lan Bai, Li-Ming Liu, Nai-Yang Deng", "title": "Insensitive Stochastic Gradient Twin Support Vector Machine for Large\n  Scale Problems", "comments": "31 pages, 31 figures", "journal-ref": "Information Sciences, Volume 462, September 2018, Pages 114-131", "doi": "10.1016/j.ins.2018.06.007", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent algorithm has been successfully applied on\nsupport vector machines (called PEGASOS) for many classification problems. In\nthis paper, stochastic gradient descent algorithm is investigated to twin\nsupport vector machines for classification. Compared with PEGASOS, the proposed\nstochastic gradient twin support vector machines (SGTSVM) is insensitive on\nstochastic sampling for stochastic gradient descent algorithm. In theory, we\nprove the convergence of SGTSVM instead of almost sure convergence of PEGASOS.\nFor uniformly sampling, the approximation between SGTSVM and twin support\nvector machines is also given, while PEGASOS only has an opportunity to obtain\nan approximation of support vector machines. In addition, the nonlinear SGTSVM\nis derived directly from its linear case. Experimental results on both\nartificial datasets and large scale problems show the stable performance of\nSGTSVM with a fast learning speed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 03:08:38 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 06:38:48 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Wang", "Zhen", ""], ["Shao", "Yuan-Hai", ""], ["Bai", "Lan", ""], ["Liu", "Li-Ming", ""], ["Deng", "Nai-Yang", ""]]}, {"id": "1704.05624", "submitter": "Hossein Ziaei Nafchi", "authors": "Hossein Ziaei Nafchi, Atena Shahkolaei, Reza Farrahi Moghaddam,\n  Mohamed Cheriet", "title": "FSITM: A Feature Similarity Index For Tone-Mapped Images", "comments": "4 Pages, 1 Figure, 1 Table", "journal-ref": "IEEE Signal Processing Letters, vol. 22, no. 8, Aug 2015", "doi": "10.1109/LSP.2014.2381458", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, based on the local phase information of images, an objective\nindex, called the feature similarity index for tone-mapped images (FSITM), is\nproposed. To evaluate a tone mapping operator (TMO), the proposed index\ncompares the locally weighted mean phase angle map of an original high dynamic\nrange (HDR) to that of its associated tone-mapped image calculated using the\noutput of the TMO method. In experiments on two standard databases, it is shown\nthat the proposed FSITM method outperforms the state-of-the-art index, the tone\nmapped quality index (TMQI). In addition, a higher performance is obtained by\ncombining the FSITM and TMQI indices. The MATLAB source code of the proposed\nmetric(s) is available at\nhttps://www.mathworks.com/matlabcentral/fileexchange/59814.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 06:23:21 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Nafchi", "Hossein Ziaei", ""], ["Shahkolaei", "Atena", ""], ["Moghaddam", "Reza Farrahi", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1704.05629", "submitter": "Bob De Vos", "authors": "Bob D. de Vos, Jelmer M. Wolterink, Pim A. de Jong, Tim Leiner, Max A.\n  Viergever, Ivana I\\v{s}gum", "title": "ConvNet-Based Localization of Anatomical Structures in 3D Medical Images", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging , vol.PP, no.99, pp.1-1\n  (2017)", "doi": "10.1109/TMI.2017.2673121", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of anatomical structures is a prerequisite for many tasks in\nmedical image analysis. We propose a method for automatic localization of one\nor more anatomical structures in 3D medical images through detection of their\npresence in 2D image slices using a convolutional neural network (ConvNet).\n  A single ConvNet is trained to detect presence of the anatomical structure of\ninterest in axial, coronal, and sagittal slices extracted from a 3D image. To\nallow the ConvNet to analyze slices of different sizes, spatial pyramid pooling\nis applied. After detection, 3D bounding boxes are created by combining the\noutput of the ConvNet in all slices.\n  In the experiments 200 chest CT, 100 cardiac CT angiography (CTA), and 100\nabdomen CT scans were used. The heart, ascending aorta, aortic arch, and\ndescending aorta were localized in chest CT scans, the left cardiac ventricle\nin cardiac CTA scans, and the liver in abdomen CT scans. Localization was\nevaluated using the distances between automatically and manually defined\nreference bounding box centroids and walls.\n  The best results were achieved in localization of structures with clearly\ndefined boundaries (e.g. aortic arch) and the worst when the structure boundary\nwas not clearly visible (e.g. liver). The method was more robust and accurate\nin localization multiple structures.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 06:54:34 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["de Vos", "Bob D.", ""], ["Wolterink", "Jelmer M.", ""], ["de Jong", "Pim A.", ""], ["Leiner", "Tim", ""], ["Viergever", "Max A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1704.05643", "submitter": "Bo Li", "authors": "Bo Li, Huahui Chen, Yucheng Chen, Yuchao Dai, Mingyi He", "title": "Skeleton Boxes: Solving skeleton based action detection with a single\n  deep convolutional neural network", "comments": "4 pages,3 figures, icmew 2017", "journal-ref": "icmew 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition from well-segmented 3D skeleton video has been intensively\nstudied. However, due to the difficulty in representing the 3D skeleton video\nand the lack of training data, action detection from streaming 3D skeleton\nvideo still lags far behind its recognition counterpart and image based object\ndetection. In this paper, we propose a novel approach for this problem, which\nleverages both effective skeleton video encoding and deep regression based\nobject detection from images. Our framework consists of two parts:\nskeleton-based video image mapping, which encodes a skeleton video to a color\nimage in a temporal preserving way, and an end-to-end trainable fast skeleton\naction detector (Skeleton Boxes) based on image detection. Experimental results\non the latest and largest PKU-MMD benchmark dataset demonstrate that our method\noutperforms the state-of-the-art methods with a large margin. We believe our\nidea would inspire and benefit future research in this important area.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 08:16:13 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Li", "Bo", ""], ["Chen", "Huahui", ""], ["Chen", "Yucheng", ""], ["Dai", "Yuchao", ""], ["He", "Mingyi", ""]]}, {"id": "1704.05645", "submitter": "Bo Li", "authors": "Bo Li, Mingyi He, Xuelian Cheng, Yucheng Chen, Yuchao Dai", "title": "Skeleton based action recognition using translation-scale invariant\n  image mapping and multi-scale deep cnn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an image classification based approach for skeleton-based\nvideo action recognition problem. Firstly, A dataset independent\ntranslation-scale invariant image mapping method is proposed, which transformes\nthe skeleton videos to colour images, named skeleton-images. Secondly, A\nmulti-scale deep convolutional neural network (CNN) architecture is proposed\nwhich could be built and fine-tuned on the powerful pre-trained CNNs, e.g.,\nAlexNet, VGGNet, ResNet etal.. Even though the skeleton-images are very\ndifferent from natural images, the fine-tune strategy still works well. At\nlast, we prove that our method could also work well on 2D skeleton video data.\nWe achieve the state-of-the-art results on the popular benchmard datasets e.g.\nNTU RGB+D, UTD-MHAD, MSRC-12, and G3D. Especially on the largest and challenge\nNTU RGB+D, UTD-MHAD, and MSRC-12 dataset, our method outperforms other methods\nby a large margion, which proves the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 08:30:19 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 01:59:13 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Li", "Bo", ""], ["He", "Mingyi", ""], ["Cheng", "Xuelian", ""], ["Chen", "Yucheng", ""], ["Dai", "Yuchao", ""]]}, {"id": "1704.05674", "submitter": "Emanuela Haller", "authors": "Emanuela Haller and Marius Leordeanu", "title": "Unsupervised object segmentation in video by efficient selection of\n  highly probable positive features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an essential problem in computer vision, that of unsupervised\nobject segmentation in video, where a main object of interest in a video\nsequence should be automatically separated from its background. An efficient\nsolution to this task would enable large-scale video interpretation at a high\nsemantic level in the absence of the costly manually labeled ground truth. We\npropose an efficient unsupervised method for generating foreground object\nsoft-segmentation masks based on automatic selection and learning from highly\nprobable positive features. We show that such features can be selected\nefficiently by taking into consideration the spatio-temporal, appearance and\nmotion consistency of the object during the whole observed sequence. We also\nemphasize the role of the contrasting properties between the foreground object\nand its background. Our model is created in two stages: we start from pixel\nlevel analysis, on top of which we add a regression model trained on a\ndescriptor that considers information over groups of pixels and is both\ndiscriminative and invariant to many changes that the object undergoes\nthroughout the video. We also present theoretical properties of our\nunsupervised learning method, that under some mild constraints is guaranteed to\nlearn a correct discriminative classifier even in the unsupervised case. Our\nmethod achieves competitive and even state of the art results on the\nchallenging Youtube-Objects and SegTrack datasets, while being at least one\norder of magnitude faster than the competition. We believe that the competitive\nperformance of our method in practice, along with its theoretical properties,\nconstitute an important step towards solving unsupervised discovery in video.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 10:00:46 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Haller", "Emanuela", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1704.05678", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, and Stefan Winkler", "title": "Design of low-cost, compact and weather-proof whole sky imagers for\n  high-dynamic-range captures", "comments": "Published in Proc. IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS), July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-based whole sky imagers are popular for monitoring cloud formations,\nwhich is necessary for various applications. We present two new Wide Angle\nHigh-Resolution Sky Imaging System (WAHRSIS) models, which were designed\nespecially to withstand the hot and humid climate of Singapore. The first uses\na fully sealed casing, whose interior temperature is regulated using a Peltier\ncooler. The second features a double roof design with ventilation grids on the\nsides, allowing the outside air to flow through the device. Measurements of\ntemperature inside these two devices show their ability to operate in Singapore\nweather conditions. Unlike our original WAHRSIS model, neither uses a\nmechanical sun blocker to prevent the direct sunlight from reaching the camera;\ninstead they rely on high-dynamic-range imaging (HDRI) techniques to reduce the\nglare from the sun.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 10:27:30 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1704.05693", "submitter": "Lior Wolf", "authors": "Lior Wolf, Yaniv Taigman, Adam Polyak", "title": "Unsupervised Creation of Parameterized Avatars", "comments": "v2 -- a change in the references due to a request from authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of mapping an input image to a tied pair consisting of a\nvector of parameters and an image that is created using a graphical engine from\nthe vector of parameters. The mapping's objective is to have the output image\nas similar as possible to the input image. During training, no supervision is\ngiven in the form of matching inputs and outputs.\n  This learning problem extends two literature problems: unsupervised domain\nadaptation and cross domain transfer. We define a generalization bound that is\nbased on discrepancy, and employ a GAN to implement a network solution that\ncorresponds to this bound. Experimentally, our method is shown to solve the\nproblem of automatically creating avatars.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 11:19:45 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 16:10:53 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""], ["Polyak", "Adam", ""]]}, {"id": "1704.05698", "submitter": "Majd Zreik", "authors": "Majd Zreik, Tim Leiner, Bob D. de Vos, Robbert W. van Hamersvelt, Max\n  A. Viergever, Ivana Isgum", "title": "Automatic Segmentation of the Left Ventricle in Cardiac CT Angiography\n  Using Convolutional Neural Network", "comments": "This work has been published as: Zreik, M., Leiner, T., de Vos, B.\n  D., van Hamersvelt, R. W., Viergever, M. A., I\\v{s}gum, I. (2016, April).\n  Automatic segmentation of the left ventricle in cardiac CT angiography using\n  convolutional neural networks. In Biomedical Imaging (ISBI), 2016 IEEE 13th\n  International Symposium on (pp. 40-43). IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate delineation of the left ventricle (LV) is an important step in\nevaluation of cardiac function. In this paper, we present an automatic method\nfor segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation\nis performed in two stages. First, a bounding box around the LV is detected\nusing a combination of three convolutional neural networks (CNNs).\nSubsequently, to obtain the segmentation of the LV, voxel classification is\nperformed within the defined bounding box using a CNN. The study included CCTA\nscans of sixty patients, fifty scans were used to train the CNNs for the LV\nlocalization, five scans were used to train LV segmentation and the remaining\nfive scans were used for testing the method. Automatic segmentation resulted in\nthe average Dice coefficient of 0.85 and mean absolute surface distance of 1.1\nmm. The results demonstrate that automatic segmentation of the LV in CCTA scans\nusing voxel classification with convolutional neural networks is feasible.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 11:29:59 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Zreik", "Majd", ""], ["Leiner", "Tim", ""], ["de Vos", "Bob D.", ""], ["van Hamersvelt", "Robbert W.", ""], ["Viergever", "Max A.", ""], ["Isgum", "Ivana", ""]]}, {"id": "1704.05708", "submitter": "Usman Mahmood Khan Usman Mahmood Khan", "authors": "U. M. Khan, Z. Kabir, S. A. Hassan, S. H. Ahmed", "title": "A Deep Learning Framework using Passive WiFi Sensing for Respiration\n  Monitoring", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end deep learning framework using passive WiFi\nsensing to classify and estimate human respiration activity. A passive radar\ntest-bed is used with two channels where the first channel provides the\nreference WiFi signal, whereas the other channel provides a surveillance signal\nthat contains reflections from the human target. Adaptive filtering is\nperformed to make the surveillance signal source-data invariant by eliminating\nthe echoes of the direct transmitted signal. We propose a novel convolutional\nneural network to classify the complex time series data and determine if it\ncorresponds to a breathing activity, followed by a random forest estimator to\ndetermine breathing rate. We collect an extensive dataset to train the learning\nmodels and develop reference benchmarks for the future studies in the field.\nBased on the results, we conclude that deep learning techniques coupled with\npassive radars offer great potential for end-to-end human activity recognition.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:35:17 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Khan", "U. M.", ""], ["Kabir", "Z.", ""], ["Hassan", "S. A.", ""], ["Ahmed", "S. H.", ""]]}, {"id": "1704.05712", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker\n  Fischer", "title": "Universal Adversarial Perturbations Against Semantic Image Segmentation", "comments": "Final version for ICCV including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is remarkably successful on perceptual tasks, it was also\nshown to be vulnerable to adversarial perturbations of the input. These\nperturbations denote noise added to the input that was generated specifically\nto fool the system while being quasi-imperceptible for humans. More severely,\nthere even exist universal perturbations that are input-agnostic but fool the\nnetwork on the majority of inputs. While recent work has focused on image\nclassification, this work proposes attacks against semantic image segmentation:\nwe present an approach for generating (universal) adversarial perturbations\nthat make the network yield a desired target segmentation as output. We show\nempirically that there exist barely perceptible universal noise patterns which\nresult in nearly the same predicted segmentation for arbitrary inputs.\nFurthermore, we also show the existence of universal noise which removes a\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\nsegmentation mostly unchanged otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:48:52 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 08:35:25 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 18:55:54 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Kumar", "Mummadi Chaithanya", ""], ["Brox", "Thomas", ""], ["Fischer", "Volker", ""]]}, {"id": "1704.05737", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Karteek Alahari, Cordelia Schmid", "title": "Learning Video Object Segmentation with Visual Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of segmenting moving objects in unconstrained\nvideos. We introduce a novel two-stream neural network with an explicit memory\nmodule to achieve this. The two streams of the network encode spatial and\ntemporal features in a video sequence respectively, while the memory module\ncaptures the evolution of objects over time. The module to build a \"visual\nmemory\" in video, i.e., a joint representation of all the video frames, is\nrealized with a convolutional recurrent unit learned from a small number of\ntraining video sequences. Given a video frame as input, our approach assigns\neach pixel an object or background label based on the learned spatio-temporal\nfeatures as well as the \"visual memory\" specific to the video, acquired\nautomatically without any manually-annotated frames. The visual memory is\nimplemented with convolutional gated recurrent units, which allows to propagate\nspatial information over time. We evaluate our method extensively on two\nbenchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show\nstate-of-the-art results. For example, our approach outperforms the top method\non the DAVIS dataset by nearly 6%. We also provide an extensive ablative\nanalysis to investigate the influence of each component in the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 14:09:49 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 13:26:13 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1704.05754", "submitter": "Federico Magliani", "authors": "Federico Magliani, Navid Mahmoudian Bidgoli, Andrea Prati", "title": "A location-aware embedding technique for accurate landmark recognition", "comments": "6 pages, 5 figures, ICDSC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state of the research in landmark recognition highlights the good\naccuracy which can be achieved by embedding techniques, such as Fisher vector\nand VLAD. All these techniques do not exploit spatial information, i.e.\nconsider all the features and the corresponding descriptors without embedding\ntheir location in the image. This paper presents a new variant of the\nwell-known VLAD (Vector of Locally Aggregated Descriptors) embedding technique\nwhich accounts, at a certain degree, for the location of features. The driving\nmotivation comes from the observation that, usually, the most interesting part\nof an image (e.g., the landmark to be recognized) is almost at the center of\nthe image, while the features at the borders are irrelevant features which do\nno depend on the landmark. The proposed variant, called locVLAD (location-aware\nVLAD), computes the mean of the two global descriptors: the VLAD executed on\nthe entire original image, and the one computed on a cropped image which\nremoves a certain percentage of the image borders. This simple variant shows an\naccuracy greater than the existing state-of-the-art approach. Experiments are\nconducted on two public datasets (ZuBuD and Holidays) which are used both for\ntraining and testing. Morever a more balanced version of ZuBuD is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 14:45:23 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Magliani", "Federico", ""], ["Bidgoli", "Navid Mahmoudian", ""], ["Prati", "Andrea", ""]]}, {"id": "1704.05773", "submitter": "David V\\'azquez-Pad\\'in", "authors": "David V\\'azquez-Pad\\'in, Fernando P\\'erez-Gonz\\'alez, Pedro\n  Comesa\\~na-Alfaro", "title": "Derivation of the Asymptotic Eigenvalue Distribution for Causal 2D-AR\n  Models under Upscaling", "comments": "This technical report complements the work by David\n  V\\'azquez-Pad\\'in, Fernando P\\'erez-Gonz\\'alez, and Pedro Comesa\\~na-Alfaro,\n  \"A random matrix approach to the forensic analysis of upscaled images,\"\n  submitted to IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": "UV-TSC-DVP-19042017", "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes the derivation of the asymptotic eigenvalue\ndistribution for causal 2D-AR models under an upscaling scenario. Specifically,\nit tackles the analytical derivation of the asymptotic eigenvalue distribution\nof the sample autocorrelation matrix corresponding to genuine and upscaled\nimages. It also includes the pseudocode of the derived approaches for\nresampling detection and resampling factor estimation that are based on this\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 15:25:45 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["V\u00e1zquez-Pad\u00edn", "David", ""], ["P\u00e9rez-Gonz\u00e1lez", "Fernando", ""], ["Comesa\u00f1a-Alfaro", "Pedro", ""]]}, {"id": "1704.05775", "submitter": "Pierre Baque", "authors": "Pierre Baqu\\'e, Fran\\c{c}ois Fleuret and Pascal Fua", "title": "Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People detection in single 2D images has improved greatly in recent years.\nHowever, comparatively little of this progress has percolated into multi-camera\nmulti-people tracking algorithms, whose performance still degrades severely\nwhen scenes become very crowded. In this work, we introduce a new architecture\nthat combines Convolutional Neural Nets and Conditional Random Fields to\nexplicitly model those ambiguities. One of its key ingredients are high-order\nCRF terms that model potential occlusions and give our approach its robustness\neven when many people are present. Our model is trained end-to-end and we show\nthat it outperforms several state-of-art algorithms on challenging scenes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 15:30:20 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 05:39:50 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Baqu\u00e9", "Pierre", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""]]}, {"id": "1704.05776", "submitter": "Jimmy Ren", "authors": "Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong\n  Yan, Yu-Wing Tai, Li Xu", "title": "Accurate Single Stage Detector Using Recurrent Rolling Convolution", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the recent successful methods in accurate object detection and\nlocalization used some variants of R-CNN style two stage Convolutional Neural\nNetworks (CNN) where plausible regions were proposed in the first stage then\nfollowed by a second stage for decision refinement. Despite the simplicity of\ntraining and the efficiency in deployment, the single stage detection methods\nhave not been as competitive when evaluated in benchmarks consider mAP for high\nIoU thresholds. In this paper, we proposed a novel single stage end-to-end\ntrainable object detection network to overcome this limitation. We achieved\nthis by introducing Recurrent Rolling Convolution (RRC) architecture over\nmulti-scale feature maps to construct object classifiers and bounding box\nregressors which are \"deep in context\". We evaluated our method in the\nchallenging KITTI dataset which measures methods under IoU threshold of 0.7. We\nshowed that with RRC, a single reduced VGG-16 based model already significantly\noutperformed all the previously published results. At the time this paper was\nwritten our models ranked the first in KITTI car detection (the hard level),\nthe first in cyclist detection and the second in pedestrian detection. These\nresults were not reached by the previous single stage methods. The code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 15:31:01 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Ren", "Jimmy", ""], ["Chen", "Xiaohao", ""], ["Liu", "Jianbo", ""], ["Sun", "Wenxiu", ""], ["Pang", "Jiahao", ""], ["Yan", "Qiong", ""], ["Tai", "Yu-Wing", ""], ["Xu", "Li", ""]]}, {"id": "1704.05796", "submitter": "David Bau", "authors": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba", "title": "Network Dissection: Quantifying Interpretability of Deep Visual\n  Representations", "comments": "First two authors contributed equally. Oral presentation at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 16:10:38 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Bau", "David", ""], ["Zhou", "Bolei", ""], ["Khosla", "Aditya", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1704.05817", "submitter": "Wenbin Li", "authors": "Wenbin Li, Da Chen, Zhihan Lv, Yan Yan, Darren Cosker", "title": "Learn to Model Motion from Blurry Footages", "comments": "Preprint of our paper accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is difficult to recover the motion field from a real-world footage given a\nmixture of camera shake and other photometric effects. In this paper we propose\na hybrid framework by interleaving a Convolutional Neural Network (CNN) and a\ntraditional optical flow energy. We first conduct a CNN architecture using a\nnovel learnable directional filtering layer. Such layer encodes the angle and\ndistance similarity matrix between blur and camera motion, which is able to\nenhance the blur features of the camera-shake footages. The proposed CNNs are\nthen integrated into an iterative optical flow framework, which enable the\ncapability of modelling and solving both the blind deconvolution and the\noptical flow estimation problems simultaneously. Our framework is trained\nend-to-end on a synthetic dataset and yields competitive precision and\nperformance against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 16:54:54 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Li", "Wenbin", ""], ["Chen", "Da", ""], ["Lv", "Zhihan", ""], ["Yan", "Yan", ""], ["Cosker", "Darren", ""]]}, {"id": "1704.05831", "submitter": "Ruben Villegas", "authors": "Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin,\n  Honglak Lee", "title": "Learning to Generate Long-term Future via Hierarchical Prediction", "comments": "International Conference on Machine Learning (ICML) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 17:25:56 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 04:35:39 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 00:18:01 GMT"}, {"version": "v4", "created": "Sun, 13 Aug 2017 03:31:18 GMT"}, {"version": "v5", "created": "Mon, 8 Jan 2018 01:24:36 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Villegas", "Ruben", ""], ["Yang", "Jimei", ""], ["Zou", "Yuliang", ""], ["Sohn", "Sungryull", ""], ["Lin", "Xunyu", ""], ["Lee", "Honglak", ""]]}, {"id": "1704.05832", "submitter": "Daniele De Gregorio", "authors": "Daniele De Gregorio, and Luigi Di Stefano", "title": "SkiMap: An Efficient Mapping Framework for Robot Navigation", "comments": "Accepted by International Conference on Robotics and Automation\n  (ICRA) 2017. This is the submitted version. The final published version may\n  be slightly different", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel mapping framework for robot navigation which features a\nmulti-level querying system capable to obtain rapidly representations as\ndiverse as a 3D voxel grid, a 2.5D height map and a 2D occupancy grid. These\nare inherently embedded into a memory and time efficient core data structure\norganized as a Tree of SkipLists. Compared to the well-known Octree\nrepresentation, our approach exhibits a better time efficiency, thanks to its\nsimple and highly parallelizable computational structure, and a similar memory\nfootprint when mapping large workspaces. Peculiarly within the realm of mapping\nfor robot navigation, our framework supports realtime erosion and\nre-integration of measurements upon reception of optimized poses from the\nsensor tracker, so as to improve continuously the accuracy of the map.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 17:29:04 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["De Gregorio", "Daniele", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1704.05838", "submitter": "Yijun Li", "authors": "Yijun Li, Sifei Liu, Jimei Yang, Ming-Hsuan Yang", "title": "Generative Face Completion", "comments": "Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective face completion algorithm using a deep\ngenerative model. Different from well-studied background completion, the face\ncompletion task is more challenging as it often requires to generate\nsemantically new pixels for the missing key components (e.g., eyes and mouths)\nthat contain large appearance variations. Unlike existing nonparametric\nalgorithms that search for patches to synthesize, our algorithm directly\ngenerates contents for missing regions based on a neural network. The model is\ntrained with a combination of a reconstruction loss, two adversarial losses and\na semantic parsing loss, which ensures pixel faithfulness and local-global\ncontents consistency. With extensive experimental results, we demonstrate\nqualitatively and quantitatively that our model is able to deal with a large\narea of missing pixels in arbitrary shapes and generate realistic face\ncompletion results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 17:53:29 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Li", "Yijun", ""], ["Liu", "Sifei", ""], ["Yang", "Jimei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1704.05939", "submitter": "Karel Lenc", "authors": "Vassileios Balntas and Karel Lenc and Andrea Vedaldi and Krystian\n  Mikolajczyk", "title": "HPatches: A benchmark and evaluation of handcrafted and learned local\n  descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel benchmark for evaluating local image\ndescriptors. We demonstrate that the existing datasets and evaluation protocols\ndo not specify unambiguously all aspects of evaluation, leading to ambiguities\nand inconsistencies in results reported in the literature. Furthermore, these\ndatasets are nearly saturated due to the recent improvements in local\ndescriptors obtained by learning them from large annotated datasets. Therefore,\nwe introduce a new large dataset suitable for training and testing modern\ndescriptors, together with strictly defined evaluation protocols in several\ntasks such as matching, retrieval and classification. This allows for more\nrealistic, and thus more reliable comparisons in different application\nscenarios. We evaluate the performance of several state-of-the-art descriptors\nand analyse their properties. We show that a simple normalisation of\ntraditional hand-crafted descriptors can boost their performance to the level\nof deep learning based descriptors within a realistic benchmarks evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 21:37:03 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Balntas", "Vassileios", ""], ["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1704.05952", "submitter": "Raydonal Ospina", "authors": "Luis Gomez, Raydonal Ospina and Alejandro C. Frery", "title": "Unassisted Quantitative Evaluation Of Despeckling Filters", "comments": "Accepted for publication in Remote Sensing - Open Access Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAR (Synthetic Aperture Radar) imaging plays a central role in Remote Sensing\ndue to, among other important features, its ability to provide high-resolution,\nday-and-night and almost weather-independent images. SAR images are affected\nfrom a granular contamination, speckle, that can be described by a\nmultiplicative model. Many despeckling techniques have been proposed in the\nliterature, as well as measures of the quality of the results they provide.\nAssuming the multiplicative model, the observed image $Z$ is the product of two\nindependent fields: the backscatter $X$ and the speckle $Y$. The result of any\nspeckle filter is $\\widehat X$, an estimator of the backscatter $X$, based\nsolely on the observed data $Z$. An ideal estimator would be the one for which\nthe ratio of the observed image to the filtered one $I=Z/\\widehat X$ is only\nspeckle: a collection of independent identically distributed samples from Gamma\nvariates. We, then, assess the quality of a filter by the closeness of $I$ to\nthe hypothesis that it is adherent to the statistical properties of pure\nspeckle. We analyze filters through the ratio image they produce with regards\nto first- and second-order statistics: the former check marginal properties,\nwhile the latter verifies lack of structure. A new quantitative image-quality\nindex is then defined, and applied to state-of-the-art despeckling filters.\nThis new measure provides consistent results with commonly used quality\nmeasures (equivalent number of looks, PSNR, MSSIM, $\\beta$ edge correlation,\nand preservation of the mean), and ranks the filters results also in agreement\nwith their visual analysis. We conclude our study showing that the proposed\nmeasure can be successfully used to optimize the (often many) parameters that\ndefine a speckle filter.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 23:01:30 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Gomez", "Luis", ""], ["Ospina", "Raydonal", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1704.05959", "submitter": "Beipeng Mu", "authors": "Beipeng Mu, Shih-Yuan Liu, Liam Paull, John Leonard, Jonathan How", "title": "SLAM with Objects using a Nonparametric Pose Graph", "comments": "published at IROS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Mapping and self-localization in unknown environments are fundamental\ncapabilities in many robotic applications. These tasks typically involve the\nidentification of objects as unique features or landmarks, which requires the\nobjects both to be detected and then assigned a unique identifier that can be\nmaintained when viewed from different perspectives and in different images. The\n\\textit{data association} and \\textit{simultaneous localization and mapping}\n(SLAM) problems are, individually, well-studied in the literature. But these\ntwo problems are inherently tightly coupled, and that has not been\nwell-addressed. Without accurate SLAM, possible data associations are\ncombinatorial and become intractable easily. Without accurate data association,\nthe error of SLAM algorithms diverge easily. This paper proposes a novel\nnonparametric pose graph that models data association and SLAM in a single\nframework. An algorithm is further introduced to alternate between inferring\ndata association and performing SLAM. Experimental results show that our\napproach has the new capability of associating object detections and localizing\nobjects at the same time, leading to significantly better performance on both\nthe data association and SLAM problems than achieved by considering only one\nand ignoring imperfections in the other.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 23:54:57 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Mu", "Beipeng", ""], ["Liu", "Shih-Yuan", ""], ["Paull", "Liam", ""], ["Leonard", "John", ""], ["How", "Jonathan", ""]]}, {"id": "1704.06001", "submitter": "Pooya Khorrami", "authors": "Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad\n  Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A. Hasegawa-Johnson, Roy H.\n  Campbell, Thomas S. Huang", "title": "Fast Generation for Convolutional Autoregressive Models", "comments": "Accepted at ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional autoregressive models have recently demonstrated\nstate-of-the-art performance on a number of generation tasks. While fast,\nparallel training methods have been crucial for their success, generation is\ntypically implemented in a na\\\"{i}ve fashion where redundant computations are\nunnecessarily repeated. This results in slow generation, making such models\ninfeasible for production environments. In this work, we describe a method to\nspeed up generation in convolutional autoregressive models. The key idea is to\ncache hidden states to avoid redundant computation. We apply our fast\ngeneration method to the Wavenet and PixelCNN++ models and achieve up to\n$21\\times$ and $183\\times$ speedups respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 04:13:21 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Ramachandran", "Prajit", ""], ["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Babaeizadeh", "Mohammad", ""], ["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Hasegawa-Johnson", "Mark A.", ""], ["Campbell", "Roy H.", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1704.06010", "submitter": "Karim Ahmed", "authors": "Karim Ahmed and Lorenzo Torresani", "title": "BranchConnect: Large-Scale Visual Recognition with Learned Branch\n  Connections", "comments": "WACV 2018", "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV)\n  2018, pp. 1244-1253", "doi": "10.1109/WACV.2018.00141", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an architecture for large-scale image categorization that\nenables the end-to-end learning of separate visual features for the different\nclasses to distinguish. The proposed model consists of a deep CNN shaped like a\ntree. The stem of the tree includes a sequence of convolutional layers common\nto all classes. The stem then splits into multiple branches implementing\nparallel feature extractors, which are ultimately connected to the final\nclassification layer via learned gated connections. These learned gates\ndetermine for each individual class the subset of features to use. Such a\nscheme naturally encourages the learning of a heterogeneous set of specialized\nfeatures through the separate branches and it allows each class to use the\nsubset of features that are optimal for its recognition. We show the generality\nof our proposed method by reshaping several popular CNNs from the literature\ninto our proposed architecture. Our experiments on the CIFAR100, CIFAR10, and\nSynth datasets show that in each case our resulting model yields a substantial\nimprovement in accuracy over the original CNN. Our empirical analysis also\nsuggests that our scheme acts as a form of beneficial regularization improving\ngeneralization performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 04:48:58 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 14:28:40 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2018 18:56:25 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ahmed", "Karim", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1704.06018", "submitter": "Erkan Bostanci", "authors": "Erkan Bostanci and Nadia Kanwal and Betul Bostanci and Mehmet Serdar\n  Guzel", "title": "A Fuzzy Brute Force Matching Method for Binary Image Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching of binary image features is an important step in many different\ncomputer vision applications. Conventionally, an arbitrary threshold is used to\nidentify a correct match from incorrect matches using Hamming distance which\nmay improve or degrade the matching results for different input images. This is\nmainly due to the image content which is affected by the scene, lighting and\nimaging conditions. This paper presents a fuzzy logic based approach for brute\nforce matching of image features to overcome this situation. The method was\ntested using a well-known image database with known ground truth. The approach\nis shown to produce a higher number of correct matches when compared against\nconstant distance thresholds. The nature of fuzzy logic which allows the\nvagueness of information and tolerance to errors has been successfully\nexploited in an image processing context. The uncertainty arising from the\nimaging conditions has been overcome with the use of compact fuzzy matching\nmembership functions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 05:29:06 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bostanci", "Erkan", ""], ["Kanwal", "Nadia", ""], ["Bostanci", "Betul", ""], ["Guzel", "Mehmet Serdar", ""]]}, {"id": "1704.06020", "submitter": "Xun Yang", "authors": "Xun Yang, Meng Wang, Richang Hong, Qi Tian, Yong Rui", "title": "Enhancing Person Re-identification in a Self-trained Subspace", "comments": "Accepted by ACM Transactions on Multimedia Computing, Communications,\n  and Applications (TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the promising progress made in recent years, person re-identification\n(re-ID) remains a challenging task due to the complex variations in human\nappearances from different camera views. For this challenging problem, a large\nvariety of algorithms have been developed in the fully-supervised setting,\nrequiring access to a large amount of labeled training data. However, the main\nbottleneck for fully-supervised re-ID is the limited availability of labeled\ntraining samples. To address this problem, in this paper, we propose a\nself-trained subspace learning paradigm for person re-ID which effectively\nutilizes both labeled and unlabeled data to learn a discriminative subspace\nwhere person images across disjoint camera views can be easily matched. The\nproposed approach first constructs pseudo pairwise relationships among\nunlabeled persons using the k-nearest neighbors algorithm. Then, with the\npseudo pairwise relationships, the unlabeled samples can be easily combined\nwith the labeled samples to learn a discriminative projection by solving an\neigenvalue problem. In addition, we refine the pseudo pairwise relationships\niteratively, which further improves the learning performance. A multi-kernel\nembedding strategy is also incorporated into the proposed approach to cope with\nthe non-linearity in person's appearance and explore the complementation of\nmultiple kernels. In this way, the performance of person re-ID can be greatly\nenhanced when training data are insufficient. Experimental results on six\nwidely-used datasets demonstrate the effectiveness of our approach and its\nperformance can be comparable to the reported results of most state-of-the-art\nfully-supervised methods while using much fewer labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 05:43:05 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 00:28:52 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yang", "Xun", ""], ["Wang", "Meng", ""], ["Hong", "Richang", ""], ["Tian", "Qi", ""], ["Rui", "Yong", ""]]}, {"id": "1704.06033", "submitter": "Hongyoon Choi Dr", "authors": "Hongyoon Choi, Kyong Hwan Jin", "title": "Predicting Cognitive Decline with Deep Learning of Brain Metabolism and\n  Amyloid Imaging", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For effective treatment of Alzheimer disease (AD), it is important to\nidentify subjects who are most likely to exhibit rapid cognitive decline.\nHerein, we developed a novel framework based on a deep convolutional neural\nnetwork which can predict future cognitive decline in mild cognitive impairment\n(MCI) patients using flurodeoxyglucose and florbetapir positron emission\ntomography (PET). The architecture of the network only relies on baseline PET\nstudies of AD and normal subjects as the training dataset. Feature extraction\nand complicated image preprocessing including nonlinear warping are unnecessary\nfor our approach. Accuracy of prediction (84.2%) for conversion to AD in MCI\npatients outperformed conventional feature-based quantification approaches. ROC\nanalyses revealed that performance of CNN-based approach was significantly\nhigher than that of the conventional quantification methods (p < 0.05). Output\nscores of the network were strongly correlated with the longitudinal change in\ncognitive measurements. These results show the feasibility of deep learning as\na tool for predicting disease outcome using brain images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 07:33:18 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Choi", "Hongyoon", ""], ["Jin", "Kyong Hwan", ""]]}, {"id": "1704.06036", "submitter": "Jack Valmadre", "authors": "Jack Valmadre, Luca Bertinetto, Jo\\~ao F. Henriques, Andrea Vedaldi,\n  Philip H. S. Torr", "title": "End-to-end representation learning for Correlation Filter based tracking", "comments": "To appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Correlation Filter is an algorithm that trains a linear template to\ndiscriminate between images and their translations. It is well suited to object\ntracking because its formulation in the Fourier domain provides a fast\nsolution, enabling the detector to be re-trained once per frame. Previous works\nthat use the Correlation Filter, however, have adopted features that were\neither manually designed or trained for a different task. This work is the\nfirst to overcome this limitation by interpreting the Correlation Filter\nlearner, which has a closed-form solution, as a differentiable layer in a deep\nneural network. This enables learning deep features that are tightly coupled to\nthe Correlation Filter. Experiments illustrate that our method has the\nimportant practical benefit of allowing lightweight architectures to achieve\nstate-of-the-art performance at high framerates.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 07:51:27 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Valmadre", "Jack", ""], ["Bertinetto", "Luca", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Vedaldi", "Andrea", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1704.06040", "submitter": "Prasad Sudhakar", "authors": "Hariharan Ravishankar and Prasad Sudhakar and Rahul Venkataramani and\n  Sheshadri Thiruvenkadam and Pavan Annangi and Narayanan Babu and Vivek Vaidya", "title": "Understanding the Mechanisms of Deep Transfer Learning for Medical\n  Images", "comments": "Published in MICCAI Workshop on Deep Learning in Medical Image\n  Analysis, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The ability to automatically learn task specific feature representations has\nled to a huge success of deep learning methods. When large training data is\nscarce, such as in medical imaging problems, transfer learning has been very\neffective. In this paper, we systematically investigate the process of\ntransferring a Convolutional Neural Network, trained on ImageNet images to\nperform image classification, to kidney detection problem in ultrasound images.\nWe study how the detection performance depends on the extent of transfer. We\nshow that a transferred and tuned CNN can outperform a state-of-the-art feature\nengineered pipeline and a hybridization of these two techniques achieves 20\\%\nhigher performance. We also investigate how the evolution of intermediate\nresponse images from our network. Finally, we compare these responses to\nstate-of-the-art image processing filters in order to gain greater insight into\nhow transfer learning is able to effectively manage widely varying imaging\nregimes.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 08:04:52 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Ravishankar", "Hariharan", ""], ["Sudhakar", "Prasad", ""], ["Venkataramani", "Rahul", ""], ["Thiruvenkadam", "Sheshadri", ""], ["Annangi", "Pavan", ""], ["Babu", "Narayanan", ""], ["Vaidya", "Vivek", ""]]}, {"id": "1704.06065", "submitter": "Bob De Vos", "authors": "Bob D. de Vos, Floris F. Berendsen, Max A. Viergever, Marius Staring,\n  Ivana I\\v{s}gum", "title": "End-to-End Unsupervised Deformable Image Registration with a\n  Convolutional Neural Network", "comments": null, "journal-ref": "DLMIA/ML-CDS@MICCAI 2017", "doi": "10.1007/978-3-319-67558-9_24", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a deep learning network for deformable image\nregistration (DIRNet). The DIRNet consists of a convolutional neural network\n(ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet\nanalyzes a pair of fixed and moving images and outputs parameters for the\nspatial transformer, which generates the displacement vector field that enables\nthe resampler to warp the moving image to the fixed image. The DIRNet is\ntrained end-to-end by unsupervised optimization of a similarity metric between\ninput image pairs. A trained DIRNet can be applied to perform registration on\nunseen image pairs in one pass, thus non-iteratively. Evaluation was performed\nwith registration of images of handwritten digits (MNIST) and cardiac cine MR\nscans (Sunnybrook Cardiac Data). The results demonstrate that registration with\nDIRNet is as accurate as a conventional deformable image registration method\nwith substantially shorter execution times.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 09:40:50 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["de Vos", "Bob D.", ""], ["Berendsen", "Floris F.", ""], ["Viergever", "Max A.", ""], ["Staring", "Marius", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1704.06109", "submitter": "Yashar Deldjoo", "authors": "Yashar Deldjoo, Massimo Quadrana, Mehdi Elahi, Paolo Cremonesi", "title": "Using Mise-En-Sc\\`ene Visual Features based on MPEG-7 and Deep Learning\n  for Movie Recommendation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item features play an important role in movie recommender systems, where\nrecommendations can be generated by using explicit or implicit preferences of\nusers on traditional features (attributes) such as tag, genre, and cast.\nTypically, movie features are human-generated, either editorially (e.g., genre\nand cast) or by leveraging the wisdom of the crowd (e.g., tag), and as such,\nthey are prone to noise and are expensive to collect. Moreover, these features\nare often rare or absent for new items, making it difficult or even impossible\nto provide good quality recommendations.\n  In this paper, we show that user's preferences on movies can be better\ndescribed in terms of the mise-en-sc\\`ene features, i.e., the visual aspects of\na movie that characterize design, aesthetics and style (e.g., colors,\ntextures). We use both MPEG-7 visual descriptors and Deep Learning hidden\nlayers as example of mise-en-sc\\`ene features that can visually describe\nmovies. Interestingly, mise-en-sc\\`ene features can be computed automatically\nfrom video files or even from trailers, offering more flexibility in handling\nnew items, avoiding the need for costly and error-prone human-based tagging,\nand providing good scalability.\n  We have conducted a set of experiments on a large catalogue of 4K movies.\nResults show that recommendations based on mise-en-sc\\`ene features\nconsistently provide the best performance with respect to richer sets of more\ntraditional features, such as genre and tag.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 12:33:48 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Quadrana", "Massimo", ""], ["Elahi", "Mehdi", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1704.06176", "submitter": "Cem Deniz", "authors": "Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, James\n  S. Babb, Stephen Honig, Kyunghyun Cho, and Gregory Chang", "title": "Segmentation of the Proximal Femur from MR Images using Deep\n  Convolutional Neural Networks", "comments": "This is a pre-print of an article published in Scientific Reports.\n  The final authenticated version is available online at:\n  https://doi.org/10.1038/s41598-018-34817-6", "journal-ref": "Scientific Reports, volume 8, Article number: 16485 (2018)", "doi": "10.1038/s41598-018-34817-6", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) has been proposed as a complimentary method\nto measure bone quality and assess fracture risk. However, manual segmentation\nof MR images of bone is time-consuming, limiting the use of MRI measurements in\nthe clinical practice. The purpose of this paper is to present an automatic\nproximal femur segmentation method that is based on deep convolutional neural\nnetworks (CNNs). This study had institutional review board approval and written\ninformed consent was obtained from all subjects. A dataset of volumetric\nstructural MR images of the proximal femur from 86 subject were\nmanually-segmented by an expert. We performed experiments by training two\ndifferent CNN architectures with multiple number of initial feature maps and\nlayers, and tested their segmentation performance against the gold standard of\nmanual segmentations using four-fold cross-validation. Automatic segmentation\nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05\nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN\narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The\nhigh segmentation accuracy provided by CNNs has the potential to help bring the\nuse of structural MRI measurements of bone quality into clinical practice for\nmanagement of osteoporosis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 14:54:29 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 21:15:40 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 20:36:28 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 18:32:16 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 14:46:00 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Deniz", "Cem M.", ""], ["Xiang", "Siyuan", ""], ["Hallyburton", "Spencer", ""], ["Welbeck", "Arakua", ""], ["Babb", "James S.", ""], ["Honig", "Stephen", ""], ["Cho", "Kyunghyun", ""], ["Chang", "Gregory", ""]]}, {"id": "1704.06178", "submitter": "Andrea Esuli", "authors": "Fabio Carrara, Andrea Esuli, Fabrizio Falchi, Alejandro Moreo\n  Fern\\'andez", "title": "Exploring epoch-dependent stochastic residual networks", "comments": "Preliminary report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed stochastic residual networks selectively activate or\nbypass the layers during training, based on independent stochastic choices,\neach of which following a probability distribution that is fixed in advance. In\nthis paper we present a first exploration on the use of an epoch-dependent\ndistribution, starting with a higher probability of bypassing deeper layers and\nthen activating them more frequently as training progresses. Preliminary\nresults are mixed, yet they show some potential of adding an epoch-dependent\nmanagement of distributions, worth of further investigation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:08:28 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Carrara", "Fabio", ""], ["Esuli", "Andrea", ""], ["Falchi", "Fabrizio", ""], ["Fern\u00e1ndez", "Alejandro Moreo", ""]]}, {"id": "1704.06189", "submitter": "Dim Papadopoulos P", "authors": "Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio\n  Ferrari", "title": "Training object class detectors with click supervision", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training object class detectors typically requires a large set of images with\nobjects annotated by bounding boxes. However, manually drawing bounding boxes\nis very time consuming. In this paper we greatly reduce annotation time by\nproposing center-click annotations: we ask annotators to click on the center of\nan imaginary bounding box which tightly encloses the object instance. We then\nincorporate these clicks into existing Multiple Instance Learning techniques\nfor weakly supervised object localization, to jointly localize object bounding\nboxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS\nCOCO show that: (1) our scheme delivers high-quality detectors, performing\nsubstantially better than those produced by weakly supervised techniques, with\na modest extra annotation effort; (2) these detectors in fact perform in a\nrange close to those trained from manually drawn bounding boxes; (3) as the\ncenter-click task is very fast, our scheme reduces total annotation time by 9x\nto 18x.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:31:48 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 17:19:38 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Papadopoulos", "Dim P.", ""], ["Uijlings", "Jasper R. R.", ""], ["Keller", "Frank", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1704.06196", "submitter": "Rui Zhao", "authors": "Rui Zhao and Raymond H. Chan", "title": "A Nuclear-norm Model for Multi-Frame Super-Resolution Reconstruction\n  from Video Clips", "comments": "12 pages, 7 numberical examples, 12 figure groups, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variational approach to obtain super-resolution images from\nmultiple low-resolution frames extracted from video clips. First the\ndisplacement between the low-resolution frames and the reference frame are\ncomputed by an optical flow algorithm. Then a low-rank model is used to\nconstruct the reference frame in high-resolution by incorporating the\ninformation of the low-resolution frames. The model has two terms: a 2-norm\ndata fidelity term and a nuclear-norm regularization term. Alternating\ndirection method of multipliers is used to solve the model. Comparison of our\nmethods with other models on synthetic and real video clips show that our\nresulting images are more accurate with less artifacts. It also provides much\nfiner and discernable details.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 17:17:34 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Zhao", "Rui", ""], ["Chan", "Raymond H.", ""]]}, {"id": "1704.06228", "submitter": "Yuanjun Xiong", "authors": "Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, Dahua\n  Lin", "title": "Temporal Action Detection with Structured Segment Networks", "comments": "To appear in ICCV2017. Code & models available at\n  http://yjxiong.me/others/ssn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting actions in untrimmed videos is an important yet challenging task.\nIn this paper, we present the structured segment network (SSN), a novel\nframework which models the temporal structure of each action instance via a\nstructured temporal pyramid. On top of the pyramid, we further introduce a\ndecomposed discriminative model comprising two classifiers, respectively for\nclassifying actions and determining completeness. This allows the framework to\neffectively distinguish positive proposals from background or incomplete ones,\nthus leading to both accurate recognition and localization. These components\nare integrated into a unified network that can be efficiently trained in an\nend-to-end fashion. Additionally, a simple yet effective temporal action\nproposal scheme, dubbed temporal actionness grouping (TAG) is devised to\ngenerate high quality action proposals. On two challenging benchmarks, THUMOS14\nand ActivityNet, our method remarkably outperforms previous state-of-the-art\nmethods, demonstrating superior accuracy and strong adaptivity in handling\nactions with various temporal structures.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 16:51:45 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 08:43:11 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Zhao", "Yue", ""], ["Xiong", "Yuanjun", ""], ["Wang", "Limin", ""], ["Wu", "Zhirong", ""], ["Tang", "Xiaoou", ""], ["Lin", "Dahua", ""]]}, {"id": "1704.06244", "submitter": "Xi Yin", "authors": "Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, Manmohan Chandraker", "title": "Towards Large-Pose Face Frontalization in the Wild", "comments": "To appear at ICCV2017. Details refer to\n  http://cvlab.cse.msu.edu/project-face-frontalization.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in face recognition using deep learning, severe\naccuracy drops are observed for large pose variations in unconstrained\nenvironments. Learning pose-invariant features is one solution, but needs\nexpensively labeled large-scale data and carefully designed feature learning\nalgorithms. In this work, we focus on frontalizing faces in the wild under\nvarious head poses, including extreme profile views. We propose a novel deep 3D\nMorphable Model (3DMM) conditioned Face Frontalization Generative Adversarial\nNetwork (GAN), termed as FF-GAN, to generate neutral head pose face images. Our\nframework differs from both traditional GANs and 3DMM based modeling.\nIncorporating 3DMM into the GAN structure provides shape and appearance priors\nfor fast convergence with less training data, while also supporting end-to-end\ntraining. The 3DMM-conditioned GAN employs not only the discriminator and\ngenerator loss but also a new masked symmetry loss to retain visual quality\nunder occlusions, besides an identity loss to recover high frequency\ninformation. Experiments on face recognition, landmark localization and 3D\nreconstruction consistently show the advantage of our frontalization method on\nfaces in the wild datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 17:36:41 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 00:37:29 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 18:12:07 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Yin", "Xi", ""], ["Yu", "Xiang", ""], ["Sohn", "Kihyuk", ""], ["Liu", "Xiaoming", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1704.06254", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, Jitendra Malik", "title": "Multi-view Supervision for Single-view Reconstruction via Differentiable\n  Ray Consistency", "comments": "To appear at CVPR 2017. Project webpage :\n  https://shubhtuls.github.io/drc/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the notion of consistency between a 3D shape and a 2D observation\nand propose a differentiable formulation which allows computing gradients of\nthe 3D shape given an observation from an arbitrary view. We do so by\nreformulating view consistency using a differentiable ray consistency (DRC)\nterm. We show that this formulation can be incorporated in a learning framework\nto leverage different types of multi-view observations e.g. foreground masks,\ndepth, color images, semantics etc. as supervision for learning single-view 3D\nprediction. We present empirical analysis of our technique in a controlled\nsetting. We also show that this approach allows us to improve over existing\ntechniques for single-view reconstruction of objects from the PASCAL VOC\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 17:56:53 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Zhou", "Tinghui", ""], ["Efros", "Alexei A.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1704.06305", "submitter": "Qing Tian", "authors": "Qing Tian, Tal Arbel, James J. Clark", "title": "Efficient Gender Classification Using a Deep LDA-Pruned Net", "comments": "The only difference with the previous version v2 is the title on the\n  arxiv page. I am changing it back to the original title in v1 because\n  otherwise google scholar cannot track the citations to this arxiv paper\n  correctly. You could cite either the conference version or this arxiv\n  version. They are equivalent", "journal-ref": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "doi": "10.1109/CVPRW.2017.78", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-time tasks, such as human-computer interaction, require fast and\nefficient facial gender classification. Although deep CNN nets have been very\neffective for a multitude of classification tasks, their high space and time\ndemands make them impractical for personal computers and mobile devices without\na powerful GPU. In this paper, we develop a 16-layer, yet lightweight, neural\nnetwork which boosts efficiency while maintaining high accuracy. Our net is\npruned from the VGG-16 model starting from the last convolutional (conv) layer\nwhere we find neuron activations are highly uncorrelated given the gender.\nThrough Fisher's Linear Discriminant Analysis (LDA), we show that this high\ndecorrelation makes it safe to discard directly last conv layer neurons with\nhigh within-class variance and low between-class variance. Combined with either\nSupport Vector Machines (SVM) or Bayesian classification, the reduced CNNs are\ncapable of achieving comparable (or even higher) accuracies on the LFW and\nCelebA datasets than the original net with fully connected layers. On LFW, only\nfour Conv5_3 neurons are able to maintain a comparably high recognition\naccuracy, which results in a reduction of total network size by a factor of 70X\nwith a 11 fold speedup. Comparisons with a state-of-the-art pruning method as\nwell as two smaller nets in terms of accuracy loss and convolutional layers\npruning rate are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 19:06:55 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 00:00:20 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 12:40:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Tian", "Qing", ""], ["Arbel", "Tal", ""], ["Clark", "James J.", ""]]}, {"id": "1704.06326", "submitter": "Erhan Gundogdu", "authors": "Erhan Gundogdu, A. Aydin Alatan", "title": "Good Features to Correlate for Visual Tracking", "comments": "Accepted version of IEEE Transactions on Image Processing", "journal-ref": "IEEE Transactions on Image Processing, vol. 27, no. 5, pp.\n  2526-2540, May 2018", "doi": "10.1109/TIP.2018.2806280", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the recent years, correlation filters have shown dominant and\nspectacular results for visual object tracking. The types of the features that\nare employed in these family of trackers significantly affect the performance\nof visual tracking. The ultimate goal is to utilize robust features invariant\nto any kind of appearance change of the object, while predicting the object\nlocation as properly as in the case of no appearance change. As the deep\nlearning based methods have emerged, the study of learning features for\nspecific tasks has accelerated. For instance, discriminative visual tracking\nmethods based on deep architectures have been studied with promising\nperformance. Nevertheless, correlation filter based (CFB) trackers confine\nthemselves to use the pre-trained networks which are trained for object\nclassification problem. To this end, in this manuscript the problem of learning\ndeep fully convolutional features for the CFB visual tracking is formulated. In\norder to learn the proposed model, a novel and efficient backpropagation\nalgorithm is presented based on the loss function of the network. The proposed\nlearning framework enables the network model to be flexible for a custom\ndesign. Moreover, it alleviates the dependency on the network trained for\nclassification. Extensive performance analysis shows the efficacy of the\nproposed custom design in the CFB tracking framework. By fine-tuning the\nconvolutional parts of a state-of-the-art network and integrating this model to\na CFB tracker, which is the top performing one of VOT2016, 18% increase is\nachieved in terms of expected average overlap, and tracking failures are\ndecreased by 25%, while maintaining the superiority over the state-of-the-art\nmethods in OTB-2013 and OTB-2015 tracking datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 20:24:50 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 12:43:55 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gundogdu", "Erhan", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "1704.06340", "submitter": "Chenyou Fan", "authors": "Chenyou Fan, Jangwon Lee, Mingze Xu, Krishna Kumar Singh, Yong Jae\n  Lee, David J. Crandall and Michael S. Ryoo", "title": "Identifying First-person Camera Wearers in Third-person Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider scenarios in which we wish to perform joint scene understanding,\nobject tracking, activity recognition, and other tasks in environments in which\nmultiple people are wearing body-worn cameras while a third-person static\ncamera also captures the scene. To do this, we need to establish person-level\ncorrespondences across first- and third-person videos, which is challenging\nbecause the camera wearer is not visible from his/her own egocentric video,\npreventing the use of direct feature matching. In this paper, we propose a new\nsemi-Siamese Convolutional Neural Network architecture to address this novel\nchallenge. We formulate the problem as learning a joint embedding space for\nfirst- and third-person videos that considers both spatial- and motion-domain\ncues. A new triplet loss function is designed to minimize the distance between\ncorrect first- and third-person matches while maximizing the distance between\nincorrect ones. This end-to-end approach performs significantly better than\nseveral baselines, in part by learning the first- and third-person features\noptimized for matching jointly with the distance measure itself.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 21:16:26 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Fan", "Chenyou", ""], ["Lee", "Jangwon", ""], ["Xu", "Mingze", ""], ["Singh", "Krishna Kumar", ""], ["Lee", "Yong Jae", ""], ["Crandall", "David J.", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1704.06363", "submitter": "Arthur Szlam", "authors": "Sam Gross and Marc'Aurelio Ranzato and Arthur Szlam", "title": "Hard Mixtures of Experts for Large Scale Weakly Supervised Vision", "comments": "Appearing in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training convolutional networks (CNN's) that fit on a single GPU with\nminibatch stochastic gradient descent has become effective in practice.\nHowever, there is still no effective method for training large CNN's that do\nnot fit in the memory of a few GPU cards, or for parallelizing CNN training. In\nthis work we show that a simple hard mixture of experts model can be\nefficiently trained to good effect on large scale hashtag (multilabel)\nprediction tasks. Mixture of experts models are not new (Jacobs et. al. 1991,\nCollobert et. al. 2003), but in the past, researchers have had to devise\nsophisticated methods to deal with data fragmentation. We show empirically that\nmodern weakly supervised data sets are large enough to support naive\npartitioning schemes where each data point is assigned to a single expert.\nBecause the experts are independent, training them in parallel is easy, and\nevaluation is cheap for the size of the model. Furthermore, we show that we can\nuse a single decoding layer for all the experts, allowing a unified feature\nembedding space. We demonstrate that it is feasible (and in fact relatively\npainless) to train far larger models than could be practically trained with\nstandard CNN architectures, and that the extra capacity can be well used on\ncurrent datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 23:45:27 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Gross", "Sam", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""]]}, {"id": "1704.06369", "submitter": "Feng Wang", "authors": "Feng Wang, Xiang Xiang, Jian Cheng, Alan L. Yuille", "title": "NormFace: L2 Hypersphere Embedding for Face Verification", "comments": "camera-ready version", "journal-ref": null, "doi": "10.1145/3123266.3123359", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Thanks to the recent developments of Convolutional Neural Networks, the\nperformance of face verification methods has increased rapidly. In a typical\nface verification method, feature normalization is a critical step for boosting\nperformance. This motivates us to introduce and study the effect of\nnormalization during training. But we find this is non-trivial, despite\nnormalization being differentiable. We identify and study four issues related\nto normalization through mathematical analysis, which yields understanding and\nhelps with parameter settings. Based on this analysis we propose two strategies\nfor training using normalized features. The first is a modification of softmax\nloss, which optimizes cosine similarity instead of inner-product. The second is\na reformulation of metric learning by introducing an agent vector for each\nclass. We show that both strategies, and small variants, consistently improve\nperformance by between 0.2% to 0.4% on the LFW dataset based on two models.\nThis is significant because the performance of the two models on LFW dataset is\nclose to saturation at over 98%. Codes and models are released on\nhttps://github.com/happynear/NormFace\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 00:07:03 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 15:56:57 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 05:48:54 GMT"}, {"version": "v4", "created": "Wed, 26 Jul 2017 17:58:43 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Wang", "Feng", ""], ["Xiang", "Xiang", ""], ["Cheng", "Jian", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1704.06370", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom and Tarek M. Taha", "title": "Robust Multi-view Pedestrian Tracking Using Neural Networks", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a real-time robust multi-view pedestrian detection\nand tracking system for video surveillance using neural networks which can be\nused in dynamic environments. The proposed system consists of two phases:\nmulti-view pedestrian detection and tracking. First, pedestrian detection\nutilizes background subtraction to segment the foreground blob. An adaptive\nbackground subtraction method where each of the pixel of input image models as\na mixture of Gaussians and uses an on-line approximation to update the model\napplies to extract the foreground region. The Gaussian distributions are then\nevaluated to determine which are most likely to result from a background\nprocess. This method produces a steady, real-time tracker in outdoor\nenvironment that consistently deals with changes of lighting condition, and\nlong-term scene change. Second, the Tracking is performed at two phases:\npedestrian classification and tracking the individual subject. A sliding window\nis applied on foreground binary image to select an input window which is used\nfor selecting the input image patches from actually input frame. The neural\nnetworks is used for classification with PHOG features. Finally, a Kalman\nfilter is applied to calculate the subsequent step for tracking that aims at\nfinding the exact position of pedestrians in an input image. The experimental\nresult shows that the proposed approach yields promising performance on\nmulti-view pedestrian detection and tracking on different benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 00:12:23 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Taha", "Tarek M.", ""]]}, {"id": "1704.06378", "submitter": "Victor Stamatescu", "authors": "Avishek Chakraborty, Victor Stamatescu, Sebastien C. Wong, Grant\n  Wigley, David Kearney", "title": "A data set for evaluating the performance of multi-class multi-object\n  video tracking", "comments": "Originally presented at SPIE Defense + Security conference on\n  Automatic Target Recognition XXVII (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in evaluating multi-object video detection, tracking\nand classification systems is having publically available data sets with which\nto compare different systems. However, the measures of performance for tracking\nand classification are different. Data sets that are suitable for evaluating\ntracking systems may not be appropriate for classification. Tracking video data\nsets typically only have ground truth track IDs, while classification video\ndata sets only have ground truth class-label IDs. The former identifies the\nsame object over multiple frames, while the latter identifies the type of\nobject in individual frames. This paper describes an advancement of the ground\ntruth meta-data for the DARPA Neovision2 Tower data set to allow both the\nevaluation of tracking and classification. The ground truth data sets presented\nin this paper contain unique object IDs across 5 different classes of object\n(Car, Bus, Truck, Person, Cyclist) for 24 videos of 871 image frames each. In\naddition to the object IDs and class labels, the ground truth data also\ncontains the original bounding box coordinates together with new bounding boxes\nin instances where un-annotated objects were present. The unique IDs are\nmaintained during occlusions between multiple objects or when objects re-enter\nthe field of view. This will provide: a solid foundation for evaluating the\nperformance of multi-object tracking of different types of objects, a\nstraightforward comparison of tracking system performance using the standard\nMulti Object Tracking (MOT) framework, and classification performance using the\nNeovision2 metrics. These data have been hosted publically.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 02:14:13 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Chakraborty", "Avishek", ""], ["Stamatescu", "Victor", ""], ["Wong", "Sebastien C.", ""], ["Wigley", "Grant", ""], ["Kearney", "David", ""]]}, {"id": "1704.06382", "submitter": "Holger Roth", "authors": "Holger R. Roth, Hirohisa Oda, Yuichiro Hayashi, Masahiro Oda, Natsuki\n  Shimizu, Michitaka Fujiwara, Kazunari Misawa, Kensaku Mori", "title": "Hierarchical 3D fully convolutional networks for multi-organ\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D fully convolutional networks (FCN) have made it\nfeasible to produce dense voxel-wise predictions of full volumetric images. In\nthis work, we show that a multi-class 3D FCN trained on manually labeled CT\nscans of seven abdominal structures (artery, vein, liver, spleen, stomach,\ngallbladder, and pancreas) can achieve competitive segmentation results, while\navoiding the need for handcrafting features or training organ-specific models.\nTo this end, we propose a two-stage, coarse-to-fine approach that trains an FCN\nmodel to roughly delineate the organs of interest in the first stage (seeing\n$\\sim$40% of the voxels within a simple, automatically generated binary mask of\nthe patient's body). We then use these predictions of the first-stage FCN to\ndefine a candidate region that will be used to train a second FCN. This step\nreduces the number of voxels the FCN has to classify to $\\sim$10% while\nmaintaining a recall high of $>$99%. This second-stage FCN can now focus on\nmore detailed segmentation of the organs. We respectively utilize training and\nvalidation sets consisting of 281 and 50 clinical CT images. Our hierarchical\napproach provides an improved Dice score of 7.5 percentage points per organ on\naverage in our validation set. We furthermore test our models on a completely\nunseen data collection acquired at a different hospital that includes 150 CT\nscans with three anatomical labels (liver, spleen, and pancreas). In such\nchallenging organs as the pancreas, our hierarchical approach improves the mean\nDice score from 68.5 to 82.2%, achieving the highest reported average score on\nthis dataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 03:05:15 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Roth", "Holger R.", ""], ["Oda", "Hirohisa", ""], ["Hayashi", "Yuichiro", ""], ["Oda", "Masahiro", ""], ["Shimizu", "Natsuki", ""], ["Fujiwara", "Michitaka", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""]]}, {"id": "1704.06392", "submitter": "Mohamed Elawady", "authors": "Mohamed Elawady, Olivier Alata, Christophe Ducottet, Cecile Barat,\n  Philippe Colantoni", "title": "Multiple Reflection Symmetry Detection via Linear-Directional Kernel\n  Density Estimation", "comments": "Submitted to CAIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is an important composition feature by investigating similar sides\ninside an image plane. It has a crucial effect to recognize man-made or nature\nobjects within the universe. Recent symmetry detection approaches used a\nsmoothing kernel over different voting maps in the polar coordinate system to\ndetect symmetry peaks, which split the regions of symmetry axis candidates in\ninefficient way. We propose a reliable voting representation based on weighted\nlinear-directional kernel density estimation, to detect multiple symmetries\nover challenging real-world and synthetic images. Experimental evaluation on\ntwo public datasets demonstrates the superior performance of the proposed\nalgorithm to detect global symmetry axes respect to the major image shapes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 04:15:15 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Elawady", "Mohamed", ""], ["Alata", "Olivier", ""], ["Ducottet", "Christophe", ""], ["Barat", "Cecile", ""], ["Colantoni", "Philippe", ""]]}, {"id": "1704.06410", "submitter": "Nevrez Imamoglu", "authors": "Nevrez Imamoglu and Motoki Kimura and Hiroki Miyamoto and Aito Fujita\n  and Ryosuke Nakamura", "title": "Solar Power Plant Detection on Multi-Spectral Satellite Imagery using\n  Weakly-Supervised CNN with Feedback Features and m-PCNN Fusion", "comments": "9 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the traditional convolutional neural networks (CNNs) implements\nbottom-up approach (feed-forward) for image classifications. However, many\nscientific studies demonstrate that visual perception in primates rely on both\nbottom-up and top-down connections. Therefore, in this work, we propose a CNN\nnetwork with feedback structure for Solar power plant detection on\nmiddle-resolution satellite images. To express the strength of the top-down\nconnections, we introduce feedback CNN network (FB-Net) to a baseline CNN model\nused for solar power plant classification on multi-spectral satellite data.\nMoreover, we introduce a method to improve class activation mapping (CAM) to\nour FB-Net, which takes advantage of multi-channel pulse coupled neural network\n(m-PCNN) for weakly-supervised localization of the solar power plants from the\nfeatures of proposed FB-Net. For the proposed FB-Net CAM with m-PCNN,\nexperimental results demonstrated promising results on both solar-power plant\nimage classification and detection task.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 06:23:44 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 06:49:45 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Imamoglu", "Nevrez", ""], ["Kimura", "Motoki", ""], ["Miyamoto", "Hiroki", ""], ["Fujita", "Aito", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1704.06415", "submitter": "Sebastien Wong", "authors": "Sebastien C. Wong, Victor Stamatescu, Adam Gatt, David Kearney, Ivan\n  Lee, and Mark D. McDonnell", "title": "Track Everything: Limiting Prior Knowledge in Online Multi-Object\n  Recognition", "comments": "15 pages", "journal-ref": null, "doi": "10.1109/TIP.2017.2696744", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of online tracking and classification of\nmultiple objects in an image sequence. Our proposed solution is to first track\nall objects in the scene without relying on object-specific prior knowledge,\nwhich in other systems can take the form of hand-crafted features or user-based\ntrack initialization. We then classify the tracked objects with a fast-learning\nimage classifier that is based on a shallow convolutional neural network\narchitecture and demonstrate that object recognition improves when this is\ncombined with object state information from the tracking algorithm. We argue\nthat by transferring the use of prior knowledge from the detection and tracking\nstages to the classification stage we can design a robust, general purpose\nobject recognition system with the ability to detect and track a variety of\nobject types. We describe our biologically inspired implementation, which\nadaptively learns the shape and motion of tracked objects, and apply it to the\nNeovision2 Tower benchmark data set, which contains multiple object types. An\nexperimental evaluation demonstrates that our approach is competitive with\nstate-of-the-art video object recognition systems that do make use of\nobject-specific prior knowledge in detection and tracking, while providing\nadditional practical advantages by virtue of its generality.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 06:49:51 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Wong", "Sebastien C.", ""], ["Stamatescu", "Victor", ""], ["Gatt", "Adam", ""], ["Kearney", "David", ""], ["Lee", "Ivan", ""], ["McDonnell", "Mark D.", ""]]}, {"id": "1704.06447", "submitter": "Zhibo Yang", "authors": "Zhibo Yang, Huanle Xu, Jianyuan Deng, Chen Change Loy, Wing Cheong Lau", "title": "Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile\n  Applications", "comments": "15 pages, 10 figures, submitted to IEEE Transaction on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2855419", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of color in QR codes brings extra data capacity, but also inflicts\ntremendous challenges on the decoding process due to chromatic distortion,\ncross-channel color interference and illumination variation. Particularly, we\nfurther discover a new type of chromatic distortion in high-density color QR\ncodes, cross-module color interference, caused by the high density which also\nmakes the geometric distortion correction more challenging. To address these\nproblems, we propose two approaches, namely, LSVM-CMI and QDA-CMI, which\njointly model these different types of chromatic distortion. Extended from SVM\nand QDA, respectively, both LSVM-CMI and QDA-CMI optimize over a particular\nobjective function to learn a color classifier. Furthermore, a robust geometric\ntransformation method and several pipeline refinements are proposed to boost\nthe decoding performance for mobile applications. We put forth and implement a\nframework for high-capacity color QR codes equipped with our methods, called\nHiQ. To evaluate the performance of HiQ, we collect a challenging large-scale\ncolor QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR\ncode samples. The comparison with the baseline method [2] on CUHK-CQRC shows\nthat HiQ at least outperforms [2] by 188% in decoding success rate and 60% in\nbit error rate. Our implementation of HiQ in iOS and Android also demonstrates\nthe effectiveness of our framework in real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 09:01:43 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 01:08:34 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 21:00:18 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Yang", "Zhibo", ""], ["Xu", "Huanle", ""], ["Deng", "Jianyuan", ""], ["Loy", "Chen Change", ""], ["Lau", "Wing Cheong", ""]]}, {"id": "1704.06456", "submitter": "Qianru Sun", "authors": "Qianru Sun, Bernt Schiele and Mario Fritz", "title": "A Domain Based Approach to Social Relation Recognition", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social relations are the foundation of human daily life. Developing\ntechniques to analyze such relations from visual data bears great potential to\nbuild machines that better understand us and are capable of interacting with us\nat a social level. Previous investigations have remained partial due to the\noverwhelming diversity and complexity of the topic and consequently have only\nfocused on a handful of social relations. In this paper, we argue that the\ndomain-based theory from social psychology is a great starting point to\nsystematically approach this problem. The theory provides coverage of all\naspects of social relations and equally is concrete and predictive about the\nvisual attributes and behaviors defining the relations included in each domain.\nWe provide the first dataset built on this holistic conceptualization of social\nlife that is composed of a hierarchical label space of social domains and\nsocial relations. We also contribute the first models to recognize such domains\nand relations and find superior performance for attribute based features.\nBeyond the encouraging performance of the attribute based approach, we also\nfind interpretable features that are in accordance with the predictions from\nsocial psychology literature. Beyond our findings, we believe that our\ncontributions more tightly interleave visual recognition and social psychology\ntheory that has the potential to complement the theoretical work in the area\nwith empirical and data-driven models of social life.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 09:27:32 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sun", "Qianru", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1704.06485", "submitter": "Byeongchang Kim", "authors": "Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim", "title": "Attend to You: Personalized Image Captioning with Context Sequence\n  Memory Networks", "comments": "Accepted paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address personalization issues of image captioning, which have not been\ndiscussed yet in previous research. For a query image, we aim to generate a\ndescriptive sentence, accounting for prior knowledge such as the user's active\nvocabularies in previous documents. As applications of personalized image\ncaptioning, we tackle two post automation tasks: hashtag prediction and post\ngeneration, on our newly collected Instagram dataset, consisting of 1.1M posts\nfrom 6.3K users. We propose a novel captioning model named Context Sequence\nMemory Network (CSMN). Its unique updates over previous memory network models\ninclude (i) exploiting memory as a repository for multiple types of context\ninformation, (ii) appending previously generated words into memory to capture\nlong-term information without suffering from the vanishing gradient problem,\nand (iii) adopting CNN memory structure to jointly represent nearby ordered\nmemory slots for better context understanding. With quantitative evaluation and\nuser studies via Amazon Mechanical Turk, we show the effectiveness of the three\nnovel features of CSMN and its performance enhancement for personalized image\ncaptioning over state-of-the-art captioning models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:29:07 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 23:30:43 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Park", "Cesc Chunseong", ""], ["Kim", "Byeongchang", ""], ["Kim", "Gunhee", ""]]}, {"id": "1704.06544", "submitter": "Tobias Fechter", "authors": "Tobias Fechter, Sonja Adebahr, Dimos Baltas, Ismail Ben Ayed,\n  Christian Desrosiers, Jose Dolz", "title": "A 3D fully convolutional neural network and a random walker to segment\n  the esophagus in CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise delineation of organs at risk (OAR) is a crucial task in radiotherapy\ntreatment planning, which aims at delivering high dose to the tumour while\nsparing healthy tissues. In recent years algorithms showed high performance and\nthe possibility to automate this task for many OAR. However, for some OAR\nprecise delineation remains challenging. The esophagus with a versatile shape\nand poor contrast is among these structures. To tackle these issues we propose\na 3D fully (convolutional neural network (CNN) driven random walk (RW) approach\nto automatically segment the esophagus on CT. First, a soft probability map is\ngenerated by the CNN. Then an active contour model (ACM) is fitted on the\nprobability map to get a first estimation of the center line. The outputs of\nthe CNN and ACM are then used in addition to CT Hounsfield values to drive the\nRW. Evaluation and training was done on 50 CTs with peer reviewed esophagus\ncontours. Results were assessed regarding spatial overlap and shape\nsimilarities.\n  The generated contours showed a mean Dice coefficient of 0.76, an average\nsymmetric square distance of 1.36 mm and an average Hausdorff distance of 11.68\ncompared to the reference. These figures translate into a very good agreement\nwith the reference contours and an increase in accuracy compared to other\nmethods.\n  We show that by employing a CNN accurate estimations of esophagus location\ncan be obtained and refined by a post processing RW step. One of the main\nadvantages compared to previous methods is that our network performs\nconvolutions in a 3D manner, fully exploiting the 3D spatial context and\nperforming an efficient and precise volume-wise prediction. The whole\nsegmentation process is fully automatic and yields esophagus delineations in\nvery good agreement with the used gold standard, showing that it can compete\nwith previously published methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 13:54:00 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Fechter", "Tobias", ""], ["Adebahr", "Sonja", ""], ["Baltas", "Dimos", ""], ["Ayed", "Ismail Ben", ""], ["Desrosiers", "Christian", ""], ["Dolz", "Jose", ""]]}, {"id": "1704.06556", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Toshihiko Yamasaki, Kiyoharu Aizawa", "title": "PQTable: Non-exhaustive Fast Search for Product-quantized Codes using\n  Hash Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a product quantization table (PQTable); a fast\nsearch method for product-quantized codes via hash-tables. An identifier of\neach database vector is associated with the slot of a hash table by using its\nPQ-code as a key. For querying, an input vector is PQ-encoded and hashed, and\nthe items associated with that code are then retrieved. The proposed PQTable\nproduces the same results as a linear PQ scan, and is 10^2 to 10^5 times\nfaster. Although state-of-the-art performance can be achieved by previous\ninverted-indexing-based approaches, such methods require manually-designed\nparameter setting and significant training; our PQTable is free of these\nlimitations, and therefore offers a practical and effective solution for\nreal-world problems. Specifically, when the vectors are highly compressed, our\nPQTable achieves one of the fastest search performances on a single CPU to date\nwith significantly efficient memory usage (0.059 ms per query over 10^9 data\npoints with just 5.5 GB memory consumption). Finally, we show that our proposed\nPQTable can naturally handle the codes of an optimized product quantization\n(OPQTable).\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 14:22:16 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Matsui", "Yusuke", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1704.06591", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, Ondrej Chum", "title": "Panorama to panorama matching for location recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location recognition is commonly treated as visual instance retrieval on\n\"street view\" imagery. The dataset items and queries are panoramic views, i.e.\ngroups of images taken at a single location. This work introduces a novel\npanorama-to-panorama matching process, either by aggregating features of\nindividual images in a group or by explicitly constructing a larger panorama.\nIn either case, multiple views are used as queries. We reach near perfect\nlocation recognition on a standard benchmark with only four query views.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 15:23:29 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Furon", "Teddy", ""], ["Chum", "Ondrej", ""]]}, {"id": "1704.06610", "submitter": "Jose Oramas", "authors": "Jose Oramas and Luc De Raedt and Tinne Tuytelaars", "title": "Context-based Object Viewpoint Estimation: A 2D Relational Approach", "comments": "Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2017.04.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of object viewpoint estimation has been a challenge since the early\ndays of computer vision. To estimate the viewpoint (or pose) of an object,\npeople have mostly looked at object intrinsic features, such as shape or\nappearance. Surprisingly, informative features provided by other, extrinsic\nelements in the scene, have so far mostly been ignored. At the same time,\ncontextual cues have been proven to be of great benefit for related tasks such\nas object detection or action recognition. In this paper, we explore how\ninformation from other objects in the scene can be exploited for viewpoint\nestimation. In particular, we look at object configurations by following a\nrelational neighbor-based approach for reasoning about object relations. We\nshow that, starting from noisy object detections and viewpoint estimates,\nexploiting the estimated viewpoint and location of other objects in the scene\ncan lead to improved object viewpoint predictions. Experiments on the KITTI\ndataset demonstrate that object configurations can indeed be used as a\ncomplementary cue to appearance-based viewpoint estimation. Our analysis\nreveals that the proposed context-based method can improve object viewpoint\nestimation by reducing specific types of viewpoint estimation errors commonly\nmade by methods that only consider local information. Moreover, considering\ncontextual information produces superior performance in scenes where a high\nnumber of object instances occur. Finally, our results suggest that, following\na cautious relational neighbor formulation brings improvements over its\naggressive counterpart for the task of object viewpoint estimation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 15:55:54 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Oramas", "Jose", ""], ["De Raedt", "Luc", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1704.06687", "submitter": "Mathieu Cliche", "authors": "Mathieu Cliche, David Rosenberg, Dhruv Madeka and Connie Yee", "title": "Scatteract: Automated extraction of data from scatter plots", "comments": "Submitted to ECML PKDD 2017 proceedings, 16 pages", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2017. Lecture Notes in Computer Science, vol 10534. Springer, Cham", "doi": "10.1007/978-3-319-71249-9_9", "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:25:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Cliche", "Mathieu", ""], ["Rosenberg", "David", ""], ["Madeka", "Dhruv", ""], ["Yee", "Connie", ""]]}, {"id": "1704.06693", "submitter": "Sandipan Banerjee", "authors": "Sandipan Banerjee, John S. Bernhard, Walter J. Scheirer, Kevin W.\n  Bowyer, Patrick J. Flynn", "title": "SREFI: Synthesis of Realistic Example Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel face synthesis approach that can generate\nan arbitrarily large number of synthetic images of both real and synthetic\nidentities. Thus a face image dataset can be expanded in terms of the number of\nidentities represented and the number of images per identity using this\napproach, without the identity-labeling and privacy complications that come\nfrom downloading images from the web. To measure the visual fidelity and\nuniqueness of the synthetic face images and identities, we conducted face\nmatching experiments with both human participants and a CNN pre-trained on a\ndataset of 2.6M real face images. To evaluate the stability of these synthetic\nfaces, we trained a CNN model with an augmented dataset containing close to\n200,000 synthetic faces. We used a snapshot of this trained CNN to recognize\nextremely challenging frontal (real) face images. Experiments showed training\nwith the augmented faces boosted the face recognition performance of the CNN.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:59:47 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 03:54:34 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Bernhard", "John S.", ""], ["Scheirer", "Walter J.", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""]]}, {"id": "1704.06729", "submitter": "Tal Hassner", "authors": "Yuval Nirkin, Iacopo Masi, Anh Tuan Tran, Tal Hassner, and Gerard\n  Medioni", "title": "On Face Segmentation, Face Swapping, and Face Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that even when face images are unconstrained and arbitrarily paired,\nface swapping between them is actually quite simple. To this end, we make the\nfollowing contributions. (a) Instead of tailoring systems for face\nsegmentation, as others previously proposed, we show that a standard fully\nconvolutional network (FCN) can achieve remarkably fast and accurate\nsegmentations, provided that it is trained on a rich enough example set. For\nthis purpose, we describe novel data collection and generation routines which\nprovide challenging segmented face examples. (b) We use our segmentations to\nenable robust face swapping under unprecedented conditions. (c) Unlike previous\nwork, our swapping is robust enough to allow for extensive quantitative tests.\nTo this end, we use the Labeled Faces in the Wild (LFW) benchmark and measure\nthe effect of intra- and inter-subject face swapping on recognition. We show\nthat our intra-subject swapped faces remain as recognizable as their sources,\ntestifying to the effectiveness of our method. In line with well known\nperceptual studies, we show that better face swapping produces less\nrecognizable inter-subject results. This is the first time this effect was\nquantitatively demonstrated for machine vision systems.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 00:50:51 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Nirkin", "Yuval", ""], ["Masi", "Iacopo", ""], ["Tran", "Anh Tuan", ""], ["Hassner", "Tal", ""], ["Medioni", "Gerard", ""]]}, {"id": "1704.06743", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy (University of Sydney and Capital Markets\n  Cooperative Research Centre (CMCRC)), Aditya Krishna Menon (Data61/CSIRO and\n  the Australian National University), and Sanjay Chawla (Qatar Computing\n  Research Institute (QCRI), HBKU)", "title": "Robust, Deep and Inductive Anomaly Detection", "comments": "Accepted ECML PKDD 2017 Skopje, Macedonia 18-22 September the\n  European Conference On Machine Learning & Principles and Practice of\n  Knowledge Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  PCA is a classical statistical technique whose simplicity and maturity has\nseen it find widespread use as an anomaly detection technique. However, it is\nlimited in this regard by being sensitive to gross perturbations of the input,\nand by seeking a linear subspace that captures normal behaviour. The first\nissue has been dealt with by robust PCA, a variant of PCA that explicitly\nallows for some data points to be arbitrarily corrupted, however, this does not\nresolve the second issue, and indeed introduces the new issue that one can no\nlonger inductively find anomalies on a test set. This paper addresses both\nissues in a single model, the robust autoencoder. This method learns a\nnonlinear subspace that captures the majority of data points, while allowing\nfor some data to have arbitrary corruption. The model is simple to train and\nleverages recent advances in the optimisation of deep neural networks.\nExperiments on a range of real-world datasets highlight the model's\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 04:12:24 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 18:46:25 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 08:47:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chalapathy", "Raghavendra", "", "University of Sydney and Capital Markets\n  Cooperative Research Centre"], ["Menon", "Aditya Krishna", "", "Data61/CSIRO and\n  the Australian National University"], ["Chawla", "Sanjay", "", "Qatar Computing\n  Research Institute"]]}, {"id": "1704.06752", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Wei Shen, Weichao Qiu, Chenxi Liu, Alan Yuille", "title": "ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by product detection in supermarkets, this paper studies the\nproblem of object proposal generation in supermarket images and other natural\nimages. We argue that estimation of object scales in images is helpful for\ngenerating object proposals, especially for supermarket images where object\nscales are usually within a small range. Therefore, we propose to estimate\nobject scales of images before generating object proposals. The proposed method\nfor predicting object scales is called ScaleNet. To validate the effectiveness\nof ScaleNet, we build three supermarket datasets, two of which are real-world\ndatasets used for testing and the other one is a synthetic dataset used for\ntraining. In short, we extend the previous state-of-the-art object proposal\nmethods by adding a scale prediction phase. The resulted method outperforms the\nprevious state-of-the-art on the supermarket datasets by a large margin. We\nalso show that the approach works for object proposal on other natural images\nand it outperforms the previous state-of-the-art object proposal methods on the\nMS COCO dataset. The supermarket datasets, the virtual supermarkets, and the\ntools for creating more synthetic datasets will be made public.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 06:05:31 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Qiao", "Siyuan", ""], ["Shen", "Wei", ""], ["Qiu", "Weichao", ""], ["Liu", "Chenxi", ""], ["Yuille", "Alan", ""]]}, {"id": "1704.06756", "submitter": "Shima Alizadeh", "authors": "Shima Alizadeh and Azar Fazel", "title": "Convolutional Neural Networks for Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed convolutional neural networks (CNN) for a facial expression\nrecognition task. The goal is to classify each facial image into one of the\nseven facial emotion categories considered in this study. We trained CNN models\nwith different depth using gray-scale images. We developed our models in Torch\nand exploited Graphics Processing Unit (GPU) computation in order to expedite\nthe training process. In addition to the networks performing based on raw pixel\ndata, we employed a hybrid feature strategy by which we trained a novel CNN\nmodel with the combination of raw pixel data and Histogram of Oriented\nGradients (HOG) features. To reduce the overfitting of the models, we utilized\ndifferent techniques including dropout and batch normalization in addition to\nL2 regularization. We applied cross validation to determine the optimal\nhyper-parameters and evaluated the performance of the developed models by\nlooking at their training histories. We also present the visualization of\ndifferent layers of a network to show what features of a face can be learned by\nCNN models.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 06:27:56 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Alizadeh", "Shima", ""], ["Fazel", "Azar", ""]]}, {"id": "1704.06761", "submitter": "Sungeun Hong", "authors": "Sungeun Hong, Woobin Im, Hyun S. Yang", "title": "Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure\n  Constraint", "comments": "13 pages, 9 figures, 4 tables, supplementary material link >>\n  https://youtu.be/ZyINqDMo3Fg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Up to now, only limited research has been conducted on cross-modal retrieval\nof suitable music for a specified video or vice versa. Moreover, much of the\nexisting research relies on metadata such as keywords, tags, or associated\ndescription that must be individually produced and attached posterior. This\npaper introduces a new content-based, cross-modal retrieval method for video\nand music that is implemented through deep neural networks. We train the\nnetwork via inter-modal ranking loss such that videos and music with similar\nsemantics end up close together in the embedding space. However, if only the\ninter-modal ranking constraint is used for embedding, modality-specific\ncharacteristics can be lost. To address this problem, we propose a novel soft\nintra-modal structure loss that leverages the relative distance relationship\nbetween intra-modal samples before embedding. We also introduce reasonable\nquantitative and qualitative experimental protocols to solve the lack of\nstandard protocols for less-mature video-music related tasks. Finally, we\nconstruct a large-scale 200K video-music pair benchmark. All the datasets and\nsource code can be found in our online repository\n(https://github.com/csehong/VM-NET).\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 07:40:16 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 14:08:06 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Hong", "Sungeun", ""], ["Im", "Woobin", ""], ["Yang", "Hyun S.", ""]]}, {"id": "1704.06821", "submitter": "Saad Bin Ahmed", "authors": "Saad Bin Ahmed, Saeeda Naz, Muhammad Imran Razzak, and Rubiyah Yousaf", "title": "Deep Learning based Isolated Arabic Scene Character Recognition", "comments": "6 pages, 8 Figures, 3 Tables, Accepted in IEEE Workshop on Arabic\n  Script Analysis and Recognition (ASAR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technological advancement and sophistication in cameras and gadgets\nprompt researchers to have focus on image analysis and text understanding. The\ndeep learning techniques demonstrated well to assess the potential for\nclassifying text from natural scene images as reported in recent years. There\nare variety of deep learning approaches that prospects the detection and\nrecognition of text, effectively from images. In this work, we presented Arabic\nscene text recognition using Convolutional Neural Networks (ConvNets) as a deep\nlearning classifier. As the scene text data is slanted and skewed, thus to deal\nwith maximum variations, we employ five orientations with respect to single\noccurrence of a character. The training is formulated by keeping filter size 3\nx 3 and 5 x 5 with stride value as 1 and 2. During text classification phase,\nwe trained network with distinct learning rates. Our approach reported\nencouraging results on recognition of Arabic characters from segmented Arabic\nscene images.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 17:09:02 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ahmed", "Saad Bin", ""], ["Naz", "Saeeda", ""], ["Razzak", "Muhammad Imran", ""], ["Yousaf", "Rubiyah", ""]]}, {"id": "1704.06825", "submitter": "Imran Razzak", "authors": "Muhammad Imran Razzak, Saeeda Naz and Ahmad Zaib", "title": "Deep Learning for Medical Image Processing: Overview, Challenges and\n  Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare sector is totally different from other industry. It is on high\npriority sector and people expect highest level of care and services regardless\nof cost. It did not achieve social expectation even though it consume huge\npercentage of budget. Mostly the interpretations of medical data is being done\nby medical expert. In terms of image interpretation by human expert, it is\nquite limited due to its subjectivity, the complexity of the image, extensive\nvariations exist across different interpreters, and fatigue. After the success\nof deep learning in other real world application, it is also providing exciting\nsolutions with good accuracy for medical imaging and is seen as a key method\nfor future applications in health secotr. In this chapter, we discussed state\nof the art deep learning architecture and its optimization used for medical\nimage segmentation and classification. In the last section, we have discussed\nthe challenges deep learning based methods for medical imaging and open\nresearch issue.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 17:49:04 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Razzak", "Muhammad Imran", ""], ["Naz", "Saeeda", ""], ["Zaib", "Ahmad", ""]]}, {"id": "1704.06843", "submitter": "Cenek Albl", "authors": "Cenek Albl, Zuzana Kukelova, Andrew Fitzgibbon, Jan Heller, Matej Smid\n  and Tomas Pajdla", "title": "On the Two-View Geometry of Unsynchronized Cameras", "comments": "12 pages, 9 figures, Computer Vision and Pattern Recognition (CVPR)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for simultaneously estimating camera geometry and time\nshift from video sequences from multiple unsynchronized cameras. Algorithms for\nsimultaneous computation of a fundamental matrix or a homography with unknown\ntime shift between images are developed. Our methods use minimal correspondence\nsets (eight for fundamental matrix and four and a half for homography) and\ntherefore are suitable for robust estimation using RANSAC. Furthermore, we\npresent an iterative algorithm that extends the applicability on sequences\nwhich are significantly unsynchronized, finding the correct time shift up to\nseveral seconds. We evaluated the methods on synthetic and wide range of real\nworld datasets and the results show a broad applicability to the problem of\ncamera synchronization.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 19:45:46 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Albl", "Cenek", ""], ["Kukelova", "Zuzana", ""], ["Fitzgibbon", "Andrew", ""], ["Heller", "Jan", ""], ["Smid", "Matej", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1704.06857", "submitter": "Alberto Garcia-Garcia", "authors": "Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor\n  Villena-Martinez, Jose Garcia-Rodriguez", "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation", "comments": "Submitted to TPAMI on Apr. 22, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image semantic segmentation is more and more being of interest for computer\nvision and machine learning researchers. Many applications on the rise need\naccurate and efficient segmentation mechanisms: autonomous driving, indoor\nnavigation, and even virtual or augmented reality systems to name a few. This\ndemand coincides with the rise of deep learning approaches in almost every\nfield or application target related to computer vision, including semantic\nsegmentation or scene understanding. This paper provides a review on deep\nlearning methods for semantic segmentation applied to various application\nareas. Firstly, we describe the terminology of this field as well as mandatory\nbackground concepts. Next, the main datasets and challenges are exposed to help\nresearchers decide which are the ones that best suit their needs and their\ntargets. Then, existing methods are reviewed, highlighting their contributions\nand their significance in the field. Finally, quantitative results are given\nfor the described methods and the datasets in which they were evaluated,\nfollowing up with a discussion of the results. At last, we point out a set of\npromising future works and draw our own conclusions about the state of the art\nof semantic segmentation using deep learning techniques.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 23:37:43 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Garcia-Garcia", "Alberto", ""], ["Orts-Escolano", "Sergio", ""], ["Oprea", "Sergiu", ""], ["Villena-Martinez", "Victor", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1704.06885", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "A General Theory for Training Learning Machine", "comments": "55 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1602.03950", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the deep learning is pushing the machine learning to a new stage,\nbasic theories of machine learning are still limited. The principle of\nlearning, the role of the a prior knowledge, the role of neuron bias, and the\nbasis for choosing neural transfer function and cost function, etc., are still\nfar from clear. In this paper, we present a general theoretical framework for\nmachine learning. We classify the prior knowledge into common and\nproblem-dependent parts, and consider that the aim of learning is to maximally\nincorporate them. The principle we suggested for maximizing the former is the\ndesign risk minimization principle, while the neural transfer function, the\ncost function, as well as pretreatment of samples, are endowed with the role\nfor maximizing the latter. The role of the neuron bias is explained from a\ndifferent angle. We develop a Monte Carlo algorithm to establish the\ninput-output responses, and we control the input-output sensitivity of a\nlearning machine by controlling that of individual neurons. Applications of\nfunction approaching and smoothing, pattern recognition and classification, are\nprovided to illustrate how to train general learning machines based on our\ntheory and algorithm. Our method may in addition induce new applications, such\nas the transductive inference.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 05:48:18 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1704.06888", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang,\n  Stefan Schaal, Sergey Levine", "title": "Time-Contrastive Networks: Self-Supervised Learning from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised approach for learning representations and\nrobotic behaviors entirely from unlabeled videos recorded from multiple\nviewpoints, and study how this representation can be used in two robotic\nimitation settings: imitating object interactions from videos of humans, and\nimitating human poses. Imitation of human behavior requires a\nviewpoint-invariant representation that captures the relationships between\nend-effectors (hands or robot grippers) and the environment, object attributes,\nand body pose. We train our representations using a metric learning loss, where\nmultiple simultaneous viewpoints of the same observation are attracted in the\nembedding space, while being repelled from temporal neighbors which are often\nvisually similar but functionally different. In other words, the model\nsimultaneously learns to recognize what is common between different-looking\nimages, and what is different between similar-looking images. This signal\ncauses our model to discover attributes that do not change across viewpoint,\nbut do change across time, while ignoring nuisance variables such as\nocclusions, motion blur, lighting and background. We demonstrate that this\nrepresentation can be used by a robot to directly mimic human poses without an\nexplicit correspondence, and that it can be used as a reward function within a\nreinforcement learning algorithm. While representations are learned from an\nunlabeled collection of task-related videos, robot behaviors such as pouring\nare learned by watching a single 3rd-person demonstration by a human. Reward\nfunctions obtained by following the human demonstrations under the learned\nrepresentation enable efficient reinforcement learning that is practical for\nreal-world robotic systems. Video results, open-source code and dataset are\navailable at https://sermanet.github.io/imitate\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 06:03:56 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 01:26:56 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 01:02:45 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Sermanet", "Pierre", ""], ["Lynch", "Corey", ""], ["Chebotar", "Yevgen", ""], ["Hsu", "Jasmine", ""], ["Jang", "Eric", ""], ["Schaal", "Stefan", ""], ["Levine", "Sergey", ""]]}, {"id": "1704.06904", "submitter": "Fei Wang", "authors": "Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang\n  Zhang, Xiaogang Wang, Xiaoou Tang", "title": "Residual Attention Network for Image Classification", "comments": "accepted to CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose \"Residual Attention Network\", a convolutional neural\nnetwork using attention mechanism which can incorporate with state-of-art feed\nforward network architecture in an end-to-end training fashion. Our Residual\nAttention Network is built by stacking Attention Modules which generate\nattention-aware features. The attention-aware features from different modules\nchange adaptively as layers going deeper. Inside each Attention Module,\nbottom-up top-down feedforward structure is used to unfold the feedforward and\nfeedback attention process into a single feedforward process. Importantly, we\npropose attention residual learning to train very deep Residual Attention\nNetworks which can be easily scaled up to hundreds of layers. Extensive\nanalyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the\neffectiveness of every module mentioned above. Our Residual Attention Network\nachieves state-of-the-art object recognition performance on three benchmark\ndatasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and\nImageNet (4.8% single model and single crop, top-5 error). Note that, our\nmethod achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%\nforward FLOPs comparing to ResNet-200. The experiment also demonstrates that\nour network is robust against noisy labels.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 10:03:49 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wang", "Fei", ""], ["Jiang", "Mengqing", ""], ["Qian", "Chen", ""], ["Yang", "Shuo", ""], ["Li", "Cheng", ""], ["Zhang", "Honggang", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1704.06925", "submitter": "Anoop Cherian", "authors": "Anoop Cherian and Stephen Gould", "title": "Second-order Temporal Pooling for Action Recognition", "comments": "Accepted in the International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models for video-based action recognition usually generate\nfeatures for short clips (consisting of a few frames); such clip-level features\nare aggregated to video-level representations by computing statistics on these\nfeatures. Typically zero-th (max) or the first-order (average) statistics are\nused. In this paper, we explore the benefits of using second-order statistics.\nSpecifically, we propose a novel end-to-end learnable feature aggregation\nscheme, dubbed temporal correlation pooling that generates an action descriptor\nfor a video sequence by capturing the similarities between the temporal\nevolution of clip-level CNN features computed across the video. Such a\ndescriptor, while being computationally cheap, also naturally encodes the\nco-activations of multiple CNN features, thereby providing a richer\ncharacterization of actions than their first-order counterparts. We also\npropose higher-order extensions of this scheme by computing correlations after\nembedding the CNN features in a reproducing kernel Hilbert space. We provide\nexperiments on benchmark datasets such as HMDB-51 and UCF-101, fine-grained\ndatasets such as MPII Cooking activities and JHMDB, as well as the recent\nKinetics-600. Our results demonstrate the advantages of higher-order pooling\nschemes that when combined with hand-crafted features (as is standard practice)\nachieves state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 14:10:55 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 01:38:50 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Cherian", "Anoop", ""], ["Gould", "Stephen", ""]]}, {"id": "1704.06967", "submitter": "Christopher Ham", "authors": "Christopher Ham, Simon Lucey, Surya Singh", "title": "Proxy Templates for Inverse Compositional Photometric Bundle Adjustment", "comments": "8 pages, 5 figures, for supplementary material file, see\n  https://goo.gl/p0SID1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D vision have demonstrated the strengths of photometric\nbundle adjustment. By directly minimizing reprojected pixel errors, instead of\ngeometric reprojection errors, such methods can achieve sub-pixel alignment\naccuracy in both high and low textured regions. Typically, these problems are\nsolved using a forwards compositional Lucas-Kanade formulation parameterized by\n6-DoF rigid camera poses and a depth per point in the structure. For large\nproblems the most CPU-intensive component of the pipeline is the creation and\nfactorization of the Hessian matrix at each iteration. For many warps, the\ninverse compositional formulation can offer significant speed-ups since the\nHessian need only be inverted once. In this paper, we show that an ordinary\ninverse compositional formulation does not work for warps of this type of\nparameterization due to ill-conditioning of its partial derivatives. However,\nwe show that it is possible to overcome this limitation by introducing the\nconcept of a proxy template image. We show an order of magnitude improvement in\nspeed, with little effect on quality, going from forwards to inverse\ncompositional in our own photometric bundle adjustment method designed for\nobject-centric structure from motion. This means less processing time for large\nsystems or denser reconstructions under the same real-time constraints. We\nadditionally show that this theory can be readily applied to existing methods\nby integrating it with the recently released Direct Sparse Odometry SLAM\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 19:42:48 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ham", "Christopher", ""], ["Lucey", "Simon", ""], ["Singh", "Surya", ""]]}, {"id": "1704.06972", "submitter": "Yufei Wang", "authors": "Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, Garrison W. Cottrell", "title": "Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition", "comments": "Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a lot of interest in automatically generating\ndescriptions for an image. Most existing language-model based approaches for\nthis task learn to generate an image description word by word in its original\nword order. However, for humans, it is more natural to locate the objects and\ntheir relationships first, and then elaborate on each object, describing\nnotable attributes. We present a coarse-to-fine method that decomposes the\noriginal image description into a skeleton sentence and its attributes, and\ngenerates the skeleton sentence and attribute phrases separately. By this\ndecomposition, our method can generate more accurate and novel descriptions\nthan the previous state-of-the-art. Experimental results on the MS-COCO and a\nlarger scale Stock3M datasets show that our algorithm yields consistent\nimprovements across different evaluation metrics, especially on the SPICE\nmetric, which has much higher correlation with human ratings than the\nconventional metrics. Furthermore, our algorithm can generate descriptions with\nvaried length, benefiting from the separate control of the skeleton and\nattributes. This enables image description generation that better accommodates\nuser preferences.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:17:12 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wang", "Yufei", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Cohen", "Scott", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1704.07019", "submitter": "Yandong Guo", "authors": "Yandong Guo, Cheng Lu, Jan P. Allebach, and Charles A. Bouman", "title": "Model-based Iterative Restoration for Binary Document Image Compression\n  with Dictionary Learning", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inherent noise in the observed (e.g., scanned) binary document image\ndegrades the image quality and harms the compression ratio through breaking the\npattern repentance and adding entropy to the document images. In this paper, we\ndesign a cost function in Bayesian framework with dictionary learning.\nMinimizing our cost function produces a restored image which has better quality\nthan that of the observed noisy image, and a dictionary for representing and\nencoding the image. After the restoration, we use this dictionary (from the\nsame cost function) to encode the restored image following the\nsymbol-dictionary framework by JBIG2 standard with the lossless mode.\nExperimental results with a variety of document images demonstrate that our\nmethod improves the image quality compared with the observed image, and\nsimultaneously improves the compression ratio. For the test images with\nsynthetic noise, our method reduces the number of flipped pixels by 48.2% and\nimproves the compression ratio by 36.36% as compared with the best encoding\nmethods. For the test images with real noise, our method visually improves the\nimage quality, and outperforms the cutting-edge method by 28.27% in terms of\nthe compression ratio.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 02:31:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Guo", "Yandong", ""], ["Lu", "Cheng", ""], ["Allebach", "Jan P.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1704.07023", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Lan Tang, Xin Liu", "title": "Group-based Sparse Representation for Image Compressive Sensing\n  Reconstruction with Non-Convex Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patch-based sparse representation modeling has shown great potential in image\ncompressive sensing (CS) reconstruction. However, this model usually suffers\nfrom some limits, such as dictionary learning with great computational\ncomplexity, neglecting the relationship among similar patches. In this paper, a\ngroup-based sparse representation method with non-convex regularization\n(GSR-NCR) for image CS reconstruction is proposed. In GSR-NCR, the local\nsparsity and nonlocal self-similarity of images is simultaneously considered in\na unified framework. Different from the previous methods based on\nsparsity-promoting convex regularization, we extend the non-convex weighted Lp\n(0 < p < 1) penalty function on group sparse coefficients of the data matrix,\nrather than conventional L1-based regularization. To reduce the computational\ncomplexity, instead of learning the dictionary with a high computational\ncomplexity from natural images, we learn the principle component analysis (PCA)\nbased dictionary for each group. Moreover, to make the proposed scheme\ntractable and robust, we have developed an efficient iterative\nshrinkage/thresholding algorithm to solve the non-convex optimization problem.\nExperimental results demonstrate that the proposed method outperforms many\nstate-of-the-art techniques for image CS reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 03:17:20 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 03:58:42 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Zhang", "Xinggan", ""], ["Wang", "Qiong", ""], ["Tang", "Lan", ""], ["Liu", "Xin", ""]]}, {"id": "1704.07056", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xinggan Zhang, Yu Wu, Qiong Wang and Lan Tang", "title": "Non-Convex Weighted Lp Nuclear Norm based ADMM Framework for Image\n  Restoration", "comments": "arXiv admin note: text overlap with arXiv:1611.08983", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the matrix formed by nonlocal similar patches in a natural image is of\nlow rank, the nuclear norm minimization (NNM) has been widely used in various\nimage processing studies. Nonetheless, nuclear norm based convex surrogate of\nthe rank function usually over-shrinks the rank components and makes different\ncomponents equally, and thus may produce a result far from the optimum. To\nalleviate the above-mentioned limitations of the nuclear norm, in this paper we\npropose a new method for image restoration via the non-convex weighted Lp\nnuclear norm minimization (NCW-NNM), which is able to more accurately enforce\nthe image structural sparsity and self-similarity simultaneously. To make the\nproposed model tractable and robust, the alternative direction multiplier\nmethod (ADMM) is adopted to solve the associated non-convex minimization\nproblem. Experimental results on various types of image restoration problems,\nincluding image deblurring, image inpainting and image compressive sensing (CS)\nrecovery, demonstrate that the proposed method outperforms many current\nstate-of-the-art methods in both the objective and the perceptual qualities.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 07:02:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 05:33:48 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Zhang", "Xinggan", ""], ["Wu", "Yu", ""], ["Wang", "Qiong", ""], ["Tang", "Lan", ""]]}, {"id": "1704.07063", "submitter": "Hong Sun", "authors": "Hong Sun, Chen-guang Liu, Cheng-wei Sang", "title": "A Dual Sparse Decomposition Method for Image Denoising", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the image denoising problem in the situations of\nstrong noise. We propose a dual sparse decomposition method. This method makes\na sub-dictionary decomposition on the over-complete dictionary in the sparse\ndecomposition. The sub-dictionary decomposition makes use of a novel criterion\nbased on the occurrence frequency of atoms of the over-complete dictionary over\nthe data set. The experimental results demonstrate that the\ndual-sparse-decomposition method surpasses state-of-art denoising performance\nin terms of both peak-signal-to-noise ratio and\nstructural-similarity-index-metric, and also at subjective visual quality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 07:28:32 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Sun", "Hong", ""], ["Liu", "Chen-guang", ""], ["Sang", "Cheng-wei", ""]]}, {"id": "1704.07072", "submitter": "Benjamin Busam", "authors": "Benjamin Busam and Tolga Birdal and Nassir Navab", "title": "Camera Pose Filtering with Local Regression Geodesics on the Riemannian\n  Manifold of Dual Quaternions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying, smooth trajectory estimation is of great interest to the vision\ncommunity for accurate and well behaving 3D systems. In this paper, we propose\na novel principal component local regression filter acting directly on the\nRiemannian manifold of unit dual quaternions $\\mathbb{D} \\mathbb{H}_1$. We use\na numerically stable Lie algebra of the dual quaternions together with $\\exp$\nand $\\log$ operators to locally linearize the 6D pose space. Unlike state of\nthe art path smoothing methods which either operate on $SO\\left(3\\right)$ of\nrotation matrices or the hypersphere $\\mathbb{H}_1$ of quaternions, we treat\nthe orientation and translation jointly on the dual quaternion quadric in the\n7-dimensional real projective space $\\mathbb{R}\\mathbb{P}^7$. We provide an\noutlier-robust IRLS algorithm for generic pose filtering exploiting this\nmanifold structure. Besides our theoretical analysis, our experiments on\nsynthetic and real data show the practical advantages of the manifold aware\nfiltering on pose tracking and smoothing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 07:52:41 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 22:34:06 GMT"}, {"version": "v3", "created": "Thu, 4 May 2017 15:46:29 GMT"}, {"version": "v4", "created": "Tue, 29 Aug 2017 16:28:16 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Busam", "Benjamin", ""], ["Birdal", "Tolga", ""], ["Navab", "Nassir", ""]]}, {"id": "1704.07077", "submitter": "Han-Mu Park", "authors": "Han-Mu Park and Kuk-Jin Yoon", "title": "Exploiting Multi-layer Graph Factorization for Multi-attributed Graph\n  Matching", "comments": "10 pages, 4 figures, conference submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-attributed graph matching is a problem of finding correspondences\nbetween two sets of data while considering their complex properties described\nin multiple attributes. However, the information of multiple attributes is\nlikely to be oversimplified during a process that makes an integrated\nattribute, and this degrades the matching accuracy. For that reason, a\nmulti-layer graph structure-based algorithm has been proposed recently. It can\neffectively avoid the problem by separating attributes into multiple layers.\nNonetheless, there are several remaining issues such as a scalability problem\ncaused by the huge matrix to describe the multi-layer structure and a\nback-projection problem caused by the continuous relaxation of the quadratic\nassignment problem. In this work, we propose a novel multi-attributed graph\nmatching algorithm based on the multi-layer graph factorization. We reformulate\nthe problem to be solved with several small matrices that are obtained by\nfactorizing the multi-layer structure. Then, we solve the problem using a\nconvex-concave relaxation procedure for the multi-layer structure. The proposed\nalgorithm exhibits better performance than state-of-the-art algorithms based on\nthe single-layer structure.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 08:08:32 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Park", "Han-Mu", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1704.07082", "submitter": "Zaidao Wen", "authors": "Biao Hou, Zaidao Wen, Licheng Jiao and Qian Wu", "title": "Target Oriented High Resolution SAR Image Formation via Semantic\n  Information Guided Regularizations", "comments": "Submitted to IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2017.2769808", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-regularized synthetic aperture radar (SAR) imaging framework has\nshown its remarkable performance to generate a feature enhanced high resolution\nimage, in which a sparsity-inducing regularizer is involved by exploiting the\nsparsity priors of some visual features in the underlying image. However, since\nthe simple prior of low level features are insufficient to describe different\nsemantic contents in the image, this type of regularizer will be incapable of\ndistinguishing between the target of interest and unconcerned background\nclutters. As a consequence, the features belonging to the target and clutters\nare simultaneously affected in the generated image without concerning their\nunderlying semantic labels. To address this problem, we propose a novel\nsemantic information guided framework for target oriented SAR image formation,\nwhich aims at enhancing the interested target scatters while suppressing the\nbackground clutters. Firstly, we develop a new semantics-specific regularizer\nfor image formation by exploiting the statistical properties of different\nsemantic categories in a target scene SAR image. In order to infer the semantic\nlabel for each pixel in an unsupervised way, we moreover induce a novel\nhigh-level prior-driven regularizer and some semantic causal rules from the\nprior knowledge. Finally, our regularized framework for image formation is\nfurther derived as a simple iteratively reweighted $\\ell_1$ minimization\nproblem which can be conveniently solved by many off-the-shelf solvers.\nExperimental results demonstrate the effectiveness and superiority of our\nframework for SAR image formation in terms of target enhancement and clutters\nsuppression, compared with the state of the arts. Additionally, the proposed\nframework opens a new direction of devoting some machine learning strategies to\nimage formation, which can benefit the subsequent decision making tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 08:27:06 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hou", "Biao", ""], ["Wen", "Zaidao", ""], ["Jiao", "Licheng", ""], ["Wu", "Qian", ""]]}, {"id": "1704.07085", "submitter": "Yeong-Jun Cho", "authors": "Yeong-Jun Cho, Jae-Han Park, Su-A Kim, Kyuewang Lee, Kuk-Jin Yoon", "title": "Unified Framework for Automated Person Re-identification and Camera\n  Network Topology Inference in Camera Networks", "comments": "Accepted to International Workshop on Cross-domain Human\n  Identification (in conjunction with ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification in large-scale multi-camera networks is a\nchallenging task because of the spatio-temporal uncertainty and high complexity\ndue to large numbers of cameras and people. To handle these difficulties,\nadditional information such as camera network topology should be provided,\nwhich is also difficult to automatically estimate. In this paper, we propose a\nunified framework which jointly solves both person re-id and camera network\ntopology inference problems. The proposed framework takes general multi-camera\nnetwork environments into account. To effectively show the superiority of the\nproposed framework, we also provide a new person re-id dataset with full\nannotations, named SLP, captured in the synchronized multi-camera network.\nExperimental results show that the proposed methods are promising for both\nperson re-id and camera topology inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 08:39:16 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 02:31:52 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 06:12:22 GMT"}, {"version": "v4", "created": "Sat, 26 Aug 2017 11:40:13 GMT"}, {"version": "v5", "created": "Mon, 2 Oct 2017 03:45:03 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Cho", "Yeong-Jun", ""], ["Park", "Jae-Han", ""], ["Kim", "Su-A", ""], ["Lee", "Kyuewang", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1704.07121", "submitter": "Hexiang Hu", "authors": "Wei-Lun Chao, Hexiang Hu, Fei Sha", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better\n  Visual Question Answering Datasets", "comments": "Accepted for Oral Presentation at NAACL-HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (Visual QA) has attracted a lot of attention\nlately, seen essentially as a form of (visual) Turing test that artificial\nintelligence should strive to achieve. In this paper, we study a crucial\ncomponent of this task: how can we design good datasets for the task? We focus\non the design of multiple-choice based datasets where the learner has to select\nthe right answer from a set of candidate ones including the target (\\ie the\ncorrect one) and the decoys (\\ie the incorrect ones). Through careful analysis\nof the results attained by state-of-the-art learning models and human\nannotators on existing datasets, we show that the design of the decoy answers\nhas a significant impact on how and what the learning models learn from the\ndatasets. In particular, the resulting learner can ignore the visual\ninformation, the question, or both while still doing well on the task. Inspired\nby this, we propose automatic procedures to remedy such design deficiencies. We\napply the procedures to re-construct decoy answers for two popular Visual QA\ndatasets as well as to create a new Visual QA dataset from the Visual Genome\nproject, resulting in the largest dataset for this task. Extensive empirical\nstudies show that the design deficiencies have been alleviated in the remedied\ndatasets and the performance on them is likely a more faithful indicator of the\ndifference among learning models. The datasets are released and publicly\navailable via http://www.teds.usc.edu/website_vqa/.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:05:19 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 20:34:21 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chao", "Wei-Lun", ""], ["Hu", "Hexiang", ""], ["Sha", "Fei", ""]]}, {"id": "1704.07129", "submitter": "Spandana Gella", "authors": "Spandana Gella, Frank Keller", "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks", "comments": "To appear in Proceedings of ACL 2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of recent research has focused on tasks that combine language\nand vision, resulting in a proliferation of datasets and methods. One such task\nis action recognition, whose applications include image annotation, scene\nunder- standing and image retrieval. In this survey, we categorize the existing\nap- proaches based on how they conceptualize this problem and provide a\ndetailed review of existing datasets, highlighting their di- versity as well as\nadvantages and disad- vantages. We focus on recently devel- oped datasets which\nlink visual informa- tion with linguistic resources and provide a fine-grained\nsyntactic and semantic anal- ysis of actions in images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:38:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Gella", "Spandana", ""], ["Keller", "Frank", ""]]}, {"id": "1704.07142", "submitter": "Shu Zhang", "authors": "Shu Zhang, Hui Yu, Ting Wang, Junyu Dong and Honghai Liu", "title": "Dense 3D Facial Reconstruction from a Single Depth Image in\n  Unconstrained Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing demands of applications in virtual reality such as 3D\nfilms, virtual Human-Machine Interactions and virtual agents, the analysis of\n3D human face analysis is considered to be more and more important as a\nfundamental step for those virtual reality tasks. Due to information provided\nby an additional dimension, 3D facial reconstruction enables aforementioned\ntasks to be achieved with higher accuracy than those based on 2D facial\nanalysis. The denser the 3D facial model is, the more information it could\nprovide. However, most existing dense 3D facial reconstruction methods require\ncomplicated processing and high system cost. To this end, this paper presents a\nnovel method that simplifies the process of dense 3D facial reconstruction by\nemploying only one frame of depth data obtained with an off-the-shelf RGB-D\nsensor. The experiments showed competitive results with real world data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:58:47 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zhang", "Shu", ""], ["Yu", "Hui", ""], ["Wang", "Ting", ""], ["Dong", "Junyu", ""], ["Liu", "Honghai", ""]]}, {"id": "1704.07160", "submitter": "Congqi Cao", "authors": "Congqi Cao, Yifan Zhang, Chunjie Zhang and Hanqing Lu", "title": "Body Joint guided 3D Deep Convolutional Descriptors for Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dimensional convolutional neural networks (3D CNNs) have been\nestablished as a powerful tool to simultaneously learn features from both\nspatial and temporal dimensions, which is suitable to be applied to video-based\naction recognition. In this work, we propose not to directly use the\nactivations of fully-connected layers of a 3D CNN as the video feature, but to\nuse selective convolutional layer activations to form a discriminative\ndescriptor for video. It pools the feature on the convolutional layers under\nthe guidance of body joint positions. Two schemes of mapping body joints into\nconvolutional feature maps for pooling are discussed. The body joint positions\ncan be obtained from any off-the-shelf skeleton estimation algorithm. The\nhelpfulness of the body joint guided feature pooling with inaccurate skeleton\nestimation is systematically evaluated. To make it end-to-end and do not rely\non any sophisticated body joint detection algorithm, we further propose a\ntwo-stream bilinear model which can learn the guidance from the body joints and\ncapture the spatio-temporal features simultaneously. In this model, the body\njoint guided feature pooling is conveniently formulated as a bilinear product\noperation. Experimental results on three real-world datasets demonstrate the\neffectiveness of body joint guided pooling which achieves promising\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:58:24 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 15:08:05 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Cao", "Congqi", ""], ["Zhang", "Yifan", ""], ["Zhang", "Chunjie", ""], ["Lu", "Hanqing", ""]]}, {"id": "1704.07163", "submitter": "Chang-Ryeol Lee", "authors": "Chang-Ryeol Lee and Kuk-Jin Yoon", "title": "Monocular Visual Odometry with a Rolling Shutter Camera", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rolling Shutter (RS) cameras have become popularized because of low-cost\nimaging capability. However, the RS cameras suffer from undesirable artifacts\nwhen the camera or the subject is moving, or illumination condition changes.\nFor that reason, Monocular Visual Odometry (MVO) with RS cameras produces\ninaccurate ego-motion estimates. Previous works solve this RS distortion\nproblem with motion prediction from images and/or inertial sensors. However,\nthe MVO still has trouble in handling the RS distortion when the camera motion\nchanges abruptly (e.g. vibration of mobile cameras causes extremely fast motion\ninstantaneously). To address the problem, we propose the novel MVO algorithm in\nconsideration of the geometric characteristics of RS cameras. The key idea of\nthe proposed algorithm is the new RS essential matrix which incorporates the\ninstantaneous angular and linear velocities at each frame. Our algorithm\nproduces accurate and robust ego-motion estimates in an online manner, and is\napplicable to various mobile applications with RS cameras. The superiority of\nthe proposed algorithm is validated through quantitative and qualitative\ncomparison on both synthetic and real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 12:02:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Lee", "Chang-Ryeol", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1704.07239", "submitter": "Xiao Han", "authors": "Xiao Han", "title": "Automatic Liver Lesion Segmentation Using A Deep Convolutional Neural\n  Network Method", "comments": "Submission for ISBI'2017 LiTS Challenge ISIC2017", "journal-ref": null, "doi": "10.1002/mp.12155", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liver lesion segmentation is an important step for liver cancer diagnosis,\ntreatment planning and treatment evaluation. LiTS (Liver Tumor Segmentation\nChallenge) provides a common testbed for comparing different automatic liver\nlesion segmentation methods. We participate in this challenge by developing a\ndeep convolutional neural network (DCNN) method. The particular DCNN model\nworks in 2.5D in that it takes a stack of adjacent slices as input and produces\nthe segmentation map corresponding to the center slice. The model has 32 layers\nin total and makes use of both long range concatenation connections of U-Net\n[1] and short-range residual connections from ResNet [2]. The model was trained\nusing the 130 LiTS training datasets and achieved an average Dice score of 0.67\nwhen evaluated on the 70 test CT scans, which ranked first for the LiTS\nchallenge at the time of the ISBI 2017 conference.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:58:29 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Han", "Xiao", ""]]}, {"id": "1704.07242", "submitter": "Hengyue Pan", "authors": "Hengyue Pan and Hui Jiang", "title": "Supervised Adversarial Networks for Image Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, Generative Adversarial Network (GAN) became a\nprevalent research topic. By defining two convolutional neural networks\n(G-Network and D-Network) and introducing an adversarial procedure between them\nduring the training process, GAN has ability to generate good quality images\nthat look like natural images from a random vector. Besides image generation,\nGAN may have potential to deal with wide range of real world problems. In this\npaper, we follow the basic idea of GAN and propose a novel model for image\nsaliency detection, which is called Supervised Adversarial Networks (SAN).\nSpecifically, SAN also trains two models simultaneously: the G-Network takes\nnatural images as inputs and generates corresponding saliency maps (synthetic\nsaliency maps), and the D-Network is trained to determine whether one sample is\na synthetic saliency map or ground-truth saliency map. However, different from\nGAN, the proposed method uses fully supervised learning to learn both G-Network\nand D-Network by applying class labels of the training set. Moreover, a novel\nkind of layer call conv-comparison layer is introduced into the D-Network to\nfurther improve the saliency performance by forcing the high-level feature of\nsynthetic saliency maps and ground-truthes as similar as possible. Experimental\nresults on Pascal VOC 2012 database show that the SAN model can generate high\nquality saliency maps for many complicate natural images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 14:06:18 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 01:37:03 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Pan", "Hengyue", ""], ["Jiang", "Hui", ""]]}, {"id": "1704.07244", "submitter": "Jieqing Jiao Jieqing Jiao", "authors": "Jieqing Jiao and Sebastien Ourselin", "title": "Fast PET reconstruction using Multi-scale Fully Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of PET images is an ill-posed inverse problem and often\nrequires iterative algorithms to achieve good image quality for reliable\nclinical use in practice, at huge computational costs. In this paper, we\nconsider the PET reconstruction a dense prediction problem where the large\nscale contextual information is essential, and propose a novel architecture of\nmulti-scale fully convolutional neural networks (msfCNN) for fast PET image\nreconstruction. The proposed msfCNN gains large receptive fields with both\nmemory and computational efficiency, by using a downscaling-upscaling structure\nand dilated convolutions. Instead of pooling and deconvolution, we propose to\nuse the periodic shuffling operation from sub-pixel convolution and its inverse\nto scale the size of feature maps without losing resolution. Residual\nconnections were added to improve training. We trained the proposed msfCNN\nmodel with simulated data, and applied it to clinical PET data acquired on a\nSiemens mMR scanner. The results from real oncological and neurodegenerative\ncases show that the proposed msfCNN-based reconstruction outperforms the\niterative approaches in terms of computational time while achieving comparable\nimage quality for quantification. The proposed msfCNN model can be applied to\nother dense prediction tasks, and fast msfCNN-based PET reconstruction could\nfacilitate the potential use of molecular imaging in interventional/surgical\nprocedures, where cancer surgery can particularly benefit.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 14:09:22 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Jiao", "Jieqing", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1704.07293", "submitter": "Tobias B\\\"ottger", "authors": "Tobias Bottger, Patrick Follmann, Michael Fauser", "title": "Measuring the Accuracy of Object Detectors and Trackers", "comments": "10 pages, 7 Figures", "journal-ref": null, "doi": "10.1007/978-3-319-66709-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of object detectors and trackers is most commonly evaluated by\nthe Intersection over Union (IoU) criterion. To date, most approaches are\nrestricted to axis-aligned or oriented boxes and, as a consequence, many\ndatasets are only labeled with boxes. Nevertheless, axis-aligned or oriented\nboxes cannot accurately capture an object's shape. To address this, a number of\ndensely segmented datasets has started to emerge in both the object detection\nand the object tracking communities. However, evaluating the accuracy of object\ndetectors and trackers that are restricted to boxes on densely segmented data\nis not straightforward. To close this gap, we introduce the relative\nIntersection over Union (rIoU) accuracy measure. The measure normalizes the IoU\nwith the optimal box for the segmentation to generate an accuracy measure that\nranges between 0 and 1 and allows a more precise measurement of accuracies.\nFurthermore, it enables an efficient and easy way to understand scenes and the\nstrengths and weaknesses of an object detection or tracking approach. We\ndisplay how the new measure can be efficiently calculated and present an\neasy-to-use evaluation framework. The framework is tested on the DAVIS and the\nVOT2016 segmentations and has been made available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:41:35 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Bottger", "Tobias", ""], ["Follmann", "Patrick", ""], ["Fauser", "Michael", ""]]}, {"id": "1704.07296", "submitter": "Pei Xu", "authors": "Pei Xu", "title": "A Real-time Hand Gesture Recognition and Human-Computer Interaction\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we design a real-time human-computer interaction system\nbased on hand gesture. The whole system consists of three components: hand\ndetection, gesture recognition and human-computer interaction (HCI) based on\nrecognition; and realizes the robust control of mouse and keyboard events with\na higher accuracy of gesture recognition. Specifically, we use the\nconvolutional neural network (CNN) to recognize gestures and makes it\nattainable to identify relatively complex gestures using only one cheap\nmonocular camera. We introduce the Kalman filter to estimate the hand position\nbased on which the mouse cursor control is realized in a stable and smooth way.\nDuring the HCI stage, we develop a simple strategy to avoid the false\nrecognition caused by noises - mostly transient, false gestures, and thus to\nimprove the reliability of interaction. The developed system is highly\nextendable and can be used in human-robotic or other human-machine interaction\nscenarios with more complex command formats rather than just mouse and keyboard\nevents.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:44:56 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Xu", "Pei", ""]]}, {"id": "1704.07312", "submitter": "Kumar Sankar Ray", "authors": "Kumar S Ray, Sayandip Dutta, Anit Chakraborty", "title": "Detection, Recognition and Tracking of Moving Objects from Real-time\n  Video via SP Theory of Intelligence and Species Inspired PSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the basic problem of recognizing moving objects in\nvideo images using SP Theory of Intelligence. The concept of SP Theory of\nIntelligence which is a framework of artificial intelligence, was first\nintroduced by Gerard J Wolff, where S stands for Simplicity and P stands for\nPower. Using the concept of multiple alignment, we detect and recognize object\nof our interest in video frames with multilevel hierarchical parts and\nsubparts, based on polythetic categories. We track the recognized objects using\nthe species based Particle Swarm Optimization (PSO). First, we extract the\nmultiple alignment of our object of interest from training images. In order to\nrecognize accurately and handle occlusion, we use the polythetic concepts on\nraw data line to omit the redundant noise via searching for best alignment\nrepresenting the features from the extracted alignments. We recognize the\ndomain of interest from the video scenes in form of wide variety of multiple\nalignments to handle scene variability. Unsupervised learning is done in the SP\nmodel following the DONSVIC principle and natural structures are discovered via\ninformation compression and pattern analysis. After successful recognition of\nobjects, we use species based PSO algorithm as the alignments of our object of\ninterest is analogues to observation likelihood and fitness ability of species.\nSubsequently, we analyze the competition and repulsion among species with\nannealed Gaussian based PSO. We have tested our algorithms on David, Walking2,\nFaceOcc1, Jogging and Dudek, obtaining very satisfactory and competitive\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 10:42:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ray", "Kumar S", ""], ["Dutta", "Sayandip", ""], ["Chakraborty", "Anit", ""]]}, {"id": "1704.07325", "submitter": "Jia Xu", "authors": "Jia Xu and Ren\\'e Ranftl and Vladlen Koltun", "title": "Accurate Optical Flow via Direct Cost Volume Processing", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optical flow estimation approach that operates on the full\nfour-dimensional cost volume. This direct approach shares the structural\nbenefits of leading stereo matching pipelines, which are known to yield high\naccuracy. To this day, such approaches have been considered impractical due to\nthe size of the cost volume. We show that the full four-dimensional cost volume\ncan be constructed in a fraction of a second due to its regularity. We then\nexploit this regularity further by adapting semi-global matching to the\nfour-dimensional setting. This yields a pipeline that achieves significantly\nhigher accuracy than state-of-the-art optical flow methods while being faster\nthan most. Our approach outperforms all published general-purpose optical flow\nmethods on both Sintel and KITTI 2015 benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:03:53 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Xu", "Jia", ""], ["Ranftl", "Ren\u00e9", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1704.07333", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari, Ross Girshick, Piotr Doll\\'ar, Kaiming He", "title": "Detecting and Recognizing Human-Object Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the visual world, a machine must not only recognize individual\nobject instances but also how they interact. Humans are often at the center of\nsuch interactions and detecting human-object interactions is an important\npractical and scientific problem. In this paper, we address the task of\ndetecting <human, verb, object> triplets in challenging everyday photos. We\npropose a novel model that is driven by a human-centric approach. Our\nhypothesis is that the appearance of a person -- their pose, clothing, action\n-- is a powerful cue for localizing the objects they are interacting with. To\nexploit this cue, our model learns to predict an action-specific density over\ntarget object locations based on the appearance of a detected person. Our model\nalso jointly learns to detect people and objects, and by fusing these\npredictions it efficiently infers interaction triplets in a clean, jointly\ntrained end-to-end system we call InteractNet. We validate our approach on the\nrecently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show\nquantitatively compelling results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:14:24 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:10:24 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 02:57:19 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["He", "Kaiming", ""]]}, {"id": "1704.07355", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e (Technicolor) and Anne-Marie Kermarrec (Inria) and\n  Nicolas Le Scouarnec (Technicolor)", "title": "Accelerated Nearest Neighbor Search with Quick ADC", "comments": "8 pages, 5 figures, published in Proceedings of ICMR'17, Bucharest,\n  Romania, June 06-09, 2017", "journal-ref": null, "doi": "10.1145/3078971.3078992", "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:49:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Andr\u00e9", "Fabien", "", "Technicolor"], ["Kermarrec", "Anne-Marie", "", "Inria"], ["Scouarnec", "Nicolas Le", "", "Technicolor"]]}, {"id": "1704.07402", "submitter": "Hamed R. Tavakoli", "authors": "Hamed R. Tavakoli, Jorma Laaksonen", "title": "Towards Instance Segmentation with Object Priority: Prominent Object\n  Detection and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript introduces the problem of prominent object detection and\nrecognition inspired by the fact that human seems to priorities perception of\nscene elements. The problem deals with finding the most important region of\ninterest, segmenting the relevant item/object in that area, and assigning it an\nobject class label. In other words, we are solving the three problems of\nsaliency modeling, saliency detection, and object recognition under one\numbrella. The motivation behind such a problem formulation is (1) the benefits\nto the knowledge representation-based vision pipelines, and (2) the potential\nimprovements in emulating bio-inspired vision systems by solving these three\nproblems together. We are foreseeing extending this problem formulation to\nfully semantically segmented scenes with instance object priority for\nhigh-level inferences in various applications including assistive vision. Along\nwith a new problem definition, we also propose a method to achieve such a task.\nThe proposed model predicts the most important area in the image, segments the\nassociated objects, and labels them. The proposed problem and method are\nevaluated against human fixations, annotated segmentation masks, and object\nclass categories. We define a chance level for each of the evaluation criterion\nto compare the proposed algorithm with. Despite the good performance of the\nproposed baseline, the overall evaluations indicate that the problem of\nprominent object detection and recognition is a challenging task that is still\nworth investigating further.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 18:12:12 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 12:15:34 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Tavakoli", "Hamed R.", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1704.07434", "submitter": "Hamed R. Tavakoli", "authors": "Hamed R. Tavakoli, Rakshith Shetty, Ali Borji, Jorma Laaksonen", "title": "Paying Attention to Descriptions Generated by Image Captioning Models", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To bridge the gap between humans and machines in image understanding and\ndescribing, we need further insight into how people describe a perceived scene.\nIn this paper, we study the agreement between bottom-up saliency-based visual\nattention and object referrals in scene description constructs. We investigate\nthe properties of human-written descriptions and machine-generated ones. We\nthen propose a saliency-boosted image captioning model in order to investigate\nbenefits from low-level cues in language models. We learn that (1) humans\nmention more salient objects earlier than less salient ones in their\ndescriptions, (2) the better a captioning model performs, the better attention\nagreement it has with human descriptions, (3) the proposed saliency-boosted\nmodel, compared to its baseline form, does not improve significantly on the MS\nCOCO database, indicating explicit bottom-up boosting does not help when the\ntask is well learnt and tuned on a data, (4) a better generalization is,\nhowever, observed for the saliency-boosted model on unseen data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 19:51:16 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 10:13:45 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 11:24:45 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Tavakoli", "Hamed R.", ""], ["Shetty", "Rakshith", ""], ["Borji", "Ali", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1704.07489", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Mohit Bansal", "title": "Multi-Task Video Captioning with Video and Entailment Generation", "comments": "ACL 2017 (14 pages w/ supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning, the task of describing the content of a video, has seen\nsome promising improvements in recent years with sequence-to-sequence models,\nbut accurately learning the temporal and logical dynamics involved in the task\nstill remains a challenge, especially given the lack of sufficient annotated\ndata. We improve video captioning by sharing knowledge with two related\ndirected-generation tasks: a temporally-directed unsupervised video prediction\ntask to learn richer context-aware video encoder representations, and a\nlogically-directed language entailment generation task to learn better\nvideo-entailed caption decoder representations. For this, we present a\nmany-to-many multi-task learning model that shares parameters across the\nencoders and decoders of the three tasks. We achieve significant improvements\nand the new state-of-the-art on several standard video captioning datasets\nusing diverse automatic and human evaluations. We also show mutual multi-task\nimprovements on the entailment generation task.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 23:07:32 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 17:08:58 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1704.07490", "submitter": "Manuel Marques", "authors": "Miguel Costa and Beatriz Quintino Ferreira and Manuel Marques", "title": "A Context Aware and Video-Based Risk Descriptor for Cyclists", "comments": "Submitted to ITSC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aiming to reduce pollutant emissions, bicycles are regaining popularity\nspecially in urban areas. However, the number of cyclists' fatalities is not\nshowing the same decreasing trend as the other traffic groups. Hence,\nmonitoring cyclists' data appears as a keystone to foster urban cyclists'\nsafety by helping urban planners to design safer cyclist routes. In this work,\nwe propose a fully image-based framework to assess the rout risk from the\ncyclist perspective. From smartphone sequences of images, this generic\nframework is able to automatically identify events considering different risk\ncriteria based on the cyclist's motion and object detection. Moreover, since it\nis entirely based on images, our method provides context on the situation and\nis independent from the expertise level of the cyclist. Additionally, we build\non an existing platform and introduce several improvements on its mobile app to\nacquire smartphone sensor data, including video. From the inertial sensor data,\nwe automatically detect the route segments performed by bicycle, applying\nbehavior analysis techniques. We test our methods on real data, attaining very\npromising results in terms of risk classification, according to two different\ncriteria, and behavior analysis accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 23:10:05 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Costa", "Miguel", ""], ["Ferreira", "Beatriz Quintino", ""], ["Marques", "Manuel", ""]]}, {"id": "1704.07502", "submitter": "Yongliang Chen", "authors": "Yongliang Chen", "title": "A Labeling-Free Approach to Supervising Deep Neural Networks for Retinal\n  Blood Vessel Segmentation", "comments": "10 pages, 8 figures, 3 tables, forbidden work, correct the citation\n  typo of [29]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting blood vessels in fundus imaging plays an important role in medical\ndiagnosis. Many algorithms have been proposed. While deep Neural Networks have\nbeen attracting enormous attention from computer vision community recent years\nand several novel works have been done in terms of its application in retinal\nblood vessel segmentation, most of them are based on supervised learning which\nrequires amount of labeled data, which is both scarce and expensive to obtain.\nWe leverage the power of Deep Convolutional Neural Networks (DCNN) in feature\nlearning, in this work, to achieve this ultimate goal. The highly efficient\nfeature learning of DCNN inspires our novel approach that trains the networks\nwith automatically-generated samples to achieve desirable performance on\nreal-world fundus images. For this, we design a set of rules abstracted from\nthe domain-specific prior knowledge to generate these samples. We argue that,\nwith the high efficiency of DCNN in feature learning, one can achieve this goal\nby constructing the training dataset with prior knowledge, no manual labeling\nis needed. This approach allows us to take advantages of supervised learning\nwithout labeling. We also build a naive DCNN model to test it. The results on\nstandard benchmarks of fundus imaging show it is competitive to the\nstate-of-the-art methods which implies a potential way to leverage the power of\nDCNN in feature learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:04:21 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 12:13:47 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Chen", "Yongliang", ""]]}, {"id": "1704.07575", "submitter": "Huiguang He", "authors": "Changde Du, Changying Du, Huiguang He", "title": "Sharing deep generative representation for perceived image\n  reconstruction from human brain activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding human brain activities via functional magnetic resonance imaging\n(fMRI) has gained increasing attention in recent years. While encouraging\nresults have been reported in brain states classification tasks, reconstructing\nthe details of human visual experience still remains difficult. Two main\nchallenges that hinder the development of effective models are the perplexing\nfMRI measurement noise and the high dimensionality of limited data instances.\nExisting methods generally suffer from one or both of these issues and yield\ndissatisfactory results. In this paper, we tackle this problem by casting the\nreconstruction of visual stimulus as the Bayesian inference of missing view in\na multiview latent variable model. Sharing a common latent representation, our\njoint generative model of external stimulus and brain response is not only\n\"deep\" in extracting nonlinear features from visual images, but also powerful\nin capturing correlations among voxel activities of fMRI recordings. The\nnonlinearity and deep structure endow our model with strong representation\nability, while the correlations of voxel activities are critical for\nsuppressing noise and improving prediction. We devise an efficient variational\nBayesian method to infer the latent variables and the model parameters. To\nfurther improve the reconstruction accuracy, the latent representations of\ntesting instances are enforced to be close to that of their neighbours from the\ntraining set via posterior regularization. Experiments on three fMRI recording\ndatasets demonstrate that our approach can more accurately reconstruct visual\nstimuli.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 08:20:42 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 05:16:14 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 01:50:34 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Du", "Changde", ""], ["Du", "Changying", ""], ["He", "Huiguang", ""]]}, {"id": "1704.07576", "submitter": "Vamsi Kiran Adhikarla", "authors": "Vamsi Kiran Adhikarla and Marek Vinkler and Denis Sumin and Rafa{\\l}\n  K. Mantiuk and Karol Myszkowski and Hans-Peter Seidel and Piotr Didyk", "title": "Towards a quality metric for dense light fields", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2017", "doi": "10.1109/CVPR.2017.396", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light fields become a popular representation of three dimensional scenes, and\nthere is interest in their processing, resampling, and compression. As those\noperations often result in loss of quality, there is a need to quantify it. In\nthis work, we collect a new dataset of dense reference and distorted light\nfields as well as the corresponding quality scores which are scaled in\nperceptual units. The scores were acquired in a subjective experiment using an\ninteractive light-field viewing setup. The dataset contains typical artifacts\nthat occur in light-field processing chain due to light-field reconstruction,\nmulti-view compression, and limitations of automultiscopic displays. We test a\nnumber of existing objective quality metrics to determine how well they can\npredict the quality of light fields. We find that the existing image quality\nmetrics provide good measures of light-field quality, but require dense\nreference light- fields for optimal performance. For more complex tasks of\ncomparing two distorted light fields, their performance drops significantly,\nwhich reveals the need for new, light-field-specific metrics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 08:21:47 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Adhikarla", "Vamsi Kiran", ""], ["Vinkler", "Marek", ""], ["Sumin", "Denis", ""], ["Mantiuk", "Rafa\u0142 K.", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Didyk", "Piotr", ""]]}, {"id": "1704.07595", "submitter": "Di Xie", "authors": "Chao Li and Qiaoyong Zhong and Di Xie and Shiliang Pu", "title": "Skeleton-based Action Recognition with Convolutional Neural Networks", "comments": "ICMEW 2017", "journal-ref": null, "doi": "10.1109/LSP.2017.2678539", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches to skeleton-based action recognition are\nmostly based on recurrent neural networks (RNN). In this paper, we propose a\nnovel convolutional neural networks (CNN) based framework for both action\nclassification and detection. Raw skeleton coordinates as well as skeleton\nmotion are fed directly into CNN for label prediction. A novel skeleton\ntransformer module is designed to rearrange and select important skeleton\njoints automatically. With a simple 7-layer network, we obtain 89.3% accuracy\non validation set of the NTU RGB+D dataset. For action detection in untrimmed\nvideos, we develop a window proposal network to extract temporal segment\nproposals, which are further classified within the same network. On the recent\nPKU-MMD dataset, we achieve 93.7% mAP, surpassing the baseline by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 09:09:00 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Li", "Chao", ""], ["Zhong", "Qiaoyong", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1704.07632", "submitter": "Jeong-Kyun Lee", "authors": "Jeong-Kyun Lee, Jae-Won Yea, Min-Gyu Park, and Kuk-Jin Yoon", "title": "Joint Layout Estimation and Global Multi-View Registration for Indoor\n  Reconstruction", "comments": "Accepted to 2017 IEEE International Conference on Computer Vision\n  (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method to jointly solve scene layout\nestimation and global registration problems for accurate indoor 3D\nreconstruction. Given a sequence of range data, we first build a set of scene\nfragments using KinectFusion and register them through pose graph optimization.\nAfterwards, we alternate between layout estimation and layout-based global\nregistration processes in iterative fashion to complement each other. We\nextract the scene layout through hierarchical agglomerative clustering and\nenergy-based multi-model fitting in consideration of noisy measurements. Having\nthe estimated scene layout in one hand, we register all the range data through\nthe global iterative closest point algorithm where the positions of 3D points\nthat belong to the layout such as walls and a ceiling are constrained to be\nclose to the layout. We experimentally verify the proposed method with the\npublicly available synthetic and real-world datasets in both quantitative and\nqualitative ways.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 11:11:20 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 11:48:16 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Lee", "Jeong-Kyun", ""], ["Yea", "Jae-Won", ""], ["Park", "Min-Gyu", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1704.07699", "submitter": "Lucia Ballerini", "authors": "Lucia Ballerini, Ruggiero Lovreglio, Maria del C. Valdes-Hernandez,\n  Joel Ramirez, Bradley J. MacIntosh, Sandra E. Black and Joanna M. Wardlaw", "title": "Perivascular Spaces Segmentation in Brain MRI Using Optimal 3D Filtering", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-018-19781-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perivascular Spaces (PVS) are a recently recognised feature of Small Vessel\nDisease (SVD), also indicating neuroinflammation, and are an important part of\nthe brain's circulation and glymphatic drainage system. Quantitative analysis\nof PVS on Magnetic Resonance Images (MRI) is important for understanding their\nrelationship with neurological diseases. In this work, we propose a\nsegmentation technique based on the 3D Frangi filtering for extraction of PVS\nfrom MRI. Based on prior knowledge from neuroradiological ratings of PVS, we\nused ordered logit models to optimise Frangi filter parameters in response to\nthe variability in the scanner's parameters and study protocols. We optimized\nand validated our proposed models on two independent cohorts, a dementia sample\n(N=20) and patients who previously had mild to moderate stroke (N=48). Results\ndemonstrate the robustness and generalisability of our segmentation method.\nSegmentation-based PVS burden estimates correlated with neuroradiological\nassessments (Spearman's $\\rho$ = 0.74, p $<$ 0.001), suggesting the great\npotential of our proposed method\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:02:06 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ballerini", "Lucia", ""], ["Lovreglio", "Ruggiero", ""], ["Valdes-Hernandez", "Maria del C.", ""], ["Ramirez", "Joel", ""], ["MacIntosh", "Bradley J.", ""], ["Black", "Sandra E.", ""], ["Wardlaw", "Joanna M.", ""]]}, {"id": "1704.07709", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha", "title": "Inception Recurrent Convolutional Neural Network for Object Recognition", "comments": "11 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (DCNNs) are an influential tool for\nsolving various problems in the machine learning and computer vision fields. In\nthis paper, we introduce a new deep learning model called an Inception-\nRecurrent Convolutional Neural Network (IRCNN), which utilizes the power of an\ninception network combined with recurrent layers in DCNN architecture. We have\nempirically evaluated the recognition performance of the proposed IRCNN model\nusing different benchmark datasets such as MNIST, CIFAR-10, CIFAR- 100, and\nSVHN. Experimental results show similar or higher recognition accuracy when\ncompared to most of the popular DCNNs including the RCNN. Furthermore, we have\ninvestigated IRCNN performance against equivalent Inception Networks and\nInception-Residual Networks using the CIFAR-100 dataset. We report about 3.5%,\n3.47% and 2.54% improvement in classification accuracy when compared to the\nRCNN, equivalent Inception Networks, and Inception- Residual Networks on the\naugmented CIFAR- 100 dataset respectively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:19:26 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Hasan", "Mahmudul", ""], ["Yakopcic", "Chris", ""], ["Taha", "Tarek M.", ""]]}, {"id": "1704.07711", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and Yao Wang", "title": "An ADMM Approach to Masked Signal Decomposition Using Subspace\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal decomposition is a classical problem in signal processing, which aims\nto separate an observed signal into two or more components each with its own\nproperty. Usually each component is described by its own subspace or\ndictionary. Extensive research has been done for the case where the components\nare additive, but in real world applications, the components are often\nnon-additive. For example, an image may consist of a foreground object overlaid\non a background, where each pixel either belongs to the foreground or the\nbackground. In such a situation, to separate signal components, we need to find\na binary mask which shows the location of each component. Therefore it requires\nto solve a binary optimization problem. Since most of the binary optimization\nproblems are intractable, we relax this problem to the approximated continuous\nproblem, and solve it by alternating optimization technique. We show the\napplication of the proposed algorithm for three applications: separation of\ntext from background in images, separation of moving objects from a background\nundergoing global camera motion in videos, separation of sinusoidal and spike\ncomponents in one dimensional signals. We demonstrate in each case that\nconsidering the non-additive nature of the problem can lead to significant\nimprovement.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:22:44 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 21:11:43 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1704.07724", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi and Xiaowen Chu", "title": "Speeding up Convolutional Neural Networks By Exploiting the Sparsity of\n  Rectifier Units", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectifier neuron units (ReLUs) have been widely used in deep convolutional\nnetworks. An ReLU converts negative values to zeros, and does not change\npositive values, which leads to a high sparsity of neurons. In this work, we\nfirst examine the sparsity of the outputs of ReLUs in some popular deep\nconvolutional architectures. And then we use the sparsity property of ReLUs to\naccelerate the calculation of convolution by skipping calculations of\nzero-valued neurons. The proposed sparse convolution algorithm achieves some\nspeedup improvements on CPUs compared to the traditional matrix-matrix\nmultiplication algorithm for convolution when the sparsity is not less than\n0.9.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:56:19 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 05:03:28 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1704.07754", "submitter": "Kuan-Lun Tseng", "authors": "Kuan-Lun Tseng, Yen-Liang Lin, Winston Hsu, Chung-Yang Huang", "title": "Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical\n  Segmentation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models such as convolutional neural net- work have been widely\nused in 3D biomedical segmentation and achieve state-of-the-art performance.\nHowever, most of them often adapt a single modality or stack multiple\nmodalities as different input channels. To better leverage the multi-\nmodalities, we propose a deep encoder-decoder structure with cross-modality\nconvolution layers to incorporate different modalities of MRI data. In\naddition, we exploit convolutional LSTM to model a sequence of 2D slices, and\njointly learn the multi-modalities and convolutional LSTM in an end-to-end\nmanner. To avoid converging to the certain labels, we adopt a re-weighting\nscheme and two-phase training to handle the label imbalance. Experimental\nresults on BRATS-2015 show that our method outperforms state-of-the-art\nbiomedical segmentation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:54:56 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Tseng", "Kuan-Lun", ""], ["Lin", "Yen-Liang", ""], ["Hsu", "Winston", ""], ["Huang", "Chung-Yang", ""]]}, {"id": "1704.07793", "submitter": "Jos\\'e Ignacio Orlando Eng", "authors": "Jos\\'e Ignacio Orlando and Hugo Luis Manterola and Enzo Ferrante and\n  Federico Ariel", "title": "Arabidopsis roots segmentation based on morphological operations and\n  CRFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabidopsis thaliana is a plant species widely utilized by scientists to\nestimate the impact of genetic differences in root morphological features. For\nthis purpose, images of this plant after genetic modifications are taken to\nstudy differences in the root architecture. This task requires manual\nsegmentations of radicular structures, although this is a particularly tedious\nand time-consuming labor. In this work, we present an unsupervised method for\nArabidopsis thaliana root segmentation based on morphological operations and\nfully-connected Conditional Random Fields. Although other approaches have been\nproposed to this purpose, all of them are based on more complex and expensive\nimaging modalities. Our results prove that our method can be easily applied\nover images taken using conventional scanners, with a minor user intervention.\nA first data set, our results and a fully open source implementation are\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:19:19 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Orlando", "Jos\u00e9 Ignacio", ""], ["Manterola", "Hugo Luis", ""], ["Ferrante", "Enzo", ""], ["Ariel", "Federico", ""]]}, {"id": "1704.07804", "submitter": "Sudheendra Vijayanarasimhan", "authors": "Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul\n  Sukthankar and Katerina Fragkiadaki", "title": "SfM-Net: Learning of Structure and Motion from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SfM-Net, a geometry-aware neural network for motion estimation in\nvideos that decomposes frame-to-frame pixel motion in terms of scene and object\ndepth, camera motion and 3D object rotations and translations. Given a sequence\nof frames, SfM-Net predicts depth, segmentation, camera and rigid object\nmotions, converts those into a dense frame-to-frame motion field (optical\nflow), differentiably warps frames in time to match pixels and back-propagates.\nThe model can be trained with various degrees of supervision: 1)\nself-supervised by the re-projection photometric error (completely\nunsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by\ndepth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth\nestimates and successfully estimates frame-to-frame camera rotations and\ntranslations. It often successfully segments the moving objects in the scene,\neven though such supervision is never provided.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:30:05 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Vijayanarasimhan", "Sudheendra", ""], ["Ricco", "Susanna", ""], ["Schmid", "Cordelia", ""], ["Sukthankar", "Rahul", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1704.07809", "submitter": "Tomas Simon", "authors": "Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh", "title": "Hand Keypoint Detection in Single Images using Multiview Bootstrapping", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach that uses a multi-camera system to train fine-grained\ndetectors for keypoints that are prone to occlusion, such as the joints of a\nhand. We call this procedure multiview bootstrapping: first, an initial\nkeypoint detector is used to produce noisy labels in multiple views of the\nhand. The noisy detections are then triangulated in 3D using multiview geometry\nor marked as outliers. Finally, the reprojected triangulations are used as new\nlabeled training data to improve the detector. We repeat this process,\ngenerating more labeled data in each iteration. We derive a result analytically\nrelating the minimum number of views to achieve target true and false positive\nrates for a given detector. The method is used to train a hand keypoint\ndetector for single images. The resulting keypoint detector runs in realtime on\nRGB images and has accuracy comparable to methods that use depth sensors. The\nsingle view detector, triangulated over multiple views, enables 3D markerless\nhand motion capture with complex object interactions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:37:48 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Simon", "Tomas", ""], ["Joo", "Hanbyul", ""], ["Matthews", "Iain", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1704.07813", "submitter": "Tinghui Zhou", "authors": "Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe", "title": "Unsupervised Learning of Depth and Ego-Motion from Video", "comments": "Accepted to CVPR 2017. Project webpage:\n  https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised learning framework for the task of monocular depth\nand camera motion estimation from unstructured video sequences. We achieve this\nby simultaneously training depth and camera pose estimation networks using the\ntask of view synthesis as the supervisory signal. The networks are thus coupled\nvia the view synthesis objective during training, but can be applied\nindependently at test time. Empirical evaluation on the KITTI dataset\ndemonstrates the effectiveness of our approach: 1) monocular depth performing\ncomparably with supervised methods that use either ground-truth pose or depth\nfor training, and 2) pose estimation performing favorably with established SLAM\nsystems under comparable input settings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:44:33 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 02:23:45 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhou", "Tinghui", ""], ["Brown", "Matthew", ""], ["Snavely", "Noah", ""], ["Lowe", "David G.", ""]]}, {"id": "1704.07816", "submitter": "Long Jin", "authors": "Long Jin, Justin Lazarow, Zhuowen Tu", "title": "Introspective Classification with Convolutional Nets", "comments": "12 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose introspective convolutional networks (ICN) that emphasize the\nimportance of having convolutional neural networks empowered with generative\ncapabilities. We employ a reclassification-by-synthesis algorithm to perform\ntraining using a formulation stemmed from the Bayes theory. Our ICN tries to\niteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by\nimproving the classification. The single CNN classifier learned is at the same\ntime generative --- being able to directly synthesize new samples within its\nown discriminative model. We conduct experiments on benchmark datasets\nincluding MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures,\nand observe improved classification results.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:49:03 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 05:09:48 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Jin", "Long", ""], ["Lazarow", "Justin", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1704.07820", "submitter": "Long Jin", "authors": "Justin Lazarow, Long Jin, Zhuowen Tu", "title": "Introspective Generative Modeling: Decide Discriminatively", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised learning by developing introspective generative\nmodeling (IGM) that attains a generator using progressively learned deep\nconvolutional neural networks. The generator is itself a discriminator, capable\nof introspection: being able to self-evaluate the difference between its\ngenerated samples and the given training data. When followed by repeated\ndiscriminative learning, desirable properties of modern discriminative\nclassifiers are directly inherited by the generator. IGM learns a cascade of\nCNN classifiers using a synthesis-by-classification algorithm. In the\nexperiments, we observe encouraging results on a number of applications\nincluding texture modeling, artistic style transferring, face modeling, and\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:57:33 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lazarow", "Justin", ""], ["Jin", "Long", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1704.07863", "submitter": "Andr\\'es Felipe Romero Vergara", "authors": "Andres Romero, Juan Leon and Pablo Arbelaez", "title": "Multi-View Dynamic Facial Action Unit Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel convolutional neural network approach to address the\nfine-grained recognition problem of multi-view dynamic facial action unit\ndetection. We leverage recent gains in large-scale object recognition by\nformulating the task of predicting the presence or absence of a specific action\nunit in a still image of a human face as holistic classification. We then\nexplore the design space of our approach by considering both shared and\nindependent representations for separate action units, and also different CNN\narchitectures for combining color and motion information. We then move to the\nnovel setup of the FERA 2017 Challenge, in which we propose a multi-view\nextension of our approach that operates by first predicting the viewpoint from\nwhich the video was taken, and then evaluating an ensemble of action unit\ndetectors that were trained for that specific viewpoint. Our approach is\nholistic, efficient, and modular, since new action units can be easily included\nin the overall system. Our approach significantly outperforms the baseline of\nthe FERA 2017 Challenge, with an absolute improvement of 14% on the F1-metric.\nAdditionally, it compares favorably against the winner of the FERA 2017\nchallenge. Code source is available at https://github.com/BCV-Uniandes/AUNets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 18:59:33 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 13:05:29 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Romero", "Andres", ""], ["Leon", "Juan", ""], ["Arbelaez", "Pablo", ""]]}, {"id": "1704.07910", "submitter": "Johan Ekekrantz", "authors": "Johan Ekekrantz, John Folkesson and Patric Jensfelt", "title": "Adaptive Cost Function for Pointcloud Registration", "comments": "10 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce an adaptive cost function for pointcloud\nregistration. The algorithm automatically estimates the sensor noise, which is\nimportant for generalization across different sensors and environments. Through\nexperiments on real and synthetic data, we show significant improvements in\naccuracy and robustness over state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 21:25:13 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Ekekrantz", "Johan", ""], ["Folkesson", "John", ""], ["Jensfelt", "Patric", ""]]}, {"id": "1704.07911", "submitter": "Urs Muller", "authors": "Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof\n  Choromanski, Bernhard Firner, Lawrence Jackel, Urs Muller", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning\n  Steers a Car", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of a complete software stack for autonomous driving, NVIDIA has\ncreated a neural-network-based system, known as PilotNet, which outputs\nsteering angles given images of the road ahead. PilotNet is trained using road\nimages paired with the steering angles generated by a human driving a\ndata-collection car. It derives the necessary domain knowledge by observing\nhuman drivers. This eliminates the need for human engineers to anticipate what\nis important in an image and foresee all the necessary rules for safe driving.\nRoad tests demonstrated that PilotNet can successfully perform lane keeping in\na wide variety of driving conditions, regardless of whether lane markings are\npresent or not.\n  The goal of the work described here is to explain what PilotNet learns and\nhow it makes its decisions. To this end we developed a method for determining\nwhich elements in the road image most influence PilotNet's steering decision.\nResults show that PilotNet indeed learns to recognize relevant objects on the\nroad.\n  In addition to learning the obvious features such as lane markings, edges of\nroads, and other cars, PilotNet learns more subtle features that would be hard\nto anticipate and program by engineers, for example, bushes lining the edge of\nthe road and atypical vehicle classes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 21:25:41 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Yeres", "Philip", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""], ["Firner", "Bernhard", ""], ["Jackel", "Lawrence", ""], ["Muller", "Urs", ""]]}, {"id": "1704.07945", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada", "title": "Spatio-temporal Person Retrieval via Natural Language Queries", "comments": "Accepted to ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of spatio-temporal person retrieval\nfrom multiple videos using a natural language query, in which we output a tube\n(i.e., a sequence of bounding boxes) which encloses the person described by the\nquery. For this problem, we introduce a novel dataset consisting of videos\ncontaining people annotated with bounding boxes for each second and with five\nnatural language descriptions. To retrieve the tube of the person described by\na given natural language query, we design a model that combines methods for\nspatio-temporal human detection and multimodal retrieval. We conduct\ncomprehensive experiments to compare a variety of tube and text representations\nand multimodal retrieval methods, and present a strong baseline in this task as\nwell as demonstrate the efficacy of our tube representation and multimodal\nfeature embedding technique. Finally, we demonstrate the versatility of our\nmodel by applying it to two other important tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 02:26:01 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 21:16:27 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Yamaguchi", "Masataka", ""], ["Saito", "Kuniaki", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1704.07961", "submitter": "James Murphy", "authors": "James M. Murphy, Mauro Maggioni", "title": "Unsupervised Clustering and Active Learning of Hyperspectral Images with\n  Nonlinear Diffusion", "comments": "17 pages, 22 figures, 3 tables. IEEE accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of unsupervised learning and segmentation of hyperspectral images\nis a significant challenge in remote sensing. The high dimensionality of\nhyperspectral data, presence of substantial noise, and overlap of classes all\ncontribute to the difficulty of automatically clustering and segmenting\nhyperspectral images. We propose an unsupervised learning technique called\nspectral-spatial diffusion learning (DLSS) that combines a geometric estimation\nof class modes with a diffusion-inspired labeling that incorporates both\nspectral and spatial information. The mode estimation incorporates the geometry\nof the hyperspectral data by using diffusion distance to promote learning a\nunique mode from each class. These class modes are then used to label all\npoints by a joint spectral-spatial nonlinear diffusion process. A related\nvariation of DLSS is also discussed, which enables active learning by\nrequesting labels for a very small number of well-chosen pixels, dramatically\nboosting overall clustering results. Extensive experimental analysis\ndemonstrates the efficacy of the proposed methods against benchmark and\nstate-of-the-art hyperspectral analysis techniques on a variety of real\ndatasets, their robustness to choices of parameters, and their low\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 03:11:21 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 04:05:36 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 06:00:02 GMT"}, {"version": "v4", "created": "Sun, 9 Sep 2018 19:49:05 GMT"}, {"version": "v5", "created": "Mon, 15 Oct 2018 21:36:25 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Murphy", "James M.", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1704.07969", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Teng Zhang, Amit Singer", "title": "Anisotropic twicing for single particle reconstruction using\n  autocorrelation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.BM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The missing phase problem in X-ray crystallography is commonly solved using\nthe technique of molecular replacement, which borrows phases from a previously\nsolved homologous structure, and appends them to the measured Fourier\nmagnitudes of the diffraction patterns of the unknown structure. More recently,\nmolecular replacement has been proposed for solving the missing orthogonal\nmatrices problem arising in Kam's autocorrelation analysis for single particle\nreconstruction using X-ray free electron lasers and cryo-EM. In classical\nmolecular replacement, it is common to estimate the magnitudes of the unknown\nstructure as twice the measured magnitudes minus the magnitudes of the\nhomologous structure, a procedure known as `twicing'. Mathematically, this is\nequivalent to finding an unbiased estimator for a complex-valued scalar. We\ngeneralize this scheme for the case of estimating real or complex valued\nmatrices arising in single particle autocorrelation analysis. We name this\napproach \"Anisotropic Twicing\" because unlike the scalar case, the unbiased\nestimator is not obtained by a simple magnitude isotropic correction. We\ncompare the performance of the least squares, twicing and anisotropic twicing\nestimators on synthetic and experimental datasets. We demonstrate 3D homology\nmodeling in cryo-EM directly from experimental data without iterative\nrefinement or class averaging, for the first time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 04:47:01 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhang", "Teng", ""], ["Singer", "Amit", ""]]}, {"id": "1704.08028", "submitter": "Adriana Fernandez Lopez", "authors": "Adriana Fernandez-Lopez, Oriol Martinez and Federico M. Sukno", "title": "Towards Estimating the Upper Bound of Visual-Speech Recognition: The\n  Visual Lip-Reading Feasibility Database", "comments": "IEEE International Conference on Automatic Face and Gesture\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is the most used communication method between humans and it involves\nthe perception of auditory and visual channels. Automatic speech recognition\nfocuses on interpreting the audio signals, although the video can provide\ninformation that is complementary to the audio. Exploiting the visual\ninformation, however, has proven challenging. On one hand, researchers have\nreported that the mapping between phonemes and visemes (visual units) is\none-to-many because there are phonemes which are visually similar and\nindistinguishable between them. On the other hand, it is known that some people\nare very good lip-readers (e.g: deaf people). We study the limit of visual only\nspeech recognition in controlled conditions. With this goal, we designed a new\ndatabase in which the speakers are aware of being read and aim to facilitate\nlip-reading. In the literature, there are discrepancies on whether\nhearing-impaired people are better lip-readers than normal-hearing people.\nThen, we analyze if there are differences between the lip-reading abilities of\n9 hearing-impaired and 15 normal-hearing people. Finally, human abilities are\ncompared with the performance of a visual automatic speech recognition system.\nIn our tests, hearing-impaired participants outperformed the normal-hearing\nparticipants but without reaching statistical significance. Human observers\nwere able to decode 44% of the spoken message. In contrast, the visual only\nautomatic system achieved 20% of word recognition rate. However, if we repeat\nthe comparison in terms of phonemes both obtained very similar recognition\nrates, just above 50%. This suggests that the gap between human lip-reading and\nautomatic speech-reading might be more related to the use of context than to\nthe ability to interpret mouth appearance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 09:19:26 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Fernandez-Lopez", "Adriana", ""], ["Martinez", "Oriol", ""], ["Sukno", "Federico M.", ""]]}, {"id": "1704.08030", "submitter": "Qier Meng", "authors": "Qier Meng, Takayuki Kitasaka, Masahiro Oda, Junji Ueno, Kensaku Mori", "title": "Airway segmentation from 3D chest CT volumes based on volume of interest\n  using gradient vector flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some lung diseases are related to bronchial airway structures and morphology.\nAlthough airway segmentation from chest CT volumes is an important task in the\ncomputer-aided diagnosis and surgery assistance systems for the chest, complete\n3-D airway structure segmentation is a quite challenging task due to its\ncomplex tree-like structure. In this paper, we propose a new airway\nsegmentation method from 3D chest CT volumes based on volume of interests (VOI)\nusing gradient vector flow (GVF). This method segments the bronchial regions by\napplying the cavity enhancement filter (CEF) to trace the bronchial tree\nstructure from the trachea. It uses the CEF in the VOI to segment each branch.\nAnd a tube-likeness function based on GVF and the GVF magnitude map in each VOI\nare utilized to assist predicting the positions and directions of child\nbranches. By calculating the tube-likeness function based on GVF and the GVF\nmagnitude map, the airway-like candidate structures are identified and their\ncentrelines are extracted. Based on the extracted centrelines, we can detect\nthe branch points of the bifurcations and directions of the airway branches in\nthe next level. At the same time, a leakage detection is performed to avoid the\nleakage by analysing the pixel information and the shape information of airway\ncandidate regions extracted in the VOI. Finally, we unify all of the extracted\nbronchial regions to form an integrated airway tree. Preliminary experiments\nusing four cases of chest CT volumes demonstrated that the proposed method can\nextract more bronchial branches in comparison with other methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 09:27:18 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Meng", "Qier", ""], ["Kitasaka", "Takayuki", ""], ["Oda", "Masahiro", ""], ["Ueno", "Junji", ""], ["Mori", "Kensaku", ""]]}, {"id": "1704.08035", "submitter": "Adriana Fernandez Lopez", "authors": "Adriana Fernandez-Lopez and Federico M. Sukno", "title": "Automatic Viseme Vocabulary Construction to Enhance Continuous\n  Lip-reading", "comments": "International Conference on Computer Vision Theory and Applications\n  (VISAPP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is the most common communication method between humans and involves\nthe perception of both auditory and visual channels. Automatic speech\nrecognition focuses on interpreting the audio signals, but it has been\ndemonstrated that video can provide information that is complementary to the\naudio. Thus, the study of automatic lip-reading is important and is still an\nopen problem. One of the key challenges is the definition of the visual\nelementary units (the visemes) and their vocabulary. Many researchers have\nanalyzed the importance of the phoneme to viseme mapping and have proposed\nviseme vocabularies with lengths between 11 and 15 visemes. These viseme\nvocabularies have usually been manually defined by their linguistic properties\nand in some cases using decision trees or clustering techniques. In this work,\nwe focus on the automatic construction of an optimal viseme vocabulary based on\nthe association of phonemes with similar appearance. To this end, we construct\nan automatic system that uses local appearance descriptors to extract the main\ncharacteristics of the mouth region and HMMs to model the statistic relations\nof both viseme and phoneme sequences. To compare the performance of the system\ndifferent descriptors (PCA, DCT and SIFT) are analyzed. We test our system in a\nSpanish corpus of continuous speech. Our results indicate that we are able to\nrecognize approximately 58% of the visemes, 47% of the phonemes and 23% of the\nwords in a continuous speech scenario and that the optimal viseme vocabulary\nfor Spanish is composed by 20 visemes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 09:34:59 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Fernandez-Lopez", "Adriana", ""], ["Sukno", "Federico M.", ""]]}, {"id": "1704.08045", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen and Matthias Hein", "title": "The loss surface of deep and wide neural networks", "comments": "ICML 2017. Main results now hold for larger classes of loss functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the optimization problem behind deep neural networks is highly\nnon-convex, it is frequently observed in practice that training deep networks\nseems possible without getting stuck in suboptimal points. It has been argued\nthat this is the case as all local minima are close to being globally optimal.\nWe show that this is (almost) true, in fact almost all local minima are\nglobally optimal, for a fully connected network with squared loss and analytic\nactivation function given that the number of hidden units of one layer of the\nnetwork is larger than the number of training points and the network structure\nfrom this layer on is pyramidal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:24:54 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 19:43:39 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1704.08063", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song", "title": "SphereFace: Deep Hypersphere Embedding for Face Recognition", "comments": "CVPR 2017 (v4: updated the Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses deep face recognition (FR) problem under open-set\nprotocol, where ideal face features are expected to have smaller maximal\nintra-class distance than minimal inter-class distance under a suitably chosen\nmetric space. However, few existing algorithms can effectively achieve this\ncriterion. To this end, we propose the angular softmax (A-Softmax) loss that\nenables convolutional neural networks (CNNs) to learn angularly discriminative\nfeatures. Geometrically, A-Softmax loss can be viewed as imposing\ndiscriminative constraints on a hypersphere manifold, which intrinsically\nmatches the prior that faces also lie on a manifold. Moreover, the size of\nangular margin can be quantitatively adjusted by a parameter $m$. We further\nderive specific $m$ to approximate the ideal feature criterion. Extensive\nanalysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF)\nand MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. The\ncode has also been made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 11:37:22 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 21:55:51 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 00:45:26 GMT"}, {"version": "v4", "created": "Mon, 29 Jan 2018 23:24:56 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Liu", "Weiyang", ""], ["Wen", "Yandong", ""], ["Yu", "Zhiding", ""], ["Li", "Ming", ""], ["Raj", "Bhiksha", ""], ["Song", "Le", ""]]}, {"id": "1704.08082", "submitter": "Fabio Maria Carlucci", "authors": "Fabio Maria Carlucci and Lorenzo Porzi and Barbara Caputo and Elisa\n  Ricci and Samuel Rota Bul\\`o", "title": "AutoDIAL: Automatic DomaIn Alignment Layers", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.06332\n  added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers trained on given databases perform poorly when tested on data\nacquired in different settings. This is explained in domain adaptation through\na shift among distributions of the source and target domains. Attempts to align\nthem have traditionally resulted in works reducing the domain shift by\nintroducing appropriate loss terms, measuring the discrepancies between source\nand target distributions, in the objective function. Here we take a different\nroute, proposing to align the learned representations by embedding in any given\nnetwork specific Domain Alignment Layers, designed to match the source and\ntarget feature distributions to a reference one. Opposite to previous works\nwhich define a priori in which layers adaptation should be performed, our\nmethod is able to automatically learn the degree of feature alignment required\nat different levels of the deep network. Thorough experiments on different\npublic benchmarks, in the unsupervised setting, confirm the power of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 12:50:33 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 09:35:22 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 19:10:40 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Carlucci", "Fabio Maria", ""], ["Porzi", "Lorenzo", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""], ["Bul\u00f2", "Samuel Rota", ""]]}, {"id": "1704.08090", "submitter": "Badre Munir", "authors": "Badre Munir", "title": "A Faster Patch Ordering Method for Image Denoising", "comments": "4 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the patch-based image denoising processing methods, smooth ordering of\nlocal patches (patch ordering) has been shown to give state-of-art results. For\nimage denoising the patch ordering method forms two large TSPs (Traveling\nSalesman Problem) comprised of nodes in N-dimensional space. Ten approximate\nsolutions of the two large TSPs are then used in a filtering process to form\nthe reconstructed image. Use of large TSPs makes patch ordering a\ncomputationally intensive method. A modified patch ordering method for image\ndenoising is proposed. In the proposed method, several smaller-sized TSPs are\nformed and the filtering process varied to work with solutions of these smaller\nTSPs. In terms of PSNR, denoising results of the proposed method differed by\n0.032 dB to 0.016 dB on average. In original method, solving TSPs was observed\nto consume 85% of execution time. In proposed method, the time for solving TSPs\ncan be reduced to half of the time required in original method. The proposed\nmethod can denoise images in 40% less time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:06:56 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Munir", "Badre", ""]]}, {"id": "1704.08121", "submitter": "Jie Luo", "authors": "Jie Luo, Karteek Popuri, Dana Cobzas, Hongyi Ding, William M. Wells\n  III and Masashi Sugiyama", "title": "Misdirected Registration Uncertainty", "comments": "raw version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being a task of establishing spatial correspondences, medical image\nregistration is often formalized as finding the optimal transformation that\nbest aligns two images. Since the transformation is such an essential component\nof registration, most existing researches conventionally quantify the\nregistration uncertainty, which is the confidence in the estimated spatial\ncorrespondences, by the transformation uncertainty. In this paper, we give\nconcrete examples and reveal that using the transformation uncertainty to\nquantify the registration uncertainty is inappropriate and sometimes\nmisleading. Based on this finding, we also raise attention to an important yet\nsubtle aspect of probabilistic image registration, that is whether it is\nreasonable to determine the correspondence of a registered voxel solely by the\nmode of its transformation distribution.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:52:57 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 21:53:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Luo", "Jie", ""], ["Popuri", "Karteek", ""], ["Cobzas", "Dana", ""], ["Ding", "Hongyi", ""], ["Wells", "William M.", "III"], ["Sugiyama", "Masashi", ""]]}, {"id": "1704.08134", "submitter": "Mohammadreza Soltaninejad", "authors": "Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou, Nigel Allinson,\n  Xujiong Ye", "title": "Multimodal MRI brain tumor segmentation using random forests with\n  features learned from fully convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning based method for automated\nsegmenta-tion of brain tumor in multimodal MRI images. The machine learned\nfeatures from fully convolutional neural network (FCN) and hand-designed texton\nfea-tures are used to classify the MRI image voxels. The score map with\npixel-wise predictions is used as a feature map which is learned from\nmultimodal MRI train-ing dataset using the FCN. The learned features are then\napplied to random for-ests to classify each MRI image voxel into normal brain\ntissues and different parts of tumor. The method was evaluated on BRATS 2013\nchallenge dataset. The results show that the application of the random forest\nclassifier to multimodal MRI images using machine-learned features based on FCN\nand hand-designed features based on textons provides promising segmentations.\nThe Dice overlap measure for automatic brain tumor segmentation against ground\ntruth is 0.88, 080 and 0.73 for complete tumor, core and enhancing tumor,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 14:22:02 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Soltaninejad", "Mohammadreza", ""], ["Zhang", "Lei", ""], ["Lambrou", "Tryphon", ""], ["Allinson", "Nigel", ""], ["Ye", "Xujiong", ""]]}, {"id": "1704.08141", "submitter": "Yihang Lou", "authors": "Ling-Yu Duan, Vijay Chandrasekhar, Shiqi Wang, Yihang Lou, Jie Lin,\n  Yan Bai, Tiejun Huang, Alex Chichung Kot, Wen Gao", "title": "Compact Descriptors for Video Analysis: the Emerging MPEG Standard", "comments": "4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of the on-going compact descriptors for video\nanalysis standard (CDVA) from the ISO/IEC moving pictures experts group (MPEG).\nMPEG-CDVA targets at defining a standardized bitstream syntax to enable\ninteroperability in the context of video analysis applications. During the\ndevelopments of MPEGCDVA, a series of techniques aiming to reduce the\ndescriptor size and improve the video representation ability have been\nproposed. This article describes the new standard that is being developed and\nreports the performance of these key technical contributions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 14:33:24 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Duan", "Ling-Yu", ""], ["Chandrasekhar", "Vijay", ""], ["Wang", "Shiqi", ""], ["Lou", "Yihang", ""], ["Lin", "Jie", ""], ["Bai", "Yan", ""], ["Huang", "Tiejun", ""], ["Kot", "Alex Chichung", ""], ["Gao", "Wen", ""]]}, {"id": "1704.08165", "submitter": "Yotam Hechtlinger", "authors": "Yotam Hechtlinger, Purvasha Chakravarti and Jining Qin", "title": "A Generalization of Convolutional Neural Networks to Graph-Structured\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generalization of Convolutional Neural Networks\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\nthe relations within the input, analogous to the way the standard convolution\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\nintuitive interpretation, is efficient and scalable and can also be used on\ndata with varying graph structure. Furthermore, this generalization can be\napplied to many standard regression or classification problems, by learning the\nthe underlying graph. We empirically demonstrate the performance of the\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\nactivity data set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 15:37:50 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Hechtlinger", "Yotam", ""], ["Chakravarti", "Purvasha", ""], ["Qin", "Jining", ""]]}, {"id": "1704.08218", "submitter": "Ke Wei", "authors": "Ke Wei, Ke Yin, Xue-Cheng Tai, Tony F. Chan", "title": "New region force for variational models in image segmentation and high\n  dimensional data clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective framework for multi-phase image segmentation and\nsemi-supervised data clustering by introducing a novel region force term into\nthe Potts model. Assume the probability that a pixel or a data point belongs to\neach class is known a priori. We show that the corresponding indicator function\nobeys the Bernoulli distribution and the new region force function can be\ncomputed as the negative log-likelihood function under the Bernoulli\ndistribution. We solve the Potts model by the primal-dual hybrid gradient\nmethod and the augmented Lagrangian method, which are based on two different\ndual problems of the same primal problem. Empirical evaluations of the Potts\nmodel with the new region force function on benchmark problems show that it is\ncompetitive with existing variational methods in both image segmentation and\nsemi-supervised data clustering.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:11:00 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Wei", "Ke", ""], ["Yin", "Ke", ""], ["Tai", "Xue-Cheng", ""], ["Chan", "Tony F.", ""]]}, {"id": "1704.08224", "submitter": "Arjun Chandrasekaran", "authors": "Arjun Chandrasekaran and Devi Parikh and Mohit Bansal", "title": "Punny Captions: Witty Wordplay in Image Descriptions", "comments": "NAACL 2018 (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wit is a form of rich interaction that is often grounded in a specific\nsituation (e.g., a comment in response to an event). In this work, we attempt\nto build computational models that can produce witty descriptions for a given\nimage. Inspired by a cognitive account of humor appreciation, we employ\nlinguistic wordplay, specifically puns, in image descriptions. We develop two\napproaches which involve retrieving witty descriptions for a given image from a\nlarge corpus of sentences, or generating them via an encoder-decoder neural\nnetwork architecture. We compare our approach against meaningful baseline\napproaches via human studies and show substantial improvements. We find that\nwhen a human is subject to similar constraints as the model regarding word\nusage and style, people vote the image descriptions generated by our model to\nbe slightly wittier than human-written witty descriptions. Unsurprisingly,\nhumans are almost always wittier than the model when they are free to choose\nthe vocabulary, style, etc.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:22:53 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 17:45:50 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Parikh", "Devi", ""], ["Bansal", "Mohit", ""]]}, {"id": "1704.08243", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, Devi Parikh", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has received a lot of attention over the past\ncouple of years. A number of deep learning models have been proposed for this\ntask. However, it has been shown that these models are heavily driven by\nsuperficial correlations in the training data and lack compositionality -- the\nability to answer questions about unseen compositions of seen concepts. This\ncompositionality is desirable and central to intelligence. In this paper, we\npropose a new setting for Visual Question Answering where the test\nquestion-answer pairs are compositionally novel compared to training\nquestion-answer pairs. To facilitate developing models under this setting, we\npresent a new compositional split of the VQA v1.0 dataset, which we call\nCompositional VQA (C-VQA). We analyze the distribution of questions and answers\nin the C-VQA splits. Finally, we evaluate several existing VQA models under\nthis new setting and show that the performances of these models degrade by a\nsignificant amount compared to the original VQA setting.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:57:59 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Kembhavi", "Aniruddha", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1704.08292", "submitter": "Chenliang Xu", "authors": "Lele Chen, Sudhanshu Srivastava, Zhiyao Duan and Chenliang Xu", "title": "Deep Cross-Modal Audio-Visual Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal audio-visual perception has been a long-lasting topic in\npsychology and neurology, and various studies have discovered strong\ncorrelations in human perception of auditory and visual stimuli. Despite works\nin computational multimodal modeling, the problem of cross-modal audio-visual\ngeneration has not been systematically studied in the literature. In this\npaper, we make the first attempt to solve this cross-modal generation problem\nleveraging the power of deep generative adversarial training. Specifically, we\nuse conditional generative adversarial networks to achieve cross-modal\naudio-visual generation of musical performances. We explore different encoding\nmethods for audio and visual signals, and work on two scenarios:\ninstrument-oriented generation and pose-oriented generation. Being the first to\nexplore this new problem, we compose two new datasets with pairs of images and\nsounds of musical performances of different instruments. Our experiments using\nboth classification and human evaluations demonstrate that our model has the\nability to generate one modality, i.e., audio/visual, from the other modality,\ni.e., visual/audio, to a good extent. Our experiments on various design choices\nalong with the datasets will facilitate future research in this new problem\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 18:46:10 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Chen", "Lele", ""], ["Srivastava", "Sudhanshu", ""], ["Duan", "Zhiyao", ""], ["Xu", "Chenliang", ""]]}, {"id": "1704.08328", "submitter": "Atul Dhingra", "authors": "Atul Dhingra", "title": "Face Identification and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we study two problems based on clustering algorithms. In the\nfirst problem, we study the role of visual attributes using an agglomerative\nclustering algorithm to whittle down the search area where the number of\nclasses is high to improve the performance of clustering. We observe that as we\nadd more attributes, the clustering performance increases overall. In the\nsecond problem, we study the role of clustering in aggregating templates in a\n1:N open set protocol using multi-shot video as a probe. We observe that by\nincreasing the number of clusters, the performance increases with respect to\nthe baseline and reaches a peak, after which increasing the number of clusters\ncauses the performance to degrade. Experiments are conducted using recently\nintroduced unconstrained IARPA Janus IJB-A, CS2, and CS3 face recognition\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 19:50:28 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Dhingra", "Atul", ""]]}, {"id": "1704.08331", "submitter": "Narapureddy Dinesh Reddy", "authors": "Nazrul Haque, N Dinesh Reddy and K. Madhava Krishna", "title": "Joint Semantic and Motion Segmentation for dynamic scenes using Deep\n  Convolutional Networks", "comments": "In Proceedings of the 12th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications - Volume 5:\n  Visapp, (Visigrapp 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scene understanding is a challenging problem and motion segmentation\nplays a crucial role in solving it. Incorporating semantics and motion enhances\nthe overall perception of the dynamic scene. For applications of outdoor\nrobotic navigation, joint learning methods have not been extensively used for\nextracting spatio-temporal features or adding different priors into the\nformulation. The task becomes even more challenging without stereo information\nbeing incorporated. This paper proposes an approach to fuse semantic features\nand motion clues using CNNs, to address the problem of monocular semantic\nmotion segmentation. We deduce semantic and motion labels by integrating\noptical flow as a constraint with semantic features into dilated convolution\nnetwork. The pipeline consists of three main stages i.e Feature extraction,\nFeature amplification and Multi Scale Context Aggregation to fuse the semantics\nand flow features. Our joint formulation shows significant improvements in\nmonocular motion segmentation over the state of the art methods on challenging\nKITTI tracking dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 03:06:03 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Haque", "Nazrul", ""], ["Reddy", "N Dinesh", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1704.08345", "submitter": "Elyor Kodirov", "authors": "Elyor Kodirov, Tao Xiang, Shaogang Gong", "title": "Semantic Autoencoder for Zero-Shot Learning", "comments": "accepted to CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing zero-shot learning (ZSL) models typically learn a projection\nfunction from a feature space to a semantic embedding space (e.g.~attribute\nspace). However, such a projection function is only concerned with predicting\nthe training seen class semantic representation (e.g.~attribute prediction) or\nclassification. When applied to test data, which in the context of ZSL contains\ndifferent (unseen) classes without training data, a ZSL model typically suffers\nfrom the project domain shift problem. In this work, we present a novel\nsolution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the\nencoder-decoder paradigm, an encoder aims to project a visual feature vector\ninto the semantic space as in the existing ZSL models. However, the decoder\nexerts an additional constraint, that is, the projection/code must be able to\nreconstruct the original visual feature. We show that with this additional\nreconstruction constraint, the learned projection function from the seen\nclasses is able to generalise better to the new unseen classes. Importantly,\nthe encoder and decoder are linear and symmetric which enable us to develop an\nextremely efficient learning algorithm. Extensive experiments on six benchmark\ndatasets demonstrate that the proposed SAE outperforms significantly the\nexisting ZSL models with the additional benefit of lower computational cost.\nFurthermore, when the SAE is applied to supervised clustering problem, it also\nbeats the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 20:45:53 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Kodirov", "Elyor", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1704.08382", "submitter": "Christopher Tralie", "authors": "Christopher J. Tralie and Jose A. Perea", "title": "(Quasi)Periodicity Quantification in Video Data, Using Topology", "comments": "27 pages, 1 column, 23 figures, SIAM Journal on Imaging Sciences,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel framework for quantifying the presence and\nstrength of recurrent dynamics in video data. Specifically, we provide\ncontinuous measures of periodicity (perfect repetition) and quasiperiodicity\n(superposition of periodic modes with non-commensurate periods), in a way which\ndoes not require segmentation, training, object tracking or 1-dimensional\nsurrogate signals. Our methodology operates directly on video data. The\napproach combines ideas from nonlinear time series analysis (delay embeddings)\nand computational topology (persistent homology), by translating the problem of\nfinding recurrent dynamics in video data, into the problem of determining the\ncircularity or toroidality of an associated geometric space. Through extensive\ntesting, we show the robustness of our scores with respect to several noise\nmodels/levels, we show that our periodicity score is superior to other methods\nwhen compared to human-generated periodicity rankings, and furthermore, we show\nthat our quasiperiodicity score clearly indicates the presence of biphonation\nin videos of vibrating vocal folds, which has never before been accomplished\nend to end quantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 23:54:40 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 23:45:44 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Tralie", "Christopher J.", ""], ["Perea", "Jose A.", ""]]}, {"id": "1704.08458", "submitter": "Boyue Wang", "authors": "Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Haoran Chen and Baocai\n  Yin", "title": "Locality Preserving Projections for Grassmann manifold", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning on Grassmann manifold has become popular in many computer vision\ntasks, with the strong capability to extract discriminative information for\nimagesets and videos. However, such learning algorithms particularly on\nhigh-dimensional Grassmann manifold always involve with significantly high\ncomputational cost, which seriously limits the applicability of learning on\nGrassmann manifold in more wide areas. In this research, we propose an\nunsupervised dimensionality reduction algorithm on Grassmann manifold based on\nthe Locality Preserving Projections (LPP) criterion. LPP is a commonly used\ndimensionality reduction algorithm for vector-valued data, aiming to preserve\nlocal structure of data in the dimension-reduced space. The strategy is to\nconstruct a mapping from higher dimensional Grassmann manifold into the one in\na relative low-dimensional with more discriminative capability. The proposed\nmethod can be optimized as a basic eigenvalue problem. The performance of our\nproposed method is assessed on several classification and clustering tasks and\nthe experimental results show its clear advantages over other Grassmann based\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 07:24:35 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Chen", "Haoran", ""], ["Yin", "Baocai", ""]]}, {"id": "1704.08509", "submitter": "Bo-Cheng Tsai", "authors": "Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai, Yu-Chiang\n  Frank Wang, Min Sun", "title": "No More Discrimination: Cross City Adaptation of Road Scene Segmenters", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of deep-learning based semantic segmentation,\ndeploying a pre-trained road scene segmenter to a city whose images are not\npresented in the training set would not achieve satisfactory performance due to\ndataset biases. Instead of collecting a large number of annotated images of\neach city of interest to train or refine the segmenter, we propose an\nunsupervised learning approach to adapt road scene segmenters across different\ncities. By utilizing Google Street View and its time-machine feature, we can\ncollect unannotated images for each road scene at different times, so that the\nassociated static-object priors can be extracted accordingly. By advancing a\njoint global and class-specific domain adversarial learning framework,\nadaptation of pre-trained segmenters to that city can be achieved without the\nneed of any user annotation or interaction. We show that our method improves\nthe performance of semantic segmentation in multiple cities across continents,\nwhile it performs favorably against state-of-the-art approaches requiring\nannotated training data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 11:14:21 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Chen", "Yi-Hsin", ""], ["Chen", "Wei-Yu", ""], ["Chen", "Yu-Ting", ""], ["Tsai", "Bo-Cheng", ""], ["Wang", "Yu-Chiang Frank", ""], ["Sun", "Min", ""]]}, {"id": "1704.08545", "submitter": "Hengshuang Zhao", "authors": "Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia", "title": "ICNet for Real-Time Semantic Segmentation on High-Resolution Images", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the challenging task of real-time semantic segmentation in this\npaper. It finds many practical applications and yet is with fundamental\ndifficulty of reducing a large portion of computation for pixel-wise label\ninference. We propose an image cascade network (ICNet) that incorporates\nmulti-resolution branches under proper label guidance to address this\nchallenge. We provide in-depth analysis of our framework and introduce the\ncascade feature fusion unit to quickly achieve high-quality segmentation. Our\nsystem yields real-time inference on a single GPU card with decent quality\nresults evaluated on challenging datasets like Cityscapes, CamVid and\nCOCO-Stuff.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 13:02:49 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 03:34:25 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zhao", "Hengshuang", ""], ["Qi", "Xiaojuan", ""], ["Shen", "Xiaoyong", ""], ["Shi", "Jianping", ""], ["Jia", "Jiaya", ""]]}, {"id": "1704.08614", "submitter": "Michael Wilber", "authors": "Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John\n  Collomosse, Serge Belongie", "title": "BAM! The Behance Artistic Media Dataset for Recognition Beyond\n  Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems are designed to work well within the context of\neveryday photography. However, artists often render the world around them in\nways that do not resemble photographs. Artwork produced by people is not\nconstrained to mimic the physical world, making it more challenging for\nmachines to recognize.\n  This work is a step toward teaching machines how to categorize images in ways\nthat are valuable to humans. First, we collect a large-scale dataset of\ncontemporary artwork from Behance, a website containing millions of portfolios\nfrom professional and commercial artists. We annotate Behance imagery with rich\nattribute labels for content, emotions, and artistic media. Furthermore, we\ncarry out baseline experiments to show the value of this dataset for artistic\nstyle prediction, for improving the generality of existing object classifiers,\nand for the study of visual domain adaptation. We believe our Behance Artistic\nMedia dataset will be a good starting point for researchers wishing to study\nartistic imagery and relevant problems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:05:30 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 02:48:53 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wilber", "Michael J.", ""], ["Fang", "Chen", ""], ["Jin", "Hailin", ""], ["Hertzmann", "Aaron", ""], ["Collomosse", "John", ""], ["Belongie", "Serge", ""]]}, {"id": "1704.08615", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Thomas S. A. Wallis, Matthias Bethge", "title": "Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics", "comments": "published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dozens of new models on fixation prediction are published every year and\ncompared on open benchmarks such as MIT300 and LSUN. However, progress in the\nfield can be difficult to judge because models are compared using a variety of\ninconsistent metrics. Here we show that no single saliency map can perform well\nunder all metrics. Instead, we propose a principled approach to solve the\nbenchmarking problem by separating the notions of saliency models, maps and\nmetrics. Inspired by Bayesian decision theory, we define a saliency model to be\na probabilistic model of fixation density prediction and a saliency map to be a\nmetric-specific prediction derived from the model density which maximizes the\nexpected performance on that metric given the model density. We derive these\noptimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,\nNSS, CC, SIM, KL-Div) and show that they can be computed analytically or\napproximated with high precision. We show that this leads to consistent\nrankings in all metrics and avoids the penalties of using one saliency map for\nall metrics. Our method allows researchers to have their model compete on many\ndifferent metrics with state-of-the-art in those metrics: \"good\" models will\nperform well in all metrics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:07:42 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 13:31:14 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1704.08619", "submitter": "Panagiotis Tzirakis", "authors": "Panagiotis Tzirakis, George Trigeorgis, Mihalis A. Nicolaou, Bj\\\"orn\n  Schuller, and Stefanos Zafeiriou", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2017.2764438", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic affect recognition is a challenging task due to the various\nmodalities emotions can be expressed with. Applications can be found in many\ndomains including multimedia retrieval and human computer interaction. In\nrecent years, deep neural networks have been used with great success in\ndetermining emotional states. Inspired by this success, we propose an emotion\nrecognition system using auditory and visual modalities. To capture the\nemotional content for various styles of speaking, robust features need to be\nextracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to\nextract features from the speech, while for the visual modality a deep residual\nnetwork (ResNet) of 50 layers. In addition to the importance of feature\nextraction, a machine learning algorithm needs also to be insensitive to\noutliers while being able to model the context. To tackle this problem, Long\nShort-Term Memory (LSTM) networks are utilized. The system is then trained in\nan end-to-end fashion where - by also taking advantage of the correlations of\nthe each of the streams - we manage to significantly outperform the traditional\napproaches based on auditory and visual handcrafted features for the prediction\nof spontaneous and natural emotions on the RECOLA database of the AVEC 2016\nresearch challenge on emotion recognition.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:14:33 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Tzirakis", "Panagiotis", ""], ["Trigeorgis", "George", ""], ["Nicolaou", "Mihalis A.", ""], ["Schuller", "Bj\u00f6rn", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1704.08628", "submitter": "Bastien Moysset", "authors": "Bastien Moysset, Christopher Kermorvant, Christian Wolf", "title": "Full-Page Text Recognition: Learning Where to Start and When to Stop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text line detection and localization is a crucial step for full page document\nanalysis, but still suffers from heterogeneity of real life documents. In this\npaper, we present a new approach for full page text recognition. Localization\nof the text lines is based on regressions with Fully Convolutional Neural\nNetworks and Multidimensional Long Short-Term Memory as contextual layers. In\norder to increase the efficiency of this localization method, only the position\nof the left side of the text lines are predicted. The text recognizer is then\nin charge of predicting the end of the text to recognize. This method has shown\ngood results for full page text recognition on the highly heterogeneous Maurdor\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:50:37 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Moysset", "Bastien", ""], ["Kermorvant", "Christopher", ""], ["Wolf", "Christian", ""]]}, {"id": "1704.08631", "submitter": "Nicolas Honnorat", "authors": "Nicolas Honnorat, Christos Davatzikos", "title": "Sparse Hierachical Extrapolated Parametric Methods for Cortical Data\n  Analysis", "comments": "Technical report (ongoing work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neuroimaging studies focus on the cortex, in order to benefit from\nbetter signal to noise ratios and reduced computational burden. Cortical data\nare usually projected onto a reference mesh, where subsequent analyses are\ncarried out. Several multiscale approaches have been proposed for analyzing\nthese surface data, such as spherical harmonics and graph wavelets. As far as\nwe know, however, the hierarchical structure of the template icosahedral meshes\nused by most neuroimaging software has never been exploited for cortical data\nfactorization. In this paper, we demonstrate how the structure of the\nubiquitous icosahedral meshes can be exploited by data factorization methods\nsuch as sparse dictionary learning, and we assess the optimization speed-up\noffered by extrapolation methods in this context. By testing different\nsparsity-inducing norms, extrapolation methods, and factorization schemes, we\ncompare the performances of eleven methods for analyzing four datasets: two\nstructural and two functional MRI datasets obtained by processing the data\npublicly available for the hundred unrelated subjects of the Human Connectome\nProject. Our results demonstrate that, depending on the level of details\nrequested, a speedup of several orders of magnitudes can be obtained.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:52:23 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Honnorat", "Nicolas", ""], ["Davatzikos", "Christos", ""]]}, {"id": "1704.08686", "submitter": "Emanuele Rodol\\`a", "authors": "Or Litany, Tal Remez, Emanuele Rodol\\`a, Alex M. Bronstein, Michael M.\n  Bronstein", "title": "Deep Functional Maps: Structured Prediction for Dense Shape\n  Correspondence", "comments": "Accepted for publication at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for learning dense correspondence between\ndeformable 3D shapes. Existing learning based approaches model shape\ncorrespondence as a labelling problem, where each point of a query shape\nreceives a label identifying a point on some reference domain; the\ncorrespondence is then constructed a posteriori by composing the label\npredictions of two input shapes. We propose a paradigm shift and design a\nstructured prediction model in the space of functional maps, linear operators\nthat provide a compact representation of the correspondence. We model the\nlearning process via a deep residual network which takes dense descriptor\nfields defined on two shapes as input, and outputs a soft map between the two\ngiven objects. The resulting correspondence is shown to be accurate on several\nchallenging benchmarks comprising multiple categories, synthetic models, real\nscans with acquisition artifacts, topological noise, and partiality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:56:20 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 07:45:23 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Litany", "Or", ""], ["Remez", "Tal", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Alex M.", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1704.08723", "submitter": "Chenliang Xu", "authors": "Chenliang Xu, Caiming Xiong and Jason J. Corso", "title": "Action Understanding with Multiple Classes of Actors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid progress, existing works on action understanding focus\nstrictly on one type of action agent, which we call actor---a human adult,\nignoring the diversity of actions performed by other actors. To overcome this\nnarrow viewpoint, our paper marks the first effort in the computer vision\ncommunity to jointly consider algorithmic understanding of various types of\nactors undergoing various actions. To begin with, we collect a large annotated\nActor-Action Dataset (A2D) that consists of 3782 short videos and 31 temporally\nuntrimmed long videos. We formulate the general actor-action understanding\nproblem and instantiate it at various granularities: video-level single- and\nmultiple-label actor-action recognition, and pixel-level actor-action\nsegmentation. We propose and examine a comprehensive set of graphical models\nthat consider the various types of interplay among actors and actions. Our\nfindings have led us to conclusive evidence that the joint modeling of actor\nand action improves performance over modeling each of them independently, and\nfurther improvement can be obtained by considering the multi-scale natural in\nvideo understanding. Hence, our paper concludes the argument of the value of\nexplicit consideration of various actors in comprehensive action understanding\nand provides a dataset and a benchmark for later works exploring this new\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 19:20:50 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Xu", "Chenliang", ""], ["Xiong", "Caiming", ""], ["Corso", "Jason J.", ""]]}, {"id": "1704.08740", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh, Boqing Gong, Mubarak Shah", "title": "Improving Facial Attribute Prediction using Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes are semantically meaningful characteristics whose applicability\nwidely crosses category boundaries. They are particularly important in\ndescribing and recognizing concepts where no explicit training example is\ngiven, \\textit{e.g., zero-shot learning}. Additionally, since attributes are\nhuman describable, they can be used for efficient human-computer interaction.\nIn this paper, we propose to employ semantic segmentation to improve facial\nattribute prediction. The core idea lies in the fact that many facial\nattributes describe local properties. In other words, the probability of an\nattribute to appear in a face image is far from being uniform in the spatial\ndomain. We build our facial attribute prediction model jointly with a deep\nsemantic segmentation network. This harnesses the localization cues learned by\nthe semantic segmentation to guide the attention of the attribute prediction to\nthe regions where different attributes naturally show up. As a result of this\napproach, in addition to recognition, we are able to localize the attributes,\ndespite merely having access to image level labels (weak supervision) during\ntraining. We evaluate our proposed method on CelebA and LFWA datasets and\nachieve superior results to the prior arts. Furthermore, we show that in the\nreverse problem, semantic face parsing improves when facial attributes are\navailable. That reaffirms the need to jointly model these two interconnected\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 20:41:50 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Gong", "Boqing", ""], ["Shah", "Mubarak", ""]]}, {"id": "1704.08759", "submitter": "Shichao Yang", "authors": "Shichao Yang, Sandeep Konam, Chen Ma, Stephanie Rosenthal, Manuela\n  Veloso, Sebastian Scherer", "title": "Obstacle Avoidance through Deep Networks based Intermediate Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstacle avoidance from monocular images is a challenging problem for robots.\nThough multi-view structure-from-motion could build 3D maps, it is not robust\nin textureless environments. Some learning based methods exploit human\ndemonstration to predict a steering command directly from a single image.\nHowever, this method is usually biased towards certain tasks or demonstration\nscenarios and also biased by human understanding. In this paper, we propose a\nnew method to predict a trajectory from images. We train our system on more\ndiverse NYUv2 dataset. The ground truth trajectory is computed from the\ndesigned cost functions automatically. The Convolutional Neural Network\nperception is divided into two stages: first, predict depth map and surface\nnormal from RGB images, which are two important geometric properties related to\n3D obstacle representation. Second, predict the trajectory from the depth and\nnormal. Results show that our intermediate perception increases the accuracy by\n20% than the direct prediction. Our model generalizes well to other public\nindoor datasets and is also demonstrated for robot flights in simulation and\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 21:55:07 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Yang", "Shichao", ""], ["Konam", "Sandeep", ""], ["Ma", "Chen", ""], ["Rosenthal", "Stephanie", ""], ["Veloso", "Manuela", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1704.08763", "submitter": "Erroll Wood", "authors": "Erroll Wood, Tadas Baltrusaitis, Louis-Philippe Morency, Peter\n  Robinson, Andreas Bulling", "title": "GazeDirector: Fully Articulated Eye Gaze Redirection in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GazeDirector, a new approach for eye gaze redirection that uses\nmodel-fitting. Our method first tracks the eyes by fitting a multi-part eye\nregion model to video frames using analysis-by-synthesis, thereby recovering\neye region shape, texture, pose, and gaze simultaneously. It then redirects\ngaze by 1) warping the eyelids from the original image using a model-derived\nflow field, and 2) rendering and compositing synthesized 3D eyeballs onto the\noutput image in a photorealistic manner. GazeDirector allows us to change where\npeople are looking without person-specific training data, and with full\narticulation, i.e. we can precisely specify new gaze directions in 3D.\nQuantitatively, we evaluate both model-fitting and gaze synthesis, with\nexperiments for gaze estimation and redirection on the Columbia gaze dataset.\nQualitatively, we compare GazeDirector against recent work on gaze redirection,\nshowing better results especially for large redirection angles. Finally, we\ndemonstrate gaze redirection on YouTube videos by introducing new 3D gaze\ntargets and by manipulating visual behavior.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 22:23:53 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Wood", "Erroll", ""], ["Baltrusaitis", "Tadas", ""], ["Morency", "Louis-Philippe", ""], ["Robinson", "Peter", ""], ["Bulling", "Andreas", ""]]}, {"id": "1704.08772", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Stefanos Zafeiriou", "title": "Deep Face Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deblurring consists a long studied task, however the outcomes of\ngeneric methods are not effective in real world blurred images. Domain-specific\nmethods for deblurring targeted object categories, e.g. text or faces,\nfrequently outperform their generic counterparts, hence they are attracting an\nincreasing amount of attention. In this work, we develop such a domain-specific\nmethod to tackle deblurring of human faces, henceforth referred to as face\ndeblurring. Studying faces is of tremendous significance in computer vision,\nhowever face deblurring has yet to demonstrate some convincing results. This\ncan be partly attributed to the combination of i) poor texture and ii) highly\nstructure shape that yield the contour/gradient priors (that are typically\nused) sub-optimal. In our work instead of making assumptions over the prior, we\nadopt a learning approach by inserting weak supervision that exploits the\nwell-documented structure of the face. Namely, we utilise a deep network to\nperform the deblurring and employ a face alignment technique to pre-process\neach face. We additionally surpass the requirement of the deep network for\nthousands training samples, by introducing an efficient framework that allows\nthe generation of a large dataset. We utilised this framework to create 2MF2, a\ndataset of over two million frames. We conducted experiments with real world\nblurred facial images and report that our method returns a result close to the\nsharp natural latent image.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 23:01:45 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 07:45:36 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1704.08778", "submitter": "Ayan Chaudhury", "authors": "Ayan Chaudhury and John L. Barron", "title": "Partially Occluded Leaf Recognition via Subgraph Matching and Energy\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to match partially occluded plant leaves with\ndatabases of full plant leaves. Although contour based 2D shape matching has\nbeen studied extensively in the last couple of decades, matching occluded\nleaves with full leaf databases is an open and little worked on problem.\nClassifying occluded plant leaves is even more challenging than full leaf\nmatching because of large variations and complexity of leaf structures.\nMatching an occluded contour with all the full contours in a database is an\nNP-hard problem [Su et al. ICCV2015], so our algorithm is necessarily\nsuboptimal.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 00:15:49 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 20:04:12 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Chaudhury", "Ayan", ""], ["Barron", "John L.", ""]]}, {"id": "1704.08797", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Kunlin Cao, Qi Song, Ulas Bagci", "title": "Risk Stratification of Lung Nodules Using 3D CNN-Based Multi-task\n  Learning", "comments": "Accepted for publication at Information Processing in Medical Imaging\n  (IPMI) 2017", "journal-ref": "Information Processing in Medical Imaging. IPMI 2017. Lecture\n  Notes in Computer Science, vol 10265. Springer, Cham", "doi": "10.1007/978-3-319-59050-9_20", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk stratification of lung nodules is a task of primary importance in lung\ncancer diagnosis. Any improvement in robust and accurate nodule\ncharacterization can assist in identifying cancer stage, prognosis, and\nimproving treatment planning. In this study, we propose a 3D Convolutional\nNeural Network (CNN) based nodule characterization strategy. With a completely\n3D approach, we utilize the volumetric information from a CT scan which would\nbe otherwise lost in the conventional 2D CNN based approaches. In order to\naddress the need for a large amount for training data for CNN, we resort to\ntransfer learning to obtain highly discriminative features. Moreover, we also\nacquire the task dependent feature representation for six high-level nodule\nattributes and fuse this complementary information via a Multi-task learning\n(MTL) framework. Finally, we propose to incorporate potential disagreement\namong radiologists while scoring different nodule attributes in a graph\nregularized sparse multi-task learning. We evaluated our proposed approach on\none of the largest publicly available lung nodule datasets comprising 1018\nscans and obtained state-of-the-art results in regressing the malignancy\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 03:32:54 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Bagci", "Ulas", ""]]}, {"id": "1704.08812", "submitter": "Xiaoyong Shen", "authors": "Xiaoyong Shen, Ruixing Wang, Hengshuang Zhao, Jiaya Jia", "title": "Automatic Real-time Background Cut for Portrait Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We in this paper solve the problem of high-quality automatic real-time\nbackground cut for 720p portrait videos. We first handle the background\nambiguity issue in semantic segmentation by proposing a global background\nattenuation model. A spatial-temporal refinement network is developed to\nfurther refine the segmentation errors in each frame and ensure temporal\ncoherence in the segmentation map. We form an end-to-end network for training\nand testing. Each module is designed considering efficiency and accuracy. We\nbuild a portrait dataset, which includes 8,000 images with high-quality labeled\nmap for training and testing. To further improve the performance, we build a\nportrait video dataset with 50 sequences to fine-tune video segmentation. Our\nframework benefits many video processing applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 05:29:34 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Shen", "Xiaoyong", ""], ["Wang", "Ruixing", ""], ["Zhao", "Hengshuang", ""], ["Jia", "Jiaya", ""]]}, {"id": "1704.08821", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba, Shin Ishii", "title": "Active Collaborative Ensemble Tracking", "comments": "AVSS 2017 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discriminative ensemble tracker employs multiple classifiers, each of which\ncasts a vote on all of the obtained samples. The votes are then aggregated in\nan attempt to localize the target object. Such method relies on collective\ncompetence and the diversity of the ensemble to approach the target/non-target\nclassification task from different views. However, by updating all of the\nensemble using a shared set of samples and their final labels, such diversity\nis lost or reduced to the diversity provided by the underlying features or\ninternal classifiers' dynamics. Additionally, the classifiers do not exchange\ninformation with each other while striving to serve the collective goal, i.e.,\nbetter classification. In this study, we propose an active collaborative\ninformation exchange scheme for ensemble tracking. This, not only orchestrates\ndifferent classifier towards a common goal but also provides an intelligent\nupdate mechanism to keep the diversity of classifiers and to mitigate the\nshortcomings of one with the others. The data exchange is optimized with regard\nto an ensemble uncertainty utility function, and the ensemble is updated via\nco-training. The evaluations demonstrate promising results realized by the\nproposed algorithm for the real-world online tracking.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 06:46:27 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Mirzaei", "Maryam Sadat", ""], ["Oba", "Shigeyuki", ""], ["Ishii", "Shin", ""]]}, {"id": "1704.08822", "submitter": "Yaser Sadra", "authors": "Yaser Sadra", "title": "A new image compression by gradient Haar wavelet", "comments": "9 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of human communications the usage of Visual\nCommunications has also increased. The advancement of image compression methods\nis one of the main reasons for the enhancement. This paper first presents main\nmodes of image compression methods such as JPEG and JPEG2000 without\nmathematical details. Also, the paper describes gradient Haar wavelet\ntransforms in order to construct a preliminary image compression algorithm.\nThen, a new image compression method is proposed based on the preliminary image\ncompression algorithm that can improve standards of image compression. The new\nmethod is compared with original modes of JPEG and JPEG2000 (based on Haar\nwavelet) by image quality measures such as MAE, PSNAR, and SSIM. The image\nquality and statistical results confirm that can boost image compression\nstandards. It is suggested that the new method is used in a part or all of an\nimage compression standard.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 06:49:37 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 17:21:30 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Sadra", "Yaser", ""]]}, {"id": "1704.08834", "submitter": "Kevin Frans", "authors": "Kevin Frans", "title": "Outline Colorization through Tandem Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When creating digital art, coloring and shading are often time consuming\ntasks that follow the same general patterns. A solution to automatically\ncolorize raw line art would have many practical applications. We propose a\nsetup utilizing two networks in tandem: a color prediction network based only\non outlines, and a shading network conditioned on both outlines and a color\nscheme. We present processing methods to limit information passed in the color\nscheme, improving generalization. Finally, we demonstrate natural-looking\nresults when colorizing outlines from scratch, as well as from a messy,\nuser-defined color scheme.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 07:57:18 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Frans", "Kevin", ""]]}, {"id": "1704.08841", "submitter": "Matthew Rosen", "authors": "Bo Zhu, Jeremiah Z. Liu, Bruce R. Rosen, Matthew S. Rosen", "title": "Image reconstruction by domain transform manifold learning", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": "10.1038/nature25988", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction plays a critical role in the implementation of all\ncontemporary imaging modalities across the physical and life sciences including\noptical, MRI, CT, PET, and radio astronomy. During an image acquisition, the\nsensor encodes an intermediate representation of an object in the sensor\ndomain, which is subsequently reconstructed into an image by an inversion of\nthe encoding function. Image reconstruction is challenging because analytic\nknowledge of the inverse transform may not exist a priori, especially in the\npresence of sensor non-idealities and noise. Thus, the standard reconstruction\napproach involves approximating the inverse function with multiple ad hoc\nstages in a signal processing chain whose composition depends on the details of\neach acquisition strategy, and often requires expert parameter tuning to\noptimize reconstruction performance. We present here a unified framework for\nimage reconstruction, AUtomated TransfOrm by Manifold APproximation (AUTOMAP),\nwhich recasts image reconstruction as a data-driven, supervised learning task\nthat allows a mapping between sensor and image domain to emerge from an\nappropriate corpus of training data. We implement AUTOMAP with a deep neural\nnetwork and exhibit its flexibility in learning reconstruction transforms for a\nvariety of MRI acquisition strategies, using the same network architecture and\nhyperparameters. We further demonstrate its efficiency in sparsely representing\ntransforms along low-dimensional manifolds, resulting in superior immunity to\nnoise and reconstruction artifacts compared with conventional handcrafted\nreconstruction methods. In addition to improving the reconstruction performance\nof existing acquisition methodologies, we anticipate accelerating the discovery\nof new acquisition strategies across modalities as the burden of reconstruction\nbecomes lifted by AUTOMAP and learned-reconstruction approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:24:03 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Zhu", "Bo", ""], ["Liu", "Jeremiah Z.", ""], ["Rosen", "Bruce R.", ""], ["Rosen", "Matthew S.", ""]]}, {"id": "1704.08881", "submitter": "Christian Eggert", "authors": "Christian Eggert, Dan Zecha, Stephan Brehm, Rainer Lienhart", "title": "Improving Small Object Proposals for Company Logo Detection", "comments": "8 Pages, ICMR 2017", "journal-ref": null, "doi": "10.1145/3078971.3078990", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern approaches for object detection are two-staged pipelines. The\nfirst stage identifies regions of interest which are then classified in the\nsecond stage. Faster R-CNN is such an approach for object detection which\ncombines both stages into a single pipeline. In this paper we apply Faster\nR-CNN to the task of company logo detection. Motivated by its weak performance\non small object instances, we examine in detail both the proposal and the\nclassification stage with respect to a wide range of object sizes. We\ninvestigate the influence of feature map resolution on the performance of those\nstages.\n  Based on theoretical considerations, we introduce an improved scheme for\ngenerating anchor proposals and propose a modification to Faster R-CNN which\nleverages higher-resolution feature maps for small objects. We evaluate our\napproach on the FlickrLogos dataset improving the RPN performance from 0.52 to\n0.71 (MABO) and the detection performance from 0.52 to 0.67 (mAP).\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 11:30:10 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Eggert", "Christian", ""], ["Zecha", "Dan", ""], ["Brehm", "Stephan", ""], ["Lienhart", "Rainer", ""]]}, {"id": "1704.08908", "submitter": "Jose Dolz", "authors": "Jose Dolz and Ismail Ben Ayed and Christian Desrosiers", "title": "Unbiased Shape Compactness for Segmentation", "comments": "Accepted at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to constrain segmentation functionals with a dimensionless,\nunbiased and position-independent shape compactness prior, which we solve\nefficiently with an alternating direction method of multipliers (ADMM).\nInvolving a squared sum of pairwise potentials, our prior results in a\nchallenging high-order optimization problem, which involves dense (fully\nconnected) graphs. We split the problem into a sequence of easier sub-problems,\neach performed efficiently at each iteration: (i) a sparse-matrix inversion\nbased on Woodbury identity, (ii) a closed-form solution of a cubic equation and\n(iii) a graph-cut update of a sub-modular pairwise sub-problem with a sparse\ngraph. We deploy our prior in an energy minimization, in conjunction with a\nsupervised classifier term based on CNNs and standard regularization\nconstraints. We demonstrate the usefulness of our energy in several medical\napplications. In particular, we report comprehensive evaluations of our fully\nautomated algorithm over 40 subjects, showing a competitive performance for the\nchallenging task of abdominal aorta segmentation in MRI.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 12:54:44 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 00:49:38 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1704.08944", "submitter": "Guanjun Guo", "authors": "Guanjun Guo, Hanzi Wang, Wan-Lei Zhao, Yan Yan, Xuelong Li", "title": "Object Discovery via Cohesion Measurement", "comments": "14 pages, 14 figures", "journal-ref": "IEEE Transactions on Cybernetics (2017) 1-14", "doi": "10.1109/TCYB.2017.2661995", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color and intensity are two important components in an image. Usually, groups\nof image pixels, which are similar in color or intensity, are an informative\nrepresentation for an object. They are therefore particularly suitable for\ncomputer vision tasks, such as saliency detection and object proposal\ngeneration. However, image pixels, which share a similar real-world color, may\nbe quite different since colors are often distorted by intensity. In this\npaper, we reinvestigate the affinity matrices originally used in image\nsegmentation methods based on spectral clustering. A new affinity matrix, which\nis robust to color distortions, is formulated for object discovery. Moreover, a\nCohesion Measurement (CM) for object regions is also derived based on the\nformulated affinity matrix. Based on the new Cohesion Measurement, a novel\nobject discovery method is proposed to discover objects latent in an image by\nutilizing the eigenvectors of the affinity matrix. Then we apply the proposed\nmethod to both saliency detection and object proposal generation. Experimental\nresults on several evaluation benchmarks demonstrate that the proposed CM based\nmethod has achieved promising performance for these two tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 14:19:00 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Guo", "Guanjun", ""], ["Wang", "Hanzi", ""], ["Zhao", "Wan-Lei", ""], ["Yan", "Yan", ""], ["Li", "Xuelong", ""]]}, {"id": "1704.08949", "submitter": "Chollette Olisah Dr", "authors": "Chollette C. Olisah, Solomon Nunoo, Peter Ofedebe, Ghazali Sulong", "title": "Expressing Facial Structure and Appearance Information in Frequency\n  Domain for Face Recognition", "comments": "17 pages, 9 figures, ISSA CONFERENCE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beneath the uncertain primitive visual features of face images are the\nprimitive intrinsic structural patterns (PISP) essential for characterizing a\nsample face discriminative attributes. It is on this basis that this paper\npresents a simple yet effective facial descriptor formed from derivatives of\nGaussian and Gabor Wavelets. The new descriptor is coined local edge gradient\nGabor magnitude (LEGGM) pattern. LEGGM first uncovers the PISP locked in every\npixel through determining the pixel gradient in relation to its neighbors using\nthe Derivatives of Gaussians. Then, the resulting output is embedded into the\nglobal appearance of the face which are further processed using Gabor wavelets\nin order to express its frequency characteristics. Additionally, we adopted\nvarious subspace models for dimensionality reduction in order to ascertain the\nbest fit model for reporting a more effective representation of the LEGGM\npatterns. The proposed descriptor-based face recognition method is evaluated on\nthree databases: Plastic surgery, LFW, and GT face databases. Through\nexperiments, using a base classifier, the efficacy of the proposed method is\ndemonstrated, especially in the case of plastic surgery database. The\nheterogeneous database, which we created to typify real-world scenario, show\nthat the proposed method is to an extent insensitive to image formation factors\nwith impressive recognition performances.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 14:25:46 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Olisah", "Chollette C.", ""], ["Nunoo", "Solomon", ""], ["Ofedebe", "Peter", ""], ["Sulong", "Ghazali", ""]]}, {"id": "1704.08992", "submitter": "Jinsun Park", "authors": "Jinsun Park, Yu-Wing Tai, Donghyeon Cho and In So Kweon", "title": "A Unified Approach of Multi-scale Deep and Hand-crafted Features for\n  Defocus Estimation", "comments": "10 pages, 14 figures. To appear in CVPR 2017. Project page :\n  https://github.com/zzangjinsun/DHDE_CVPR17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce robust and synergetic hand-crafted features and a\nsimple but efficient deep feature from a convolutional neural network (CNN)\narchitecture for defocus estimation. This paper systematically analyzes the\neffectiveness of different features, and shows how each feature can compensate\nfor the weaknesses of other features when they are concatenated. For a full\ndefocus map estimation, we extract image patches on strong edges sparsely,\nafter which we use them for deep and hand-crafted feature extraction. In order\nto reduce the degree of patch-scale dependency, we also propose a multi-scale\npatch extraction strategy. A sparse defocus map is generated using a neural\nnetwork classifier followed by a probability-joint bilateral filter. The final\ndefocus map is obtained from the sparse defocus map with guidance from an\nedge-preserving filtered input image. Experimental results show that our\nalgorithm is superior to state-of-the-art algorithms in terms of defocus\nestimation. Our work can be used for applications such as segmentation, blur\nmagnification, all-in-focus image generation, and 3-D estimation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 16:16:41 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Park", "Jinsun", ""], ["Tai", "Yu-Wing", ""], ["Cho", "Donghyeon", ""], ["Kweon", "In So", ""]]}]