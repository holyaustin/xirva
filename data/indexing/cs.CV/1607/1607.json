[{"id": "1607.00137", "submitter": "Chunlei Peng", "authors": "Chunlei Peng, Xinbo Gao, Nannan Wang, Jie Li", "title": "Sparse Graphical Representation based Discriminant Analysis for\n  Heterogeneous Face Recognition", "comments": "13 pages, 17 figures, submitted to IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images captured in heterogeneous environments, e.g., sketches generated\nby the artists or composite-generation software, photos taken by common cameras\nand infrared images captured by corresponding infrared imaging devices, usually\nsubject to large texture (i.e., style) differences. This results in heavily\ndegraded performance of conventional face recognition methods in comparison\nwith the performance on images captured in homogeneous environments. In this\npaper, we propose a novel sparse graphical representation based discriminant\nanalysis (SGR-DA) approach to address aforementioned face recognition in\nheterogeneous scenarios. An adaptive sparse graphical representation scheme is\ndesigned to represent heterogeneous face images, where a Markov networks model\nis constructed to generate adaptive sparse vectors. To handle the complex\nfacial structure and further improve the discriminability, a spatial\npartition-based discriminant analysis framework is presented to refine the\nadaptive sparse vectors for face matching. We conducted experiments on six\ncommonly used heterogeneous face datasets and experimental results illustrate\nthat our proposed SGR-DA approach achieves superior performance in comparison\nwith state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:41:25 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Peng", "Chunlei", ""], ["Gao", "Xinbo", ""], ["Wang", "Nannan", ""], ["Li", "Jie", ""]]}, {"id": "1607.00267", "submitter": "Gustavo Carneiro", "authors": "Gustavo Carneiro and Luke Oakden-Rayner and Andrew P. Bradley and\n  Jacinto Nascimento and Lyle Palmer", "title": "Automated 5-year Mortality Prediction using Deep Learning and Radiomics\n  Features from Chest Computed Tomography", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new methods for the prediction of 5-year mortality in elderly\nindividuals using chest computed tomography (CT). The methods consist of a\nclassifier that performs this prediction using a set of features extracted from\nthe CT image and segmentation maps of multiple anatomic structures. We explore\ntwo approaches: 1) a unified framework based on deep learning, where features\nand classifier are automatically learned in a single optimisation process; and\n2) a multi-stage framework based on the design and selection/extraction of\nhand-crafted radiomics features, followed by the classifier learning process.\nExperimental results, based on a dataset of 48 annotated chest CTs, show that\nthe deep learning model produces a mean 5-year mortality prediction accuracy of\n68.5%, while radiomics produces a mean accuracy that varies between 56% to 66%\n(depending on the feature selection/extraction method and classifier). The\nsuccessful development of the proposed models has the potential to make a\nprofound impact in preventive and personalised healthcare.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 14:44:37 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Carneiro", "Gustavo", ""], ["Oakden-Rayner", "Luke", ""], ["Bradley", "Andrew P.", ""], ["Nascimento", "Jacinto", ""], ["Palmer", "Lyle", ""]]}, {"id": "1607.00273", "submitter": "Pablo F. Alcantarilla Dr.", "authors": "Pablo F. Alcantarilla and Oliver J. Woodford", "title": "Noise Models in Feature-based Stereo Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based visual structure and motion reconstruction pipelines, common in\nvisual odometry and large-scale reconstruction from photos, use the location of\ncorresponding features in different images to determine the 3D structure of the\nscene, as well as the camera parameters associated with each image. The noise\nmodel, which defines the likelihood of the location of each feature in each\nimage, is a key factor in the accuracy of such pipelines, alongside\noptimization strategy. Many different noise models have been proposed in the\nliterature; in this paper we investigate the performance of several. We\nevaluate these models specifically w.r.t. stereo visual odometry, as this task\nis both simple (camera intrinsics are constant and known; geometry can be\ninitialized reliably) and has datasets with ground truth readily available\n(KITTI Odometry and New Tsukuba Stereo Dataset). Our evaluation shows that\nnoise models which are more adaptable to the varying nature of noise generally\nperform better.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:02:38 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Alcantarilla", "Pablo F.", ""], ["Woodford", "Oliver J.", ""]]}, {"id": "1607.00331", "submitter": "Ghada Zamzmi", "authors": "Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Yu Sun, and Terri\n  Ashmeade", "title": "Machine-based Multimodal Pain Assessment Tool for Infants: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bedside caregivers assess infants' pain at constant intervals by observing\nspecific behavioral and physiological signs of pain. This standard has two main\nlimitations. The first limitation is the intermittent assessment of pain, which\nmight lead to missing pain when the infants are left unattended. Second, it is\ninconsistent since it depends on the observer's subjective judgment and differs\nbetween observers. The intermittent and inconsistent assessment can induce poor\ntreatment and, therefore, cause serious long-term consequences. To mitigate\nthese limitations, the current standard can be augmented by an automated system\nthat monitors infants continuously and provides quantitative and consistent\nassessment of pain. Several automated methods have been introduced to assess\ninfants' pain automatically based on analysis of behavioral or physiological\npain indicators. This paper comprehensively reviews the automated approaches\n(i.e., approaches to feature extraction) for analyzing infants' pain and the\ncurrent efforts in automatic pain recognition. In addition, it reviews the\ndatabases available to the research community and discusses the current\nlimitations of the automated pain assessment.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 17:53:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 17:52:33 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 16:16:13 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Zamzmi", "Ghada", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Sun", "Yu", ""], ["Ashmeade", "Terri", ""]]}, {"id": "1607.00417", "submitter": "Abir Das", "authors": "Abir Das, Rameswar Panda and Amit K. Roy-Chowdhury", "title": "Continuous Adaptation of Multi-Camera Person Identification Models\n  through Sparse Non-redundant Representative Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of image-base person identification/recognition is to provide an\nidentity to the image of an individual based on learned models that describe\nhis/her appearance. Most traditional person identification systems rely on\nlearning a static model on tediously labeled training data. Though labeling\nmanually is an indispensable part of a supervised framework, for a large scale\nidentification system labeling huge amount of data is a significant overhead.\nFor large multi-sensor data as typically encountered in camera networks,\nlabeling a lot of samples does not always mean more information, as redundant\nimages are labeled several times. In this work, we propose a convex\noptimization based iterative framework that progressively and judiciously\nchooses a sparse but informative set of samples for labeling, with minimal\noverlap with previously labeled images. We also use a structure preserving\nsparse reconstruction based classifier to reduce the training burden typically\nseen in discriminative classifiers. The two stage approach leads to a novel\nframework for online update of the classifiers involving only the incorporation\nof new labeled data rather than any expensive training phase. We demonstrate\nthe effectiveness of our approach on multi-camera person re-identification\ndatasets, to demonstrate the feasibility of learning online classification\nmodels in multi-camera big data applications. Using three benchmark datasets,\nwe validate our approach and demonstrate that our framework achieves superior\nperformance with significantly less amount of manual labeling.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 21:48:16 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Das", "Abir", ""], ["Panda", "Rameswar", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1607.00464", "submitter": "Le Dong", "authors": "Le Dong, Xiuyuan Chen, Mengdie Mao, Qianni Zhang", "title": "NIST: An Image Classification Network to Image Semantic Retrieval", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a classification network to image semantic retrieval\n(NIST) framework to counter the image retrieval challenge. Our approach\nleverages the successful classification network GoogleNet based on\nConvolutional Neural Networks to obtain the semantic feature matrix which\ncontains the serial number of classes and corresponding probabilities. Compared\nwith traditional image retrieval using feature matching to compute the\nsimilarity between two images, NIST leverages the semantic information to\nconstruct semantic feature matrix and uses the semantic distance algorithm to\ncompute the similarity. Besides, the fusion strategy can significantly reduce\nstorage and time consumption due to less classes participating in the last\nsemantic distance computation. Experiments demonstrate that our NIST framework\nproduces state-of-the-art results in retrieval experiments on MIRFLICKR-25K\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 04:39:24 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Dong", "Le", ""], ["Chen", "Xiuyuan", ""], ["Mao", "Mengdie", ""], ["Zhang", "Qianni", ""]]}, {"id": "1607.00470", "submitter": "Georges Younes Mr.", "authors": "Georges Younes, Daniel Asmar, Elie Shammas, John Zelek", "title": "Keyframe-based monocular SLAM: design, survey, and future directions", "comments": null, "journal-ref": "Robotics and Autonomous Systems, Volume 98, 2017, Pages 67-88", "doi": "10.1016/j.robot.2017.09.010", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive research in the field of monocular SLAM for the past fifteen years\nhas yielded workable systems that found their way into various applications in\nrobotics and augmented reality. Although filter-based monocular SLAM systems\nwere common at some time, the more efficient keyframe-based solutions are\nbecoming the de facto methodology for building a monocular SLAM system. The\nobjective of this paper is threefold: first, the paper serves as a guideline\nfor people seeking to design their own monocular SLAM according to specific\nenvironmental constraints. Second, it presents a survey that covers the various\nkeyframe-based monocular SLAM systems in the literature, detailing the\ncomponents of their implementation, and critically assessing the specific\nstrategies made in each proposed solution. Third, the paper provides insight\ninto the direction of future research in this field, to address the major\nlimitations still facing monocular SLAM; namely, in the issues of illumination\nchanges, initialization, highly dynamic motion, poorly textured scenes,\nrepetitive textures, map maintenance, and failure recovery.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 06:01:16 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 20:47:25 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Younes", "Georges", ""], ["Asmar", "Daniel", ""], ["Shammas", "Elie", ""], ["Zelek", "John", ""]]}, {"id": "1607.00501", "submitter": "Le Dong", "authors": "Le Dong, Na Lv, Qianni Zhang, Shanshan Xie, Ling He, Mengdie Mao", "title": "A Distributed Deep Representation Learning Model for Big Image Data\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an effective and efficient image classification\nframework nominated distributed deep representation learning model (DDRL). The\naim is to strike the balance between the computational intensive deep learning\napproaches (tuned parameters) which are intended for distributed computing, and\nthe approaches that focused on the designed parameters but often limited by\nsequential computing and cannot scale up. In the evaluation of our approach, it\nis shown that DDRL is able to achieve state-of-art classification accuracy\nefficiently on both medium and large datasets. The result implies that our\napproach is more efficient than the conventional deep learning approaches, and\ncan be applied to big data that is too complex for parameter designing focused\napproaches. More specifically, DDRL contains two main components, i.e., feature\nextraction and selection. A hierarchical distributed deep representation\nlearning algorithm is designed to extract image statistics and a nonlinear\nmapping algorithm is used to map the inherent statistics into abstract\nfeatures. Both algorithms are carefully designed to avoid millions of\nparameters tuning. This leads to a more compact solution for image\nclassification of big data. We note that the proposed approach is designed to\nbe friendly with parallel computing. It is generic and easy to be deployed to\ndifferent distributed computing resources. In the experiments, the largescale\nimage datasets are classified with a DDRM implementation on Hadoop MapReduce,\nwhich shows high scalability and resilience.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 12:33:12 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Dong", "Le", ""], ["Lv", "Na", ""], ["Zhang", "Qianni", ""], ["Xie", "Shanshan", ""], ["He", "Ling", ""], ["Mao", "Mengdie", ""]]}, {"id": "1607.00548", "submitter": "Melanie Mitchell", "authors": "Max H. Quinn, Anthony D. Rhodes, Melanie Mitchell", "title": "Active Object Localization in Visual Situations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for performing active localization of objects in\ninstances of visual situations. A visual situation is an abstract\nconcept---e.g., \"a boxing match\", \"a birthday party\", \"walking the dog\",\n\"waiting for a bus\"---whose image instantiations are linked more by their\ncommon spatial and semantic structure than by low-level visual similarity. Our\nsystem combines given and learned knowledge of the structure of a particular\nsituation, and adapts that knowledge to a new situation instance as it actively\nsearches for objects. More specifically, the system learns a set of probability\ndistributions describing spatial and other relationships among relevant\nobjects. The system uses those distributions to iteratively sample object\nproposals on a test image, but also continually uses information from those\nobject proposals to adaptively modify the distributions based on what the\nsystem has detected. We test our approach's ability to efficiently localize\nobjects, using a situation-specific image dataset created by our group. We\ncompare the results with several baselines and variations on our method, and\ndemonstrate the strong benefit of using situation knowledge and active\ncontext-driven localization. Finally, we contrast our method with several other\napproaches that use context as well as active search for object localization in\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 18:43:07 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Quinn", "Max H.", ""], ["Rhodes", "Anthony D.", ""], ["Mitchell", "Melanie", ""]]}, {"id": "1607.00577", "submitter": "Le Dong", "authors": "Le Dong, Zhiyu Lin, Yan Liang, Ling He, Ning Zhang, Qi Chen, Xiaochun\n  Cao, Ebroul lzquierdo", "title": "A Hierarchical Distributed Processing Framework for Big Image Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an effective processing framework nominated ICP (Image\nCloud Processing) to powerfully cope with the data explosion in image\nprocessing field. While most previous researches focus on optimizing the image\nprocessing algorithms to gain higher efficiency, our work dedicates to\nproviding a general framework for those image processing algorithms, which can\nbe implemented in parallel so as to achieve a boost in time efficiency without\ncompromising the results performance along with the increasing image scale. The\nproposed ICP framework consists of two mechanisms, i.e. SICP (Static ICP) and\nDICP (Dynamic ICP). Specifically, SICP is aimed at processing the big image\ndata pre-stored in the distributed system, while DICP is proposed for dynamic\ninput. To accomplish SICP, two novel data representations named P-Image and\nBig-Image are designed to cooperate with MapReduce to achieve more optimized\nconfiguration and higher efficiency. DICP is implemented through a parallel\nprocessing procedure working with the traditional processing mechanism of the\ndistributed system. Representative results of comprehensive experiments on the\nchallenging ImageNet dataset are selected to validate the capacity of our\nproposed ICP framework over the traditional state-of-the-art methods, both in\ntime efficiency and quality of results.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 02:16:49 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Dong", "Le", ""], ["Lin", "Zhiyu", ""], ["Liang", "Yan", ""], ["He", "Ling", ""], ["Zhang", "Ning", ""], ["Chen", "Qi", ""], ["Cao", "Xiaochun", ""], ["lzquierdo", "Ebroul", ""]]}, {"id": "1607.00582", "submitter": "Qi Dou", "authors": "Qi Dou, Hao Chen, Yueming Jin, Lequan Yu, Jing Qin, Pheng-Ann Heng", "title": "3D Deeply Supervised Network for Automatic Liver Segmentation from CT\n  Volumes", "comments": "Accepted to MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic liver segmentation from CT volumes is a crucial prerequisite yet\nchallenging task for computer-aided hepatic disease diagnosis and treatment. In\nthis paper, we present a novel 3D deeply supervised network (3D DSN) to address\nthis challenging task. The proposed 3D DSN takes advantage of a fully\nconvolutional architecture which performs efficient end-to-end learning and\ninference. More importantly, we introduce a deep supervision mechanism during\nthe learning process to combat potential optimization difficulties, and thus\nthe model can acquire a much faster convergence rate and more powerful\ndiscrimination capability. On top of the high-quality score map produced by the\n3D DSN, a conditional random field model is further employed to obtain refined\nsegmentation results. We evaluated our framework on the public MICCAI-SLiver07\ndataset. Extensive experiments demonstrated that our method achieves\ncompetitive segmentation results to state-of-the-art approaches with a much\nfaster processing speed.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 02:52:56 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Jin", "Yueming", ""], ["Yu", "Lequan", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1607.00589", "submitter": "Naima Kaabouch", "authors": "Naima Kaabouch, Richard R. Schultz, Barry Milavetz, and Lata\n  Balakrishnan", "title": "An Analysis System for DNA Gel Electrophoresis Images Based on Automatic\n  Thresholding an Enhancement", "comments": "6 pages", "journal-ref": "IEEE Electro/Information Technology, 2007", "doi": "10.1109/EIT.2007.4374496", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gel electrophoresis, a widely used technique to separate DNA according to\ntheir size and weight, generates images that can be analyzed automatically.\nManual or semiautomatic image processing presents a bottleneck for further\ndevelopment and leads to reproducibility issues. In this paper, we present a\nfully automated system with high accuracy for analyzing DNA and proteins. The\nproposed algorithm consists of four main steps: automatic thresholding,\nshifting, filtering, and data processing. Automatic thresholding, used to\nequalize the gray values of the gel electrophoresis image background, is one of\nthe novel operations in this algorithm. Enhancement is also used to improve\npoor quality images that have faint DNA bands. Experimental results show that\nthe proposed technique eliminates defects due to noise for average quality gel\nelectrophoresis images, while it also improves the quality of poor images.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 04:23:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kaabouch", "Naima", ""], ["Schultz", "Richard R.", ""], ["Milavetz", "Barry", ""], ["Balakrishnan", "Lata", ""]]}, {"id": "1607.00592", "submitter": "Naima Kaabouch", "authors": "Naima Kaabouch and Hamid Shahbazkia", "title": "Automatic Techniques for Gridding cDNA Microarray Images", "comments": "5 pages, IEEE Electro/Information Technology, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray is considered an important instrument and powerful new technology\nfor large-scale gene sequence and gene expression analysis. One of the major\nchallenges of this technique is the image processing phase. The accuracy of\nthis phase has an important impact on the accuracy and effectiveness of the\nsubsequent gene expression and identification analysis. The processing can be\norganized mainly into four steps: gridding, spot isolation, segmentation, and\nquantification. Although several commercial software packages are now\navailable, microarray image analysis still requires some intervention by the\nuser, and thus a certain level of image processing expertise. This paper\ndescribes and compares four techniques that perform automatic gridding and spot\nisolation. The proposed techniques are based on template matching technique,\nstandard deviation, sum, and derivative of these profiles. Experimental results\nshow that the accuracy of the derivative of the sum profile is highly accurate\ncompared to other techniques for good and poor quality microarray images.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 05:03:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kaabouch", "Naima", ""], ["Shahbazkia", "Hamid", ""]]}, {"id": "1607.00598", "submitter": "Yuzhuo Ren", "authors": "Yuzhuo Ren, Chen Chen, Shangwen Li, and C.-C. Jay Kuo", "title": "A Coarse-to-Fine Indoor Layout Estimation (CFILE) Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of estimating the spatial layout of cluttered indoor scenes from a\nsingle RGB image is addressed in this work. Existing solutions to this problems\nlargely rely on hand-craft features and vanishing lines, and they often fail in\nhighly cluttered indoor rooms. The proposed coarse-to-fine indoor layout\nestimation (CFILE) method consists of two stages: 1) coarse layout estimation;\nand 2) fine layout localization. In the first stage, we adopt a fully\nconvolutional neural network (FCN) to obtain a coarse-scale room layout\nestimate that is close to the ground truth globally. The proposed FCN considers\ncombines the layout contour property and the surface property so as to provide\na robust estimate in the presence of cluttered objects. In the second stage, we\nformulate an optimization framework that enforces several constraints such as\nlayout contour straightness, surface smoothness and geometric constraints for\nlayout detail refinement. Our proposed system offers the state-of-the-art\nperformance on two commonly used benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 05:55:47 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Ren", "Yuzhuo", ""], ["Chen", "Chen", ""], ["Li", "Shangwen", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1607.00623", "submitter": "Kaveh Hassani", "authors": "Kaveh Hassani and Won-Sook Lee", "title": "Visualizing Natural Language Descriptions: A Survey", "comments": "Due to copyright most of the figures only appear in the journal\n  version", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, Article No. 17, June\n  2016", "doi": "10.1145/2932710", "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural language interface exploits the conceptual simplicity and\nnaturalness of the language to create a high-level user-friendly communication\nchannel between humans and machines. One of the promising applications of such\ninterfaces is generating visual interpretations of semantic content of a given\nnatural language that can be then visualized either as a static scene or a\ndynamic animation. This survey discusses requirements and challenges of\ndeveloping such systems and reports 26 graphical systems that exploit natural\nlanguage interfaces and addresses both artificial intelligence and\nvisualization aspects. This work serves as a frame of reference to researchers\nand to enable further advances in the field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 10:30:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hassani", "Kaveh", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1607.00659", "submitter": "Kha Gia Quach", "authors": "Kha Gia Quach, Chi Nhan Duong, Khoa Luu and Tien D. Bui", "title": "Robust Deep Appearance Models", "comments": "6 pages, 8 figures, submitted to ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Robust Deep Appearance Models to learn the\nnon-linear correlation between shape and texture of face images. In this\napproach, two crucial components of face images, i.e. shape and texture, are\nrepresented by Deep Boltzmann Machines and Robust Deep Boltzmann Machines\n(RDBM), respectively. The RDBM, an alternative form of Robust Boltzmann\nMachines, can separate corrupted/occluded pixels in the texture modeling to\nachieve better reconstruction results. The two models are connected by\nRestricted Boltzmann Machines at the top layer to jointly learn and capture the\nvariations of both facial shapes and appearances. This paper also introduces\nnew fitting algorithms with occlusion awareness through the mask obtained from\nthe RDBM reconstruction. The proposed approach is evaluated in various\napplications by using challenging face datasets, i.e. Labeled Face Parts in the\nWild (LFPW), Helen, EURECOM and AR databases, to demonstrate its robustness and\ncapabilities.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 17:31:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Quach", "Kha Gia", ""], ["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Bui", "Tien D.", ""]]}, {"id": "1607.00662", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and S. M. Ali Eslami and Shakir Mohamed and\n  Peter Battaglia and Max Jaderberg and Nicolas Heess", "title": "Unsupervised Learning of 3D Structure from Images", "comments": "Appears in Advances in Neural Information Processing Systems 29 (NIPS\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key goal of computer vision is to recover the underlying 3D structure from\n2D observations of the world. In this paper we learn strong deep generative\nmodels of 3D structures, and recover these structures from 3D and 2D images via\nprobabilistic inference. We demonstrate high-quality samples and report\nlog-likelihoods on several datasets, including ShapeNet [2], and establish the\nfirst benchmarks in the literature. We also show how these models and their\ninference networks can be trained end-to-end from 2D images. This demonstrates\nfor the first time the feasibility of learning to infer 3D representations of\nthe world in a purely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 17:53:11 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 17:26:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Eslami", "S. M. Ali", ""], ["Mohamed", "Shakir", ""], ["Battaglia", "Peter", ""], ["Jaderberg", "Max", ""], ["Heess", "Nicolas", ""]]}, {"id": "1607.00719", "submitter": "Le Dong", "authors": "Gaipeng Kong, Le Dong, Wenpu Dong, Liang Zheng, Qi Tian", "title": "Coarse2Fine: Two-Layer Fusion For Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of large-scale image retrieval. We propose a\ntwo-layer fusion method which takes advantage of global and local cues and\nranks database images from coarse to fine (C2F). Departing from the previous\nmethods fusing multiple image descriptors simultaneously, C2F is featured by a\nlayered procedure composed by filtering and refining. In particular, C2F\nconsists of three components. 1) Distractor filtering. With holistic\nrepresentations, noise images are filtered out from the database, so the number\nof candidate images to be used for comparison with the query can be greatly\nreduced. 2) Adaptive weighting. For a certain query, the similarity of\ncandidate images can be estimated by holistic similarity scores in\ncomplementary to the local ones. 3) Candidate refining. Accurate retrieval is\nconducted via local features, combining the pre-computed adaptive weights.\nExperiments are presented on two benchmarks, \\emph{i.e.,} Holidays and Ukbench\ndatasets. We show that our method outperforms recent fusion methods in terms of\nstorage consumption and computation complexity, and that the accuracy is\ncompetitive to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:56:20 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kong", "Gaipeng", ""], ["Dong", "Le", ""], ["Dong", "Wenpu", ""], ["Zheng", "Liang", ""], ["Tian", "Qi", ""]]}, {"id": "1607.00730", "submitter": "Jun Li", "authors": "Jun Li and Reinhard Klein and Angela Yao", "title": "A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single\n  RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from a single RGB image is an ill-posed and inherently\nambiguous problem. State-of-the-art deep learning methods can now estimate\naccurate 2D depth maps, but when the maps are projected into 3D, they lack\nlocal detail and are often highly distorted. We propose a fast-to-train\ntwo-streamed CNN that predicts depth and depth gradients, which are then fused\ntogether into an accurate and detailed depth map. We also define a novel set\nloss over multiple images; by regularizing the estimation between a common set\nof images, the network is less prone to over-fitting and achieves better\naccuracy than competing methods. Experiments on the NYU Depth v2 dataset shows\nthat our depth predictions are competitive with state-of-the-art and lead to\nfaithful 3D projections.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 03:22:45 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 16:54:46 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 08:49:37 GMT"}, {"version": "v4", "created": "Mon, 4 Dec 2017 01:52:43 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Jun", ""], ["Klein", "Reinhard", ""], ["Yao", "Angela", ""]]}, {"id": "1607.00969", "submitter": "Eleonora Russo", "authors": "Eleonora Russo and Daniel Durstewitz", "title": "Cell assemblies at multiple time scales with arbitrary lag\n  constellations", "comments": null, "journal-ref": "eLife 2017;6:e19428", "doi": "10.7554/eLife.19428", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hebb's idea of a cell assembly as the fundamental unit of neural information\nprocessing has dominated neuroscience like no other theoretical concept within\nthe past 60 years. A range of different physiological phenomena, from precisely\nsynchronized spiking to broadly simultaneous rate increases, has been subsumed\nunder this term. Yet progress in this area is hampered by the lack of\nstatistical tools that would enable to extract assemblies with arbitrary\nconstellations of time lags, and at multiple temporal scales, partly due to the\nsevere computational burden. Here we present such a unifying methodological and\nconceptual framework which detects assembly structure at many different time\nscales, levels of precision, and with arbitrary internal organization. Applying\nthis methodology to multiple single unit recordings from various cortical\nareas, we find that there is no universal cortical coding scheme, but that\nassembly structure and precision significantly depends on brain area recorded\nand ongoing task demands.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:35:35 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 14:24:54 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Russo", "Eleonora", ""], ["Durstewitz", "Daniel", ""]]}, {"id": "1607.00971", "submitter": "Eduardo Romera Carmena", "authors": "Eduardo Romera, Luis M. Bergasa, Roberto Arroyo", "title": "Can we unify monocular detectors for autonomous driving by using the\n  pixel-wise semantic segmentation of CNNs?", "comments": "Extended abstract presented in IV16-WS Deepdriving\n  (http://iv2016.berkeleyvision.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a challenging topic that requires complex solutions in\nperception tasks such as recognition of road, lanes, traffic signs or lights,\nvehicles and pedestrians. Through years of research, computer vision has grown\ncapable of tackling these tasks with monocular detectors that can provide\nremarkable detection rates with relatively low processing times. However, the\nrecent appearance of Convolutional Neural Networks (CNNs) has revolutionized\nthe computer vision field and has made possible approaches to perform full\npixel-wise semantic segmentation in times close to real time (even on hardware\nthat can be carried on a vehicle). In this paper, we propose to use full image\nsegmentation as an approach to simplify and unify most of the detection tasks\nrequired in the perception module of an autonomous vehicle, analyzing major\nconcerns such as computation time and detection performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:44:13 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Romera", "Eduardo", ""], ["Bergasa", "Luis M.", ""], ["Arroyo", "Roberto", ""]]}, {"id": "1607.01040", "submitter": "KitIan Kou", "authors": "Cuiming Zou and Kit Ian Kou", "title": "Facial Expression Classification Using Rotation Slepian-based Moment\n  Invariants", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation moment invariants have been of great interest in image processing\nand pattern recognition. This paper presents a novel kind of rotation moment\ninvariants based on the Slepian functions, which were originally introduced in\nthe method of separation of variables for Helmholtz equations. They were first\nproposed for time series by Slepian and his coworkers in the 1960s. Recent\nstudies have shown that these functions have an good performance in local\napproximation compared to other approximation basis. Motivated by the good\napproximation performance, we construct the Slepian-based moments and derive\nthe rotation invariant. We not only theoretically prove the invariance, but\nalso discuss the experiments on real data. The proposed rotation invariants are\nrobust to noise and yield decent performance in facial expression\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 07:54:03 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Zou", "Cuiming", ""], ["Kou", "Kit Ian", ""]]}, {"id": "1607.01059", "submitter": "Chelsea Weaver", "authors": "Chelsea Weaver and Naoki Saito", "title": "Improving Sparse Representation-Based Classification Using Local\n  Principal Component Analysis", "comments": "Published in \"Computational Intelligence for Pattern Recognition,\"\n  editors Shyi-Ming Chen and Witold Pedrycz. The original publication is\n  available at http://www.springerlink.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation-based classification (SRC), proposed by Wright et al.,\nseeks the sparsest decomposition of a test sample over the dictionary of\ntraining samples, with classification to the most-contributing class. Because\nit assumes test samples can be written as linear combinations of their\nsame-class training samples, the success of SRC depends on the size and\nrepresentativeness of the training set. Our proposed classification algorithm\nenlarges the training set by using local principal component analysis to\napproximate the basis vectors of the tangent hyperplane of the class manifold\nat each training sample. The dictionary in SRC is replaced by a local\ndictionary that adapts to the test sample and includes training samples and\ntheir corresponding tangent basis vectors. We use a synthetic data set and\nthree face databases to demonstrate that this method can achieve higher\nclassification accuracy than SRC in cases of sparse sampling, nonlinear class\nmanifolds, and stringent dimension reduction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 21:48:29 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 17:09:28 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 20:32:17 GMT"}, {"version": "v4", "created": "Sat, 9 Dec 2017 20:00:48 GMT"}, {"version": "v5", "created": "Sun, 20 May 2018 00:14:31 GMT"}, {"version": "v6", "created": "Sat, 2 Jun 2018 05:27:13 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Weaver", "Chelsea", ""], ["Saito", "Naoki", ""]]}, {"id": "1607.01076", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan, Gerald Knapp", "title": "Aggressive actions and anger detection from multiple modalities using\n  Kinect", "comments": "11 pages, 2 figures, 5 tables, in peer review with ACM TIST, Key\n  words: Aggression, multimodal anger recognition, Kinect", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prison facilities, mental correctional institutions, sports bars and places\nof public protest are prone to sudden violence and conflicts. Surveillance\nsystems play an important role in mitigation of hostile behavior and\nimprovement of security by detecting such provocative and aggressive\nactivities. This research proposed using automatic aggressive behavior and\nanger detection to improve the effectiveness of the surveillance systems. An\nemotion and aggression aware component will make the surveillance system highly\nresponsive and capable of alerting the security guards in real time. This\nresearch proposed facial expression, head, hand and body movement and speech\ntracking for detecting anger and aggressive actions. Recognition was achieved\nusing support vector machines and rule based features. The multimodal affect\nrecognition precision rate for anger improved by 15.2% and recall rate improved\nby 11.7% when behavioral rule based features were used in aggressive action\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 00:04:45 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.01077", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "EmoFit: Affect Monitoring System for Sedentary Jobs", "comments": "9 pages, 10 figures, Preprint, arXiv.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional and physical well-being at workplace is important for a positive\nwork environment and higher productivity. Jobs such as software programming\nlead to a sedentary lifestyle and require high interaction with computers.\nWorking at the same job for years can cause a feeling of intellectual\nstagnation and lack of drive. Many employees experience lack of motivation,\nmild to extreme depression due to reasons such as aversion towards job\nresponsibilities and incompatibility with coworkers or boss. This research\nproposed an affect monitoring system EmoFit that would play the role of\npsychological and physical health trainer. The day to day computer activity and\nbody language was analyzed to detect the physical and emotional well-being of\nthe user. Keystrokes, activity interruptions, eye tracking, facial expressions,\nbody posture and speech were monitored to gauge the users health. The system\nalso provided activities such as at-desk exercise and stress relief game and\nmotivational quotes in an attempt to promote users well-being. The experimental\nresults and positive feedback from test subjects showed that EmoFit would help\nimprove emotional and physical well-being at jobs that involve significant\ncomputer usage.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 00:08:21 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.01092", "submitter": "Masoud S. Nosrati", "authors": "Masoud S. Nosrati and Ghassan Hamarneh", "title": "Incorporating prior knowledge in medical image segmentation: a survey", "comments": "Survey paper, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation, the task of partitioning an image into meaningful\nparts, is an important step toward automating medical image analysis and is at\nthe crux of a variety of medical imaging applications, such as computer aided\ndiagnosis, therapy planning and delivery, and computer aided interventions.\nHowever, the existence of noise, low contrast and objects' complexity in\nmedical images are critical obstacles that stand in the way of achieving an\nideal segmentation system. Incorporating prior knowledge into image\nsegmentation algorithms has proven useful for obtaining more accurate and\nplausible results. This paper surveys the different types of prior knowledge\nthat have been utilized in different segmentation frameworks. We focus our\nsurvey on optimization-based methods that incorporate prior information into\ntheir frameworks. We review and compare these methods in terms of the types of\nprior employed, the domain of formulation (continuous vs. discrete), and the\noptimization techniques (global vs. local). We also created an interactive\nonline database of existing works and categorized them based on the type of\nprior knowledge they use. Our website is interactive so that researchers can\ncontribute to keep the database up to date. We conclude the survey by\ndiscussing different aspects of designing an energy functional for image\nsegmentation, open problems, and future perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 02:19:57 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Nosrati", "Masoud S.", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1607.01115", "submitter": "Suyog Jain", "authors": "Suyog Dutt Jain, Kristen Grauman", "title": "Click Carving: Segmenting Objects in Video with Point Clicks", "comments": "A preliminary version of the material in this document was filed as\n  University of Texas technical report no. UT AI16-01", "journal-ref": null, "doi": null, "report-no": "University of Texas Technical Report UT AI16-01", "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel form of interactive video object segmentation where a few\nclicks by the user helps the system produce a full spatio-temporal segmentation\nof the object of interest. Whereas conventional interactive pipelines take the\nuser's initialization as a starting point, we show the value in the system\ntaking the lead even in initialization. In particular, for a given video frame,\nthe system precomputes a ranked list of thousands of possible segmentation\nhypotheses (also referred to as object region proposals) using image and motion\ncues. Then, the user looks at the top ranked proposals, and clicks on the\nobject boundary to carve away erroneous ones. This process iterates (typically\n2-3 times), and each time the system revises the top ranked proposal set, until\nthe user is satisfied with a resulting segmentation mask. Finally, the mask is\npropagated across the video to produce a spatio-temporal object tube. On three\nchallenging datasets, we provide extensive comparisons with both existing work\nand simpler alternative methods. In all, the proposed Click Carving approach\nstrikes an excellent balance of accuracy and human effort. It outperforms all\nsimilarly fast methods, and is competitive or better than those requiring 2 to\n12 times the effort.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 05:35:22 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Jain", "Suyog Dutt", ""], ["Grauman", "Kristen", ""]]}, {"id": "1607.01205", "submitter": "David Novotn\\'y", "authors": "David Novotny, Diane Larlus, Andrea Vedaldi", "title": "Learning the semantic structure of objects from Web supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent research in image understanding has often focused on recognizing\nmore types of objects, understanding more about the objects is just as\nimportant. Recognizing object parts and attributes has been extensively studied\nbefore, yet learning large space of such concepts remains elusive due to the\nhigh cost of providing detailed object annotations for supervision. The key\ncontribution of this paper is an algorithm to learn the nameable parts of\nobjects automatically, from images obtained by querying Web search engines. The\nkey challenge is the high level of noise in the annotations; to address it, we\npropose a new unified embedding space where the appearance and geometry of\nobjects and their semantic parts are represented uniformly. Geometric\nrelationships are induced in a soft manner by a rich set of nonsemantic\nmid-level anchors, bridging the gap between semantic and non-semantic parts. We\nalso show that the resulting embedding provides a visually-intuitive mechanism\nto navigate the learned concepts and their corresponding images.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 11:56:31 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Novotny", "David", ""], ["Larlus", "Diane", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1607.01232", "submitter": "Giuseppe Boccignone", "authors": "Giuseppe Boccignone", "title": "A probabilistic tour of visual attention and gaze shift computational\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a number of problems are considered which are related to the\nmodelling of eye guidance under visual attention in a natural setting. From a\ncrude discussion of a variety of available models spelled in probabilistic\nterms, it appears that current approaches in computational vision are hitherto\nfar from achieving the goal of an active observer relying upon eye guidance to\naccomplish real-world tasks. We argue that this challenging goal not only\nrequires to embody, in a principled way, the problem of eye guidance within the\naction/perception loop, but to face the inextricable link tying up visual\nattention, emotion and executive control, in so far as recent neurobiological\nfindings are weighed up.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:54:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Boccignone", "Giuseppe", ""]]}, {"id": "1607.01327", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo", "title": "Feature Selection Library (MATLAB Toolbox)", "comments": "Feature Selection Library (FSLib) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Selection Library (FSLib) is a widely applicable MATLAB library for\nFeature Selection (FS). FS is an essential component of machine learning and\ndata mining which has been studied for many years under many different\nconditions and in diverse scenarios. These algorithms aim at ranking and\nselecting a subset of relevant features according to their degrees of\nrelevance, preference, or importance as defined in a specific application.\nBecause feature selection can reduce the amount of features used for training\nclassification models, it alleviates the effect of the curse of dimensionality,\nspeeds up the learning process, improves model's performance, and enhances data\nunderstanding. This short report provides an overview of the feature selection\nalgorithms included in the FSLib MATLAB toolbox among filter, embedded, and\nwrappers methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 16:50:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 09:55:41 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 12:07:43 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 15:33:08 GMT"}, {"version": "v5", "created": "Sat, 19 Nov 2016 10:54:13 GMT"}, {"version": "v6", "created": "Mon, 6 Aug 2018 14:34:39 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Roffo", "Giorgio", ""]]}, {"id": "1607.01355", "submitter": "Ehsan Taghavi", "authors": "E. Taghavi, D. Song, R. Tharmarasa, T. Kirubarajan, Anne-Claire\n  Boury-Brisset and Bhashyam Balaji", "title": "Object Recognition and Identification Using ESM Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition and identification of unknown targets is a crucial task in\nsurveillance and security systems. Electronic Support Measures (ESM) are one of\nthe most effective sensors for identification, especially for maritime and\nair--to--ground applications. In typical surveillance systems multiple ESM\nsensors are usually deployed along with kinematic sensors like radar. Different\nESM sensors may produce different types of reports ready to be sent to the\nfusion center. The focus of this paper is to develop a new architecture for\ntarget recognition and identification when non--homogeneous ESM and possibly\nkinematic reports are received at the fusion center. The new fusion\narchitecture is evaluated using simulations to show the benefit of utilizing\ndifferent ESM reports such as attributes and signal level ESM data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 15:03:59 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Taghavi", "E.", ""], ["Song", "D.", ""], ["Tharmarasa", "R.", ""], ["Kirubarajan", "T.", ""], ["Boury-Brisset", "Anne-Claire", ""], ["Balaji", "Bhashyam", ""]]}, {"id": "1607.01437", "submitter": "Luwei Yang", "authors": "Luwei Yang, Ligen Zhu, Yichen Wei, Shuang Liang, Ping Tan", "title": "Attribute Recognition from Adaptive Parts", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous part-based attribute recognition approaches perform part detection\nand attribute recognition in separate steps. The parts are not optimized for\nattribute recognition and therefore could be sub-optimal. We present an\nend-to-end deep learning approach to overcome the limitation. It generates\nobject parts from key points and perform attribute recognition accordingly,\nallowing adaptive spatial transform of the parts. Both key point estimation and\nattribute recognition are learnt jointly in a multi-task setting. Extensive\nexperiments on two datasets verify the efficacy of proposed end-to-end\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 23:29:06 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 21:08:19 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Yang", "Luwei", ""], ["Zhu", "Ligen", ""], ["Wei", "Yichen", ""], ["Liang", "Shuang", ""], ["Tan", "Ping", ""]]}, {"id": "1607.01450", "submitter": "Tal Hassner", "authors": "Tal Hassner, Iacopo Masi, Jungyeon Kim, Jongmoo Choi, Shai Harel, Prem\n  Natarajan and Gerard Medioni", "title": "Pooling Faces: Template based Face Recognition with Pooled Face Images", "comments": "Appeared in the IEEE Computer Society Workshop on Biometrics, IEEE\n  Conf. on Computer Vision and Pattern Recognition (CVPR), June, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to template based face recognition. Our dual goal\nis to both increase recognition accuracy and reduce the computational and\nstorage costs of template matching. To do this, we leverage on an approach\nwhich was proven effective in many other domains, but, to our knowledge, never\nfully explored for face images: average pooling of face photos. We show how\n(and why!) the space of a template's images can be partitioned and then pooled\nbased on image quality and head pose and the effect this has on accuracy and\ntemplate size. We perform extensive tests on the IJB-A and Janus CS2 template\nbased face identification and verification benchmarks. These show that not only\ndoes our approach outperform published state of the art despite requiring far\nfewer cross template comparisons, but also, surprisingly, that image pooling\nperforms on par with deep feature pooling.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 01:05:13 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Hassner", "Tal", ""], ["Masi", "Iacopo", ""], ["Kim", "Jungyeon", ""], ["Choi", "Jongmoo", ""], ["Harel", "Shai", ""], ["Natarajan", "Prem", ""], ["Medioni", "Gerard", ""]]}, {"id": "1607.01577", "submitter": "Le Dong", "authors": "Le Dong, Ling He, Gaipeng Kong, Qianni Zhang, Xiaochun Cao, and Ebroul\n  Izquierdo", "title": "CUNet: A Compact Unsupervised Network for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a compact network called CUNet (compact\nunsupervised network) to counter the image classification challenge. Different\nfrom the traditional convolutional neural networks learning filters by the\ntime-consuming stochastic gradient descent, CUNet learns the filter bank from\ndiverse image patches with the simple K-means, which significantly avoids the\nrequirement of scarce labeled training images, reduces the training\nconsumption, and maintains the high discriminative ability. Besides, we propose\na new pooling method named weighted pooling considering the different weight\nvalues of adjacent neurons, which helps to improve the robustness to small\nimage distortions. In the output layer, CUNet integrates the feature maps\ngained in the last hidden layer, and straightforwardly computes histograms in\nnon-overlapped blocks. To reduce feature redundancy, we implement the\nmax-pooling operation on adjacent blocks to select the most competitive\nfeatures. Comprehensive experiments are conducted to demonstrate the\nstate-of-the-art classification performances with CUNet on CIFAR-10, STL-10,\nMNIST and Caltech101 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 11:56:52 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Dong", "Le", ""], ["He", "Ling", ""], ["Kong", "Gaipeng", ""], ["Zhang", "Qianni", ""], ["Cao", "Xiaochun", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1607.01679", "submitter": "Manuel Blanco Valentin Eng.", "authors": "Manuel Blanco Valentin, Clecio Roque De Bom, Marcio Portes de\n  Albuquerque, Marcelo Portes de Albuquerque, Elisangela Faria, Maury Duarte\n  Correia, Rodrigo Surmas", "title": "On a method for Rock Classification using Textural Features and Genetic\n  Optimization", "comments": "13 pages, 3 figures, 1 appendix. Replaced to match the published\n  version", "journal-ref": "Notas Tecnicas do CBPF, v.7, n.1 (2017)", "doi": "10.7437/NT2236-7640/2017.01.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method to classify a set of rock textures based on\na Spectral Analysis and the extraction of the texture Features of the resulted\nimages. Up to 520 features were tested using 4 different filters and all 31\ndifferent combinations were verified. The classification process relies on a\nNaive Bayes classifier. We performed two kinds of optimizations: statistical\noptimization with covariance-based Principal Component Analysis (PCA) and a\ngenetic optimization, for 10,000 randomly defined samples, achieving a final\nmaximum classification success of 91% against the original 70% success ratio\n(without any optimization nor filters used). After the optimization 9 types of\nfeatures emerged as most relevant.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 15:45:21 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 19:16:31 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Valentin", "Manuel Blanco", ""], ["De Bom", "Clecio Roque", ""], ["de Albuquerque", "Marcio Portes", ""], ["de Albuquerque", "Marcelo Portes", ""], ["Faria", "Elisangela", ""], ["Correia", "Maury Duarte", ""], ["Surmas", "Rodrigo", ""]]}, {"id": "1607.01719", "submitter": "Baochen Sun", "authors": "Baochen Sun, Kate Saenko", "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are able to learn powerful representations from large\nquantities of labeled input data, however they cannot always generalize well\nacross changes in input distributions. Domain adaptation algorithms have been\nproposed to compensate for the degradation in performance due to domain shift.\nIn this paper, we address the case when the target domain is unlabeled,\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\ndomain adaptation method that aligns the second-order statistics of the source\nand target distributions with a linear transformation. Here, we extend CORAL to\nlearn a nonlinear transformation that aligns correlations of layer activations\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\ndatasets show state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Sun", "Baochen", ""], ["Saenko", "Kate", ""]]}, {"id": "1607.01794", "submitter": "Zhenyang Li", "authors": "Zhenyang Li, Efstratios Gavves, Mihir Jain, Cees G. M. Snoek", "title": "VideoLSTM Convolves, Attends and Flows for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new architecture for end-to-end sequence learning of actions in\nvideo, we call VideoLSTM. Rather than adapting the video to the peculiarities\nof established recurrent or convolutional architectures, we adapt the\narchitecture to fit the requirements of the video medium. Starting from the\nsoft-Attention LSTM, VideoLSTM makes three novel contributions. First, video\nhas a spatial layout. To exploit the spatial correlation we hardwire\nconvolutions in the soft-Attention LSTM architecture. Second, motion not only\ninforms us about the action content, but also guides better the attention\ntowards the relevant spatio-temporal locations. We introduce motion-based\nattention. And finally, we demonstrate how the attention from VideoLSTM can be\nused for action localization by relying on just the action class label.\nExperiments and comparisons on challenging datasets for action classification\nand localization support our claims.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 20:00:20 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Li", "Zhenyang", ""], ["Gavves", "Efstratios", ""], ["Jain", "Mihir", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1607.01855", "submitter": "Hao Chen", "authors": "Hao Chen, Yefeng Zheng, Jin-Hyeong Park, Pheng-Ann Heng and S. Kevin\n  Zhou", "title": "Iterative Multi-domain Regularized Deep Learning for Anatomical\n  Structure Detection and Segmentation from Ultrasound Images", "comments": "MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and segmentation of anatomical structures from ultrasound\nimages are crucial for clinical diagnosis and biometric measurements. Although\nultrasound imaging has been widely used with superiorities such as low cost and\nportability, the fuzzy border definition and existence of abounding artifacts\npose great challenges for automatically detecting and segmenting the complex\nanatomical structures. In this paper, we propose a multi-domain regularized\ndeep learning method to address this challenging problem. By leveraging the\ntransfer learning from cross domains, the feature representations are\neffectively enhanced. The results are further improved by the iterative\nrefinement. Moreover, our method is quite efficient by taking advantage of a\nfully convolutional network, which is formulated as an end-to-end learning\nframework of detection and segmentation. Extensive experimental results on a\nlarge-scale database corroborated that our method achieved a superior detection\nand segmentation accuracy, outperforming other methods by a significant margin\nand demonstrating competitive capability even compared to human performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 02:21:25 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Chen", "Hao", ""], ["Zheng", "Yefeng", ""], ["Park", "Jin-Hyeong", ""], ["Heng", "Pheng-Ann", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1607.01895", "submitter": "Xianming Liu", "authors": "Xianming Liu, Gene Cheung, Xiaolin Wu, Debin Zhao", "title": "Random Walk Graph Laplacian based Smoothness Prior for Soft Decoding of\n  JPEG Images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2627807", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the prevalence of JPEG compressed images, optimizing image\nreconstruction from the compressed format remains an important problem. Instead\nof simply reconstructing a pixel block from the centers of indexed DCT\ncoefficient quantization bins (hard decoding), soft decoding reconstructs a\nblock by selecting appropriate coefficient values within the indexed bins with\nthe help of signal priors. The challenge thus lies in how to define suitable\npriors and apply them effectively.\n  In this paper, we combine three image priors---Laplacian prior for DCT\ncoefficients, sparsity prior and graph-signal smoothness prior for image\npatches---to construct an efficient JPEG soft decoding algorithm. Specifically,\nwe first use the Laplacian prior to compute a minimum mean square error (MMSE)\ninitial solution for each code block. Next, we show that while the sparsity\nprior can reduce block artifacts, limiting the size of the over-complete\ndictionary (to lower computation) would lead to poor recovery of high DCT\nfrequencies. To alleviate this problem, we design a new graph-signal smoothness\nprior (desired signal has mainly low graph frequencies) based on the left\neigenvectors of the random walk graph Laplacian matrix (LERaG). Compared to\nprevious graph-signal smoothness priors, LERaG has desirable image filtering\nproperties with low computation overhead. We demonstrate how LERaG can\nfacilitate recovery of high DCT frequencies of a piecewise smooth (PWS) signal\nvia an interpretation of low graph frequency components as relaxed solutions to\nnormalized cut in spectral clustering. Finally, we construct a soft decoding\nalgorithm using the three signal priors with appropriate prior weights.\nExperimental results show that our proposal outperforms state-of-the-art soft\ndecoding algorithms in both objective and subjective evaluations noticeably.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 07:31:22 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Liu", "Xianming", ""], ["Cheung", "Gene", ""], ["Wu", "Xiaolin", ""], ["Zhao", "Debin", ""]]}, {"id": "1607.01971", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI), Rebecca Thomas, Gavin Bhakta (DESW), Andrew\n  Crowder (DESW), David Owens, Peter Boyle (SIGPH@iPRI, IPRI)", "title": "Superimposition of eye fundus images for longitudinal analysis from\n  large public health databases", "comments": "This is an author-created, un-copyedited version of an article\n  published in Biomedical Physics \\& Engineering Express. IOP Publishing Ltd is\n  not responsible for any errors or omissions in this version of the manuscript\n  or any version derived from it. The Version of Record is available online at\n  https://doi.org/10.1088/2057-1976/aa7d16", "journal-ref": "Biomedical Physics \\& Engineering Express, 2017, 3 (4)", "doi": "10.1088/2057-1976/aa7d16", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method is presented for superimposition (i.e. registration)\nof eye fundus images from persons with diabetes screened over many years for\ndiabetic retinopathy. The method is fully automatic and robust to camera\nchanges and colour variations across the images both in space and time. All the\nstages of the process are designed for longitudinal analysis of cohort public\nhealth databases where retinal examinations are made at approximately yearly\nintervals. The method relies on a model correcting two radial distortions and\nan affine transformation between pairs of images which is robustly fitted on\nsalient points. Each stage involves linear estimators followed by non-linear\noptimisation. The model of image warping is also invertible for fast\ncomputation. The method has been validated (1) on a simulated montage and (2)\non public health databases with 69 patients with high quality images (271 pairs\nacquired mostly with different types of camera and 268 pairs acquired mostly\nwith the same type of camera) with success rates of 92% and 98%, and five\npatients (20 pairs) with low quality images with a success rate of 100%.\nCompared to two state-of-the-art methods, ours gives better results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 11:43:24 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 09:44:05 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 13:07:32 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI"], ["Thomas", "Rebecca", "", "DESW"], ["Bhakta", "Gavin", "", "DESW"], ["Crowder", "Andrew", "", "DESW"], ["Owens", "David", "", "SIGPH@iPRI, IPRI"], ["Boyle", "Peter", "", "SIGPH@iPRI, IPRI"]]}, {"id": "1607.01977", "submitter": "Yuchao Dai Dr.", "authors": "Xibin Song, Yuchao Dai, Xueying Qin", "title": "Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep\n  Convolutional Neural Network", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth image super-resolution is an extremely challenging task due to the\ninformation loss in sub-sampling. Deep convolutional neural network have been\nwidely applied to color image super-resolution. Quite surprisingly, this\nsuccess has not been matched to depth super-resolution. This is mainly due to\nthe inherent difference between color and depth images. In this paper, we\nbridge up the gap and extend the success of deep convolutional neural network\nto depth super-resolution. The proposed deep depth super-resolution method\nlearns the mapping from a low-resolution depth image to a high resolution one\nin an end-to-end style. Furthermore, to better regularize the learned depth\nmap, we propose to exploit the depth field statistics and the local correlation\nbetween depth image and color image. These priors are integrated in an energy\nminimization formulation, where the deep neural network learns the unary term,\nthe depth field statistics works as global model constraint and the color-depth\ncorrelation is utilized to enforce the local structure in depth images.\nExtensive experiments on various depth super-resolution benchmark datasets show\nthat our method outperforms the state-of-the-art depth image super-resolution\nmethods with a margin.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:01:59 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Song", "Xibin", ""], ["Dai", "Yuchao", ""], ["Qin", "Xueying", ""]]}, {"id": "1607.01979", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh and Fabio Cuzzolin", "title": "Untrimmed Video Classification for Activity Detection: submission to\n  ActivityNet Challenge", "comments": "3 pages, Presented at ActivityNet Large Scale Activity Recognition\n  Challenge workshop at CVPR 2016, Second position in ActivityNet Detection\n  challenge 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current state-of-the-art human activity recognition is focused on the\nclassification of temporally trimmed videos in which only one action occurs per\nframe. We propose a simple, yet effective, method for the temporal detection of\nactivities in temporally untrimmed videos with the help of untrimmed\nclassification. Firstly, our model predicts the top k labels for each untrimmed\nvideo by analysing global video-level features. Secondly, frame-level binary\nclassification is combined with dynamic programming to generate the temporally\ntrimmed activity proposals. Finally, each proposal is assigned a label based on\nthe global label, and scored with the score of the temporal activity proposal\nand the global score. Ultimately, we show that untrimmed video classification\nmodels can be used as stepping stone for temporal detection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:08:04 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 22:53:46 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1607.02003", "submitter": "Mihir Jain", "authors": "Mihir Jain, Jan van Gemert, Herv\\'e J\\'egou, Patrick Bouthemy, Cees\n  G.M. Snoek", "title": "Tubelets: Unsupervised action proposals from spatiotemporal super-voxels", "comments": "submitted to International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of localizing actions in videos as a\nsequences of bounding boxes. The objective is to generate action proposals that\nare likely to include the action of interest, ideally achieving high recall\nwith few proposals. Our contributions are threefold. First, inspired by\nselective search for object proposals, we introduce an approach to generate\naction proposals from spatiotemporal super-voxels in an unsupervised manner, we\ncall them Tubelets. Second, along with the static features from individual\nframes our approach advantageously exploits motion. We introduce independent\nmotion evidence as a feature to characterize how the action deviates from the\nbackground and explicitly incorporate such motion information in various stages\nof the proposal generation. Finally, we introduce spatiotemporal refinement of\nTubelets, for more precise localization of actions, and pruning to keep the\nnumber of Tubelets limited. We demonstrate the suitability of our approach by\nextensive experiments for action proposal quality and action localization on\nthree public datasets: UCF Sports, MSR-II and UCF101. For action proposal\nquality, our unsupervised proposals beat all other existing approaches on the\nthree datasets. For action localization, we show top performance on both the\ntrimmed videos of UCF Sports and UCF101 as well as the untrimmed videos of\nMSR-II.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 13:30:17 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Jain", "Mihir", ""], ["van Gemert", "Jan", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Bouthemy", "Patrick", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1607.02046", "submitter": "Gr\\'egory Rogez", "authors": "Gr\\'egory Rogez and Cordelia Schmid", "title": "MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild", "comments": "9 pages, accepted to appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D human pose estimation in the wild. A\nsignificant challenge is the lack of training data, i.e., 2D images of humans\nannotated with 3D poses. Such data is necessary to train state-of-the-art CNN\narchitectures. Here, we propose a solution to generate a large set of\nphotorealistic synthetic images of humans with 3D pose annotations. We\nintroduce an image-based synthesis engine that artificially augments a dataset\nof real images with 2D human pose annotations using 3D Motion Capture (MoCap)\ndata. Given a candidate 3D pose our algorithm selects for each joint an image\nwhose 2D pose locally matches the projected 3D pose. The selected images are\nthen combined to generate a new synthetic image by stitching local image\npatches in a kinematically constrained manner. The resulting images are used to\ntrain an end-to-end CNN for full-body 3D pose estimation. We cluster the\ntraining data into a large number of pose classes and tackle pose estimation as\na K-way classification problem. Such an approach is viable only with large\ntraining sets such as ours. Our method outperforms the state of the art in\nterms of 3D pose estimation in controlled environments (Human3.6M) and shows\npromising results for in-the-wild images (LSP). This demonstrates that CNNs\ntrained on artificial images generalize well to real images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 15:30:05 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 12:43:51 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Rogez", "Gr\u00e9gory", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1607.02104", "submitter": "Qian Wang", "authors": "Qian Wang, Ke Chen", "title": "Zero-Shot Visual Recognition via Bidirectional Latent Embedding", "comments": "Technical report, School of Computer Science, The University of\n  Manchester. Accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning for visual recognition, e.g., object and action\nrecognition, has recently attracted a lot of attention. However, it still\nremains challenging in bridging the semantic gap between visual features and\ntheir underlying semantics and transferring knowledge to semantic categories\nunseen during learning. Unlike most of the existing zero-shot visual\nrecognition methods, we propose a stagewise bidirectional latent embedding\nframework to two subsequent learning stages for zero-shot visual recognition.\nIn the bottom-up stage, a latent embedding space is first created by exploring\nthe topological and labeling information underlying training data of known\nclasses via a proper supervised subspace learning algorithm and the latent\nembedding of training data are used to form landmarks that guide embedding\nsemantics underlying unseen classes into this learned latent space. In the\ntop-down stage, semantic representations of unseen-class labels in a given\nlabel vocabulary are then embedded to the same latent space to preserve the\nsemantic relatedness between all different classes via our proposed\nsemi-supervised Sammon mapping with the guidance of landmarks. Thus, the\nresultant latent embedding space allows for predicting the label of a test\ninstance with a simple nearest-neighbor rule. To evaluate the effectiveness of\nthe proposed framework, we have conducted extensive experiments on four\nbenchmark datasets in object and action recognition, i.e., AwA, CUB-200-2011,\nUCF101 and HMDB51. The experimental results under comparative studies\ndemonstrate that our proposed approach yields the state-of-the-art performance\nunder inductive and transductive settings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 17:48:21 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 12:00:07 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 09:39:36 GMT"}, {"version": "v4", "created": "Fri, 2 Jun 2017 17:18:27 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Wang", "Qian", ""], ["Chen", "Ke", ""]]}, {"id": "1607.02196", "submitter": "Sofya Chepushtanova", "authors": "Sofya Chepushtanova, Michael Kirby, Chris Peterson, Lori Ziegelmeier", "title": "Persistent Homology on Grassmann Manifolds for Analysis of Hyperspectral\n  Movies", "comments": "version 2: typos correction", "journal-ref": "Computational Topology in Image Context, Volume 9667 of the series\n  Lecture Notes in Computer Science, pp. 228-239, June 2016", "doi": "10.1007/978-3-319-39441-1_21", "report-no": null, "categories": "cs.CV cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of characteristic structure, or shape, in complex data sets has\nbeen recognized as increasingly important for mathematical data analysis. This\nrealization has motivated the development of new tools such as persistent\nhomology for exploring topological invariants, or features, in large data sets.\nIn this paper we apply persistent homology to the characterization of gas\nplumes in time dependent sequences of hyperspectral cubes, i.e. the analysis of\n4-way arrays. We investigate hyperspectral movies of Long-Wavelength Infrared\ndata monitoring an experimental release of chemical simulant into the air. Our\napproach models regions of interest within the hyperspectral data cubes as\npoints on the real Grassmann manifold $G(k, n)$ (whose points parameterize the\n$k$-dimensional subspaces of $\\mathbb{R}^n$), contrasting our approach with the\nmore standard framework in Euclidean space. An advantage of this approach is\nthat it allows a sequence of time slices in a hyperspectral movie to be\ncollapsed to a sequence of points in such a way that some of the key structure\nwithin and between the slices is encoded by the points on the Grassmann\nmanifold. This motivates the search for topological features, associated with\nthe evolution of the frames of a hyperspectral movie, within the corresponding\npoints on the Grassmann manifold. The proposed mathematical model affords the\nprocessing of large data sets while retaining valuable discriminatory\ninformation. In this paper, we discuss how embedding our data in the Grassmann\nmanifold, together with topological data analysis, captures dynamical events\nthat occur as the chemical plume is released and evolves.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 23:39:29 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 13:53:15 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Chepushtanova", "Sofya", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""], ["Ziegelmeier", "Lori", ""]]}, {"id": "1607.02204", "submitter": "Giuseppe Lisanti", "authors": "Giuseppe Lisanti, Svebor Karaman, Iacopo Masi", "title": "Multi Channel-Kernel Canonical Correlation Analysis for Cross-View\n  Person Re-Identification", "comments": "The latest/updated version of the manuscript with more experiments\n  can be found at https://doi.org/10.1145/3038916. Please cite the paper using\n  https://doi.org/10.1145/3038916", "journal-ref": "ACM Transactions on Multimedia Computing, Communications, and\n  Applications (TOMM), Volume 13 Issue 2, March 2017", "doi": "10.1145/3038916", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method to overcome one of the main challenges of\nperson re-identification in multi-camera networks, namely cross-view appearance\nchanges. The proposed solution addresses the extreme variability of person\nappearance in different camera views by exploiting multiple feature\nrepresentations. For each feature, Kernel Canonical Correlation Analysis (KCCA)\nwith different kernels is exploited to learn several projection spaces in which\nthe appearance correlation between samples of the same person observed from\ndifferent cameras is maximized. An iterative logistic regression is finally\nused to select and weigh the contributions of each feature projections and\nperform the matching between the two views. Experimental evaluation shows that\nthe proposed solution obtains comparable performance on VIPeR and PRID 450s\ndatasets and improves on PRID and CUHK01 datasets with respect to the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 00:40:38 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 10:14:10 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Lisanti", "Giuseppe", ""], ["Karaman", "Svebor", ""], ["Masi", "Iacopo", ""]]}, {"id": "1607.02235", "submitter": "EPTCS", "authors": "Gina Belmonte (Azienda Ospedaliera Universitaria Senese), Vincenzo\n  Ciancia (Istituto di Scienza e Tecnologie dell'Informazione \"A. Faedo\",\n  Consiglio Nazionale delle Ricerche), Diego Latella (Istituto di Scienza e\n  Tecnologie dell'Informazione \"A. Faedo\", Consiglio Nazionale delle Ricerche),\n  Mieke Massink (Istituto di Scienza e Tecnologie dell'Informazione \"A. Faedo\",\n  Consiglio Nazionale delle Ricerche)", "title": "From Collective Adaptive Systems to Human Centric Computation and Back:\n  Spatial Model Checking for Medical Imaging", "comments": "In Proceedings FORECAST 2016, arXiv:1607.02001", "journal-ref": "EPTCS 217, 2016, pp. 81-92", "doi": "10.4204/EPTCS.217.10", "report-no": null, "categories": "cs.LO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on formal verification for Collective Adaptive Systems (CAS)\npushed advancements in spatial and spatio-temporal model checking, and as a\nside result provided novel image analysis methodologies, rooted in logical\nmethods for topological spaces. Medical Imaging (MI) is a field where such\ntechnologies show potential for ground-breaking innovation. In this position\npaper, we present a preliminary investigation centred on applications of\nspatial model checking to MI. The focus is shifted from pure logics to a\nmixture of logical, statistical and algorithmic approaches, driven by the\nlogical nature intrinsic to the specification of the properties of interest in\nthe field. As a result, novel operators are introduced, that could as well be\nbrought back to the setting of CAS.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:37:08 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Belmonte", "Gina", "", "Azienda Ospedaliera Universitaria Senese"], ["Ciancia", "Vincenzo", "", "Istituto di Scienza e Tecnologie dell'Informazione \"A. Faedo\",\n  Consiglio Nazionale delle Ricerche"], ["Latella", "Diego", "", "Istituto di Scienza e\n  Tecnologie dell'Informazione \"A. Faedo\", Consiglio Nazionale delle Ricerche"], ["Massink", "Mieke", "", "Istituto di Scienza e Tecnologie dell'Informazione \"A. Faedo\",\n  Consiglio Nazionale delle Ricerche"]]}, {"id": "1607.02241", "submitter": "Darryl Lin", "authors": "Darryl D. Lin and Sachin S. Talathi", "title": "Overcoming Challenges in Fixed Point Training of Deep Convolutional\n  Networks", "comments": "ICML2016 - Workshop on On-Device Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that training deep neural networks, in particular, deep\nconvolutional networks, with aggressively reduced numerical precision is\nchallenging. The stochastic gradient descent algorithm becomes unstable in the\npresence of noisy gradient updates resulting from arithmetic with limited\nnumeric precision. One of the well-accepted solutions facilitating the training\nof low precision fixed point networks is stochastic rounding. However, to the\nbest of our knowledge, the source of the instability in training neural\nnetworks with noisy gradient updates has not been well investigated. This work\nis an attempt to draw a theoretical connection between low numerical precision\nand training algorithm stability. In doing so, we will also propose and verify\nthrough experiments methods that are able to improve the training performance\nof deep convolutional networks in fixed point.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:07:03 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Lin", "Darryl D.", ""], ["Talathi", "Sachin S.", ""]]}, {"id": "1607.02257", "submitter": "Rigas Kouskouridas", "authors": "Andreas Doumanoglou, Vassileios Balntas, Rigas Kouskouridas, Tae-Kyun\n  Kim", "title": "Siamese Regression Networks with Efficient mid-level Feature Extraction\n  for 3D Object Pose Estimation", "comments": "9 pages, paper submitted to NIPS 2016, project page:\n  http://www.iis.ee.ic.ac.uk/rkouskou/research/SRN.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of estimating the 3D pose of object\ninstances, using convolutional neural networks. State of the art methods\nusually solve the challenging problem of regression in angle space indirectly,\nfocusing on learning discriminative features that are later fed into a separate\narchitecture for 3D pose estimation. In contrast, we propose an end-to-end\nlearning framework for directly regressing object poses by exploiting Siamese\nNetworks. For a given image pair, we enforce a similarity measure between the\nrepresentation of the sample images in the feature and pose space respectively,\nthat is shown to boost regression performance. Furthermore, we argue that our\npose-guided feature learning using our Siamese Regression Network generates\nmore discriminative features that outperform the state of the art. Last, our\nfeature learning formulation provides the ability of learning features that can\nperform under severe occlusions, demonstrating high performance on our novel\nhand-object dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 07:25:47 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Doumanoglou", "Andreas", ""], ["Balntas", "Vassileios", ""], ["Kouskouridas", "Rigas", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1607.02290", "submitter": "Pedro Miraldo", "authors": "Andre Mateus, Pedro Miraldo, and Pedro U. Lima", "title": "Non-Central Catadioptric Cameras Pose Estimation using 3D Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we purpose a novel method for planar pose estimation of\nmobile robots. This method is based on an analytic solution (which we derived)\nfor the projection of 3D straight lines, onto the mirror of Non-Central\nCatadioptric Cameras (NCCS). The resulting solution is rewritten as a function\nof the rotation and translation parameters, which is then used as an error\nfunction for a set of mirror points. Those should be the result of the\nprojection of a set of points incident with the respective 3D lines. The\ncamera's pose is given by minimizing the error function, with the associated\nconstraints. The method is validated by experiments both with synthetic and\nreal data. The latter was collected from a mobile robot equipped with a NCCS.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 09:50:36 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Mateus", "Andre", ""], ["Miraldo", "Pedro", ""], ["Lima", "Pedro U.", ""]]}, {"id": "1607.02303", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label\n  Tree Embeddings for Audio Scene Recognition", "comments": "Task1 technical report for the DCASE2016 challenge. arXiv admin note:\n  text overlap with arXiv:1606.07908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this report our audio scene recognition system submitted to\nthe DCASE 2016 challenge. Firstly, given the label set of the scenes, a label\ntree is automatically constructed. This category taxonomy is then used in the\nfeature extraction step in which an audio scene instance is represented by a\nlabel tree embedding image. Different convolutional neural networks, which are\ntailored for the task at hand, are finally learned on top of the image features\nfor scene recognition. Our system reaches an overall recognition accuracy of\n81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute\nimprovements of 8.7% and 6.1% on the development and test data, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:39:05 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:05:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02397", "submitter": "Yu Zhong", "authors": "Yu Zhong and Gil Ettinger", "title": "Enlightening Deep Neural Networks with Knowledge of Confounding Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have demonstrated significant capacity in modeling\nsome of the most challenging real world problems of high complexity. Despite\nthe popularity of deep models, we still strive to better understand the\nunderlying mechanism that drives their success. Motivated by observations that\nneurons in trained deep nets predict attributes indirectly related to the\ntraining tasks, we recognize that a deep network learns representations more\ngeneral than the task at hand to disentangle impacts of multiple confounding\nfactors governing the data, in order to isolate the effects of the concerning\nfactors and optimize a given objective. Consequently, we propose a general\nframework to augment training of deep models with information on auxiliary\nexplanatory data variables, in an effort to boost this disentanglement and\ntrain deep networks that comprehend the data interactions and distributions\nmore accurately, and thus improve their generalizability. We incorporate\ninformation on prominent auxiliary explanatory factors of the data population\ninto existing architectures as secondary objective/loss blocks that take inputs\nfrom hidden layers during training. Once trained, these secondary circuits can\nbe removed to leave a model with the same architecture as the original, but\nmore generalizable and discerning thanks to its comprehension of data\ninteractions. Since pose is one of the most dominant confounding factors for\nobject recognition, we apply this principle to instantiate a pose-aware deep\nconvolutional neural network and demonstrate that auxiliary pose information\nindeed improves the classification accuracy in our experiments on SAR target\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 15:00:11 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Zhong", "Yu", ""], ["Ettinger", "Gil", ""]]}, {"id": "1607.02504", "submitter": "Xiao Yang", "authors": "Xiao Yang, Roland Kwitt, Marc Niethammer", "title": "Fast Predictive Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to predict image deformations based on patch-wise image\nappearance. Specifically, we design a patch-based deep encoder-decoder network\nwhich learns the pixel/voxel-wise mapping between image appearance and\nregistration parameters. Our approach can predict general deformation\nparameterizations, however, we focus on the large deformation diffeomorphic\nmetric mapping (LDDMM) registration model. By predicting the LDDMM\nmomentum-parameterization we retain the desirable theoretical properties of\nLDDMM, while reducing computation time by orders of magnitude: combined with\npatch pruning, we achieve a 1500x/66x speed up compared to GPU-based\noptimization for 2D/3D image registration. Our approach has better prediction\naccuracy than predicting deformation or velocity fields and results in\ndiffeomorphic transformations. Additionally, we create a Bayesian probabilistic\nversion of our network, which allows evaluation of deformation field\nuncertainty through Monte Carlo sampling using dropout at test time. We show\nthat deformation uncertainty highlights areas of ambiguous deformations. We\ntest our method on the OASIS brain image dataset in 2D and 3D.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 19:58:56 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Yang", "Xiao", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""]]}, {"id": "1607.02533", "submitter": "Alexey Kurakin", "authors": "Alexey Kurakin, Ian Goodfellow and Samy Bengio", "title": "Adversarial examples in the physical world", "comments": "14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:12:11 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 22:57:31 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 20:34:37 GMT"}, {"version": "v4", "created": "Sat, 11 Feb 2017 00:39:39 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kurakin", "Alexey", ""], ["Goodfellow", "Ian", ""], ["Bengio", "Samy", ""]]}, {"id": "1607.02537", "submitter": "Heng Fan", "authors": "Heng Fan, Xue Mei, Danil Prokhorov and Haibin Ling", "title": "Multi-level Contextual RNNs with Attention Model for Scene Labeling", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context in image is crucial for scene labeling while existing methods only\nexploit local context generated from a small surrounding area of an image patch\nor a pixel, by contrast long-range and global contextual information is\nignored. To handle this issue, we in this work propose a novel approach for\nscene labeling by exploring multi-level contextual recurrent neural networks\n(ML-CRNNs). Specifically, we encode three kinds of contextual cues, i.e., local\ncontext, global context and image topic context in structural recurrent neural\nnetworks (RNNs) to model long-range local and global dependencies in image. In\nthis way, our method is able to `see' the image in terms of both long-range\nlocal and holistic views, and make a more reliable inference for image\nlabeling. Besides, we integrate the proposed contextual RNNs into hierarchical\nconvolutional neural networks (CNNs), and exploit dependence relationships in\nmultiple levels to provide rich spatial and semantic information. Moreover, we\nnovelly adopt an attention model to effectively merge multiple levels and show\nthat it outperforms average- or max-pooling fusion strategies. Extensive\nexperiments demonstrate that the proposed approach achieves new\nstate-of-the-art results on the CamVid, SiftFlow and Stanford-background\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:51:53 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 21:15:51 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Fan", "Heng", ""], ["Mei", "Xue", ""], ["Prokhorov", "Danil", ""], ["Ling", "Haibin", ""]]}, {"id": "1607.02539", "submitter": "Liansheng Zhuang", "authors": "Liansheng Zhuang, Zihan Zhou, Jingwen Yin, Shenghua Gao, Zhouchen Lin,\n  Yi Ma, Nenghai Yu", "title": "Graph Construction with Label Information for Semi-Supervised Learning", "comments": "This paper is withdrawn by the authors for some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, most existing graph-based semi-supervised learning (SSL)\nmethods only use the label information of observed samples in the label\npropagation stage, while ignoring such valuable information when learning the\ngraph. In this paper, we argue that it is beneficial to consider the label\ninformation in the graph learning stage. Specifically, by enforcing the weight\nof edges between labeled samples of different classes to be zero, we explicitly\nincorporate the label information into the state-of-the-art graph learning\nmethods, such as the Low-Rank Representation (LRR), and propose a novel\nsemi-supervised graph learning method called Semi-Supervised Low-Rank\nRepresentation (SSLRR). This results in a convex optimization problem with\nlinear constraints, which can be solved by the linearized alternating direction\nmethod. Though we take LRR as an example, our proposed method is in fact very\ngeneral and can be applied to any self-representation graph learning methods.\nExperiment results on both synthetic and real datasets demonstrate that the\nproposed graph learning method can better capture the global geometric\nstructure of the data, and therefore is more effective for semi-supervised\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 22:24:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 01:17:57 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 22:20:26 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Zhuang", "Liansheng", ""], ["Zhou", "Zihan", ""], ["Yin", "Jingwen", ""], ["Gao", "Shenghua", ""], ["Lin", "Zhouchen", ""], ["Ma", "Yi", ""], ["Yu", "Nenghai", ""]]}, {"id": "1607.02547", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and Yao Wang", "title": "Screen Content Image Segmentation Using Robust Regression and Sparse\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers how to separate text and/or graphics from smooth\nbackground in screen content and mixed document images and proposes two\napproaches to perform this segmentation task. The proposed methods make use of\nthe fact that the background in each block is usually smoothly varying and can\nbe modeled well by a linear combination of a few smoothly varying basis\nfunctions, while the foreground text and graphics create sharp discontinuity.\nThe algorithms separate the background and foreground pixels by trying to fit\nbackground pixel values in the block into a smooth function using two different\nschemes. One is based on robust regression, where the inlier pixels will be\nconsidered as background, while remaining outlier pixels will be considered\nforeground. The second approach uses a sparse decomposition framework where the\nbackground and foreground layers are modeled with a smooth and sparse\ncomponents respectively. These algorithms have been tested on images extracted\nfrom HEVC standard test sequences for screen content coding, and are shown to\nhave superior performance over previous approaches. The proposed methods can be\nused in different applications such as text extraction, separate coding of\nbackground and foreground for compression of screen content, and medical image\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 23:16:45 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1607.02555", "submitter": "Jakob Engel", "authors": "Jakob Engel and Vladyslav Usenko and Daniel Cremers", "title": "A Photometrically Calibrated Benchmark For Monocular Visual Odometry", "comments": "* Corrected a bug in the evaluation setup, which caused the real-time\n  results for ORB-SLAM (dashed lines in Figure 8) to be much worse than they\n  should be. * https://vision.in.tum.de/data/datasets/mono-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataset for evaluating the tracking accuracy of monocular visual\nodometry and SLAM methods. It contains 50 real-world sequences comprising more\nthan 100 minutes of video, recorded across dozens of different environments --\nranging from narrow indoor corridors to wide outdoor scenes. All sequences\ncontain mostly exploring camera motion, starting and ending at the same\nposition. This allows to evaluate tracking accuracy via the accumulated drift\nfrom start to end, without requiring ground truth for the full sequence. In\ncontrast to existing datasets, all sequences are photometrically calibrated. We\nprovide exposure times for each frame as reported by the sensor, the camera\nresponse function, and dense lens attenuation factors. We also propose a novel,\nsimple approach to non-parametric vignette calibration, which requires minimal\nset-up and is easy to reproduce. Finally, we thoroughly evaluate two existing\nmethods (ORB-SLAM and DSO) on the dataset, including an analysis of the effect\nof image resolution, camera field of view, and the camera motion direction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 00:11:14 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 20:06:10 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Engel", "Jakob", ""], ["Usenko", "Vladyslav", ""], ["Cremers", "Daniel", ""]]}, {"id": "1607.02556", "submitter": "Jialin Wu", "authors": "Jialin Wu, Gu Wang, Wukui Yang, Xiangyang Ji", "title": "Action Recognition with Joint Attention on Multi-Level Deep Features", "comments": "13 pages, submitted to BMVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep supervised neural network for the task of action\nrecognition in videos, which implicitly takes advantage of visual tracking and\nshares the robustness of both deep Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN). In our method, a multi-branch model is proposed\nto suppress noise from background jitters. Specifically, we firstly extract\nmulti-level deep features from deep CNNs and feed them into 3d-convolutional\nnetwork. After that we feed those feature cubes into our novel joint LSTM\nmodule to predict labels and to generate attention regularization. We evaluate\nour model on two challenging datasets: UCF101 and HMDB51. The results show that\nour model achieves the state-of-art by only using convolutional features.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 01:25:24 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Wu", "Jialin", ""], ["Wang", "Gu", ""], ["Yang", "Wukui", ""], ["Ji", "Xiangyang", ""]]}, {"id": "1607.02565", "submitter": "Jakob Engel", "authors": "Jakob Engel, Vladlen Koltun, Daniel Cremers", "title": "Direct Sparse Odometry", "comments": "** Corrected a bug which caused the real-time results for ORB-SLAM\n  (dashed lines in Fig. 10 and 12) to be much worse than they should be **\n  Added references [12], [13],[19], and Fig. 11. ** Partly re-formulated and\n  extended [5. Conclusion]. ** Fixed typos and minor re-formulations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel direct sparse visual odometry formulation. It combines a\nfully direct probabilistic model (minimizing a photometric error) with\nconsistent, joint optimization of all model parameters, including geometry --\nrepresented as inverse depth in a reference frame -- and camera motion. This is\nachieved in real time by omitting the smoothness prior used in other direct\nmethods and instead sampling pixels evenly throughout the images. Since our\nmethod does not depend on keypoint detectors or descriptors, it can naturally\nsample pixels from across all image regions that have intensity gradient,\nincluding edges or smooth intensity variations on mostly white walls. The\nproposed model integrates a full photometric calibration, accounting for\nexposure time, lens vignetting, and non-linear response functions. We\nthoroughly evaluate our method on three different datasets comprising several\nhours of video. The experiments show that the presented approach significantly\noutperforms state-of-the-art direct and indirect methods in a variety of\nreal-world settings, both in terms of tracking accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 04:02:31 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 04:31:21 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Engel", "Jakob", ""], ["Koltun", "Vladlen", ""], ["Cremers", "Daniel", ""]]}, {"id": "1607.02568", "submitter": "Mengyao Zhai", "authors": "Mengyao Zhai, Mehrsan Javan Roshtkhari, Greg Mori", "title": "Deep Learning of Appearance Models for Online Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel deep learning based approach for vision based\nsingle target tracking. We address this problem by proposing a network\narchitecture which takes the input video frames and directly computes the\ntracking score for any candidate target location by estimating the probability\ndistributions of the positive and negative examples. This is achieved by\ncombining a deep convolutional neural network with a Bayesian loss layer in a\nunified framework. In order to deal with the limited number of positive\ntraining examples, the network is pre-trained offline for a generic image\nfeature representation and then is fine-tuned in multiple steps. An online\nfine-tuning step is carried out at every frame to learn the appearance of the\ntarget. We adopt a two-stage iterative algorithm to adaptively update the\nnetwork parameters and maintain a probability density for target/non-target\nregions. The tracker has been tested on the standard tracking benchmark and the\nresults indicate that the proposed solution achieves state-of-the-art tracking\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 06:15:20 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Zhai", "Mengyao", ""], ["Roshtkhari", "Mehrsan Javan", ""], ["Mori", "Greg", ""]]}, {"id": "1607.02586", "submitter": "Jiajun Wu", "authors": "Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman", "title": "Visual Dynamics: Probabilistic Future Frame Synthesis via Cross\n  Convolutional Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of synthesizing a number of likely future frames from a\nsingle input image. In contrast to traditional methods, which have tackled this\nproblem in a deterministic or non-parametric way, we propose a novel approach\nthat models future frames in a probabilistic manner. Our probabilistic model\nmakes it possible for us to sample and synthesize many possible future frames\nfrom a single input image. Future frame synthesis is challenging, as it\ninvolves low- and high-level image and motion understanding. We propose a novel\nnetwork structure, namely a Cross Convolutional Network to aid in synthesizing\nfuture frames; this network structure encodes image and motion information as\nfeature maps and convolutional kernels, respectively. In experiments, our model\nperforms well on synthetic data, such as 2D shapes and animated game sprites,\nas well as on real-wold videos. We also show that our model can be applied to\ntasks such as visual analogy-making, and present an analysis of the learned\nnetwork representations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 08:41:40 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Xue", "Tianfan", ""], ["Wu", "Jiajun", ""], ["Bouman", "Katherine L.", ""], ["Freeman", "William T.", ""]]}, {"id": "1607.02643", "submitter": "Mostafa Ibrahim Mostafa Ibrahim", "authors": "Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat,\n  Greg Mori", "title": "Hierarchical Deep Temporal Models for Group Activity Recognition", "comments": "arXiv admin note: text overlap with arXiv:1511.06040", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach for classifying the activity performed\nby a group of people in a video sequence. This problem of group activity\nrecognition can be addressed by examining individual person actions and their\nrelations. Temporal dynamics exist both at the level of individual person\nactions as well as at the level of group activity. Given a video sequence as\ninput, methods can be developed to capture these dynamics at both person-level\nand group-level detail. We build a deep model to capture these dynamics based\non LSTM (long short-term memory) models. In order to model both person-level\nand group-level dynamics, we present a 2-stage deep temporal model for the\ngroup activity recognition problem. In our approach, one LSTM model is designed\nto represent action dynamics of individual people in a video sequence and\nanother LSTM model is designed to aggregate person-level information for group\nactivity recognition. We collected a new dataset consisting of volleyball\nvideos labeled with individual and group activities in order to evaluate our\nmethod. Experimental results on this new Volleyball Dataset and the standard\nbenchmark Collective Activity Dataset demonstrate the efficacy of the proposed\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 18:23:36 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Ibrahim", "Mostafa S.", ""], ["Muralidharan", "Srikanth", ""], ["Deng", "Zhiwei", ""], ["Vahdat", "Arash", ""], ["Mori", "Greg", ""]]}, {"id": "1607.02652", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "Multimodal Affect Recognition using Kinect", "comments": "9 pages, 2 tables, 1 figure, Peer reviewed in ACM TIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect (emotion) recognition has gained significant attention from\nresearchers in the past decade. Emotion-aware computer systems and devices have\nmany applications ranging from interactive robots, intelligent online tutor to\nemotion based navigation assistant. In this research data from multiple\nmodalities such as face, head, hand, body and speech was utilized for affect\nrecognition. The research used color and depth sensing device such as Kinect\nfor facial feature extraction and tracking human body joints. Temporal features\nacross multiple frames were used for affect recognition. Event driven decision\nlevel fusion was used to combine the results from each individual modality\nusing majority voting to recognize the emotions. The study also implemented\naffect recognition by matching the features to the rule based emotion templates\nper modality. Experiments showed that multimodal affect recognition rates using\ncombination of emotion templates and supervised learning were better compared\nto recognition rates based on supervised learning alone. Recognition rates\nobtained using temporal feature were higher compared to recognition rates\nobtained using position based features only.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:01:33 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.02654", "submitter": "Yanwei Cui", "authors": "Yanwei Cui, S\\'ebastien Lefevre, Laetitia Chapel, Anne Puissant", "title": "Combining multiple resolutions into hierarchical representations for\n  kernel-based image classification", "comments": "International Conference on Geographic Object-Based Image Analysis\n  (GEOBIA 2016), University of Twente in Enschede, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographic object-based image analysis (GEOBIA) framework has gained\nincreasing interest recently. Following this popular paradigm, we propose a\nnovel multiscale classification approach operating on a hierarchical image\nrepresentation built from two images at different resolutions. They capture the\nsame scene with different sensors and are naturally fused together through the\nhierarchical representation, where coarser levels are built from a Low Spatial\nResolution (LSR) or Medium Spatial Resolution (MSR) image while finer levels\nare generated from a High Spatial Resolution (HSR) or Very High Spatial\nResolution (VHSR) image. Such a representation allows one to benefit from the\ncontext information thanks to the coarser levels, and subregions spatial\narrangement information thanks to the finer levels. Two dedicated structured\nkernels are then used to perform machine learning directly on the constructed\nhierarchical representation. This strategy overcomes the limits of conventional\nGEOBIA classification procedures that can handle only one or very few\npre-selected scales. Experiments run on an urban classification task show that\nthe proposed approach can highly improve the classification accuracy w.r.t.\nconventional approaches working on a single scale.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:07:37 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 08:34:49 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Cui", "Yanwei", ""], ["Lefevre", "S\u00e9bastien", ""], ["Chapel", "Laetitia", ""], ["Puissant", "Anne", ""]]}, {"id": "1607.02660", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "Augmenting Supervised Emotion Recognition with Rule-Based Decision Model", "comments": "8 pages, 6 figures, 23 tables, IEEE TAC (in review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this research is development of rule based decision model for\nemotion recognition. This research also proposes using the rules for augmenting\ninter-corporal recognition accuracy in multimodal systems that use supervised\nlearning techniques. The classifiers for such learning based recognition\nsystems are susceptible to over fitting and only perform well on intra-corporal\ndata. To overcome the limitation this research proposes using rule based model\nas an additional modality. The rules were developed using raw feature data from\nvisual channel, based on human annotator agreement and existing studies that\nhave attributed movement and postures to emotions. The outcome of the rule\nevaluations was combined during the decision phase of emotion recognition\nsystem. The results indicate rule based emotion recognition augment recognition\naccuracy of learning based systems and also provide better recognition rate\nacross inter corpus emotion test data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:34:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.02678", "submitter": "Wei Li", "authors": "Wei Li, Farnaz Abtahi, Christina Tsangouri, Zhigang Zhu", "title": "Towards an \"In-the-Wild\" Emotion Dataset Using a Game-based Framework", "comments": "This paper is accepted at CVPR 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to create an \"in-the-wild\" dataset of facial emotions with large\nnumber of balanced samples, this paper proposes a game-based data collection\nframework. The framework mainly include three components: a game engine, a game\ninterface, and a data collection and evaluation module. We use a deep learning\napproach to build an emotion classifier as the game engine. Then a emotion web\ngame to allow gamers to enjoy the games, while the data collection module\nobtains automatically-labelled emotion images. Using our game, we have\ncollected more than 15,000 images within a month of the test run and built an\nemotion dataset \"GaMo\". To evaluate the dataset, we compared the performance of\ntwo deep learning models trained on both GaMo and CIFE. The results of our\nexperiments show that because of being large and balanced, GaMo can be used to\nbuild a more robust emotion detector than the emotion detector trained on CIFE,\nwhich was used in the game engine to collect the face images.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 02:16:10 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Li", "Wei", ""], ["Abtahi", "Farnaz", ""], ["Tsangouri", "Christina", ""], ["Zhu", "Zhigang", ""]]}, {"id": "1607.02715", "submitter": "Bingwen Jin", "authors": "Bingwen Jin, Songhua Xu, and Weidong Geng", "title": "Learning to Sketch Human Facial Portraits using Personal Styles by\n  Case-Based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper employs case-based reasoning (CBR) to capture the personal styles\nof individual artists and generate the human facial portraits from photos\naccordingly. For each human artist to be mimicked, a series of cases are\nfirstly built-up from her/his exemplars of source facial photo and hand-drawn\nsketch, and then its stylization for facial photo is transformed as a\nstyle-transferring process of iterative refinement by looking-for and applying\nbest-fit cases in a sense of style optimization. Two models, fitness evaluation\nmodel and parameter estimation model, are learned for case retrieval and\nadaptation respectively from these cases. The fitness evaluation model is to\ndecide which case is best-fitted to the sketching of current interest, and the\nparameter estimation model is to automate case adaptation. The resultant sketch\nis synthesized progressively with an iterative loop of retrieval and adaptation\nof candidate cases until the desired aesthetic style is achieved. To explore\nthe effectiveness and advantages of the novel approach, we experimentally\ncompare the sketch portraits generated by the proposed method with that of a\nstate-of-the-art example-based facial sketch generation algorithm as well as a\ncouple commercial software packages. The comparisons reveal that our CBR based\nsynthesis method for facial portraits is superior both in capturing and\nreproducing artists' personal illustration styles to the peer methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 08:48:09 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 15:40:04 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Jin", "Bingwen", ""], ["Xu", "Songhua", ""], ["Geng", "Weidong", ""]]}, {"id": "1607.02720", "submitter": "Jun Lin", "authors": "Fangxuan Sun, Jun Lin and Zhongfeng Wang", "title": "Intra-layer Nonuniform Quantization for Deep Convolutional Neural\n  Network", "comments": "submitted to WCSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (DCNN) has achieved remarkable performance\non object detection and speech recognition in recent years. However, the\nexcellent performance of a DCNN incurs high computational complexity and large\nmemory requirement. In this paper, an equal distance nonuniform quantization\n(ENQ) scheme and a K-means clustering nonuniform quantization (KNQ) scheme are\nproposed to reduce the required memory storage when low complexity hardware or\nsoftware implementations are considered. For the VGG-16 and the AlexNet, the\nproposed nonuniform quantization schemes reduce the number of required memory\nstorage by approximately 50\\% while achieving almost the same or even better\nclassification accuracy compared to the state-of-the-art quantization method.\nCompared to the ENQ scheme, the proposed KNQ scheme provides a better tradeoff\nwhen higher accuracy is required.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 09:23:34 GMT"}, {"version": "v2", "created": "Sat, 6 Aug 2016 08:41:25 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Sun", "Fangxuan", ""], ["Lin", "Jun", ""], ["Wang", "Zhongfeng", ""]]}, {"id": "1607.02737", "submitter": "Guillermo Garcia-Hernando", "authors": "Guillermo Garcia-Hernando and Tae-Kyun Kim", "title": "Transition Forests: Learning Discriminative Temporal Transitions for\n  Action Recognition and Detection", "comments": "to appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A human action can be seen as transitions between one's body poses over time,\nwhere the transition depicts a temporal relation between two poses. Recognizing\nactions thus involves learning a classifier sensitive to these pose transitions\nas well as to static poses. In this paper, we introduce a novel method called\ntransitions forests, an ensemble of decision trees that both learn to\ndiscriminate static poses and transitions between pairs of two independent\nframes. During training, node splitting is driven by alternating two criteria:\nthe standard classification objective that maximizes the discrimination power\nin individual frames, and the proposed one in pairwise frame transitions.\nGrowing the trees tends to group frames that have similar associated\ntransitions and share same action label incorporating temporal information that\nwas not available otherwise. Unlike conventional decision trees where the best\nsplit in a node is determined independently of other nodes, the transition\nforests try to find the best split of nodes jointly (within a layer) for\nincorporating distant node transitions. When inferring the class label of a new\nframe, it is passed down the trees and the prediction is made based on previous\nframe predictions and the current one in an efficient and online manner. We\napply our method on varied skeleton action recognition and online detection\ndatasets showing its suitability over several baselines and state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 12:05:41 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 17:21:46 GMT"}, {"version": "v3", "created": "Fri, 31 Mar 2017 15:39:45 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Garcia-Hernando", "Guillermo", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1607.02748", "submitter": "Antonia Creswell", "authors": "Antonia Creswell and Anil Anthony Bharath", "title": "Adversarial Training For Sketch Retrieval", "comments": "Accepted to ECCV2016 VisArt Workshop", "journal-ref": null, "doi": "10.1007/978-3-319-46604-0_55", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) are able to learn excellent\nrepresentations for unlabelled data which can be applied to image generation\nand scene classification. Representations learned by GANs have not yet been\napplied to retrieval. In this paper, we show that the representations learned\nby GANs can indeed be used for retrieval. We consider heritage documents that\ncontain unlabelled Merchant Marks, sketch-like symbols that are similar to\nhieroglyphs. We introduce a novel GAN architecture with design features that\nmake it suitable for sketch retrieval. The performance of this sketch-GAN is\ncompared to a modified version of the original GAN architecture with respect to\nsimple invariance properties. Experiments suggest that sketch-GANs learn\nrepresentations that are suitable for retrieval and which also have increased\nstability to rotation, scale and translation compared to the standard GAN\narchitecture.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 14:15:38 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 11:24:37 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Creswell", "Antonia", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1607.02769", "submitter": "Gitit Kehat", "authors": "Gitit Kehat and James Pustejovsky", "title": "Annotation Methodologies for Vision and Language Dataset Creation", "comments": "in Scene Understanding Workshop (SUNw) in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated datasets are commonly used in the training and evaluation of tasks\ninvolving natural language and vision (image description generation, action\nrecognition and visual question answering). However, many of the existing\ndatasets reflect problems that emerge in the process of data selection and\nannotation. Here we point out some of the difficulties and problems one\nconfronts when creating and validating annotated vision and language datasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 18:11:27 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kehat", "Gitit", ""], ["Pustejovsky", "James", ""]]}, {"id": "1607.02801", "submitter": "Francesco Renna", "authors": "Hugo Reboredo and Francesco Renna and Robert Calderbank and Miguel R.\n  D. Rodrigues", "title": "Bounds on the Number of Measurements for Reliable Compressive\n  Classification", "comments": "16 pages, 5 figures, 4 tables. Submitted to the IEEE Transactions on\n  Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2599496", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the classification of high-dimensional Gaussian signals\nfrom low-dimensional noisy, linear measurements. In particular, it provides\nupper bounds (sufficient conditions) on the number of measurements required to\ndrive the probability of misclassification to zero in the low-noise regime,\nboth for random measurements and designed ones. Such bounds reveal two\nimportant operational regimes that are a function of the characteristics of the\nsource: i) when the number of classes is less than or equal to the dimension of\nthe space spanned by signals in each class, reliable classification is possible\nin the low-noise regime by using a one-vs-all measurement design; ii) when the\ndimension of the spaces spanned by signals in each class is lower than the\nnumber of classes, reliable classification is guaranteed in the low-noise\nregime by using a simple random measurement design. Simulation results both\nwith synthetic and real data show that our analysis is sharp, in the sense that\nit is able to gauge the number of measurements required to drive the\nmisclassification probability to zero in the low-noise regime.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 01:08:14 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 17:59:32 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Reboredo", "Hugo", ""], ["Renna", "Francesco", ""], ["Calderbank", "Robert", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1607.02815", "submitter": "Chao-Yeh Chen", "authors": "Chao-Yeh Chen and Kristen Grauman", "title": "Efficient Activity Detection in Untrimmed Video with Max-Subgraph Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approach for activity detection in video that unifies\nactivity categorization with space-time localization. The main idea is to pose\nactivity detection as a maximum-weight connected subgraph problem. Offline, we\nlearn a binary classifier for an activity category using positive video\nexemplars that are \"trimmed\" in time to the activity of interest. Then, given a\nnovel \\emph{untrimmed} video sequence, we decompose it into a 3D array of\nspace-time nodes, which are weighted based on the extent to which their\ncomponent features support the learned activity model. To perform detection, we\nthen directly localize instances of the activity by solving for the\nmaximum-weight connected subgraph in the test video's space-time graph. We show\nthat this detection strategy permits an efficient branch-and-cut solution for\nthe best-scoring---and possibly non-cubically shaped---portion of the video for\na given activity classifier. The upshot is a fast method that can search a\nbroader space of space-time region candidates than was previously practical,\nwhich we find often leads to more accurate detection. We demonstrate the\nproposed algorithm on four datasets, and we show its speed and accuracy\nadvantages over multiple existing search strategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 03:48:21 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Chen", "Chao-Yeh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1607.02829", "submitter": "Xiao Guobao", "authors": "Guobao Xiao and Hanzi Wang and Taotao Lai and David Suter", "title": "Hypergraph Modelling for Geometric Model Fitting", "comments": "Pattern Recognition, 2016", "journal-ref": null, "doi": "10.1016/J.PATCOG.2016.06.026", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel hypergraph based method (called HF) to fit\nand segment multi-structural data. The proposed HF formulates the geometric\nmodel fitting problem as a hypergraph partition problem based on a novel\nhypergraph model. In the hypergraph model, vertices represent data points and\nhyperedges denote model hypotheses. The hypergraph, with large and\n\"data-determined\" degrees of hyperedges, can express the complex relationships\nbetween model hypotheses and data points. In addition, we develop a robust\nhypergraph partition algorithm to detect sub-hypergraphs for model fitting. HF\ncan effectively and efficiently estimate the number of, and the parameters of,\nmodel instances in multi-structural data heavily corrupted with outliers\nsimultaneously. Experimental results show the advantages of the proposed method\nover previous methods on both synthetic data and real images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 06:16:37 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Xiao", "Guobao", ""], ["Wang", "Hanzi", ""], ["Lai", "Taotao", ""], ["Suter", "David", ""]]}, {"id": "1607.02936", "submitter": "Geoff Jones", "authors": "Geoffrey Jones, Neil T. Clancy, Yusuf Helo, Simon Arridge, Daniel S.\n  Elson, Danail Stoyanov", "title": "Inference of Haemoglobin Concentration From Stereo RGB", "comments": "To appear at the 6th International Conference on Medical Imaging and\n  Augmented Reality, MIAR 2016, held in Bern, Switzerland during August 2016,\n  and in the corresponding proceedings", "journal-ref": null, "doi": "10.1007/978-3-319-43775-0_5", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multispectral imaging (MSI) can provide information about tissue oxygenation,\nperfusion and potentially function during surgery. In this paper we present a\nnovel, near real-time technique for intrinsic measurements of total haemoglobin\n(THb) and blood oxygenation (SO2) in tissue using only RGB images from a stereo\nlaparoscope. The high degree of spectral overlap between channels makes\ninference of haemoglobin concentration challenging, non-linear and under\nconstrained. We decompose the problem into two constrained linear sub-problems\nand show that with Tikhonov regularisation the estimation significantly\nimproves, giving robust estimation of the Thb. We demonstrate by using the\nco-registered stereo image data from two cameras it is possible to get robust\nSO2 estimation as well. Our method is closed from, providing computational\nefficiency even with multiple cameras. The method we present requires only\nspectral response calibration of each camera, without modification of existing\nlaparoscopic imaging hardware. We validate our technique on synthetic data from\nMonte Carlo simulation % of light transport through soft tissue containing\nsubmerged blood vessels and further, in vivo, on a multispectral porcine data\nset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 13:29:54 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 10:23:09 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Jones", "Geoffrey", ""], ["Clancy", "Neil T.", ""], ["Helo", "Yusuf", ""], ["Arridge", "Simon", ""], ["Elson", "Daniel S.", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1607.02937", "submitter": "Gabriel Gon\\c{c}alves", "authors": "Gabriel Resende Gon\\c{c}alves, Sirlene Pio Gomes da Silva, David\n  Menotti, William Robson Schwartz", "title": "Benchmark for License Plate Character Segmentation", "comments": "32 pages, single column", "journal-ref": "J. Electron. Imaging. 25(5), 053034 (Oct 24, 2016)", "doi": "10.1117/1.JEI.25.5.053034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic License Plate Recognition (ALPR) has been the focus of many\nresearches in the past years. In general, ALPR is divided into the following\nproblems: detection of on-track vehicles, license plates detection, segmention\nof license plate characters and optical character recognition (OCR). Even\nthough commercial solutions are available for controlled acquisition\nconditions, e.g., the entrance of a parking lot, ALPR is still an open problem\nwhen dealing with data acquired from uncontrolled environments, such as roads\nand highways when relying only on imaging sensors. Due to the multiple\norientations and scales of the license plates captured by the camera, a very\nchallenging task of the ALPR is the License Plate Character Segmentation (LPCS)\nstep, which effectiveness is required to be (near) optimal to achieve a high\nrecognition rate by the OCR. To tackle the LPCS problem, this work proposes a\nnovel benchmark composed of a dataset designed to focus specifically on the\ncharacter segmentation step of the ALPR within an evaluation protocol.\nFurthermore, we propose the Jaccard-Centroid coefficient, a new evaluation\nmeasure more suitable than the Jaccard coefficient regarding the location of\nthe bounding box within the ground-truth annotation. The dataset is composed of\n2,000 Brazilian license plates consisting of 14,000 alphanumeric symbols and\ntheir corresponding bounding box annotations. We also present a new\nstraightforward approach to perform LPCS efficiently. Finally, we provide an\nexperimental evaluation for the dataset based on four LPCS approaches and\ndemonstrate the importance of character segmentation for achieving an accurate\nOCR.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 13:32:19 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 16:11:21 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Gon\u00e7alves", "Gabriel Resende", ""], ["da Silva", "Sirlene Pio Gomes", ""], ["Menotti", "David", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1607.03021", "submitter": "Sikha O K", "authors": "Sikha O K, Sachin Kumar S, K P Soman", "title": "Salient Region Detection and Segmentation in Images using Dynamic Mode\n  Decomposition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Saliency is the capability of vision system to select distinctive\nparts of scene and reduce the amount of visual data that need to be processed.\nThe presentpaper introduces (1) a novel approach to detect salient regions by\nconsidering color and luminance based saliency scores using Dynamic Mode\nDecomposition (DMD), (2) a new interpretation to use DMD approach in static\nimage processing. This approach integrates two data analysis methods: (1)\nFourier Transform, (2) Principle Component Analysis.The key idea of our work is\nto create a color based saliency map. This is based on the observation\nthatsalient part of an image usually have distinct colors compared to the\nremaining portion of the image. We have exploited the power of different color\nspaces to model the complex and nonlinear behavior of human visual system to\ngenerate a color based saliency map. To further improve the effect of final\nsaliency map, weutilized luminance information exploiting the fact that human\neye is more sensitive towards brightness than color.The experimental results\nshows that our method based on DMD theory is effective in comparison with\nprevious state-of-art saliency estimation approaches. The approach presented in\nthis paperis evaluated using ROC curve, F-measure rate, Precision-Recall rate,\nAUC score etc.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 16:30:06 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["K", "Sikha O", ""], ["S", "Sachin Kumar", ""], ["Soman", "K P", ""]]}, {"id": "1607.03105", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Systholic Boolean Orthonormalizer Network in Wavelet Domain for SAR\n  Image Despeckling", "comments": "11 pages, 9 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:1405.0632", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method for removing speckle (in wavelet domain) of\nunknown variance from SAR images. The me-thod is based on the following\nprocedure: We apply 1) Bidimentional Discrete Wavelet Transform (DWT-2D) to the\nspeckled image, 2) scaling and rounding to the coefficients of the highest\nsubbands (to obtain integer and positive coefficients), 3) bit-slicing to the\nnew highest subbands (to obtain bit-planes), 4) then we apply the Systholic\nBoolean Orthonormalizer Network (SBON) to the input bit-plane set and we obtain\ntwo orthonormal output bit-plane sets (in a Boolean sense), we project a set on\nthe other one, by means of an AND operation, and then, 5) we apply\nre-assembling, and, 6) re-sca-ling. Finally, 7) we apply Inverse DWT-2D and\nreconstruct a SAR image from the modified wavelet coefficients. Despeckling\nresults compare favorably to the most of methods in use at the moment.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 16:08:25 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1607.03164", "submitter": "Mario Mastriani", "authors": "Mario Mastriani, and Juliana Gambini", "title": "Fast Cosine Transform to increase speed-up and efficiency of\n  Karhunen-Loeve Transform for lossy image compression", "comments": "10 pages, 20 figures, 2 tables", "journal-ref": "Intern. Journal of Engineering and Mathematical Sciences. v.6(2),\n  pp.82-92 (2010)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a comparison between two techniques of image\ncompression. In the first case, the image is divided in blocks which are\ncollected according to zig-zag scan. In the second one, we apply the Fast\nCosine Transform to the image, and then the transformed image is divided in\nblocks which are collected according to zig-zag scan too. Later, in both cases,\nthe Karhunen-Loeve transform is applied to mentioned blocks. On the other hand,\nwe present three new metrics based on eigenvalues for a better comparative\nevaluation of the techniques. Simulations show that the combined version is the\nbest, with minor Mean Absolute Error (MAE) and Mean Squared Error (MSE), higher\nPeak Signal to Noise Ratio (PSNR) and better image quality. Finally, new\ntechnique was far superior to JPEG and JPEG2000.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 21:18:24 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Mastriani", "Mario", ""], ["Gambini", "Juliana", ""]]}, {"id": "1607.03222", "submitter": "Yipei Wang", "authors": "Yan Xu, Yang Li, Mingyuan Liu, Yipei Wang, Maode Lai, Eric I-Chao\n  Chang", "title": "Gland Instance Segmentation by Deep Multichannel Side Supervision", "comments": "conditionally accepted at MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new image instance segmentation method that\nsegments individual glands (instances) in colon histology images. This is a\ntask called instance segmentation that has recently become increasingly\nimportant. The problem is challenging since not only do the glands need to be\nsegmented from the complex background, they are also required to be\nindividually identified. Here we leverage the idea of image-to-image prediction\nin recent deep learning by building a framework that automatically exploits and\nfuses complex multichannel information, regional and boundary patterns, with\nside supervision (deep supervision on side responses) in gland histology\nimages. Our proposed system, deep multichannel side supervision (DMCS),\nalleviates heavy feature design due to the use of convolutional neural networks\nguided by side supervision. Compared to methods reported in the 2015 MICCAI\nGland Segmentation Challenge, we observe state-of-the-art results based on a\nnumber of evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 03:08:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 03:01:31 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Xu", "Yan", ""], ["Li", "Yang", ""], ["Liu", "Mingyuan", ""], ["Wang", "Yipei", ""], ["Lai", "Maode", ""], ["Chang", "Eric I-Chao", ""]]}, {"id": "1607.03226", "submitter": "Xiaoyue Jiang", "authors": "Xiaoyue Jiang, Dong Zhang and Xiaoyi Feng", "title": "Local feature hierarchy for face recognition across pose and\n  illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though face recognition in frontal view and normal lighting condition\nworks very well, the performance degenerates sharply in extreme conditions.\nRecently there are many work dealing with pose and illumination problems,\nrespectively. However both the lighting and pose variation will always be\nencountered at the same time. Accordingly we propose an end-to-end face\nrecognition method to deal with pose and illumination simultaneously based on\nconvolutional networks where the discriminative nonlinear features that are\ninvariant to pose and illumination are extracted. Normally the global structure\nfor images taken in different views is quite diverse. Therefore we propose to\nuse the 1*1 convolutional kernel to extract the local features. Furthermore the\nparallel multi-stream multi-layer 1*1 convolution network is developed to\nextract multi-hierarchy features. In the experiments we obtained the average\nface recognition rate of 96.9% on multiPIE dataset,which improves the\nstate-of-the-art of face recognition across poses and illumination by 7.5%.\nEspecially for profile-wise positions, the average recognition rate of our\nproposed network is 97.8%, which increases the state-of-the-art recognition\nrate by 19%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 03:52:30 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Jiang", "Xiaoyue", ""], ["Zhang", "Dong", ""], ["Feng", "Xiaoyi", ""]]}, {"id": "1607.03240", "submitter": "Sohil Shah", "authors": "Sohil Shah, Kuldeep Kulkarni, Arijit Biswas, Ankit Gandhi, Om Deshmukh\n  and Larry Davis", "title": "Weakly Supervised Learning of Heterogeneous Concepts in Videos", "comments": "To appear at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical textual descriptions that accompany online videos are 'weak': i.e.,\nthey mention the main concepts in the video but not their corresponding\nspatio-temporal locations. The concepts in the description are typically\nheterogeneous (e.g., objects, persons, actions). Certain location constraints\non these concepts can also be inferred from the description. The goal of this\npaper is to present a generalization of the Indian Buffet Process (IBP) that\ncan (a) systematically incorporate heterogeneous concepts in an integrated\nframework, and (b) enforce location constraints, for efficient classification\nand localization of the concepts in the videos. Finally, we develop posterior\ninference for the proposed formulation using mean-field variational\napproximation. Comparative evaluations on the Casablanca and the A2D datasets\nshow that the proposed approach significantly outperforms other\nstate-of-the-art techniques: 24% relative improvement for pairwise concept\nclassification in the Casablanca dataset and 9% relative improvement for\nlocalization in the A2D dataset as compared to the most competitive baseline.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 06:49:49 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Shah", "Sohil", ""], ["Kulkarni", "Kuldeep", ""], ["Biswas", "Arijit", ""], ["Gandhi", "Ankit", ""], ["Deshmukh", "Om", ""], ["Davis", "Larry", ""]]}, {"id": "1607.03250", "submitter": "Hengyuan Hu", "authors": "Hengyuan Hu, Rui Peng, Yu-Wing Tai, Chi-Keung Tang", "title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards\n  Efficient Deep Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural networks are getting deeper and wider. While their\nperformance increases with the increasing number of layers and neurons, it is\ncrucial to design an efficient deep architecture in order to reduce\ncomputational and memory costs. Designing an efficient neural network, however,\nis labor intensive requiring many experiments, and fine-tunings. In this paper,\nwe introduce network trimming which iteratively optimizes the network by\npruning unimportant neurons based on analysis of their outputs on a large\ndataset. Our algorithm is inspired by an observation that the outputs of a\nsignificant portion of neurons in a large network are mostly zero, regardless\nof what inputs the network received. These zero activation neurons are\nredundant, and can be removed without affecting the overall accuracy of the\nnetwork. After pruning the zero activation neurons, we retrain the network\nusing the weights before pruning as initialization. We alternate the pruning\nand retraining to further reduce zero activations in a network. Our experiments\non the LeNet and VGG-16 show that we can achieve high compression ratio of\nparameters without losing or even achieving higher accuracy than the original\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 07:43:01 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Hu", "Hengyuan", ""], ["Peng", "Rui", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1607.03255", "submitter": "Hendrik Dirks", "authors": "Martin Burger, Hendrik Dirks, Carola-Bibiane Sch\\\"onlieb", "title": "A Variational Model for Joint Motion Estimation and Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to derive and analyze a variational model for the\njoint estimation of motion and reconstruction of image sequences, which is\nbased on a time-continuous Eulerian motion model. The model can be set up in\nterms of the continuity equation or the brightness constancy equation. The\nanalysis in this paper focuses on the latter for robust motion estimation on\nsequences of two-dimensional images. We rigorously prove the existence of a\nminimizer in a suitable function space setting. Moreover, we discuss the\nnumerical solution of the model based on primal-dual algorithms and investigate\nseveral examples. Finally, the benefits of our model compared to existing\ntechniques, such as sequential image reconstruction and motion estimation, are\nshown.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 08:29:40 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Burger", "Martin", ""], ["Dirks", "Hendrik", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1607.03257", "submitter": "Benjamin Elizalde", "authors": "Benjamin Elizalde, Guan-Lin Chao, Ming Zeng, Ian Lane", "title": "City-Identification of Flickr Videos Using Semantic Acoustic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  City-identification of videos aims to determine the likelihood of a video\nbelonging to a set of cities. In this paper, we present an approach using only\naudio, thus we do not use any additional modality such as images, user-tags or\ngeo-tags. In this manner, we show to what extent the city-location of videos\ncorrelates to their acoustic information. Success in this task suggests\nimprovements can be made to complement the other modalities. In particular, we\npresent a method to compute and use semantic acoustic features to perform\ncity-identification and the features show semantic evidence of the\nidentification. The semantic evidence is given by a taxonomy of urban sounds\nand expresses the potential presence of these sounds in the city- soundtracks.\nWe used the MediaEval Placing Task set, which contains Flickr videos labeled by\ncity. In addition, we used the UrbanSound8K set containing audio clips labeled\nby sound- type. Our method improved the state-of-the-art performance and\nprovides a novel semantic approach to this task\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 08:30:45 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Elizalde", "Benjamin", ""], ["Chao", "Guan-Lin", ""], ["Zeng", "Ming", ""], ["Lane", "Ian", ""]]}, {"id": "1607.03284", "submitter": "Lyes Abada Dr.", "authors": "Lyes Abada, Saliha Aouat", "title": "A Machine learning approach for Shape From Shading", "comments": "2nd International Conference on Signal, Image, Vision and their\n  Applications (SIVA'13), November 18-20, 2013 - Guelma, Algeria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of Shape From Shading (SFS) problem is to reconstruct the relief of\nan object from a single gray level image. In this paper we present a new method\nto solve the problem of SFS using Machine learning method. Our approach belongs\nto Local resolution category. The orientation of each part of the object is\nrepresented by the perpendicular vector to the surface (Normal Vector), this\nvector is defined by two angles SLANT and TILT, such as the TILT is the angle\nbetween the normal vector and Z-axis, and the SLANT is the angle between the\nthe X-axis and the projection of the normal to the plane. The TILT can be\ndetermined from the gray level, the unknown is the SLANT. To calculate the\nnormal of each part of the surface (pixel) a supervised Machine learning method\nhas been proposed. This method divided into three steps: the first step is the\npreparation of the training data from 3D mathematical functions and synthetic\nobjects. The second step is the creation of database of examples from 3D\nobjects (off-line process). The third step is the application of test images\n(on-line process). The idea is to find for each pixel of the test image the\nmost similar element in the examples database using a similarity value.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 09:37:00 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Abada", "Lyes", ""], ["Aouat", "Saliha", ""]]}, {"id": "1607.03289", "submitter": "Lyes Abada", "authors": "Lyes Abada, Saliha Aouat, Omar el farouk Bourahla", "title": "Boundary conditions for Shape from Shading", "comments": "International Conference on Pattern Analysis and Intelligent Systems\n  (PAIS'15), October.26-27, 2015 - Tebessa, Algeria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shape From Shading is one of a computer vision field. It studies the 3D\nreconstruction of an object from a single grayscale image. The difficulty of\nthis field can be expressed in the local ambiguity (convex / concave). J.Shi\nand Q.Zhu have proposed a method (Global View) to solve the local ambiguity.\nThis method based on the graph theory and the relationship between the singular\npoints. In this work we will show that the use of singular points is not\nsufficient and requires further information on the object to resolve this\nambiguity.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 09:54:05 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Abada", "Lyes", ""], ["Aouat", "Saliha", ""], ["Bourahla", "Omar el farouk", ""]]}, {"id": "1607.03305", "submitter": "Martin Cadik", "authors": "Martin Cadik and Jan Vasicek and Michal Hradis and Filip Radenovic and\n  Ondrej Chum", "title": "Camera Elevation Estimation from a Single Mountain Landscape Photograph", "comments": null, "journal-ref": "In Xianghua Xie, Mark W. Jones, and Gary K. L. Tam, editors,\n  Proceedings of the British Machine Vision Conference (BMVC), pages\n  30.1-30.12. BMVA Press, September 2015", "doi": "10.5244/C.29.30", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of camera elevation estimation from a single\nphotograph in an outdoor environment. We introduce a new benchmark dataset of\none-hundred thousand images with annotated camera elevation called Alps100K. We\npropose and experimentally evaluate two automatic data-driven approaches to\ncamera elevation estimation: one based on convolutional neural networks, the\nother on local features. To compare the proposed methods to human performance,\nan experiment with 100 subjects is conducted. The experimental results show\nthat both proposed approaches outperform humans and that the best result is\nachieved by their combination.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 10:47:51 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Cadik", "Martin", ""], ["Vasicek", "Jan", ""], ["Hradis", "Michal", ""], ["Radenovic", "Filip", ""], ["Chum", "Ondrej", ""]]}, {"id": "1607.03333", "submitter": "Liangqiong Qu", "authors": "Liangqiong Qu, Shengfeng He, Jiawei Zhang, Jiandong Tian, Yandong\n  Tang, and Qingxiong Yang", "title": "RGBD Salient Object Detection via Deep Fusion", "comments": "This paper has been submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2017.2682981", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous efforts have been made to design different low level saliency cues\nfor the RGBD saliency detection, such as color or depth contrast features,\nbackground and color compactness priors. However, how these saliency cues\ninteract with each other and how to incorporate these low level saliency cues\neffectively to generate a master saliency map remain a challenging problem. In\nthis paper, we design a new convolutional neural network (CNN) to fuse\ndifferent low level saliency cues into hierarchical features for automatically\ndetecting salient objects in RGBD images. In contrast to the existing works\nthat directly feed raw image pixels to the CNN, the proposed method takes\nadvantage of the knowledge in traditional saliency detection by adopting\nvarious meaningful and well-designed saliency feature vectors as input. This\ncan guide the training of CNN towards detecting salient object more effectively\ndue to the reduced learning ambiguity. We then integrate a Laplacian\npropagation framework with the learned CNN to extract a spatially consistent\nsaliency map by exploiting the intrinsic structure of the input image.\nExtensive quantitative and qualitative experimental evaluations on three\ndatasets demonstrate that the proposed method consistently outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 12:32:56 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Qu", "Liangqiong", ""], ["He", "Shengfeng", ""], ["Zhang", "Jiawei", ""], ["Tian", "Jiandong", ""], ["Tang", "Yandong", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1607.03343", "submitter": "Michael Iliadis", "authors": "Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos", "title": "DeepBinaryMask: Learning a Binary Mask for Video Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel encoder-decoder neural network model\nreferred to as DeepBinaryMask for video compressive sensing. In video\ncompressive sensing one frame is acquired using a set of coded masks (sensing\nmatrix) from which a number of video frames is reconstructed, equal to the\nnumber of coded masks. The proposed framework is an end-to-end model where the\nsensing matrix is trained along with the video reconstruction. The encoder\nlearns the binary elements of the sensing matrix and the decoder is trained to\nrecover the unknown video sequence. The reconstruction performance is found to\nimprove when using the trained sensing mask from the network as compared to\nother mask designs such as random, across a wide variety of compressive sensing\nreconstruction algorithms. Finally, our analysis and discussion offers insights\ninto understanding the characteristics of the trained mask designs that lead to\nthe improved reconstruction quality.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 13:14:02 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 17:21:36 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Iliadis", "Michael", ""], ["Spinoulas", "Leonidas", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1607.03406", "submitter": "Renata Khasanova", "authors": "Renata Khasanova, Xiaowen Dong, Pascal Frossard", "title": "Multi-modal image retrieval with random walk on multi-layer graphs", "comments": null, "journal-ref": null, "doi": "10.1109/ISM.2016.0011", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large collections of image data is still a challenging\nproblem due to the difficulty of capturing the true concepts in visual data.\nThe similarity between images could be computed using different and possibly\nmultimodal features such as color or edge information or even text labels. This\nmotivates the design of image analysis solutions that are able to effectively\nintegrate the multi-view information provided by different feature sets. We\ntherefore propose a new image retrieval solution that is able to sort images\nthrough a random walk on a multi-layer graph, where each layer corresponds to a\ndifferent type of information about the image data. We study in depth the\ndesign of the image graph and propose in particular an effective method to\nselect the edge weights for the multi-layer graph, such that the image ranking\nscores are optimised. We then provide extensive experiments in different\nreal-world photo collections, which confirm the high performance of our new\nimage retrieval algorithm that generally surpasses state-of-the-art solutions\ndue to a more meaningful image similarity computation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:35:01 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Khasanova", "Renata", ""], ["Dong", "Xiaowen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1607.03425", "submitter": "Matthias Vestner", "authors": "Matthias Vestner, Roee Litman, Alex Bronstein, Emanuele Rodol\\`a and\n  Daniel Cremers", "title": "Bayesian Inference of Bijective Non-Rigid Shape Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for the computation of correspondences between deformable\nshapes rely on some variant of nearest neighbor matching in a descriptor space.\nSuch are, for example, various point-wise correspondence recovery algorithms\nused as a postprocessing stage in the functional correspondence framework. In\nthis paper, we show that such frequently used techniques in practice suffer\nfrom lack of accuracy and result in poor surjectivity. We propose an\nalternative recovery technique guaranteeing a bijective correspondence and\nproducing significantly higher accuracy. We derive the proposed method from a\nstatistical framework of Bayesian inference and demonstrate its performance on\nseveral challenging deformable 3D shape matching datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 16:04:54 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Vestner", "Matthias", ""], ["Litman", "Roee", ""], ["Bronstein", "Alex", ""], ["Rodol\u00e0", "Emanuele", ""], ["Cremers", "Daniel", ""]]}, {"id": "1607.03434", "submitter": "Manish Gupta", "authors": "Dixita Limbachiya and Dhaval Trivedi and Manish K Gupta", "title": "DNA Image Pro -- A Tool for Generating Pixel Patterns using DNA Tile\n  Assembly", "comments": "14 pages, draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-assembly is a process found everywhere in the Nature. In particular, it\nis known that DNA self-assembly is Turing universal. Thus one can do arbitrary\ncomputations or build nano-structures using DNA self-assembly. In order to\nunderstand the DNA self-assembly process, many mathematical models have been\nproposed in the literature. In particular, abstract Tile Assembly Model (aTAM)\nreceived much attention. In this work, we investigate pixel pattern generation\nusing aTAM. For a given image, a tile assembly system is given which can\ngenerate the image by self-assembly process. We also consider image blocks with\nspecific cyclic pixel patterns (uniform shift and non uniform shift) self\nassembly. A software, DNA Image Pro, for generating pixel patterns using DNA\ntile assembly is also given.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 16:46:25 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Limbachiya", "Dixita", ""], ["Trivedi", "Dhaval", ""], ["Gupta", "Manish K", ""]]}, {"id": "1607.03464", "submitter": "Roy R. Lederman", "authors": "Roy R. Lederman, Amit Singer", "title": "A Representation Theory Perspective on Simultaneous Alignment and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the difficulties in 3D reconstruction of molecules from images in\nsingle particle Cryo-Electron Microscopy (Cryo-EM), in addition to high levels\nof noise and unknown image orientations, is heterogeneity in samples: in many\ncases, the samples contain a mixture of molecules, or multiple conformations of\none molecule. Many algorithms for the reconstruction of molecules from images\nin heterogeneous Cryo-EM experiments are based on iterative approximations of\nthe molecules in a non-convex optimization that is prone to reaching suboptimal\nlocal minima. Other algorithms require an alignment in order to perform\nclassification, or vice versa. The recently introduced Non-Unique Games\nframework provides a representation theoretic approach to studying problems of\nalignment over compact groups, and offers convex relaxations for alignment\nproblems which are formulated as semidefinite programs (SDPs) with certificates\nof global optimality under certain circumstances. In this manuscript, we\npropose to extend Non-Unique Games to the problem of simultaneous alignment and\nclassification with the goal of simultaneously classifying Cryo-EM images and\naligning them within their respective classes. Our proposed approach can also\nbe extended to the case of continuous heterogeneity.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:50:20 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Lederman", "Roy R.", ""], ["Singer", "Amit", ""]]}, {"id": "1607.03468", "submitter": "Guillermo Gallego", "authors": "Guillermo Gallego, Jon E.A. Lund, Elias Mueggler, Henri Rebecq, Tobi\n  Delbruck, Davide Scaramuzza", "title": "Event-based, 6-DOF Camera Tracking from Photometric Depth Maps", "comments": "12 pages, 13 figures. 2 tables. (in press)", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Vol. 40, No. 2, pp. 2402-2412, Oct. 2018", "doi": "10.1109/TPAMI.2017.2769655", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired vision sensors that output pixel-level\nbrightness changes instead of standard intensity frames. These cameras do not\nsuffer from motion blur and have a very high dynamic range, which enables them\nto provide reliable visual information during high-speed motions or in scenes\ncharacterized by high dynamic range. These features, along with a very low\npower consumption, make event cameras an ideal complement to standard cameras\nfor VR/AR and video game applications. With these applications in mind, this\npaper tackles the problem of accurate, low-latency tracking of an event camera\nfrom an existing photometric depth map (i.e., intensity plus depth information)\nbuilt via classic dense reconstruction pipelines. Our approach tracks the 6-DOF\npose of the event camera upon the arrival of each event, thus virtually\neliminating latency. We successfully evaluate the method in both indoor and\noutdoor scenes and show that---because of the technological advantages of the\nevent camera---our pipeline works in scenes characterized by high-speed motion,\nwhich are still unaccessible to standard cameras.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:08:24 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:00:23 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gallego", "Guillermo", ""], ["Lund", "Jon E. A.", ""], ["Mueggler", "Elias", ""], ["Rebecq", "Henri", ""], ["Delbruck", "Tobi", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1607.03476", "submitter": "Paul Henderson", "authors": "Paul Henderson, Vittorio Ferrari", "title": "End-to-end training of object class detectors for mean average precision", "comments": "This version has minor additions to results (ablation study) and\n  discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training CNN-based object class detectors directly\nusing mean average precision (mAP) as the training loss, in a truly end-to-end\nfashion that includes non-maximum suppression (NMS) at training time. This\ncontrasts with the traditional approach of training a CNN for a window\nclassification loss, then applying NMS only at test time, when mAP is used as\nthe evaluation metric in place of classification accuracy. However, mAP\nfollowing NMS forms a piecewise-constant structured loss over thousands of\nwindows, with gradients that do not convey useful information for gradient\ndescent. Hence, we define new, general gradient-like quantities for piecewise\nconstant functions, which have wide applicability. We describe how to calculate\nthese efficiently for mAP following NMS, enabling to train a detector based on\nFast R-CNN directly for mAP. This model achieves equivalent performance to the\nstandard Fast R-CNN on the PASCAL VOC 2007 and 2012 datasets, while being\nconceptually more appealing as the very same model and loss are used at both\ntraining and test time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:45:12 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 13:55:07 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Henderson", "Paul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1607.03516", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi and Wen Li", "title": "Deep Reconstruction-Classification Networks for Unsupervised Domain\n  Adaptation", "comments": "to appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised domain adaptation algorithm\nbased on deep learning for visual object recognition. Specifically, we design a\nnew model called Deep Reconstruction-Classification Network (DRCN), which\njointly learns a shared encoding representation for two tasks: i) supervised\nclassification of labeled source data, and ii) unsupervised reconstruction of\nunlabeled target data.In this way, the learnt representation not only preserves\ndiscriminability, but also encodes useful information from the target domain.\nOur new DRCN model can be optimized by using backpropagation similarly as the\nstandard neural networks.\n  We evaluate the performance of DRCN on a series of cross-domain object\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\nobserve that the reconstruction pipeline of DRCN transforms images from the\nsource domain into images whose appearance resembles the target dataset. This\nsuggests that DRCN's performance is due to constructing a single composite\nrepresentation that encodes information about both the structure of target\nimages and the classification of source images. Finally, we provide a formal\nanalysis to justify the algorithm's objective in domain adaptation context.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:48:58 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 09:58:13 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""], ["Li", "Wen", ""]]}, {"id": "1607.03547", "submitter": "Ron Appel", "authors": "Ron Appel, Xavier Burgos-Artizzu, Pietro Perona", "title": "Improved Multi-Class Cost-Sensitive Boosting via Estimation of the\n  Minimum-Risk Class", "comments": "Project website: http://www.vision.caltech.edu/~appel/projects/REBEL/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple unified framework for multi-class cost-sensitive\nboosting. The minimum-risk class is estimated directly, rather than via an\napproximation of the posterior distribution. Our method jointly optimizes\nbinary weak learners and their corresponding output vectors, requiring classes\nto share features at each iteration. By training in a cost-sensitive manner,\nweak learners are invested in separating classes whose discrimination is\nimportant, at the expense of less relevant classification boundaries.\nAdditional contributions are a family of loss functions along with proof that\nour algorithm is Boostable in the theoretical sense, as well as an efficient\nprocedure for growing decision trees for use as weak learners. We evaluate our\nmethod on a variety of datasets: a collection of synthetic planar data, common\nUCI datasets, MNIST digits, SUN scenes, and CUB-200 birds. Results show\nstate-of-the-art performance across all datasets against several strong\nbaselines, including non-boosting multi-class approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 23:56:33 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 19:29:30 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Appel", "Ron", ""], ["Burgos-Artizzu", "Xavier", ""], ["Perona", "Pietro", ""]]}, {"id": "1607.03597", "submitter": "Jonathan Tompson", "authors": "Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, Ken Perlin", "title": "Accelerating Eulerian Fluid Simulation With Convolutional Networks", "comments": "Significant revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient simulation of the Navier-Stokes equations for fluid flow is a long\nstanding problem in applied mathematics, for which state-of-the-art methods\nrequire large compute resources. In this work, we propose a data-driven\napproach that leverages the approximation power of deep-learning with the\nprecision of standard solvers to obtain fast and highly realistic simulations.\nOur method solves the incompressible Euler equations using the standard\noperator splitting method, in which a large sparse linear system with many free\nparameters must be solved. We use a Convolutional Network with a highly\ntailored architecture, trained using a novel unsupervised learning framework to\nsolve the linear system. We present real-time 2D and 3D simulations that\noutperform recently proposed data-driven methods; the obtained results are\nrealistic and show good generalization properties.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 05:57:59 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 13:33:31 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 20:31:52 GMT"}, {"version": "v4", "created": "Mon, 28 Nov 2016 19:21:25 GMT"}, {"version": "v5", "created": "Fri, 3 Mar 2017 02:49:22 GMT"}, {"version": "v6", "created": "Thu, 22 Jun 2017 17:28:58 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Tompson", "Jonathan", ""], ["Schlachter", "Kristofer", ""], ["Sprechmann", "Pablo", ""], ["Perlin", "Ken", ""]]}, {"id": "1607.03681", "submitter": "Yong Xu", "authors": "Yong Xu, Qiang Huang, Wenwu Wang, Peter Foster, Siddharth Sigtia,\n  Philip J. B. Jackson, and Mark D. Plumbley", "title": "Unsupervised Feature Learning Based on Deep Models for Environmental\n  Audio Tagging", "comments": "10 pages, dcase 2016 challenge", "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing\n  25(6):1230-1241, Jun 2017", "doi": "10.1109/TASLP.2017.2690563", "report-no": null, "categories": "cs.SD cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental audio tagging aims to predict only the presence or absence of\ncertain acoustic events in the interested acoustic scene. In this paper we make\ncontributions to audio tagging in two parts, respectively, acoustic modeling\nand feature learning. We propose to use a shrinking deep neural network (DNN)\nframework incorporating unsupervised feature learning to handle the multi-label\nclassification task. For the acoustic modeling, a large set of contextual\nframes of the chunk are fed into the DNN to perform a multi-label\nclassification for the expected tags, considering that only chunk (or\nutterance) level rather than frame-level labels are available. Dropout and\nbackground noise aware training are also adopted to improve the generalization\ncapability of the DNNs. For the unsupervised feature learning, we propose to\nuse a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to\ngenerate new data-driven features from the Mel-Filter Banks (MFBs) features.\nThe new features, which are smoothed against background noise and more compact\nwith contextual information, can further improve the performance of the DNN\nbaseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of\nthe DCASE 2016 audio tagging challenge, our proposed method obtains a\nsignificant equal error rate (EER) reduction from 0.21 to 0.13 on the\ndevelopment set. The proposed aDAE system can get a relative 6.7% EER reduction\ncompared with the strong DNN baseline on the development set. Finally, the\nresults also show that our approach obtains the state-of-the-art performance\nwith 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while\nEER of the first prize of this challenge is 0.17.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:31:14 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 15:56:36 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Xu", "Yong", ""], ["Huang", "Qiang", ""], ["Wang", "Wenwu", ""], ["Foster", "Peter", ""], ["Sigtia", "Siddharth", ""], ["Jackson", "Philip J. B.", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1607.03682", "submitter": "Yong Xu", "authors": "Yong Xu, Qiang Huang, Wenwu Wang, Mark D. Plumbley", "title": "Hierarchical learning for DNN-based acoustic scene classification", "comments": "5 pages, DCASE 2016 challenge workshop paper, poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep neural network (DNN)-based acoustic scene\nclassification framework. Two hierarchical learning methods are proposed to\nimprove the DNN baseline performance by incorporating the hierarchical taxonomy\ninformation of environmental sounds. Firstly, the parameters of the DNN are\ninitialized by the proposed hierarchical pre-training. Multi-level objective\nfunction is then adopted to add more constraint on the cross-entropy based loss\nfunction. A series of experiments were conducted on the Task1 of the Detection\nand Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The\nfinal DNN-based system achieved a 22.9% relative improvement on average scene\nclassification error as compared with the Gaussian Mixture Model (GMM)-based\nbenchmark system across four standard folds.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:31:25 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 15:32:42 GMT"}, {"version": "v3", "created": "Sat, 13 Aug 2016 10:37:53 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Xu", "Yong", ""], ["Huang", "Qiang", ""], ["Wang", "Wenwu", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1607.03738", "submitter": "Davide Modolo", "authors": "Abel Gonzalez-Garcia, Davide Modolo, Vittorio Ferrari", "title": "Do semantic parts emerge in Convolutional Neural Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic object parts can be useful for several visual recognition tasks.\nLately, these tasks have been addressed using Convolutional Neural Networks\n(CNN), achieving outstanding results. In this work we study whether CNNs learn\nsemantic parts in their internal representation. We investigate the responses\nof convolutional filters and try to associate their stimuli with semantic\nparts. We perform two extensive quantitative analyses. First, we use\nground-truth part bounding-boxes from the PASCAL-Part dataset to determine how\nmany of those semantic parts emerge in the CNN. We explore this emergence for\ndifferent layers, network depths, and supervision levels. Second, we collect\nhuman judgements in order to study what fraction of all filters systematically\nfire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we\nexplore several connections between discriminative power and semantics. We find\nout which are the most discriminative filters for object recognition, and\nanalyze whether they respond to semantic parts or to other image patches. We\nalso investigate the other direction: we determine which semantic parts are the\nmost discriminative and whether they correspond to those parts emerging in the\nnetwork. This enables to gain an even deeper understanding of the role of\nsemantic parts in the network.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 13:58:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 08:32:08 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 20:15:02 GMT"}, {"version": "v4", "created": "Tue, 11 Oct 2016 13:03:55 GMT"}, {"version": "v5", "created": "Wed, 20 Sep 2017 18:09:33 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Gonzalez-Garcia", "Abel", ""], ["Modolo", "Davide", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1607.03785", "submitter": "Suyash Shetty", "authors": "Suyash Shetty", "title": "Application of Convolutional Neural Network for Image Classification on\n  Pascal VOC Challenge 2012 dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we work on creating a model to classify images for the Pascal\nVOC Challenge 2012. We use convolutional neural networks trained on a single\nGPU instance provided by Amazon via their cloud service Amazon Web Services\n(AWS) to classify images in the Pascal VOC 2012 data set. We train multiple\nconvolutional neural network models and finally settle on the best model which\nproduced a validation accuracy of 85.6% and a testing accuracy of 85.24%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 15:18:25 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Shetty", "Suyash", ""]]}, {"id": "1607.03827", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Christian Mandery, Tamim Asfour", "title": "The KIT Motion-Language Dataset", "comments": "5 figures, 4 tables, submitted to Big Data journal, Special Issue on\n  Robotics", "journal-ref": null, "doi": "10.1089/big.2016.0028", "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking human motion and natural language is of great interest for the\ngeneration of semantic representations of human activities as well as for the\ngeneration of robot activities based on natural language input. However, while\nthere have been years of research in this area, no standardized and openly\navailable dataset exists to support the development and evaluation of such\nsystems. We therefore propose the KIT Motion-Language Dataset, which is large,\nopen, and extensible. We aggregate data from multiple motion capture databases\nand include them in our dataset using a unified representation that is\nindependent of the capture system or marker set, making it easy to work with\nthe data regardless of its origin. To obtain motion annotations in natural\nlanguage, we apply a crowd-sourcing approach and a web-based tool that was\nspecifically build for this purpose, the Motion Annotation Tool. We thoroughly\ndocument the annotation process itself and discuss gamification methods that we\nused to keep annotators motivated. We further propose a novel method,\nperplexity-based selection, which systematically selects motions for further\nannotation that are either under-represented in our dataset or that have\nerroneous annotations. We show that our method mitigates the two aforementioned\nproblems and ensures a systematic annotation process. We provide an in-depth\nanalysis of the structure and contents of our resulting dataset, which, as of\nOctober 10, 2016, contains 3911 motions with a total duration of 11.23 hours\nand 6278 annotations in natural language that contain 52,903 words. We believe\nthis makes our dataset an excellent choice that enables more transparent and\ncomparable research in this important area.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 17:08:01 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 14:24:47 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Plappert", "Matthias", ""], ["Mandery", "Christian", ""], ["Asfour", "Tamim", ""]]}, {"id": "1607.03856", "submitter": "Yanlin Qian", "authors": "Yanlin Qian, Ke Chen, Joni-Kristian Kamarainen, Jarno Nikkanen, Jiri\n  Matas", "title": "Deep Structured-Output Regression Learning for Computational Color\n  Constancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational color constancy that requires esti- mation of illuminant colors\nof images is a fundamental yet active problem in computer vision, which can be\nformulated into a regression problem. To learn a robust regressor for color\nconstancy, obtaining meaningful imagery features and capturing latent\ncorrelations across output variables play a vital role. In this work, we\nintroduce a novel deep structured-output regression learning framework to\nachieve both goals simultaneously. By borrowing the power of deep convolutional\nneural networks (CNN) originally designed for visual recognition, the proposed\nframework can automatically discover strong features for white balancing over\ndifferent illumination conditions and learn a multi-output regressor beyond\nunderlying relationships between features and targets to find the complex\ninterdependence of dif- ferent dimensions of target variables. Experiments on\ntwo public benchmarks demonstrate that our method achieves competitive\nperformance in comparison with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 18:36:35 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 17:32:22 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Qian", "Yanlin", ""], ["Chen", "Ke", ""], ["Kamarainen", "Joni-Kristian", ""], ["Nikkanen", "Jarno", ""], ["Matas", "Jiri", ""]]}, {"id": "1607.03949", "submitter": "Chris Sweeney", "authors": "Chris Sweeney, Victor Fragoso, Tobias Hollerer and Matthew Turk", "title": "Large Scale SfM with the Distributed Camera Model", "comments": "Published at 2016 3DV Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the distributed camera model, a novel model for\nStructure-from-Motion (SfM). This model describes image observations in terms\nof light rays with ray origins and directions rather than pixels. As such, the\nproposed model is capable of describing a single camera or multiple cameras\nsimultaneously as the collection of all light rays observed. We show how the\ndistributed camera model is a generalization of the standard camera model and\ndescribe a general formulation and solution to the absolute camera pose problem\nthat works for standard or distributed cameras. The proposed method computes a\nsolution that is up to 8 times more efficient and robust to rotation\nsingularities in comparison with gDLS. Finally, this method is used in an novel\nlarge-scale incremental SfM pipeline where distributed cameras are accurately\nand robustly merged together. This pipeline is a direct generalization of\ntraditional incremental SfM; however, instead of incrementally adding one\ncamera at a time to grow the reconstruction the reconstruction is grown by\nadding a distributed camera. Our pipeline produces highly accurate\nreconstructions efficiently by avoiding the need for many bundle adjustment\niterations and is capable of computing a 3D model of Rome from over 15,000\nimages in just 22 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 22:39:11 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 02:09:31 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Sweeney", "Chris", ""], ["Fragoso", "Victor", ""], ["Hollerer", "Tobias", ""], ["Turk", "Matthew", ""]]}, {"id": "1607.03961", "submitter": "Simon Korman", "authors": "Omri Ben-Eliezer and Simon Korman and Daniel Reichman", "title": "Deleting and Testing Forbidden Patterns in Multi-Dimensional Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the local behaviour of structured multi-dimensional data is a\nfundamental problem in various areas of computer science. As the amount of data\nis often huge, it is desirable to obtain sublinear time algorithms, and\nspecifically property testers, to understand local properties of the data.\n  We focus on the natural local problem of testing pattern freeness: given a\nlarge $d$-dimensional array $A$ and a fixed $d$-dimensional pattern $P$ over a\nfinite alphabet, we say that $A$ is $P$-free if it does not contain a copy of\nthe forbidden pattern $P$ as a consecutive subarray. The distance of $A$ to\n$P$-freeness is the fraction of entries of $A$ that need to be modified to make\nit $P$-free. For any $\\epsilon \\in [0,1]$ and any large enough pattern $P$ over\nany alphabet, other than a very small set of exceptional patterns, we design a\ntolerant tester that distinguishes between the case that the distance is at\nleast $\\epsilon$ and the case that it is at most $a_d \\epsilon$, with query\ncomplexity and running time $c_d \\epsilon^{-1}$, where $a_d < 1$ and $c_d$\ndepend only on $d$.\n  To analyze the testers we establish several combinatorial results, including\nthe following $d$-dimensional modification lemma, which might be of independent\ninterest: for any large enough pattern $P$ over any alphabet (excluding a small\nset of exceptional patterns for the binary case), and any array $A$ containing\na copy of $P$, one can delete this copy by modifying one of its locations\nwithout creating new $P$-copies in $A$.\n  Our results address an open question of Fischer and Newman, who asked whether\nthere exist efficient testers for properties related to tight substructures in\nmulti-dimensional structured data. They serve as a first step towards a general\nunderstanding of local properties of multi-dimensional arrays, as any such\nproperty can be characterized by a fixed family of forbidden patterns.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 23:55:56 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 20:01:50 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 12:43:59 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Korman", "Simon", ""], ["Reichman", "Daniel", ""]]}, {"id": "1607.03967", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Hoang D. Tuan, Ho N. Phien, Minh N. Do", "title": "Concatenated image completion via tensor augmentation and completion", "comments": "7 pages, 6 figures, submitted to ICSPCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework called concatenated image completion\nvia tensor augmentation and completion (ICTAC), which recovers missing entries\nof color images with high accuracy. Typical images are second- or third-order\ntensors (2D/3D) depending if they are grayscale or color, hence tensor\ncompletion algorithms are ideal for their recovery. The proposed framework\nperforms image completion by concatenating copies of a single image that has\nmissing entries into a third-order tensor, applying a dimensionality\naugmentation technique to the tensor, utilizing a tensor completion algorithm\nfor recovering its missing entries, and finally extracting the recovered image\nfrom the tensor. The solution relies on two key components that have been\nrecently proposed to take advantage of the tensor train (TT) rank: A tensor\naugmentation tool called ket augmentation (KA) that represents a low-order\ntensor by a higher-order tensor, and the algorithm tensor completion by\nparallel matrix factorization via tensor train (TMac-TT), which has been\ndemonstrated to outperform state-of-the-art tensor completion algorithms.\nSimulation results for color image recovery show the clear advantage of our\nframework against current state-of-the-art tensor completion algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 00:24:33 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Bengua", "Johann A.", ""], ["Tuan", "Hoang D.", ""], ["Phien", "Ho N.", ""], ["Do", "Minh N.", ""]]}, {"id": "1607.03991", "submitter": "Bin Liu", "authors": "Bin Liu, Hao Ji, Yi Dai", "title": "Vision-based Traffic Flow Prediction using Dynamic Texture Model and\n  Gaussian Process", "comments": "8 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe work in progress towards a real-time vision-based\ntraffic flow prediction (TFP) system. The proposed method consists of three\nelemental operators, that are dynamic texture model based motion segmentation,\nfeature extraction and Gaussian process (GP) regression. The objective of\nmotion segmentation is to recognize the target regions covering the moving\nvehicles in the sequence of visual processes. The feature extraction operator\naims to extract useful features from the target regions. The extracted features\nare then mapped to the number of vehicles through the operator of GP\nregression. A training stage using historical visual data is required for\ndetermining the parameter values of the GP. Using a low-resolution visual data\nset, we performed preliminary evaluations on the performance of the proposed\nmethod. The results show that our method beats a benchmark solution based on\nGaussian mixture model, and has the potential to be developed into qualified\nand practical solutions to real-time TFP.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 05:12:56 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 18:22:48 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Liu", "Bin", ""], ["Ji", "Hao", ""], ["Dai", "Yi", ""]]}, {"id": "1607.04032", "submitter": "Faik Boray Tek", "authors": "F. Boray Tek, Andrew G. Dempster, \\.Izzet Kale", "title": "Adaptive Gray World-Based Color Normalization of Thin Blood Film Images", "comments": "5 pages, 3 figures, conference unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective color normalization method for thin blood\nfilm images of peripheral blood specimens. Thin blood film images can easily be\nseparated to foreground (cell) and background (plasma) parts. The color of the\nplasma region is used to estimate and reduce the differences arising from\ndifferent illumination conditions. A second stage normalization based on the\ndatabase-gray world algorithm transforms the color of the foreground objects to\nmatch a reference color character. The quantitative experiments demonstrate the\neffectiveness of the method and its advantages against two other general\npurpose color correction methods: simple gray world and Retinex.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 08:40:07 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Tek", "F. Boray", ""], ["Dempster", "Andrew G.", ""], ["Kale", "\u0130zzet", ""]]}, {"id": "1607.04147", "submitter": "Nikos Deligiannis", "authors": "Nikos Deligiannis, Joao F. C. Mota, Bruno Cornelis, Miguel R. D.\n  Rodrigues, Ingrid Daubechies", "title": "Multi-modal dictionary learning for image separation with application in\n  art investigation", "comments": "submitted to IEEE Transactions on Images Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2623484", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support of art investigation, we propose a new source separation method\nthat unmixes a single X-ray scan acquired from double-sided paintings. In this\nproblem, the X-ray signals to be separated have similar morphological\ncharacteristics, which brings previous source separation methods to their\nlimits. Our solution is to use photographs taken from the front and back-side\nof the panel to drive the separation process. The crux of our approach relies\non the coupling of the two imaging modalities (photographs and X-rays) using a\nnovel coupled dictionary learning framework able to capture both common and\ndisparate features across the modalities using parsimonious representations;\nthe common component models features shared by the multi-modal images, whereas\nthe innovation component captures modality-specific information. As such, our\nmodel enables the formulation of appropriately regularized convex optimization\nprocedures that lead to the accurate separation of the X-rays. Our dictionary\nlearning framework can be tailored both to a single- and a multi-scale\nframework, with the latter leading to a significant performance improvement.\nMoreover, to improve further on the visual quality of the separated images, we\npropose to train coupled dictionaries that ignore certain parts of the painting\ncorresponding to craquelure. Experimentation on synthetic and real data - taken\nfrom digital acquisition of the Ghent Altarpiece (1432) - confirms the\nsuperiority of our method against the state-of-the-art morphological component\nanalysis technique that uses either fixed or trained dictionaries to perform\nimage separation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 14:25:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Deligiannis", "Nikos", ""], ["Mota", "Joao F. C.", ""], ["Cornelis", "Bruno", ""], ["Rodrigues", "Miguel R. D.", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1607.04174", "submitter": "Shawn Andrews", "authors": "Shawn Andrews and Ghassan Hamarneh", "title": "Adaptable Precomputation for Random Walker Image Segmentation and\n  Registration", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random walker (RW) algorithm is used for both image segmentation and\nregistration, and possesses several useful properties that make it popular in\nmedical imaging, such as being globally optimizable, allowing user interaction,\nand providing uncertainty information. The RW algorithm defines a weighted\ngraph over an image and uses the graph's Laplacian matrix to regularize its\nsolutions. This regularization reduces to solving a large system of equations,\nwhich may be excessively time consuming in some applications, such as when\ninteracting with a human user. Techniques have been developed that precompute\neigenvectors of a Laplacian offline, after image acquisition but before any\nanalysis, in order speed up the RW algorithm online, when segmentation or\nregistration is being performed. However, precomputation requires certain\nalgorithm parameters be fixed offline, limiting their flexibility. In this\npaper, we develop techniques to update the precomputed data online when RW\nparameters are altered. Specifically, we dynamically determine the number of\neigenvectors needed for a desired accuracy based on user input, and derive\nupdate equations for the eigenvectors when the edge weights or topology of the\nimage graph are changed. We present results demonstrating that our techniques\nmake RW with precomputation much more robust to offline settings while only\nsacrificing minimal accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 15:59:56 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Andrews", "Shawn", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1607.04243", "submitter": "Thomas Bamford", "authors": "Thomas Bamford, Kamran Esmaeili, Angela P. Schoellig", "title": "A real-time analysis of rock fragmentation using UAV technology", "comments": "12 pages, 12 figures, 6th International Conference on Computer\n  Applications in the Minerals Industries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate measurement of blast-induced rock fragmentation is of great\nimportance for many mining operations. The post-blast rock size distribution\ncan significantly influence the efficiency of all the downstream mining and\ncomminution processes. Image analysis methods are one of the most common\nmethods used to measure rock fragment size distribution in mines regardless of\ncriticism for lack of accuracy to measure fine particles and other perceived\ndeficiencies. The current practice of collecting rock fragmentation data for\nimage analysis is highly manual and provides data with low temporal and spatial\nresolution. Using UAVs for collecting images of rock fragments can not only\nimprove the quality of the image data but also automate the data collection\nprocess. Ultimately, real-time acquisition of high temporal- and\nspatial-resolution data based on UAV technology will provide a broad range of\nopportunities for both improving blast design without interrupting the\nproduction process and reducing the cost of the human operator. This paper\npresents the results of a series of laboratory-scale rock fragment measurements\nusing a quadrotor UAV equipped with a camera. The goal of this work is to\nhighlight the benefits of aerial fragmentation analysis in terms of both\nprediction accuracy and time effort. A pile of rock fragments with different\nfragment sizes was placed in a lab that is equipped with a motion capture\ncamera system for precise UAV localization and control. Such an environment\npresents optimal conditions for UAV flight and thus, is well-suited for\nconducting proof-of-concept experiments before testing them in large-scale\nfield experiments. The pile was photographed by a camera attached to the UAV,\nand the particle size distribution curves were generated in almost real-time.\nThe pile was also manually photographed and the results of the manual method\nwere compared to the UAV method.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 18:47:18 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Bamford", "Thomas", ""], ["Esmaeili", "Kamran", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1607.04311", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini and David Wagner", "title": "Defensive Distillation is Not Robust to Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that defensive distillation is not secure: it is no more resistant to\ntargeted misclassification attacks than unprotected neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 20:44:27 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Carlini", "Nicholas", ""], ["Wagner", "David", ""]]}, {"id": "1607.04381", "submitter": "Song Han", "authors": "Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian\n  Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, Bryan Catanzaro,\n  William J. Dally", "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks have a large number of parameters, making them\nvery hard to train. We propose DSD, a dense-sparse-dense training flow, for\nregularizing deep neural networks and achieving better optimization\nperformance. In the first D (Dense) step, we train a dense network to learn\nconnection weights and importance. In the S (Sparse) step, we regularize the\nnetwork by pruning the unimportant connections with small weights and\nretraining the network given the sparsity constraint. In the final D (re-Dense)\nstep, we increase the model capacity by removing the sparsity constraint,\nre-initialize the pruned parameters from zero and retrain the whole dense\nnetwork. Experiments show that DSD training can improve the performance for a\nwide range of CNNs, RNNs and LSTMs on the tasks of image classification,\ncaption generation and speech recognition. On ImageNet, DSD improved the Top1\naccuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50\nby 1.1%, respectively. On the WSJ'93 dataset, DSD improved DeepSpeech and\nDeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the\nNeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training\ntime, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S\nstep. At testing time, DSD doesn't change the network architecture or incur any\ninference overhead. The consistent and significant performance gain of DSD\nexperiments shows the inadequacy of the current training methods for finding\nthe best local optimum, while DSD effectively achieves superior optimization\nperformance for finding a better solution. DSD models are available to download\nat https://songhan.github.io/DSD.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 04:56:27 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 20:51:05 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Han", "Song", ""], ["Pool", "Jeff", ""], ["Narang", "Sharan", ""], ["Mao", "Huizi", ""], ["Gong", "Enhao", ""], ["Tang", "Shijian", ""], ["Elsen", "Erich", ""], ["Vajda", "Peter", ""], ["Paluri", "Manohar", ""], ["Tran", "John", ""], ["Catanzaro", "Bryan", ""], ["Dally", "William J.", ""]]}, {"id": "1607.04411", "submitter": "Yinxiao Li", "authors": "Yinxiao Li, Yan Wang, Yonghao Yue, Danfei Xu, Michael Case, Shih-Fu\n  Chang, Eitan Grinspun, Peter Allen", "title": "Model-Driven Feed-Forward Prediction for Manipulation of Deformable\n  Objects", "comments": "21 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic manipulation of deformable objects is a difficult problem especially\nbecause of the complexity of the many different ways an object can deform.\nSearching such a high dimensional state space makes it difficult to recognize,\ntrack, and manipulate deformable objects. In this paper, we introduce a\npredictive, model-driven approach to address this challenge, using a\npre-computed, simulated database of deformable object models. Mesh models of\ncommon deformable garments are simulated with the garments picked up in\nmultiple different poses under gravity, and stored in a database for fast and\nefficient retrieval. To validate this approach, we developed a comprehensive\npipeline for manipulating clothing as in a typical laundry task. First, the\ndatabase is used for category and pose estimation for a garment in an arbitrary\nposition. A fully featured 3D model of the garment is constructed in real-time\nand volumetric features are then used to obtain the most similar model in the\ndatabase to predict the object category and pose. Second, the database can\nsignificantly benefit the manipulation of deformable objects via non-rigid\nregistration, providing accurate correspondences between the reconstructed\nobject model and the database models. Third, the accurate model simulation can\nalso be used to optimize the trajectories for manipulation of deformable\nobjects, such as the folding of garments. Extensive experimental results are\nshown for the tasks above using a variety of different clothing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 08:01:13 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Li", "Yinxiao", ""], ["Wang", "Yan", ""], ["Yue", "Yonghao", ""], ["Xu", "Danfei", ""], ["Case", "Michael", ""], ["Chang", "Shih-Fu", ""], ["Grinspun", "Eitan", ""], ["Allen", "Peter", ""]]}, {"id": "1607.04433", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Bernhard Sch\\\"olkopf, Hendrik P.A. Lensch,\n  Michael Hirsch", "title": "End-to-End Learning for Image Burst Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network model approach for multi-frame blind\ndeconvolution. The discriminative approach adopts and combines two recent\ntechniques for image deblurring into a single neural network architecture. Our\nproposed hybrid-architecture combines the explicit prediction of a\ndeconvolution filter and non-trivial averaging of Fourier coefficients in the\nfrequency domain. In order to make full use of the information contained in all\nimages in one burst, the proposed network embeds smaller networks, which\nexplicitly allow the model to transfer information between images in early\nlayers. Our system is trained end-to-end using standard backpropagation on a\nset of artificially generated training examples, enabling competitive\nperformance in multi-frame blind deconvolution, both with respect to quality\nand runtime.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:46:49 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 18:06:15 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Lensch", "Hendrik P. A.", ""], ["Hirsch", "Michael", ""]]}, {"id": "1607.04436", "submitter": "Pedro Miraldo", "authors": "David Ribeiro, Andre Mateus, Pedro Miraldo, and Jacinto C. Nascimento", "title": "A Real-Time Deep Learning Pedestrian Detector for Robot Navigation", "comments": null, "journal-ref": "IEEE Int'l Conf. Autonomous Robot Systems and Competitions\n  (ICARSC), 2017", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A real-time Deep Learning based method for Pedestrian Detection (PD) is\napplied to the Human-Aware robot navigation problem. The pedestrian detector\ncombines the Aggregate Channel Features (ACF) detector with a deep\nConvolutional Neural Network (CNN) in order to obtain fast and accurate\nperformance. Our solution is firstly evaluated using a set of real images taken\nfrom onboard and offboard cameras and, then, it is validated in a typical robot\nnavigation environment with pedestrians (two distinct experiments are\nconducted). The results on both tests show that our pedestrian detector is\nrobust and fast enough to be used on robot navigation applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:58:08 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 09:31:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Ribeiro", "David", ""], ["Mateus", "Andre", ""], ["Miraldo", "Pedro", ""], ["Nascimento", "Jacinto C.", ""]]}, {"id": "1607.04441", "submitter": "Andr\\'e Mateus", "authors": "Andre Mateus, David Ribeiro, Pedro Miraldo, and Jacinto C. Nascimento", "title": "Efficient and Robust Pedestrian Detection using Deep Learning for\n  Human-Aware Navigation", "comments": "Accepted in Robotics and Autonomous Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Human-Aware Navigation (HAN), using multi\ncamera sensors to implement a vision-based person tracking system. The main\ncontributions of this paper are as follows: a novel and efficient Deep Learning\nperson detection and a standardization of human-aware constraints. In the first\nstage of the approach, we propose to cascade the Aggregate Channel Features\n(ACF) detector with a deep Convolutional Neural Network (CNN) to achieve fast\nand accurate Pedestrian Detection (PD). Regarding the human awareness (that can\nbe defined as constraints associated with the robot's motion), we use a mixture\nof asymmetric Gaussian functions, to define the cost functions associated to\neach constraint. Both methods proposed herein are evaluated individually to\nmeasure the impact of each of the components. The final solution (including\nboth the proposed pedestrian detection and the human-aware constraints) is\ntested in a typical domestic indoor scenario, in four distinct experiments. The\nresults show that the robot is able to cope with human-aware constraints,\ndefined after common proxemics and social rules.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 10:16:45 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:21:54 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 11:43:14 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Mateus", "Andre", ""], ["Ribeiro", "David", ""], ["Miraldo", "Pedro", ""], ["Nascimento", "Jacinto C.", ""]]}, {"id": "1607.04515", "submitter": "Yuchao Dai Dr.", "authors": "Suryansh Kumar, Yuchao Dai, and Hongdong Li", "title": "Multi-body Non-rigid Structure-from-Motion", "comments": "21 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional structure-from-motion (SFM) research is primarily concerned with\nthe 3D reconstruction of a single, rigidly moving object seen by a static\ncamera, or a static and rigid scene observed by a moving camera --in both cases\nthere are only one relative rigid motion involved. Recent progress have\nextended SFM to the areas of {multi-body SFM} (where there are {multiple rigid}\nrelative motions in the scene), as well as {non-rigid SFM} (where there is a\nsingle non-rigid, deformable object or scene). Along this line of thinking,\nthere is apparently a missing gap of \"multi-body non-rigid SFM\", in which the\ntask would be to jointly reconstruct and segment multiple 3D structures of the\nmultiple, non-rigid objects or deformable scenes from images. Such a multi-body\nnon-rigid scenario is common in reality (e.g. two persons shaking hands,\nmulti-person social event), and how to solve it represents a natural\n{next-step} in SFM research. By leveraging recent results of subspace\nclustering, this paper proposes, for the first time, an effective framework for\nmulti-body NRSFM, which simultaneously reconstructs and segments each 3D\ntrajectory into their respective low-dimensional subspace. Under our\nformulation, 3D trajectories for each non-rigid structure can be well\napproximated with a sparse affine combination of other 3D trajectories from the\nsame structure (self-expressiveness). We solve the resultant optimization with\nthe alternating direction method of multipliers (ADMM). We demonstrate the\nefficacy of the proposed framework through extensive experiments on both\nsynthetic and real data sequences. Our method clearly outperforms other\nalternative methods, such as first clustering the 2D feature tracks to groups\nand then doing non-rigid reconstruction in each group or first conducting 3D\nreconstruction by using single subspace assumption and then clustering the 3D\ntrajectories into groups.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:04:30 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Kumar", "Suryansh", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1607.04564", "submitter": "Yi Zhou", "authors": "Yi Zhou, Li Liu, Ling Shao and Matt Mellor", "title": "DAVE: A Unified Framework for Fast Vehicle Detection and Annotation", "comments": "This paper has been accepted by ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection and annotation for streaming video data with complex scenes\nis an interesting but challenging task for urban traffic surveillance. In this\npaper, we present a fast framework of Detection and Annotation for Vehicles\n(DAVE), which effectively combines vehicle detection and attributes annotation.\nDAVE consists of two convolutional neural networks (CNNs): a fast vehicle\nproposal network (FVPN) for vehicle-like objects extraction and an attributes\nlearning network (ALN) aiming to verify each proposal and infer each vehicle's\npose, color and type simultaneously. These two nets are jointly optimized so\nthat abundant latent knowledge learned from the ALN can be exploited to guide\nFVPN training. Once the system is trained, it can achieve efficient vehicle\ndetection and annotation for real-world traffic surveillance data. We evaluate\nDAVE on a new self-collected UTS dataset and the public PASCAL VOC2007 car and\nLISA 2010 datasets, with consistent improvements over existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 15:58:16 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 10:55:12 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 08:52:55 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Zhou", "Yi", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""], ["Mellor", "Matt", ""]]}, {"id": "1607.04573", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Analyzing features learned for Offline Signature Verification using Deep\n  CNNs", "comments": "Accepted as a conference paper to ICPR 2016", "journal-ref": null, "doi": "10.1109/ICPR.2016.7900092", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on Offline Handwritten Signature Verification explored a large\nvariety of handcrafted feature extractors, ranging from graphology, texture\ndescriptors to interest points. In spite of advancements in the last decades,\nperformance of such systems is still far from optimal when we test the systems\nagainst skilled forgeries - signature forgeries that target a particular\nindividual. In previous research, we proposed a formulation of the problem to\nlearn features from data (signature images) in a Writer-Independent format,\nusing Deep Convolutional Neural Networks (CNNs), seeking to improve performance\non the task. In this research, we push further the performance of such method,\nexploring a range of architectures, and obtaining a large improvement in\nstate-of-the-art performance on the GPDS dataset, the largest publicly\navailable dataset on the task. In the GPDS-160 dataset, we obtained an Equal\nError Rate of 2.74%, compared to 6.97% in the best result published in\nliterature (that used a combination of multiple classifiers). We also present a\nvisual analysis of the feature space learned by the model, and an analysis of\nthe errors made by the classifier. Our analysis shows that the model is very\neffective in separating signatures that have a different global appearance,\nwhile being particularly vulnerable to forgeries that very closely resemble\ngenuine signatures, even if their line quality is bad, which is the case of\nslowly-traced forgeries.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:35:20 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 14:52:55 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1607.04593", "submitter": "Saurabh Prasad", "authors": "Minshan Cui, Saurabh Prasad", "title": "Spatial Context based Angular Information Preserving Projection for\n  Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dimensionality reduction is a crucial preprocessing for hyperspectral data\nanalysis - finding an appropriate subspace is often required for subsequent\nimage classification. In recent work, we proposed supervised angular\ninformation based dimensionality reduction methods to find effective subspaces.\nSince unlabeled data are often more readily available compared to labeled data,\nwe propose an unsupervised projection that finds a lower dimensional subspace\nwhere local angular information is preserved. To exploit spatial information\nfrom the hyperspectral images, we further extend our unsupervised projection to\nincorporate spatial contextual information around each pixel in the image.\nAdditionally, we also propose a sparse representation based classifier which is\noptimized to exploit spatial information during classification - we hence\nassert that our proposed projection is particularly suitable for classifiers\nwhere local similarity and spatial context are both important. Experimental\nresults with two real-world hyperspectral datasets demonstrate that our\nproposed methods provide a robust classification performance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 17:38:34 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Cui", "Minshan", ""], ["Prasad", "Saurabh", ""]]}, {"id": "1607.04609", "submitter": "Saurabh Prasad", "authors": "Saurabh Prasad, Tanu Priya, Minshan Cui, Shishir Shah", "title": "Person Re-identification with Hyperspectral Multi-Camera Systems --- A\n  Pilot Study", "comments": "Accepted for presentation at the 8'th Workshop on Hyperspectral Image\n  and Signal Processing, UCLA, August 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person re-identification in a multi-camera environment is an important part\nof modern surveillance systems. Person re-identification from color images has\nbeen the focus of much active research, due to the numerous challenges posed\nwith such analysis tasks, such as variations in illumination, pose and\nviewpoints. In this paper, we suggest that hyperspectral imagery has the\npotential to provide unique information that is expected to be beneficial for\nthe re-identification task. Specifically, we assert that by accurately\ncharacterizing the unique spectral signature for each person's skin,\nhyperspectral imagery can provide very useful descriptors (e.g. spectral\nsignatures from skin pixels) for re-identification. Towards this end, we\nacquired proof-of-concept hyperspectral re-identification data under\nchallenging (practical) conditions from 15 people. Our results indicate that\nhyperspectral data result in a substantially enhanced re-identification\nperformance compared to color (RGB) images, when using spectral signatures over\nskin as the feature descriptor.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 18:38:38 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Prasad", "Saurabh", ""], ["Priya", "Tanu", ""], ["Cui", "Minshan", ""], ["Shah", "Shishir", ""]]}, {"id": "1607.04648", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Zachary C. Lipton and Serge Belongie and Truong\n  Nguyen", "title": "Context Matters: Refining Object Detection in Video with Recurrent\n  Neural Networks", "comments": "To appear in BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the vast amounts of video available online, and recent breakthroughs in\nobject detection with static images, object detection in video offers a\npromising new frontier. However, motion blur and compression artifacts cause\nsubstantial frame-level variability, even in videos that appear smooth to the\neye. Additionally, video datasets tend to have sparsely annotated frames. We\npresent a new framework for improving object detection in videos that captures\ntemporal context and encourages consistency of predictions. First, we train a\npseudo-labeler, that is, a domain-adapted convolutional neural network for\nobject detection. The pseudo-labeler is first trained individually on the\nsubset of labeled frames, and then subsequently applied to all frames. Then we\ntrain a recurrent neural network that takes as input sequences of\npseudo-labeled frames and optimizes an objective that encourages both accuracy\non the target frame and consistency across consecutive frames. The approach\nincorporates strong supervision of target frames, weak-supervision on context\nframes, and regularization via a smoothness penalty. Our approach achieves mean\nAverage Precision (mAP) of 68.73, an improvement of 7.1 over the strongest\nimage-based baselines for the Youtube-Video Objects dataset. Our experiments\ndemonstrate that neighboring frames can provide valuable information, even\nabsent labels.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 20:02:25 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 03:00:35 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Tripathi", "Subarna", ""], ["Lipton", "Zachary C.", ""], ["Belongie", "Serge", ""], ["Nguyen", "Truong", ""]]}, {"id": "1607.04673", "submitter": "Abhineet Singh", "authors": "Abhineet Singh, Mennatullah Siam and Martin Jagersand", "title": "Unifying Registration based Tracking: A Case Study with Structural\n  Similarity", "comments": "Accepted at WACV 2017. Supplementary available at:\n  http://webdocs.cs.ualberta.ca/~vis/mtf/ssim_supplementary.pdf arXiv admin\n  note: text overlap with arXiv:1603.01292", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adapts a popular image quality measure called structural\nsimilarity for high precision registration based tracking while also\nintroducing a simpler and faster variant of the same. Further, these are\nevaluated comprehensively against existing measures using a unified approach to\nstudy registration based trackers that decomposes them into three constituent\nsub modules - appearance model, state space model and search method. Several\npopular trackers in literature are broken down using this method so that their\ncontributions - as of this paper - are shown to be limited to only one or two\nof these submodules. An open source tracking framework is made available that\nfollows this decomposition closely through extensive use of generic\nprogramming. It is used to perform all experiments on four publicly available\ndatasets so the results are easily reproducible. This framework provides a\nconvenient interface to plug in a new method for any sub module and combine it\nwith existing methods for the other two. It can also serve as a fast and\nflexible solution for practical tracking needs due to its highly efficient\nimplementation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 22:25:46 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 08:19:18 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 04:52:14 GMT"}, {"version": "v4", "created": "Mon, 30 Jan 2017 14:50:49 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Singh", "Abhineet", ""], ["Siam", "Mennatullah", ""], ["Jagersand", "Martin", ""]]}, {"id": "1607.04730", "submitter": "Aykut Erdem", "authors": "Cagdas Bak, Aysun Kocak, Erkut Erdem, Aykut Erdem", "title": "Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational saliency models for still images have gained significant\npopularity in recent years. Saliency prediction from videos, on the other hand,\nhas received relatively little interest from the community. Motivated by this,\nin this work, we study the use of deep learning for dynamic saliency prediction\nand propose the so-called spatio-temporal saliency networks. The key to our\nmodels is the architecture of two-stream networks where we investigate\ndifferent fusion mechanisms to integrate spatial and temporal information. We\nevaluate our models on the DIEM and UCF-Sports datasets and present highly\ncompetitive results against the existing state-of-the-art models. We also carry\nout some experiments on a number of still images from the MIT300 dataset by\nexploiting the optical flow maps predicted from these images. Our results show\nthat considering inherent motion information in this way can be helpful for\nstatic saliency estimation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 11:46:38 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 06:56:11 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Bak", "Cagdas", ""], ["Kocak", "Aysun", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""]]}, {"id": "1607.04731", "submitter": "Ke Yang", "authors": "Ke Yang, Dongsheng Li, Yong Dou, Shaohe Lv, Qiang Wang", "title": "Weakly supervised object detection using pseudo-strong labels", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an import task of computer vision.A variety of methods\nhave been proposed,but methods using the weak labels still do not have a\nsatisfactory result.In this paper,we propose a new framework that using the\nweakly supervised method's output as the pseudo-strong labels to train a\nstrongly supervised model.One weakly supervised method is treated as black-box\nto generate class-specific bounding boxes on train dataset.A de-noise method is\nthen applied to the noisy bounding boxes.Then the de-noised pseudo-strong\nlabels are used to train a strongly object detection network.The whole\nframework is still weakly supervised because the entire process only uses the\nimage-level labels.The experiment results on PASCAL VOC 2007 prove the validity\nof our framework, and we get result 43.4% on mean average precision compared to\n39.5% of the previous best result and 34.5% of the initial\nmethod,respectively.And this frame work is simple and distinct,and is promising\nto be applied to other method easily.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 11:49:18 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Yang", "Ke", ""], ["Li", "Dongsheng", ""], ["Dou", "Yong", ""], ["Lv", "Shaohe", ""], ["Wang", "Qiang", ""]]}, {"id": "1607.04759", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "New version of Gram-Schmidt Process with inverse for Signal and Image\n  Processing", "comments": "12 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gram-Schmidt Process (GSP) is used to convert a non-orthogonal basis (a\nset of linearly independent vectors, matrices, etc) into an orthonormal basis\n(a set of orthogonal, unit-length vectors, bi or tri dimensional matrices). The\nprocess consists of taking each array and then subtracting the projections in\ncommon with the previous arrays. This paper introduces an enhanced version of\nthe Gram-Schmidt Process (EGSP) with inverse, which is useful for Digital\nSignal and Image Processing, among others applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 16:29:15 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1607.04760", "submitter": "Ary Setijadi Prihatmanto", "authors": "Setyaki Sholata Sya, Ary Setijadi Prihatmanto", "title": "Design and implementation of image processing system for Lumen social\n  robot-humanoid as an exhibition guide for Electrical Engineering Days 2015", "comments": "Lumen, image processing system, face detection, face recognition,\n  face tracking, human detection", "journal-ref": null, "doi": "10.13140/RG.2.1.3432.0889/1", "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lumen Social Robot is a humanoid robot development with the purpose that it\ncould be a good friend to all people. In this year, the Lumen Social Robot is\nbeing developed into a guide in the exhibition and in the seminar of the Final\nExam of undergraduate and graduate students in Electrical Engineering ITB,\nnamed Electrical Engineering Days 2015. In order to be the guide in that\noccasion, Lumen is supported by several things. They are Nao robot components,\nservers, and multiple processor systems. The image processing system is a\nprocessing application system that allows Lumen to recognize and determine an\nobject from the image taken from the camera eye. The image processing system is\nprovided with four modules. They are face detection module to detect a person's\nface, face recognition module to recognize a person's face, face tracking\nmodule to follow a person's face, and human detection module to detect humans\nbased on the upper parts of person's body. Face detection module and human\ndetection module are implemented by using the library harcascade.xml on EMGU\nCV. Face recognition module is implemented by adding the database for the face\nthat has been detected and store it in that database. Face tracking module is\nimplemented by using the Smooth Gaussian filter to the image.\n  -----\n  Lumen Sosial Robot merupakan sebuah pengembangan robot humanoid agar dapat\nmenjadi teman bagi banyak orang. Sistem pengolahan citra merupakan sistem\naplikasi pengolah yang bertujuan Lumen dapat mengenali dan mengetahui suatu\nobjek pada citra yang diambil dari camera mata Lumen. System pengolahan citra\ndilengkapi dengan empat buah modul, yaitu modul face detection untuk mendeteksi\nwajah seseorang, modul face recognition untuk mengenali wajah orang tersebut,\nmodul face tracking untuk mengikuti wajah seseorang, dan modul human detection\nuntuk mendeteksi manusia berdasarkan bagian tubuh atas orang\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 16:34:09 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Sya", "Setyaki Sholata", ""], ["Prihatmanto", "Ary Setijadi", ""]]}, {"id": "1607.04773", "submitter": "Achraf Ben-Hamadou", "authors": "Achraf Ben-Hamadou, Christian Daul, Charles Soussen", "title": "Construction of extended 3D field of views of the internal bladder wall\n  surface: a proof of concept", "comments": "23 pages, Springer 3D Research, 2016", "journal-ref": null, "doi": "10.1007/s13319-016-0095-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D extended field of views (FOVs) of the internal bladder wall facilitate\nlesion diagnosis, patient follow-up and treatment traceability. In this paper,\nwe propose a 3D image mosaicing algorithm guided by 2D cystoscopic video-image\nregistration for obtaining textured FOV mosaics. In this feasibility study, the\nregistration makes use of data from a 3D cystoscope prototype providing, in\naddition to each small FOV image, some 3D points located on the surface. This\nproof of concept shows that textured surfaces can be constructed with minimally\nmodified cystoscopes. The potential of the method is demonstrated on numerical\nand real phantoms reproducing various surface shapes. Pig and human bladder\ntextures are superimposed on phantoms with known shape and dimensions. These\ndata allow for quantitative assessment of the 3D mosaicing algorithm based on\nthe registration of images simulating bladder textures.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 17:32:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Ben-Hamadou", "Achraf", ""], ["Daul", "Christian", ""], ["Soussen", "Charles", ""]]}, {"id": "1607.04780", "submitter": "Junwei Liang", "authors": "Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann", "title": "Exploiting Multi-modal Curriculum in Noisy Web Data for Large-scale\n  Concept Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning video concept detectors automatically from the big but noisy web\ndata with no additional manual annotations is a novel but challenging area in\nthe multimedia and the machine learning community. A considerable amount of\nvideos on the web are associated with rich but noisy contextual information,\nsuch as the title, which provides weak annotations or labels about the video\ncontent. To leverage the big noisy web labels, this paper proposes a novel\nmethod called WEbly-Labeled Learning (WELL), which is established on the\nstate-of-the-art machine learning algorithm inspired by the learning process of\nhuman. WELL introduces a number of novel multi-modal approaches to incorporate\nmeaningful prior knowledge called curriculum from the noisy web videos. To\ninvestigate this problem, we empirically study the curriculum constructed from\nthe multi-modal features of the videos collected from YouTube and Flickr. The\nefficacy and the scalability of WELL have been extensively demonstrated on two\npublic benchmarks, including the largest multimedia dataset and the largest\nmanually-labeled video set. The comprehensive experimental results demonstrate\nthat WELL outperforms state-of-the-art studies by a statically significant\nmargin on learning concepts from noisy web video data. In addition, the results\nalso verify that WELL is robust to the level of noisiness in the video data.\nNotably, WELL trained on sufficient noisy web labels is able to achieve a\ncomparable accuracy to supervised learning methods trained on the clean\nmanually-labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 18:14:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Liang", "Junwei", ""], ["Jiang", "Lu", ""], ["Meng", "Deyu", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1607.04889", "submitter": "Yipei Wang", "authors": "Yan Xu, Yang Li, Mingyuan Liu, Yipei Wang, Yubo Fan, Maode Lai, Eric\n  I-Chao Chang", "title": "Gland Instance Segmentation by Deep Multichannel Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new image instance segmentation method that\nsegments individual glands (instances) in colon histology images. This is a\ntask called instance segmentation that has recently become increasingly\nimportant. The problem is challenging since not only do the glands need to be\nsegmented from the complex background, they are also required to be\nindividually identified. Here we leverage the idea of image-to-image prediction\nin recent deep learning by building a framework that automatically exploits and\nfuses complex multichannel information, regional, location and boundary\npatterns in gland histology images. Our proposed system, deep multichannel\nframework, alleviates heavy feature design due to the use of convolutional\nneural networks and is able to meet multifarious requirement by altering\nchannels. Compared to methods reported in the 2015 MICCAI Gland Segmentation\nChallenge and other currently prevalent methods of instance segmentation, we\nobserve state-of-the-art results based on a number of evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 16:18:38 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 15:38:02 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Xu", "Yan", ""], ["Li", "Yang", ""], ["Liu", "Mingyuan", ""], ["Wang", "Yipei", ""], ["Fan", "Yubo", ""], ["Lai", "Maode", ""], ["Chang", "Eric I-Chao", ""]]}, {"id": "1607.04917", "submitter": "Blaine Rister", "authors": "Blaine Rister, Daniel L Rubin", "title": "Piecewise convexity of artificial neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although artificial neural networks have shown great promise in applications\nincluding computer vision and speech recognition, there remains considerable\npractical and theoretical difficulty in optimizing their parameters. The\nseemingly unreasonable success of gradient descent methods in minimizing these\nnon-convex functions remains poorly understood. In this work we offer some\ntheoretical guarantees for networks with piecewise affine activation functions,\nwhich have in recent years become the norm. We prove three main results.\nFirstly, that the network is piecewise convex as a function of the input data.\nSecondly, that the network, considered as a function of the parameters in a\nsingle layer, all others held constant, is again piecewise convex. Finally,\nthat the network as a function of all its parameters is piecewise multi-convex,\na generalization of biconvexity. From here we characterize the local minima and\nstationary points of the training objective, showing that they minimize certain\nsubsets of the parameter space. We then analyze the performance of two\noptimization algorithms on multi-convex problems: gradient descent, and a\nmethod which repeatedly solves a number of convex sub-problems. We prove\nnecessary convergence conditions for the first algorithm and both necessary and\nsufficient conditions for the second, after introducing regularization to the\nobjective. Finally, we remark on the remaining difficulty of the global\noptimization problem. Under the squared error objective, we show that by\nvarying the training data, a single rectifier neuron admits local minima\narbitrarily far apart, both in objective value and parameter space.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 21:49:00 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 06:39:01 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Rister", "Blaine", ""], ["Rubin", "Daniel L", ""]]}, {"id": "1607.04939", "submitter": "Saurabh Prasad", "authors": "Saurabh Prasad, Minshan Cui, Lifeng Yan", "title": "Composite Kernel Local Angular Discriminant Analysis for Multi-Sensor\n  Geospatial Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emergence of passive and active optical sensors available for\ngeospatial imaging, information fusion across sensors is becoming ever more\nimportant. An important aspect of single (or multiple) sensor geospatial image\nanalysis is feature extraction - the process of finding \"optimal\" lower\ndimensional subspaces that adequately characterize class-specific information\nfor subsequent analysis tasks, such as classification, change and anomaly\ndetection etc. In recent work, we proposed and developed an angle-based\ndiscriminant analysis approach that projected data onto subspaces with maximal\n\"angular\" separability in the input (raw) feature space and Reproducible Kernel\nHilbert Space (RKHS). We also developed an angular locality preserving variant\nof this algorithm. In this letter, we advance this work and make it suitable\nfor information fusion - we propose and validate a composite kernel local\nangular discriminant analysis projection, that can operate on an ensemble of\nfeature sources (e.g. from different sources), and project the data onto a\nunified space through composite kernels where the data are maximally separated\nin an angular sense. We validate this method with the multi-sensor University\nof Houston hyperspectral and LiDAR dataset, and demonstrate that the proposed\nmethod significantly outperforms other composite kernel approaches to sensor\n(information) fusion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 02:50:40 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Prasad", "Saurabh", ""], ["Cui", "Minshan", ""], ["Yan", "Lifeng", ""]]}, {"id": "1607.04942", "submitter": "Saurabh Prasad", "authors": "Minshan Cui, Saurabh Prasad", "title": "Sparse Representation-Based Classification: Orthogonal Least Squares or\n  Orthogonal Matching Pursuit?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spare representation of signals has received significant attention in recent\nyears. Based on these developments, a sparse representation-based\nclassification (SRC) has been proposed for a variety of classification and\nrelated tasks, including face recognition. Recently, a class dependent variant\nof SRC was proposed to overcome the limitations of SRC for remote sensing image\nclassification. Traditionally, greedy pursuit based method such as orthogonal\nmatching pursuit (OMP) are used for sparse coefficient recovery due to their\nsimplicity as well as low time-complexity. However, orthogonal least square\n(OLS) has not yet been widely used in classifiers that exploit the sparse\nrepresentation properties of data. Since OLS produces lower signal\nreconstruction error than OMP under similar conditions, we hypothesize that\nmore accurate signal estimation will further improve the classification\nperformance of classifiers that exploiting the sparsity of data. In this paper,\nwe present a classification method based on OLS, which implements OLS in a\nclasswise manner to perform the classification. We also develop and present its\nkernelized variant to handle nonlinearly separable data. Based on two\nreal-world benchmarking hyperspectral datasets, we demonstrate that class\ndependent OLS based methods outperform several baseline methods including\ntraditional SRC and the support vector machine classifier.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 03:05:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Cui", "Minshan", ""], ["Prasad", "Saurabh", ""]]}, {"id": "1607.04965", "submitter": "Huynh Van Luong", "authors": "Huynh Van Luong and Nikos Deligiannis and S{\\o}ren Forchhammer and\n  Andr\\'e Kaup", "title": "Distributed Coding of Multiview Sparse Sources with Joint Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support of applications involving multiview sources in distributed object\nrecognition using lightweight cameras, we propose a new method for the\ndistributed coding of sparse sources as visual descriptor histograms extracted\nfrom multiview images. The problem is challenging due to the computational and\nenergy constraints at each camera as well as the limitations regarding\ninter-camera communication. Our approach addresses these challenges by\nexploiting the sparsity of the visual descriptor histograms as well as their\nintra- and inter-camera correlations. Our method couples distributed source\ncoding of the sparse sources with a new joint recovery algorithm that\nincorporates multiple side information signals, where prior knowledge (low\nquality) of all the sparse sources is initially sent to exploit their\ncorrelations. Experimental evaluation using the histograms of shift-invariant\nfeature transform (SIFT) descriptors extracted from multiview images shows that\nour method leads to bit-rate saving of up to 43% compared to the\nstate-of-the-art distributed compressed sensing method with independent\nencoding of the sources.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 07:41:43 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Van Luong", "Huynh", ""], ["Deligiannis", "Nikos", ""], ["Forchhammer", "S\u00f8ren", ""], ["Kaup", "Andr\u00e9", ""]]}, {"id": "1607.05006", "submitter": "Johannes Ball\\'e", "authors": "Johannes Ball\\'e, Valero Laparra, Eero P. Simoncelli", "title": "End-to-end optimization of nonlinear transform codes for perceptual\n  quality", "comments": "Accepted as a conference contribution to Picture Coding Symposium\n  2016", "journal-ref": "Proc. 32nd Picture Coding Symposium, Nuremberg, Germany, Dec 2016.\n  IEEE Signal Proc Society", "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for end-to-end optimization of the\nrate--distortion performance of nonlinear transform codes assuming scalar\nquantization. The framework can be used to optimize any differentiable pair of\nanalysis and synthesis transforms in combination with any differentiable\nperceptual metric. As an example, we consider a code built from a linear\ntransform followed by a form of multi-dimensional local gain control.\nDistortion is measured with a state-of-the-art perceptual metric. When\noptimized over a large database of images, this representation offers\nsubstantial improvements in bitrate and perceptual appearance over fixed (DCT)\ncodes, and over linear transform codes optimized for mean squared error.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 10:41:17 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 19:41:09 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ball\u00e9", "Johannes", ""], ["Laparra", "Valero", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1607.05046", "submitter": "Zhu Shizhan", "authors": "Shizhan Zhu, Sifei Liu, Chen Change Loy, Xiaoou Tang", "title": "Deep Cascaded Bi-Network for Face Hallucination", "comments": "This paper is to appear in Proceedings of ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel framework for hallucinating faces of unconstrained poses\nand with very low resolution (face size as small as 5pxIOD). In contrast to\nexisting studies that mostly ignore or assume pre-aligned face spatial\nconfiguration (e.g. facial landmarks localization or dense correspondence\nfield), we alternatingly optimize two complementary tasks, namely face\nhallucination and dense correspondence field estimation, in a unified\nframework. In addition, we propose a new gated deep bi-network that contains\ntwo functionality-specialized branches to recover different levels of texture\ndetails. Extensive experiments demonstrate that such formulation allows\nexceptional hallucination quality on in-the-wild low-res faces with significant\npose and illumination variations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 12:39:37 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Zhu", "Shizhan", ""], ["Liu", "Sifei", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1607.05066", "submitter": "Wei Li", "authors": "Wei Li and Matthias Breier and Dorit Merhof", "title": "Recycle deep features for better object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at improving the performance of existing detection algorithms\ndeveloped for different applications, we propose a region regression-based\nmulti-stage class-agnostic detection pipeline, whereby the existing algorithms\nare employed for providing the initial detection proposals. Better detection is\nobtained by exploiting the power of deep learning in the region regress scheme\nwhile avoiding the requirement on a huge amount of reference data for training\ndeep neural networks. Additionally, a novel network architecture with recycled\ndeep features is proposed, which provides superior regression results compared\nto the commonly used architectures. As demonstrated on a data set with ~1200\nsamples of different classes, it is feasible to successfully train a deep\nneural network in our proposed architecture and use it to obtain the desired\ndetection performance. Since only slight modifications are required to common\nnetwork architectures and since the deep neural network is trained using the\nstandard hyperparameters, the proposed detection is well accessible and can be\neasily adopted to a broad variety of detection tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 13:42:56 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Li", "Wei", ""], ["Breier", "Matthias", ""], ["Merhof", "Dorit", ""]]}, {"id": "1607.05074", "submitter": "Christian Rupprecht", "authors": "Christian Rupprecht, Elizabeth Huaroc, Maximilian Baust, Nassir Navab", "title": "Deep Active Contours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for interactive boundary extraction which combines a\ndeep, patch-based representation with an active contour framework. We train a\nclass-specific convolutional neural network which predicts a vector pointing\nfrom the respective point on the evolving contour towards the closest point on\nthe boundary of the object of interest. These predictions form a vector field\nwhich is then used for evolving the contour by the Sobolev active contour\nframework proposed by Sundaramoorthi et al. The resulting interactive\nsegmentation method is very efficient in terms of required computational\nresources and can even be trained on comparatively small graphics cards. We\nevaluate the potential of the proposed method on both medical and non-medical\nchallenge data sets, such as the STACOM data set and the PASCAL VOC 2012 data\nset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 13:53:29 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Rupprecht", "Christian", ""], ["Huaroc", "Elizabeth", ""], ["Baust", "Maximilian", ""], ["Navab", "Nassir", ""]]}, {"id": "1607.05140", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do, Anh-Dzung Doan, Ngai-Man Cheung", "title": "Learning to Hash with Binary Deep Neural Network", "comments": "Appearing in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes deep network models and learning algorithms for\nunsupervised and supervised binary hashing. Our novel network design constrains\none hidden layer to directly output the binary codes. This addresses a\nchallenging issue in some previous works: optimizing non-smooth objective\nfunctions due to binarization. Moreover, we incorporate independence and\nbalance properties in the direct and strict forms in the learning. Furthermore,\nwe include similarity preserving property in our objective function. Our\nresulting optimization with these binary, independence, and balance constraints\nis difficult to solve. We propose to attack it with alternating optimization\nand careful relaxation. Experimental results on three benchmark datasets show\nthat our proposed methods compare favorably with the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:48:58 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Doan", "Anh-Dzung", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1607.05177", "submitter": "Aidean Sharghi", "authors": "Aidean Sharghi, Boqing Gong, Mubarak Shah", "title": "Query-Focused Extractive Video Summarization", "comments": "Accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video data is explosively growing. As a result of the \"big video data\",\nintelligent algorithms for automatic video summarization have re-emerged as a\npressing need. We develop a probabilistic model, Sequential and Hierarchical\nDeterminantal Point Process (SH-DPP), for query-focused extractive video\nsummarization. Given a user query and a long video sequence, our algorithm\nreturns a summary by selecting key shots from the video. The decision to\ninclude a shot in the summary depends on the shot's relevance to the user query\nand importance in the context of the video, jointly. We verify our approach on\ntwo densely annotated video datasets. The query-focused video summarization is\nparticularly useful for search engines, e.g., to display snippets of videos.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:49:19 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Sharghi", "Aidean", ""], ["Gong", "Boqing", ""], ["Shah", "Mubarak", ""]]}, {"id": "1607.05194", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei and Nicolas Guizard and Nicolas Chapados and Yoshua\n  Bengio", "title": "HeMIS: Hetero-Modal Image Segmentation", "comments": "Accepted as an oral presentation at MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a deep learning image segmentation framework that is extremely\nrobust to missing imaging modalities. Instead of attempting to impute or\nsynthesize missing data, the proposed approach learns, for each modality, an\nembedding of the input image into a single latent vector space for which\narithmetic operations (such as taking the mean) are well defined. Points in\nthat space, which are averaged over modalities available at inference time, can\nthen be further processed to yield the desired segmentation. As such, any\ncombinatorial subset of available modalities can be provided as input, without\nhaving to learn a combinatorial number of imputation models. Evaluated on two\nneurological MRI datasets (brain tumors and MS lesions), the approach yields\nstate-of-the-art segmentation results when provided with all modalities;\nmoreover, its performance degrades remarkably gracefully when modalities are\nremoved, significantly more so than alternative mean-filling or other synthesis\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 17:11:57 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Havaei", "Mohammad", ""], ["Guizard", "Nicolas", ""], ["Chapados", "Nicolas", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1607.05208", "submitter": "Jurandy Almeida", "authors": "Leonardo A. Duarte, Ot\\'avio A. B. Penatti, and Jurandy Almeida", "title": "Bag of Attributes for Video Event Retrieval", "comments": null, "journal-ref": "in 2018 31st SIBGRAPI Conference on Graphics, Patterns and Images\n  (SIBGRAPI), Foz do Igua\\c{c}u, Brazil, 2018, pp. 447-454", "doi": "10.1109/SIBGRAPI.2018.00064", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Bag-of-Attributes (BoA) model for video\nrepresentation aiming at video event retrieval. The BoA model is based on a\nsemantic feature space for representing videos, resulting in high-level video\nfeature vectors. For creating a semantic space, i.e., the attribute space, we\ncan train a classifier using a labeled image dataset, obtaining a\nclassification model that can be understood as a high-level codebook. This\nmodel is used to map low-level frame vectors into high-level vectors (e.g.,\nclassifier probability scores). Then, we apply pooling operations to the frame\nvectors to create the final bag of attributes for the video. In the BoA\nrepresentation, each dimension corresponds to one category (or attribute) of\nthe semantic space. Other interesting properties are: compactness, flexibility\nregarding the classifier, and ability to encode multiple semantic concepts in a\nsingle video representation. Our experiments considered the semantic space\ncreated by state-of-the-art convolutional neural networks pre-trained on 1000\nobject categories of ImageNet. Such deep neural networks were used to classify\neach video frame and then different coding strategies were used to encode the\nprobability distribution from the softmax layer into a frame vector. Next,\ndifferent pooling strategies were used to combine frame vectors in the BoA\nrepresentation for a video. Results using BoA were comparable or superior to\nthe baselines in the task of video event retrieval using the EVVE dataset, with\nthe advantage of providing a much more compact representation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 17:24:23 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 13:47:31 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Duarte", "Leonardo A.", ""], ["Penatti", "Ot\u00e1vio A. B.", ""], ["Almeida", "Jurandy", ""]]}, {"id": "1607.05258", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei and Nicolas Guizard and Hugo Larochelle and\n  Pierre-Marc Jodoin", "title": "Deep learning trends for focal brain pathology segmentation in MRI", "comments": "Published in Machine Learning for Health Informatics", "journal-ref": null, "doi": "10.1007/978-3-319-50478-0_6", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmentation of focal (localized) brain pathologies such as brain tumors and\nbrain lesions caused by multiple sclerosis and ischemic strokes are necessary\nfor medical diagnosis, surgical planning and disease development as well as\nother applications such as tractography. Over the years, attempts have been\nmade to automate this process for both clinical and research reasons. In this\nregard, machine learning methods have long been a focus of attention. Over the\npast two years, the medical imaging field has seen a rise in the use of a\nparticular branch of machine learning commonly known as deep learning. In the\nnon-medical computer vision world, deep learning based methods have obtained\nstate-of-the-art results on many datasets. Recent studies in computer aided\ndiagnostics have shown deep learning methods (and especially convolutional\nneural networks - CNN) to yield promising results. In this chapter, we provide\na survey of CNN methods applied to medical imaging with a focus on brain\npathology segmentation. In particular, we discuss their characteristic\npeculiarities and their specific configuration and adjustments that are best\nsuited to segment medical images. We also underline the intrinsic differences\ndeep learning methods have with other machine learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 19:52:00 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 16:41:46 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 02:44:48 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Havaei", "Mohammad", ""], ["Guizard", "Nicolas", ""], ["Larochelle", "Hugo", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "1607.05338", "submitter": "Joseph DeGol", "authors": "Joseph DeGol, Mani Golparvar-Fard, Derek Hoiem", "title": "Geometry-Informed Material Recognition", "comments": "IEEE Conference on Computer Vision and Pattern Recognition 2016 (CVPR\n  '16)", "journal-ref": null, "doi": "10.1109/CVPR.2016.172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to recognize material categories using images and geometry\ninformation. In many applications, such as construction management, coarse\ngeometry information is available. We investigate how 3D geometry (surface\nnormals, camera intrinsic and extrinsic parameters) can be used with 2D\nfeatures (texture and color) to improve material classification. We introduce a\nnew dataset, GeoMat, which is the first to provide both image and geometry data\nin the form of: (i) training and testing patches that were extracted at\ndifferent scales and perspectives from real world examples of each material\ncategory, and (ii) a large scale construction site scene that includes 160\nimages and over 800,000 hand labeled 3D points. Our results show that using 2D\nand 3D features both jointly and independently to model materials improves\nclassification accuracy across multiple scales and viewing directions for both\nmaterial patches and images of a large scale construction site scene.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 22:15:49 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["DeGol", "Joseph", ""], ["Golparvar-Fard", "Mani", ""], ["Hoiem", "Derek", ""]]}, {"id": "1607.05369", "submitter": "Weihua Chen", "authors": "Weihua Chen, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang", "title": "A Multi-task Deep Network for Person Re-identification", "comments": "Accepted by AAAI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) focuses on identifying people across\ndifferent scenes in video surveillance, which is usually formulated as a binary\nclassification task or a ranking task in current person ReID approaches. In\nthis paper, we take both tasks into account and propose a multi-task deep\nnetwork (MTDnet) that makes use of their own advantages and jointly optimize\nthe two tasks simultaneously for person ReID. To the best of our knowledge, we\nare the first to integrate both tasks in one network to solve the person ReID.\nWe show that our proposed architecture significantly boosts the performance.\nFurthermore, deep architecture in general requires a sufficient dataset for\ntraining, which is usually not met in person ReID. To cope with this situation,\nwe further extend the MTDnet and propose a cross-domain architecture that is\ncapable of using an auxiliary set to assist training on small target sets. In\nthe experiments, our approach outperforms most of existing person ReID\nalgorithms on representative datasets including CUHK03, CUHK01, VIPeR, iLIDS\nand PRID2011, which clearly demonstrates the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 01:59:02 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 14:32:38 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 06:22:57 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Chen", "Weihua", ""], ["Chen", "Xiaotang", ""], ["Zhang", "Jianguo", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1607.05387", "submitter": "Hanock Kwak", "authors": "Hanock Kwak, Byoung-Tak Zhang", "title": "Generating Images Part by Part with Composite Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation remains a fundamental problem in artificial intelligence in\ngeneral and deep learning in specific. The generative adversarial network (GAN)\nwas successful in generating high quality samples of natural images. We propose\na model called composite generative adversarial network, that reveals the\ncomplex structure of images with multiple generators in which each generator\ngenerates some part of the image. Those parts are combined by alpha blending\nprocess to create a new single image. It can generate, for example, background\nand face sequentially with two generators, after training on face dataset.\nTraining was done in an unsupervised way without any labels about what each\ngenerator should generate. We found possibilities of learning the structure by\nusing this generative model empirically.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 03:09:31 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 07:32:35 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kwak", "Hanock", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1607.05396", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do, Anh-Dzung Doan, Duc-Thanh Nguyen, Ngai-Man Cheung", "title": "Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian", "comments": "Appearing in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes two approaches for inferencing binary codes in two-step\n(supervised, unsupervised) hashing. We first introduce an unified formulation\nfor both supervised and unsupervised hashing. Then, we cast the learning of one\nbit as a Binary Quadratic Problem (BQP). We propose two approaches to solve\nBQP. In the first approach, we relax BQP as a semidefinite programming problem\nwhich its global optimum can be achieved. We theoretically prove that the\nobjective value of the binary solution achieved by this approach is well\nbounded. In the second approach, we propose an augmented Lagrangian based\napproach to solve BQP directly without relaxing the binary constraint.\nExperimental results on three benchmark datasets show that our proposed methods\ncompare favorably with the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 04:20:24 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Doan", "Anh-Dzung", ""], ["Nguyen", "Duc-Thanh", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1607.05418", "submitter": "Soheil Hashemi", "authors": "Hokchhay Tann, Soheil Hashemi, R. Iris Bahar, Sherief Reda", "title": "Runtime Configurable Deep Neural Networks for Energy-Accuracy Trade-off", "comments": null, "journal-ref": null, "doi": "10.1145/2968456.2968458", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dynamic configuration technique for deep neural networks\nthat permits step-wise energy-accuracy trade-offs during runtime. Our\nconfiguration technique adjusts the number of channels in the network\ndynamically depending on response time, power, and accuracy targets. To enable\nthis dynamic configuration technique, we co-design a new training algorithm,\nwhere the network is incrementally trained such that the weights in channels\ntrained in earlier steps are fixed. Our technique provides the flexibility of\nmultiple networks while storing and utilizing one set of weights. We evaluate\nour techniques using both an ASIC-based hardware accelerator as well as a\nlow-power embedded GPGPU and show that our approach leads to only a small or\nnegligible loss in the final network accuracy. We analyze the performance of\nour proposed methodology using three well-known networks for MNIST, CIFAR-10,\nand SVHN datasets, and we show that we are able to achieve up to 95% energy\nreduction with less than 1% accuracy loss across the three benchmarks. In\naddition, compared to prior work on dynamic network reconfiguration, we show\nthat our approach leads to approximately 50% savings in storage requirements,\nwhile achieving similar accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:27:05 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 20:42:51 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Tann", "Hokchhay", ""], ["Hashemi", "Soheil", ""], ["Bahar", "R. Iris", ""], ["Reda", "Sherief", ""]]}, {"id": "1607.05423", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Xiaotong Yuan, Jiashi Feng, Shuicheng Yan", "title": "Training Skinny Deep Neural Networks with Iterative Hard Thresholding\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable success in a wide range of\npractical problems. However, due to the inherent large parameter space, deep\nmodels are notoriously prone to overfitting and difficult to be deployed in\nportable devices with limited memory. In this paper, we propose an iterative\nhard thresholding (IHT) approach to train Skinny Deep Neural Networks (SDNNs).\nAn SDNN has much fewer parameters yet can achieve competitive or even better\nperformance than its full CNN counterpart. More concretely, the IHT approach\ntrains an SDNN through following two alternative phases: (I) perform hard\nthresholding to drop connections with small activations and fine-tune the other\nsignificant filters; (II)~re-activate the frozen connections and train the\nentire network to improve its overall discriminative capability. We verify the\nsuperiority of SDNNs in terms of efficiency and classification performance on\nfour benchmark object recognition datasets, including CIFAR-10, CIFAR-100,\nMNIST and ImageNet. Experimental results clearly demonstrate that IHT can be\napplied for training SDNN based on various CNN architectures such as NIN and\nAlexNet.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:41:31 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Jin", "Xiaojie", ""], ["Yuan", "Xiaotong", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1607.05427", "submitter": "Dacheng Tao", "authors": "Changxing Ding and Dacheng Tao", "title": "Trunk-Branch Ensemble Convolutional Neural Networks for Video-based Face\n  Recognition", "comments": "Accepted Version to IEEE T-PAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2700390", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human faces in surveillance videos often suffer from severe image blur,\ndramatic pose variations, and occlusion. In this paper, we propose a\ncomprehensive framework based on Convolutional Neural Networks (CNN) to\novercome challenges in video-based face recognition (VFR). First, to learn\nblur-robust face representations, we artificially blur training data composed\nof clear still images to account for a shortfall in real-world video training\ndata. Using training data composed of both still images and artificially\nblurred data, CNN is encouraged to learn blur-insensitive features\nautomatically. Second, to enhance robustness of CNN features to pose variations\nand occlusion, we propose a Trunk-Branch Ensemble CNN model (TBE-CNN), which\nextracts complementary information from holistic face images and patches\ncropped around facial components. TBE-CNN is an end-to-end model that extracts\nfeatures efficiently by sharing the low- and middle-level convolutional layers\nbetween the trunk and branch networks. Third, to further promote the\ndiscriminative power of the representations learnt by TBE-CNN, we propose an\nimproved triplet loss function. Systematic experiments justify the\neffectiveness of the proposed techniques. Most impressively, TBE-CNN achieves\nstate-of-the-art performance on three popular video face databases: PaSC, COX\nFace, and YouTube Faces. With the proposed techniques, we also obtain the first\nplace in the BTAS 2016 Video Person Recognition Evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 07:14:28 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 09:12:19 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Ding", "Changxing", ""], ["Tao", "Dacheng", ""]]}, {"id": "1607.05440", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Yunpeng Chen, Jian Dong, Jiashi Feng, Shuicheng Yan", "title": "Collaborative Layer-wise Discriminative Learning in Deep Neural Networks", "comments": "To appear in ECCV 2016. Maybe subject to minor changes before\n  camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermediate features at different layers of a deep neural network are known\nto be discriminative for visual patterns of different complexities. However,\nmost existing works ignore such cross-layer heterogeneities when classifying\nsamples of different complexities. For example, if a training sample has\nalready been correctly classified at a specific layer with high confidence, we\nargue that it is unnecessary to enforce rest layers to classify this sample\ncorrectly and a better strategy is to encourage those layers to focus on other\nsamples.\n  In this paper, we propose a layer-wise discriminative learning method to\nenhance the discriminative capability of a deep network by allowing its layers\nto work collaboratively for classification. Towards this target, we introduce\nmultiple classifiers on top of multiple layers. Each classifier not only tries\nto correctly classify the features from its input layer, but also coordinates\nwith other classifiers to jointly maximize the final classification\nperformance. Guided by the other companion classifiers, each classifier learns\nto concentrate on certain training examples and boosts the overall performance.\nAllowing for end-to-end training, our method can be conveniently embedded into\nstate-of-the-art deep networks. Experiments with multiple popular deep\nnetworks, including Network in Network, GoogLeNet and VGGNet, on scale-various\nobject classification benchmarks, including CIFAR100, MNIST and ImageNet, and\nscene classification benchmarks, including MIT67, SUN397 and Places205,\ndemonstrate the effectiveness of our method. In addition, we also analyze the\nrelationship between the proposed method and classical conditional random\nfields models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 07:56:37 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Jin", "Xiaojie", ""], ["Chen", "Yunpeng", ""], ["Dong", "Jian", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1607.05447", "submitter": "Stephen Gould", "authors": "Stephen Gould and Basura Fernando and Anoop Cherian and Peter Anderson\n  and Rodrigo Santa Cruz and Edison Guo", "title": "On Differentiating Parameterized Argmin and Argmax Problems with\n  Application to Bi-level Optimization", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some recent works in machine learning and computer vision involve the\nsolution of a bi-level optimization problem. Here the solution of a\nparameterized lower-level problem binds variables that appear in the objective\nof an upper-level problem. The lower-level problem typically appears as an\nargmin or argmax optimization problem. Many techniques have been proposed to\nsolve bi-level optimization problems, including gradient descent, which is\npopular with current end-to-end learning approaches. In this technical report\nwe collect some results on differentiating argmin and argmax optimization\nproblems with and without constraints and provide some insightful motivating\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 08:09:30 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 03:43:35 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Gould", "Stephen", ""], ["Fernando", "Basura", ""], ["Cherian", "Anoop", ""], ["Anderson", "Peter", ""], ["Cruz", "Rodrigo Santa", ""], ["Guo", "Edison", ""]]}, {"id": "1607.05477", "submitter": "Dong Chen", "authors": "Dong Chen, Gang Hua, Fang Wen, Jian Sun", "title": "Supervised Transformer Network for Efficient Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pose variations remain to be a challenge that confronts real-word face\ndetection. We propose a new cascaded Convolutional Neural Network, dubbed the\nname Supervised Transformer Network, to address this challenge. The first stage\nis a multi-task Region Proposal Network (RPN), which simultaneously predicts\ncandidate face regions along with associated facial landmarks. The candidate\nregions are then warped by mapping the detected facial landmarks to their\ncanonical positions to better normalize the face patterns. The second stage,\nwhich is a RCNN, then verifies if the warped candidate regions are valid faces\nor not. We conduct end-to-end learning of the cascaded network, including\noptimizing the canonical positions of the facial landmarks. This supervised\nlearning of the transformations automatically selects the best scale to\ndifferentiate face/non-face patterns. By combining feature maps from both\nstages of the network, we achieve state-of-the-art detection accuracies on\nseveral public benchmarks. For real-time performance, we run the cascaded\nnetwork only on regions of interests produced from a boosting cascade face\ndetector. Our detector runs at 30 FPS on a single CPU core for a VGA-resolution\nimage.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 09:15:38 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chen", "Dong", ""], ["Hua", "Gang", ""], ["Wen", "Fang", ""], ["Sun", "Jian", ""]]}, {"id": "1607.05523", "submitter": "Muhammad Usman Ghani", "authors": "Muhammad Usman Ghani, Ertunc Erdil, Sumeyra Demir Kanik, Ali Ozgur\n  Argunsah, Anna Felicity Hobbiss, Inbal Israely, Devrim Unay, Tolga Tasdizen,\n  Mujdat Cetin", "title": "Dendritic Spine Shape Analysis: A Clustering Perspective", "comments": "Accepted for BioImageComputing workshop at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional properties of neurons are strongly coupled with their morphology.\nChanges in neuronal activity alter morphological characteristics of dendritic\nspines. First step towards understanding the structure-function relationship is\nto group spines into main spine classes reported in the literature. Shape\nanalysis of dendritic spines can help neuroscientists understand the underlying\nrelationships. Due to unavailability of reliable automated tools, this analysis\nis currently performed manually which is a time-intensive and subjective task.\nSeveral studies on spine shape classification have been reported in the\nliterature, however, there is an on-going debate on whether distinct spine\nshape classes exist or whether spines should be modeled through a continuum of\nshape variations. Another challenge is the subjectivity and bias that is\nintroduced due to the supervised nature of classification approaches. In this\npaper, we aim to address these issues by presenting a clustering perspective.\nIn this context, clustering may serve both confirmation of known patterns and\ndiscovery of new ones. We perform cluster analysis on two-photon microscopic\nimages of spines using morphological, shape, and appearance based features and\ngain insights into the spine shape analysis problem. We use histogram of\noriented gradients (HOG), disjunctive normal shape models (DNSM), morphological\nfeatures, and intensity profile based features for cluster analysis. We use\nx-means to perform cluster analysis that selects the number of clusters\nautomatically using the Bayesian information criterion (BIC). For all features,\nthis analysis produces 4 clusters and we observe the formation of at least one\ncluster consisting of spines which are difficult to be assigned to a known\nclass. This observation supports the argument of intermediate shape types.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 11:18:52 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Ghani", "Muhammad Usman", ""], ["Erdil", "Ertunc", ""], ["Kanik", "Sumeyra Demir", ""], ["Argunsah", "Ali Ozgur", ""], ["Hobbiss", "Anna Felicity", ""], ["Israely", "Inbal", ""], ["Unay", "Devrim", ""], ["Tasdizen", "Tolga", ""], ["Cetin", "Mujdat", ""]]}, {"id": "1607.05529", "submitter": "Haomiao Liu", "authors": "Haomiao Liu, Ruiping Wang, Shiguang Shan, Xilin Chen", "title": "Dual Purpose Hashing", "comments": "With supplementary materials added to the end", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen more and more demand for a unified framework to\naddress multiple realistic image retrieval tasks concerning both category and\nattributes. Considering the scale of modern datasets, hashing is favorable for\nits low complexity. However, most existing hashing methods are designed to\npreserve one single kind of similarity, thus improper for dealing with the\ndifferent tasks simultaneously. To overcome this limitation, we propose a new\nhashing method, named Dual Purpose Hashing (DPH), which jointly preserves the\ncategory and attribute similarities by exploiting the Convolutional Neural\nNetwork (CNN) models to hierarchically capture the correlations between\ncategory and attributes. Since images with both category and attribute labels\nare scarce, our method is designed to take the abundant partially labelled\nimages on the Internet as training inputs. With such a framework, the binary\ncodes of new-coming images can be readily obtained by quantizing the network\noutputs of a binary-like layer, and the attributes can be recovered from the\ncodes easily. Experiments on two large-scale datasets show that our dual\npurpose hash codes can achieve comparable or even better performance than those\nstate-of-the-art methods specifically designed for each individual retrieval\ntask, while being more compact than the compared methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 11:37:00 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Liu", "Haomiao", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1607.05620", "submitter": "Alina Marcu B.Sc", "authors": "Alina Elena Marcu", "title": "A Local-Global Approach to Semantic Segmentation in Aerial Images", "comments": "50 pages, 18 figures. Master's Thesis, University Politehnica of\n  Bucharest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial images are often taken under poor lighting conditions and contain low\nresolution objects, many times occluded by other objects. In this domain,\nvisual context could be of great help, but there are still very few papers that\nconsider context in aerial image understanding and still remains an open\nproblem in computer vision. We propose a dual-stream deep neural network that\nprocesses information along two independent pathways. Our model learns to\ncombine local and global appearance in a complementary way, such that together\nform a powerful classifier. We test our dual-stream network on the task of\nbuildings segmentation in aerial images and obtain state-of-the-art results on\nthe Massachusetts Buildings Dataset. We study the relative importance of local\nappearance versus the larger scene, as well as their performance in combination\non three new buildings datasets. We clearly demonstrate the effectiveness of\nvisual context in conjunction with deep neural networks for aerial image\nunderstanding.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:02:57 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Marcu", "Alina Elena", ""]]}, {"id": "1607.05691", "submitter": "Francois Chollet", "authors": "Fran\\c{c}ois Chollet", "title": "Information-theoretical label embeddings for large-scale image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training multi-label, massively multi-class image\nclassification models, that is faster and more accurate than supervision via a\nsigmoid cross-entropy loss (logistic regression). Our method consists in\nembedding high-dimensional sparse labels onto a lower-dimensional dense sphere\nof unit-normed vectors, and treating the classification problem as a cosine\nproximity regression problem on this sphere. We test our method on a dataset of\n300 million high-resolution images with 17,000 labels, where it yields\nconsiderably faster convergence, as well as a 7% higher mean average precision\ncompared to logistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 18:40:01 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chollet", "Fran\u00e7ois", ""]]}, {"id": "1607.05695", "submitter": "Vishakh Hegde", "authors": "Vishakh Hegde, Reza Zadeh", "title": "FusionNet: 3D Object Classification Using Multiple Data Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality 3D object recognition is an important component of many vision\nand robotics systems. We tackle the object recognition problem using two data\nrepresentations, to achieve leading results on the Princeton ModelNet\nchallenge. The two representations: 1. Volumetric representation: the 3D object\nis discretized spatially as binary voxels - $1$ if the voxel is occupied and\n$0$ otherwise. 2. Pixel representation: the 3D object is represented as a set\nof projected 2D pixel images. Current leading submissions to the ModelNet\nChallenge use Convolutional Neural Networks (CNNs) on pixel representations.\nHowever, we diverge from this trend and additionally, use Volumetric CNNs to\nbridge the gap between the efficiency of the above two representations. We\ncombine both representations and exploit them to learn new features, which\nyield a significantly better classifier than using either of the\nrepresentations in isolation. To do this, we introduce new Volumetric CNN\n(V-CNN) architectures.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 18:50:23 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 01:30:59 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 00:58:55 GMT"}, {"version": "v4", "created": "Sun, 27 Nov 2016 01:00:22 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Hegde", "Vishakh", ""], ["Zadeh", "Reza", ""]]}, {"id": "1607.05781", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Zhi Zhang, Chen Huang, Zhihai He, Xiaobo Ren, Haohong\n  Wang", "title": "Spatially Supervised Recurrent Convolutional Neural Networks for Visual\n  Object Tracking", "comments": "10 pages, 9 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new approach of spatially supervised recurrent\nconvolutional neural networks for visual object tracking. Our recurrent\nconvolutional network exploits the history of locations as well as the\ndistinctive visual features learned by the deep neural networks. Inspired by\nrecent bounding box regression methods for object detection, we study the\nregression capability of Long Short-Term Memory (LSTM) in the temporal domain,\nand propose to concatenate high-level visual features produced by convolutional\nnetworks with region information. In contrast to existing deep learning based\ntrackers that use binary classification for region candidates, we use\nregression for direct prediction of the tracking locations both at the\nconvolutional layer and at the recurrent unit. Our extensive experimental\nresults and performance comparison with state-of-the-art tracking methods on\nchallenging benchmark video tracking datasets shows that our tracker is more\naccurate and robust while maintaining low computational cost. For most test\nvideo sequences, our method achieves the best tracking performance, often\noutperforms the second best by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 23:27:56 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Ning", "Guanghan", ""], ["Zhang", "Zhi", ""], ["Huang", "Chen", ""], ["He", "Zhihai", ""], ["Ren", "Xiaobo", ""], ["Wang", "Haohong", ""]]}, {"id": "1607.05836", "submitter": "Jiaping Zhao", "authors": "Jiaping Zhao and Laurent Itti", "title": "Improved Deep Learning of Object Category using Pose Information", "comments": "10 pages, accepted by WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent progress, the best available computer vision\nalgorithms still lag far behind human capabilities, even for recognizing\nindividual discrete objects under various poses, illuminations, and\nbackgrounds. Here we present a new approach to using object pose information to\nimprove deep network learning. While existing large-scale datasets, e.g.\nImageNet, do not have pose information, we leverage the newly published\nturntable dataset, iLab-20M, which has ~22M images of 704 object instances shot\nunder different lightings, camera viewpoints and turntable rotations, to do\nmore controlled object recognition experiments. We introduce a new\nconvolutional neural network architecture, what/where CNN (2W-CNN), built on a\nlinear-chain feedforward CNN (e.g., AlexNet), augmented by hierarchical layers\nregularized by object poses. Pose information is only used as feedback signal\nduring training, in addition to category information; during test, the\nfeedforward network only predicts category. To validate the approach, we train\nboth 2W-CNN and AlexNet using a fraction of the dataset, and 2W-CNN achieves 6%\nperformance improvement in category prediction. We show mathematically that\n2W-CNN has inherent advantages over AlexNet under the stochastic gradient\ndescent (SGD) optimization procedure. Further more, we fine-tune object\nrecognition on ImageNet by using the pretrained 2W-CNN and AlexNet features on\niLab-20M, results show that significant improvements have been achieved,\ncompared with training AlexNet from scratch. Moreover, fine-tuning 2W-CNN\nfeatures performs even better than fine-tuning the pretrained AlexNet features.\nThese results show pretrained features on iLab- 20M generalizes well to natural\nimage datasets, and 2WCNN learns even better features for object recognition\nthan AlexNet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:11:08 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 19:07:10 GMT"}, {"version": "v3", "created": "Sun, 22 Jan 2017 23:53:15 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Zhao", "Jiaping", ""], ["Itti", "Laurent", ""]]}, {"id": "1607.05839", "submitter": "Xiao Guobao", "authors": "Guobao Xiao, Hanzi Wang, Yan Yan and David Suter", "title": "Superpixel-based Two-view Deterministic Fitting for Multiple-structure\n  Data", "comments": "Accepted by European Conference on Computer Vision (ECCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a two-view deterministic geometric model fitting method,\ntermed Superpixel-based Deterministic Fitting (SDF), for multiple-structure\ndata. SDF starts from superpixel segmentation, which effectively captures prior\ninformation of feature appearances. The feature appearances are beneficial to\nreduce the computational complexity for deterministic fitting methods. SDF also\nincludes two original elements, i.e., a deterministic sampling algorithm and a\nnovel model selection algorithm. The two algorithms are tightly coupled to\nboost the performance of SDF in both speed and accuracy. Specifically, the\nproposed sampling algorithm leverages the grouping cues of superpixels to\ngenerate reliable and consistent hypotheses. The proposed model selection\nalgorithm further makes use of desirable properties of the generated\nhypotheses, to improve the conventional fit-and-remove framework for more\nefficient and effective performance. The key characteristic of SDF is that it\ncan efficiently and deterministically estimate the parameters of model\ninstances in multi-structure data. Experimental results demonstrate that the\nproposed SDF shows superiority over several state-of-the-art fitting methods\nfor real images with single-structure and multiple-structure data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:28:49 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Xiao", "Guobao", ""], ["Wang", "Hanzi", ""], ["Yan", "Yan", ""], ["Suter", "David", ""]]}, {"id": "1607.05851", "submitter": "Jiaping Zhao", "authors": "Jiaping Zhao, Chin-kai Chang and Laurent Itti", "title": "Learning to Recognize Objects by Retaining other Factors of Variation", "comments": "9 pages, accepted by WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images are generated under many factors, including shape, pose,\nillumination etc. Most existing ConvNets formulate object recognition from\nnatural images as a single task classification problem, and attempt to learn\nfeatures useful for object categories, but invariant to other factors of\nvariation as much as possible. These architectures do not explicitly learn\nother factors, like pose and lighting, instead, they usually discard them by\npooling and normalization. In this work, we take the opposite approach: we\ntrain ConvNets for object recognition by retaining other factors (pose in our\ncase) and learn them jointly with object category. We design a new multi-task\nleaning (MTL) ConvNet, named disentangling CNN (disCNN), which explicitly\nenforces the disentangled representations of object identity and pose, and is\ntrained to predict object categories and pose transformations. We show that\ndisCNN achieves significantly better object recognition accuracies than AlexNet\ntrained solely to predict object categories on the iLab-20M dataset, which is a\nlarge scale turntable dataset with detailed object pose and lighting\ninformation. We further show that the pretrained disCNN/AlexNet features on\niLab- 20M generalize to object recognition on both Washington RGB-D and\nImageNet datasets, and the pretrained disCNN features are significantly better\nthan the pretrained AlexNet features for fine-tuning object recognition on the\nImageNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:58:57 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 19:05:35 GMT"}, {"version": "v3", "created": "Sun, 22 Jan 2017 23:56:42 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Zhao", "Jiaping", ""], ["Chang", "Chin-kai", ""], ["Itti", "Laurent", ""]]}, {"id": "1607.05910", "submitter": "Chunhua Shen", "authors": "Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, Anton van\n  den Hengel", "title": "Visual Question Answering: A Survey of Methods and Datasets", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a challenging task that has received\nincreasing attention from both the computer vision and the natural language\nprocessing communities. Given an image and a question in natural language, it\nrequires reasoning over visual elements of the image and general knowledge to\ninfer the correct answer. In the first part of this survey, we examine the\nstate of the art by comparing modern approaches to the problem. We classify\nmethods by their mechanism to connect the visual and textual modalities. In\nparticular, we examine the common approach of combining convolutional and\nrecurrent neural networks to map images and questions to a common feature\nspace. We also discuss memory-augmented and modular architectures that\ninterface with structured knowledge bases. In the second part of this survey,\nwe review the datasets available for training and evaluating VQA systems. The\nvarious datatsets contain questions at different levels of complexity, which\nrequire different capabilities and types of reasoning. We examine in depth the\nquestion/answer pairs from the Visual Genome project, and evaluate the\nrelevance of the structured annotations of images with scene graphs for VQA.\nFinally, we discuss promising future directions for the field, in particular\nthe connection to structured knowledge bases and the use of natural language\nprocessing models.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 10:53:29 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Wu", "Qi", ""], ["Teney", "Damien", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1607.05947", "submitter": "Han Gong", "authors": "Graham D. Finlayson, Han Gong, Robert B. Fisher", "title": "Color Homography Color Correction", "comments": "Accepted by Color Imaging Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homographies -- a mathematical formalism for relating image points across\ndifferent camera viewpoints -- are at the foundations of geometric methods in\ncomputer vision and are used in geometric camera calibration, image\nregistration, and stereo vision and other tasks. In this paper, we show the\nsurprising result that colors across a change in viewing condition (changing\nlight color, shading and camera) are also related by a homography. We propose a\nnew color correction method based on color homography. Experiments demonstrate\nthat solving the color homography problem leads to more accurate calibration.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 13:39:16 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 13:35:43 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 13:17:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Finlayson", "Graham D.", ""], ["Gong", "Han", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1607.05967", "submitter": "Han Gong", "authors": "Han Gong, Graham Finlayson", "title": "Interactive Illumination Invariance", "comments": null, "journal-ref": "Color and Imaging Conference, Volume 2015, Number 1, October 2015,\n  pp. 186-190(5)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illumination effects cause problems for many computer vision algorithms. We\npresent a user-friendly interactive system for robust illumination-invariant\nimage generation. Compared with the previous automated illumination-invariant\nimage derivation approaches, our system enables users to specify a particular\nkind of illumination variation for removal. The derivation of\nillumination-invariant image is guided by the user input. The input is a stroke\nthat defines an area covering a set of pixels whose intensities are influenced\npredominately by the illumination variation. This additional flexibility\nenhances the robustness for processing non-linearly rendered images and the\nimages of the scenes where their illumination variations are difficult to\nestimate automatically. Finally, we present some evaluation results of our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:15:12 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Gong", "Han", ""], ["Finlayson", "Graham", ""]]}, {"id": "1607.05969", "submitter": "Yun Gu", "authors": "Yun Gu, Guang-Zhong Yang, Jie Yang and Kun Sun", "title": "4D Cardiac Ultrasound Standard Plane Location by Spatial-Temporal\n  Correlation", "comments": "submitted to MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Echocardiography plays an important part in diagnostic aid in cardiac\ndiseases. A critical step in echocardiography-aided diagnosis is to extract the\nstandard planes since they tend to provide promising views to present different\nstructures that are benefit to diagnosis. To this end, this paper proposes a\nspatial-temporal embedding framework to extract the standard view planes from\n4D STIC (spatial-temporal image corre- lation) volumes. The proposed method is\ncomprised of three stages, the frame smoothing, spatial-temporal embedding and\nfinal classification. In first stage, an L 0 smoothing filter is used to\npreprocess the frames that removes the noise and preserves the boundary. Then a\ncompact repre- sentation is learned via embedding spatial and temporal features\ninto a latent space in the supervised scheme considering both standard plane\ninformation and diagnosis result. In last stage, the learned features are fed\ninto support vector machine to identify the standard plane. We eval- uate the\nproposed method on a 4D STIC volume dataset with 92 normal cases and 93\nabnormal cases in three standard planes. It demonstrates that our method\noutperforms the baselines in both classification accuracy and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:19:03 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Gu", "Yun", ""], ["Yang", "Guang-Zhong", ""], ["Yang", "Jie", ""], ["Sun", "Kun", ""]]}, {"id": "1607.05975", "submitter": "Furqan Khan", "authors": "Furqan M. Khan and Francois Bremond", "title": "Person Re-identification for Real-world Surveillance Systems", "comments": "Person re-identification, Visual surveillance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance based person re-identification in a real-world video surveillance\nsystem with non-overlapping camera views is a challenging problem for many\nreasons. Current state-of-the-art methods often address the problem by relying\non supervised learning of similarity metrics or ranking functions to implicitly\nmodel appearance transformation between cameras for each camera pair, or group,\nin the system. This requires considerable human effort to annotate data.\nFurthermore, the learned models are camera specific and not transferable from\none set of cameras to another. Therefore, the annotation process is required\nafter every network expansion or camera replacement, which strongly limits\ntheir applicability. Alternatively, we propose a novel modeling approach to\nharness complementary appearance information without supervised learning that\nsignificantly outperforms current state-of-the-art unsupervised methods on\nmultiple benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:34:23 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Khan", "Furqan M.", ""], ["Bremond", "Francois", ""]]}, {"id": "1607.06011", "submitter": "Julius Julius", "authors": "Julius, Gopinath Mahale, Sumana T., C. S. Adityakrishna", "title": "On the Modeling of Error Functions as High Dimensional Landscapes for\n  Weight Initialization in Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation deep neural networks for classification hosted on embedded\nplatforms will rely on fast, efficient, and accurate learning algorithms.\nInitialization of weights in learning networks has a great impact on the\nclassification accuracy. In this paper we focus on deriving good initial\nweights by modeling the error function of a deep neural network as a\nhigh-dimensional landscape. We observe that due to the inherent complexity in\nits algebraic structure, such an error function may conform to general results\nof the statistics of large systems. To this end we apply some results from\nRandom Matrix Theory to analyse these functions. We model the error function in\nterms of a Hamiltonian in N-dimensions and derive some theoretical results\nabout its general behavior. These results are further used to make better\ninitial guesses of weights for the learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:25:27 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Julius", "", ""], ["Mahale", "Gopinath", ""], ["T.", "Sumana", ""], ["Adityakrishna", "C. S.", ""]]}, {"id": "1607.06029", "submitter": "Jordan Malof", "authors": "Jordan M. Malof and Kyle Bradbury and Leslie M. Collins and Richard G.\n  Newell", "title": "Automatic Detection of Solar Photovoltaic Arrays in High Resolution\n  Aerial Imagery", "comments": "11 Page manuscript, and 1 page of supplemental information, 10\n  figures, currently under review as a journal publication", "journal-ref": null, "doi": "10.1016/j.apenergy.2016.08.191", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantity of small scale solar photovoltaic (PV) arrays in the United\nStates has grown rapidly in recent years. As a result, there is substantial\ninterest in high quality information about the quantity, power capacity, and\nenergy generated by such arrays, including at a high spatial resolution (e.g.,\ncounties, cities, or even smaller regions). Unfortunately, existing methods for\nobtaining this information, such as surveys and utility interconnection\nfilings, are limited in their completeness and spatial resolution. This work\npresents a computer algorithm that automatically detects PV panels using very\nhigh resolution color satellite imagery. The approach potentially offers a\nfast, scalable method for obtaining accurate information on PV array location\nand size, and at much higher spatial resolutions than are currently available.\nThe method is validated using a very large (135 km^2) collection of publicly\navailable [1] aerial imagery, with over 2,700 human annotated PV array\nlocations. The results demonstrate the algorithm is highly effective on a\nper-pixel basis. It is likewise effective at object-level PV array detection,\nbut with significant potential for improvement in estimating the precise\nshape/size of the PV arrays. These results are the first of their kind for the\ndetection of solar PV in aerial imagery, demonstrating the feasibility of the\napproach and establishing a baseline performance for future investigations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 17:07:53 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Malof", "Jordan M.", ""], ["Bradbury", "Kyle", ""], ["Collins", "Leslie M.", ""], ["Newell", "Richard G.", ""]]}, {"id": "1607.06032", "submitter": "Michael Robinson", "authors": "Michael Robinson", "title": "A Topological Lowpass Filter for Quasiperiodic Signals", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2619678", "report-no": null, "categories": "cs.CV math.DS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a two-stage topological algorithm for recovering an\nestimate of a quasiperiodic function from a set of noisy measurements. The\nfirst stage of the algorithm is a topological phase estimator, which detects\nthe quasiperiodic structure of the function without placing additional\nrestrictions on the function. By respecting this phase estimate, the algorithm\navoids creating distortion even when it uses a large number of samples for the\nestimate of the function.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:10:03 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Robinson", "Michael", ""]]}, {"id": "1607.06038", "submitter": "Wadim Kehl", "authors": "Wadim Kehl, Fausto Milletari, Federico Tombari, Slobodan Ilic, Nassir\n  Navab", "title": "Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose\n  Estimation", "comments": "To appear at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D object detection method that uses regressed descriptors of\nlocally-sampled RGB-D patches for 6D vote casting. For regression, we employ a\nconvolutional auto-encoder that has been trained on a large collection of\nrandom local patches. During testing, scene patch descriptors are matched\nagainst a database of synthetic model view patches and cast 6D object votes\nwhich are subsequently filtered to refined hypotheses. We evaluate on three\ndatasets to show that our method generalizes well to previously unseen input\ndata, delivers robust detection results that compete with and surpass the\nstate-of-the-art while being scalable in the number of objects.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 17:38:15 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Kehl", "Wadim", ""], ["Milletari", "Fausto", ""], ["Tombari", "Federico", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "1607.06062", "submitter": "Wadim Kehl", "authors": "Wadim Kehl, Federico Tombari, Nassir Navab, Slobodan Ilic, Vincent\n  Lepetit", "title": "Hashmod: A Hashing Method for Scalable 3D Object Detection", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable method for detecting objects and estimating their 3D\nposes in RGB-D data. To this end, we rely on an efficient representation of\nobject views and employ hashing techniques to match these views against the\ninput frame in a scalable way. While a similar approach already exists for 2D\ndetection, we show how to extend it to estimate the 3D pose of the detected\nobjects. In particular, we explore different hashing strategies and identify\nthe one which is more suitable to our problem. We show empirically that the\ncomplexity of our method is sublinear with the number of objects and we enable\ndetection and pose estimation of many 3D objects with high accuracy while\noutperforming the state-of-the-art in terms of runtime.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 19:06:30 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Kehl", "Wadim", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""], ["Ilic", "Slobodan", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1607.06125", "submitter": "Ahmed Hassanien", "authors": "Ahmed Mamdouh A. Hassanien", "title": "Sequence to sequence learning for unconstrained scene text recognition", "comments": "It is my master thesis. The thesis was done at Sony Technology Center\n  Stuttgart and presented to Nile University. The thesis supervisors are Mark\n  Blaxall, Fabien Cardinaux, and Motaz Abdelwahab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a state-of-the-art approach for unconstrained natural\nscene text recognition. We propose a cascade approach that incorporates a\nconvolutional neural network (CNN) architecture followed by a long short term\nmemory model (LSTM). The CNN learns visual features for the characters and uses\nthem with a softmax layer to detect sequence of characters. While the CNN gives\nvery good recognition results, it does not model relation between characters,\nhence gives rise to false positive and false negative cases (confusing\ncharacters due to visual similarities like \"g\" and \"9\", or confusing background\npatches with characters; either removing existing characters or adding\nnon-existing ones) To alleviate these problems we leverage recent developments\nin LSTM architectures to encode contextual information. We show that the LSTM\ncan dramatically reduce such errors and achieve state-of-the-art accuracy in\nthe task of unconstrained natural scene text recognition. Moreover we manually\nremove all occurrences of the words that exist in the test set from our\ntraining set to test whether our approach will generalize to unseen data. We\nuse the ICDAR 13 test set for evaluation and compare the results with the state\nof the art approaches [11, 18]. We finally present an application of the work\nin the domain of for traffic monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 21:02:16 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Hassanien", "Ahmed Mamdouh A.", ""]]}, {"id": "1607.06140", "submitter": "Rafael Reisenhofer", "authors": "Rafael Reisenhofer, Sebastian Bosse, Gitta Kutyniok and Thomas Wiegand", "title": "A Haar Wavelet-Based Perceptual Similarity Index for Image Quality\n  Assessment", "comments": null, "journal-ref": "Signal Processing: Image Communication 61 (2018) 33-43", "doi": "10.1016/j.image.2017.11.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most practical situations, the compression or transmission of images and\nvideos creates distortions that will eventually be perceived by a human\nobserver. Vice versa, image and video restoration techniques, such as\ninpainting or denoising, aim to enhance the quality of experience of human\nviewers. Correctly assessing the similarity between an image and an undistorted\nreference image as subjectively experienced by a human viewer can thus lead to\nsignificant improvements in any transmission, compression, or restoration\nsystem. This paper introduces the Haar wavelet-based perceptual similarity\nindex (HaarPSI), a novel and computationally inexpensive similarity measure for\nfull reference image quality assessment. The HaarPSI utilizes the coefficients\nobtained from a Haar wavelet decomposition to assess local similarities between\ntwo images, as well as the relative importance of image areas. The consistency\nof the HaarPSI with the human quality of experience was validated on four large\nbenchmark databases containing thousands of differently distorted images. On\nthese databases, the HaarPSI achieves higher correlations with human opinion\nscores than state-of-the-art full reference similarity measures like the\nstructural similarity index (SSIM), the feature similarity index (FSIM), and\nthe visual saliency-based index (VSI). Along with the simple computational\nstructure and the short execution time, these experimental results suggest a\nhigh applicability of the HaarPSI in real world tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 22:30:31 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 19:11:14 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 11:16:29 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 01:33:21 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Reisenhofer", "Rafael", ""], ["Bosse", "Sebastian", ""], ["Kutyniok", "Gitta", ""], ["Wiegand", "Thomas", ""]]}, {"id": "1607.06144", "submitter": "Tatiana Tommasi", "authors": "Tatiana Tommasi, Martina Lanzi, Paolo Russo, Barbara Caputo", "title": "Learning the Roots of Visual Domain Shift", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the spatial nature of visual domain shift,\nattempting to learn where domain adaptation originates in each given image of\nthe source and target set. We borrow concepts and techniques from the CNN\nvisualization literature, and learn domainnes maps able to localize the degree\nof domain specificity in images. We derive from these maps features related to\ndifferent domainnes levels, and we show that by considering them as a\npreprocessing step for a domain adaptation algorithm, the final classification\nperformance is strongly improved. Combined with the whole image representation,\nthese features provide state of the art results on the Office dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 22:43:44 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Tommasi", "Tatiana", ""], ["Lanzi", "Martina", ""], ["Russo", "Paolo", ""], ["Caputo", "Barbara", ""]]}, {"id": "1607.06166", "submitter": "Zheng Zhang", "authors": "Lunke Fei, Jie Wen, Zheng Zhang, Ke Yan, Zuofeng Zhong", "title": "Local Multiple Directional Pattern of Palmprint Image", "comments": "Accepted by ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lines are the most essential and discriminative features of palmprint images,\nwhich motivate researches to propose various line direction based methods for\npalmprint recognition. Conventional methods usually capture the only one of the\nmost dominant direction of palmprint images. However, a number of points in\npalmprint images have double or even more than two dominant directions because\nof a plenty of crossing lines of palmprint images. In this paper, we propose a\nlocal multiple directional pattern (LMDP) to effectively characterize the\nmultiple direction features of palmprint images. LMDP can not only exactly\ndenote the number and positions of dominant directions but also effectively\nreflect the confidence of each dominant direction. Then, a simple and effective\ncoding scheme is designed to represent the LMDP and a block-wise LMDP\ndescriptor is used as the feature space of palmprint images in palmprint\nrecognition. Extensive experimental results demonstrate the superiority of the\nLMDP over the conventional powerful descriptors and the state-of-the-art\ndirection based methods in palmprint recognition.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 01:27:27 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Fei", "Lunke", ""], ["Wen", "Jie", ""], ["Zhang", "Zheng", ""], ["Yan", "Ke", ""], ["Zhong", "Zuofeng", ""]]}, {"id": "1607.06178", "submitter": "Alessandro Pieropan", "authors": "Alessandro Pieropan, M{\\aa}rten Bj\\\"orkman, Niklas Bergstr\\\"om and\n  Danica Kragic", "title": "Feature Descriptors for Tracking by Detection: a Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an extensive evaluation of the performance of local\ndescriptors for tracking applications. Many different descriptors have been\nproposed in the literature for a wide range of application in computer vision\nsuch as object recognition and 3D reconstruction. More recently, due to fast\nkey-point detectors, local image features can be used in online tracking\nframeworks. However, while much effort has been spent on evaluating their\nperformance in terms of distinctiveness and robustness to image\ntransformations, very little has been done in the contest of tracking. Our\nevaluation is performed in terms of distinctiveness, tracking precision and\ntracking speed. Our results show that binary descriptors like ORB or BRISK have\ncomparable results to SIFT or AKAZE due to a higher number of key-points.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 03:06:43 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Pieropan", "Alessandro", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""], ["Bergstr\u00f6m", "Niklas", ""], ["Kragic", "Danica", ""]]}, {"id": "1607.06235", "submitter": "Yu Li", "authors": "Yu Li, Shaodi You, Michael S. Brown, Robby T. Tan", "title": "Haze Visibility Enhancement: A Survey and Quantitative Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a comprehensive survey of methods dealing with visibility\nenhancement of images taken in hazy or foggy scenes. The survey begins with\ndiscussing the optical models of atmospheric scattering media and image\nformation. This is followed by a survey of existing methods, which are grouped\nto multiple image methods, polarizing filters based methods, methods with known\ndepth, and single-image methods. We also provide a benchmark of a number of\nwell known single-image methods, based on a recent dataset provided by Fattal\nand our newly generated scattering media dataset that contains ground truth\nimages for quantitative evaluation. To our knowledge, this is the first\nbenchmark using numerical metrics to evaluate dehazing techniques. This\nbenchmark allows us to objectively compare the results of existing methods and\nto better identify the strengths and limitations of each method.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 08:57:13 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Li", "Yu", ""], ["You", "Shaodi", ""], ["Brown", "Michael S.", ""], ["Tan", "Robby T.", ""]]}, {"id": "1607.06250", "submitter": "Arnaud Dapogny", "authors": "Arnaud Dapogny, K\\'evin Bailly, S\\'everine Dubuisson", "title": "Dynamic Pose-Robust Facial Expression Recognition by Multi-View Pairwise\n  Conditional Random Forests", "comments": "Extension of an ICCV 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial expression classification (FER) from videos is a critical\nproblem for the development of intelligent human-computer interaction systems.\nStill, it is a challenging problem that involves capturing high-dimensional\nspatio-temporal patterns describing the variation of one's appearance over\ntime. Such representation undergoes great variability of the facial morphology\nand environmental factors as well as head pose variations. In this paper, we\nuse Conditional Random Forests to capture low-level expression transition\npatterns. More specifically, heterogeneous derivative features (e.g. feature\npoint movements or texture variations) are evaluated upon pairs of images. When\ntesting on a video frame, pairs are created between this current frame and\nprevious ones and predictions for each previous frame are used to draw trees\nfrom Pairwise Conditional Random Forests (PCRF) whose pairwise outputs are\naveraged over time to produce robust estimates. Moreover, PCRF collections can\nalso be conditioned on head pose estimation for multi-view dynamic FER. As\nsuch, our approach appears as a natural extension of Random Forests for\nlearning spatio-temporal patterns, potentially from multiple viewpoints.\nExperiments on popular datasets show that our method leads to significant\nimprovements over standard Random Forests as well as state-of-the-art\napproaches on several scenarios, including a novel multi-view video corpus\ngenerated from a publicly available database.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 10:07:33 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Dapogny", "Arnaud", ""], ["Bailly", "K\u00e9vin", ""], ["Dubuisson", "S\u00e9verine", ""]]}, {"id": "1607.06264", "submitter": "Alejandro Betancourt", "authors": "Alejandro Betancourt, Pietro Morerio, Emilia Barakova, Lucio\n  Marcenaro, Matthias Rauterberg, Carlo Regazzoni", "title": "Left/Right Hand Segmentation in Egocentric Videos", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.09.005", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras allow people to record their daily activities from a\nuser-centered (First Person Vision) perspective. Due to their favorable\nlocation, wearable cameras frequently capture the hands of the user, and may\nthus represent a promising user-machine interaction tool for different\napplications. Existent First Person Vision methods handle hand segmentation as\na background-foreground problem, ignoring two important facts: i) hands are not\na single \"skin-like\" moving element, but a pair of interacting cooperative\nentities, ii) close hand interactions may lead to hand-to-hand occlusions and,\nas a consequence, create a single hand-like segment. These facts complicate a\nproper understanding of hand movements and interactions. Our approach extends\ntraditional background-foreground strategies, by including a\nhand-identification step (left-right) based on a Maxwell distribution of angle\nand position. Hand-to-hand occlusions are addressed by exploiting temporal\nsuperpixels. The experimental results show that, in addition to a reliable\nleft/right hand-segmentation, our approach considerably improves the\ntraditional background-foreground hand-segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:06:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Betancourt", "Alejandro", ""], ["Morerio", "Pietro", ""], ["Barakova", "Emilia", ""], ["Marcenaro", "Lucio", ""], ["Rauterberg", "Matthias", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1607.06283", "submitter": "Christian Reinbacher", "authors": "Christian Reinbacher and Gottfried Graber and Thomas Pock", "title": "Real-Time Intensity-Image Reconstruction for Event Cameras Using\n  Manifold Regularisation", "comments": "Accepted to BMVC 2016 as oral presentation, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras or neuromorphic cameras mimic the human perception system as\nthey measure the per-pixel intensity change rather than the actual intensity\nlevel. In contrast to traditional cameras, such cameras capture new information\nabout the scene at MHz frequency in the form of sparse events. The high\ntemporal resolution comes at the cost of losing the familiar per-pixel\nintensity information. In this work we propose a variational model that\naccurately models the behaviour of event cameras, enabling reconstruction of\nintensity images with arbitrary frame rate in real-time. Our method is\nformulated on a per-event-basis, where we explicitly incorporate information\nabout the asynchronous nature of events via an event manifold induced by the\nrelative timestamps of events. In our experiments we verify that solving the\nvariational model on the manifold produces high-quality images without\nexplicitly estimating optical flow.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:55:31 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 14:11:45 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Reinbacher", "Christian", ""], ["Graber", "Gottfried", ""], ["Pock", "Thomas", ""]]}, {"id": "1607.06290", "submitter": "Arnaud Dapogny", "authors": "Arnaud Dapogny, K\\'evin Bailly, S\\'everine Dubuisson", "title": "Confidence-Weighted Local Expression Predictions for Occlusion Handling\n  in Expression Recognition and Action Unit detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-Automatic Facial Expression Recognition (FER) from still images is a\nchallenging task as it involves handling large interpersonal morphological\ndifferences, and as partial occlusions can occasionally happen. Furthermore,\nlabelling expressions is a time-consuming process that is prone to\nsubjectivity, thus the variability may not be fully covered by the training\ndata. In this work, we propose to train Random Forests upon spatially defined\nlocal subspaces of the face. The output local predictions form a categorical\nexpression-driven high-level representation that we call Local Expression\nPredictions (LEPs). LEPs can be combined to describe categorical facial\nexpressions as well as Action Units (AUs). Furthermore, LEPs can be weighted by\nconfidence scores provided by an autoencoder network. Such network is trained\nto locally capture the manifold of the non-occluded training data in a\nhierarchical way. Extensive experiments show that the proposed LEP\nrepresentation yields high descriptive power for categorical expressions and AU\noccurrence prediction, and leads to interesting perspectives towards the design\nof occlusion-robust and confidence-aware FER systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:17:34 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Dapogny", "Arnaud", ""], ["Bailly", "K\u00e9vin", ""], ["Dubuisson", "S\u00e9verine", ""]]}, {"id": "1607.06317", "submitter": "Margret Keuper", "authors": "Margret Keuper, Siyu Tang, Yu Zhongjie, Bjoern Andres, Thomas Brox,\n  Bernt Schiele", "title": "A Multi-cut Formulation for Joint Segmentation and Tracking of Multiple\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Minimum Cost Multicut Formulations have been proposed and proven to\nbe successful in both motion trajectory segmentation and multi-target tracking\nscenarios. Both tasks benefit from decomposing a graphical model into an\noptimal number of connected components based on attractive and repulsive\npairwise terms. The two tasks are formulated on different levels of granularity\nand, accordingly, leverage mostly local information for motion segmentation and\nmostly high-level information for multi-target tracking. In this paper we argue\nthat point trajectories and their local relationships can contribute to the\nhigh-level task of multi-target tracking and also argue that high-level cues\nfrom object detection and tracking are helpful to solve motion segmentation. We\npropose a joint graphical model for point trajectories and object detections\nwhose Multicuts are solutions to motion segmentation {\\it and} multi-target\ntracking problems at once. Results on the FBMS59 motion segmentation benchmark\nas well as on pedestrian tracking sequences from the 2D MOT 2015 benchmark\ndemonstrate the promise of this joint approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 13:41:32 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Keuper", "Margret", ""], ["Tang", "Siyu", ""], ["Zhongjie", "Yu", ""], ["Andres", "Bjoern", ""], ["Brox", "Thomas", ""], ["Schiele", "Bernt", ""]]}, {"id": "1607.06318", "submitter": "Gergely Odor", "authors": "Gergely Odor", "title": "Hierarchical Manifold Clustering on Diffusion Maps for Connectomics (MIT\n  18.S096 final project)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel algorithm for segmentation of imperfect\nboundary probability maps (BPM) in connectomics. Our algorithm can be a\nconsidered as an extension of spectral clustering. Instead of clustering the\ndiffusion maps with traditional clustering algorithms, we learn the manifold\nand compute an estimate of the minimum normalized cut. We proceed by divide and\nconquer. We also introduce a novel criterion for determining if further splits\nare necessary in a component based on it's topological properties. Our\nalgorithm complements the currently popular agglomeration approaches in\nconnectomics, which overlook the geometrical aspects of this segmentation\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 00:38:48 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Odor", "Gergely", ""]]}, {"id": "1607.06349", "submitter": "Michele Mancini", "authors": "Michele Mancini, Gabriele Costante, Paolo Valigi and Thomas\n  A.Ciarfuglia", "title": "Fast Robust Monocular Depth Estimation for Obstacle Detection with Fully\n  Convolutional Networks", "comments": "Accepted for publication in the Proceedings of the 2016 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstacle Detection is a central problem for any robotic system, and critical\nfor autonomous systems that travel at high speeds in unpredictable environment.\nThis is often achieved through scene depth estimation, by various means. When\nfast motion is considered, the detection range must be longer enough to allow\nfor safe avoidance and path planning. Current solutions often make assumption\non the motion of the vehicle that limit their applicability, or work at very\nlimited ranges due to intrinsic constraints. We propose a novel\nappearance-based Object Detection system that is able to detect obstacles at\nvery long range and at a very high speed (~300Hz), without making assumptions\non the type of motion. We achieve these results using a Deep Neural Network\napproach trained on real and synthetic images and trading some depth accuracy\nfor fast, robust and consistent operation. We show how photo-realistic\nsynthetic images are able to solve the problem of training set dimension and\nvariety typical of machine learning approaches, and how our system is robust to\nmassive blurring of test images.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:49:07 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Mancini", "Michele", ""], ["Costante", "Gabriele", ""], ["Valigi", "Paolo", ""], ["Ciarfuglia", "Thomas A.", ""]]}, {"id": "1607.06356", "submitter": "Jose Oramas", "authors": "Marc Mart\\'inez-Camarena, Jose Oramas, Mario Montagud-Climent and\n  Tinne Tuytelaars", "title": "Reasoning about Body-Parts Relations for Sign Language Recognition", "comments": "Under Review ( 15 Pages: 13 Figures, 6 Tables )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, hand gesture recognition has been mostly addressed\nconsidering hand trajectories in isolation. However, in most sign languages,\nhand gestures are defined on a particular context (body region). We propose a\npipeline to perform sign language recognition which models hand movements in\nthe context of other parts of the body captured in the 3D space using the MS\nKinect sensor. In addition, we perform sign recognition based on the different\nhand postures that occur during a sign. Our experiments show that considering\ndifferent body parts brings improved performance when compared to other methods\nwhich only consider global hand trajectories. Finally, we demonstrate that the\ncombination of hand postures features with hand gestures features helps to\nimprove the prediction of a given sign.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 15:10:41 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Mart\u00ednez-Camarena", "Marc", ""], ["Oramas", "Jose", ""], ["Montagud-Climent", "Mario", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1607.06407", "submitter": "Julian Straub", "authors": "Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III", "title": "Small-Variance Nonparametric Clustering on the Hypersphere", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  (pp. 334-342). (2015)", "doi": "10.1109/CVPR.2015.7298630", "report-no": null, "categories": "cs.CV math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural regularities in man-made environments reflect in the distribution\nof their surface normals. Describing these surface normal distributions is\nimportant in many computer vision applications, such as scene understanding,\nplane segmentation, and regularization of 3D reconstructions. Based on the\nsmall-variance limit of Bayesian nonparametric von-Mises-Fisher (vMF) mixture\ndistributions, we propose two new flexible and efficient k-means-like\nclustering algorithms for directional data such as surface normals. The first,\nDP-vMF-means, is a batch clustering algorithm derived from the Dirichlet\nprocess (DP) vMF mixture. Recognizing the sequential nature of data collection\nin many applications, we extend this algorithm to DDP-vMF-means, which infers\ntemporally evolving cluster structure from streaming data. Both algorithms\nnaturally respect the geometry of directional data, which lies on the unit\nsphere. We demonstrate their performance on synthetic directional data and real\n3D surface normals from RGB-D sensors. While our experiments focus on 3D data,\nboth algorithms generalize to high dimensional directional data such as protein\nbackbone configurations and semantic word vectors.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 17:52:08 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Straub", "Julian", ""], ["Campbell", "Trevor", ""], ["How", "Jonathan P.", ""], ["Fisher", "John W.", "III"]]}, {"id": "1607.06408", "submitter": "Yongkang Wong", "authors": "Wenhui Li, Yongkang Wong, An-An Liu, Yang Li, Yu-Ting Su, Mohan\n  Kankanhalli", "title": "Multi-Camera Action Dataset for Cross-Camera Action Recognition\n  Benchmarking", "comments": null, "journal-ref": null, "doi": "10.1109/WACV.2017.28", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has received increasing attention from the computer vision\nand machine learning communities in the last decade. To enable the study of\nthis problem, there exist a vast number of action datasets, which are recorded\nunder controlled laboratory settings, real-world surveillance environments, or\ncrawled from the Internet. Apart from the \"in-the-wild\" datasets, the training\nand test split of conventional datasets often possess similar environments\nconditions, which leads to close to perfect performance on constrained\ndatasets. In this paper, we introduce a new dataset, namely Multi-Camera Action\nDataset (MCAD), which is designed to evaluate the open view classification\nproblem under the surveillance environment. In total, MCAD contains 14,298\naction samples from 18 action categories, which are performed by 20 subjects\nand independently recorded with 5 cameras. Inspired by the well received\nevaluation approach on the LFW dataset, we designed a standard evaluation\nprotocol and benchmarked MCAD under several scenarios. The benchmark shows that\nwhile an average of 85% accuracy is achieved under the closed-view scenario,\nthe performance suffers from a significant drop under the cross-view scenario.\nIn the worst case scenario, the performance of 10-fold cross validation drops\nfrom 87.0% to 47.4%.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 17:58:19 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 10:00:59 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 05:21:31 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Li", "Wenhui", ""], ["Wong", "Yongkang", ""], ["Liu", "An-An", ""], ["Li", "Yang", ""], ["Su", "Yu-Ting", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1607.06416", "submitter": "Yilin Wang", "authors": "Yilin Wang, Suhang Wang, Jiliang Tang, Neil O'Hare, Yi Chang, Baoxin\n  Li", "title": "Hierarchical Attention Network for Action Recognition in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding human actions in wild videos is an important task with a broad\nrange of applications. In this paper we propose a novel approach named\nHierarchical Attention Network (HAN), which enables to incorporate static\nspatial information, short-term motion information and long-term video temporal\nstructures for complex human action understanding. Compared to recent\nconvolutional neural network based approaches, HAN has following advantages (1)\nHAN can efficiently capture video temporal structures in a longer range; (2)\nHAN is able to reveal temporal transitions between frame chunks with different\ntime steps, i.e. it explicitly models the temporal transitions between frames\nas well as video segments and (3) with a multiple step spatial temporal\nattention mechanism, HAN automatically learns important regions in video frames\nand temporal segments in the video. The proposed model is trained and evaluated\non the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and it\nsignificantly outperforms the state-of-the arts\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 18:16:39 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Wang", "Yilin", ""], ["Wang", "Suhang", ""], ["Tang", "Jiliang", ""], ["O'Hare", "Neil", ""], ["Chang", "Yi", ""], ["Li", "Baoxin", ""]]}, {"id": "1607.06514", "submitter": "Lingxi Xie", "authors": "Lingxi Xie, Qi Tian, John Flynn, Jingdong Wang, Alan Yuille", "title": "Geometric Neural Phrase Pooling: Modeling the Spatial Co-occurrence of\n  Neurons", "comments": "To appear, in ECCV 2016 (18 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are playing important roles in\nstate-of-the-art visual recognition. This paper focuses on modeling the spatial\nco-occurrence of neuron responses, which is less studied in the previous work.\nFor this, we consider the neurons in the hidden layer as neural words, and\nconstruct a set of geometric neural phrases on top of them. The idea that\ngrouping neural words into neural phrases is borrowed from the\nBag-of-Visual-Words (BoVW) model. Next, the Geometric Neural Phrase Pooling\n(GNPP) algorithm is proposed to efficiently encode these neural phrases. GNPP\nacts as a new type of hidden layer, which punishes the isolated neuron\nresponses after convolution, and can be inserted into a CNN model with little\nextra computational overhead. Experimental results show that GNPP produces\nsignificant and consistent accuracy gain in image classification.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 21:51:58 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Xie", "Lingxi", ""], ["Tian", "Qi", ""], ["Flynn", "John", ""], ["Wang", "Jingdong", ""], ["Yuille", "Alan", ""]]}, {"id": "1607.06583", "submitter": "Saman Sarraf", "authors": "Saman Sarraf, Ghassem Tofighi", "title": "Classification of Alzheimer's Disease Structural MRI Data by Deep\n  Learning Convolutional Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1603.08631", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning techniques especially predictive modeling and\npattern recognition in biomedical sciences from drug delivery system to medical\nimaging has become one of the important methods which are assisting researchers\nto have deeper understanding of entire issue and to solve complex medical\nproblems. Deep learning is a powerful machine learning algorithm in\nclassification while extracting low to high-level features. In this paper, we\nused convolutional neural network to classify Alzheimer's brain from normal\nhealthy brain. The importance of classifying this kind of medical data is to\npotentially develop a predict model or system in order to recognize the type\ndisease from normal subjects or to estimate the stage of the disease.\nClassification of clinical data such as Alzheimer's disease has been always\nchallenging and most problematic part has been always selecting the most\ndiscriminative features. Using Convolutional Neural Network (CNN) and the\nfamous architecture LeNet-5, we successfully classified structural MRI data of\nAlzheimer's subjects from normal controls where the accuracy of test data on\ntrained data reached 98.84%. This experiment suggests us the shift and scale\ninvariant features extracted by CNN followed by deep learning classification is\nmost powerful method to distinguish clinical data from healthy data in fMRI.\nThis approach also enables us to expand our methodology to predict more\ncomplicated systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 07:48:18 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 20:41:51 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Sarraf", "Saman", ""], ["Tofighi", "Ghassem", ""]]}, {"id": "1607.06676", "submitter": "Grasha Jacob Mrs", "authors": "Grasha Jacob, R. Shenbagavalli, S. Karthika", "title": "Detection of surface defects on ceramic tiles based on morphological\n  techniques", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Ceramic tiles have become very popular and are used in the flooring of\noffices and shopping malls. As testing the quality of tiles manually in a\nhighly polluted environment in the manufacturing industry is a labor-intensive\nand time consuming process, analysis is carried out on the tile images. This\npaper discusses an automated system to detect the defects on the surface of\nceramic tiles based on dilation, erosion, SMEE and boundary detection\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 08:25:41 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Jacob", "Grasha", ""], ["Shenbagavalli", "R.", ""], ["Karthika", "S.", ""]]}, {"id": "1607.06783", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari, Norman Poh, Miroslaw Bober and David Windridge", "title": "Can DMD obtain a Scene Background in Color?", "comments": "International Conference on Image, Vision and Computing (ICIVC 2016),\n  August 3-5, 2016, Portsmouth, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A background model describes a scene without any foreground objects and has a\nnumber of applications, ranging from video surveillance to computational\nphotography. Recent studies have introduced the method of Dynamic Mode\nDecomposition (DMD) for robustly separating video frames into a background\nmodel and foreground components. While the method introduced operates by\nconverting color images to grayscale, we in this study propose a technique to\nobtain the background model in the color domain. The effectiveness of our\ntechnique is demonstrated using a publicly available Scene Background\nInitialisation (SBI) dataset. Our results both qualitatively and quantitatively\nshow that DMD can successfully obtain a colored background model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:41:01 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Tirunagari", "Santosh", ""], ["Poh", "Norman", ""], ["Bober", "Miroslaw", ""], ["Windridge", "David", ""]]}, {"id": "1607.06787", "submitter": "Enzo Ferrante", "authors": "Mahsa Shakeri (2 and 4), Enzo Ferrante (1), Stavros Tsogkas (1), Sarah\n  Lippe (3 and 4), Samuel Kadoury (2 and 4), Iasonas Kokkinos (1), Nikos\n  Paragios (1) ((1) CVN, CentraleSupelec-Inria, Universite Paris-Saclay,\n  France, (2) Polytechnique Montreal, Canada (3) University of Montreal, Canada\n  (4) CHU Sainte-Justine Research Center, Montreal, Canada)", "title": "Prior-based Coregistration and Cosegmentation", "comments": "The first two authors contributed equally", "journal-ref": "MICCAI 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modular and scalable framework for dense coregistration and\ncosegmentation with two key characteristics: first, we substitute ground truth\ndata with the semantic map output of a classifier; second, we combine this\noutput with population deformable registration to improve both alignment and\nsegmentation. Our approach deforms all volumes towards consensus, taking into\naccount image similarities and label consistency. Our pipeline can incorporate\nany classifier and similarity metric. Results on two datasets, containing\nannotations of challenging brain structures, demonstrate the potential of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:49:09 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Shakeri", "Mahsa", "", "2 and 4"], ["Ferrante", "Enzo", "", "3 and 4"], ["Tsogkas", "Stavros", "", "3 and 4"], ["Lippe", "Sarah", "", "3 and 4"], ["Kadoury", "Samuel", "", "2 and 4"], ["Kokkinos", "Iasonas", ""], ["Paragios", "Nikos", ""]]}, {"id": "1607.06794", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani and Reza Hedayati", "title": "An ensemble learning method for scene classification based on Hidden\n  Markov Model image representation", "comments": "This paper has been withdrawn by the authors due to significant\n  errors in equations 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low level images representation in feature space performs poorly for\nclassification with high accuracy since this level of representation is not\nable to project images into the discriminative feature space. In this work, we\npropose an efficient image representation model for classification. First we\napply Hidden Markov Model (HMM) on ordered grids represented by different type\nof image descriptors in order to include causality of local properties existing\nin image for feature extraction and then we train up a separate classifier for\neach of these features sets. Finally we ensemble these classifiers efficiently\nin a way that they can cancel out each other errors for obtaining higher\naccuracy. This method is evaluated on 15 natural scene dataset. Experimental\nresults show the superiority of the proposed method in comparison to some\ncurrent existing methods\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 19:06:17 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 02:04:30 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 06:25:16 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Hedayati", "Reza", ""]]}, {"id": "1607.06797", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani", "title": "A probabilistic patch based image representation using Conditional\n  Random Field model for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we proposed an ordered patch based method using Conditional\nRandom Field (CRF) in order to encode local properties and their spatial\nrelationship in images to address texture classification, face recognition, and\nscene classification problems. Typical image classification approaches work\nwithout considering spatial causality among distinctive properties of an image\nfor image representation in feature space. In this method first, each image is\nencoded as a sequence of ordered patches, including local properties. Second,\nthe sequence of these ordered patches is modeled as a probabilistic feature\nvector by CRF to model spatial relationship of these local properties. And\nfinally, image classification is performed on such probabilistic image\nrepresentation. Experimental results on several standard image datasets\nindicate that proposed method outperforms some of existing image classification\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 19:19:47 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 06:24:06 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Taherkhani", "Fariborz", ""]]}, {"id": "1607.06854", "submitter": "Filip Piekniewski", "authors": "Filip Piekniewski, Patryk Laurent, Csaba Petre, Micah Richert, Dimitry\n  Fisher, Todd Hylton", "title": "Unsupervised Learning from Continuous Video in a Scalable Predictive\n  Recurrent Network", "comments": "38 pages, 20 figures, v3. Added several citations to relevant papers,\n  expanded the discussion of existing approach in deep learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding visual reality involves acquiring common-sense knowledge about\ncountless regularities in the visual world, e.g., how illumination alters the\nappearance of objects in a scene, and how motion changes their apparent spatial\nrelationship. These regularities are hard to label for training supervised\nmachine learning algorithms; consequently, algorithms need to learn these\nregularities from the real world in an unsupervised way. We present a novel\nnetwork meta-architecture that can learn world dynamics from raw, continuous\nvideo. The components of this network can be implemented using any algorithm\nthat possesses three key capabilities: prediction of a signal over time,\nreduction of signal dimensionality (compression), and the ability to use\nsupplementary contextual information to inform the prediction. The presented\narchitecture is highly-parallelized and scalable, and is implemented using\nlocalized connectivity, processing, and learning. We demonstrate an\nimplementation of this architecture where the components are built from\nmulti-layer perceptrons. We apply the implementation to create a system capable\nof stable and robust visual tracking of objects as seen by a moving camera.\nResults show performance on par with or exceeding state-of-the-art tracking\nalgorithms. The tracker can be trained in either fully supervised or\nunsupervised-then-briefly-supervised regimes. Success of the briefly-supervised\nregime suggests that the unsupervised portion of the model extracts useful\ninformation about visual reality. The results suggest a new class of AI\nalgorithms that uniquely combine prediction and scalability in a way that makes\nthem suitable for learning from and --- and eventually acting within --- the\nreal world.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 22:13:04 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 23:31:18 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 17:41:41 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Piekniewski", "Filip", ""], ["Laurent", "Patryk", ""], ["Petre", "Csaba", ""], ["Richert", "Micah", ""], ["Fisher", "Dimitry", ""], ["Hylton", "Todd", ""]]}, {"id": "1607.06871", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui", "title": "Deep Appearance Models: A Deep Boltzmann Machine Approach for Face\n  Modeling", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-018-1113-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"interpretation through synthesis\" approach to analyze face images,\nparticularly Active Appearance Models (AAMs) method, has become one of the most\nsuccessful face modeling approaches over the last two decades. AAM models have\nability to represent face images through synthesis using a controllable\nparameterized Principal Component Analysis (PCA) model. However, the accuracy\nand robustness of the synthesized faces of AAM are highly depended on the\ntraining sets and inherently on the generalizability of PCA subspaces. This\npaper presents a novel Deep Appearance Models (DAMs) approach, an efficient\nreplacement for AAMs, to accurately capture both shape and texture of face\nimages under large variations. In this approach, three crucial components\nrepresented in hierarchical layers are modeled using the Deep Boltzmann\nMachines (DBM) to robustly capture the variations of facial shapes and\nappearances. DAMs are therefore superior to AAMs in inferencing a\nrepresentation for new face images under various challenging conditions. The\nproposed approach is evaluated in various applications to demonstrate its\nrobustness and capabilities, i.e. facial super-resolution reconstruction,\nfacial off-angle reconstruction or face frontalization, facial occlusion\nremoval and age estimation using challenging face databases, i.e. Labeled Face\nParts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep\nlearning based approaches, the proposed DAMs achieve competitive results in\nthose applications, thus this showed their advantages in handling occlusions,\nfacial representation, and reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 01:12:26 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 16:32:57 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 20:12:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Quach", "Kha Gia", ""], ["Bui", "Tien D.", ""]]}, {"id": "1607.06902", "submitter": "Zhe Jin", "authors": "Zhe Jin, Yen-Lung Lai, Andrew Beng Jin Teoh", "title": "Rank Correlation Measure: A Representational Transformation for\n  Biometric Template Protection", "comments": "6 pages, 5 figures, 2 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a variety of theoretical-sound techniques have been proposed for\nbiometric template protection, there is rarely practical solution that\nguarantees non-invertibility, cancellability, non-linkability and performance\nsimultaneously. In this paper, a ranking-based representational transformation\nis proposed for fingerprint templates. The proposed method transforms a\nreal-valued feature vector into index code such that the pairwise-order measure\nin the resultant codes are closely correlated with rank similarity measure.\nSuch a ranking based technique offers two major merits: 1) Resilient to\nnoises/perturbations in numeric values; and 2) Highly nonlinear embedding based\non partial order statistics. The former takes care of the accuracy performance\nmitigating numeric noises/perturbations while the latter offers strong\nnon-invertible transformation via nonlinear feature embedding from Euclidean to\nRank space that leads to toughness in inversion. The experimental results\ndemonstrate reasonable accuracy performance on benchmark FVC2002 and FVC2004\nfingerprint databases, thus confirm the proposition of the rank correlation.\nMoreover, the security and privacy analysis justify the strong capability\nagainst the existing major privacy attacks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 08:12:01 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Jin", "Zhe", ""], ["Lai", "Yen-Lung", ""], ["Teoh", "Andrew Beng Jin", ""]]}, {"id": "1607.06972", "submitter": "Zhiyuan Shi", "authors": "Seungryul Baek, Zhiyuan Shi, Masato Kawade, Tae-Kyun Kim", "title": "Kinematic-Layout-aware Random Forests for Depth-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of 24 hours-monitoring patient actions\nin a ward such as \"stretching an arm out of the bed\", \"falling out of the bed\",\nwhere temporal movements are subtle or significant. In the concerned scenarios,\nthe relations between scene layouts and body kinematics (skeletons) become\nimportant cues to recognize actions; however they are hard to be secured at a\ntesting stage. To address this problem, we propose a kinematic-layout-aware\nrandom forest which takes into account the kinematic-layout (\\ie layout and\nskeletons), to maximize the discriminative power of depth image appearance. We\nintegrate the kinematic-layout in the split criteria of random forests to guide\nthe learning process by 1) determining the switch to either the depth\nappearance or the kinematic-layout information, and 2) implicitly closing the\ngap between two distributions obtained by the kinematic-layout and the\nappearance, when the kinematic-layout appears useful. The kinematic-layout\ninformation is not required for the test data, thus called \"privileged\ninformation prior\". The proposed method has also been testified in cross-view\nsettings, by the use of view-invariant features and enforcing the consistency\namong synthetic-view data. Experimental evaluations on our new dataset PATIENT,\nCAD-60 and UWA3D (multiview) demonstrate that our method outperforms various\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 20:36:39 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 16:30:28 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 11:32:54 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Baek", "Seungryul", ""], ["Shi", "Zhiyuan", ""], ["Kawade", "Masato", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1607.06973", "submitter": "Ahmad H. A. Eid", "authors": "Ahmad H. A. Eid", "title": "Combined Classifiers for Invariant Face Recognition", "comments": "M.Sc. Thesis, 2004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No single classifier can alone solve the complex problem of face recognition.\nResearchers found that combining some base classifiers usually enhances their\nrecognition rate. The weaknesses of the base classifiers are reflected on the\nresulting combined system. In this work, a system for combining unstable, low\nperformance classifiers is proposed. The system is applied to face images of\n392 persons. The system shows remarkable stability and high recognition rate\nusing a reduced number of parameters. The system illustrates the possibility of\ndesigning a combined system that benefits from the strengths of its base\nclassifiers while avoiding many of their weaknesses.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 20:38:56 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Eid", "Ahmad H. A.", ""]]}, {"id": "1607.06986", "submitter": "Shervin Ardeshir", "authors": "Shervin Ardeshir, Ali Borji", "title": "Ego2Top: Matching Viewers in Egocentric and Top-view Videos", "comments": "European Conference on Computer Vision (ECCV) 2016. Amsterdam, the\n  Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric cameras are becoming increasingly popular and provide us with\nlarge amounts of videos, captured from the first person perspective. At the\nsame time, surveillance cameras and drones offer an abundance of visual\ninformation, often captured from top-view. Although these two sources of\ninformation have been separately studied in the past, they have not been\ncollectively studied and related. Having a set of egocentric cameras and a\ntop-view camera capturing the same area, we propose a framework to identify the\negocentric viewers in the top-view video. We utilize two types of features for\nour assignment procedure. Unary features encode what a viewer (seen from\ntop-view or recording an egocentric video) visually experiences over time.\nPairwise features encode the relationship between the visual content of a pair\nof viewers. Modeling each view (egocentric or top) by a graph, the assignment\nprocess is formulated as spectral graph matching. Evaluating our method over a\ndataset of 50 top-view and 188 egocentric videos taken in different scenarios\ndemonstrates the efficiency of the proposed approach in assigning egocentric\nviewers to identities present in top-view camera. We also study the effect of\ndifferent parameters such as the number of egocentric viewers and visual\nfeatures.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 00:28:01 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 21:49:56 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Ardeshir", "Shervin", ""], ["Borji", "Ali", ""]]}, {"id": "1607.06997", "submitter": "Xiangyun Zhao", "authors": "Xiangyun Zhao, Xiaodan Liang, Luoqi Liu, Teng Li, Yugang Han, Nuno\n  Vasconcelos, Shuicheng Yan", "title": "Peak-Piloted Deep Network for Facial Expression Recognition", "comments": "Published in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective functions for training of deep networks for face-related\nrecognition tasks, such as facial expression recognition (FER), usually\nconsider each sample independently. In this work, we present a novel\npeak-piloted deep network (PPDN) that uses a sample with peak expression (easy\nsample) to supervise the intermediate feature responses for a sample of\nnon-peak expression (hard sample) of the same type and from the same subject.\nThe expression evolving process from non-peak expression to peak expression can\nthus be implicitly embedded in the network to achieve the invariance to\nexpression intensities. A special purpose back-propagation procedure, peak\ngradient suppression (PGS), is proposed for network training. It drives the\nintermediate-layer feature responses of non-peak expression samples towards\nthose of the corresponding peak expression samples, while avoiding the inverse.\nThis avoids degrading the recognition capability for samples of peak expression\ndue to interference from their non-peak expression counterparts. Extensive\ncomparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the\nsuperiority of the PPDN over state-ofthe-art FER methods, as well as the\nadvantages of both the network structure and the optimization strategy.\nMoreover, it is shown that PPDN is a general architecture, extensible to other\ntasks by proper definition of peak and non-peak samples. This is validated by\nexperiments that show state-of-the-art performance on pose-invariant face\nrecognition, using the Multi-PIE dataset.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 04:26:41 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 08:19:24 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Zhao", "Xiangyun", ""], ["Liang", "Xiaodan", ""], ["Liu", "Luoqi", ""], ["Li", "Teng", ""], ["Han", "Yugang", ""], ["Vasconcelos", "Nuno", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1607.06999", "submitter": "Zhen Cui", "authors": "Yang Li, Wenming Zheng, Zhen Cui", "title": "Recurrent Regression for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the sequential changes of images including poses, in this paper we\npropose a recurrent regression neural network(RRNN) framework to unify two\nclassic tasks of cross-pose face recognition on still images and video-based\nface recognition. To imitate the changes of images, we explicitly construct the\npotential dependencies of sequential images so as to regularize the final\nlearning model. By performing progressive transforms for sequentially adjacent\nimages, RRNN can adaptively memorize and forget the information that benefits\nfor the final classification. For face recognition of still images, given any\none image with any one pose, we recurrently predict the images with its\nsequential poses to expect to capture some useful information of others poses.\nFor video-based face recognition, the recurrent regression takes one entire\nsequence rather than one image as its input. We verify RRNN in static face\ndataset MultiPIE and face video dataset YouTube Celebrities(YTC). The\ncomprehensive experimental results demonstrate the effectiveness of the\nproposed RRNN method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 05:11:40 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Li", "Yang", ""], ["Zheng", "Wenming", ""], ["Cui", "Zhen", ""]]}, {"id": "1607.07006", "submitter": "Abhinav Pachauri", "authors": "Abhinav Pachauri, Vikrant More, Pradeep Gaidhani, Nitin Gupta", "title": "Autonomous Ingress of a UAV through a window using Monocular Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of autonomous UAVs for surveillance purposes and other reconnaissance\ntasks is increasingly becoming popular and convenient.These tasks requires the\nability to successfully ingress through the rectangular openings or windows of\nthe target structure.In this paper, a method to robustly detect the window in\nthe surrounding using basic image processing techniques and efficient distance\nmeasure, is proposed.Furthermore, a navigation scheme which incorporates this\ndetection method for performing navigation task has also been proposed.The\nwhole navigation task is performed and tested in the simulation environment\nGAZEBO.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 06:31:13 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Pachauri", "Abhinav", ""], ["More", "Vikrant", ""], ["Gaidhani", "Pradeep", ""], ["Gupta", "Nitin", ""]]}, {"id": "1607.07032", "submitter": "Kaiming He", "authors": "Liliang Zhang and Liang Lin and Xiaodan Liang and Kaiming He", "title": "Is Faster R-CNN Doing Well for Pedestrian Detection?", "comments": "To appear in ECCV 2016, 15 pages, 5 figures (v2: fixed some typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting pedestrian has been arguably addressed as a special topic beyond\ngeneral object detection. Although recent deep learning object detectors such\nas Fast/Faster R-CNN [1, 2] have shown excellent performance for general object\ndetection, they have limited success for detecting pedestrian, and previous\nleading pedestrian detectors were in general hybrid methods combining\nhand-crafted and deep convolutional features. In this paper, we investigate\nissues involving Faster R-CNN [2] for pedestrian detection. We discover that\nthe Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a\nstand-alone pedestrian detector, but surprisingly, the downstream classifier\ndegrades the results. We argue that two reasons account for the unsatisfactory\naccuracy: (i) insufficient resolution of feature maps for handling small\ninstances, and (ii) lack of any bootstrapping strategy for mining hard negative\nexamples. Driven by these observations, we propose a very simple but effective\nbaseline for pedestrian detection, using an RPN followed by boosted forests on\nshared, high-resolution convolutional feature maps. We comprehensively evaluate\nthis method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting\ncompetitive accuracy and good speed. Code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 11:56:13 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 03:35:19 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Zhang", "Liliang", ""], ["Lin", "Liang", ""], ["Liang", "Xiaodan", ""], ["He", "Kaiming", ""]]}, {"id": "1607.07043", "submitter": "Amir Shahroudy", "authors": "Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang", "title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition - analysis of human actions based on 3D skeleton data -\nbecomes popular recently due to its succinctness, robustness, and\nview-invariant representation. Recent attempts on this problem suggested to\ndevelop RNN-based learning methods to model the contextual dependency in the\ntemporal domain. In this paper, we extend this idea to spatio-temporal domains\nto analyze the hidden sources of action-related information within the input\ndata over both domains concurrently. Inspired by the graphical structure of the\nhuman skeleton, we further propose a more powerful tree-structure based\ntraversal method. To handle the noise and occlusion in 3D skeleton data, we\nintroduce new gating mechanism within LSTM to learn the reliability of the\nsequential input data and accordingly adjust its effect on updating the\nlong-term context information stored in the memory cell. Our method achieves\nstate-of-the-art performance on 4 challenging benchmark datasets for 3D human\naction analysis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 13:39:11 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Xu", "Dong", ""], ["Wang", "Gang", ""]]}, {"id": "1607.07129", "submitter": "Yuan Gao", "authors": "Yuan Gao and Alan L. Yuille", "title": "Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure\n  Estimation from Single and Multiple Images", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many man-made objects have intrinsic symmetries and Manhattan structure. By\nassuming an orthographic projection model, this paper addresses the estimation\nof 3D structures and camera projection using symmetry and/or Manhattan\nstructure cues, which occur when the input is single- or multiple-image from\nthe same category, e.g., multiple different cars. Specifically, analysis on the\nsingle image case implies that Manhattan alone is sufficient to recover the\ncamera projection, and then the 3D structure can be reconstructed uniquely\nexploiting symmetry. However, Manhattan structure can be difficult to observe\nfrom a single image due to occlusion. To this end, we extend to the\nmultiple-image case which can also exploit symmetry but does not require\nManhattan axes. We propose a novel rigid structure from motion method,\nexploiting symmetry and using multiple images from the same category as input.\nExperimental results on the Pascal3D+ dataset show that our method\nsignificantly outperforms baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 02:36:51 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 19:28:56 GMT"}, {"version": "v3", "created": "Wed, 29 Mar 2017 08:15:16 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Gao", "Yuan", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1607.07155", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai and Quanfu Fan and Rogerio S. Feris and Nuno Vasconcelos", "title": "A Unified Multi-scale Deep Convolutional Neural Network for Fast Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is\nproposed for fast multi-scale object detection. The MS-CNN consists of a\nproposal sub-network and a detection sub-network. In the proposal sub-network,\ndetection is performed at multiple output layers, so that receptive fields\nmatch objects of different scales. These complementary scale-specific detectors\nare combined to produce a strong multi-scale object detector. The unified\nnetwork is learned end-to-end, by optimizing a multi-task loss. Feature\nupsampling by deconvolution is also explored, as an alternative to input\nupsampling, to reduce the memory and computation costs. State-of-the-art object\ndetection performance, at up to 15 fps, is reported on datasets, such as KITTI\nand Caltech, containing a substantial number of small objects.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 05:15:31 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Cai", "Zhaowei", ""], ["Fan", "Quanfu", ""], ["Feris", "Rogerio S.", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1607.07160", "submitter": "Savas Ozkan", "authors": "Ersin Esen, Savas Ozkan, Ilkay Atil", "title": "Large-Scale Video Search with Efficient Temporal Voting Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a fast content-based video querying system for\nlarge-scale video search. The proposed system is distinguished from similar\nworks with two major contributions. First contribution is superiority of joint\nusage of repeated content representation and efficient hashing mechanisms.\nRepeated content representation is utilized with a simple yet robust feature,\nwhich is based on edge energy of frames. Each of the representation is\nconverted into hash code with Hamming Embedding method for further queries.\nSecond contribution is novel queue-based voting scheme that leads to modest\nmemory requirements with gradual memory allocation capability, contrary to\ncomplete brute-force temporal voting schemes. This aspect enables us to make\nqueries on large video databases conveniently, even on commodity computers with\nlimited memory capacity. Our results show that the system can respond to video\nqueries on a large video database with fast query times, high recall rate and\nvery low memory and disk requirements.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 06:41:43 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Esen", "Ersin", ""], ["Ozkan", "Savas", ""], ["Atil", "Ilkay", ""]]}, {"id": "1607.07215", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, Victor Lempitsky", "title": "DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation", "comments": "Fixed typos, 14 + 2 + 2 pages, ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the task of generating highly-realistic images of a\ngiven face with a redirected gaze. We treat this problem as a specific instance\nof conditional image generation and suggest a new deep architecture that can\nhandle this task very well as revealed by numerical comparison with prior art\nand a user study. Our deep architecture performs coarse-to-fine warping with an\nadditional intensity correction of individual pixels. All these operations are\nperformed in a feed-forward manner, and the parameters associated with\ndifferent operations are learned jointly in the end-to-end fashion. After\nlearning, the resulting neural network can synthesize images with manipulated\ngaze, while the redirection angle can be selected arbitrarily from a certain\nrange and provided as an input to the network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 11:27:46 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 07:04:43 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Kononenko", "Daniil", ""], ["Sungatullina", "Diana", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1607.07216", "submitter": "Niki Martinel", "authors": "Niki Martinel, Abir Das, Christian Micheloni, Amit K. Roy-Chowdhury", "title": "Temporal Model Adaptation for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is an open and challenging problem in computer\nvision. Majority of the efforts have been spent either to design the best\nfeature representation or to learn the optimal matching metric. Most approaches\nhave neglected the problem of adapting the selected features or the learned\nmodel over time. To address such a problem, we propose a temporal model\nadaptation scheme with human in the loop. We first introduce a\nsimilarity-dissimilarity learning method which can be trained in an incremental\nfashion by means of a stochastic alternating directions methods of multipliers\noptimization procedure. Then, to achieve temporal adaptation with limited human\neffort, we exploit a graph-based approach to present the user only the most\ninformative probe-gallery matches that should be used to update the model.\nResults on three datasets have shown that our approach performs on par or even\nbetter than state-of-the-art approaches while reducing the manual pairwise\nlabeling effort by about 80%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 11:30:03 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Martinel", "Niki", ""], ["Das", "Abir", ""], ["Micheloni", "Christian", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1607.07220", "submitter": "Liang Lin", "authors": "Yukai Shi and Keze Wang and Li Xu and Liang Lin", "title": "Local- and Holistic- Structure Preserving Image Super Resolution via\n  Deep Joint Component Learning", "comments": "Published on ICME 2016 (oral), 6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning based single image super resolution (SR)\napproaches focus on jointly learning representations for high-resolution (HR)\nand low-resolution (LR) image patch pairs to improve the quality of the\nsuper-resolved images. However, due to treat all image pixels equally without\nconsidering the salient structures, these approaches usually fail to produce\nvisual pleasant images with sharp edges and fine details. To address this\nissue, in this work we present a new novel SR approach, which replaces the main\nbuilding blocks of the classical interpolation pipeline by a flexible,\ncontent-adaptive deep neural networks. In particular, two well-designed\nstructure-aware components, respectively capturing local- and holistic- image\ncontents, are naturally incorporated into the fully-convolutional\nrepresentation learning to enhance the image sharpness and naturalness.\nExtensively evaluations on several standard benchmarks (e.g., Set5, Set14 and\nBSD200) demonstrate that our approach can achieve superior results, especially\non the image with salient structures, over many existing state-of-the-art SR\nmethods under both quantitative and qualitative measures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 11:45:48 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Shi", "Yukai", ""], ["Wang", "Keze", ""], ["Xu", "Li", ""], ["Lin", "Liang", ""]]}, {"id": "1607.07262", "submitter": "Kota Yamaguchi", "authors": "Sirion Vittayakorn and Takayuki Umeda and Kazuhiko Murasaki and Kyoko\n  Sudo and Takayuki Okatani and Kota Yamaguchi", "title": "Automatic Attribute Discovery with Neural Activations", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can a machine learn to recognize visual attributes emerging out of online\ncommunity without a definitive supervised dataset? This paper proposes an\nautomatic approach to discover and analyze visual attributes from a noisy\ncollection of image-text data on the Web. Our approach is based on the\nrelationship between attributes and neural activations in the deep network. We\ncharacterize the visual property of the attribute word as a divergence within\nweakly-annotated set of images. We show that the neural activations are useful\nfor discovering and learning a classifier that well agrees with human\nperception from the noisy real-world Web data. The empirical study suggests the\nlayered structure of the deep neural networks also gives us insights into the\nperceptual depth of the given word. Finally, we demonstrate that we can utilize\nhighly-activating neurons for finding semantically relevant regions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 13:30:10 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Vittayakorn", "Sirion", ""], ["Umeda", "Takayuki", ""], ["Murasaki", "Kazuhiko", ""], ["Sudo", "Kyoko", ""], ["Okatani", "Takayuki", ""], ["Yamaguchi", "Kota", ""]]}, {"id": "1607.07270", "submitter": "Francesco Solera", "authors": "Francesco Solera and Andrea Palazzi", "title": "A Statistical Test for Joint Distributions Equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a distribution-free test that can be used to determine whether any\ntwo joint distributions $p$ and $q$ are statistically different by inspection\nof a large enough set of samples. Following recent efforts from Long et al.\n[1], we rely on joint kernel distribution embedding to extend the kernel\ntwo-sample test of Gretton et al. [2] to the case of joint probability\ndistributions. Our main result can be directly applied to verify if a\ndataset-shift has occurred between training and test distributions in a\nlearning framework, without further assuming the shift has occurred only in the\ninput, in the target or in the conditional distribution.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 13:48:20 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Solera", "Francesco", ""], ["Palazzi", "Andrea", ""]]}, {"id": "1607.07295", "submitter": "Lluis Castrejon", "authors": "Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, Antonio\n  Torralba", "title": "Learning Aligned Cross-Modal Representations from Weakly Aligned Data", "comments": "Conference paper at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize cross-modal scenes well, they also learn an intermediate\nrepresentation not aligned across modalities, which is undesirable for\ncross-modal transfer applications. We present methods to regularize cross-modal\nconvolutional neural networks so that they have a shared representation that is\nagnostic of the modality. Our experiments suggest that our scene representation\ncan help transfer representations across modalities for retrieval. Moreover,\nour visualizations suggest that units emerge in the shared representation that\ntend to activate on consistent concepts independently of the modality.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 14:38:36 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Castrejon", "Lluis", ""], ["Aytar", "Yusuf", ""], ["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1607.07297", "submitter": "Nasser Mohieddin Abukhdeir", "authors": "Jianjin Dong and Irene A. Goldthorpe and Nasser Mohieddin Abukhdeir", "title": "Automated quantification of one-dimensional nanostructure alignment on\n  surfaces", "comments": "14 pages, 6 figures, submitted to Nanotechnology", "journal-ref": null, "doi": "10.1088/0957-4484/27/23/235701", "report-no": null, "categories": "physics.ins-det cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for automated quantification of the alignment of one-dimensional\nnanostructures from microscopy imaging is presented. Nanostructure alignment\nmetrics are formulated and shown to able to rigorously quantify the\norientational order of nanostructures within a two-dimensional domain\n(surface). A complementary image processing method is also presented which\nenables robust processing of microscopy images where overlapping nanostructures\nmight be present. Scanning electron microscopy (SEM) images of nanowire-covered\nsurfaces are analyzed using the presented methods and it is shown that past\nsingle parameter alignment metrics are insufficient for highly aligned domains.\nThrough the use of multiple parameter alignment metrics, automated quantitative\nanalysis of SEM images is shown to be possible and the alignment\ncharacteristics of different samples are able to be rigorously compared using a\nsimilarity metric. The results of this work provide researchers in nanoscience\nand nanotechnology with a rigorous method for the determination of\nstructure/property relationships where alignment of one-dimensional\nnanostructures is significant.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 20:04:14 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Dong", "Jianjin", ""], ["Goldthorpe", "Irene A.", ""], ["Abukhdeir", "Nasser Mohieddin", ""]]}, {"id": "1607.07304", "submitter": "Roberto Henschel", "authors": "Roberto Henschel, Laura Leal-Taix\\'e, Bodo Rosenhahn, Konrad Schindler", "title": "Tracking with multi-level features", "comments": "Submitted as an IEEE PAMI short article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel formulation of the multiple object tracking problem which\nintegrates low and mid-level features. In particular, we formulate the tracking\nproblem as a quadratic program coupling detections and dense point\ntrajectories. Due to the computational complexity of the initial QP, we propose\nan approximation by two auxiliary problems, a temporal and spatial association,\nwhere the temporal subproblem can be efficiently solved by a linear program and\nthe spatial association by a clustering algorithm. The objective function of\nthe QP is used in order to find the optimal number of clusters, where each\ncluster ideally represents one person. Evaluation is provided for multiple\nscenarios, showing the superiority of our method with respect to classic\ntracking-by-detection methods and also other methods that greedily integrate\nlow-level features.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 15:07:45 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Henschel", "Roberto", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Rosenhahn", "Bodo", ""], ["Schindler", "Konrad", ""]]}, {"id": "1607.07387", "submitter": "Francesco Silvestri", "authors": "Francesco Silvestri and Gerhard Reinelt and Christoph Schn\\\"orr", "title": "Symmetry-free SDP Relaxations for Affine Subspace Clustering", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider clustering problems where the goal is to determine an optimal\npartition of a given point set in Euclidean space in terms of a collection of\naffine subspaces. While there is vast literature on heuristics for this kind of\nproblem, such approaches are known to be susceptible to poor initializations\nand getting trapped in bad local optima. We alleviate these issues by\nintroducing a semidefinite relaxation based on Lasserre's method of moments.\nWhile a similiar approach is known for classical Euclidean clustering problems,\na generalization to our more general subspace scenario is not straightforward,\ndue to the high symmetry of the objective function that weakens any convex\nrelaxation. We therefore introduce a new mechanism for symmetry breaking based\non covering the feasible region with polytopes. Additionally, we introduce and\nanalyze a deterministic rounding heuristic.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 18:01:17 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Silvestri", "Francesco", ""], ["Reinelt", "Gerhard", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1607.07405", "submitter": "Ankur Handa", "authors": "Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John\n  McCormac, Andrew Davison", "title": "gvnn: Neural Network Library for Geometric Computer Vision", "comments": "Submitted to ECCV Workshop on Deep Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce gvnn, a neural network library in Torch aimed towards bridging\nthe gap between classic geometric computer vision and deep learning. Inspired\nby the recent success of Spatial Transformer Networks, we propose several new\nlayers which are often used as parametric transformations on the data in\ngeometric computer vision. These layers can be inserted within a neural network\nmuch in the spirit of the original spatial transformers and allow\nbackpropagation to enable end-to-end learning of a network involving any domain\nknowledge in geometric computer vision. This opens up applications in learning\ninvariance to 3D geometric transformation for place recognition, end-to-end\nvisual odometry, depth estimation and unsupervised learning through warping\nwith a parametric transformation for image reconstruction error.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 18:57:17 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 22:49:32 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 17:28:24 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Handa", "Ankur", ""], ["Bloesch", "Michael", ""], ["Patraucean", "Viorica", ""], ["Stent", "Simon", ""], ["McCormac", "John", ""], ["Davison", "Andrew", ""]]}, {"id": "1607.07427", "submitter": "Sunil Kumar Yadav", "authors": "S. K. Yadav, U. Reitebuch and K. Polthier", "title": "Mesh Denoising based on Normal Voting Tensor and Binary Optimization", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TVCG.2017.2740384", "report-no": null, "categories": "cs.CV cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tensor multiplication based smoothing algorithm that\nfollows a two step denoising method. Unlike other traditional averaging\napproaches, our approach uses an element based normal voting tensor to compute\nsmooth surfaces. By introducing a binary optimization on the proposed tensor\ntogether with a local binary neighborhood concept, our algorithm better retains\nsharp features and produces smoother umbilical regions than previous\napproaches. On top of that, we provide a stochastic analysis on the different\nkinds of noise based on the average edge length. The quantitative and visual\nresults demonstrate the performance our method is better compared to state of\nthe art smoothing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 20:39:37 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:52:53 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Yadav", "S. K.", ""], ["Reitebuch", "U.", ""], ["Polthier", "K.", ""]]}, {"id": "1607.07429", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Olga Russakovsky, Ali Farhadi, Ivan Laptev,\n  Abhinav Gupta", "title": "Much Ado About Time: Exhaustive Annotation of Temporal Data", "comments": "HCOMP 2016 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale annotated datasets allow AI systems to learn from and build upon\nthe knowledge of the crowd. Many crowdsourcing techniques have been developed\nfor collecting image annotations. These techniques often implicitly rely on the\nfact that a new input image takes a negligible amount of time to perceive. In\ncontrast, we investigate and determine the most cost-effective way of obtaining\nhigh-quality multi-label annotations for temporal data such as videos. Watching\neven a short 30-second video clip requires a significant time investment from a\ncrowd worker; thus, requesting multiple annotations following a single viewing\nis an important cost-saving strategy. But how many questions should we ask per\nvideo? We conclude that the optimal strategy is to ask as many questions as\npossible in a HIT (up to 52 binary questions after watching a 30-second video\nclip in our experiments). We demonstrate that while workers may not correctly\nanswer all questions, the cost-benefit analysis nevertheless favors consensus\nfrom multiple such cheap-yet-imperfect iterations over more complex\nalternatives. When compared with a one-question-per-video baseline, our method\nis able to achieve a 10% improvement in recall 76.7% ours versus 66.7%\nbaseline) at comparable precision (83.8% ours versus 83.0% baseline) in about\nhalf the annotation time (3.8 minutes ours compared to 7.1 minutes baseline).\nWe demonstrate the effectiveness of our method by collecting multi-label\nannotations of 157 human activities on 1,815 videos.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:51:42 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 01:20:32 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Russakovsky", "Olga", ""], ["Farhadi", "Ali", ""], ["Laptev", "Ivan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1607.07525", "submitter": "Jianming Zhang", "authors": "Jianming Zhang, Shugao Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit\n  Betke, Zhe Lin, Xiaohui Shen, Brian Price, Radomir Mech", "title": "Salient Object Subitizing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of Salient Object Subitizing, i.e. predicting the\nexistence and the number of salient objects in an image using holistic cues.\nThis task is inspired by the ability of people to quickly and accurately\nidentify the number of items within the subitizing range (1-4). To this end, we\npresent a salient object subitizing image dataset of about 14K everyday images\nwhich are annotated using an online crowdsourcing marketplace. We show that\nusing an end-to-end trained Convolutional Neural Network (CNN) model, we\nachieve prediction accuracy comparable to human performance in identifying\nimages with zero or one salient object. For images with multiple salient\nobjects, our model also provides significantly better than chance performance\nwithout requiring any localization process. Moreover, we propose a method to\nimprove the training of the CNN subitizing model by leveraging synthetic\nimages. In experiments, we demonstrate the accuracy and generalizability of our\nCNN subitizing model and its applications in salient object detection and image\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 02:26:01 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Zhang", "Jianming", ""], ["Ma", "Shugao", ""], ["Sameki", "Mehrnoosh", ""], ["Sclaroff", "Stan", ""], ["Betke", "Margrit", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Price", "Brian", ""], ["Mech", "Radomir", ""]]}, {"id": "1607.07539", "submitter": "Raymond A. Yeh", "authors": "Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark\n  Hasegawa-Johnson, Minh N. Do", "title": "Semantic Image Inpainting with Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image inpainting is a challenging task where large missing regions\nhave to be filled based on the available visual data. Existing methods which\nextract information from only a single image generally produce unsatisfactory\nresults due to the lack of high level context. In this paper, we propose a\nnovel method for semantic image inpainting, which generates the missing content\nby conditioning on the available data. Given a trained generative model, we\nsearch for the closest encoding of the corrupted image in the latent image\nmanifold using our context and prior losses. This encoding is then passed\nthrough the generative model to infer the missing content. In our method,\ninference is possible irrespective of how the missing content is structured,\nwhile the state-of-the-art learning based method requires specific information\nabout the holes in the training phase. Experiments on three datasets show that\nour method successfully predicts information in large missing regions and\nachieves pixel-level photorealism, significantly outperforming the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 04:52:48 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 20:21:21 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 16:29:21 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Yeh", "Raymond A.", ""], ["Chen", "Chen", ""], ["Lim", "Teck Yian", ""], ["Schwing", "Alexander G.", ""], ["Hasegawa-Johnson", "Mark", ""], ["Do", "Minh N.", ""]]}, {"id": "1607.07561", "submitter": "Jiqing Wu", "authors": "Jiqing Wu, Radu Timofte, and Luc Van Gool", "title": "Generic 3D Convolutional Fusion for image restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Also recently, exciting strides forward have been made in the area of image\nrestoration, particularly for image denoising and single image\nsuper-resolution. Deep learning techniques contributed to this significantly.\nThe top methods differ in their formulations and assumptions, so even if their\naverage performance may be similar, some work better on certain image types and\nimage regions than others. This complementarity motivated us to propose a novel\n3D convolutional fusion (3DCF) method. Unlike other methods adapted to\ndifferent tasks, our method uses the exact same convolutional network\narchitecture to address both image denois- ing and single image\nsuper-resolution. As a result, our 3DCF method achieves substantial\nimprovements (0.1dB-0.4dB PSNR) over the state-of-the-art methods that it\nfuses, and this on standard benchmarks for both tasks. At the same time, the\nmethod still is computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 07:02:55 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Wu", "Jiqing", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1607.07604", "submitter": "Santi Segui Dr.", "authors": "Santi Segu\\'i, Michal Drozdzal, Guillem Pascual, Petia Radeva,\n  Carolina Malagelada, Fernando Azpiroz, Jordi Vitri\\`a", "title": "Generic Feature Learning for Wireless Capsule Endoscopy Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation and analysis of the wireless capsule endoscopy recording\nis a complex task which requires sophisticated computer aided decision (CAD)\nsystems in order to help physicians with the video screening and, finally, with\nthe diagnosis. Most of the CAD systems in the capsule endoscopy share a common\nsystem design, but use very different image and video representations. As a\nresult, each time a new clinical application of WCE appears, new CAD system has\nto be designed from scratch. This characteristic makes the design of new CAD\nsystems a very time consuming. Therefore, in this paper we introduce a system\nfor small intestine motility characterization, based on Deep Convolutional\nNeural Networks, which avoids the laborious step of designing specific features\nfor individual motility events. Experimental results show the superiority of\nthe learned features over alternative classifiers constructed by using state of\nthe art hand-crafted features. In particular, it reaches a mean classification\naccuracy of 96% for six intestinal motility events, outperforming the other\nclassifiers by a large margin (a 14% relative performance increase).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:21:22 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Segu\u00ed", "Santi", ""], ["Drozdzal", "Michal", ""], ["Pascual", "Guillem", ""], ["Radeva", "Petia", ""], ["Malagelada", "Carolina", ""], ["Azpiroz", "Fernando", ""], ["Vitri\u00e0", "Jordi", ""]]}, {"id": "1607.07614", "submitter": "Marian George", "authors": "Marian George, Mandar Dixit, G\\'abor Zogg and Nuno Vasconcelos", "title": "Semantic Clustering for Robust Fine-Grained Scene Recognition", "comments": "Accepted at the European Conference on Computer Vision (ECCV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domain generalization, the knowledge learnt from one or multiple source\ndomains is transferred to an unseen target domain. In this work, we propose a\nnovel domain generalization approach for fine-grained scene recognition. We\nfirst propose a semantic scene descriptor that jointly captures the subtle\ndifferences between fine-grained scenes, while being robust to varying object\nconfigurations across domains. We model the occurrence patterns of objects in\nscenes, capturing the informativeness and discriminability of each object for\neach scene. We then transform such occurrences into scene probabilities for\neach scene image. Second, we argue that scene images belong to hidden semantic\ntopics that can be discovered by clustering our semantic descriptors. To\nevaluate the proposed method, we propose a new fine-grained scene dataset in\ncross-domain settings. Extensive experiments on the proposed dataset and three\nbenchmark scene datasets show the effectiveness of the proposed approach for\nfine-grained scene transfer, where we outperform state-of-the-art scene\nrecognition and domain generalization methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:46:48 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["George", "Marian", ""], ["Dixit", "Mandar", ""], ["Zogg", "G\u00e1bor", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1607.07639", "submitter": "Miguel Alejandro Duval-Poo", "authors": "Miguel A. Duval-Poo, Nicoletta Noceti, Francesca Odone, Ernesto De\n  Vito", "title": "Scale Invariant Interest Points with Shearlets", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2687122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shearlets are a relatively new directional multi-scale framework for signal\nanalysis, which have been shown effective to enhance signal discontinuities\nsuch as edges and corners at multiple scales. In this work we address the\nproblem of detecting and describing blob-like features in the shearlets\nframework. We derive a measure which is very effective for blob detection and\nclosely related to the Laplacian of Gaussian. We demonstrate the measure\nsatisfies the perfect scale invariance property in the continuous case. In the\ndiscrete setting, we derive algorithms for blob detection and keypoint\ndescription. Finally, we provide qualitative justifications of our findings as\nwell as a quantitative evaluation on benchmark data. We also report an\nexperimental evidence that our method is very suitable to deal with compressed\nand noisy images, thanks to the sparsity property of shearlets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 11:03:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Duval-Poo", "Miguel A.", ""], ["Noceti", "Nicoletta", ""], ["Odone", "Francesca", ""], ["De Vito", "Ernesto", ""]]}, {"id": "1607.07646", "submitter": "Moin Nabi", "authors": "Hamidreza Rabiee, Javad Haddadnia, Hossein Mousavi, Moin Nabi,\n  Vittorio Murino and Nicu Sebe", "title": "Emotion-Based Crowd Representation for Abnormality Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd behavior understanding, a model of crowd behavior need to be trained\nusing the information extracted from video sequences. Since there is no\nground-truth available in crowd datasets except the crowd behavior labels, most\nof the methods proposed so far are just based on low-level visual features.\nHowever, there is a huge semantic gap between low-level motion/appearance\nfeatures and high-level concept of crowd behaviors. In this paper we propose an\nattribute-based strategy to alleviate this problem. While similar strategies\nhave been recently adopted for object and action recognition, as far as we\nknow, we are the first showing that the crowd emotions can be used as\nattributes for crowd behavior understanding. The main idea is to train a set of\nemotion-based classifiers, which can subsequently be used to represent the\ncrowd motion. For this purpose, we collect a big dataset of video clips and\nprovide them with both annotations of \"crowd behaviors\" and \"crowd emotions\".\nWe show the results of the proposed method on our dataset, which demonstrate\nthat the crowd emotions enable the construction of more descriptive models for\ncrowd behaviors. We aim at publishing the dataset with the article, to be used\nas a benchmark for the communities.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 11:26:44 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Rabiee", "Hamidreza", ""], ["Haddadnia", "Javad", ""], ["Mousavi", "Hossein", ""], ["Nabi", "Moin", ""], ["Murino", "Vittorio", ""], ["Sebe", "Nicu", ""]]}, {"id": "1607.07660", "submitter": "Yoni Kasten", "authors": "Yoni Kasten, Gil Ben-Artzi, Shmuel Peleg, Michael Werman", "title": "Fundamental Matrices from Moving Objects Using Line Motion Barcodes", "comments": null, "journal-ref": "ECCV'16, Amsterdam, Oct. 2016, Vol II, pp. 220-118", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the epipolar geometry between cameras with very different\nviewpoints is often very difficult. The appearance of objects can vary greatly,\nand it is difficult to find corresponding feature points. Prior methods\nsearched for corresponding epipolar lines using points on the convex hull of\nthe silhouette of a single moving object. These methods fail when the scene\nincludes multiple moving objects. This paper extends previous work to scenes\nhaving multiple moving objects by using the \"Motion Barcodes\", a temporal\nsignature of lines. Corresponding epipolar lines have similar motion barcodes,\nand candidate pairs of corresponding epipoar lines are found by the similarity\nof their motion barcodes. As in previous methods we assume that cameras are\nrelatively stationary and that moving objects have already been extracted using\nbackground subtraction.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 12:16:51 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Kasten", "Yoni", ""], ["Ben-Artzi", "Gil", ""], ["Peleg", "Shmuel", ""], ["Werman", "Michael", ""]]}, {"id": "1607.07671", "submitter": "Holger Caesar", "authors": "Holger Caesar, Jasper Uijlings, Vittorio Ferrari", "title": "Region-based semantic segmentation with end-to-end training", "comments": "ECCV 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for semantic segmentation, the task of labeling\neach pixel in an image with a semantic class. Our method combines the\nadvantages of the two main competing paradigms. Methods based on region\nclassification offer proper spatial support for appearance measurements, but\ntypically operate in two separate stages, none of which targets pixel labeling\nperformance at the end of the pipeline. More recent fully convolutional methods\nare capable of end-to-end training for the final pixel labeling, but resort to\nfixed patches as spatial support. We show how to modify modern region-based\napproaches to enable end-to-end training for semantic segmentation. This is\nachieved via a differentiable region-to-pixel layer and a differentiable\nfree-form Region-of-Interest pooling layer. Our method improves the\nstate-of-the-art in terms of class-average accuracy with 64.0% on SIFT Flow and\n49.9% on PASCAL Context, and is particularly accurate at object boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 12:46:51 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Caesar", "Holger", ""], ["Uijlings", "Jasper", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1607.07680", "submitter": "Yifan Wang", "authors": "Yifan Wang, Lijun Wang, Hongyu Wang, Peihua Li", "title": "End-to-End Image Super-Resolution via Deep and Shallow Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One impressive advantage of convolutional neural networks (CNNs) is their\nability to automatically learn feature representation from raw pixels,\neliminating the need for hand-designed procedures. However, recent methods for\nsingle image super-resolution (SR) fail to maintain this advantage. They\nutilize CNNs in two decoupled steps, i.e., first upsampling the low resolution\n(LR) image to the high resolution (HR) size with hand-designed techniques\n(e.g., bicubic interpolation), and then applying CNNs on the upsampled LR image\nto reconstruct HR results. In this paper, we seek an alternative and propose a\nnew image SR method, which jointly learns the feature extraction, upsampling\nand HR reconstruction modules, yielding a completely end-to-end trainable deep\nCNN. As opposed to existing approaches, the proposed method conducts upsampling\nin the latent feature space with filters that are optimized for the task of\nimage SR. In addition, the HR reconstruction is performed in a multi-scale\nmanner to simultaneously incorporate both short- and long-range contextual\ninformation, ensuring more accurate restoration of HR images. To facilitate\nnetwork training, a new training approach is designed, which jointly trains the\nproposed deep network with a relatively shallow network, leading to faster\nconvergence and more superior performance. The proposed method is extensively\nevaluated on widely adopted data sets and improves the performance of\nstate-of-the-art methods with a considerable margin. Moreover, in-depth\nablation studies are conducted to verify the contribution of different network\ndesigns to image SR, providing additional insights for future research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 13:15:53 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Wang", "Yifan", ""], ["Wang", "Lijun", ""], ["Wang", "Hongyu", ""], ["Li", "Peihua", ""]]}, {"id": "1607.07695", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal Ertugrul, Mete Ozay, Fatos Tunay Yarman Vural", "title": "Hierarchical Multi-resolution Mesh Networks for Brain Decoding", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework, called Hierarchical Multi-resolution Mesh\nNetworks (HMMNs), which establishes a set of brain networks at multiple time\nresolutions of fMRI signal to represent the underlying cognitive process. The\nsuggested framework, first, decomposes the fMRI signal into various frequency\nsubbands using wavelet transforms. Then, a brain network, called mesh network,\nis formed at each subband by ensembling a set of local meshes. The locality\naround each anatomic region is defined with respect to a neighborhood system\nbased on functional connectivity. The arc weights of a mesh are estimated by\nridge regression formed among the average region time series. In the final\nstep, the adjacency matrices of mesh networks obtained at different subbands\nare ensembled for brain decoding under a hierarchical learning architecture,\ncalled, fuzzy stacked generalization (FSG). Our results on Human Connectome\nProject task-fMRI dataset reflect that the suggested HMMN model can\nsuccessfully discriminate tasks by extracting complementary information\nobtained from mesh arc weights of multiple subbands. We study the topological\nproperties of the mesh networks at different resolutions using the network\nmeasures, namely, node degree, node strength, betweenness centrality and global\nefficiency; and investigate the connectivity of anatomic regions, during a\ncognitive task. We observe significant variations among the network topologies\nobtained for different subbands. We, also, analyze the diversity properties of\nclassifier ensemble, trained by the mesh networks in multiple subbands and\nobserve that the classifiers in the ensemble collaborate with each other to\nfuse the complementary information freed at each subband. We conclude that the\nfMRI data, recorded during a cognitive task, embed diverse information across\nthe anatomic regions at each resolution.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 17:26:31 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 20:42:47 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ertugrul", "Itir Onal", ""], ["Ozay", "Mete", ""], ["Vural", "Fatos Tunay Yarman", ""]]}, {"id": "1607.07697", "submitter": "Raj Bhagath Pudi", "authors": "P Raj Bhagath, Kallol Mallick, Jayanta Mukherjee, Sudipta Mukopadhayay", "title": "Low-complexity feedback-channel-free distributed video coding using\n  Local Rank Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new feedback-channel-free Distributed Video\nCoding (DVC) algorithm using Local Rank Transform (LRT). The encoder computes\nLRT by considering selected neighborhood pixels of Wyner-Ziv frame. The ranks\nfrom the modified LRT are merged, and their positions are entropy coded and\nsent to the decoder. In addition, means of each block of Wyner-Ziv frame are\nalso transmitted to assist motion estimation. Using these measurements, the\ndecoder generates side information (SI) by implementing motion estimation and\ncompensation in LRT domain. An iterative algorithm is executed on SI using LRT\nto reconstruct the Wyner-Ziv frame. Experimental results show that the coding\nefficiency of our codec is close to the efficiency of pixel domain distributed\nvideo coders based on Low-Density Parity Check and Accumulate (LDPCA) or turbo\ncodes, with less encoder complexity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 06:13:21 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Bhagath", "P Raj", ""], ["Mallick", "Kallol", ""], ["Mukherjee", "Jayanta", ""], ["Mukopadhayay", "Sudipta", ""]]}, {"id": "1607.07716", "submitter": "Junhwa Hur", "authors": "Junhwa Hur, Stefan Roth", "title": "Joint Optical Flow and Temporally Consistent Semantic Segmentation", "comments": "14 pages, Accepted for CVRSUAD workshop at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance and demands of visual scene understanding have been steadily\nincreasing along with the active development of autonomous systems.\nConsequently, there has been a large amount of research dedicated to semantic\nsegmentation and dense motion estimation. In this paper, we propose a method\nfor jointly estimating optical flow and temporally consistent semantic\nsegmentation, which closely connects these two problem domains and leverages\neach other. Semantic segmentation provides information on plausible physical\nmotion to its associated pixels, and accurate pixel-level temporal\ncorrespondences enhance the accuracy of semantic segmentation in the temporal\ndomain. We demonstrate the benefits of our approach on the KITTI benchmark,\nwhere we observe performance gains for flow and segmentation. We achieve\nstate-of-the-art optical flow results, and outperform all published algorithms\nby a large margin on challenging, but crucial dynamic objects.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 14:25:37 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "1607.07770", "submitter": "Behrooz Mahasseni", "authors": "Behrooz Mahasseni, Sinisa Todorovic, and Alan Fern", "title": "Approximate Policy Iteration for Budgeted Semantic Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates and presents a solution to the new problem of budgeted\nsemantic video segmentation. Given a video, the goal is to accurately assign a\nsemantic class label to every pixel in the video within a specified time\nbudget. Typical approaches to such labeling problems, such as Conditional\nRandom Fields (CRFs), focus on maximizing accuracy but do not provide a\nprincipled method for satisfying a time budget. For video data, the time\nrequired by CRF and related methods is often dominated by the time to compute\nlow-level descriptors of supervoxels across the video. Our key contribution is\nthe new budgeted inference framework for CRF models that intelligently selects\nthe most useful subsets of descriptors to run on subsets of supervoxels within\nthe time budget. The objective is to maintain an accuracy as close as possible\nto the CRF model with no time bound, while remaining within the time budget.\nOur second contribution is the algorithm for learning a policy for the sparse\nselection of supervoxels and their descriptors for budgeted CRF inference. This\nlearning algorithm is derived by casting our problem in the framework of Markov\nDecision Processes, and then instantiating a state-of-the-art policy learning\nalgorithm known as Classification-Based Approximate Policy Iteration. Our\nexperiments on multiple video datasets show that our learning approach and\nframework is able to significantly reduce computation time, and maintain\ncompetitive accuracy under varying budgets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:58:32 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Mahasseni", "Behrooz", ""], ["Todorovic", "Sinisa", ""], ["Fern", "Alan", ""]]}, {"id": "1607.07983", "submitter": "Zhaoyang Lv", "authors": "Zhaoyang Lv, Chris Beall, Pablo F. Alcantarilla, Fuxin Li, Zsolt Kira,\n  Frank Dellaert", "title": "A Continuous Optimization Approach for Efficient and Accurate Scene Flow", "comments": "Accepted in ECCV 2016. Please refer to the ECCV16-springer for\n  detailed information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a continuous optimization method for solving dense 3D scene flow\nproblems from stereo imagery. As in recent work, we represent the dynamic 3D\nscene as a collection of rigidly moving planar segments. The scene flow problem\nthen becomes the joint estimation of pixel-to-segment assignment, 3D position,\nnormal vector and rigid motion parameters for each segment, leading to a\ncomplex and expensive discrete-continuous optimization problem. In contrast, we\npropose a purely continuous formulation which can be solved more efficiently.\nUsing a fine superpixel segmentation that is fixed a-priori, we propose a\nfactor graph formulation that decomposes the problem into photometric,\ngeometric, and smoothing constraints. We initialize the solution with a novel,\nhigh-quality initialization method, then independently refine the geometry and\nmotion of the scene, and finally perform a global non-linear refinement using\nLevenberg-Marquardt. We evaluate our method in the challenging KITTI Scene Flow\nbenchmark, ranking in third position, while being 3 to 30 times faster than the\ntop competitors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 07:07:07 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Lv", "Zhaoyang", ""], ["Beall", "Chris", ""], ["Alcantarilla", "Pablo F.", ""], ["Li", "Fuxin", ""], ["Kira", "Zsolt", ""], ["Dellaert", "Frank", ""]]}, {"id": "1607.07987", "submitter": "Hosein M. Golshan", "authors": "Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud,\n  Mohammad H. Mahoor", "title": "A Multiple Kernel Learning Approach for Human Behavioral Task\n  Classification using STN-LFP Signal", "comments": "38th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Scociety", "journal-ref": null, "doi": "10.1109/EMBC.2016.7590878", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Brain Stimulation (DBS) has gained increasing attention as an effective\nmethod to mitigate Parkinsons disease (PD) disorders. Existing DBS systems are\nopen-loop such that the system parameters are not adjusted automatically based\non patients behavior. Classification of human behavior is an important step in\nthe design of the next generation of DBS systems that are closed-loop. This\npaper presents a classification approach to recognize such behavioral tasks\nusing the subthalamic nucleus (STN) Local Field Potential (LFP) signals. In our\napproach, we use the time-frequency representation (spectrogram) of the raw LFP\nsignals recorded from left and right STNs as the feature vectors. Then these\nfeatures are combined together via Support Vector Machines (SVM) with Multiple\nKernel Learning (MKL) formulation. The MKL-based classification method is\nutilized to classify different tasks: button press, mouth movement, speech, and\narm movement. Our experiments show that the lp-norm MKL significantly\noutperforms single kernel SVM-based classifiers in classifying behavioral tasks\nof five subjects even using signals acquired with a low sampling rate of 10 Hz.\nThis leads to a lower computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 07:28:40 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Golshan", "Hosein M.", ""], ["Hebb", "Adam O.", ""], ["Hanrahan", "Sara J.", ""], ["Nedrud", "Joshua", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1607.07988", "submitter": "Gernot Riegler", "authors": "Gernot Riegler, Matthias R\\\"uther, Horst Bischof", "title": "ATGV-Net: Accurate Depth Super-Resolution", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel approach for single depth map\nsuper-resolution. Modern consumer depth sensors, especially Time-of-Flight\nsensors, produce dense depth measurements, but are affected by noise and have a\nlow lateral resolution. We propose a method that combines the benefits of\nrecent advances in machine learning based single image super-resolution, i.e.\ndeep convolutional networks, with a variational method to recover accurate\nhigh-resolution depth maps. In particular, we integrate a variational method\nthat models the piecewise affine structures apparent in depth data via an\nanisotropic total generalized variation regularization term on top of a deep\nnetwork. We call our method ATGV-Net and train it end-to-end by unrolling the\noptimization procedure of the variational method. To train deep networks, a\nlarge corpus of training data with accurate ground-truth is required. We\ndemonstrate that it is feasible to train our method solely on synthetic data\nthat we generate in large quantities for this task. Our evaluations show that\nwe achieve state-of-the-art results on three different benchmarks, as well as\non a challenging Time-of-Flight dataset, all without utilizing an additional\nintensity image as guidance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 07:29:08 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Riegler", "Gernot", ""], ["R\u00fcther", "Matthias", ""], ["Bischof", "Horst", ""]]}, {"id": "1607.08022", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "Instance Normalization: The Missing Ingredient for Fast Stylization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It this paper we revisit the fast stylization method introduced in Ulyanov\net. al. (2016). We show how a small change in the stylization architecture\nresults in a significant qualitative improvement in the generated images. The\nchange is limited to swapping batch normalization with instance normalization,\nand to apply the latter both at training and testing times. The resulting\nmethod can be used to train high-performance architectures for real-time image\ngeneration. The code will is made available on github at\nhttps://github.com/DmitryUlyanov/texture_nets. Full paper can be found at\narXiv:1701.02096.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 10:23:00 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 14:35:57 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 14:21:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1607.08040", "submitter": "Bohan Zhuang", "authors": "Bohan Zhuang, Lijun Wang, Huchuan Lu", "title": "Visual Tracking via Shallow and Deep Collaborative Model", "comments": "Undergraduate Thesis, appearing in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust tracking method based on the collaboration\nof a generative model and a discriminative classifier, where features are\nlearned by shallow and deep architectures, respectively. For the generative\nmodel, we introduce a block-based incremental learning scheme, in which a local\nbinary mask is constructed to deal with occlusion. The similarity degrees\nbetween the local patches and their corresponding subspace are integrated to\nformulate a more accurate global appearance model. In the discriminative model,\nwe exploit the advances of deep learning architectures to learn generic\nfeatures which are robust to both background clutters and foreground appearance\nvariations. To this end, we first construct a discriminative training set from\nauxiliary video sequences. A deep classification neural network is then trained\noffline on this training set. Through online fine-tuning, both the hierarchical\nfeature extractor and the classifier can be adapted to the appearance change of\nthe target for effective online tracking. The collaboration of these two models\nachieves a good balance in handling occlusion and target appearance change,\nwhich are two contradictory challenging factors in visual tracking. Both\nquantitative and qualitative evaluations against several state-of-the-art\nalgorithms on challenging image sequences demonstrate the accuracy and the\nrobustness of the proposed tracker.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 11:18:12 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Zhuang", "Bohan", ""], ["Wang", "Lijun", ""], ["Lu", "Huchuan", ""]]}, {"id": "1607.08064", "submitter": "Christian Bailer", "authors": "Christian Bailer and Kiran Varanasi and Didier Stricker", "title": "CNN-based Patch Matching for Optical Flow with Thresholded Hinge\n  Embedding Loss", "comments": "Fixed bracket error in equation 3 (it has no major influence in the\n  approach, but on the optimal t value)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based approaches have not yet achieved their full potential in\noptical flow estimation, where their performance still trails heuristic\napproaches. In this paper, we present a CNN based patch matching approach for\noptical flow estimation. An important contribution of our approach is a novel\nthresholded loss for Siamese networks. We demonstrate that our loss performs\nclearly better than existing losses. It also allows to speed up training by a\nfactor of 2 in our tests. Furthermore, we present a novel way for calculating\nCNN based features for different image scales, which performs better than\nexisting methods. We also discuss new ways of evaluating the robustness of\ntrained features for the application of patch matching for optical flow. An\ninteresting discovery in our paper is that low-pass filtering of feature maps\ncan increase the robustness of features created by CNNs. We proved the\ncompetitive performance of our approach by submitting it to the KITTI 2012,\nKITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art\nresults on all three datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 12:41:00 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:29:19 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 18:57:55 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 06:28:24 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bailer", "Christian", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1607.08085", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Improving Semantic Embedding Consistency by Metric Learning for\n  Zero-Shot Classification", "comments": "in ECCV 2016, Oct 2016, amsterdam, Netherlands. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of zero-shot image classification. The key\ncontribution of the proposed approach is to control the semantic embedding of\nimages -- one of the main ingredients of zero-shot learning -- by formulating\nit as a metric learning problem. The optimized empirical criterion associates\ntwo types of sub-task constraints: metric discriminating capacity and accurate\nattribute prediction. This results in a novel expression of zero-shot learning\nnot requiring the notion of class in the training phase: only pairs of\nimage/attributes, augmented with a consistency indicator, are given as ground\ntruth. At test time, the learned model can predict the consistency of a test\nimage with a given set of attributes , allowing flexible ways to produce\nrecognition inferences. Despite its simplicity, the proposed approach gives\nstate-of-the-art results on four challenging datasets used for zero-shot\nrecognition evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:35:16 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1607.08112", "submitter": "Steffen Urban", "authors": "Steffen Urban, Jens Leitloff, Stefan Hinz", "title": "MLPnP - A Real-Time Maximum Likelihood Solution to the\n  Perspective-n-Point Problem", "comments": "Submitted to the ISPRS congress (2016) in Prague. Oral Presentation.\n  Published in ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., III-3,\n  131-138", "journal-ref": null, "doi": "10.5194/isprs-annals-III-3-131-2016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a statistically optimal solution to the Perspective-n-Point\n(PnP) problem is presented. Many solutions to the PnP problem are geometrically\noptimal, but do not consider the uncertainties of the observations. In\naddition, it would be desirable to have an internal estimation of the accuracy\nof the estimated rotation and translation parameters of the camera pose. Thus,\nwe propose a novel maximum likelihood solution to the PnP problem, that\nincorporates image observation uncertainties and remains real-time capable at\nthe same time. Further, the presented method is general, as is works with 3D\ndirection vectors instead of 2D image points and is thus able to cope with\narbitrary central camera models. This is achieved by projecting (and thus\nreducing) the covariance matrices of the observations to the corresponding\nvector tangent space.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:23:52 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Urban", "Steffen", ""], ["Leitloff", "Jens", ""], ["Hinz", "Stefan", ""]]}, {"id": "1607.08128", "submitter": "Angjoo Kanazawa", "authors": "Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler,\n  Javier Romero, Michael J. Black", "title": "Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a\n  Single Image", "comments": "To appear in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first method to automatically estimate the 3D pose of the\nhuman body as well as its 3D shape from a single unconstrained image. We\nestimate a full 3D mesh and show that 2D joints alone carry a surprising amount\nof information about body shape. The problem is challenging because of the\ncomplexity of the human body, articulation, occlusion, clothing, lighting, and\nthe inherent ambiguity in inferring 3D from 2D. To solve this, we first use a\nrecently published CNN-based method, DeepCut, to predict (bottom-up) the 2D\nbody joint locations. We then fit (top-down) a recently published statistical\nbody shape model, called SMPL, to the 2D joints. We do so by minimizing an\nobjective function that penalizes the error between the projected 3D model\njoints and detected 2D joints. Because SMPL captures correlations in human\nshape across the population, we are able to robustly fit it to very little\ndata. We further leverage the 3D model to prevent solutions that cause\ninterpenetration. We evaluate our method, SMPLify, on the Leeds Sports,\nHumanEva, and Human3.6M datasets, showing superior pose accuracy with respect\nto the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:46:36 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bogo", "Federica", ""], ["Kanazawa", "Angjoo", ""], ["Lassner", "Christoph", ""], ["Gehler", "Peter", ""], ["Romero", "Javier", ""], ["Black", "Michael J.", ""]]}, {"id": "1607.08129", "submitter": "Robert Amelard", "authors": "Robert Amelard, David A Clausi, Alexander Wong", "title": "Spatial probabilistic pulsatility model for enhancing\n  photoplethysmographic imaging systems", "comments": null, "journal-ref": null, "doi": "10.1117/1.JBO.21.11.116010", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photolethysmographic imaging (PPGI) is a widefield non-contact biophotonic\ntechnology able to remotely monitor cardiovascular function over anatomical\nareas. Though spatial context can provide increased physiological insight,\nexisting PPGI systems rely on coarse spatial averaging with no anatomical\npriors for assessing arterial pulsatility. Here, we developed a continuous\nprobabilistic pulsatility model for importance-weighted blood pulse waveform\nextraction. Using a data-driven approach, the model was constructed using a 23\nparticipant sample with large demographic variation (11/12 female/male, age\n11-60 years, BMI 16.4-35.1 kg$\\cdot$m$^{-2}$). Using time-synchronized\nground-truth waveforms, spatial correlation priors were computed and projected\ninto a co-aligned importance-weighted Cartesian space. A modified\nParzen-Rosenblatt kernel density estimation method was used to compute the\ncontinuous resolution-agnostic probabilistic pulsatility model. The model\nidentified locations that consistently exhibited pulsatility across the sample.\nBlood pulse waveform signals extracted with the model exhibited significantly\nstronger temporal correlation ($W=35,p<0.01$) and spectral SNR ($W=31,p<0.01$)\ncompared to uniform spatial averaging. Heart rate estimation was in strong\nagreement with true heart rate ($r^2=0.9619$, error $(\\mu,\\sigma)=(0.52,1.69)$\nbpm).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:49:33 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Amelard", "Robert", ""], ["Clausi", "David A", ""], ["Wong", "Alexander", ""]]}, {"id": "1607.08188", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff", "title": "Online Trajectory Segmentation and Summary With Applications to\n  Visualization and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory segmentation is the process of subdividing a trajectory into parts\neither by grouping points similar with respect to some measure of interest, or\nby minimizing a global objective function. Here we present a novel online\nalgorithm for segmentation and summary, based on point density along the\ntrajectory, and based on the nature of the naturally occurring structure of\nintermittent bouts of locomotive and local activity. We show an application to\nvisualization of trajectory datasets, and discuss the use of the summary as an\nindex allowing efficient queries which are otherwise impossible or\ncomputationally expensive, over very large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 14:50:45 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Resheff", "Yehezkel S.", ""]]}, {"id": "1607.08196", "submitter": "Lili Tao", "authors": "Lili Tao, Tilo Burghardt, Majid Mirmehdi, Dima Damen, Ashley Cooper,\n  Sion Hannuna, Massimo Camplani, Adeline Paiement, Ian Craddock", "title": "Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at\n  Home", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for vision-based estimation of calorific\nexpenditure from RGB-D data - the first that is validated on physical gas\nexchange measurements and applied to daily living scenarios. Deriving a\nperson's energy expenditure from sensors is an important tool in tracking\nphysical activity levels for health and lifestyle monitoring. Most existing\nmethods use metabolic lookup tables (METs) for a manual estimate or systems\nwith inertial sensors which ultimately require users to wear devices. In\ncontrast, the proposed pose-invariant and individual-independent vision\nframework allows for a remote estimation of calorific expenditure. We\nintroduce, and evaluate our approach on, a new dataset called SPHERE-calorie,\nfor which visual estimates can be compared against simultaneously obtained,\nindirect calorimetry measures based on gas exchange. % based on per breath gas\nexchange. We conclude from our experiments that the proposed vision pipeline is\nsuitable for home monitoring in a controlled environment, with calorific\nexpenditure estimates above accuracy levels of commonly used manual estimations\nvia METs. With the dataset released, our work establishes a baseline for future\nresearch for this little-explored area of computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 17:47:44 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Tao", "Lili", ""], ["Burghardt", "Tilo", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""], ["Cooper", "Ashley", ""], ["Hannuna", "Sion", ""], ["Camplani", "Massimo", ""], ["Paiement", "Adeline", ""], ["Craddock", "Ian", ""]]}, {"id": "1607.08221", "submitter": "Yandong Guo", "authors": "Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao", "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a benchmark task and provide the associated datasets\nfor recognizing face images and link them to corresponding entity keys in a\nknowledge base. More specifically, we propose a benchmark task to recognize one\nmillion celebrities from their face images, by using all the possibly collected\nface images of this individual on the web as training data. The rich\ninformation provided by the knowledge base helps to conduct disambiguation and\nimprove the recognition accuracy, and contributes to various real-world\napplications, such as image captioning and news video analysis. Associated with\nthis task, we design and provide concrete measurement set, evaluation protocol,\nas well as training data. We also present in details our experiment setup and\nreport promising baseline results. Our benchmark task could lead to one of the\nlargest classification problems in computer vision. To the best of our\nknowledge, our training dataset, which contains 10M images in version 1, is the\nlargest publicly available one in the world.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 19:18:16 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Guo", "Yandong", ""], ["Zhang", "Lei", ""], ["Hu", "Yuxiao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1607.08236", "submitter": "David Phillips", "authors": "David B. Phillips, Ming-Jie Sun, Jonathan M. Taylor, Matthew P. Edgar,\n  Stephen M. Barnett, Graham G. Gibson and Miles J. Padgett", "title": "Adaptive foveated single-pixel imaging with dynamic super-sampling", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an alternative to conventional multi-pixel cameras, single-pixel cameras\nenable images to be recorded using a single detector that measures the\ncorrelations between the scene and a set of patterns. However, to fully sample\na scene in this way requires at least the same number of correlation\nmeasurements as there are pixels in the reconstructed image. Therefore\nsingle-pixel imaging systems typically exhibit low frame-rates. To mitigate\nthis, a range of compressive sensing techniques have been developed which rely\non a priori knowledge of the scene to reconstruct images from an under-sampled\nset of measurements. In this work we take a different approach and adopt a\nstrategy inspired by the foveated vision systems found in the animal kingdom -\na framework that exploits the spatio-temporal redundancy present in many\ndynamic scenes. In our single-pixel imaging system a high-resolution foveal\nregion follows motion within the scene, but unlike a simple zoom, every frame\ndelivers new spatial information from across the entire field-of-view. Using\nthis approach we demonstrate a four-fold reduction in the time taken to record\nthe detail of rapidly evolving features, whilst simultaneously accumulating\ndetail of more slowly evolving regions over several consecutive frames. This\ntiered super-sampling technique enables the reconstruction of video streams in\nwhich both the resolution and the effective exposure-time spatially vary and\nadapt dynamically in response to the evolution of the scene. The methods\ndescribed here can complement existing compressive sensing approaches and may\nbe applied to enhance a variety of computational imagers that rely on\nsequential correlation measurements.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 15:27:42 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Phillips", "David B.", ""], ["Sun", "Ming-Jie", ""], ["Taylor", "Jonathan M.", ""], ["Edgar", "Matthew P.", ""], ["Barnett", "Stephen M.", ""], ["Gibson", "Graham G.", ""], ["Padgett", "Miles J.", ""]]}, {"id": "1607.08362", "submitter": "Konstantinos Raftopoulos", "authors": "Konstantinos A. Raftopoulos, Marin Ferecatu, Dionyssios D. Sourlas,\n  Stefanos D. Kollias", "title": "Incremental Noising and its Fractal Behavior", "comments": "10 pages, 5 figures", "journal-ref": "Int J Comput Vis (2017)", "doi": "10.1007/s11263-017-1034-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript is about further elucidating the concept of noising. The\nconcept of noising first appeared in \\cite{CVPR14}, in the context of curvature\nestimation and vertex localization on planar shapes. There are indications that\nnoising can play for global methods the role smoothing plays for local methods\nin this task. This manuscript is about investigating this claim by introducing\nincremental noising, in a recursive deterministic manner, analogous to how\nsmoothing is extended to progressive smoothing in similar tasks. As\ninvestigating the properties and behavior of incremental noising is the purpose\nof this manuscript, a surprising connection between incremental noising and\nprogressive smoothing is revealed by the experiments. To explain this\nphenomenon, the fractal and the space filling properties of the two methods\nrespectively, are considered in a unifying context.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 08:51:02 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 17:11:33 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Raftopoulos", "Konstantinos A.", ""], ["Ferecatu", "Marin", ""], ["Sourlas", "Dionyssios D.", ""], ["Kollias", "Stefanos D.", ""]]}, {"id": "1607.08366", "submitter": "Sebastian Stabinger MSc", "authors": "Sebastian Stabinger, Antonio Rodr\\'iguez-S\\'anchez, Justus Piater", "title": "25 years of CNNs: Can we compare to human abstraction capabilities?", "comments": "To appear in the proceedings of ICANN 2016, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We try to determine the progress made by convolutional neural networks over\nthe past 25 years in classifying images into abstractc lasses. For this purpose\nwe compare the performance of LeNet to that of GoogLeNet at classifying\nrandomly generated images which are differentiated by an abstract property\n(e.g., one class contains two objects of the same size, the other class two\nobjects of different sizes). Our results show that there is still work to do in\norder to solve vision problems humans are able to solve without much\ndifficulty.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:00:59 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Stabinger", "Sebastian", ""], ["Rodr\u00edguez-S\u00e1nchez", "Antonio", ""], ["Piater", "Justus", ""]]}, {"id": "1607.08368", "submitter": "Yusuke Uchida", "authors": "Yusuke Uchida", "title": "Local Feature Detectors, Descriptors, and Image Representations: A\n  Survey", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in both stable interest region detectors and robust and\ndistinctive descriptors, local feature-based image or object retrieval has\nbecome a popular research topic. %All of the local feature-based image\nretrieval system involves two important processes: local feature extraction and\nimage representation. The other key technology for image retrieval systems is\nimage representation such as the bag-of-visual words (BoVW), Fisher vector, or\nVector of Locally Aggregated Descriptors (VLAD) framework. In this paper, we\nreview local features and image representations for image retrieval. Because\nmany and many methods are proposed in this area, these methods are grouped into\nseveral classes and summarized. In addition, recent deep learning-based\napproaches for image retrieval are briefly reviewed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:10:19 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Uchida", "Yusuke", ""]]}, {"id": "1607.08378", "submitter": "Rahul  Rama Varior Mr.", "authors": "Rahul Rama Varior, Mrinal Haloi, and Gang Wang", "title": "Gated Siamese Convolutional Neural Network Architecture for Human\n  Re-Identification", "comments": "Accepted to ECCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pedestrians across multiple camera views, known as human\nre-identification, is a challenging research problem that has numerous\napplications in visual surveillance. With the resurgence of Convolutional\nNeural Networks (CNNs), several end-to-end deep Siamese CNN architectures have\nbeen proposed for human re-identification with the objective of projecting the\nimages of similar pairs (i.e. same identity) to be closer to each other and\nthose of dissimilar pairs to be distant from each other. However, current\nnetworks extract fixed representations for each image regardless of other\nimages which are paired with it and the comparison with other images is done\nonly at the final level. In this setting, the network is at risk of failing to\nextract finer local patterns that may be essential to distinguish positive\npairs from hard negative pairs. In this paper, we propose a gating function to\nselectively emphasize such fine common local patterns by comparing the\nmid-level features across pairs of images. This produces flexible\nrepresentations for the same image according to the images they are paired\nwith. We conduct experiments on the CUHK03, Market-1501 and VIPeR datasets and\ndemonstrate improved performance compared to a baseline Siamese CNN\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:40:18 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 16:28:58 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Varior", "Rahul Rama", ""], ["Haloi", "Mrinal", ""], ["Wang", "Gang", ""]]}, {"id": "1607.08381", "submitter": "Rahul  Rama Varior Mr.", "authors": "Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, and Gang Wang", "title": "A Siamese Long Short-Term Memory Architecture for Human\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pedestrians across multiple camera views known as human\nre-identification (re-identification) is a challenging problem in visual\nsurveillance. In the existing works concentrating on feature extraction,\nrepresentations are formed locally and independent of other regions. We present\na novel siamese Long Short-Term Memory (LSTM) architecture that can process\nimage regions sequentially and enhance the discriminative capability of local\nfeature representation by leveraging contextual information. The feedback\nconnections and internal gating mechanism of the LSTM cells enable our model to\nmemorize the spatial dependencies and selectively propagate relevant contextual\ninformation through the network. We demonstrate improved performance compared\nto the baseline algorithm with no LSTM units and promising results compared to\nstate-of-the-art methods on Market-1501, CUHK03 and VIPeR datasets.\nVisualization of the internal mechanism of LSTM cells shows meaningful patterns\ncan be learned by our method.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:43:52 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Varior", "Rahul Rama", ""], ["Shuai", "Bing", ""], ["Lu", "Jiwen", ""], ["Xu", "Dong", ""], ["Wang", "Gang", ""]]}, {"id": "1607.08414", "submitter": "Michael Wray", "authors": "Michael Wray, Davide Moltisanti, Walterio Mayol-Cuevas and Dima Damen", "title": "SEMBED: Semantic Embedding of Egocentric Action Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SEMBED, an approach for embedding an egocentric object interaction\nvideo in a semantic-visual graph to estimate the probability distribution over\nits potential semantic labels. When object interactions are annotated using\nunbounded choice of verbs, we embrace the wealth and ambiguity of these labels\nby capturing the semantic relationships as well as the visual similarities over\nmotion and appearance features. We show how SEMBED can interpret a challenging\ndataset of 1225 freely annotated egocentric videos, outperforming SVM\nclassification by more than 5%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 11:55:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 09:40:37 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Wray", "Michael", ""], ["Moltisanti", "Davide", ""], ["Mayol-Cuevas", "Walterio", ""], ["Damen", "Dima", ""]]}, {"id": "1607.08421", "submitter": "Anita Sellent", "authors": "Anita Sellent, Carsten Rother and Stefan Roth", "title": "Stereo Video Deblurring", "comments": "Accepted to the 14th European Conference on Computer Vision (ECCV\n  2016). Includes supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos acquired in low-light conditions often exhibit motion blur, which\ndepends on the motion of the objects relative to the camera. This is not only\nvisually unpleasing, but can hamper further processing. With this paper we are\nthe first to show how the availability of stereo video can aid the challenging\nvideo deblurring task. We leverage 3D scene flow, which can be estimated\nrobustly even under adverse conditions. We go beyond simply determining the\nobject motion in two ways: First, we show how a piecewise rigid 3D scene flow\nrepresentation allows to induce accurate blur kernels via local homographies.\nSecond, we exploit the estimated motion boundaries of the 3D scene flow to\nmitigate ringing artifacts using an iterative weighting scheme. Being aware of\n3D object motion, our approach can deal robustly with an arbitrary number of\nindependently moving objects. We demonstrate its benefit over state-of-the-art\nvideo deblurring using quantitative and qualitative experiments on rendered\nscenes and real videos.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 12:13:10 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Sellent", "Anita", ""], ["Rother", "Carsten", ""], ["Roth", "Stefan", ""]]}, {"id": "1607.08434", "submitter": "Stefano Alletto", "authors": "Stefano Alletto, Giuseppe Serra, Rita Cucchiara", "title": "Video Registration in Egocentric Vision under Day and Night Illumination\n  Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the spread of wearable devices and head mounted cameras, a wide range of\napplication requiring precise user localization is now possible. In this paper\nwe propose to treat the problem of obtaining the user position with respect to\na known environment as a video registration problem. Video registration, i.e.\nthe task of aligning an input video sequence to a pre-built 3D model, relies on\na matching process of local keypoints extracted on the query sequence to a 3D\npoint cloud. The overall registration performance is strictly tied to the\nactual quality of this 2D-3D matching, and can degrade if environmental\nconditions such as steep changes in lighting like the ones between day and\nnight occur. To effectively register an egocentric video sequence under these\nconditions, we propose to tackle the source of the problem: the matching\nprocess. To overcome the shortcomings of standard matching techniques, we\nintroduce a novel embedding space that allows us to obtain robust matches by\njointly taking into account local descriptors, their spatial arrangement and\ntheir temporal robustness. The proposal is evaluated using unconstrained\negocentric video sequences both in terms of matching quality and resulting\nregistration performance using different 3D models of historical landmarks. The\nresults show that the proposed method can outperform state of the art\nregistration algorithms, in particular when dealing with the challenges of\nnight and day sequences.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:00:03 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Alletto", "Stefano", ""], ["Serra", "Giuseppe", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1607.08438", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele", "title": "Faceless Person Recognition; Privacy Implications in Social Media", "comments": "Accepted to ECCV'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we shift more of our lives into the virtual domain, the volume of data\nshared on the web keeps increasing and presents a threat to our privacy. This\nworks contributes to the understanding of privacy implications of such data\nsharing by analysing how well people are recognisable in social media data. To\nfacilitate a systematic study we define a number of scenarios considering\nfactors such as how many heads of a person are tagged and if those heads are\nobfuscated or not. We propose a robust person recognition system that can\nhandle large variations in pose and clothing, and can be trained with few\ntraining samples. Our results indicate that a handful of images is enough to\nthreaten users' privacy, even in the presence of obfuscation. We show detailed\nexperimental results, and discuss their implications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:10:27 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Oh", "Seong Joon", ""], ["Benenson", "Rodrigo", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1607.08477", "submitter": "Yuxin Peng", "authors": "Jian Zhang, and Yuxin Peng", "title": "SSDH: Semi-supervised Deep Hashing for Large Scale Image Retrieval", "comments": "14 pages, accepted by IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2771332", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods have been widely used for efficient similarity retrieval on\nlarge scale image database. Traditional hashing methods learn hash functions to\ngenerate binary codes from hand-crafted features, which achieve limited\naccuracy since the hand-crafted features cannot optimally represent the image\ncontent and preserve the semantic similarity. Recently, several deep hashing\nmethods have shown better performance because the deep architectures generate\nmore discriminative feature representations. However, these deep hashing\nmethods are mainly designed for supervised scenarios, which only exploit the\nsemantic similarity information, but ignore the underlying data structures. In\nthis paper, we propose the semi-supervised deep hashing (SSDH) approach, to\nperform more effective hash function learning by simultaneously preserving\nsemantic similarity and underlying data structures. The main contributions are\nas follows: (1) We propose a semi-supervised loss to jointly minimize the\nempirical error on labeled data, as well as the embedding error on both labeled\nand unlabeled data, which can preserve the semantic similarity and capture the\nmeaningful neighbors on the underlying data structures for effective hashing.\n(2) A semi-supervised deep hashing network is designed to extensively exploit\nboth labeled and unlabeled data, in which we propose an online graph\nconstruction method to benefit from the evolving deep features during training\nto better capture semantic neighbors. To the best of our knowledge, the\nproposed deep network is the first deep hashing method that can perform hash\ncode learning and feature learning simultaneously in a semi-supervised fashion.\nExperimental results on 5 widely-used datasets show that our proposed approach\noutperforms the state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 14:30:21 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 07:52:53 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 03:23:28 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Zhang", "Jian", ""], ["Peng", "Yuxin", ""]]}, {"id": "1607.08481", "submitter": "Friederike Johanna Laus", "authors": "Friederike Laus, Mila Nikolova, Johannes Persch, Gabriele Steidl", "title": "A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second\n  Order Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlocal patch-based methods, in particular the Bayes' approach of Lebrun,\nBuades and Morel (2013), are considered as state-of-the-art methods for\ndenoising (color) images corrupted by white Gaussian noise of moderate\nvariance. This paper is the first attempt to generalize this technique to\nmanifold-valued images. Such images, for example images with phase or\ndirectional entries or with values in the manifold of symmetric positive\ndefinite matrices, are frequently encountered in real-world applications.\nGeneralizing the normal law to manifolds is not canonical and different\nattempts have been considered. Here we focus on a straightforward intrinsic\nmodel and discuss the relation to other approaches for specific manifolds. We\nreinterpret the Bayesian approach of Lebrun et al. (2013) in terms of minimum\nmean squared error estimation, which motivates our definition of a\ncorresponding estimator on the manifold. With this estimator at hand we present\na nonlocal patch-based method for the restoration of manifold-valued images.\nVarious proof of concept examples demonstrate the potential of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 14:39:13 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 13:56:57 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 09:22:32 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Laus", "Friederike", ""], ["Nikolova", "Mila", ""], ["Persch", "Johannes", ""], ["Steidl", "Gabriele", ""]]}, {"id": "1607.08539", "submitter": "Maciej Halber", "authors": "Maciej Halber and Thomas Funkhouser", "title": "Fine-To-Coarse Global Registration of RGB-D Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D scanning of indoor environments is important for many applications,\nincluding real estate, interior design, and virtual reality. However, it is\nstill challenging to register RGB-D images from a hand-held camera over a long\nvideo sequence into a globally consistent 3D model. Current methods often can\nlose tracking or drift and thus fail to reconstruct salient structures in large\nenvironments (e.g., parallel walls in different rooms). To address this\nproblem, we propose a \"fine-to-coarse\" global registration algorithm that\nleverages robust registrations at finer scales to seed detection and\nenforcement of new correspondence and structural constraints at coarser scales.\nTo test global registration algorithms, we provide a benchmark with 10,401\nmanually-clicked point correspondences in 25 scenes from the SUN3D dataset.\nDuring experiments with this benchmark, we find that our fine-to-coarse\nalgorithm registers long RGB-D sequences better than previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 17:19:46 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 15:59:00 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 04:55:29 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Halber", "Maciej", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1607.08569", "submitter": "Gernot Riegler", "authors": "Gernot Riegler, David Ferstl, Matthias R\\\"uther, Horst Bischof", "title": "A Deep Primal-Dual Network for Guided Depth Super-Resolution", "comments": "BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel method to increase the spatial resolution of\ndepth images. We combine a deep fully convolutional network with a non-local\nvariational method in a deep primal-dual network. The joint network computes a\nnoise-free, high-resolution estimate from a noisy, low-resolution input depth\nmap. Additionally, a high-resolution intensity image is used to guide the\nreconstruction in the network. By unrolling the optimization steps of a\nfirst-order primal-dual algorithm and formulating it as a network, we can train\nour joint method end-to-end. This not only enables us to learn the weights of\nthe fully convolutional network, but also to optimize all parameters of the\nvariational method and its optimization procedure. The training of such a deep\nnetwork requires a large dataset for supervision. Therefore, we generate\nhigh-quality depth maps and corresponding color images with a physically based\nrenderer. In an exhaustive evaluation we show that our method outperforms the\nstate-of-the-art on multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 18:49:55 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Riegler", "Gernot", ""], ["Ferstl", "David", ""], ["R\u00fcther", "Matthias", ""], ["Bischof", "Horst", ""]]}, {"id": "1607.08584", "submitter": "De-An Huang", "authors": "De-An Huang, Li Fei-Fei, Juan Carlos Niebles", "title": "Connectionist Temporal Modeling for Weakly Supervised Action Labeling", "comments": "To appear in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weakly-supervised framework for action labeling in video, where\nonly the order of occurring actions is required during training time. The key\nchallenge is that the per-frame alignments between the input (video) and label\n(action) sequences are unknown during training. We address this by introducing\nthe Extended Connectionist Temporal Classification (ECTC) framework to\nefficiently evaluate all possible alignments via dynamic programming and\nexplicitly enforce their consistency with frame-to-frame visual similarities.\nThis protects the model from distractions of visually inconsistent or\ndegenerated alignments without the need of temporal supervision. We further\nextend our framework to the semi-supervised case when a few frames are sparsely\nannotated in a video. With less than 1% of labeled frames per video, our method\nis able to outperform existing semi-supervised approaches and achieve\ncomparable performance to that of fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:35:50 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Huang", "De-An", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1607.08635", "submitter": "Zhengdong Zhang", "authors": "Amr Suleiman, Zhengdong Zhang, Vivienne Sze", "title": "A 58.6mW Real-Time Programmable Object Detector with Multi-Scale\n  Multi-Object Support Using Deformable Parts Model on 1920x1080 Video at 30fps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a programmable, energy-efficient and real-time object\ndetection accelerator using deformable parts models (DPM), with 2x higher\naccuracy than traditional rigid body models. With 8 deformable parts detection,\nthree methods are used to address the high computational complexity:\nclassification pruning for 33x fewer parts classification, vector quantization\nfor 15x memory size reduction, and feature basis projection for 2x reduction of\nthe cost of each classification. The chip is implemented in 65nm CMOS\ntechnology, and can process HD (1920x1080) images at 30fps without any off-chip\nstorage while consuming only 58.6mW (0.94nJ/pixel, 1168 GOPS/W). The chip has\ntwo classification engines to simultaneously detect two different classes of\nobjects. With a tested high throughput of 60fps, the classification engines can\nbe time multiplexed to detect even more than two object classes. It is energy\nscalable by changing the pruning factor or disabling the parts classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 19:20:33 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Suleiman", "Amr", ""], ["Zhang", "Zhengdong", ""], ["Sze", "Vivienne", ""]]}, {"id": "1607.08659", "submitter": "Helge Rhodin", "authors": "Helge Rhodin, Nadia Robertini, Dan Casas, Christian Richardt,\n  Hans-Peter Seidel, Christian Theobalt", "title": "General Automatic Human Shape and Motion Capture Using Volumetric\n  Contour Cues", "comments": "Accepted to ECCV 2016, added additional references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markerless motion capture algorithms require a 3D body with properly\npersonalized skeleton dimension and/or body shape and appearance to\nsuccessfully track a person. Unfortunately, many tracking methods consider\nmodel personalization a different problem and use manual or semi-automatic\nmodel initialization, which greatly reduces applicability. In this paper, we\npropose a fully automatic algorithm that jointly creates a rigged actor model\ncommonly used for animation - skeleton, volumetric shape, appearance, and\noptionally a body surface - and estimates the actor's motion from multi-view\nvideo input only. The approach is rigorously designed to work on footage of\ngeneral outdoor scenes recorded with very few cameras and without background\nsubtraction. Our method uses a new image formation model with analytic\nvisibility and analytically differentiable alignment energy. For\nreconstruction, 3D body shape is approximated as Gaussian density field. For\npose and shape estimation, we minimize a new edge-based alignment energy\ninspired by volume raycasting in an absorbing medium. We further propose a new\nstatistical human body model that represents the body surface, volumetric\nGaussian density, as well as variability in skeleton shape. Given any\nmulti-view sequence, our method jointly optimizes the pose and shape parameters\nof this model fully automatically in a spatiotemporal way.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 22:59:55 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 11:23:31 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rhodin", "Helge", ""], ["Robertini", "Nadia", ""], ["Casas", "Dan", ""], ["Richardt", "Christian", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1607.08665", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Sam Zeng, J. Andrew Bagnell and Martial Hebert", "title": "Introspective Perception: Learning to Predict Failures in Vision Systems", "comments": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots aspire for long-term autonomous operations in complex dynamic\nenvironments, the ability to reliably take mission-critical decisions in\nambiguous situations becomes critical. This motivates the need to build systems\nthat have situational awareness to assess how qualified they are at that moment\nto make a decision. We call this self-evaluating capability as introspection.\nIn this paper, we take a small step in this direction and propose a generic\nframework for introspective behavior in perception systems. Our goal is to\nlearn a model to reliably predict failures in a given system, with respect to a\ntask, directly from input sensor data. We present this in the context of\nvision-based autonomous MAV flight in outdoor natural environments, and show\nthat it effectively handles uncertain situations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 23:27:13 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Zeng", "Sam", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1607.08707", "submitter": "Wenkun Zhang", "authors": "Hanming Zhang, Liang Li, Kai Qiao, Linyuan Wang, Bin Yan, Lei Li,\n  Guoen Hu", "title": "Image Prediction for Limited-angle Tomography via Deep Learning with\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited angle problem is a challenging issue in x-ray computed tomography\n(CT) field. Iterative reconstruction methods that utilize the additional prior\ncan suppress artifacts and improve image quality, but unfortunately require\nincreased computation time. An interesting way is to restrain the artifacts in\nthe images reconstructed from the practical filtered back projection (FBP)\nmethod. Frikel and Quinto have proved that the streak artifacts in FBP results\ncould be characterized. It indicates that the artifacts created by FBP method\nhave specific and similar characteristics in a stationary limited-angle\nscanning configuration. Based on this understanding, this work aims at\ndeveloping a method to extract and suppress specific artifacts of FBP\nreconstructions for limited-angle tomography. A data-driven learning-based\nmethod is proposed based on a deep convolutional neural network. An end-to-end\nmapping between the FBP and artifact-free images is learned and the implicit\nfeatures involving artifacts will be extracted and suppressed via nonlinear\nmapping. The qualitative and quantitative evaluations of experimental results\nindicate that the proposed method show a stable and prospective performance on\nartifacts reduction and detail recovery for limited angle tomography. The\npresented strategy provides a simple and efficient approach for improving image\nquality of the reconstruction results from limited projection data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 07:31:18 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zhang", "Hanming", ""], ["Li", "Liang", ""], ["Qiao", "Kai", ""], ["Wang", "Linyuan", ""], ["Yan", "Bin", ""], ["Li", "Lei", ""], ["Hu", "Guoen", ""]]}, {"id": "1607.08764", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Shiv Surya, Srinivas S S Kruthiventi,\n  Venkatesh Babu R", "title": "SwiDeN : Convolutional Neural Networks For Depiction Invariant Object\n  Recognition", "comments": "Accepted at ACMMM 2016. The first two authors contributed equally.\n  Code and models at https://github.com/val-iisc/swiden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state of the art object recognition architectures achieve impressive\nperformance but are typically specialized for a single depictive style (e.g.\nphotos only, sketches only). In this paper, we present SwiDeN : our\nConvolutional Neural Network (CNN) architecture which recognizes objects\nregardless of how they are visually depicted (line drawing, realistic shaded\ndrawing, photograph etc.). In SwiDeN, we utilize a novel `deep' depictive\nstyle-based switching mechanism which appropriately addresses the\ndepiction-specific and depiction-invariant aspects of the problem. We compare\nSwiDeN with alternative architectures and prior work on a 50-category Photo-Art\ndataset containing objects depicted in multiple styles. Experimental results\nshow that SwiDeN outperforms other approaches for the depiction-invariant\nobject recognition problem.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 11:00:08 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Surya", "Shiv", ""], ["Kruthiventi", "Srinivas S S", ""], ["R", "Venkatesh Babu", ""]]}, {"id": "1607.08811", "submitter": "Pedro Herruzo", "authors": "Pedro Herruzo, Marc Bola\\~nos and Petia Radeva", "title": "Can a CNN Recognize Catalan Diet?", "comments": "9 pages, 6 figures, 6 tables", "journal-ref": null, "doi": "10.1063/1.4964956", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, we can find several diseases related to the unhealthy diet habits\nof the population, such as diabetes, obesity, anemia, bulimia and anorexia. In\nmany cases, these diseases are related to the food consumption of people.\nMediterranean diet is scientifically known as a healthy diet that helps to\nprevent many metabolic diseases. In particular, our work focuses on the\nrecognition of Mediterranean food and dishes. The development of this\nmethodology would allow to analise the daily habits of users with wearable\ncameras, within the topic of lifelogging. By using automatic mechanisms we\ncould build an objective tool for the analysis of the patient's behaviour,\nallowing specialists to discover unhealthy food patterns and understand the\nuser's lifestyle.\n  With the aim to automatically recognize a complete diet, we introduce a\nchallenging multi-labeled dataset related to Mediterranean diet called FoodCAT.\nThe first type of label provided consists of 115 food classes with an average\nof 400 images per dish, and the second one consists of 12 food categories with\nan average of 3800 pictures per class. This dataset will serve as a basis for\nthe development of automatic diet recognition. In this context, deep learning\nand more specifically, Convolutional Neural Networks (CNNs), currently are\nstate-of-the-art methods for automatic food recognition. In our work, we\ncompare several architectures for image classification, with the purpose of\ndiet recognition. Applying the best model for recognising food categories, we\nachieve a top-1 accuracy of 72.29\\%, and top-5 of 97.07\\%. In a complete diet\nrecognition of dishes from Mediterranean diet, enlarged with the Food-101\ndataset for international dishes recognition, we achieve a top-1 accuracy of\n68.07\\%, and top-5 of 89.53\\%, for a total of 115+101 food classes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:55:21 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Herruzo", "Pedro", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1607.08822", "submitter": "Peter Anderson", "authors": "Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould", "title": "SPICE: Semantic Propositional Image Caption Evaluation", "comments": "14 pages plus references, accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable interest in the task of automatically generating image\ncaptions. However, evaluation is challenging. Existing automatic evaluation\nmetrics are primarily sensitive to n-gram overlap, which is neither necessary\nnor sufficient for the task of simulating human judgment. We hypothesize that\nsemantic propositional content is an important component of human caption\nevaluation, and propose a new automated caption evaluation metric defined over\nscene graphs coined SPICE. Extensive evaluations across a range of models and\ndatasets indicate that SPICE captures human judgments over model-generated\ncaptions better than other automatic metrics (e.g., system-level correlation of\n0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and\n0.53 for METEOR). Furthermore, SPICE can answer questions such as `which\ncaption-generator best understands colors?' and `can caption-generators count?'\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 14:26:27 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Anderson", "Peter", ""], ["Fernando", "Basura", ""], ["Johnson", "Mark", ""], ["Gould", "Stephen", ""]]}, {"id": "1607.08905", "submitter": "Alexander Shekhovtsov", "authors": "Mengtian Li, Alexander Shekhovtsov, Daniel Huber", "title": "Complexity of Discrete Energy Minimization Problems", "comments": "ECCV'16 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete energy minimization is widely-used in computer vision and machine\nlearning for problems such as MAP inference in graphical models. The problem,\nin general, is notoriously intractable, and finding the global optimal solution\nis known to be NP-hard. However, is it possible to approximate this problem\nwith a reasonable ratio bound on the solution quality in polynomial time? We\nshow in this paper that the answer is no. Specifically, we show that general\nenergy minimization, even in the 2-label pairwise case, and planar energy\nminimization with three or more labels are exp-APX-complete. This finding rules\nout the existence of any approximation algorithm with a sub-exponential\napproximation ratio in the input size for these two problems, including\nconstant factor approximations. Moreover, we collect and review the\ncomputational complexity of several subclass problems and arrange them on a\ncomplexity scale consisting of three major complexity classes -- PO, APX, and\nexp-APX, corresponding to problems that are solvable, approximable, and\ninapproximable in polynomial time. Problems in the first two complexity classes\ncan serve as alternative tractable formulations to the inapproximable ones.\nThis paper can help vision researchers to select an appropriate model for an\napplication or guide them in designing new algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 19:34:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Li", "Mengtian", ""], ["Shekhovtsov", "Alexander", ""], ["Huber", "Daniel", ""]]}]