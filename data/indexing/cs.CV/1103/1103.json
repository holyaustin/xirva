[{"id": "1103.0120", "submitter": "Srimanta Kundu", "authors": "Srimanta Kundu (1), Nibaran Das and Mita Nasipuri", "title": "Automatic Detection of Ringworm using Local Binary Pattern (LBP)", "comments": "International Symposium on Medical Imaging: Perspectives on\n  Perception and Diagnostics (MED-IMAGE 2010) organized in conjunction with the\n  Seventh Indian Conference on Computer Vision, Graphics and Image Processing\n  (ICVGIP), 9-10th December, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel approach for automatic recognition of ring\nworm skin disease based on LBP (Local Binary Pattern) feature extracted from\nthe affected skin images. The proposed method is evaluated by extensive\nexperiments on the skin images collected from internet. The dataset is tested\nusing three different classifiers i.e. Bayesian, MLP and SVM. Experimental\nresults show that the proposed methodology efficiently discriminates between a\nring worm skin and a normal skin. It is a low cost technique and does not\nrequire any special imaging devices.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 10:06:31 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2011 20:04:52 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Kundu", "Srimanta", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1103.0540", "submitter": "Ling Shao", "authors": "Lijun Wang, Ling Shao", "title": "An Algorithm for Repairing Low-Quality Video Enhancement Techniques\n  Based on Trained Filter", "comments": "Part of the work is published as a journal paper titled \"Repairing\n  imperfect video enhancement algorithms using classification-based trained\n  filters\" in Signal, Image and Video Processing (Springer); Signal, Image and\n  Video Processing, 2011", "journal-ref": null, "doi": "10.1007/s11760-010-0202-8", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifarious image enhancement algorithms have been used in different\napplications. Still, some algorithms or modules are imperfect for practical\nuse. When the image enhancement modules have been fixed or combined by a series\nof algorithms, we need to repair them as a whole part without changing the\ninside. This report aims to find an algorithm based on trained filters to\nrepair low-quality image enhancement modules. A brief review on basic image\nenhancement techniques and pixel classification methods will be presented, and\nthe procedure of trained filters will be described step by step. The\nexperiments and result comparisons for this algorithm will be described in\ndetail.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 20:50:22 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Wang", "Lijun", ""], ["Shao", "Ling", ""]]}, {"id": "1103.0738", "submitter": "Soumen Bag", "authors": "Soumen Bag and Gaurav Harit", "title": "A Medial Axis Based Thinning Strategy for Character Images", "comments": "6 pages, 5 figures. In proceedings of the second National Conference\n  on Computer Vision, Pattern Recognition, Image Processing and Graphics\n  (NCVPRIPG), pp. 67-72, Jaipur, India, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thinning of character images is a big challenge. Removal of strokes or\ndeformities in thinning is a difficult problem. In this paper, we have proposed\na medial axis based thinning strategy used for performing skeletonization of\nprinted and handwritten character images. In this method, we have used shape\ncharacteristics of text to get skeleton of nearly same as the true character\nshape. This approach helps to preserve the local features and true shape of the\ncharacter images. The proposed algorithm produces one pixel width thin\nskeleton. As a by-product of our thinning approach, the skeleton also gets\nsegmented into strokes in vector form. Hence further stroke segmentation is not\nrequired. Experiment is done on printed English and Bengali characters and we\nobtain less spurious branches comparing with other thinning methods without any\npost processing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 17:31:48 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Bag", "Soumen", ""], ["Harit", "Gaurav", ""]]}, {"id": "1103.1077", "submitter": "Anton Osokin", "authors": "Anton Osokin, Dmitry Vetrov, Vladimir Kolmogorov", "title": "Submodular Decomposition Framework for Inference in Associative Markov\n  Networks with Global Constraints", "comments": "17 pages. Shorter version to appear in CVPR 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we address the problem of finding the most probable state of\ndiscrete Markov random field (MRF) with associative pairwise terms. Although of\npractical importance, this problem is known to be NP-hard in general. We\npropose a new type of MRF decomposition, submodular decomposition (SMD). Unlike\nexisting decomposition approaches SMD decomposes the initial problem into\nsubproblems corresponding to a specific class label while preserving the graph\nstructure of each subproblem. Such decomposition enables us to take into\naccount several types of global constraints in an efficient manner. We study\ntheoretical properties of the proposed approach and demonstrate its\napplicability on a number of problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2011 19:36:43 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Osokin", "Anton", ""], ["Vetrov", "Dmitry", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1103.1474", "submitter": "Jan Egger", "authors": "Jan Egger, Miriam H. A. Bauer, Daniela Kuhnt, Barbara Carl, Christoph\n  Kappus, Bernd Freisleben, Christopher Nimsky", "title": "Evaluation of a Novel Approach for Automatic Volume Determination of\n  Glioblastomas Based on Several Manual Expert Segmentations", "comments": "4 pages, 4 figures, BMT 2010, Rostock", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The glioblastoma multiforme is the most common malignant primary brain tumor\nand is one of the highest malignant human neoplasms. During the course of\ndisease, the evaluation of tumor volume is an essential part of the clinical\nfollow-up. However, manual segmentation for acquisition of tumor volume is a\ntime-consuming process. In this paper, a new approach for the automatic\nsegmentation and volume determination of glioblastomas (glioblastoma\nmultiforme) is presented and evaluated. The approach uses a user-defined seed\npoint inside the glioma to set up a directed 3D graph. The nodes of the graph\nare obtained by sampling along rays that are sent through the surface points of\na polyhedron. After the graph has been constructed, the minimal s-t cut is\ncalculated to separate the glioblastoma from the background. For evaluation, 12\nMagnetic Resonance Imaging (MRI) data sets were manually segmented slice by\nslice, by neurosurgeons with several years of experience in the resection of\ngliomas. Afterwards, the manual segmentations were compared with the results of\nthe presented approach via the Dice Similarity Coefficient (DSC). For a better\nassessment of the DSC results, the manual segmentations of the experts were\nalso compared with each other and evaluated via the DSC. In addition, the 12\ndata sets were segmented once again by one of the neurosurgeons after a period\nof two weeks, to also measure the intra-physician deviation of the DSC.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 09:31:54 GMT"}], "update_date": "2011-03-10", "authors_parsed": [["Egger", "Jan", ""], ["Bauer", "Miriam H. A.", ""], ["Kuhnt", "Daniela", ""], ["Carl", "Barbara", ""], ["Kappus", "Christoph", ""], ["Freisleben", "Bernd", ""], ["Nimsky", "Christopher", ""]]}, {"id": "1103.1475", "submitter": "Jan Egger", "authors": "Miriam H. A. Bauer, Jan Egger, Daniela Kuhnt, Sebastiano Barbieri, Jan\n  Klein, Horst K. Hahn, Bernd Freisleben, Christopher Nimsky", "title": "A Semi-Automatic Graph-Based Approach for Determining the Boundary of\n  Eloquent Fiber Bundles in the Human Brain", "comments": "4 pages, 3 figures, BMT 2010, Rostock", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Tensor Imaging (DTI) allows estimating the position, orientation\nand dimension of bundles of nerve pathways. This non-invasive imaging technique\ntakes advantage of the diffusion of water molecules and determines the\ndiffusion coefficients for every voxel of the data set. The identification of\nthe diffusion coefficients and the derivation of information about fiber\nbundles is of major interest for planning and performing neurosurgical\ninterventions. To minimize the risk of neural deficits during brain surgery as\ntumor resection (e.g. glioma), the segmentation and integration of the results\nin the operating room is of prime importance. In this contribution, a robust\nand efficient graph-based approach for segmentating tubular fiber bundles in\nthe human brain is presented. To define a cost function, the fractional\nanisotropy (FA) is used, derived from the DTI data, but this value may differ\nfrom patient to patient. Besides manually definining seed regions describing\nthe structure of interest, additionally a manual definition of the cost\nfunction by the user is necessary. To improve the approach the contribution\nintroduces a solution for automatically determining the cost function by using\ndifferent 3D masks for each individual data set.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 09:39:21 GMT"}], "update_date": "2011-03-09", "authors_parsed": [["Bauer", "Miriam H. A.", ""], ["Egger", "Jan", ""], ["Kuhnt", "Daniela", ""], ["Barbieri", "Sebastiano", ""], ["Klein", "Jan", ""], ["Hahn", "Horst K.", ""], ["Freisleben", "Bernd", ""], ["Nimsky", "Christopher", ""]]}, {"id": "1103.1587", "submitter": "Xin Li", "authors": "Xin Li", "title": "All Roads Lead To Rome", "comments": "5 pages, 1 figure, submitted", "journal-ref": "IEEE SPM'2011 as a Column Paper for DSP Tips&Tricks", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short article presents a class of projection-based solution algorithms\nto the problem considered in the pioneering work on compressed sensing -\nperfect reconstruction of a phantom image from 22 radial lines in the frequency\ndomain. Under the framework of projection-based image reconstruction, we will\nshow experimentally that several old and new tools of nonlinear filtering\n(including Perona-Malik diffusion, nonlinear diffusion, Translation-Invariant\nthresholding and SA-DCT thresholding) all lead to perfect reconstruction of the\nphantom image.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 17:50:56 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2011 18:46:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Li", "Xin", ""]]}, {"id": "1103.1773", "submitter": "Jan Egger", "authors": "Jan Egger, Bernd Freisleben, Randolph Setser, Rahul Renapuraar,\n  Christina Biermann, Thomas O'Donnell", "title": "Aorta Segmentation for Stent Simulation", "comments": "10 pages, 6 figures, MICCAI Workshop on Cardiovascular Interventional\n  Imaging and Biophysical Modelling (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of arterial stenting procedures prior to intervention allows for\nappropriate device selection as well as highlights potential complications. To\nthis end, we present a framework for facilitating virtual aortic stenting from\na contrast computer tomography (CT) scan. More specifically, we present a\nmethod for both lumen and outer wall segmentation that may be employed in\ndetermining both the appropriateness of intervention as well as the selection\nand localization of the device. The more challenging recovery of the outer wall\nis based on a novel minimal closure tracking algorithm. Our aortic segmentation\nmethod has been validated on over 3000 multiplanar reformatting (MPR) planes\nfrom 50 CT angiography data sets yielding a Dice Similarity Coefficient (DSC)\nof 90.67%.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 13:18:10 GMT"}], "update_date": "2011-03-10", "authors_parsed": [["Egger", "Jan", ""], ["Freisleben", "Bernd", ""], ["Setser", "Randolph", ""], ["Renapuraar", "Rahul", ""], ["Biermann", "Christina", ""], ["O'Donnell", "Thomas", ""]]}, {"id": "1103.1952", "submitter": "Jan Egger", "authors": "Miriam H. A. Bauer, Jan Egger, Daniela Kuhnt, Sebastiano Barbieri, Jan\n  Klein, Horst K. Hahn, Bernd Freisleben, Christopher Nimsky", "title": "Ray-Based and Graph-Based Methods for Fiber Bundle Boundary Estimation", "comments": "4 pages, 6 figures, BIOSIGNAL, Berlin, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Tensor Imaging (DTI) provides the possibility of estimating the\nlocation and course of eloquent structures in the human brain. Knowledge about\nthis is of high importance for preoperative planning of neurosurgical\ninterventions and for intraoperative guidance by neuronavigation in order to\nminimize postoperative neurological deficits. Therefore, the segmentation of\nthese structures as closed, three-dimensional object is necessary. In this\ncontribution, two methods for fiber bundle segmentation between two defined\nregions are compared using software phantoms (abstract model and anatomical\nphantom modeling the right corticospinal tract). One method uses evaluation\npoints from sampled rays as candidates for boundary points, the other method\nsets up a directed and weighted (depending on a scalar measure) graph and\nperforms a min-cut for optimal segmentation results. Comparison is done by\nusing the Dice Similarity Coefficient (DSC), a measure for spatial overlap of\ndifferent segmentation results.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 07:19:23 GMT"}], "update_date": "2011-03-11", "authors_parsed": [["Bauer", "Miriam H. A.", ""], ["Egger", "Jan", ""], ["Kuhnt", "Daniela", ""], ["Barbieri", "Sebastiano", ""], ["Klein", "Jan", ""], ["Hahn", "Horst K.", ""], ["Freisleben", "Bernd", ""], ["Nimsky", "Christopher", ""]]}, {"id": "1103.2356", "submitter": "Yuri Shtemler", "authors": "Evgenia Gelman", "title": "Adaptive mosaic image representation for image processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Method for a mosaic image representation (MIR) is proposed for a selective\ntreatment of image fragments of different transition frequency. MIR method is\nbased on piecewise-constant image approximation on a non-uniform orthogonal\ngrid constructed by the following recurrent multigrid algorithm. A sequence of\nnested uniform grids is built, such that each cell of a current grid is\nsubdivided into four smaller cells for the next grid designing. In each grid\nthe cells are selected, where the color intensity function can be approximated\nby its average value with a given precision (thereafter 'good' cells). After\nreplacing colors of good cells by their approximating constants the\nreconstructed image looks like a mosaic composed of one-colored cells.\nMultigrid algorithm results in the stratification of the image space into\nregions of different transition frequency. Sizes of these regions depend on the\nfew tuning precision parameters that characterizes adaptability of the method\nto the image fragments of different non-homogeneity degree. The method is found\nefficient for prominent contour (skeleton) extraction, edge detection as well\nas for the Lossy Compression of single images and video sequence of images.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 20:13:16 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Gelman", "Evgenia", ""]]}, {"id": "1103.2539", "submitter": "Pierre Rouchon", "authors": "Nadege Zarrouati, Emanuel Aldea, Pierre Rouchon", "title": "SO(3)-invariant asymptotic observers for dense depth field estimation\n  based on visual data and known camera motion", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use known camera motion associated to a video sequence of a\nstatic scene in order to estimate and incrementally refine the surrounding\ndepth field. We exploit the SO(3)-invariance of brightness and depth fields\ndynamics to customize standard image processing techniques. Inspired by the\nHorn-Schunck method, we propose a SO(3)-invariant cost to estimate the depth\nfield. At each time step, this provides a diffusion equation on the unit\nRiemannian sphere that is numerically solved to obtain a real time depth field\nestimation of the entire field of view. Two asymptotic observers are derived\nfrom the governing equations of dynamics, respectively based on optical flow\nand depth estimations: implemented on noisy sequences of synthetic images as\nwell as on real data, they perform a more robust and accurate depth estimation.\nThis approach is complementary to most methods employing state observers for\nrange estimation, which uniquely concern single or isolated feature points.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2011 18:12:18 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2011 12:24:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zarrouati", "Nadege", ""], ["Aldea", "Emanuel", ""], ["Rouchon", "Pierre", ""]]}, {"id": "1103.2756", "submitter": "Xinmei Tian", "authors": "Xinmei Tian and Dacheng Tao and Yong Rui", "title": "Sparse Transfer Learning for Interactive Video Search Reranking", "comments": "17 pages", "journal-ref": null, "doi": "10.1145/0000000.0000000", "report-no": null, "categories": "cs.IR cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual reranking is effective to improve the performance of the text-based\nvideo search. However, existing reranking algorithms can only achieve limited\nimprovement because of the well-known semantic gap between low level visual\nfeatures and high level semantic concepts. In this paper, we adopt interactive\nvideo search reranking to bridge the semantic gap by introducing user's\nlabeling effort. We propose a novel dimension reduction tool, termed sparse\ntransfer learning (STL), to effectively and efficiently encode user's labeling\ninformation. STL is particularly designed for interactive video search\nreranking. Technically, it a) considers the pair-wise discriminative\ninformation to maximally separate labeled query relevant samples from labeled\nquery irrelevant ones, b) achieves a sparse representation for the subspace to\nencodes user's intention by applying the elastic net penalty, and c) propagates\nuser's labeling information from labeled samples to unlabeled samples by using\nthe data distribution knowledge. We conducted extensive experiments on the\nTRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular\ndimension reduction algorithms. We report superior performance by using the\nproposed STL based interactive video search reranking.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 19:48:20 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 03:49:33 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2011 00:12:42 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Tian", "Xinmei", ""], ["Tao", "Dacheng", ""], ["Rui", "Yong", ""]]}, {"id": "1103.3228", "submitter": "Emre Guven", "authors": "H. Emre Guven, Eric L. Miller, and Robin O. Cleveland", "title": "Multi-parameter acoustic imaging of uniform objects in inhomogeneous\n  media", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem studied in this paper is ultrasound image reconstruction from\nfrequency-domain measurements of the scattered field from an object with\ncontrast in attenuation and sound speed. The case where the object has uniform\nbut unknown contrast in these properties relative to the background is\nconsidered. Background clutter is taken into account in a physically realistic\nmanner by considering an exact scattering model for randomly located small\nscatterers that vary in sound speed. The resulting statistical characteristics\nof the interference is incorporated into the imaging solution, which includes\napplying a total-variation minimization based approach where the relative\neffect of perturbation in sound speed to attenuation is included as a\nparameter. Convex optimization methods provide the basis for the reconstruction\nalgorithm. Numerical data for inversion examples are generated by solving the\ndiscretized Lippman-Schwinger equation for the object and speckle-forming\nscatterers in the background. A statistical model based on the Born\napproximation is used for reconstruction of the object profile. Results are\npresented for a two dimensional problem in terms of classification performance\nand compared to minimum-l2-norm reconstruction. Classification using the\nproposed method is shown to be robust down to a signal-to-clutter ratio of less\nthan 1 dB.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 16:57:14 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Guven", "H. Emre", ""], ["Miller", "Eric L.", ""], ["Cleveland", "Robin O.", ""]]}, {"id": "1103.3430", "submitter": "Sofiene Haboubi", "authors": "Sofiene Haboubi, Samia Maddouri and Hamid Amiri", "title": "Identification of arabic word from bilingual text using character\n  features", "comments": "FAHR 2010 - the 1st International Workshop on Frontiers in Arabic\n  Handwriting Recognition", "journal-ref": "International Workshop on Frontiers in Arabic Handwriting\n  Recognition (2010) 50-54", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The identification of the language of the script is an important stage in the\nprocess of recognition of the writing. There are several works in this research\narea, which treat various languages. Most of the used methods are global or\nstatistical. In this present paper, we study the possibility of using the\nfeatures of scripts to identify the language. The identification of the\nlanguage of the script by characteristics returns the identification in the\ncase of multilingual documents less difficult. We present by this work, a study\non the possibility of using the structural features to identify the Arabic\nlanguage from an Arabic / Latin text.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 14:59:33 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Haboubi", "Sofiene", ""], ["Maddouri", "Samia", ""], ["Amiri", "Hamid", ""]]}, {"id": "1103.3440", "submitter": "Malakappa Shirdhonkar", "authors": "M.S. Shirdhonkar and Manesh Kokare", "title": "Off-Line Handwritten Signature Identification Using Rotated Complex\n  Wavelet Filters", "comments": "5 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 1, January 2011 ISSN (Online): 1694-0814", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, a new method for handwritten signature identification based on\nrotated complex wavelet filters is proposed. We have proposed to use the\nrotated complex wavelet filters (RCWF) and dual tree complex wavelet\ntransform(DTCWT) together to derive signature feature extraction, which\ncaptures information in twelve different directions. In identification phase,\nCanberra distance measure is used. The proposed method is compared with\ndiscrete wavelet transform (DWT). From experimental results it is found that\nsignature identification rate of proposed method is superior over DWT\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 15:52:15 GMT"}], "update_date": "2011-03-18", "authors_parsed": [["Shirdhonkar", "M. S.", ""], ["Kokare", "Manesh", ""]]}, {"id": "1103.3532", "submitter": "Lotfi Chaari", "authors": "Lotfi Chaari, S\\'ebastien M\\'eriaux, Solveig Badillo, Jean-Christophe\n  Pesquet and Philippe Ciuciu", "title": "4D Wavelet-Based Regularization for Parallel MRI Reconstruction: Impact\n  on Subject and Group-Levels Statistical Sensitivity in fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel MRI is a fast imaging technique that enables the acquisition of\nhighly resolved images in space. It relies on $k$-space undersampling and\nmultiple receiver coils with complementary sensitivity profiles in order to\nreconstruct a full Field-Of-View (FOV) image. The performance of parallel\nimaging mainly depends on the reconstruction algorithm, which can proceed\neither in the original $k$-space (GRAPPA, SMASH) or in the image domain\n(SENSE-like methods). To improve the performance of the widely used SENSE\nalgorithm, 2D- or slice-specific regularization in the wavelet domain has been\nefficiently investigated. In this paper, we extend this approach using\n3D-wavelet representations in order to handle all slices together and address\nreconstruction artifacts which propagate across adjacent slices. The extension\nalso accounts for temporal correlations that exist between successive scans in\nfunctional MRI (fMRI). The proposed 4D reconstruction scheme is fully\n\\emph{unsupervised} in the sense that all regularization parameters are\nestimated in the maximum likelihood sense on a reference scan. The gain induced\nby such extensions is first illustrated on EPI image reconstruction but also\nmeasured in terms of statistical sensitivity during a fast event-related fMRI\nprotocol. The proposed 4D-UWR-SENSE algorithm outperforms the SENSE\nreconstruction at the subject and group-levels (15 subjects) for different\ncontrasts of interest and using different parallel acceleration factors on\n$2\\times2\\times3$mm$^3$ EPI images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 23:11:58 GMT"}], "update_date": "2011-03-21", "authors_parsed": [["Chaari", "Lotfi", ""], ["M\u00e9riaux", "S\u00e9bastien", ""], ["Badillo", "Solveig", ""], ["Pesquet", "Jean-Christophe", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "1103.4487", "submitter": "Dan Ciresan", "authors": "Dan C. Cire\\c{s}an, Ueli Meier, Luca M. Gambardella and J\\\"urgen\n  Schmidhuber", "title": "Handwritten Digit Recognition with a Committee of Deep Neural Nets on\n  GPUs", "comments": "9 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "IDSIA-03-11", "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The competitive MNIST handwritten digit recognition benchmark has a long\nhistory of broken records since 1998. The most recent substantial improvement\nby others dates back 7 years (error rate 0.4%) . Recently we were able to\nsignificantly improve this result, using graphics cards to greatly speed up\ntraining of simple but deep MLPs, which achieved 0.35%, outperforming all the\nprevious more complex methods. Here we report another substantial improvement:\n0.31% obtained using a committee of MLPs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 10:38:50 GMT"}], "update_date": "2011-03-24", "authors_parsed": [["Cire\u015fan", "Dan C.", ""], ["Meier", "Ueli", ""], ["Gambardella", "Luca M.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1103.4723", "submitter": "Kodge B. G.", "authors": "B. G. Kodge, P. S. Hiremath", "title": "Automatic Extraction of Open Space Area from High Resolution Urban\n  Satellite Imagery", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  algorithm", "journal-ref": "International Journal of Image Processing (IJIP), Volume (4):\n  Issue (2),2010, PP: 164-174", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 21st century, Aerial and satellite images are information rich. They\nare also complex to analyze. For GIS systems, many features require fast and\nreliable extraction of open space area from high resolution satellite imagery.\nIn this paper we will study efficient and reliable automatic extraction\nalgorithm to find out the open space area from the high resolution urban\nsatellite imagery. This automatic extraction algorithm uses some filters and\nsegmentations and grouping is applying on satellite images. And the result\nimages may use to calculate the total available open space area and the built\nup area. It may also use to compare the difference between present and past\nopen space area using historical urban satellite images of that same projection\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 10:40:00 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2011 06:56:09 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2012 12:22:28 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Kodge", "B. G.", ""], ["Hiremath", "P. S.", ""]]}, {"id": "1103.4767", "submitter": "Mojgan Mohajer", "authors": "Mojgan Mohajer, Karl-Hans Englmeier, Volker J. Schmid", "title": "A comparison of Gap statistic definitions with and without logarithm\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gap statistic is a standard method for determining the number of clusters\nin a set of data. The Gap statistic standardizes the graph of $\\log(W_{k})$,\nwhere $W_{k}$ is the within-cluster dispersion, by comparing it to its\nexpectation under an appropriate null reference distribution of the data. We\nsuggest to use $W_{k}$ instead of $\\log(W_{k})$, and to compare it to the\nexpectation of $W_{k}$ under a null reference distribution. In fact, whenever a\nnumber fulfills the original Gap statistic inequality, this number also\nfulfills the inequality of a Gap statistic using $W_{k}$, but not \\textit{vice\nversa}. The two definitions of the Gap function are evaluated on several\nsimulated data sets and on a real data of DCE-MR images.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 13:51:46 GMT"}], "update_date": "2011-03-25", "authors_parsed": [["Mohajer", "Mojgan", ""], ["Englmeier", "Karl-Hans", ""], ["Schmid", "Volker J.", ""]]}, {"id": "1103.4913", "submitter": "Kodge B. G.", "authors": "B.G. Kodge, P.S. Hiremath", "title": "Automatic Open Space Area Extraction and Change Detection from High\n  Resolution Urban Satellite Images", "comments": "07 page, 13 figures", "journal-ref": "IJCA,Special Issue on RTIPPR (2):76-82, 2010. Published By\n  Foundation of Computer Science", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study efficient and reliable automatic extraction algorithm\nto find out the open space area from the high resolution urban satellite\nimagery, and to detect changes from the extracted open space area during the\nperiod 2003, 2006 and 2008. This automatic extraction and change detection\nalgorithm uses some filters, segmentation and grouping that are applied on\nsatellite images. The resultant images may be used to calculate the total\navailable open space area and the built up area. It may also be used to compare\nthe difference between present and past open space area using historical urban\nsatellite images of that same projection, which is an important geo spatial\ndata management application.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 07:02:09 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Kodge", "B. G.", ""], ["Hiremath", "P. S.", ""]]}, {"id": "1103.5621", "submitter": "Jasni Mohamad Zain", "authors": "Hafizan Mat Som, Jasni Mohamad Zain and Amzari Jihadi Ghazali", "title": "Application of Threshold Techniques for Readability Improvement of Jawi\n  Historical Manuscript Images", "comments": "10 pages, 6 figures, 2 tables, Advance Computing: An International\n  Journal (ACIJ)", "journal-ref": "Advanced Computing: An International Journal ( ACIJ ), Vol.2,\n  No.2, March 2011", "doi": "10.5121/acij.2011.2206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical documents such as old books and manuscripts have a high aesthetic\nvalue and highly appreciated. Unfortunately, there are some documents cannot be\nread due to quality problems like faded paper, ink expand, uneven colour tone,\ntorn paper and other elements disruption such as the existence of small spots.\nThe study aims to produce a copy of manuscript that shows clear wordings so\nthey can easily be read and the copy can also be displayed for visitors. 16\nsamples of Jawi historical manuscript with different quality problems were\nobtained from The Royal Museum of Pahang, Malaysia. We applied three\nbinarization techniques; Otsu's method represents global threshold technique;\nSauvola and Niblack method which are categorized as local threshold techniques.\nWe compared the binarized images with the original manuscript to be visually\ninspected by the museum's curator. The unclear features were marked and\nanalyzed. Most of the examined images show that with optimal parameters and\neffective pre processing technique, local thresholding methods are work well\ncompare with the other one. Niblack's and Sauvola's techniques seem to be the\nsuitable approaches for these types of images. Most of binarized images with\nthese two methods show improvement for readability and character recognition.\nFor this research, even the differences of image result were hard to be\ndistinguished by human capabilities, after comparing the time cost and overall\nachievement rate of recognized symbols, Niblack's method is performing better\nthan Sauvola's. We could improve the post processing step by adding edge\ndetection techniques and further enhanced by an innovative image refinement\ntechnique and a formulation of a class proper method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 12:34:53 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Som", "Hafizan Mat", ""], ["Zain", "Jasni Mohamad", ""], ["Ghazali", "Amzari Jihadi", ""]]}, {"id": "1103.5776", "submitter": "Oguz Semerci", "authors": "Oguz Semerci and Eric L. Miller", "title": "A Parametric Level Set Approach to Simultaneous Object Identification\n  and Background Reconstruction for Dual Energy Computed Tomography", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2012.2186308", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual energy computerized tomography has gained great interest because of its\nability to characterize the chemical composition of a material rather than\nsimply providing relative attenuation images as in conventional tomography. The\npurpose of this paper is to introduce a novel polychromatic dual energy\nprocessing algorithm with an emphasis on detection and characterization of\npiecewise constant objects embedded in an unknown, cluttered background.\nPhysical properties of the objects, specifically the Compton scattering and\nphotoelectric absorption coefficients, are assumed to be known with some level\nof uncertainty. Our approach is based on a level-set representation of the\ncharacteristic function of the object and encompasses a number of\nregularization techniques for addressing both the prior information we have\nconcerning the physical properties of the object as well as fundamental,\nphysics-based limitations associated with our ability to jointly recover the\nCompton scattering and photoelectric absorption properties of the scene. In the\nabsence of an object with appropriate physical properties, our approach returns\na null characteristic function and thus can be viewed as simultaneously solving\nthe detection and characterization problems. Unlike the vast majority of\nmethods which define the level set function non-parametrically, i.e., as a\ndense set of pixel values), we define our level set parametrically via radial\nbasis functions (RBF's) and employ a Gauss-Newton type algorithm for cost\nminimization. Numerical results show that the algorithm successfully detects\nobjects of interest, finds their shape and location, and gives a adequate\nreconstruction of the background.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 21:15:46 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Semerci", "Oguz", ""], ["Miller", "Eric L.", ""]]}, {"id": "1103.5808", "submitter": "Stuart Heinrich", "authors": "Stuart B. Heinrich and Wesley E. Snyder", "title": "Improved Edge Awareness in Discontinuity Preserving Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discontinuity preserving smoothing is a fundamentally important procedure\nthat is useful in a wide variety of image processing contexts. It is directly\nuseful for noise reduction, and frequently used as an intermediate step in\nhigher level algorithms. For example, it can be particularly useful in edge\ndetection and segmentation. Three well known algorithms for discontinuity\npreserving smoothing are nonlinear anisotropic diffusion, bilateral filtering,\nand mean shift filtering. Although slight differences make them each better\nsuited to different tasks, all are designed to preserve discontinuities while\nsmoothing. However, none of them satisfy this goal perfectly: they each have\nexception cases in which smoothing may occur across hard edges. The principal\ncontribution of this paper is the identification of a property we call edge\nawareness that should be satisfied by any discontinuity preserving smoothing\nalgorithm. This constraint can be incorporated into existing algorithms to\nimprove quality, and usually has negligible changes in runtime performance\nand/or complexity. We present modifications necessary to augment diffusion and\nmean shift, as well as a new formulation of the bilateral filter that unifies\nthe spatial and range spaces to achieve edge awareness.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 01:57:09 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Heinrich", "Stuart B.", ""], ["Snyder", "Wesley E.", ""]]}, {"id": "1103.6052", "submitter": "Stuart Heinrich", "authors": "Stuart B. Heinrich and Wesley E. Snyder", "title": "Internal Constraints of the Trifocal Tensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental matrix and trifocal tensor are convenient algebraic\nrepresentations of the epipolar geometry of two and three view configurations,\nrespectively. The estimation of these entities is central to most\nreconstruction algorithms, and a solid understanding of their properties and\nconstraints is therefore very important. The fundamental matrix has 1 internal\nconstraint which is well understood, whereas the trifocal tensor has 8\nindependent algebraic constraints. The internal tensor constraints can be\nrepresented in many ways, although there is only one minimal and sufficient set\nof 8 constraints known. In this paper, we derive a second set of minimal and\nsufficient constraints that is simpler. We also show how this can be used in a\nnew parameterization of the trifocal tensor. We hope that this increased\nunderstanding of the internal constraints may lead to improved algorithms for\nestimating the trifocal tensor, although the primary contribution is an\nimproved theoretical understanding.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 21:47:41 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Heinrich", "Stuart B.", ""], ["Snyder", "Wesley E.", ""]]}]