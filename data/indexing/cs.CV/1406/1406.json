[{"id": "1406.0023", "submitter": "Diego Oliva Ph.D.", "authors": "Erik Cuevas, Diego Oliva, Daniel Zaldivar, Marco Perez-Cisneros and\n  Humberto Sossa", "title": "Circle detection using electro-magnetism optimization", "comments": "arXiv admin note: substantial text overlap with arXiv:1405.7362", "journal-ref": "Information Sciences, Volume 182, Issue 1, 1 January 2012, Pages\n  40-55, ISSN 0020-0255,", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a circle detection method based on Electromagnetism-Like\nOptimization (EMO). Circle detection has received considerable attention over\nthe last years thanks to its relevance for many computer vision tasks. EMO is a\nheuristic method for solving complex optimization problems inspired in\nelectromagnetism principles. This algorithm searches a solution based in the\nattraction and repulsion among prototype candidates. In this paper the\ndetection process is considered to be similar to an optimization problem, the\nalgorithm uses the combination of three edge points (x, y, r) as parameters to\ndetermine circles candidates in the scene. An objective function determines if\nsuch circle candidates are actually present in the image. The EMO algorithm is\nused to find the circle candidate that is better related with the real circle\npresent in the image according to the objective function. The final algorithm\nis a fast circle detector that locates circles with sub-pixel accuracy even\nconsidering complicated conditions and noisy images.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 21:58:36 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Cuevas", "Erik", ""], ["Oliva", "Diego", ""], ["Zaldivar", "Daniel", ""], ["Perez-Cisneros", "Marco", ""], ["Sossa", "Humberto", ""]]}, {"id": "1406.0074", "submitter": "Shradha P Dakhare", "authors": "Shradha Dakhare, Harshal Chowhan, Manoj B.Chandak", "title": "Combined Approach for Image Segmentation", "comments": "4 pages, 5 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "IJCTT, Volume 11 Number 3 May 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image segmentation techniques have been developed over the past two\ndecades for segmenting the images, which help for object recognition, occlusion\nboundary estimation within motion or stereo systems, image compression, image\nediting.\n  In this, there is a combined approach for segmenting the image. By using\nhistogram equalization to the input image, from which it gives contrast\nenhancement output image .After that by applying median filtering,which will\nremove noise from contrast output image . At last I applied fuzzy c-mean\nclustering algorithm to denoising output image, which give segmented output\nimage. In this way it produce better segmented image with less computation\ntime.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 12:35:31 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Dakhare", "Shradha", ""], ["Chowhan", "Harshal", ""], ["Chandak", "Manoj B.", ""]]}, {"id": "1406.0132", "submitter": "Liang Zheng", "authors": "Liang Zheng, Shengjin Wang, Fei He, Qi Tian", "title": "Seeing the Big Picture: Deep Embedding with Contextual Evidences", "comments": "10 pages, 13 figures, 7 tables, submitted to ACM Multimedia 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the Bag-of-Words (BoW) model based image retrieval task, the precision of\nvisual matching plays a critical role in improving retrieval performance.\nConventionally, local cues of a keypoint are employed. However, such strategy\ndoes not consider the contextual evidences of a keypoint, a problem which would\nlead to the prevalence of false matches. To address this problem, this paper\ndefines \"true match\" as a pair of keypoints which are similar on three levels,\ni.e., local, regional, and global. Then, a principled probabilistic framework\nis established, which is capable of implicitly integrating discriminative cues\nfrom all these feature levels.\n  Specifically, the Convolutional Neural Network (CNN) is employed to extract\nfeatures from regional and global patches, leading to the so-called \"Deep\nEmbedding\" framework. CNN has been shown to produce excellent performance on a\ndozen computer vision tasks such as image classification and detection, but few\nworks have been done on BoW based image retrieval. In this paper, firstly we\nshow that proper pre-processing techniques are necessary for effective usage of\nCNN feature. Then, in the attempt to fit it into our model, a novel indexing\nstructure called \"Deep Indexing\" is introduced, which dramatically reduces\nmemory usage.\n  Extensive experiments on three benchmark datasets demonstrate that, the\nproposed Deep Embedding method greatly promotes the retrieval accuracy when CNN\nfeature is integrated. We show that our method is efficient in terms of both\nmemory and time cost, and compares favorably with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 05:04:28 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Zheng", "Liang", ""], ["Wang", "Shengjin", ""], ["He", "Fei", ""], ["Tian", "Qi", ""]]}, {"id": "1406.0156", "submitter": "Sheng Han", "authors": "Sheng Han, Suzhen Wang, Xinyu Wu", "title": "$l_1$-regularized Outlier Isolation and Regression", "comments": "Outlier Detection, Robust Regression, Robust Rank Factorization,\n  $l_1$-regularization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new regression model called $l_1$-regularized outlier\nisolation and regression (LOIRE) and a fast algorithm based on block coordinate\ndescent to solve this model. Besides, assuming outliers are gross errors\nfollowing a Bernoulli process, this paper also presented a Bernoulli estimate\nmodel which, in theory, should be very accurate and robust due to its complete\nelimination of affections caused by outliers. Though this Bernoulli estimate is\nhard to solve, it could be approximately achieved through a process which takes\nLOIRE as an important intermediate step. As a result, the approximate Bernoulli\nestimate is a good combination of Bernoulli estimate's accuracy and LOIRE\nregression's efficiency with several simulations conducted to strongly verify\nthis point. Moreover, LOIRE can be further extended to realize robust rank\nfactorization which is powerful in recovering low-rank component from massive\ncorruptions. Extensive experimental results showed that the proposed method\noutperforms state-of-the-art methods like RPCA and GoDec in the aspect of\ncomputation speed with a competitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 11:52:19 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 08:58:09 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Han", "Sheng", ""], ["Wang", "Suzhen", ""], ["Wu", "Xinyu", ""]]}, {"id": "1406.0231", "submitter": "Quanquan Wang", "authors": "Quanquan Wang, Yongping Li", "title": "Ambiguous Proximity Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximity Distribution Kernel is an effective method for bag-of-featues based\nimage representation. In this paper, we investigate the soft assignment of\nvisual words to image features for proximity distribution. Visual word\ncontribution function is proposed to model ambiguous proximity distributions.\nThree ambiguous proximity distributions is developed by three ambiguous\ncontribution functions. The experiments are conducted on both classification\nand retrieval of medical image data sets. The results show that the performance\nof the proposed methods, Proximity Distribution Kernel (PDK), is better or\ncomparable to the state-of-the-art bag-of-features based image representation\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 01:44:50 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Wang", "Quanquan", ""], ["Li", "Yongping", ""]]}, {"id": "1406.0281", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, Marco Loog", "title": "On Classification with Bags, Groups and Sets", "comments": null, "journal-ref": "Pattern Recognition Letters Volume 59, 2015, Pages 11 - 17", "doi": "10.1016/j.patrec.2015.03.008", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification problems can be difficult to formulate directly in terms\nof the traditional supervised setting, where both training and test samples are\nindividual feature vectors. There are cases in which samples are better\ndescribed by sets of feature vectors, that labels are only available for sets\nrather than individual samples, or, if individual labels are available, that\nthese are not independent. To better deal with such problems, several\nextensions of supervised learning have been proposed, where either training\nand/or test objects are sets of feature vectors. However, having been proposed\nrather independently of each other, their mutual similarities and differences\nhave hitherto not been mapped out. In this work, we provide an overview of such\nlearning scenarios, propose a taxonomy to illustrate the relationships between\nthem, and discuss directions for further research in these areas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 08:06:12 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 14:55:44 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1406.0288", "submitter": "Radu Horaud P", "authors": "Kaustubh Kulkarni, Georgios Evangelidis, Jan Cech and Radu Horaud", "title": "Continuous Action Recognition Based on Sequence Alignment", "comments": null, "journal-ref": "International Journal of Computer Vision 112(1), 90-114, 2015", "doi": "10.1007/s11263-014-0758-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous action recognition is more challenging than isolated recognition\nbecause classification and segmentation must be simultaneously carried out. We\nbuild on the well known dynamic time warping (DTW) framework and devise a novel\nvisual alignment technique, namely dynamic frame warping (DFW), which performs\nisolated recognition based on per-frame representation of videos, and on\naligning a test sequence with a model sequence. Moreover, we propose two\nextensions which enable to perform recognition concomitant with segmentation,\nnamely one-pass DFW and two-pass DFW. These two methods have their roots in the\ndomain of continuous recognition of speech and, to the best of our knowledge,\ntheir extension to continuous visual action recognition has been overlooked. We\ntest and illustrate the proposed techniques with a recently released dataset\n(RAVEL) and with two public-domain datasets widely used in action recognition\n(Hollywood-1 and Hollywood-2). We also compare the performances of the proposed\nisolated and continuous recognition algorithms with several recently published\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 08:21:27 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Kulkarni", "Kaustubh", ""], ["Evangelidis", "Georgios", ""], ["Cech", "Jan", ""], ["Horaud", "Radu", ""]]}, {"id": "1406.0289", "submitter": "Giovanna Citti", "authors": "Alessandro Sarti and Giovanna Citti", "title": "The constitution of visual perceptual units in the functional\n  architecture of V1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scope of this paper is to consider a mean field neural model which takes into\naccount the functional neurogeometry of the visual cortex modelled as a group\nof rotations and translations. The model generalizes well known results of\nBressloff and Cowan which, in absence of input, accounts for hallucination\npatterns. The main result of our study consists in showing that in presence of\na visual input, the eigenmodes of the linearized operator which become stable\nrepresent perceptual units present in the image. The result is strictly related\nto dimensionality reduction and clustering problems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 08:27:27 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Sarti", "Alessandro", ""], ["Citti", "Giovanna", ""]]}, {"id": "1406.0312", "submitter": "Naila Murray", "authors": "Naila Murray and Florent Perronnin", "title": "Generalized Max Pooling", "comments": "(to appear) CVPR 2014 - IEEE Conference on Computer Vision & Pattern\n  Recognition (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art patch-based image representations involve a pooling\noperation that aggregates statistics computed from local descriptors. Standard\npooling operations include sum- and max-pooling. Sum-pooling lacks\ndiscriminability because the resulting representation is strongly influenced by\nfrequent yet often uninformative descriptors, but only weakly influenced by\nrare yet potentially highly-informative ones. Max-pooling equalizes the\ninfluence of frequent and rare descriptors but is only applicable to\nrepresentations that rely on count statistics, such as the bag-of-visual-words\n(BOV) and its soft- and sparse-coding extensions. We propose a novel pooling\nmechanism that achieves the same effect as max-pooling but is applicable beyond\nthe BOV and especially to the state-of-the-art Fisher Vector -- hence the name\nGeneralized Max Pooling (GMP). It involves equalizing the similarity between\neach patch and the pooled representation, which is shown to be equivalent to\nre-weighting the per-patch statistics. We show on five public image\nclassification benchmarks that the proposed GMP can lead to significant\nperformance gains with respect to heuristic alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 10:17:03 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Murray", "Naila", ""], ["Perronnin", "Florent", ""]]}, {"id": "1406.0588", "submitter": "Shasha Bu", "authors": "Shasha Bu and Yu-Jin Zhang", "title": "Image retrieval with hierarchical matching pursuit", "comments": "5 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel representation of images for image retrieval is introduced in this\npaper, by using a new type of feature with remarkable discriminative power.\nDespite the multi-scale nature of objects, most existing models perform feature\nextraction on a fixed scale, which will inevitably degrade the performance of\nthe whole system. Motivated by this, we introduce a hierarchical sparse coding\narchitecture for image retrieval to explore multi-scale cues. Sparse codes\nextracted on lower layers are transmitted to higher layers recursively. With\nthis mechanism, cues from different scales are fused. Experiments on the\nHolidays dataset show that the proposed method achieves an excellent retrieval\nperformance with a small code length.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 06:32:24 GMT"}, {"version": "v2", "created": "Thu, 5 Jun 2014 02:23:21 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Bu", "Shasha", ""], ["Zhang", "Yu-Jin", ""]]}, {"id": "1406.0680", "submitter": "Ziqiong Liu", "authors": "Ziqiong Liu, Shengjin Wang, Liang Zheng, Qi Tian", "title": "Visual Reranking with Improved Image Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an improved reranking method for the Bag-of-Words (BoW)\nbased image search. Built on [1], a directed image graph robust to outlier\ndistraction is proposed. In our approach, the relevance among images is encoded\nin the image graph, based on which the initial rank list is refined. Moreover,\nwe show that the rank-level feature fusion can be adopted in this reranking\nmethod as well. Taking advantage of the complementary nature of various\nfeatures, the reranking performance is further enhanced. Particularly, we\nexploit the reranking method combining the BoW and color information.\nExperiments on two benchmark datasets demonstrate that ourmethod yields\nsignificant improvements and the reranking results are competitive to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 12:07:12 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Liu", "Ziqiong", ""], ["Wang", "Shengjin", ""], ["Zheng", "Liang", ""], ["Tian", "Qi", ""]]}, {"id": "1406.0909", "submitter": "Sedigheh Ildarabadi", "authors": "S. Ildarabadi, M. Ebrahimi, H. R. Pourreza", "title": "Improvement Tracking Dynamic Programming using Replication Function for\n  Continuous Sign Language Recognition", "comments": "5 pages, 13 figures, Published with \"International Journal of\n  Engineering Trends and Technology (IJETT)\"", "journal-ref": null, "doi": "10.14445/22315381/IJETT-V7P254", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we used a Replication Function (R. F.)for improvement tracking\nwith dynamic programming. The R. F. transforms values of gray level [0 255] to\n[0 1]. The resulting images of R. F. are more striking and visible in skin\nregions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and\nface. Results show that Tracking Error Rate 11% and Average Tracked Distance 7%\nreduced\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 00:03:51 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Ildarabadi", "S.", ""], ["Ebrahimi", "M.", ""], ["Pourreza", "H. R.", ""]]}, {"id": "1406.0924", "submitter": "Pedro Felzenszwalb", "authors": "Pedro F. Felzenszwalb and John G. Oberlin", "title": "Multiscale Fields of Patterns", "comments": "In NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework for defining high-order image models that can be used\nin a variety of applications. The approach involves modeling local patterns in\na multiscale representation of an image. Local properties of a coarsened image\nreflect non-local properties of the original image. In the case of binary\nimages local properties are defined by the binary patterns observed over small\nneighborhoods around each pixel. With the multiscale representation we capture\nthe frequency of patterns observed at different scales of resolution. This\nframework leads to expressive priors that depend on a relatively small number\nof parameters. For inference and learning we use an MCMC method for block\nsampling with very large blocks. We evaluate the approach with two example\napplications. One involves contour detection. The other involves binary\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 02:10:58 GMT"}, {"version": "v2", "created": "Sat, 30 Aug 2014 01:46:42 GMT"}, {"version": "v3", "created": "Fri, 12 Dec 2014 18:42:35 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Felzenszwalb", "Pedro F.", ""], ["Oberlin", "John G.", ""]]}, {"id": "1406.0946", "submitter": "Liang Zheng", "authors": "Fei He, Shengjin Wang", "title": "Beyond $\\chi^2$ Difference: Learning Optimal Metric for Boundary\n  Detection", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2014.2346232", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This letter focuses on solving the challenging problem of detecting natural\nimage boundaries. A boundary usually refers to the border between two regions\nwith different semantic meanings. Therefore, a measurement of dissimilarity\nbetween image regions plays a pivotal role in boundary detection of natural\nimages. To improve the performance of boundary detection, a Learning-based\nBoundary Metric (LBM) is proposed to replace $\\chi^2$ difference adopted by the\nclassical algorithm mPb. Compared with $\\chi^2$ difference, LBM is composed of\na single layer neural network and an RBF kernel, and is fine-tuned by\nsupervised learning rather than human-crafted. It is more effective in\ndescribing the dissimilarity between natural image regions while tolerating\nlarge variance of image data. After substituting $\\chi^2$ difference with LBM,\nthe F-measure metric of mPb on the BSDS500 benchmark is increased from 0.69 to\n0.71. Moreover, when image features are computed on a single scale, the\nproposed LBM algorithm still achieves competitive results compared with\n\\emph{mPb}, which makes use of multi-scale image features.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 05:58:53 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["He", "Fei", ""], ["Wang", "Shengjin", ""]]}, {"id": "1406.1134", "submitter": "Piotr Doll\\'ar", "authors": "Woonhyun Nam, Piotr Doll\\'ar, Joon Hee Han", "title": "Local Decorrelation For Improved Detection", "comments": "To appear in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even with the advent of more sophisticated, data-hungry methods, boosted\ndecision trees remain extraordinarily successful for fast rigid object\ndetection, achieving top accuracy on numerous datasets. While effective, most\nboosted detectors use decision trees with orthogonal (single feature) splits,\nand the topology of the resulting decision boundary may not be well matched to\nthe natural topology of the data. Given highly correlated data, decision trees\nwith oblique (multiple feature) splits can be effective. Use of oblique splits,\nhowever, comes at considerable computational expense. Inspired by recent work\non discriminative decorrelation of HOG features, we instead propose an\nefficient feature transform that removes correlations in local neighborhoods.\nThe result is an overcomplete but locally decorrelated representation ideally\nsuited for use with orthogonal decision trees. In fact, orthogonal trees with\nour locally decorrelated features outperform oblique trees trained over the\noriginal features at a fraction of the computational cost. The overall\nimprovement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we\nreduce false positives nearly tenfold over the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 18:20:38 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 02:50:18 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Nam", "Woonhyun", ""], ["Doll\u00e1r", "Piotr", ""], ["Han", "Joon Hee", ""]]}, {"id": "1406.1167", "submitter": "Xu-Cheng Yin", "authors": "Xu-Cheng Yin and Chun Yang and Hong-Wei Hao", "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble", "comments": "Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier ensemble generally should combine diverse component classifiers.\nHowever, it is difficult to give a definitive connection between diversity\nmeasure and ensemble accuracy. Given a list of available component classifiers,\nhow to adaptively and diversely ensemble classifiers becomes a big challenge in\nthe literature. In this paper, we argue that diversity, not direct diversity on\nsamples but adaptive diversity with data, is highly correlated to ensemble\naccuracy, and we propose a novel technology for classifier ensemble, learning\nto diversify, which learns to adaptively combine classifiers by considering\nboth accuracy and diversity. Specifically, our approach, Learning TO Diversify\nvia Weighted Kernels (L2DWK), performs classifier combination by optimizing a\ndirect but simple criterion: maximizing ensemble accuracy and adaptive\ndiversity simultaneously by minimizing a convex loss function. Given a measure\nformulation, the diversity is calculated with weighted kernels (i.e., the\ndiversity is measured on the component classifiers' outputs which are kernelled\nand weighted), and the kernel weights are automatically learned. We minimize\nthis loss function by estimating the kernel weights in conjunction with the\nclassifier weights, and propose a self-training algorithm for conducting this\nconvex optimization procedure iteratively. Extensive experiments on a variety\nof 32 UCI classification benchmark datasets show that the proposed approach\nconsistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost,\nRandom Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via\nSemi-Definite Programming.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 09:16:42 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Yin", "Xu-Cheng", ""], ["Yang", "Chun", ""], ["Hao", "Hong-Wei", ""]]}, {"id": "1406.1247", "submitter": "Dong Yi", "authors": "Dong Yi, Zhen Lei, Shengcai Liao and Stan Z. Li", "title": "Shared Representation Learning for Heterogeneous Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After intensive research, heterogenous face recognition is still a\nchallenging problem. The main difficulties are owing to the complex\nrelationship between heterogenous face image spaces. The heterogeneity is\nalways tightly coupled with other variations, which makes the relationship of\nheterogenous face images highly nonlinear. Many excellent methods have been\nproposed to model the nonlinear relationship, but they apt to overfit to the\ntraining set, due to limited samples. Inspired by the unsupervised algorithms\nin deep learning, this paper proposes an novel framework for heterogeneous face\nrecognition. We first extract Gabor features at some localized facial points,\nand then use Restricted Boltzmann Machines (RBMs) to learn a shared\nrepresentation locally to remove the heterogeneity around each facial point.\nFinally, the shared representations of local RBMs are connected together and\nprocessed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases\nare selected to evaluate the proposed method. For Sketch-Photo problem, we\nobtain perfect results on the CUFS database. For NIR-VIS problem, we produce\nnew state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:23:10 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Yi", "Dong", ""], ["Lei", "Zhen", ""], ["Liao", "Shengcai", ""], ["Li", "Stan Z.", ""]]}, {"id": "1406.1265", "submitter": "Jackie Shen", "authors": "Yoon Mo Jung and Jianhong Jackie Shen", "title": "Illusory Shapes via Phase Transition", "comments": "New Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new variational illusory shape (VIS) model via phase fields and\nphase transitions. It is inspired by the first-order variational illusory\ncontour (VIC) model proposed by Jung and Shen [{\\em J. Visual Comm. Image\nRepres.}, {\\bf 19}:42-55, 2008]. Under the new VIS model, illusory shapes are\nrepresented by phase values close to 1 while the rest by values close to 0. The\n0-1 transition is achieved by an elliptic energy with a double-well potential,\nas in the theory of $\\Gamma$-convergence. The VIS model is non-convex, with the\nzero field as its trivial global optimum. To seek visually meaningful local\noptima that can induce illusory shapes, an iterative algorithm is designed and\nits convergence behavior is closely studied. Several generic numerical examples\nconfirm the versatility of the model and the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 04:40:31 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Jung", "Yoon Mo", ""], ["Shen", "Jianhong Jackie", ""]]}, {"id": "1406.1476", "submitter": "Toufiq Parag", "authors": "Toufiq Parag, Anirban Chakraborty, Stephen Plaza and Lou Scheffer", "title": "A Context-aware Delayed Agglomeration Framework for Electron Microscopy\n  Segmentation", "comments": null, "journal-ref": "PLoS ONE 10(5): e0125825, 2015", "doi": "10.1371/journal.pone.0125825", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron Microscopy (EM) image (or volume) segmentation has become\nsignificantly important in recent years as an instrument for connectomics. This\npaper proposes a novel agglomerative framework for EM segmentation. In\nparticular, given an over-segmented image or volume, we propose a novel\nframework for accurately clustering regions of the same neuron. Unlike existing\nagglomerative methods, the proposed context-aware algorithm divides superpixels\n(over-segmented regions) of different biological entities into different\nsubsets and agglomerates them separately. In addition, this paper describes a\n\"delayed\" scheme for agglomerative clustering that postpones some of the merge\ndecisions, pertaining to newly formed bodies, in order to generate a more\nconfident boundary prediction. We report significant improvements attained by\nthe proposed approach in segmentation accuracy over existing standard methods\non 2D and 3D datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 18:46:38 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 13:06:53 GMT"}, {"version": "v3", "created": "Thu, 21 Aug 2014 17:22:34 GMT"}, {"version": "v4", "created": "Fri, 19 Sep 2014 19:57:10 GMT"}, {"version": "v5", "created": "Mon, 23 Mar 2015 15:28:02 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Parag", "Toufiq", ""], ["Chakraborty", "Anirban", ""], ["Plaza", "Stephen", ""], ["Scheffer", "Lou", ""]]}, {"id": "1406.1528", "submitter": "Dustin Lang", "authors": "Dustin Lang, David W. Hogg, and Bernhard Scholkopf", "title": "Towards building a Crowd-Sourced Sky Map", "comments": "Appeared at AI-STATS 2014", "journal-ref": "JMLR Workshop and Conference Proceedings, 33 (AI & Statistics\n  2014), 549", "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system that builds a high dynamic-range and wide-angle image of\nthe night sky by combining a large set of input images. The method makes use of\npixel-rank information in the individual input images to improve a \"consensus\"\npixel rank in the combined image. Because it only makes use of ranks and the\ncomplexity of the algorithm is linear in the number of images, the method is\nuseful for large sets of uncalibrated images that might have undergone unknown\nnon-linear tone mapping transformations for visualization or aesthetic reasons.\nWe apply the method to images of the night sky (of unknown provenance)\ndiscovered on the Web. The method permits discovery of astronomical objects or\nfeatures that are not visible in any of the input images taken individually.\nMore importantly, however, it permits scientific exploitation of a huge source\nof astronomical images that would not be available to astronomical research\nwithout our automatic system.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 21:18:44 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Lang", "Dustin", ""], ["Hogg", "David W.", ""], ["Scholkopf", "Bernhard", ""]]}, {"id": "1406.1774", "submitter": "Toufiq Parag", "authors": "Toufiq Parag, Stephen Plaza, Louis Scheffer (Janelia Farm Research\n  Campus- HHMI)", "title": "Small Sample Learning of Superpixel Classifiers for EM Segmentation-\n  Extended Version", "comments": "Accepted for MICCAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel and superpixel classifiers have become essential tools for EM\nsegmentation algorithms. Training these classifiers remains a major bottleneck\nprimarily due to the requirement of completely annotating the dataset which is\ntedious, error-prone and costly. In this paper, we propose an interactive\nlearning scheme for the superpixel classifier for EM segmentation. Our\nalgorithm is \"active semi-supervised\" because it requests the labels of a small\nnumber of examples from user and applies label propagation technique to\ngenerate these queries. Using only a small set ($<20\\%$) of all datapoints, the\nproposed algorithm consistently generates a classifier almost as accurate as\nthat estimated from a complete groundtruth. We provide segmentation results on\nmultiple datasets to show the strength of these classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 18:59:58 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 22:05:57 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Parag", "Toufiq", "", "Janelia Farm Research\n  Campus- HHMI"], ["Plaza", "Stephen", "", "Janelia Farm Research\n  Campus- HHMI"], ["Scheffer", "Louis", "", "Janelia Farm Research\n  Campus- HHMI"]]}, {"id": "1406.1881", "submitter": "Leonid Pishchulin", "authors": "Leonid Pishchulin, Mykhaylo Andriluka, Bernt Schiele", "title": "Fine-grained Activity Recognition with Holistic and Pose based Features", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holistic methods based on dense trajectories are currently the de facto\nstandard for recognition of human activities in video. Whether holistic\nrepresentations will sustain or will be superseded by higher level video\nencoding in terms of body pose and motion is the subject of an ongoing debate.\nIn this paper we aim to clarify the underlying factors responsible for good\nperformance of holistic and pose-based representations. To that end we build on\nour recent dataset leveraging the existing taxonomy of human activities. This\ndataset includes 24,920 video snippets covering 410 human activities in total.\nOur analysis reveals that holistic and pose-based methods are highly\ncomplementary, and their performance varies significantly depending on the\nactivity. We find that holistic methods are mostly affected by the number and\nspeed of trajectories, whereas pose-based methods are mostly influenced by\nviewpoint of the person. We observe striking performance differences across\nactivities: for certain activities results with pose-based features are more\nthan twice as accurate compared to holistic features, and vice versa. The best\nperforming approach in our comparison is based on the combination of holistic\nand pose-based approaches, which again underlines their complementarity.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 10:07:24 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 14:55:23 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Pishchulin", "Leonid", ""], ["Andriluka", "Mykhaylo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1406.1906", "submitter": "Jan Egger", "authors": "Jan Egger", "title": "Refinement-Cut: User-Guided Segmentation Algorithm for Translational\n  Science", "comments": "6 figures, 50 references", "journal-ref": "Sci. Rep. 4, 5164, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, a semi-automatic segmentation algorithm for (medical)\nimage analysis is presented. More precise, the approach belongs to the category\nof interactive contouring algorithms, which provide real-time feedback of the\nsegmentation result. However, even with interactive real-time contouring\napproaches there are always cases where the user cannot find a satisfying\nsegmentation, e.g. due to homogeneous appearances between the object and the\nbackground, or noise inside the object. For these difficult cases the algorithm\nstill needs additional user support. However, this additional user support\nshould be intuitive and rapid integrated into the segmentation process, without\nbreaking the interactive real-time segmentation feedback. I propose a solution\nwhere the user can support the algorithm by an easy and fast placement of one\nor more seed points to guide the algorithm to a satisfying segmentation result\nalso in difficult cases. These additional seed(s) restrict(s) the calculation\nof the segmentation for the algorithm, but at the same time, still enable to\ncontinue with the interactive real-time feedback segmentation. For a practical\nand genuine application in translational science, the approach has been tested\non medical data from the clinical routine in 2D and 3D.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 17:11:00 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Egger", "Jan", ""]]}, {"id": "1406.1925", "submitter": "Davide Boscaini", "authors": "Davide Boscaini, Davide Eynard, Michael M. Bronstein", "title": "Shape-from-intrinsic operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape-from-X is an important class of problems in the fields of geometry\nprocessing, computer graphics, and vision, attempting to recover the structure\nof a shape from some observations. In this paper, we formulate the problem of\nshape-from-operator (SfO), recovering an embedding of a mesh from intrinsic\ndifferential operators defined on the mesh. Particularly interesting instances\nof our SfO problem include synthesis of shape analogies, shape-from-Laplacian\nreconstruction, and shape exaggeration. Numerically, we approach the SfO\nproblem by splitting it into two optimization sub-problems that are applied in\nan alternating scheme: metric-from-operator (reconstruction of the discrete\nmetric from the intrinsic operator) and embedding-from-metric (finding a shape\nembedding that would realize a given metric, a setting of the multidimensional\nscaling problem).\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 19:33:28 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Boscaini", "Davide", ""], ["Eynard", "Davide", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1406.1943", "submitter": "Yuanming Suo", "authors": "Yuanming Suo, Minh Dao, Umamahesh Srinivas, Vishal Monga, Trac D. Tran", "title": "Structured Dictionary Learning for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity driven signal processing has gained tremendous popularity in the\nlast decade. At its core, the assumption is that the signal of interest is\nsparse with respect to either a fixed transformation or a signal dependent\ndictionary. To better capture the data characteristics, various dictionary\nlearning methods have been proposed for both reconstruction and classification\ntasks. For classification particularly, most approaches proposed so far have\nfocused on designing explicit constraints on the sparse code to improve\nclassification accuracy while simply adopting $l_0$-norm or $l_1$-norm for\nsparsity regularization. Motivated by the success of structured sparsity in the\narea of Compressed Sensing, we propose a structured dictionary learning\nframework (StructDL) that incorporates the structure information on both group\nand task levels in the learning process. Its benefits are two-fold: (i) the\nlabel consistency between dictionary atoms and training data are implicitly\nenforced; and (ii) the classification performance is more robust in the cases\nof a small dictionary size or limited training data than other techniques.\nUsing the subspace model, we derive the conditions for StructDL to guarantee\nthe performance and show theoretically that StructDL is superior to $l_0$-norm\nor $l_1$-norm regularized dictionary learning for classification. Extensive\nexperiments have been performed on both synthetic simulations and real world\napplications, such as face recognition and object classification, to\ndemonstrate the validity of the proposed DL framework.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 02:51:45 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Suo", "Yuanming", ""], ["Dao", "Minh", ""], ["Srinivas", "Umamahesh", ""], ["Monga", "Vishal", ""], ["Tran", "Trac D.", ""]]}, {"id": "1406.2031", "submitter": "Xianjie Chen", "authors": "Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel\n  Urtasun, Alan Yuille", "title": "Detect What You Can: Detecting and Representing Objects using Holistic\n  Models and Body Parts", "comments": "CBMM memo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects becomes difficult when we need to deal with large shape\ndeformation, occlusion and low resolution. We propose a novel approach to i)\nhandle large deformations and partial occlusions in animals (as examples of\nhighly deformable objects), ii) describe them in terms of body parts, and iii)\ndetect them when their body parts are hard to detect (e.g., animals depicted at\nlow resolution). We represent the holistic object and body parts separately and\nuse a fully connected model to arrange templates for the holistic object and\nbody parts. Our model automatically decouples the holistic object or body parts\nfrom the model when they are hard to detect. This enables us to represent a\nlarge number of holistic object and body part combinations to better deal with\ndifferent \"detectability\" patterns caused by deformations, occlusion and/or low\nresolution.\n  We apply our method to the six animal categories in the PASCAL VOC dataset\nand show that our method significantly improves state-of-the-art (by 4.1% AP)\nand provides a richer representation for objects. During training we use\nannotations for body parts (e.g., head, torso, etc), making use of a new\ndataset of fully annotated object parts for PASCAL VOC 2010, which provides a\nmask for each part.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 21:44:18 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Chen", "Xianjie", ""], ["Mottaghi", "Roozbeh", ""], ["Liu", "Xiaobai", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""], ["Yuille", "Alan", ""]]}, {"id": "1406.2049", "submitter": "Xue Li", "authors": "Xue Li, Yu-Jin Zhang, Bin Shen, Bao-Di Liu", "title": "Image Tag Completion by Low-rank Factorization with Dual Reconstruction\n  Structure Preserved", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel tag completion algorithm is proposed in this paper, which is designed\nwith the following features: 1) Low-rank and error s-parsity: the incomplete\ninitial tagging matrix D is decomposed into the complete tagging matrix A and a\nsparse error matrix E. However, instead of minimizing its nuclear norm, A is\nfurther factor-ized into a basis matrix U and a sparse coefficient matrix V,\ni.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables our\nalgorithm to recover latent structures from noisy initial data and avoid\nperforming too much denoising; 2) Local reconstruction structure consistency:\nto steer the completion of D, the local linear reconstruction structures in\nfeature space and tag space are obtained and preserved by U and V respectively.\nSuch a scheme could alleviate the negative effect of distances measured by\nlow-level features and incomplete tags. Thus, we can seek a balance between\nexploiting as much information and not being mislead to suboptimal performance.\nExperiments conducted on Corel5k dataset and the newly issued Flickr30Concepts\ndataset demonstrate the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 01:22:43 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Li", "Xue", ""], ["Zhang", "Yu-Jin", ""], ["Shen", "Bin", ""], ["Liu", "Bao-Di", ""]]}, {"id": "1406.2080", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev and\n  Rob Fergus", "title": "Training Convolutional Networks with Noisy Labels", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large labeled datasets has allowed Convolutional Network\nmodels to achieve impressive recognition results. However, in many settings\nmanual annotation of the data is impractical; instead our data has noisy\nlabels, i.e. there is some freely available label for each image which may or\nmay not be accurate. In this paper, we explore the performance of\ndiscriminatively-trained Convnets when trained on such noisy data. We introduce\nan extra noise layer into the network which adapts the network outputs to match\nthe noisy label distribution. The parameters of this noise layer can be\nestimated as part of the training process and involve simple modifications to\ncurrent training infrastructures for deep networks. We demonstrate the\napproaches on several datasets, including large scale experiments on the\nImageNet classification benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:45:12 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 21:10:03 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 21:13:47 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 16:44:00 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Bruna", "Joan", ""], ["Paluri", "Manohar", ""], ["Bourdev", "Lubomir", ""], ["Fergus", "Rob", ""]]}, {"id": "1406.2139", "submitter": "Conrad Sanderson", "authors": "Masoud Faraki, Maziar Palhang, Conrad Sanderson", "title": "Log-Euclidean Bag of Words for Human Action Recognition", "comments": null, "journal-ref": "IET Computer Vision, Vol. 9, No. 3, 2015", "doi": "10.1049/iet-cvi.2014.0018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing videos by densely extracted local space-time features has\nrecently become a popular approach for analysing actions. In this paper, we\ntackle the problem of categorising human actions by devising Bag of Words (BoW)\nmodels based on covariance matrices of spatio-temporal features, with the\nfeatures formed from histograms of optical flow. Since covariance matrices form\na special type of Riemannian manifold, the space of Symmetric Positive Definite\n(SPD) matrices, non-Euclidean geometry should be taken into account while\ndiscriminating between covariance matrices. To this end, we propose to embed\nSPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW\napproach to its Riemannian version. The proposed BoW approach takes into\naccount the manifold geometry of SPD matrices during the generation of the\ncodebook and histograms. Experiments on challenging human action datasets show\nthat the proposed method obtains notable improvements in discrimination\naccuracy, in comparison to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 11:14:03 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 09:33:58 GMT"}, {"version": "v3", "created": "Thu, 7 Jul 2016 09:27:40 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Faraki", "Masoud", ""], ["Palhang", "Maziar", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1406.2199", "submitter": "Karen Simonyan", "authors": "Karen Simonyan, Andrew Zisserman", "title": "Two-Stream Convolutional Networks for Action Recognition in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate architectures of discriminatively trained deep Convolutional\nNetworks (ConvNets) for action recognition in video. The challenge is to\ncapture the complementary information on appearance from still frames and\nmotion between frames. We also aim to generalise the best performing\nhand-crafted features within a data-driven learning framework.\n  Our contribution is three-fold. First, we propose a two-stream ConvNet\narchitecture which incorporates spatial and temporal networks. Second, we\ndemonstrate that a ConvNet trained on multi-frame dense optical flow is able to\nachieve very good performance in spite of limited training data. Finally, we\nshow that multi-task learning, applied to two different action classification\ndatasets, can be used to increase the amount of training data and improve the\nperformance on both.\n  Our architecture is trained and evaluated on the standard video actions\nbenchmarks of UCF-101 and HMDB-51, where it is competitive with the state of\nthe art. It also exceeds by a large margin previous attempts to use deep nets\nfor video classification.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 14:44:14 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 20:48:33 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Simonyan", "Karen", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1406.2227", "submitter": "Max Jaderberg", "authors": "Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman", "title": "Synthetic Data and Artificial Neural Networks for Natural Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a framework for the recognition of natural scene\ntext. Our framework does not require any human-labelled data, and performs word\nrecognition on the whole image holistically, departing from the character based\nrecognition systems of the past. The deep neural network models at the centre\nof this framework are trained solely on data produced by a synthetic text\ngeneration engine -- synthetic data that is highly realistic and sufficient to\nreplace real data, giving us infinite amounts of training data. This excess of\ndata exposes new possibilities for word recognition models, and here we\nconsider three models, each one \"reading\" words in a different way: via 90k-way\ndictionary encoding, character sequence encoding, and bag-of-N-grams encoding.\nIn the scenarios of language based and completely unconstrained text\nrecognition we greatly improve upon state-of-the-art performance on standard\ndatasets, using our fast, simple machinery and requiring zero data-acquisition\ncosts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 15:53:33 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 03:10:35 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 16:08:24 GMT"}, {"version": "v4", "created": "Tue, 9 Dec 2014 11:22:59 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Jaderberg", "Max", ""], ["Simonyan", "Karen", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1406.2282", "submitter": "Chunyu Wang", "authors": "Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L. Yuille, Wen Gao", "title": "Robust Estimation of 3D Human Poses from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is a key step to action recognition. We propose a\nmethod of estimating 3D human poses from a single image, which works in\nconjunction with an existing 2D pose/joint detector. 3D pose estimation is\nchallenging because multiple 3D poses may correspond to the same 2D pose after\nprojection due to the lack of depth information. Moreover, current 2D pose\nestimators are usually inaccurate which may cause errors in the 3D estimation.\nWe address the challenges in three ways: (i) We represent a 3D pose as a linear\ncombination of a sparse set of bases learned from 3D human skeletons. (ii) We\nenforce limb length constraints to eliminate anthropomorphically implausible\nskeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm error\nbetween the projection of the 3D pose and the corresponding 2D detection. The\n$L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use the\nalternating direction method (ADM) to solve the optimization problem\nefficiently. Our approach outperforms the state-of-the-arts on three benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 18:55:31 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Wang", "Chunyu", ""], ["Wang", "Yizhou", ""], ["Lin", "Zhouchen", ""], ["Yuille", "Alan L.", ""], ["Gao", "Wen", ""]]}, {"id": "1406.2283", "submitter": "David Eigen", "authors": "David Eigen and Christian Puhrsch and Rob Fergus", "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting depth is an essential component in understanding the 3D geometry\nof a scene. While for stereo images local correspondence suffices for\nestimation, finding depth relations from a single image is less\nstraightforward, requiring integration of both global and local information\nfrom various cues. Moreover, the task is inherently ambiguous, with a large\nsource of uncertainty coming from the overall scale. In this paper, we present\na new method that addresses this task by employing two deep network stacks: one\nthat makes a coarse global prediction based on the entire image, and another\nthat refines this prediction locally. We also apply a scale-invariant error to\nhelp measure depth relations rather than scale. By leveraging the raw datasets\nas large sources of training data, our method achieves state-of-the-art results\non both NYU Depth and KITTI, and matches detailed depth boundaries without the\nneed for superpixelation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 19:01:18 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Eigen", "David", ""], ["Puhrsch", "Christian", ""], ["Fergus", "Rob", ""]]}, {"id": "1406.2375", "submitter": "Xiaochen Lian", "authors": "Wenhao Lu, Xiaochen Lian and Alan Yuille", "title": "Parsing Semantic Parts of Cars Using Graphical Models and Segment\n  Appearance Consistency", "comments": "12 pages, CBMM memo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of semantic part parsing (segmentation) of\ncars, i.e.assigning every pixel within the car to one of the parts (e.g.body,\nwindow, lights, license plates and wheels). We formulate this as a landmark\nidentification problem, where a set of landmarks specifies the boundaries of\nthe parts. A novel mixture of graphical models is proposed, which dynamically\ncouples the landmarks to a hierarchy of segments. When modeling pairwise\nrelation between landmarks, this coupling enables our model to exploit the\nlocal image contents in addition to spatial deformation, an aspect that most\nexisting graphical models ignore. In particular, our model enforces appearance\nconsistency between segments within the same part. Parsing the car, including\nfinding the optimal coupling between landmarks and segments in the hierarchy,\nis performed by dynamic programming. We evaluate our method on a subset of\nPASCAL VOC 2010 car images and on the car subset of 3D Object Category dataset\n(CAR3D). We show good results and, in particular, quantify the effectiveness of\nusing the segment appearance consistency in terms of accuracy of part\nlocalization and segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 22:16:57 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 23:39:41 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Lu", "Wenhao", ""], ["Lian", "Xiaochen", ""], ["Yuille", "Alan", ""]]}, {"id": "1406.2390", "submitter": "Xu Chen", "authors": "Xu Chen, Xiuyuan Cheng and St\\'ephane Mallat", "title": "Unsupervised Deep Haar Scattering on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of high-dimensional data defined on graphs is particularly\ndifficult when the graph geometry is unknown. We introduce a Haar scattering\ntransform on graphs, which computes invariant signal descriptors. It is\nimplemented with a deep cascade of additions, subtractions and absolute values,\nwhich iteratively compute orthogonal Haar wavelet transforms. Multiscale\nneighborhoods of unknown graphs are estimated by minimizing an average total\nvariation, with a pair matching algorithm of polynomial complexity. Supervised\nclassification with dimension reduction is tested on data bases of scrambled\nimages, and for signals sampled on unknown irregular grids on a sphere.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 23:51:30 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 15:25:16 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chen", "Xu", ""], ["Cheng", "Xiuyuan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1406.2407", "submitter": "Hilton Bristow", "authors": "Hilton Bristow, Simon Lucey", "title": "Optimization Methods for Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse and convolutional constraints form a natural prior for many\noptimization problems that arise from physical processes. Detecting motifs in\nspeech and musical passages, super-resolving images, compressing videos, and\nreconstructing harmonic motions can all leverage redundancies introduced by\nconvolution. Solving problems involving sparse and convolutional constraints\nremains a difficult computational problem, however. In this paper we present an\noverview of convolutional sparse coding in a consistent framework. The\nobjective involves iteratively optimizing a convolutional least-squares term\nfor the basis functions, followed by an L1-regularized least squares term for\nthe sparse coefficients. We discuss a range of optimization methods for solving\nthe convolutional sparse coding objective, and the properties that make each\nmethod suitable for different applications. In particular, we concentrate on\ncomputational complexity, speed to {\\epsilon} convergence, memory usage, and\nthe effect of implied boundary conditions. We present a broad suite of examples\ncovering different signal and application domains to illustrate the general\napplicability of convolutional sparse coding, and the efficacy of the available\noptimization methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 02:41:03 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Bristow", "Hilton", ""], ["Lucey", "Simon", ""]]}, {"id": "1406.2419", "submitter": "Hilton Bristow", "authors": "Hilton Bristow, Simon Lucey", "title": "Why do linear SVMs trained on HOG features perform so well?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Support Vector Machines trained on HOG features are now a de facto\nstandard across many visual perception tasks. Their popularisation can largely\nbe attributed to the step-change in performance they brought to pedestrian\ndetection, and their subsequent successes in deformable parts models. This\npaper explores the interactions that make the HOG-SVM symbiosis perform so\nwell. By connecting the feature extraction and learning processes rather than\ntreating them as disparate plugins, we show that HOG features can be viewed as\ndoing two things: (i) inducing capacity in, and (ii) adding prior to a linear\nSVM trained on pixels. From this perspective, preserving second-order\nstatistics and locality of interactions are key to good performance. We\ndemonstrate surprising accuracy on expression recognition and pedestrian\ndetection tasks, by assuming only the importance of preserving such local\nsecond-order interactions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 04:34:43 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Bristow", "Hilton", ""], ["Lucey", "Simon", ""]]}, {"id": "1406.2528", "submitter": "Mohammad Tofighi", "authors": "A. Enis Cetin, and Mohammad Tofighi", "title": "Denosing Using Wavelets and Projections onto the L1-Ball", "comments": "Submitted to Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both wavelet denoising and denosing methods using the concept of sparsity are\nbased on soft-thresholding. In sparsity based denoising methods, it is assumed\nthat the original signal is sparse in some transform domains such as the\nwavelet domain and the wavelet subsignals of the noisy signal are projected\nonto L1-balls to reduce noise. In this lecture note, it is shown that the size\nof the L1-ball or equivalently the soft threshold value can be determined using\nlinear algebra. The key step is an orthogonal projection onto the epigraph set\nof the L1-norm cost function.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 12:42:44 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Cetin", "A. Enis", ""], ["Tofighi", "Mohammad", ""]]}, {"id": "1406.2580", "submitter": "L.T. Handoko", "authors": "D. H. Apriyanti, A.A. Arymurthy, L.T. Handoko", "title": "Identification of Orchid Species Using Content-Based Flower Image\n  Retrieval", "comments": "Proceeding of International Conference on Computer, Control,\n  Informatics and its Applications 2013, pp. 53-57", "journal-ref": null, "doi": "10.1109/IC3INA.2013.6819148", "report-no": "KRPURWODADILIPI-13044", "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we developed the system for recognizing the orchid species by\nusing the images of flower. We used MSRM (Maximal Similarity based on Region\nMerging) method for segmenting the flower object from the background and\nextracting the shape feature such as the distance from the edge to the centroid\npoint of the flower, aspect ratio, roundness, moment invariant, fractal\ndimension and also extract color feature. We used HSV color feature with\nignoring the V value. To retrieve the image, we used Support Vector Machine\n(SVM) method. Orchid is a unique flower. It has a part of flower called lip\n(labellum) that distinguishes it from other flowers even from other types of\norchids. Thus, in this paper, we proposed to do feature extraction not only on\nflower region but also on lip (labellum) region. The result shows that our\nproposed method can increase the accuracy value of content based flower image\nretrieval for orchid species up to $\\pm$ 14%. The most dominant feature is\nCentroid Contour Distance, Moment Invariant and HSV Color. The system accuracy\nis 85,33% in validation phase and 79,33% in testing phase.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:11:27 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Apriyanti", "D. H.", ""], ["Arymurthy", "A. A.", ""], ["Handoko", "L. T.", ""]]}, {"id": "1406.2602", "submitter": "Ethan Fetaya", "authors": "Ethan Fetaya, Ohad Shamir and Shimon Ullman", "title": "Graph Approximation and Clustering on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from a similarity matrix (such as\nspectral clustering and lowd imensional embedding), when computing pairwise\nsimilarities are costly, and only a limited number of entries can be observed.\nWe provide a theoretical analysis using standard notions of graph\napproximation, significantly generalizing previous results (which focused on\nspectral clustering with two clusters). We also propose a new algorithmic\napproach based on adaptive sampling, which experimentally matches or improves\non previous methods, while being considerably more general and computationally\ncheaper.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:49:05 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Fetaya", "Ethan", ""], ["Shamir", "Ohad", ""], ["Ullman", "Shimon", ""]]}, {"id": "1406.2639", "submitter": "Holger Roth", "authors": "Holger R. Roth and Le Lu and Ari Seff and Kevin M. Cherry and Joanne\n  Hoffman and Shijun Wang and Jiamin Liu and Evrim Turkbey and Ronald M.\n  Summers", "title": "A New 2.5D Representation for Lymph Node Detection using Random Sets of\n  Deep Convolutional Neural Network Observations", "comments": "This article will be presented at MICCAI (Medical Image Computing and\n  Computer-Assisted Interventions) 2014", "journal-ref": "Medical Image Computing and Computer-Assisted Intervention -\n  MICCAI 2014 Volume 8673 of the series Lecture Notes in Computer Science pp\n  520-527", "doi": "10.1007/978-3-319-10404-1_65", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automated Lymph Node (LN) detection is an important clinical diagnostic task\nbut very challenging due to the low contrast of surrounding structures in\nComputed Tomography (CT) and to their varying sizes, poses, shapes and sparsely\ndistributed locations. State-of-the-art studies show the performance range of\n52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1\nFP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this\npaper, we first operate a preliminary candidate generation stage, towards 100%\nsensitivity at the cost of high FP levels (40 per patient), to harvest volumes\nof interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by\nresampling 2D reformatted orthogonal views N times, via scale, random\ntranslations, and rotations with respect to the VOI centroid coordinates. These\nrandom views are then used to train a deep Convolutional Neural Network (CNN)\nclassifier. In testing, the CNN is employed to assign LN probabilities for all\nN random views that can be simply averaged (as a set) to compute the final\nclassification probability per VOI. We validate the approach on two datasets:\n90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs.\nWe achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in\nmediastinum and abdomen respectively, which drastically improves over the\nprevious state-of-the-art work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:43:42 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Seff", "Ari", ""], ["Cherry", "Kevin M.", ""], ["Hoffman", "Joanne", ""], ["Wang", "Shijun", ""], ["Liu", "Jiamin", ""], ["Turkbey", "Evrim", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1406.2732", "submitter": "George Papandreou", "authors": "George Papandreou", "title": "Deep Epitomic Convolutional Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have recently proven extremely competitive\nin challenging image recognition tasks. This paper proposes the epitomic\nconvolution as a new building block for deep neural networks. An epitomic\nconvolution layer replaces a pair of consecutive convolution and max-pooling\nlayers found in standard deep convolutional neural networks. The main version\nof the proposed model uses mini-epitomes in place of filters and computes\nresponses invariant to small translations by epitomic search instead of\nmax-pooling over image positions. The topographic version of the proposed model\nuses large epitomes to learn filter maps organized in translational\ntopographies. We show that error back-propagation can successfully learn\nmultiple epitomic layers in a supervised fashion. The effectiveness of the\nproposed method is assessed in image classification tasks on standard\nbenchmarks. Our experiments on Imagenet indicate improved recognition\nperformance compared to standard convolutional neural networks of similar\narchitecture. Our models pre-trained on Imagenet perform excellently on\nCaltech-101. We also obtain competitive image classification results on the\nsmall-image MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 22:07:01 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Papandreou", "George", ""]]}, {"id": "1406.2807", "submitter": "Yin Li", "authors": "Yin Li, Xiaodi Hou, Christof Koch, James M. Rehg, Alan L. Yuille", "title": "The Secrets of Salient Object Segmentation", "comments": "15 pages, 8 figures. Conference version was accepted by CVPR 2014", "journal-ref": null, "doi": null, "report-no": "CBMM Memmo #14", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we provide an extensive evaluation of fixation prediction and\nsalient object segmentation algorithms as well as statistics of major datasets.\nOur analysis identifies serious design flaws of existing salient object\nbenchmarks, called the dataset design bias, by over emphasizing the\nstereotypical concepts of saliency. The dataset design bias does not only\ncreate the discomforting disconnection between fixations and salient object\nsegmentation, but also misleads the algorithm designing. Based on our analysis,\nwe propose a new high quality dataset that offers both fixation and salient\nobject segmentation ground-truth. With fixations and salient object being\npresented simultaneously, we are able to bridge the gap between fixations and\nsalient objects, and propose a novel method for salient object segmentation.\nFinally, we report significant benchmark progress on three existing datasets of\nsegmenting salient objects\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 07:46:03 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 17:35:08 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Li", "Yin", ""], ["Hou", "Xiaodi", ""], ["Koch", "Christof", ""], ["Rehg", "James M.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1406.2895", "submitter": "J\\\"urgen Geiger", "authors": "J\\\"urgen T. Geiger, Maximilian Knei{\\ss}l, Bj\\\"orn Schuller and\n  Gerhard Rigoll", "title": "Acoustic Gait-based Person Identification using Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for identifying humans by their walking sounds. This\nproblem is also known as acoustic gait recognition. The goal of the system is\nto analyse sounds emitted by walking persons (mostly the step sounds) and\nidentify those persons. These sounds are characterised by the gait pattern and\nare influenced by the movements of the arms and legs, but also depend on the\ntype of shoe. We extract cepstral features from the recorded audio signals and\nuse hidden Markov models for dynamic classification. A cyclic model topology is\nemployed to represent individual gait cycles. This topology allows to model and\ndetect individual steps, leading to very promising identification rates. For\nexperimental validation, we use the publicly available TUM GAID database, which\nis a large gait recognition database containing 3050 recordings of 305 subjects\nin three variations. In the best setup, an identification rate of 65.5 % is\nachieved out of 155 subjects. This is a relative improvement of almost 30 %\ncompared to our previous work, which used various audio features and support\nvector machines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 13:14:32 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Geiger", "J\u00fcrgen T.", ""], ["Knei\u00dfl", "Maximilian", ""], ["Schuller", "Bj\u00f6rn", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1406.2952", "submitter": "Grant Van Horn", "authors": "Steve Branson and Grant Van Horn and Serge Belongie and Pietro Perona", "title": "Bird Species Categorization Using Pose Normalized Deep Convolutional\n  Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an architecture for fine-grained visual categorization that\napproaches expert human performance in the classification of bird species. Our\narchitecture first computes an estimate of the object's pose; this is used to\ncompute local image features which are, in turn, used for classification. The\nfeatures are computed by applying deep convolutional nets to image patches that\nare located and normalized by the pose. We perform an empirical study of a\nnumber of pose normalization schemes, including an investigation of higher\norder geometric warping functions. We propose a novel graph-based clustering\nalgorithm for learning a compact pose normalization space. We perform a\ndetailed investigation of state-of-the-art deep convolutional feature\nimplementations and fine-tuning feature learning for fine-grained\nclassification. We observe that a model that integrates lower-level feature\nlayers with pose-normalized extraction routines and higher-level feature layers\nwith unaligned image features works best. Our experiments advance\nstate-of-the-art performance on bird species recognition, with a large\nimprovement of correct classification rates over previous methods (75% vs.\n55-65%).\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 16:14:42 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Branson", "Steve", ""], ["Van Horn", "Grant", ""], ["Belongie", "Serge", ""], ["Perona", "Pietro", ""]]}, {"id": "1406.2969", "submitter": "Yilun Wang", "authors": "Yilun Wang and Xinhua Su", "title": "Truncated Nuclear Norm Minimization for Image Restoration Based On\n  Iterative Support Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a large matrix from limited measurements is a challenging task\narising in many real applications, such as image inpainting, compressive\nsensing and medical imaging, and this kind of problems are mostly formulated as\nlow-rank matrix approximation problems. Due to the rank operator being\nnon-convex and discontinuous, most of the recent theoretical studies use the\nnuclear norm as a convex relaxation and the low-rank matrix recovery problem is\nsolved through minimization of the nuclear norm regularized problem. However, a\nmajor limitation of nuclear norm minimization is that all the singular values\nare simultaneously minimized and the rank may not be well approximated\n\\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage\nalgorithm, which makes use of the concept of Truncated Nuclear Norm\nRegularization (TNNR) proposed in \\citep{hu2012fast} and Iterative Support\nDetection (ISD) proposed in \\citep{wang2010sparse} to overcome the above\nlimitation. Besides matrix completion problems considered in\n\\citep{hu2012fast}, the proposed method can be also extended to the general\nlow-rank matrix recovery problems. Extensive experiments well validate the\nsuperiority of our new algorithms over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 17:18:25 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Wang", "Yilun", ""], ["Su", "Xinhua", ""]]}, {"id": "1406.2984", "submitter": "Jonathan Tompson", "authors": "Jonathan Tompson, Arjun Jain, Yann LeCun, Christoph Bregler", "title": "Joint Training of a Convolutional Network and a Graphical Model for\n  Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new hybrid architecture that consists of a deep\nConvolutional Network and a Markov Random Field. We show how this architecture\nis successfully applied to the challenging problem of articulated human pose\nestimation in monocular images. The architecture can exploit structural domain\nconstraints such as geometric relationships between body joint locations. We\nshow that joint training of these two model paradigms improves performance and\nallows us to significantly outperform existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 18:16:29 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 22:43:45 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Tompson", "Jonathan", ""], ["Jain", "Arjun", ""], ["LeCun", "Yann", ""], ["Bregler", "Christoph", ""]]}, {"id": "1406.3010", "submitter": "Weiguang Ding", "authors": "Weiguang Ding, Graham W. Taylor", "title": "\"Mental Rotation\" by Optimizing Transforming Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system is able to recognize objects despite transformations\nthat can drastically alter their appearance. To this end, much effort has been\ndevoted to the invariance properties of recognition systems. Invariance can be\nengineered (e.g. convolutional nets), or learned from data explicitly (e.g.\ntemporal coherence) or implicitly (e.g. by data augmentation). One idea that\nhas not, to date, been explored is the integration of latent variables which\npermit a search over a learned space of transformations. Motivated by evidence\nthat people mentally simulate transformations in space while comparing\nexamples, so-called \"mental rotation\", we propose a transforming distance.\nHere, a trained relational model actively transforms pairs of examples so that\nthey are maximally similar in some feature space yet respect the learned\ntransformational constraints. We apply our method to nearest-neighbour problems\non the Toronto Face Database and NORB.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 19:38:05 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 17:55:09 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Ding", "Weiguang", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1406.3332", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann), Piotr Koniusz (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Zaid Harchaoui (INRIA Grenoble Rh\\^one-Alpes / LJK\n  Laboratoire Jean Kuntzmann), Cordelia Schmid (INRIA Grenoble Rh\\^one-Alpes /\n  LJK Laboratoire Jean Kuntzmann)", "title": "Convolutional Kernel Networks", "comments": "appears in Advances in Neural Information Processing Systems (NIPS),\n  Dec 2014, Montreal, Canada, http://nips.cc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal in visual recognition is to devise image representations\nthat are invariant to particular transformations. In this paper, we address\nthis goal with a new type of convolutional neural network (CNN) whose\ninvariance is encoded by a reproducing kernel. Unlike traditional approaches\nwhere neural networks are learned either to represent data or for solving a\nclassification task, our network learns to approximate the kernel feature map\non training data. Such an approach enjoys several benefits over classical ones.\nFirst, by teaching CNNs to be invariant, we obtain simple network architectures\nthat achieve a similar accuracy to more complex ones, while being easy to train\nand robust to overfitting. Second, we bridge a gap between the neural network\nliterature and kernels, which are natural tools to model invariance. We\nevaluate our methodology on visual recognition tasks where CNNs have proven to\nperform well, e.g., digit recognition with the MNIST dataset, and the more\nchallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive\nwith the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 19:41:03 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 16:58:48 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"], ["Koniusz", "Piotr", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"], ["Harchaoui", "Zaid", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK\n  Laboratoire Jean Kuntzmann"], ["Schmid", "Cordelia", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LJK Laboratoire Jean Kuntzmann"]]}, {"id": "1406.3418", "submitter": "Ankit Chaudhary", "authors": "Ankit Chaudhary, J.L. Raheja, K. Das, S. Raheja", "title": "Fingers' Angle Calculation using Level-Set Method", "comments": "7 pages, IGI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current age, use of natural communication in human computer\ninteraction is a known and well installed thought. Hand gesture recognition and\ngesture based applications has gained a significant amount of popularity\namongst people all over the world. It has a number of applications ranging from\nsecurity to entertainment. These applications generally are real time\napplications and need fast, accurate communication with machines. On the other\nend, gesture based communications have few limitations also like bent finger\ninformation is not provided in vision based techniques. In this paper, a novel\nmethod for fingertip detection and for angle calculation of both hands bent\nfingers is discussed. Angle calculation has been done before with sensor based\ngloves/devices. This study has been conducted in the context of natural\ncomputing for calculating angles without using any wired equipment, colors,\nmarker or any device. The pre-processing and segmentation of the region of\ninterest is performed in a HSV color space and a binary format respectively.\nFingertips are detected using level-set method and angles were calculated using\ngeometrical analysis. This technique requires no training for system to perform\nthe task.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 04:33:46 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Chaudhary", "Ankit", ""], ["Raheja", "J. L.", ""], ["Das", "K.", ""], ["Raheja", "S.", ""]]}, {"id": "1406.3474", "submitter": "Sijin Li", "authors": "Sijin Li, Zhi-Qiang Liu, Antoni B. Chan", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an heterogeneous multi-task learning framework for human pose\nestimation from monocular image with deep convolutional neural network. In\nparticular, we simultaneously learn a pose-joint regressor and a sliding-window\nbody-part detector in a deep network architecture. We show that including the\nbody-part detection task helps to regularize the network, directing it to\nconverge to a good solution. We report competitive and state-of-art results on\nseveral data sets. We also empirically show that the learned neurons in the\nmiddle layer of our network are tuned to localized body parts.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:11:18 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Li", "Sijin", ""], ["Liu", "Zhi-Qiang", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1406.3793", "submitter": "Cheston Tan", "authors": "Cheston Tan, Tomaso Poggio", "title": "Neural tuning size is a key factor underlying holistic face processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faces are a class of visual stimuli with unique significance, for a variety\nof reasons. They are ubiquitous throughout the course of a person's life, and\nface recognition is crucial for daily social interaction. Faces are also unlike\nany other stimulus class in terms of certain physical stimulus characteristics.\nFurthermore, faces have been empirically found to elicit certain characteristic\nbehavioral phenomena, which are widely held to be evidence of \"holistic\"\nprocessing of faces. However, little is known about the neural mechanisms\nunderlying such holistic face processing. In other words, for the processing of\nfaces by the primate visual system, the input and output characteristics are\nrelatively well known, but the internal neural computations are not. The main\naim of this work is to further the fundamental understanding of what causes the\nvisual processing of faces to be different from that of objects. In this\ncomputational modeling work, we show that a single factor - \"neural tuning\nsize\" - is able to account for three key phenomena that are characteristic of\nface processing, namely the Composite Face Effect (CFE), Face Inversion Effect\n(FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle\nprovides specific neural tuning properties that correspond to the\npoorly-understood notion of holistic face processing, and connects these neural\nproperties to psychophysical behavior. Overall, our work provides a unified and\nparsimonious theoretical account for the disparate empirical data on\nface-specific processing, deepening the fundamental understanding of face\nprocessing.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 03:05:07 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Tan", "Cheston", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1406.3906", "submitter": "Roozbeh Mottaghi", "authors": "Roozbeh Mottaghi, Sanja Fidler, Alan Yuille, Raquel Urtasun, Devi\n  Parikh", "title": "Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in image understanding have pushed for holistic scene\nunderstanding models that jointly reason about various tasks such as object\ndetection, scene recognition, shape analysis, contextual reasoning, and local\nappearance based classifiers. In this work, we are interested in understanding\nthe roles of these different tasks in improved scene understanding, in\nparticular semantic segmentation, object detection and scene recognition.\nTowards this goal, we \"plug-in\" human subjects for each of the various\ncomponents in a state-of-the-art conditional random field model. Comparisons\namong various hybrid human-machine CRFs give us indications of how much \"head\nroom\" there is to improve scene understanding by focusing research efforts on\nvarious individual tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 05:37:17 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Mottaghi", "Roozbeh", ""], ["Fidler", "Sanja", ""], ["Yuille", "Alan", ""], ["Urtasun", "Raquel", ""], ["Parikh", "Devi", ""]]}, {"id": "1406.3949", "submitter": "Jamil Ahmad", "authors": "Jamil Ahmad, Zahoor Jan, Zia-ud-Din and Shoaib Muhammad Khan", "title": "A Fusion of Labeled-Grid Shape Descriptors with Weighted Ranking\n  Algorithm for Shapes Recognition", "comments": null, "journal-ref": "World Applied Sciences Journal, vol. 31(6), pp. 1207-1213, 2014", "doi": "10.5829/idosi.wasj.2014.31.06.353", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving similar images from a large dataset based on the image content has\nbeen a very active research area and is a very challenging task. Studies have\nshown that retrieving similar images based on their shape is a very effective\nmethod. For this purpose a large number of methods exist in literature. The\ncombination of more than one feature has also been investigated for this\npurpose and has shown promising results. In this paper a fusion based shapes\nrecognition method has been proposed. A set of local boundary based and region\nbased features are derived from the labeled grid based representation of the\nshape and are combined with a few global shape features to produce a composite\nshape descriptor. This composite shape descriptor is then used in a weighted\nranking algorithm to find similarities among shapes from a large dataset. The\nexperimental analysis has shown that the proposed method is powerful enough to\ndiscriminate the geometrically similar shapes from the non-similar ones.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 09:50:04 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Ahmad", "Jamil", ""], ["Jan", "Zahoor", ""], ["Zia-ud-Din", "", ""], ["Khan", "Shoaib Muhammad", ""]]}, {"id": "1406.4007", "submitter": "Dibya Jyoti Bora", "authors": "Dibya Jyoti Bora, Anil Kumar Gupta", "title": "Impact of Exponent Parameter Value for the Partition Matrix on the\n  Performance of Fuzzy C Means Algorithm", "comments": "5 pages,8 figures, 2 tables, Soft clustering, Fuzzy C Means.\n  IJSRCSAMS, Volume 3, Issue 3, May 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft Clustering plays a very important rule on clustering real world data\nwhere a data item contributes to more than one cluster. Fuzzy logic based\nalgorithms are always suitable for performing soft clustering tasks. Fuzzy C\nMeans (FCM) algorithm is a very popular fuzzy logic based algorithm. In case of\nfuzzy logic based algorithm, the parameter like exponent for the partition\nmatrix that we have to fix for the clustering task plays a very important rule\non the performance of the algorithm. In this paper, an experimental analysis is\ndone on FCM algorithm to observe the impact of this parameter on the\nperformance of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 13:22:18 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Bora", "Dibya Jyoti", ""], ["Gupta", "Anil Kumar", ""]]}, {"id": "1406.4112", "submitter": "Zhenyong Fu", "authors": "Zhen-Yong Fu, Tao Xiang, Shaogang Gong", "title": "Semantic Graph for Zero-Shot Learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Zero-shot learning aims to classify visual objects without any training data\nvia knowledge transfer between seen and unseen classes. This is typically\nachieved by exploring a semantic embedding space where the seen and unseen\nclasses can be related. Previous works differ in what embedding space is used\nand how different classes and a test image can be related. In this paper, we\nutilize the annotation-free semantic word space for the former and focus on\nsolving the latter issue of modeling relatedness. Specifically, in contrast to\nprevious work which ignores the semantic relationships between seen classes and\nfocus merely on those between seen and unseen classes, in this paper a novel\napproach based on a semantic graph is proposed to represent the relationships\nbetween all the seen and unseen class in a semantic word space. Based on this\nsemantic graph, we design a special absorbing Markov chain process, in which\neach unseen class is viewed as an absorbing state. After incorporating one test\nimage into the semantic graph, the absorbing probabilities from the test data\nto each unseen class can be effectively computed; and zero-shot classification\ncan be achieved by finding the class label with the highest absorbing\nprobability. The proposed model has a closed-form solution which is linear with\nrespect to the number of test images. We demonstrate the effectiveness and\ncomputational efficiency of the proposed method over the state-of-the-arts on\nthe AwA (animals with attributes) dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 19:40:52 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 09:53:18 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Fu", "Zhen-Yong", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1406.4205", "submitter": "Peter F. Schultz", "authors": "Peter F. Schultz and Dylan M. Paiton and Wei Lu and Garrett T. Kenyon", "title": "Replicating Kernels with a Short Stride Allows Sparse Reconstructions\n  with Fewer Independent Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse coding it is common to tile an image into nonoverlapping patches,\nand then use a dictionary to create a sparse representation of each tile\nindependently. In this situation, the overcompleteness of the dictionary is the\nnumber of dictionary elements divided by the patch size. In deconvolutional\nneural networks (DCNs), dictionaries learned on nonoverlapping tiles are\nreplaced by a family of convolution kernels. Hence adjacent points in the\nfeature maps (V1 layers) have receptive fields in the image that are\ntranslations of each other. The translational distance is determined by the\ndimensions of V1 in comparison to the dimensions of the image space. We refer\nto this translational distance as the stride.\n  We implement a type of DCN using a modified Locally Competitive Algorithm\n(LCA) to investigate the relationship between the number of kernels, the\nstride, the receptive field size, and the quality of reconstruction. We find,\nfor example, that for 16x16-pixel receptive fields, using eight kernels and a\nstride of 2 leads to sparse reconstructions of comparable quality as using 512\nkernels and a stride of 16 (the nonoverlapping case). We also find that for a\ngiven stride and number of kernels, the patch size does not significantly\naffect reconstruction quality. Instead, the learned convolution kernels have a\nnatural support radius independent of the patch size.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 01:07:48 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Schultz", "Peter F.", ""], ["Paiton", "Dylan M.", ""], ["Lu", "Wei", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1406.4216", "submitter": "Shengcai Liao", "authors": "Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z. Li", "title": "Person Re-identification by Local Maximal Occurrence Representation and\n  Metric Learning", "comments": "This paper has been accepted by CVPR 2015. For source codes and\n  extracted features please visit\n  http://www.cbsr.ia.ac.cn/users/scliao/projects/lomo_xqda/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is an important technique towards automatic search\nof a person's presence in a surveillance video. Two fundamental problems are\ncritical for person re-identification, feature representation and metric\nlearning. An effective feature representation should be robust to illumination\nand viewpoint changes, and a discriminant metric should be learned to match\nvarious person images. In this paper, we propose an effective feature\nrepresentation called Local Maximal Occurrence (LOMO), and a subspace and\nmetric learning method called Cross-view Quadratic Discriminant Analysis\n(XQDA). The LOMO feature analyzes the horizontal occurrence of local features,\nand maximizes the occurrence to make a stable representation against viewpoint\nchanges. Besides, to handle illumination variations, we apply the Retinex\ntransform and a scale invariant texture operator. To learn a discriminant\nmetric, we propose to learn a discriminant low dimensional subspace by\ncross-view quadratic discriminant analysis, and simultaneously, a QDA metric is\nlearned on the derived subspace. We also present a practical computation method\nfor XQDA, as well as its regularization. Experiments on four challenging person\nre-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show\nthat the proposed method improves the state-of-the-art rank-1 identification\nrates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 01:53:37 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 14:01:28 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Liao", "Shengcai", ""], ["Hu", "Yang", ""], ["Zhu", "Xiangyu", ""], ["Li", "Stan Z.", ""]]}, {"id": "1406.4296", "submitter": "Adrien Gaidon", "authors": "Adrien Gaidon (Xerox Research Center Europe, France), Gloria Zen\n  (University of Trento, Italy), Jose A. Rodriguez-Serrano (Xerox Research\n  Center Europe, France)", "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to\n  Unlabeled Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning object detectors requires massive amounts of labeled training\nsamples from the specific data source of interest. This is impractical when\ndealing with many different sources (e.g., in camera networks), or constantly\nchanging ones such as mobile cameras (e.g., in robotics or driving assistant\nsystems). In this paper, we address the problem of self-learning detectors in\nan autonomous manner, i.e. (i) detectors continuously updating themselves to\nefficiently adapt to streaming data sources (contrary to transductive\nalgorithms), (ii) without any labeled data strongly related to the target data\nstream (contrary to self-paced learning), and (iii) without manual intervention\nto set and update hyper-parameters. To that end, we propose an unsupervised,\non-line, and self-tuning learning algorithm to optimize a multi-task learning\nconvex objective. Our method uses confident but laconic oracles (high-precision\nbut low-recall off-the-shelf generic detectors), and exploits the structure of\nthe problem to jointly learn on-line an ensemble of instance-level trackers,\nfrom which we derive an adapted category-level object detector. Our approach is\nvalidated on real-world publicly available video object datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 09:51:18 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 12:33:22 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Gaidon", "Adrien", "", "Xerox Research Center Europe, France"], ["Zen", "Gloria", "", "University of Trento, Italy"], ["Rodriguez-Serrano", "Jose A.", "", "Xerox Research\n  Center Europe, France"]]}, {"id": "1406.4444", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "PRISM: Person Re-Identification via Structured Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id), an emerging problem in visual surveillance,\ndeals with maintaining entities of individuals whilst they traverse various\nlocations surveilled by a camera network. From a visual perspective re-id is\nchallenging due to significant changes in visual appearance of individuals in\ncameras with different pose, illumination and calibration. Globally the\nchallenge arises from the need to maintain structurally consistent matches\namong all the individual entities across different camera views. We propose\nPRISM, a structured matching method to jointly account for these challenges. We\nview the global problem as a weighted graph matching problem and estimate edge\nweights by learning to predict them based on the co-occurrences of visual\npatterns in the training examples. These co-occurrence based scores in turn\naccount for appearance changes by inferring likely and unlikely visual\nco-occurrences appearing in training instances. We implement PRISM on single\nshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in\nterms of matching rate while being computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 20:07:27 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:02:26 GMT"}, {"version": "v3", "created": "Tue, 22 Jul 2014 15:04:40 GMT"}, {"version": "v4", "created": "Fri, 8 May 2015 01:55:13 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1406.4465", "submitter": "Yilun Wang", "authors": "Yaru Fan and Yilun Wang", "title": "Multi-stage Multi-task feature learning via adaptive threshold", "comments": "13 pages,12 figures. arXiv admin note: text overlap with\n  arXiv:1210.5806 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task feature learning aims to identity the shared features among tasks\nto improve generalization. It has been shown that by minimizing non-convex\nlearning models, a better solution than the convex alternatives can be\nobtained. Therefore, a non-convex model based on the capped-$\\ell_{1},\\ell_{1}$\nregularization was proposed in \\cite{Gong2013}, and a corresponding efficient\nmulti-stage multi-task feature learning algorithm (MSMTFL) was presented.\nHowever, this algorithm harnesses a prescribed fixed threshold in the\ndefinition of the capped-$\\ell_{1},\\ell_{1}$ regularization and the lack of\nadaptivity might result in suboptimal performance. In this paper we propose to\nemploy an adaptive threshold in the capped-$\\ell_{1},\\ell_{1}$ regularized\nformulation, where the corresponding variant of MSMTFL will incorporate an\nadditional component to adaptively determine the threshold value. This variant\nis expected to achieve a better feature selection performance over the original\nMSMTFL algorithm. In particular, the embedded adaptive threshold component\ncomes from our previously proposed iterative support detection (ISD) method\n\\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets\ndemonstrate the effectiveness of this new variant over the original MSMTFL.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 12:47:37 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 19:47:37 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Fan", "Yaru", ""], ["Wang", "Yilun", ""]]}, {"id": "1406.4484", "submitter": "Erik Cuevas", "authors": "Erik Cuevas", "title": "Block matching algorithm based on Harmony Search optimization for motion\n  estimation", "comments": "25 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1405.4721", "journal-ref": "Applied Intelligence, 39 (1), (2013), pp. 165-183", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion estimation is one of the major problems in developing video coding\napplications. Among all motion estimation approaches, Block-matching (BM)\nalgorithms are the most popular methods due to their effectiveness and\nsimplicity for both software and hardware implementations. A BM approach\nassumes that the movement of pixels within a defined region of the current\nframe can be modeled as a translation of pixels contained in the previous\nframe. In this procedure, the motion vector is obtained by minimizing a certain\nmatching metric that is produced for the current frame over a determined search\nwindow from the previous frame. Unfortunately, the evaluation of such matching\nmeasurement is computationally expensive and represents the most consuming\noperation in the BM process. Therefore, BM motion estimation can be viewed as\nan optimization problem whose goal is to find the best-matching block within a\nsearch space. The simplest available BM method is the Full Search Algorithm\n(FSA) which finds the most accurate motion vector through an exhaustive\ncomputation of all the elements of the search space. Recently, several fast BM\nalgorithms have been proposed to reduce the search positions by calculating\nonly a fixed subset of motion vectors despite lowering its accuracy. On the\nother hand, the Harmony Search (HS) algorithm is a population-based\noptimization method that is inspired by the music improvisation process in\nwhich a musician searches for harmony and continues to polish the pitches to\nobtain a better harmony. In this paper, a new BM algorithm that combines HS\nwith a fitness approximation model is proposed. The approach uses motion\nvectors belonging to the search window as potential solutions. A fitness\nfunction evaluates the matching quality of each motion vector candidate.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 19:13:40 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Cuevas", "Erik", ""]]}, {"id": "1406.4729", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual\n  Recognition", "comments": "This manuscript is the accepted version for IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (TPAMI) 2015. See Changelog", "journal-ref": null, "doi": "10.1007/978-3-319-10578-9_23", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep convolutional neural networks (CNNs) require a fixed-size\n(e.g., 224x224) input image. This requirement is \"artificial\" and may reduce\nthe recognition accuracy for the images or sub-images of an arbitrary\nsize/scale. In this work, we equip the networks with another pooling strategy,\n\"spatial pyramid pooling\", to eliminate the above requirement. The new network\nstructure, called SPP-net, can generate a fixed-length representation\nregardless of image size/scale. Pyramid pooling is also robust to object\ndeformations. With these advantages, SPP-net should in general improve all\nCNN-based image classification methods. On the ImageNet 2012 dataset, we\ndemonstrate that SPP-net boosts the accuracy of a variety of CNN architectures\ndespite their different designs. On the Pascal VOC 2007 and Caltech101\ndatasets, SPP-net achieves state-of-the-art classification results using a\nsingle full-image representation and no fine-tuning.\n  The power of SPP-net is also significant in object detection. Using SPP-net,\nwe compute the feature maps from the entire image only once, and then pool\nfeatures in arbitrary regions (sub-images) to generate fixed-length\nrepresentations for training the detectors. This method avoids repeatedly\ncomputing the convolutional features. In processing test images, our method is\n24-102x faster than the R-CNN method, while achieving better or comparable\naccuracy on Pascal VOC 2007.\n  In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our\nmethods rank #2 in object detection and #3 in image classification among all 38\nteams. This manuscript also introduces the improvement made for this\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 14:24:17 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 10:28:55 GMT"}, {"version": "v3", "created": "Tue, 6 Jan 2015 07:16:54 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2015 07:33:24 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1406.4770", "submitter": "Laurence Aroquiaraj", "authors": "I. Laurence Aroquiaraj and K. Thangavel", "title": "Mass Classification Method in Mammogram Using Fuzzy K-Nearest Neighbour\n  Equality", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass classification of objects is an important area of research and\napplication in a variety of fields. In this paper, we present an efficient\ncomputer aided mass classification method in digitized mammograms using Fuzzy\nK-Nearest Neighbor Equality, which performs benign or malignant classification\non region of interest that contains mass. One of the major mammographic\ncharacteristics for mass classification is texture. Fuzzy K-Nearest Neighbor\nEquality exploits this important factor to classify the mass into benign or\nmalignant. The statistical textural features used in characterizing the masses\nare Haralick and Run length features. The main aim of the method is to increase\nthe effectiveness and efficiency of the classification process in an objective\nmanner to reduce the numbers of false positive of malignancies. In this paper\nproposes a novel Fuzzy K-Nearest Neighbor Equality algorithm for classifying\nthe marked regions into benign and malignant and 94.46 sensitivity,96.81\nspecificity and 96.52 accuracy is achieved that is very much promising compare\nto the radiologists' accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:38:46 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Aroquiaraj", "I. Laurence", ""], ["Thangavel", "K.", ""]]}, {"id": "1406.4773", "submitter": "Yi Sun", "authors": "Yi Sun, Xiaogang Wang, Xiaoou Tang", "title": "Deep Learning Face Representation by Joint Identification-Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge of face recognition is to develop effective feature\nrepresentations for reducing intra-personal variations while enlarging\ninter-personal differences. In this paper, we show that it can be well solved\nwith deep learning and using both face identification and verification signals\nas supervision. The Deep IDentification-verification features (DeepID2) are\nlearned with carefully designed deep convolutional networks. The face\nidentification task increases the inter-personal variations by drawing DeepID2\nextracted from different identities apart, while the face verification task\nreduces the intra-personal variations by pulling DeepID2 extracted from the\nsame identity together, both of which are essential to face recognition. The\nlearned DeepID2 features can be well generalized to new identities unseen in\nthe training data. On the challenging LFW dataset, 99.15% face verification\naccuracy is achieved. Compared with the best deep learning result on LFW, the\nerror rate has been significantly reduced by 67%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:42:16 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Sun", "Yi", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1406.4845", "submitter": "Diego Sebasti\\'an P\\'erez", "authors": "Diego Sebasti\\'an P\\'erez, Facundo Bromberg, Francisco Gonzalez\n  Antivilo", "title": "Computer Vision Approach for Low Cost, High Precision Measurement of\n  Grapevine Trunk Diameter in Outdoor Conditions", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trunk diameter is a variable of agricultural interest, used mainly in the\nprediction of fruit trees production. It is correlated with leaf area and\nbiomass of trees, and consequently gives a good estimate of the potential\nproduction of the plants. This work presents a low cost, high precision method\nfor the measurement of trunk diameter of grapevines based on Computer Vision\ntechniques. Several methods based on Computer Vision and other techniques are\nintroduced in the literature. These methods present different advantages for\ncrop management: they are amenable to be operated by unknowledgeable personnel,\nwith lower operational costs; they result in lower stress levels to\nknowledgeable personnel, avoiding the deterioration of the measurement quality\nover time; and they make the measurement process amenable to be embedded in\nlarger autonomous systems, allowing more measurements to be taken with\nequivalent costs. To date, all existing autonomous methods are either of low\nprecision, or have a prohibitive cost for massive agricultural adoption,\nleaving the manual Vernier caliper or tape measure as the only choice in most\nsituations. In this work we present a semi-autonomous measurement method that\nis susceptible to be fully automated, cost effective for mass adoption, and its\nprecision is competitive (with slight improvements) over the caliper manual\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 18:44:58 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 21:10:16 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["P\u00e9rez", "Diego Sebasti\u00e1n", ""], ["Bromberg", "Facundo", ""], ["Antivilo", "Francisco Gonzalez", ""]]}, {"id": "1406.4966", "submitter": "Jingdong Wang", "authors": "Chao Du, Jingdong Wang", "title": "Inner Product Similarity Search using Compositional Codes", "comments": "The approach presented in this paper (ECCV14 submission) is closely\n  related to multi-stage vector quantization and residual quantization. Thanks\n  the reviewers (CVPR14 and ECCV14) for pointing out the relationship to the\n  two algorithms. Related paper:\n  http://sites.skoltech.ru/app/data/uploads/sites/2/2013/09/CVPR14.pdf, which\n  also adopts the summation of vectors for vector approximation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the nearest neighbor search problem under inner product\nsimilarity and introduces a compact code-based approach. The idea is to\napproximate a vector using the composition of several elements selected from a\nsource dictionary and to represent this vector by a short code composed of the\nindices of the selected elements. The inner product between a query vector and\na database vector is efficiently estimated from the query vector and the short\ncode of the database vector. We show the superior performance of the proposed\ngroup $M$-selection algorithm that selects $M$ elements from $M$ source\ndictionaries for vector approximation in terms of search accuracy and\nefficiency for compact codes of the same length via theoretical and empirical\nanalysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT\nfeatures, $1M$ linear models and Netflix) demonstrate the superiority of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 07:42:05 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 02:13:56 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Du", "Chao", ""], ["Wang", "Jingdong", ""]]}, {"id": "1406.5035", "submitter": "Uriel Feige", "authors": "Uriel Feige", "title": "Why are images smooth?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a well observed phenomenon that natural images are smooth, in the sense\nthat nearby pixels tend to have similar values. We describe a mathematical\nmodel of images that makes no assumptions on the nature of the environment that\nimages depict. It only assumes that images can be taken at different scales\n(zoom levels). We provide quantitative bounds on the smoothness of a typical\nimage in our model, as a function of the number of available scales. These\nbounds can serve as a baseline against which to compare the observed smoothness\nof natural images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 13:25:47 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Feige", "Uriel", ""]]}, {"id": "1406.5074", "submitter": "Vijendra Singh", "authors": "Singh Vijendra and Pathak Shivani", "title": "Robust Outlier Detection Technique in Data Mining: A Univariate Approach", "comments": "arXiv admin note: text overlap with arXiv:1402.6859 by other authors\n  without attribution", "journal-ref": null, "doi": null, "report-no": "MT CS 2011", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Outliers are the points which are different from or inconsistent with the\nrest of the data. They can be novel, new, abnormal, unusual or noisy\ninformation. Outliers are sometimes more interesting than the majority of the\ndata. The main challenges of outlier detection with the increasing complexity,\nsize and variety of datasets, are how to catch similar outliers as a group, and\nhow to evaluate the outliers. This paper describes an approach which uses\nUnivariate outlier detection as a pre-processing step to detect the outlier and\nthen applies K-means algorithm hence to analyse the effects of the outliers on\nthe cluster analysis of dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 15:12:49 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Vijendra", "Singh", ""], ["Shivani", "Pathak", ""]]}, {"id": "1406.5095", "submitter": "Conrad Sanderson", "authors": "Vikas Reddy, Conrad Sanderson, Andres Sanin, Brian C. Lovell", "title": "MRF-based Background Initialisation for Improved Foreground Detection in\n  Cluttered Surveillance Videos", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.2465", "journal-ref": null, "doi": "10.1007/978-3-642-19318-7_43", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust foreground object segmentation via background modelling is a difficult\nproblem in cluttered environments, where obtaining a clear view of the\nbackground to model is almost impossible. In this paper, we propose a method\ncapable of robustly estimating the background and detecting regions of interest\nin such environments. In particular, we propose to extend the background\ninitialisation component of a recent patch-based foreground detection algorithm\nwith an elaborate technique based on Markov Random Fields, where the optimal\nlabelling solution is computed using iterated conditional modes. Rather than\nrelying purely on local temporal statistics, the proposed technique takes into\naccount the spatial continuity of the entire background. Experiments with\nseveral tracking algorithms on the CAVIAR dataset indicate that the proposed\nmethod leads to considerable improvements in object tracking accuracy, when\ncompared to methods based on Gaussian mixture models and feature histograms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 16:06:53 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Reddy", "Vikas", ""], ["Sanderson", "Conrad", ""], ["Sanin", "Andres", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1406.5212", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik", "title": "R-CNNs for Pose Estimation and Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present convolutional neural networks for the tasks of keypoint (pose)\nprediction and action classification of people in unconstrained images. Our\napproach involves training an R-CNN detector with loss functions depending on\nthe task being tackled. We evaluate our method on the challenging PASCAL VOC\ndataset and compare it to previous leading approaches. Our method gives\nstate-of-the-art results for keypoint and action prediction. Additionally, we\nintroduce a new dataset for action detection, the task of simultaneously\nlocalizing people and classifying their actions, and present results using our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 20:56:08 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Hariharan", "Bharath", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1406.5266", "submitter": "Yaniv Taigman", "authors": "Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf", "title": "Web-Scale Training for Face Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Scaling machine learning methods to very large datasets has attracted\nconsiderable attention in recent years, thanks to easy access to ubiquitous\nsensing and data from the web. We study face recognition and show that three\ndistinct properties have surprising effects on the transferability of deep\nconvolutional networks (CNN): (1) The bottleneck of the network serves as an\nimportant transfer learning regularizer, and (2) in contrast to the common\nwisdom, performance saturation may exist in CNN's (as the number of training\nsamples grows); we propose a solution for alleviating this by replacing the\nnaive random subsampling of the training set with a bootstrapping process.\nMoreover, (3) we find a link between the representation norm and the ability to\ndiscriminate in a target domain, which sheds lights on how such networks\nrepresent faces. Based on these discoveries, we are able to improve face\nrecognition accuracy on the widely used LFW benchmark, both in the verification\n(1:1) and identification (1:N) protocols, and directly compare, for the first\ntime, with the state of the art Commercially-Off-The-Shelf system and show a\nsizable leap in performance.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 02:51:31 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 09:18:19 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Taigman", "Yaniv", ""], ["Yang", "Ming", ""], ["Ranzato", "Marc'Aurelio", ""], ["Wolf", "Lior", ""]]}, {"id": "1406.5309", "submitter": "Michael S. Ryoo", "authors": "M. S. Ryoo (1), Thomas J. Fuchs (1), Lu Xia (2), J. K. Aggarwal (2),\n  Larry Matthies (1) ((1) JPL, (2) UT Austin)", "title": "Early Recognition of Human Activities from First-Person Videos Using\n  Onset Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a methodology for early recognition of human\nactivities from videos taken with a first-person viewpoint. Early recognition,\nwhich is also known as activity prediction, is an ability to infer an ongoing\nactivity at its early stage. We present an algorithm to perform recognition of\nactivities targeted at the camera from streaming videos, making the system to\npredict intended activities of the interacting person and avoid harmful events\nbefore they actually happen. We introduce the novel concept of 'onset' that\nefficiently summarizes pre-activity observations, and design an approach to\nconsider event history in addition to ongoing video observation for early\nfirst-person recognition of activities. We propose to represent onset using\ncascade histograms of time series gradients, and we describe a novel\nalgorithmic setup to take advantage of onset for early recognition of\nactivities. The experimental results clearly illustrate that the proposed\nconcept of onset enables better/earlier recognition of human activities from\nfirst-person videos.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 08:22:09 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 08:21:49 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Ryoo", "M. S.", "", "JPL"], ["Fuchs", "Thomas J.", "", "JPL"], ["Xia", "Lu", "", "UT Austin"], ["Aggarwal", "J. K.", "", "UT Austin"], ["Matthies", "Larry", "", "JPL"]]}, {"id": "1406.5429", "submitter": "Nikos Komodakis", "authors": "Nikos Komodakis and Jean-Christophe Pesquet", "title": "Playing with Duality: An Overview of Recent Primal-Dual Approaches for\n  Solving Large-Scale Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization methods are at the core of many problems in signal/image\nprocessing, computer vision, and machine learning. For a long time, it has been\nrecognized that looking at the dual of an optimization problem may drastically\nsimplify its solution. Deriving efficient strategies which jointly brings into\nplay the primal and the dual problems is however a more recent idea which has\ngenerated many important new contributions in the last years. These novel\ndevelopments are grounded on recent advances in convex analysis, discrete\noptimization, parallel processing, and non-smooth optimization with emphasis on\nsparsity issues. In this paper, we aim at presenting the principles of\nprimal-dual approaches, while giving an overview of numerical methods which\nhave been proposed in different contexts. We show the benefits which can be\ndrawn from primal-dual algorithms both for solving large-scale convex\noptimization problems and discrete ones, and we provide various application\nexamples to illustrate their usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 15:33:00 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 20:59:42 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Komodakis", "Nikos", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1406.5472", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Deniz Oktay, Hamed Pirsiavash, Antonio Torralba", "title": "Predicting Motivations of Actions by Leveraging Text", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human actions is a key problem in computer vision. However,\nrecognizing actions is only the first step of understanding what a person is\ndoing. In this paper, we introduce the problem of predicting why a person has\nperformed an action in images. This problem has many applications in human\nactivity understanding, such as anticipating or explaining an action. To study\nthis problem, we introduce a new dataset of people performing actions annotated\nwith likely motivations. However, the information in an image alone may not be\nsufficient to automatically solve this task. Since humans can rely on their\nlifetime of experiences to infer motivation, we propose to give computer vision\nsystems access to some of these experiences by using recently developed natural\nlanguage models to mine knowledge stored in massive amounts of text. While we\nare still far away from fully understanding motivation, our results suggest\nthat transferring knowledge from language into vision can help machines\nunderstand why people in images might be performing an action.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 18:02:02 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 03:58:15 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Vondrick", "Carl", ""], ["Oktay", "Deniz", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1406.5549", "submitter": "Piotr Doll\\'ar", "authors": "Piotr Doll\\'ar and C. Lawrence Zitnick", "title": "Fast Edge Detection Using Structured Forests", "comments": "update corresponding to acceptance to PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is a critical component of many vision systems, including\nobject detectors and image segmentation algorithms. Patches of edges exhibit\nwell-known forms of local structure, such as straight lines or T-junctions. In\nthis paper we take advantage of the structure present in local image patches to\nlearn both an accurate and computationally efficient edge detector. We\nformulate the problem of predicting local edge masks in a structured learning\nframework applied to random decision forests. Our novel approach to learning\ndecision trees robustly maps the structured labels to a discrete space on which\nstandard information gain measures may be evaluated. The result is an approach\nthat obtains realtime performance that is orders of magnitude faster than many\ncompeting state-of-the-art approaches, while also achieving state-of-the-art\nedge detection results on the BSDS500 Segmentation dataset and NYU Depth\ndataset. Finally, we show the potential of our approach as a general purpose\nedge detector by showing our learned edge models generalize well across\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 22:28:29 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 02:49:28 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Doll\u00e1r", "Piotr", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1406.5565", "submitter": "Sam  Keene", "authors": "Kenneth D. Morton Jr., Peter Torrione, Leslie Collins, Sam Keene", "title": "An Open Source Pattern Recognition Toolbox for MATLAB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition and machine learning are becoming integral parts of\nalgorithms in a wide range of applications. Different algorithms and approaches\nfor machine learning include different tradeoffs between performance and\ncomputation, so during algorithm development it is often necessary to explore a\nvariety of different approaches to a given task. A toolbox with a unified\nframework across multiple pattern recognition techniques enables algorithm\ndevelopers the ability to rapidly evaluate different choices prior to\ndeployment. MATLAB is a widely used environment for algorithm development and\nprototyping, and although several MATLAB toolboxes for pattern recognition are\ncurrently available these are either incomplete, expensive, or restrictively\nlicensed. In this work we describe a MATLAB toolbox for pattern recognition and\nmachine learning known as the PRT (Pattern Recognition Toolbox), licensed under\nthe permissive MIT license. The PRT includes many popular techniques for data\npreprocessing, supervised learning, clustering, regression and feature\nselection, as well as a methodology for combining these components using a\nsimple, uniform syntax. The resulting algorithms can be evaluated using\ncross-validation and a variety of scoring metrics to ensure robust performance\nwhen the algorithm is deployed. This paper presents an overview of the PRT as\nwell as an example of usage on Fisher's Iris dataset.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 01:50:54 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Morton", "Kenneth D.", "Jr."], ["Torrione", "Peter", ""], ["Collins", "Leslie", ""], ["Keene", "Sam", ""]]}, {"id": "1406.5653", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh and Pavan Turaga", "title": "Interactively Test Driving an Object Detector: Estimating Performance on\n  Unlabeled Data", "comments": "Published at Winter Conference on Applications of Computer Vision,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of `test-driving' a detector, i.e.\nallowing a human user to get a quick sense of how well the detector generalizes\nto their specific requirement. To this end, we present the first system that\nestimates detector performance interactively without extensive ground truthing\nusing a human in the loop. We approach this as a problem of estimating\nproportions and show that it is possible to make accurate inferences on the\nproportion of classes or groups within a large data collection by observing\nonly $5-10\\%$ of samples from the data. In estimating the false detections (for\nprecision), the samples are chosen carefully such that the overall\ncharacteristics of the data collection are preserved. Next, inspired by its use\nin estimating disease propagation we apply pooled testing approaches to\nestimate missed detections (for recall) from the dataset. The estimates thus\nobtained are close to the ones obtained using ground truth, thus reducing the\nneed for extensive labeling which is expensive and time consuming.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 21:37:30 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Anirudh", "Rushil", ""], ["Turaga", "Pavan", ""]]}, {"id": "1406.5670", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang,\n  Xiaoou Tang, Jianxiong Xiao", "title": "3D ShapeNets: A Deep Representation for Volumetric Shapes", "comments": "to be appeared in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape is a crucial but heavily underutilized cue in today's computer\nvision systems, mostly due to the lack of a good generic shape representation.\nWith the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft\nKinect), it is becoming increasingly important to have a powerful 3D shape\nrepresentation in the loop. Apart from category recognition, recovering full 3D\nshapes from view-based 2.5D depth maps is also a critical part of visual\nunderstanding. To this end, we propose to represent a geometric 3D shape as a\nprobability distribution of binary variables on a 3D voxel grid, using a\nConvolutional Deep Belief Network. Our model, 3D ShapeNets, learns the\ndistribution of complex 3D shapes across different object categories and\narbitrary poses from raw CAD data, and discovers hierarchical compositional\npart representations automatically. It naturally supports joint object\nrecognition and shape completion from 2.5D depth maps, and it enables active\nobject recognition through view planning. To train our 3D deep learning model,\nwe construct ModelNet -- a large-scale 3D CAD model dataset. Extensive\nexperiments show that our 3D deep representation enables significant\nperformance improvement over the-state-of-the-arts in a variety of tasks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 03:31:52 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 04:59:49 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 16:46:05 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Wu", "Zhirong", ""], ["Song", "Shuran", ""], ["Khosla", "Aditya", ""], ["Yu", "Fisher", ""], ["Zhang", "Linguang", ""], ["Tang", "Xiaoou", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1406.5679", "submitter": "Andrej Karpathy", "authors": "Andrej Karpathy, Armand Joulin and Li Fei-Fei", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for bidirectional retrieval of images and sentences\nthrough a multi-modal embedding of visual and natural language data. Unlike\nprevious models that directly map images or sentences into a common embedding\nspace, our model works on a finer level and embeds fragments of images\n(objects) and fragments of sentences (typed dependency tree relations) into a\ncommon space. In addition to a ranking objective seen in previous work, this\nallows us to add a new fragment alignment objective that learns to directly\nassociate these fragments across modalities. Extensive experimental evaluation\nshows that reasoning on both the global level of images and sentences and the\nfiner level of their respective fragments significantly improves performance on\nimage-sentence retrieval tasks. Additionally, our model provides interpretable\npredictions since the inferred inter-modal fragment alignment is explicit.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 06:22:50 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Karpathy", "Andrej", ""], ["Joulin", "Armand", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1406.5710", "submitter": "Hanumantha Raju MC", "authors": "M. C Hanumantharaju, M. Ravishankar and D. R Rameshbabu", "title": "Natural Color Image Enhancement based on Modified Multiscale Retinex\n  Algorithm and Performance Evaluation usingWavelet Energy", "comments": "10 pages, 3 figures, Recent Advances in Intelligent Informatics\n  Advances in Intelligent Systems and Computing Volume 235, 2014, pp 83-92", "journal-ref": null, "doi": "10.1007/978-3-319-01778-5_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new color image enhancement technique based on modified\nMultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are\nevaluated using a new metric, namely, wavelet energy. The color image\nenhancement is achieved by down sampling the value component of HSV color space\nconverted image into three scales (normal, medium and fine) following the\ncontrast stretching operation. These down sampled value components are enhanced\nusing the MSR algorithm. The value component is reconstructed by averaging each\npixels of the lower scale image with that of the upper scale image subsequent\nto up sampling the lower scale image. This process replaces dark pixel by the\naverage pixels of both the lower scale and upper scale, while retaining the\nbright pixels. The quality of the reconstructed images in the proposed method\nis found to be good and far better then the other researchers method. The\nperformance of the proposed scheme is evaluated using new wavelet domain based\nassessment criterion, referred as wavelet energy. This scheme computes the\nenergy of both original and enhanced image in wavelet domain. The number of\nedge details as well as wavelet energy is less in a poor quality image compared\nwith naturally enhanced image. Experimental results presented confirms that the\nproposed wavelet energy based color image quality assessment technique\nefficiently characterizes both the local and global details of enhanced image.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 11:56:21 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Hanumantharaju", "M. C", ""], ["Ravishankar", "M.", ""], ["Rameshbabu", "D. R", ""]]}, {"id": "1406.5726", "submitter": "Yunchao Wei", "authors": "Yunchao Wei, Wei Xia, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao,\n  Shuicheng Yan", "title": "CNN: Single-label to Multi-label", "comments": "13 pages, 10 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2491929", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) has demonstrated promising performance in\nsingle-label image classification tasks. However, how CNN best copes with\nmulti-label images still remains an open problem, mainly due to the complex\nunderlying object layouts and insufficient multi-label training images. In this\nwork, we propose a flexible deep CNN infrastructure, called\nHypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment\nhypotheses are taken as the inputs, then a shared CNN is connected with each\nhypothesis, and finally the CNN output results from different hypotheses are\naggregated with max pooling to produce the ultimate multi-label predictions.\nSome unique characteristics of this flexible deep CNN infrastructure include:\n1) no ground truth bounding box information is required for training; 2) the\nwhole HCP infrastructure is robust to possibly noisy and/or redundant\nhypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may\nbe well pre-trained with a large-scale single-label image dataset, e.g.\nImageNet; and 5) it may naturally output multi-label prediction results.\nExperimental results on Pascal VOC2007 and VOC2012 multi-label image datasets\nwell demonstrate the superiority of the proposed HCP infrastructure over other\nstate-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3%\nafter the fusion with our complementary result in [47] based on hand-crafted\nfeatures on the VOC2012 dataset, which significantly outperforms the\nstate-of-the-arts with a large margin of more than 7%.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 14:03:07 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 03:32:46 GMT"}, {"version": "v3", "created": "Wed, 9 Jul 2014 11:26:56 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Wei", "Yunchao", ""], ["Xia", "Wei", ""], ["Huang", "Junshi", ""], ["Ni", "Bingbing", ""], ["Dong", "Jian", ""], ["Zhao", "Yao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1406.5774", "submitter": "Hossein Azizpour", "authors": "Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto\n  Maki, Stefan Carlsson", "title": "Factors of Transferability for a Generic ConvNet Representation", "comments": "Extended version of the workshop paper with more experiments and\n  updated text and title. Original CVPR15 DeepVision workshop paper title:\n  \"From Generic to Specific Deep Representations for Visual Recognition\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence is mounting that Convolutional Networks (ConvNets) are the most\neffective representation learning method for visual recognition tasks. In the\ncommon scenario, a ConvNet is trained on a large labeled dataset (source) and\nthe feed-forward units activation of the trained network, at a certain layer of\nthe network, is used as a generic representation of an input image for a task\nwith relatively smaller training set (target). Recent studies have shown this\nform of representation transfer to be suitable for a wide range of target\nvisual recognition tasks. This paper introduces and investigates several\nfactors affecting the transferability of such representations. It includes\nparameters for training of the source ConvNet such as its architecture,\ndistribution of the training data, etc. and also the parameters of feature\nextraction such as layer of the trained ConvNet, dimensionality reduction, etc.\nThen, by optimizing these factors, we show that significant improvements can be\nachieved on various (17) visual recognition tasks. We further show that these\nvisual recognition tasks can be categorically ordered based on their distance\nfrom the source task such that a correlation between the performance of tasks\nand their distance from the source task w.r.t. the proposed factors is\nobserved.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 21:57:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 15:37:50 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2015 10:02:19 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Azizpour", "Hossein", ""], ["Razavian", "Ali Sharif", ""], ["Sullivan", "Josephine", ""], ["Maki", "Atsuto", ""], ["Carlsson", "Stefan", ""]]}, {"id": "1406.5807", "submitter": "Peilei Liu", "authors": "Peilei Liu and Ting Wang", "title": "A Unified Quantitative Model of Vision and Audition", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have put forwards a unified quantitative framework of vision and audition,\nbased on existing data and theories. According to this model, the retina is a\nfeedforward network self-adaptive to inputs in a specific period. After fully\ngrown, cells become specialized detectors based on statistics of stimulus\nhistory. This model has provided explanations for perception mechanisms of\ncolour, shape, depth and motion. Moreover, based on this ground we have put\nforwards a bold conjecture that single ear can detect sound direction. This is\ncomplementary to existing theories and has provided better explanations for\nsound localization.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 05:00:31 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Liu", "Peilei", ""], ["Wang", "Ting", ""]]}, {"id": "1406.5824", "submitter": "Serena Yeung", "authors": "Serena Yeung, Alireza Fathi, and Li Fei-Fei", "title": "VideoSET: Video Summary Evaluation through Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present VideoSET, a method for Video Summary Evaluation\nthrough Text that can evaluate how well a video summary is able to retain the\nsemantic information contained in its original video. We observe that semantics\nis most easily expressed in words, and develop a text-based approach for the\nevaluation. Given a video summary, a text representation of the video summary\nis first generated, and an NLP-based metric is then used to measure its\nsemantic distance to ground-truth text summaries written by humans. We show\nthat our technique has higher agreement with human judgment than pixel-based\ndistance metrics. We also release text annotations and ground-truth text\nsummaries for a number of publicly available video datasets, for use by the\ncomputer vision community.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 07:56:23 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Yeung", "Serena", ""], ["Fathi", "Alireza", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1406.5910", "submitter": "Roman Shapovalov", "authors": "Roman Shapovalov, Dmitry Vetrov, Anton Osokin, Pushmeet Kohli", "title": "Multi-utility Learning: Structured-output Learning with Multiple\n  Annotation-specific Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured-output learning is a challenging problem; particularly so because\nof the difficulty in obtaining large datasets of fully labelled instances for\ntraining. In this paper we try to overcome this difficulty by presenting a\nmulti-utility learning framework for structured prediction that can learn from\ntraining instances with different forms of supervision. We propose a unified\ntechnique for inferring the loss functions most suitable for quantifying the\nconsistency of solutions with the given weak annotation. We demonstrate the\neffectiveness of our framework on the challenging semantic image segmentation\nproblem for which a wide variety of annotations can be used. For instance, the\npopular training datasets for semantic segmentation are composed of images with\nhard-to-generate full pixel labellings, as well as images with easy-to-obtain\nweak annotations, such as bounding boxes around objects, or image-level labels\nthat specify which object categories are present in an image. Experimental\nevaluation shows that the use of annotation-specific loss functions\ndramatically improves segmentation accuracy compared to the baseline system\nwhere only one type of weak annotation is used.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 14:06:24 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Shapovalov", "Roman", ""], ["Vetrov", "Dmitry", ""], ["Osokin", "Anton", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1406.5947", "submitter": "Thomas Martinetz", "authors": "Bogdan Miclut, Thomas Kaester, Thomas Martinetz, Erhardt Barth", "title": "Committees of deep feedforward networks trained with few data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are known to give good results on image\nclassification tasks. In this paper we present a method to improve the\nclassification result by combining multiple such networks in a committee. We\nadopt the STL-10 dataset which has very few training examples and show that our\nmethod can achieve results that are better than the state of the art. The\nnetworks are trained layer-wise and no backpropagation is used. We also explore\nthe effects of dataset augmentation by mirroring, rotation, and scaling.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 15:34:54 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Miclut", "Bogdan", ""], ["Kaester", "Thomas", ""], ["Martinetz", "Thomas", ""], ["Barth", "Erhardt", ""]]}, {"id": "1406.6140", "submitter": "Sadanand Kulkarni A", "authors": "Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L.\n  Yannawar", "title": "Offline Handwritten MODI Character Recognition Using HU, Zernike Moments\n  and Zoning", "comments": "This paper has been withdrawn by the author due to the paper was\n  rejected by journal with a reson \"paper was not suitable for the journal\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HOCR is abbreviated as Handwritten Optical Character Recognition. HOCR is a\nprocess of recognition of different handwritten characters from a digital image\nof documents. Handwritten automatic character recognition has attracted many\nresearchers all over the world to contribute handwritten character recognition\ndomain. Shape identification and feature extraction is very important part of\nany character recognition system and success of method is highly dependent on\nselection of features. However feature extraction is the most important step in\ndefining the shape of the character as precisely and as uniquely as possible.\nThis is indeed the most important step and complex task as well and achieved\nsuccess by using invariance property, irrespective of position and orientation.\nZernike moments describes shape, identify rotation invariant due to its\nOrthogonality property. MODI is an ancient script of India had cursive and\ncomplex representation of characters. The work described in this paper presents\nefficiency of Zernike moments over Hu 7 moment with zoning for automatic\nrecognition of handwritten MODI characters. Offline approach is used in this\npaper because MODI Script was very popular and widely used for writing purpose\ntill 19th century before Devanagari was officially adopted.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 05:40:43 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 04:49:04 GMT"}, {"version": "v3", "created": "Thu, 3 Jul 2014 15:09:24 GMT"}, {"version": "v4", "created": "Mon, 27 Oct 2014 13:17:41 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Kulkarni", "Sadanand A.", ""], ["Borde", "Prashant L.", ""], ["Manza", "Ramesh R.", ""], ["Yannawar", "Pravin L.", ""]]}, {"id": "1406.6145", "submitter": "Tyler Maunu", "authors": "Gilad Lerman and Tyler Maunu", "title": "Fast, Robust and Non-convex Subspace Recovery", "comments": null, "journal-ref": "Information and Inference: A Journal of the IMA 7 (2018) 277-336", "doi": "10.1093/imaiai/iax012", "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a fast and non-convex algorithm for robust subspace\nrecovery. The data sets considered include inliers drawn around a\nlow-dimensional subspace of a higher dimensional ambient space, and a possibly\nlarge portion of outliers that do not lie nearby this subspace. The proposed\nalgorithm, which we refer to as Fast Median Subspace (FMS), is designed to\nrobustly determine the underlying subspace of such data sets, while having\nlower computational complexity than existing methods. We prove convergence of\nthe FMS iterates to a stationary point. Further, under a special model of data,\nFMS converges to a point which is near to the global minimum with overwhelming\nprobability. Under this model, we show that the iteration complexity is\nglobally bounded and locally $r$-linear. The latter theorem holds for any fixed\nfraction of outliers (less than 1) and any fixed positive distance between the\nlimit point and the global minimum. Numerical experiments on synthetic and real\ndata demonstrate its competitive speed and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 06:15:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 22:58:10 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lerman", "Gilad", ""], ["Maunu", "Tyler", ""]]}, {"id": "1406.6147", "submitter": "Gabriela Csurka", "authors": "Neda Salamati and Diane Larlus and Gabriela Csurka and Sabine\n  S\\\"usstrunk", "title": "Incorporating Near-Infrared Information into Semantic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in computational photography has shown that we can acquire\nnear-infrared (NIR) information in addition to the normal visible (RGB) band,\nwith only slight modifications to standard digital cameras. Due to the\nproximity of the NIR band to visible radiation, NIR images share many\nproperties with visible images. However, as a result of the material dependent\nreflection in the NIR part of the spectrum, such images reveal different\ncharacteristics of the scene. We investigate how to effectively exploit these\ndifferences to improve performance on the semantic image segmentation task.\nBased on a state-of-the-art segmentation framework and a novel manually\nsegmented image database (both indoor and outdoor scenes) that contain\n4-channel images (RGB+NIR), we study how to best incorporate the specific\ncharacteristics of the NIR response. We show that adding NIR leads to improved\nperformance for classes that correspond to a specific type of material in both\noutdoor and indoor scenes. We also discuss the results with respect to the\nphysical properties of the NIR response.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 06:28:50 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Salamati", "Neda", ""], ["Larlus", "Diane", ""], ["Csurka", "Gabriela", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1406.6201", "submitter": "Reiner Lenz", "authors": "Reiner Lenz", "title": "Saccadic Eye Movements and the Generalized Pareto Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a statistical analysis of the eye tracker measurements in a\ndatabase with 15 observers viewing 1003 images under free-viewing conditions.\nIn contrast to the common approach of investigating the properties of the\nfixation points we analyze the properties of the transition phases between\nfixations. We introduce hyperbolic geometry as a tool to measure the step\nlength between consecutive eye positions. We show that the step lengths,\nmeasured in hyperbolic and euclidean geometry, follow a generalized Pareto\ndistribution. The results based on the hyperbolic distance are more robust than\nthose based on euclidean geometry. We show how the structure of the space of\ngeneralized Pareto distributions can be used to characterize and identify\nindividual observers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 10:57:50 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Lenz", "Reiner", ""]]}, {"id": "1406.6247", "submitter": "Volodymyr Mnih", "authors": "Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu", "title": "Recurrent Models of Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying convolutional neural networks to large images is computationally\nexpensive because the amount of computation scales linearly with the number of\nimage pixels. We present a novel recurrent neural network model that is capable\nof extracting information from an image or video by adaptively selecting a\nsequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it\nperforms can be controlled independently of the input image size. While the\nmodel is non-differentiable, it can be trained using reinforcement learning\nmethods to learn task-specific policies. We evaluate our model on several image\nclassification tasks, where it significantly outperforms a convolutional neural\nnetwork baseline on cluttered images, and on a dynamic visual control problem,\nwhere it learns to track a simple object without an explicit training signal\nfor doing so.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 14:16:56 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Mnih", "Volodymyr", ""], ["Heess", "Nicolas", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1406.6273", "submitter": "Julian Habigt", "authors": "Julian Habigt, Klaus Diepold", "title": "Image Completion for View Synthesis Using Markov Random Fields and\n  Efficient Belief Propagation", "comments": "Published version:\n  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6738439", "journal-ref": "Proc. 20th IEEE International Conference on Image Processing\n  (2013) 2131-2134", "doi": "10.1109/ICIP.2013.6738439", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View synthesis is a process for generating novel views from a scene which has\nbeen recorded with a 3-D camera setup. It has important applications in 3-D\npost-production and 2-D to 3-D conversion. However, a central problem in the\ngeneration of novel views lies in the handling of disocclusions. Background\ncontent, which was occluded in the original view, may become unveiled in the\nsynthesized view. This leads to missing information in the generated view which\nhas to be filled in a visually plausible manner. We present an inpainting\nalgorithm for disocclusion filling in synthesized views based on Markov random\nfields and efficient belief propagation. We compare the result to two\nstate-of-the-art algorithms and demonstrate a significant improvement in image\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 15:12:55 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Habigt", "Julian", ""], ["Diepold", "Klaus", ""]]}, {"id": "1406.6314", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Further heuristics for $k$-means: The merge-and-split heuristic and the\n  $(k,l)$-means", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the optimal $k$-means clustering is NP-hard in general and many\nheuristics have been designed for minimizing monotonically the $k$-means\nobjective. We first show how to extend Lloyd's batched relocation heuristic and\nHartigan's single-point relocation heuristic to take into account empty-cluster\nand single-point cluster events, respectively. Those events tend to\nincreasingly occur when $k$ or $d$ increases, or when performing several\nrestarts. First, we show that those special events are a blessing because they\nallow to partially re-seed some cluster centers while further minimizing the\n$k$-means objective function. Second, we describe a novel heuristic,\nmerge-and-split $k$-means, that consists in merging two clusters and splitting\nthis merged cluster again with two new centers provided it improves the\n$k$-means objective. This novel heuristic can improve Hartigan's $k$-means when\nit has converged to a local minimum. We show empirically that this\nmerge-and-split $k$-means improves over the Hartigan's heuristic which is the\n{\\em de facto} method of choice. Finally, we propose the $(k,l)$-means\nobjective that generalizes the $k$-means objective by associating the data\npoints to their $l$ closest cluster centers, and show how to either directly\nconvert or iteratively relax the $(k,l)$-means into a $k$-means in order to\nreach better local minima.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:34:34 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1406.6323", "submitter": "Tal Hassner", "authors": "Moria Tau and Tal Hassner", "title": "Dense Correspondences Across Scenes and Scales", "comments": "A longer version of this paper is in submission. Please see author\n  homepage for an updated version", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2474356", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek a practical method for establishing dense correspondences between two\nimages with similar content, but possibly different 3D scenes. One of the\nchallenges in designing such a system is the local scale differences of objects\nappearing in the two images. Previous methods often considered only small\nsubsets of image pixels; matching only pixels for which stable scales may be\nreliably estimated. More recently, others have considered dense\ncorrespondences, but with substantial costs associated with generating, storing\nand matching scale invariant descriptors. Our work here is motivated by the\nobservation that pixels in the image have contexts -- the pixels around them --\nwhich may be exploited in order to estimate local scales reliably and\nrepeatably. Specifically, we make the following contributions. (i) We show that\nscales estimated in sparse interest points may be propagated to neighboring\npixels where this information cannot be reliably determined. Doing so allows\nscale invariant descriptors to be extracted anywhere in the image, not just in\ndetected interest points. (ii) We present three different means for propagating\nthis information: using only the scales at detected interest points, using the\nunderlying image information to guide the propagation of this information\nacross each image, separately, and using both images simultaneously. Finally,\n(iii), we provide extensive results, both qualitative and quantitative,\ndemonstrating that accurate dense correspondences can be obtained even between\nvery different images, with little computational costs beyond those required by\nexisting methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 17:49:09 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Tau", "Moria", ""], ["Hassner", "Tal", ""]]}, {"id": "1406.6336", "submitter": "Erik Cuevas", "authors": "Diego Oliva, Erik Cuevas, Gonzalo Pajares, Daniel Zaldivar, Valentin\n  Osuna", "title": "A multilevel thresholding algorithm using Electromagnetism Optimization", "comments": "The figures have been shortened in order to fulfill the submission\n  requirements", "journal-ref": "Neurocomputing, 139, (2014), 357-381", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is one of the most important tasks in image processing. It\nconsist in classify the pixels into two or more groups depending on their\nintensity levels and a threshold value. The quality of the segmentation depends\non the method applied to select the threshold. The use of the classical\nimplementations for multilevel thresholding is computationally expensive since\nthey exhaustively search the best values to optimize the objective function.\nUnder such conditions, the use of optimization evolutionary approaches has been\nextended. The Electromagnetism Like algorithm (EMO) is an evolutionary method\nwhich mimics the attraction repulsion mechanism among charges to evolve the\nmembers of a population. Different to other algorithms, EMO exhibits\ninteresting search capabilities whereas maintains a low computational overhead.\nIn this paper, a multilevel thresholding (MT) algorithm based on the EMO is\nintroduced. The approach combines the good search capabilities of EMO algorithm\nwith objective functions proposed by the popular MT methods of Otsu and Kapur.\nThe algorithm takes random samples from a feasible search space inside the\nimage histogram. Such samples build each particle in the EMO context whereas\nits quality is evaluated considering the objective that is function employed by\nthe Otsu or Kapur method. Guided by these objective values the set of candidate\nsolutions are evolved through the EMO operators until an optimal solution is\nfound. The approach generates a multilevel segmentation algorithm which can\neffectively identify the threshold values of a digital image in a reduced\nnumber of iterations. Experimental results show performance evidence of the\nimplementation of EMO for digital image segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 19:01:28 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Oliva", "Diego", ""], ["Cuevas", "Erik", ""], ["Pajares", "Gonzalo", ""], ["Zaldivar", "Daniel", ""], ["Osuna", "Valentin", ""]]}, {"id": "1406.6390", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Fraser Watson, Alfred\n  O. Hero III", "title": "Image patch analysis and clustering of sunspots: a dimensionality\n  reduction approach", "comments": "5 pages, 7 figures, accepted to ICIP 2014", "journal-ref": "K.R. Moon, J.J. Li, V. Delouille, F. Watson, and A.O. Hero III,\n  \"Image patch analysis and clustering of sunspots: a dimensionality reduction\n  approach,\" In Image Processing (ICIP), 2014 IEEE Conference on, pp.\n  1623-1627, 2014", "doi": "10.1109/ICIP.2014.7025325", "report-no": null, "categories": "cs.CV astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sunspots, as seen in white light or continuum images, are associated with\nregions of high magnetic activity on the Sun, visible on magnetogram images.\nTheir complexity is correlated with explosive solar activity and so classifying\nthese active regions is useful for predicting future solar activity. Current\nclassification of sunspot groups is visually based and suffers from bias.\nSupervised learning methods can reduce human bias but fail to optimally\ncapitalize on the information present in sunspot images. This paper uses two\nimage modalities (continuum and magnetogram) to characterize the spatial and\nmodal interactions of sunspot and magnetic active region images and presents a\nnew approach to cluster the images. Specifically, in the framework of image\npatch analysis, we estimate the number of intrinsic parameters required to\ndescribe the spatial and modal dependencies, the correlation between the two\nmodalities and the corresponding spatial patterns, and examine the phenomena at\ndifferent scales within the images. To do this, we use linear and nonlinear\nintrinsic dimension estimators, canonical correlation analysis, and\nmultiresolution analysis of intrinsic dimension.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 20:48:19 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Moon", "Kevin R.", ""], ["Li", "Jimmy J.", ""], ["Delouille", "Veronique", ""], ["Watson", "Fraser", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1406.6425", "submitter": "Prasad Sudhakar", "authors": "Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc\n  Joannes", "title": "Compressive Imaging and Characterization of Sparse Light Deflection Maps", "comments": "35 pages, 17 figures. Accepted for publication in SIAM Journal on\n  Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Light rays incident on a transparent object of uniform refractive index\nundergo deflections, which uniquely characterize the surface geometry of the\nobject. Associated with each point on the surface is a deflection map (or\nspectrum) which describes the pattern of deflections in various directions.\nThis article presents a novel method to efficiently acquire and reconstruct\nsparse deflection spectra induced by smooth object surfaces. To this end, we\nleverage the framework of Compressed Sensing (CS) in a particular\nimplementation of a schlieren deflectometer, i.e., an optical system providing\nlinear measurements of deflection spectra with programmable spatial light\nmodulation patterns. We design those modulation patterns on the principle of\nspread spectrum CS for reducing the number of observations. The ability of our\ndevice to simultaneously observe the deflection spectra on a dense\ndiscretization of the object surface is related to a Multiple Measurement\nVector (MMV) model. This scheme allows us to estimate both the noise power and\nthe instrumental point spread function.\n  We formulate the spectrum reconstruction task as the solving of a linear\ninverse problem regularized by an analysis sparsity prior using a translation\ninvariant wavelet frame. Our results demonstrate the capability and advantages\nof using a CS based approach for deflectometric imaging both on simulated data\nand experimental deflectometric data.\n  Finally, the paper presents an extension of our method showing how we can\nextract the main deflection direction in each point of the object surface from\na few compressive measurements, without needing any costly reconstruction\nprocedures. This compressive characterization is then confirmed with\nexperimental results on simple plano-convex and multifocal intra-ocular lenses\nstudying the evolution of the main deflection as a function of the object point\nlocation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 00:46:10 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 18:17:21 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Sudhakar", "Prasad", ""], ["Jacques", "Laurent", ""], ["Dubois", "Xavier", ""], ["Antoine", "Philippe", ""], ["Joannes", "Luc", ""]]}, {"id": "1406.6507", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, Trevor Darrell", "title": "Weakly-supervised Discovery of Visual Pattern Configurations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prominence of weakly labeled data nurtures a growing demand\nfor object detection methods that can cope with minimal supervision. We propose\nan approach that automatically identifies discriminative configurations of\nvisual patterns that are characteristic of a given object class. We formulate\nthe problem as a constrained submodular optimization problem and demonstrate\nthe benefits of the discovered configurations in remedying mislocalizations and\nfinding informative positive and negative training examples. Together, these\nlead to state-of-the-art weakly-supervised detection results on the challenging\nPASCAL VOC dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 09:35:40 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Song", "Hyun Oh", ""], ["Lee", "Yong Jae", ""], ["Jegelka", "Stefanie", ""], ["Darrell", "Trevor", ""]]}, {"id": "1406.6538", "submitter": "Martin Kiechle", "authors": "Martin Kiechle, Tim Habigt, Simon Hawe and Martin Kleinsteuber", "title": "A Bimodal Co-Sparse Analysis Model for Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of many computer vision tasks lies in the ability to exploit the\ninterdependency between different image modalities such as intensity and depth.\nFusing corresponding information can be achieved on several levels, and one\npromising approach is the integration at a low level. Moreover, sparse signal\nmodels have successfully been used in many vision applications. Within this\narea of research, the so called co-sparse analysis model has attracted\nconsiderably less attention than its well-known counterpart, the sparse\nsynthesis model, although it has been proven to be very useful in various image\nprocessing applications. In this paper, we propose a co-sparse analysis model\nthat is able to capture the interdependency of two image modalities. It is\nbased on the assumption that a pair of analysis operators exists, so that the\nco-supports of the corresponding bimodal image structures are correlated. We\npropose an algorithm that is able to learn such a coupled pair of operators\nfrom registered and noise-free training data. Furthermore, we explain how this\nmodel can be applied to solve linear inverse problems in image processing and\nhow it can be used for image registration tasks. This paper extends the work of\nsome of the authors by two major contributions. Firstly, a modification of the\nlearning process is proposed that a priori guarantees unit norm and zero-mean\nof the rows of the operator. This accounts for the intuition that contrast in\nimage modalities carries the most information. Secondly, the model is used in a\nnovel bimodal image registration algorithm which estimates the transformation\nparameters of unregistered images of different modalities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 12:28:55 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Kiechle", "Martin", ""], ["Habigt", "Tim", ""], ["Hawe", "Simon", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1406.6558", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Victor Lempitsky", "title": "$ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image\n  Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture for difficult image processing operations, such\nas natural edge detection or thin object segmentation. The architecture is\nbased on a simple combination of convolutional neural networks with the nearest\nneighbor search.\n  We focus our attention on the situations when the desired image\ntransformation is too hard for a neural network to learn explicitly. We show\nthat in such situations, the use of the nearest neighbor search on top of the\nnetwork output allows to improve the results considerably and to account for\nthe underfitting effect during the neural network training. The approach is\nvalidated on three challenging benchmarks, where the performance of the\nproposed architecture matches or exceeds the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:10:56 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 08:07:52 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1406.6560", "submitter": "Erik Cuevas", "authors": "Erik Cuevas, Felipe Sencion-Echauri, Daniel Zaldivar and Marco Perez\n  Cisneros", "title": "Multi Circle Detection on Images Using Artificial Bee Colony (ABC)\n  Optimization", "comments": "19 Pages", "journal-ref": "Soft Computing, 16 (2), (2012), pp. 281-296", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hough transform (HT) has been the most common method for circle detection,\nexhibiting robustness, but adversely demanding considerable computational\neffort and large memory requirements. Alternative approaches include heuristic\nmethods that employ iterative optimization procedures for detecting multiple\ncircles. Since only one circle can be marked at each optimization cycle,\nmultiple executions must be enforced in order to achieve multi detection. This\npaper presents an algorithm for automatic detection of multiple circular shapes\nthat considers the overall process as a multi-modal optimization problem. The\napproach is based on the artificial bee colony (ABC) algorithm, a swarm\noptimization algorithm inspired by the intelligent foraging behavior of honey\nbees. Unlike the original ABC algorithm, the proposed approach presents the\naddition of a memory for discarded solutions. Such memory allows holding\nimportant information regarding other local optima which might have emerged\nduring the optimization process. The detector uses a combination of three\nnon-collinear edge points as parameters to determine circle candidates. A\nmatching function (nectar- amount) determines if such circle candidates\n(bee-food-sources) are actually present in the image. Guided by the values of\nsuch matching functions, the set of encoded candidate circles are evolved\nthrough the ABC algorithm so that the best candidate (global optimum) can be\nfitted into an actual circle within the edge only image. Then, an analysis of\nthe incorporated memory is executed in order to identify potential local\noptima, i.e., other circles.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:15:08 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Cuevas", "Erik", ""], ["Sencion-Echauri", "Felipe", ""], ["Zaldivar", "Daniel", ""], ["Cisneros", "Marco Perez", ""]]}, {"id": "1406.6568", "submitter": "Victor Miller", "authors": "V. A. Miller, S. Erlien, J. Piersol", "title": "Support vector machine classification of dimensionally reduced\n  structural MRI images for dementia", "comments": "technical note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify very-mild to moderate dementia in patients (CDR ranging from 0 to\n2) using a support vector machine classifier acting on dimensionally reduced\nfeature set derived from MRI brain scans of the 416 subjects available in the\nOASIS-Brains dataset. We use image segmentation and principal component\nanalysis to reduce the dimensionality of the data. Our resulting feature set\ncontains 11 features for each subject. Performance of the classifiers is\nevaluated using 10-fold cross-validation. Using linear and (gaussian) kernels,\nwe obtain a training classification accuracy of 86.4% (90.1%), test accuracy of\n85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%),\nand test Matthews correlation coefficient of 0.594 (0.616).\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:50:18 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Miller", "V. A.", ""], ["Erlien", "S.", ""], ["Piersol", "J.", ""]]}, {"id": "1406.6595", "submitter": "Charalambos Poullis", "authors": "Qing Gu, Kyriakos Herakleous, Charalambos Poullis", "title": "3DUNDERWORLD-SLS: An Open-Source Structured-Light Scanning System for\n  Rapid Geometry Acquisition", "comments": "30 pages describing the 3DUNDERWORLD-SLS open source software by the\n  ICT lab (www.theICTlab.org)", "journal-ref": null, "doi": null, "report-no": "ICT-TR-2016-01", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increase in the demand of virtual 3D objects\nrepresenting real-life objects. A plethora of methods and systems have already\nbeen proposed for the acquisition of the geometry of real-life objects ranging\nfrom those which employ active sensor technology, passive sensor technology or\na combination of various techniques.\n  In this paper we present the development of a 3D scanning system which is\nbased on the principle of structured-light, without having particular\nrequirements for specialized equipment. We discuss the intrinsic details and\ninherent difficulties of structured-light scanning techniques and present our\nsolutions. Finally, we introduce our open-source scanning software system\n\"3DUNDERWORLD-SLS\" which implements the proposed techniques both in CPU and\nGPU. We have performed extensive testing with a wide range of models and report\nthe results. Furthermore, we present a comprehensive evaluation of the system\nand a comparison with a high-end commercial 3D scanner.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 14:51:21 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 04:59:55 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Gu", "Qing", ""], ["Herakleous", "Kyriakos", ""], ["Poullis", "Charalambos", ""]]}, {"id": "1406.6811", "submitter": "Fumin Shen", "authors": "Fumin Shen, Chunhua Shen and Heng Tao Shen", "title": "Face Image Classification by Pooling Raw Features", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a very simple, efficient yet surprisingly effective feature\nextraction method for face recognition (about 20 lines of Matlab code), which\nis mainly inspired by spatial pyramid pooling in generic image classification.\nWe show that features formed by simply pooling local patches over a multi-level\npyramid, coupled with a linear classifier, can significantly outperform most\nrecent face recognition methods. The simplicity of our feature extraction\nprocedure is demonstrated by the fact that no learning is involved (except PCA\nwhitening). We show that, multi-level spatial pooling and dense extraction of\nmulti-scale patches play critical roles in face image classification. The\nextracted facial features can capture strong structural information of\nindividual faces with no label information being used. We also find that,\npre-processing on local image patches such as contrast normalization can have\nan important impact on the classification accuracy. In particular, on the\nchallenging face recognition datasets of FERET and LFW-a, our method improves\nprevious best results by more than 10% and 20%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 08:56:55 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 06:40:29 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1406.6818", "submitter": "Fumin Shen", "authors": "Fumin Shen, Chunhua Shen and Heng Tao Shen", "title": "Face Identification with Second-Order Pooling", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic face recognition has received significant performance improvement\nby developing specialised facial image representations. On the other hand,\ngeneric object recognition has rarely been applied to the face recognition.\nSpatial pyramid pooling of features encoded by an over-complete dictionary has\nbeen the key component of many state-of-the-art image classification systems.\nInspired by its success, in this work we develop a new face image\nrepresentation method inspired by the second-order pooling in Carreira et al.\n[1], which was originally proposed for image segmentation. The proposed method\ndiffers from the previous methods in that, we encode the densely extracted\nlocal patches by a small-size dictionary; and the facial image signatures are\nobtained by pooling the second-order statistics of the encoded features. We\nshow the importance of pooling on encoded features, which is bypassed by the\noriginal second-order pooling method to avoid the high computational cost.\nEquipped with a simple linear classifier, the proposed method outperforms the\nstate-of-the-art face identification performance by large margins. For example,\non the LFW databases, the proposed method performs better than the previous\nbest by around 13% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 09:21:42 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 06:41:40 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1406.6854", "submitter": "Jinwei Xu", "authors": "Jinwei Xu, Jiankun Hu, Xiuping Jia", "title": "A Fully Automated Latent Fingerprint Matcher with Embedded Self-learning\n  Segmentation Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent fingerprint has the practical value to identify the suspects who have\nunintentionally left a trace of fingerprint in the crime scenes. However,\ndesigning a fully automated latent fingerprint matcher is a very challenging\ntask as it needs to address many challenging issues including the separation of\noverlapping structured patterns over the partial and poor quality latent\nfingerprint image, and finding a match against a large background database that\nwould have different resolutions. Currently there is no fully automated latent\nfingerprint matcher available to the public and most literature reports have\nutilized a specialized latent fingerprint matcher COTS3 which is not accessible\nto the public. This will make it infeasible to assess and compare the relevant\nresearch work which is vital for this research community. In this study, we\ntarget to develop a fully automated latent matcher for adaptive detection of\nthe region of interest and robust matching of latent prints. Unlike the\nmanually conducted matching procedure, the proposed latent matcher can run like\na sealed black box without any manual intervention. This matcher consists of\nthe following two modules: (i) the dictionary learning-based region of interest\n(ROI) segmentation scheme; and (ii) the genetic algorithm-based minutiae set\nmatching unit. Experimental results on NIST SD27 latent fingerprint database\ndemonstrates that the proposed matcher outperforms the currently public\nstate-of-art latent fingerprint matcher.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 11:51:56 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Xu", "Jinwei", ""], ["Hu", "Jiankun", ""], ["Jia", "Xiuping", ""]]}, {"id": "1406.6909", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin\n  Riedmiller and Thomas Brox", "title": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional\n  Neural Networks", "comments": "PAMI submission. Includes matching experiments as in\n  arXiv:1405.5769v1. Also includes new network architectures, experiments on\n  Caltech-256, experiment on combining Exemplar-CNN with clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have proven to be very successful in learning\ntask specific features that allow for unprecedented performance on various\ncomputer vision tasks. Training of such networks follows mostly the supervised\nlearning paradigm, where sufficiently many input-output pairs are required for\ntraining. Acquisition of large training sets is one of the key challenges, when\napproaching a new task. In this paper, we aim for generic feature learning and\npresent an approach for training a convolutional network using only unlabeled\ndata. To this end, we train the network to discriminate between a set of\nsurrogate classes. Each surrogate class is formed by applying a variety of\ntransformations to a randomly sampled 'seed' image patch. In contrast to\nsupervised network training, the resulting feature representation is not class\nspecific. It rather provides robustness to the transformations that have been\napplied during training. This generic feature representation allows for\nclassification results that outperform the state of the art for unsupervised\nlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,\nCaltech-256). While such generic features cannot compete with class specific\nfeatures from supervised training on a classification task, we show that they\nare advantageous on geometric matching problems, where they also outperform the\nSIFT descriptor.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 15:07:14 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 11:43:36 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Fischer", "Philipp", ""], ["Springenberg", "Jost Tobias", ""], ["Riedmiller", "Martin", ""], ["Brox", "Thomas", ""]]}, {"id": "1406.6946", "submitter": "Erik Cuevas", "authors": "Erik Cuevas, Margarita Diaz, Miguel Manzanares, Daniel Zaldivar and\n  Marco Perez", "title": "An improved computer vision method for detecting white blood cells", "comments": "20 Pages. arXiv admin note: text overlap with arXiv:1405.5164", "journal-ref": "Computational and Mathematical Methods in Medicine, 2013, art. no.\n  137392", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of White Blood Cells (WBC) still remains as an\nunsolved issue in medical imaging. The analysis of WBC images has engaged\nresearchers from fields of medicine and computer vision alike. Since WBC can be\napproximated by an ellipsoid form, an ellipse detector algorithm may be\nsuccessfully applied in order to recognize them. This paper presents an\nalgorithm for the automatic detection of WBC embedded into complicated and\ncluttered smear images that considers the complete process as a multi-ellipse\ndetection problem. The approach, based on the Differential Evolution (DE)\nalgorithm, transforms the detection task into an optimization problem where\nindividuals emulate candidate ellipses. An objective function evaluates if such\ncandidate ellipses are really present in the edge image of the smear. Guided by\nthe values of such function, the set of encoded candidate ellipses\n(individuals) are evolved using the DE algorithm so that they can fit into the\nWBC enclosed within the edge-only map of the image. Experimental results from\nwhite blood cell images with a varying range of complexity are included to\nvalidate the efficiency of the proposed technique in terms of accuracy and\nrobustness.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 17:01:32 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 13:20:21 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Cuevas", "Erik", ""], ["Diaz", "Margarita", ""], ["Manzanares", "Miguel", ""], ["Zaldivar", "Daniel", ""], ["Perez", "Marco", ""]]}, {"id": "1406.6947", "submitter": "Ping Luo", "authors": "Zhenyao Zhu and Ping Luo and Xiaogang Wang and Xiaoou Tang", "title": "Deep Learning Multi-View Representation for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various factors, such as identities, views (poses), and illuminations, are\ncoupled in face images. Disentangling the identity and view representations is\na major challenge in face recognition. Existing face recognition systems either\nuse handcrafted features or learn features discriminatively to improve\nrecognition accuracy. This is different from the behavior of human brain.\nIntriguingly, even without accessing 3D data, human not only can recognize face\nidentity, but can also imagine face images of a person under different\nviewpoints given a single 2D image, making face perception in the brain robust\nto view changes. In this sense, human brain has learned and encoded 3D face\nmodels from 2D images. To take into account this instinct, this paper proposes\na novel deep neural net, named multi-view perceptron (MVP), which can untangle\nthe identity and view features, and infer a full spectrum of multi-view images\nin the meanwhile, given a single 2D face image. The identity features of MVP\nachieve superior performance on the MultiPIE dataset. MVP is also capable to\ninterpolate and predict images under viewpoints that are unobserved in the\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 17:09:25 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Zhu", "Zhenyao", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1406.6962", "submitter": "Jan Hosang", "authors": "Jan Hosang, Rodrigo Benenson, Bernt Schiele", "title": "How good are detection proposals, really?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current top performing Pascal VOC object detectors employ detection proposals\nto guide the search for objects thereby avoiding exhaustive sliding window\nsearch across images. Despite the popularity of detection proposals, it is\nunclear which trade-offs are made when using them during object detection. We\nprovide an in depth analysis of ten object proposal methods along with four\nbaselines regarding ground truth annotation recall (on Pascal VOC 2007 and\nImageNet 2013), repeatability, and impact on DPM detector performance. Our\nfindings show common weaknesses of existing methods, and provide insights to\nchoose the most adequate method for different settings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 18:00:56 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 15:11:02 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Hosang", "Jan", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1406.7062", "submitter": "Ke Liu", "authors": "Ke Liu, Ming Xu, Zeyun Yu", "title": "Adaptive Mesh Representation and Restoration of Biomedical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The triangulation of images has become an active research area in recent\nyears for its compressive representation and ease of image processing and\nvisualization. However, little work has been done on how to faithfully recover\nimage intensities from a triangulated mesh of an image, a process also known as\nimage restoration or decoding from meshes. The existing methods such as linear\ninterpolation, least-square interpolation, or interpolation based on radial\nbasis functions (RBFs) work to some extent, but often yield blurred features\n(edges, corners, etc.). The main reason for this problem is due to the\nisotropically-defined Euclidean distance that is taken into consideration in\nthese methods, without considering the anisotropicity of feature intensities in\nan image. Moreover, most existing methods use intensities defined at mesh nodes\nwhose intensities are often ambiguously defined on or near image edges (or\nfeature boundaries). In the current paper, a new method of restoring an image\nfrom its triangulation representation is proposed, by utilizing anisotropic\nradial basis functions (ARBFs). This method considers not only the geometrical\n(Euclidean) distances but also the local feature orientations (anisotropic\nintensities). Additionally, this method is based on the intensities of mesh\nfaces instead of mesh nodes and thus provides a more robust restoration. The\ntwo strategies together guarantee excellent feature-preserving restoration of\nan image with arbitrary super-resolutions from its triangulation\nrepresentation, as demonstrated by various experiments provided in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 05:06:22 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Liu", "Ke", ""], ["Xu", "Ming", ""], ["Yu", "Zeyun", ""]]}, {"id": "1406.7075", "submitter": "\\\"Omer Faruk Ertu\\u{g}rul", "authors": "Omer Faruk Ertugrul", "title": "Adaptive texture energy measure method", "comments": null, "journal-ref": "International Journal of Intelligent Information Systems. Vol. 3,\n  No. 2, 2014, pp. 13-18", "doi": "10.11648/j.ijiis.20140302.11", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in image quality, data storage, and computational\ncapacity have heightened the need for texture analysis in image process. To\ndate various methods have been developed and introduced for assessing textures\nin images. One of the most popular texture analysis methods is the Texture\nEnergy Measure (TEM) and it has been used for detecting edges, levels, waves,\nspots and ripples by employing predefined TEM masks to images. Despite several\nsuccess- ful studies, TEM has a number of serious weaknesses in use. The major\ndrawback is; the masks are predefined therefore they cannot be adapted to\nimage. A new method, Adaptive Texture Energy Measure Method (aTEM), was offered\nto over- come this disadvantage of TEM by using adaptive masks by adjusting the\ncontrast, sharpening and orientation angle of the mask. To assess the\napplicability of aTEM, it is compared with TEM. The accuracy of the\nclassification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292\nand 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively by\naTEM. The results of this study indicate that aTEM is a successful method for\ntexture analysis.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 06:00:17 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Ertugrul", "Omer Faruk", ""]]}, {"id": "1406.7112", "submitter": "Vasileios Zografos", "authors": "Vasileios Zografos", "title": "3D planar patch extraction from stereo using probabilistic region\n  growing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a novel 3D planar patch extraction method using a\nprobabilistic region growing algorithm. Our method works by simultaneously\ninitiating multiple planar patches from seed points, the latter determined by\nan intensity-based 2D segmentation algorithm in the stereo-pair images. The\npatches are grown incrementally and in parallel as 3D scene points are\nconsidered for membership, using a probabilistic distance likelihood measure.\nIn addition, we have incorporated prior information based on the noise model in\nthe 2D images and the scene configuration but also include the intensity\ninformation resulting from the initial segmentation. This method works well\nacross many different data-sets, involving real and synthetic examples of both\nregularly and non-regularly sampled data, and is fast enough that may be used\nfor robot navigation tasks of path detection and obstacle avoidance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 08:52:54 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Zografos", "Vasileios", ""]]}, {"id": "1406.7120", "submitter": "Anish Acharya", "authors": "Anish Acharya", "title": "Template Matching based Object Detection Using HOG Feature Pyramid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a step by step development of designing a Object\nDetection scheme using the HOG based Feature Pyramid aligned with the concept\nof Template Matching.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 09:18:44 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Acharya", "Anish", ""]]}, {"id": "1406.7128", "submitter": "Gonzalo Galiano", "authors": "Gonzalo Galiano, Juli\\'an Velasco", "title": "On a new formulation of nonlocal image filters involving the relative\n  rearrangement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlocal filters are simple and powerful techniques for image denoising. In\nthis paper we study the reformulation of a broad class of nonlocal filters in\nterms of two functional rearrangements: the decreasing and the relative\nrearrangements.\n  Independently of the dimension of the image, we reformulate these filters as\nintegral operators defined in a one-dimensional space corresponding to the\nlevel sets measures.\n  We prove the equivalency between the original and the rearranged versions of\nthe filters and propose a discretization in terms of constant-wise\ninterpolators, which we prove to be convergent to the solution of the\ncontinuous setting.\n  For some particular cases, this new formulation allows us to perform a\ndetailed analysis of the filtering properties. Among others, we prove that the\nfiltered image is a contrast change of the original image, and that the\nfiltering procedure behaves asymptotically as a shock filter combined with a\nborder diffusive term, responsible for the staircaising effect and the loss of\ncontrast.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 09:43:53 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Galiano", "Gonzalo", ""], ["Velasco", "Juli\u00e1n", ""]]}, {"id": "1406.7360", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "A framework for improving the performance of verification algorithms\n  with a low false positive rate requirement and limited training data", "comments": "IEEE/IAPR International Joint Conference on Biometrics, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of matching patterns in the so-called\nverification setting in which a novel, query pattern is verified against a\nsingle training pattern: the decision sought is whether the two match (i.e.\nbelong to the same class) or not. Unlike previous work which has universally\nfocused on the development of more discriminative distance functions between\npatterns, here we consider the equally important and pervasive task of\nselecting a distance threshold which fits a particular operational requirement\n- specifically, the target false positive rate (FPR). First, we argue on\ntheoretical grounds that a data-driven approach is inherently ill-conditioned\nwhen the desired FPR is low, because by the very nature of the challenge only a\nsmall portion of training data affects or is affected by the desired threshold.\nThis leads us to propose a general, statistical model-based method instead. Our\napproach is based on the interpretation of an inter-pattern distance as\nimplicitly defining a pattern embedding which approximately distributes\npatterns according to an isotropic multi-variate normal distribution in some\nspace. This interpretation is then used to show that the distribution of\ntraining inter-pattern distances is the non-central chi2 distribution,\ndifferently parameterized for each class. Thus, to make the class-specific\nthreshold choice we propose a novel analysis-by-synthesis iterative algorithm\nwhich estimates the three free parameters of the model (for each class) using\ntask-specific constraints. The validity of the premises of our work and the\neffectiveness of the proposed method are demonstrated by applying the method to\nthe task of set-based face verification on a large database of pseudo-random\nhead motion videos.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 05:43:30 GMT"}, {"version": "v2", "created": "Fri, 4 Jul 2014 05:52:18 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1406.7444", "submitter": "Christian J. Schuler", "authors": "Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard\n  Sch\\\"olkopf", "title": "Learning to Deblur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a learning-based approach to blind image deconvolution. It uses a\ndeep layered architecture, parts of which are borrowed from recent work on\nneural network learning, and parts of which incorporate computations that are\nspecific to image deconvolution. The system is trained end-to-end on a set of\nartificially generated training examples, enabling competitive performance in\nblind deconvolution, both with respect to quality and runtime.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 21:56:31 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Schuler", "Christian J.", ""], ["Hirsch", "Michael", ""], ["Harmeling", "Stefan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1406.7525", "submitter": "Wenqi Huang", "authors": "Wenqi Huang, Xiaojin Gong", "title": "Fusion Based Holistic Road Scene Understanding", "comments": "14 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of holistic road scene understanding based\non the integration of visual and range data. To achieve the grand goal, we\npropose an approach that jointly tackles object-level image segmentation and\nsemantic region labeling within a conditional random field (CRF) framework.\nSpecifically, we first generate semantic object hypotheses by clustering 3D\npoints, learning their prior appearance models, and using a deep learning\nmethod for reasoning their semantic categories. The learned priors, together\nwith spatial and geometric contexts, are incorporated in CRF. With this\nformulation, visual and range data are fused thoroughly, and moreover, the\ncoupled segmentation and semantic labeling problem can be inferred via Graph\nCuts. Our approach is validated on the challenging KITTI dataset that contains\ndiverse complicated road scenarios. Both quantitative and qualitative\nevaluations demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 17:11:25 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Huang", "Wenqi", ""], ["Gong", "Xiaojin", ""]]}, {"id": "1406.7799", "submitter": "Pedram Mohammadi Mr.", "authors": "Pedram Mohammadi, Abbas Ebrahimi-Moghadam, and Shahram Shirani", "title": "Subjective and Objective Quality Assessment of Image: A Survey", "comments": "50 pages, 12 figures, and 3 Tables. This work has been submitted to\n  Elsevier Journal of Visual Communication and Image Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand for image-based applications, the efficient and\nreliable evaluation of image quality has increased in importance. Measuring the\nimage quality is of fundamental importance for numerous image processing\napplications, where the goal of image quality assessment (IQA) methods is to\nautomatically evaluate the quality of images in agreement with human quality\njudgments. Numerous IQA methods have been proposed over the past years to\nfulfill this goal. In this paper, a survey of the quality assessment methods\nfor conventional image signals, as well as the newly emerged ones, which\nincludes the high dynamic range (HDR) and 3-D images, is presented. A\ncomprehensive explanation of the subjective and objective IQA and their\nclassification is provided. Six widely used subjective quality datasets, and\nperformance measures are reviewed. Emphasis is given to the full-reference\nimage quality assessment (FR-IQA) methods, and 9 often-used quality measures\n(including mean squared error (MSE), structural similarity index (SSIM),\nmulti-scale structural similarity index (MS-SSIM), visual information fidelity\n(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),\nfeature similarity measure for color images (FSIMC), dynamic range independent\nmeasure (DRIM), and tone-mapped images quality index (TMQI)) are carefully\ndescribed, and their performance and computation time on four subjective\nquality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is\nprovided and the issues related to this area of research are reviewed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:25:00 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Mohammadi", "Pedram", ""], ["Ebrahimi-Moghadam", "Abbas", ""], ["Shirani", "Shahram", ""]]}]