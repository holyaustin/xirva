[{"id": "1801.00054", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yu Qiao, Tao Xiang", "title": "Deep Reinforcement Learning for Unsupervised Video Summarization with\n  Diversity-Representativeness Reward", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization aims to facilitate large-scale video browsing by\nproducing short, concise summaries that are diverse and representative of\noriginal videos. In this paper, we formulate video summarization as a\nsequential decision-making process and develop a deep summarization network\n(DSN) to summarize videos. DSN predicts for each video frame a probability,\nwhich indicates how likely a frame is selected, and then takes actions based on\nthe probability distributions to select frames, forming video summaries. To\ntrain our DSN, we propose an end-to-end, reinforcement learning-based\nframework, where we design a novel reward function that jointly accounts for\ndiversity and representativeness of generated summaries and does not rely on\nlabels or user interactions at all. During training, the reward function judges\nhow diverse and representative the generated summaries are, while DSN strives\nfor earning higher rewards by learning to produce more diverse and more\nrepresentative summaries. Since labels are not required, our method can be\nfully unsupervised. Extensive experiments on two benchmark datasets show that\nour unsupervised method not only outperforms other state-of-the-art\nunsupervised methods, but also is comparable to or even superior than most of\npublished supervised approaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 22:51:36 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 02:30:24 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 19:34:43 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Qiao", "Yu", ""], ["Xiang", "Tao", ""]]}, {"id": "1801.00055", "submitter": "Enver Sangineto", "authors": "Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere, Nicu Sebe", "title": "Deformable GANs for Pose-based Human Image Generation", "comments": "CVPR 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of generating person images conditioned\non a given pose. Specifically, given an image of a person and a target pose, we\nsynthesize a new image of that person in the novel pose. In order to deal with\npixel-to-pixel misalignments caused by the pose differences, we introduce\ndeformable skip connections in the generator of our Generative Adversarial\nNetwork. Moreover, a nearest-neighbour loss is proposed instead of the common\nL1 and L2 losses in order to match the details of the generated image with the\ntarget image. We test our approach using photos of persons in different poses\nand we compare our method with previous work in this area showing\nstate-of-the-art results in two benchmarks. Our method can be applied to the\nwider field of deformable object generation, provided that the pose of the\narticulated object can be extracted using a keypoint detector.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 22:58:31 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 16:19:17 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Sangineto", "Enver", ""], ["Lathuiliere", "Stephane", ""], ["Sebe", "Nicu", ""]]}, {"id": "1801.00077", "submitter": "Xing Di", "authors": "Xing Di, Vishal M. Patel", "title": "Face Synthesis from Visual Attributes via Sketch using Conditional VAEs\n  and GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic synthesis of faces from visual attributes is an important problem\nin computer vision and has wide applications in law enforcement and\nentertainment. With the advent of deep generative convolutional neural networks\n(CNNs), attempts have been made to synthesize face images from attributes and\ntext descriptions. In this paper, we take a different approach, where we\nformulate the original problem as a stage-wise learning problem. We first\nsynthesize the facial sketch corresponding to the visual attributes and then we\nreconstruct the face image based on the synthesized sketch. The proposed\nAttribute2Sketch2Face framework, which is based on a combination of deep\nConditional Variational Autoencoder (CVAE) and Generative Adversarial Networks\n(GANs), consists of three stages: (1) Synthesis of facial sketch from\nattributes using a CVAE architecture, (2) Enhancement of coarse sketches to\nproduce sharper sketches using a GAN-based framework, and (3) Synthesis of face\nfrom sketch using another GAN-based network. Extensive experiments and\ncomparison with recent methods are performed to verify the effectiveness of the\nproposed attribute-based three stage face synthesis method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 02:57:09 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Di", "Xing", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1801.00096", "submitter": "Timothy Rozario", "authors": "Timothy Rozario, Troy Long, Mingli Chen, Weiguo Lu and Steve Jiang", "title": "Towards automated patient data cleaning using deep learning: A\n  feasibility study on the standardization of organ labeling", "comments": "17 pages, 7 figures, 3 tables, 39 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data cleaning consumes about 80% of the time spent on data analysis for\nclinical research projects. This is a much bigger problem in the era of big\ndata and machine learning in the field of medicine where large volumes of data\nare being generated. We report an initial effort towards automated patient data\ncleaning using deep learning: the standardization of organ labeling in\nradiation therapy. Organs are often labeled inconsistently at different\ninstitutions (sometimes even within the same institution) and at different time\nperiods, which poses a problem for clinical research, especially for\nmulti-institutional collaborative clinical research where the acquired patient\ndata is not being used effectively. We developed a convolutional neural network\n(CNN) to automatically identify each organ in the CT image and then label it\nwith the standardized nomenclature presented at AAPM Task Group 263. We tested\nthis model on the CT images of 54 patients with prostate and 100 patients with\nhead and neck cancer who previously received radiation therapy. The model\nachieved 100% accuracy in detecting organs and assigning standardized labels\nfor the patients tested. This work shows the feasibility of using deep learning\nin patient data cleaning that enables standardized datasets to be generated for\neffective intra- and interinstitutional collaborative clinical research.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 07:56:46 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Rozario", "Timothy", ""], ["Long", "Troy", ""], ["Chen", "Mingli", ""], ["Lu", "Weiguo", ""], ["Jiang", "Steve", ""]]}, {"id": "1801.00098", "submitter": "Uche Nnolim", "authors": "U. A. Nnolim", "title": "A PDE-based log-agnostic illumination correction algorithm", "comments": "22 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the results of a partial differential equation\n(PDE)-based image enhancement algorithm, for dynamic range compression and\nillumination correction in the absence of the logarithmic function. The\nproposed algorithm combines forward and reverse flows in a PDE-based\nformulation. The experimental results are compared with algorithms from the\nliterature and indicate comparable performance in most cases.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 08:02:56 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 07:52:27 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Nnolim", "U. A.", ""]]}, {"id": "1801.00182", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Guang-Zhong Yang, Su-Lin Lee", "title": "A Real-time and Registration-free Framework for Dynamic Shape\n  Instantiation", "comments": "35 Pages, 11 figures", "journal-ref": "Medical Image Analysis 44 (2017): 86-97", "doi": "10.1016/j.media.2017.11.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time 3D navigation during minimally invasive procedures is an essential\nyet challenging task, especially when considerable tissue motion is involved.\nTo balance image acquisition speed and resolution, only 2D images or\nlow-resolution 3D volumes can be used clinically. In this paper, a real-time\nand registration-free framework for dynamic shape instantiation, generalizable\nto multiple anatomical applications, is proposed to instantiate high-resolution\n3D shapes of an organ from a single 2D image intra-operatively. Firstly, an\napproximate optimal scan plane was determined by analyzing the pre-operative 3D\nstatistical shape model (SSM) of the anatomy with sparse principal component\nanalysis (SPCA) and considering practical constraints . Secondly, kernel\npartial least squares regression (KPLSR) was used to learn the relationship\nbetween the pre-operative 3D SSM and a synchronized 2D SSM constructed from 2D\nimages obtained at the approximate optimal scan plane. Finally, the derived\nrelationship was applied to the new intra-operative 2D image obtained at the\nsame scan plane to predict the high-resolution 3D shape intra-operatively. A\nmajor feature of the proposed framework is that no extra registration between\nthe pre-operative 3D SSM and the synchronized 2D SSM is required. Detailed\nvalidation was performed on studies including the liver and right ventricle\n(RV) of the heart. The derived results (mean accuracy of 2.19mm on patients and\ncomputation speed of 1ms) demonstrate its potential clinical value for\nreal-time, high-resolution, dynamic and 3D interventional guidance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 19:25:09 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Yang", "Guang-Zhong", ""], ["Lee", "Su-Lin", ""]]}, {"id": "1801.00187", "submitter": "Ayan Kumar Bhunia", "authors": "Shuvozit Ghose, Abhirup Das, Ayan Kumar Bhunia, Partha Pratim Roy", "title": "Fractional Local Neighborhood Intensity Pattern for Image Retrieval\n  using Genetic Algorithm", "comments": "MTAP, Springer(Minor Revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new texture descriptor named \"Fractional Local Neighborhood\nIntensity Pattern\" (FLNIP) has been proposed for content based image retrieval\n(CBIR). It is an extension of the Local Neighborhood Intensity Pattern\n(LNIP)[1]. FLNIP calculates the relative intensity difference between a\nparticular pixel and the center pixel of a 3x3 window by considering the\nrelationship with adjacent neighbors. In this work, the fractional change in\nthe local neighborhood involving the adjacent neighbors has been calculated\nfirst with respect to one of the eight neighbors of the center pixel of a 3x3\nwindow. Next, the fractional change has been calculated with respect to the\ncenter itself. The two values of fractional change are next compared to\ngenerate a binary bit pattern. Both sign and magnitude information are encoded\nin a single descriptor as it deals with the relative change in magnitude in the\nadjacent neighborhood i.e., the comparison of the fractional change. The\ndescriptor is applied on four multi-resolution images -- one being the raw\nimage and the other three being filtered gaussian images obtained by applying\ngaussian filters of different standard deviations on the raw image to signify\nthe importance of exploring texture information at different resolutions in an\nimage. The four sets of distances obtained between the query and the target\nimage are then combined with a genetic algorithm based approach to improve the\nretrieval performance by minimizing the distance between similar class images.\nThe performance of the method has been tested for image retrieval on four\npopular databases. The precision and recall values observed on these databases\nhave been compared with recent state-of-art local patterns. The proposed method\nhas shown a significant improvement over many other existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 20:18:32 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 03:03:08 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 19:58:40 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Ghose", "Shuvozit", ""], ["Das", "Abhirup", ""], ["Bhunia", "Ayan Kumar", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1801.00192", "submitter": "Ali Javidani", "authors": "Ali Javidani, Ahmad Mahmoudi-Aznaveh", "title": "A Unified Method for First and Third Person Action Recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new video classification methodology is proposed which can\nbe applied in both first and third person videos. The main idea behind the\nproposed strategy is to capture complementary information of appearance and\nmotion efficiently by performing two independent streams on the videos. The\nfirst stream is aimed to capture long-term motions from shorter ones by keeping\ntrack of how elements in optical flow images have changed over time. Optical\nflow images are described by pre-trained networks that have been trained on\nlarge scale image datasets. A set of multi-channel time series are obtained by\naligning descriptions beside each other. For extracting motion features from\nthese time series, PoT representation method plus a novel pooling operator is\nfollowed due to several advantages. The second stream is accomplished to\nextract appearance features which are vital in the case of video\nclassification. The proposed method has been evaluated on both first and\nthird-person datasets and results present that the proposed methodology reaches\nthe state of the art successfully.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 21:03:13 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 14:42:37 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Javidani", "Ali", ""], ["Mahmoudi-Aznaveh", "Ahmad", ""]]}, {"id": "1801.00223", "submitter": "Qiang Zheng", "authors": "Qiang Zheng, Yong Fan", "title": "Integrating semi-supervised label propagation and random forests for\n  multi-atlas based hippocampus segmentation", "comments": "Accepted paper in IEEE International Symposium on Biomedical Imaging\n  (ISBI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel multi-atlas based image segmentation method is proposed by\nintegrating a semi-supervised label propagation method and a supervised random\nforests method in a pattern recognition based label fusion framework. The\nsemi-supervised label propagation method takes into consideration local and\nglobal image appearance of images to be segmented and segments the images by\npropagating reliable segmentation results obtained by the supervised random\nforests method. Particularly, the random forests method is used to train a\nregression model based on image patches of atlas images for each voxel of the\nimages to be segmented. The regression model is used to obtain reliable\nsegmentation results to guide the label propagation for the segmentation. The\nproposed method has been compared with state-of-the-art multi-atlas based image\nsegmentation methods for segmenting the hippocampus in MR images. The\nexperiment results have demonstrated that our method obtained superior\nsegmentation performance.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 01:52:23 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Zheng", "Qiang", ""], ["Fan", "Yong", ""]]}, {"id": "1801.00224", "submitter": "Qiang Zheng", "authors": "Qiang Zheng, Gregory Tasian, Yong Fan", "title": "Transfer learning for diagnosis of congenital abnormalities of the\n  kidney and urinary tract in children based on Ultrasound imaging data", "comments": "Accepted paper in IEEE International Symposium on Biomedical Imaging\n  (ISBI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of ultrasound (US) kidney images for diagnosis of congenital\nabnormalities of the kidney and urinary tract (CAKUT) in children is a\nchallenging task. It is desirable to improve existing pattern classification\nmodels that are built upon conventional image features. In this study, we\npropose a transfer learning-based method to extract imaging features from US\nkidney images in order to improve the CAKUT diagnosis in children.\nParticularly, a pre-trained deep learning model (imagenet-caffe-alex) is\nadopted for transfer learning-based feature extraction from 3-channel feature\nmaps computed from US images, including original images, gradient features, and\ndistanced transform features. Support vector machine classifiers are then built\nupon different sets of features, including the transfer learning features,\nconventional imaging features, and their combination. Experimental results have\ndemonstrated that the combination of transfer learning features and\nconventional imaging features yielded the best classification performance for\ndistinguishing CAKUT patients from normal controls based on their US kidney\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 01:55:40 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Zheng", "Qiang", ""], ["Tasian", "Gregory", ""], ["Fan", "Yong", ""]]}, {"id": "1801.00256", "submitter": "Shadrokh Samavi", "authors": "Mahdi Ahmadi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi", "title": "Context aware saliency map generation using semantic segmentation", "comments": "5 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency map detection, as a method for detecting important regions of an\nimage, is used in many applications such as image classification and\nrecognition. We propose that context detection could have an essential role in\nimage saliency detection. This requires extraction of high level features. In\nthis paper a saliency map is proposed, based on image context detection using\nsemantic segmentation as a high level feature. Saliency map from semantic\ninformation is fused with color and contrast based saliency maps. The final\nsaliency map is then generated. Simulation results for Pascal-voc11 image\ndataset show 99% accuracy in context detection. Also final saliency map\nproduced by our proposed method shows acceptable results in detecting salient\npoints.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 08:55:17 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 08:11:57 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Ahmadi", "Mahdi", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1801.00269", "submitter": "Michael Gygli", "authors": "Arnaud Benard and Michael Gygli", "title": "Interactive Video Object Segmentation in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our system for human-in-the-loop video object\nsegmentation. The backbone of our system is a method for one-shot video object\nsegmentation. While fast, this method requires an accurate pixel-level\nsegmentation of one (or several) frames as input. As manually annotating such a\nsegmentation is impractical, we propose a deep interactive image segmentation\nmethod, that can accurately segment objects with only a handful of clicks. On\nthe GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks on\naverage, setting the new state of the art. Furthermore, as our method\niteratively refines an initial segmentation, it can effectively correct frames\nwhere the video object segmentation fails, thus allowing users to quickly\nobtain high quality results even on challenging sequences. Finally, we\ninvestigate usage patterns and give insights in how many steps users take to\nannotate frames, what kind of corrections they provide, etc., thus giving\nimportant insights for further improving interactive video segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 11:46:54 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Benard", "Arnaud", ""], ["Gygli", "Michael", ""]]}, {"id": "1801.00289", "submitter": "Ugur Demir", "authors": "Ugur Demir and Gozde Unal", "title": "Deep Stacked Networks with Residual Polishing for Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown promising results in image inpainting even if\nthe missing area is relatively large. However, most of the existing inpainting\nnetworks introduce undesired artifacts and noise to the repaired regions. To\nsolve this problem, we present a novel framework which consists of two stacked\nconvolutional neural networks that inpaint the image and remove the artifacts,\nrespectively. The first network considers the global structure of the damaged\nimage and coarsely fills the blank area. Then the second network modifies the\nrepaired image to cancel the noise introduced by the first network. The\nproposed framework splits the problem into two distinct partitions that can be\noptimized separately, therefore it can be applied to any inpainting algorithm\nby changing the first network. Second stage in our framework which aims at\npolishing the inpainted images can be treated as a denoising problem where a\nwide range of algorithms can be employed. Our results demonstrate that the\nproposed framework achieves significant improvement on both visual and\nquantitative evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 14:32:27 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Demir", "Ugur", ""], ["Unal", "Gozde", ""]]}, {"id": "1801.00318", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification", "comments": "5 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 17:13:55 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 06:19:41 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1801.00349", "submitter": "Mahmood Sharif", "authors": "Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter", "title": "A General Framework for Adversarial Examples with Objectives", "comments": "Accepted for publication at ACM TOPS", "journal-ref": null, "doi": "10.1145/3317611", "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Images perturbed subtly to be misclassified by neural networks, called\nadversarial examples, have emerged as a technically deep challenge and an\nimportant concern for several application domains. Most research on adversarial\nexamples takes as its only constraint that the perturbed images are similar to\nthe originals. However, real-world application of these ideas often requires\nthe examples to satisfy additional objectives, which are typically enforced\nthrough custom modifications of the perturbation process. In this paper, we\npropose adversarial generative nets (AGNs), a general methodology to train a\ngenerator neural network to emit adversarial examples satisfying desired\nobjectives. We demonstrate the ability of AGNs to accommodate a wide range of\nobjectives, including imprecise ones difficult to model, in two application\ndomains. In particular, we demonstrate physical adversarial examples---eyeglass\nframes designed to fool face recognition---with better robustness,\ninconspicuousness, and scalability than previous approaches, as well as a new\nattack to fool a handwritten-digit classifier.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 20:17:45 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 01:14:44 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Sharif", "Mahmood", ""], ["Bhagavatula", "Sruti", ""], ["Bauer", "Lujo", ""], ["Reiter", "Michael K.", ""]]}, {"id": "1801.00415", "submitter": "Moi Hoon Yap", "authors": "Ezak Ahmad and Manu Goyal and Jamie S. McPhee and Hans Degens and Moi\n  Hoon Yap", "title": "Semantic Segmentation of Human Thigh Quadriceps Muscle in Magnetic\n  Resonance Images", "comments": "27 pages, 7 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end solution for MRI thigh quadriceps\nsegmentation. This is the first attempt that deep learning methods are used for\nthe MRI thigh segmentation task. We use the state-of-the-art Fully\nConvolutional Networks with transfer learning approach for the semantic\nsegmentation of regions of interest in MRI thigh scans. To further improve the\nperformance of the segmentation, we propose a post-processing technique using\nbasic image processing methods. With our proposed method, we have established a\nnew benchmark for MRI thigh quadriceps segmentation with mean Jaccard\nSimilarity Index of 0.9502 and processing time of 0.117 second per image.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 08:58:10 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 17:02:05 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Ahmad", "Ezak", ""], ["Goyal", "Manu", ""], ["McPhee", "Jamie S.", ""], ["Degens", "Hans", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1801.00451", "submitter": "Alex James Dr", "authors": "Olga Krestinskaya and Alex Pappachen James", "title": "Facial emotion recognition using min-max similarity classifier", "comments": null, "journal-ref": "2017 International Conference on Advances in Computing,\n  Communications and Informatics (ICACCI), Udupi, 2017, pp. 752-758", "doi": "10.1109/ICACCI.2017.8125932", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of human emotions from the imaging templates is useful in a wide\nvariety of human-computer interaction and intelligent systems applications.\nHowever, the automatic recognition of facial expressions using image template\nmatching techniques suffer from the natural variability with facial features\nand recording conditions. In spite of the progress achieved in facial emotion\nrecognition in recent years, the effective and computationally simple feature\nselection and classification technique for emotion recognition is still an open\nproblem. In this paper, we propose an efficient and straightforward facial\nemotion recognition algorithm to reduce the problem of inter-class pixel\nmismatch during classification. The proposed method includes the application of\npixel normalization to remove intensity offsets followed-up with a Min-Max\nmetric in a nearest neighbor classifier that is capable of suppressing feature\noutliers. The results indicate an improvement of recognition performance from\n92.85% to 98.57% for the proposed Min-Max classification method when tested on\nJAFFE database. The proposed emotion recognition technique outperforms the\nexisting template matching methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 14:51:02 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Krestinskaya", "Olga", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1801.00454", "submitter": "Alex James Dr", "authors": "Diana Sadykova, Alex Pappachen James", "title": "Quality assessment metrics for edge detection and edge-aware filtering:\n  A tutorial review", "comments": null, "journal-ref": "2017 International Conference on Advances in Computing,\n  Communications and Informatics (ICACCI), Udupi, 2017, pp. 2366-2369", "doi": "10.1109/ICACCI.2017.8126200", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality assessment of edges in an image is an important topic as it helps\nto benchmark the performance of edge detectors, and edge-aware filters that are\nused in a wide range of image processing tasks. The most popular image quality\nmetrics such as Mean squared error (MSE), Peak signal-to-noise ratio (PSNR) and\nStructural similarity (SSIM) metrics for assessing and justifying the quality\nof edges. However, they do not address the structural and functional accuracy\nof edges in images with a wide range of natural variabilities. In this review,\nwe provide an overview of all the most relevant performance metrics that can be\nused to benchmark the quality performance of edges in images. We identify four\nmajor groups of metrics and also provide a critical insight into the evaluation\nprotocol and governing equations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 15:01:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Sadykova", "Diana", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1801.00455", "submitter": "Alex James Dr", "authors": "Sholpan Kauanova, Ivan Vorobjev, Alex Pappachen James", "title": "Automated image segmentation for detecting cell spreading for\n  metastasizing assessments of cancer development", "comments": null, "journal-ref": "2017 International Conference on Advances in Computing,\n  Communications and Informatics (ICACCI), Udupi, 2017, pp. 2382-2387", "doi": "10.1109/ICACCI.2017.8126203", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated segmentation of cells in microscopic images is an open research\nproblem that has important implications for studies of the developmental and\ncancer processes based on in vitro models. In this paper, we present the\napproach for segmentation of the DIC images of cultured cells using G-neighbor\nsmoothing followed by Kauwahara filtering and local standard deviation approach\nfor boundary detection. NIH FIJI/ImageJ tools are used to create the ground\ntruth dataset. The results of this work indicate that detection of cell\nboundaries using segmentation approach even in the case of realistic\nmeasurement conditions is a challenging problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 15:06:31 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Kauanova", "Sholpan", ""], ["Vorobjev", "Ivan", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1801.00470", "submitter": "Aishik Konwer", "authors": "Ankan Kumar Bhunia, Aishik Konwer, Ayan Kumar Bhunia, Abir Bhowmick,\n  Partha P. Roy, Umapada Pal", "title": "Script Identification in Natural Scene Image and Video Frame using\n  Attention based Convolutional-LSTM Network", "comments": "The first and second authors contributed equally. Accepted in Pattern\n  Recognition Journal", "journal-ref": null, "doi": "10.1016/j.patcog.2018.07.034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Script identification plays a significant role in analysing documents and\nvideos. In this paper, we focus on the problem of script identification in\nscene text images and video scripts. Because of low image quality, complex\nbackground and similar layout of characters shared by some scripts like Greek,\nLatin, etc., text recognition in those cases become challenging. In this paper,\nwe propose a novel method that involves extraction of local and global features\nusing CNN-LSTM framework and weighting them dynamically for script\nidentification. First, we convert the images into patches and feed them into a\nCNN-LSTM framework. Attention-based patch weights are calculated applying\nsoftmax layer after LSTM. Next, we do patch-wise multiplication of these\nweights with corresponding CNN to yield local features. Global features are\nalso extracted from last cell state of LSTM. We employ a fusion technique which\ndynamically weights the local and global features for an individual patch.\nExperiments have been done in four public script identification datasets:\nSIW-13, CVSI2015, ICDAR-17 and MLe2e. The proposed framework achieves superior\nresults in comparison to conventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 16:42:50 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 07:53:38 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 16:20:08 GMT"}, {"version": "v4", "created": "Tue, 7 Aug 2018 17:37:58 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Bhunia", "Ankan Kumar", ""], ["Konwer", "Aishik", ""], ["Bhunia", "Ayan Kumar", ""], ["Bhowmick", "Abir", ""], ["Roy", "Partha P.", ""], ["Pal", "Umapada", ""]]}, {"id": "1801.00476", "submitter": "Farzin Ghorban", "authors": "Farzin Ghorban, Javier Mar\\'in, Yu Su, Alessandro Colombo, Anton\n  Kummert", "title": "Aggregated Channels Network for Real-Time Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated their superiority in\nnumerous computer vision tasks, yet their computational cost results\nprohibitive for many real-time applications such as pedestrian detection which\nis usually performed on low-consumption hardware. In order to alleviate this\ndrawback, most strategies focus on using a two-stage cascade approach.\nEssentially, in the first stage a fast method generates a significant but\nreduced amount of high quality proposals that later, in the second stage, are\nevaluated by the CNN. In this work, we propose a novel detection pipeline that\nfurther benefits from the two-stage cascade strategy. More concretely, the\nenriched and subsequently compressed features used in the first stage are\nreused as the CNN input. As a consequence, a simpler network architecture,\nadapted for such small input sizes, allows to achieve real-time performance and\nobtain results close to the state-of-the-art while running significantly faster\nwithout the use of GPU. In particular, considering that the proposed pipeline\nruns in frame rate, the achieved performance is highly competitive. We\nfurthermore demonstrate that the proposed pipeline on itself can serve as an\neffective proposal generator.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 16:58:25 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Ghorban", "Farzin", ""], ["Mar\u00edn", "Javier", ""], ["Su", "Yu", ""], ["Colombo", "Alessandro", ""], ["Kummert", "Anton", ""]]}, {"id": "1801.00508", "submitter": "Chris Ying", "authors": "Chris Ying, Katerina Fragkiadaki", "title": "Depth-Adaptive Computational Policies for Efficient Visual Tracking", "comments": "presented at EMMCVPR 2017 in Venice, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current convolutional neural networks algorithms for video object tracking\nspend the same amount of computation for each object and video frame. However,\nit is harder to track an object in some frames than others, due to the varying\namount of clutter, scene complexity, amount of motion, and object's\ndistinctiveness against its background. We propose a depth-adaptive\nconvolutional Siamese network that performs video tracking adaptively at\nmultiple neural network depths. Parametric gating functions are trained to\ncontrol the depth of the convolutional feature extractor by minimizing a joint\nloss of computational cost and tracking error. Our network achieves accuracy\ncomparable to the state-of-the-art on the VOT2016 benchmark. Furthermore, our\nadaptive depth computation achieves higher accuracy for a given computational\ncost than traditional fixed-structure neural networks. The presented framework\nextends to other tasks that use convolutional neural networks and enables\ntrading speed for accuracy at runtime.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 20:54:33 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Ying", "Chris", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1801.00524", "submitter": "Dan Xu", "authors": "Dan Xu and Wanli Ouyang and Xavier Alameda-Pineda and Elisa Ricci and\n  Xiaogang Wang and Nicu Sebe", "title": "Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs\n  for Contour Prediction", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that exploiting multi-scale representations deeply\nlearned via convolutional neural networks (CNN) is of tremendous importance for\naccurate contour detection. This paper presents a novel approach for predicting\ncontours which advances the state of the art in two fundamental aspects, i.e.\nmulti-scale feature generation and fusion. Different from previous works\ndirectly consider- ing multi-scale feature maps obtained from the inner layers\nof a primary CNN architecture, we introduce a hierarchical deep model which\nproduces more rich and complementary representations. Furthermore, to refine\nand robustly fuse the representations learned at different scales, the novel\nAttention-Gated Conditional Random Fields (AG-CRFs) are proposed. The\nexperiments ran on two publicly available datasets (BSDS500 and NYUDv2)\ndemonstrate the effectiveness of the latent AG-CRF model and of the overall\nhierarchical framework.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 22:48:44 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Xu", "Dan", ""], ["Ouyang", "Wanli", ""], ["Alameda-Pineda", "Xavier", ""], ["Ricci", "Elisa", ""], ["Wang", "Xiaogang", ""], ["Sebe", "Nicu", ""]]}, {"id": "1801.00543", "submitter": "Yujia Zhang", "authors": "Yujia Zhang, Xiaodan Liang, Dingwen Zhang, Min Tan, and Eric P. Xing", "title": "Unsupervised Object-Level Video Summarization with Online Motion\n  Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised video summarization plays an important role on digesting,\nbrowsing, and searching the ever-growing videos every day, and the underlying\nfine-grained semantic and motion information (i.e., objects of interest and\ntheir key motions) in online videos has been barely touched. In this paper, we\ninvestigate a pioneer research direction towards the fine-grained unsupervised\nobject-level video summarization. It can be distinguished from existing\npipelines in two aspects: extracting key motions of participated objects, and\nlearning to summarize in an unsupervised and online manner. To achieve this\ngoal, we propose a novel online motion Auto-Encoder (online motion-AE)\nframework that functions on the super-segmented object motion clips.\nComprehensive experiments on a newly-collected surveillance dataset and public\ndatasets have demonstrated the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 03:55:36 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 09:36:32 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhang", "Yujia", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Dingwen", ""], ["Tan", "Min", ""], ["Xing", "Eric P.", ""]]}, {"id": "1801.00553", "submitter": "Naveed Akhtar Dr.", "authors": "Naveed Akhtar and Ajmal Mian", "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A\n  Survey", "comments": "Incorporates feedback provided by multiple researchers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is at the heart of the current rise of machine learning and\nartificial intelligence. In the field of Computer Vision, it has become the\nworkhorse for applications ranging from self-driving cars to surveillance and\nsecurity. Whereas deep neural networks have demonstrated phenomenal success\n(often beyond human capabilities) in solving complex problems, recent studies\nshow that they are vulnerable to adversarial attacks in the form of subtle\nperturbations to inputs that lead a model to predict incorrect outputs. For\nimages, such perturbations are often too small to be perceptible, yet they\ncompletely fool the deep learning models. Adversarial attacks pose a serious\nthreat to the success of deep learning in practice. This fact has lead to a\nlarge influx of contributions in this direction. This article presents the\nfirst comprehensive survey on adversarial attacks on deep learning in Computer\nVision. We review the works that design adversarial attacks, analyze the\nexistence of such attacks and propose defenses against them. To emphasize that\nadversarial attacks are possible in practical conditions, we separately review\nthe contributions that evaluate adversarial attacks in the real-world\nscenarios. Finally, we draw on the literature to provide a broader outlook of\nthe research direction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 05:22:06 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 08:50:39 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 06:18:58 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1801.00602", "submitter": "Kai Qiao", "authors": "Kai Qiao, Chi Zhang, Linyuan Wang, Bin Yan, Jian Chen, Lei Zeng, Li\n  Tong", "title": "Accurate reconstruction of image stimuli from human fMRI based on the\n  decoding model with capsule network architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, all kinds of computation models were designed to answer the\nopen question of how sensory stimuli are encoded by neurons and conversely, how\nsensory stimuli can be decoded from neuronal activities. Especially, functional\nMagnetic Resonance Imaging (fMRI) studies have made many great achievements\nwith the rapid development of the deep network computation. However, comparing\nwith the goal of decoding orientation, position and object category from\nactivities in visual cortex, accurate reconstruction of image stimuli from\nhuman fMRI is a still challenging work. In this paper, the capsule network\n(CapsNet) architecture based visual reconstruction (CNAVR) method is developed\nto reconstruct image stimuli. The capsule means containing a group of neurons\nto perform the better organization of feature structure and representation,\ninspired by the structure of cortical mini column including several hundred\nneurons in primates. The high-level capsule features in the CapsNet includes\ndiverse features of image stimuli such as semantic class, orientation, location\nand so on. We used these features to bridge between human fMRI and image\nstimuli. We firstly employed the CapsNet to train the nonlinear mapping from\nimage stimuli to high-level capsule features, and from high-level capsule\nfeatures to image stimuli again in an end-to-end manner. After estimating the\nserviceability of each voxel by encoding performance to accomplish the\nselecting of voxels, we secondly trained the nonlinear mapping from\ndimension-decreasing fMRI data to high-level capsule features. Finally, we can\npredict the high-level capsule features with fMRI data, and reconstruct image\nstimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset\nof handwritten digital images, and exceeded about 10% than the accuracy of all\nexisting state-of-the-art methods on the structural similarity index (SSIM).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 10:39:05 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Qiao", "Kai", ""], ["Zhang", "Chi", ""], ["Wang", "Linyuan", ""], ["Yan", "Bin", ""], ["Chen", "Jian", ""], ["Zeng", "Lei", ""], ["Tong", "Li", ""]]}, {"id": "1801.00605", "submitter": "Afonso Teodoro", "authors": "Afonso M. Teodoro, Jos\\'e M. Bioucas-Dias, M\\'ario A. T. Figueiredo", "title": "Scene-Adapted Plug-and-Play Algorithm with Guaranteed Convergence:\n  Applications to Data Fusion in Imaging", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed plug-and-play (PnP) framework allows leveraging recent\ndevelopments in image denoising to tackle other, more involved, imaging inverse\nproblems. In a PnP method, a black-box denoiser is plugged into an iterative\nalgorithm, taking the place of a formal denoising step that corresponds to the\nproximity operator of some convex regularizer. While this approach offers\nflexibility and excellent performance, convergence of the resulting algorithm\nmay be hard to analyze, as most state-of-the-art denoisers lack an explicit\nunderlying objective function. In this paper, we propose a PnP approach where a\nscene-adapted prior (i.e., where the denoiser is targeted to the specific scene\nbeing imaged) is plugged into ADMM (alternating direction method of\nmultipliers), and prove convergence of the resulting algorithm. Finally, we\napply the proposed framework in two different imaging inverse problems:\nhyperspectral sharpening/fusion and image deblurring from blurred/noisy image\npairs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 10:59:10 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Teodoro", "Afonso M.", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1801.00634", "submitter": "Simant Dube", "authors": "Simant Dube", "title": "High Dimensional Spaces, Deep Learning and Adversarial Examples", "comments": "29 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze deep learning from a mathematical point of view and\nderive several novel results. The results are based on intriguing mathematical\nproperties of high dimensional spaces. We first look at perturbation based\nadversarial examples and show how they can be understood using topological and\ngeometrical arguments in high dimensions. We point out mistake in an argument\npresented in prior published literature, and we present a more rigorous,\ngeneral and correct mathematical result to explain adversarial examples in\nterms of topology of image manifolds. Second, we look at optimization\nlandscapes of deep neural networks and examine the number of saddle points\nrelative to that of local minima. Third, we show how multiresolution nature of\nimages explains perturbation based adversarial examples in form of a stronger\nresult. Our results state that expectation of $L_2$-norm of adversarial\nperturbations is $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ and therefore shrinks to 0\nas image resolution $n$ becomes arbitrarily large. Finally, by incorporating\nthe parts-whole manifold learning hypothesis for natural images, we investigate\nthe working of deep neural networks and root causes of adversarial examples and\ndiscuss how future improvements can be made and how adversarial examples can be\neliminated.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 12:54:22 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 05:30:30 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 13:53:22 GMT"}, {"version": "v4", "created": "Sun, 14 Jan 2018 01:57:47 GMT"}, {"version": "v5", "created": "Sun, 15 Apr 2018 19:39:11 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Dube", "Simant", ""]]}, {"id": "1801.00635", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "Image denoising through bivariate shrinkage function in framelet domain", "comments": "8 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising of coefficients in a sparse domain (e.g. wavelet) has been\nresearched extensively because of its simplicity and effectiveness. Literature\nmainly has focused on designing the best global threshold. However, this paper\nproposes a new denoising method using bivariate shrinkage function in framelet\ndomain. In the proposed method, maximum aposteriori probability is used for\nestimate of the denoised coefficient and non-Gaussian bivariate function is\napplied to model the statistics of framelet coefficients. For every framelet\ncoefficient, there is a corresponding threshold depending on the local\nstatistics of framelet coefficients. Experimental results show that using\nbivariate shrinkage function in framelet domain yields significantly superior\nimage quality and higher PSNR than some well-known denoising methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 12:56:52 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1801.00688", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio", "title": "Learning audio and image representations with bio-inspired trainable\n  feature extractors", "comments": "Accepted for publication in the journal \"Eleectronic Letters on\n  Computer Vision and Image Understanding\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in pattern recognition and signal processing concern the\nautomatic learning of data representations from labeled training samples.\nTypical approaches are based on deep learning and convolutional neural\nnetworks, which require large amount of labeled training samples. In this work,\nwe propose novel feature extractors that can be used to learn the\nrepresentation of single prototype samples in an automatic configuration\nprocess. We employ the proposed feature extractors in applications of audio and\nimage processing, and show their effectiveness on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 15:34:03 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Strisciuglio", "Nicola", ""]]}, {"id": "1801.00693", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Alison Pouplin, Anil A Bharath", "title": "Denoising Adversarial Autoencoders: Classifying Skin Lesions Using\n  Limited Labelled Training Data", "comments": "Under consideration for the IET Computer Vision Journal special issue\n  on \"Computer Vision in Cancer Data Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning model for classifying medical images in the\nsetting where there is a large amount of unlabelled medical data available, but\nlabelled data is in limited supply. We consider the specific case of\nclassifying skin lesions as either malignant or benign. In this setting, the\nproposed approach -- the semi-supervised, denoising adversarial autoencoder --\nis able to utilise vast amounts of unlabelled data to learn a representation\nfor skin lesions, and small amounts of labelled data to assign class labels\nbased on the learned representation. We analyse the contributions of both the\nadversarial and denoising components of the model and find that the combination\nyields superior classification performance in the setting of limited labelled\ntraining data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 15:56:53 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Creswell", "Antonia", ""], ["Pouplin", "Alison", ""], ["Bharath", "Anil A", ""]]}, {"id": "1801.00708", "submitter": "Liuyuan Deng", "authors": "Liuyuan Deng, Ming Yang, Hao Li, Tianyi Li, Bing Hu, Chunxiang Wang", "title": "Restricted Deformable Convolution based Road Scene Semantic Segmentation\n  Using Surround View Cameras", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2019.2939832", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the surrounding environment of the vehicle is still one of the\nchallenges for autonomous driving. This paper addresses 360-degree road scene\nsemantic segmentation using surround view cameras, which are widely equipped in\nexisting production cars. First, in order to address large distortion problem\nin the fisheye images, Restricted Deformable Convolution (RDC) is proposed for\nsemantic segmentation, which can effectively model geometric transformations by\nlearning the shapes of convolutional filters conditioned on the input feature\nmap. Second, in order to obtain a large-scale training set of surround view\nimages, a novel method called zoom augmentation is proposed to transform\nconventional images to fisheye images. Finally, an RDC based semantic\nsegmentation model is built; the model is trained for real-world surround view\nimages through a multi-task learning architecture by combining real-world\nimages with transformed images. Experiments demonstrate the effectiveness of\nthe RDC to handle images with large distortions, and that the proposed approach\nshows a good performance using surround view cameras with the help of the\ntransformed images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 16:23:09 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 10:12:31 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 14:14:09 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Deng", "Liuyuan", ""], ["Yang", "Ming", ""], ["Li", "Hao", ""], ["Li", "Tianyi", ""], ["Hu", "Bing", ""], ["Wang", "Chunxiang", ""]]}, {"id": "1801.00820", "submitter": "Jindong Wang", "authors": "Jindong Wang and Yiqiang Chen and Lisha Hu and Xiaohui Peng and Philip\n  S. Yu", "title": "Stratified Transfer Learning for Cross-domain Activity Recognition", "comments": "10 pages; accepted by IEEE PerCom 2018; full paper. (camera-ready\n  version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In activity recognition, it is often expensive and time-consuming to acquire\nsufficient activity labels. To solve this problem, transfer learning leverages\nthe labeled samples from the source domain to annotate the target domain which\nhas few or none labels. Existing approaches typically consider learning a\nglobal domain shift while ignoring the intra-affinity between classes, which\nwill hinder the performance of the algorithms. In this paper, we propose a\nnovel and general cross-domain learning framework that can exploit the\nintra-affinity of classes to perform intra-class knowledge transfer. The\nproposed framework, referred to as Stratified Transfer Learning (STL), can\ndramatically improve the classification accuracy for cross-domain activity\nrecognition. Specifically, STL first obtains pseudo labels for the target\ndomain via majority voting technique. Then, it performs intra-class knowledge\ntransfer iteratively to transform both domains into the same subspaces.\nFinally, the labels of target domain are obtained via the second annotation. To\nevaluate the performance of STL, we conduct comprehensive experiments on three\nlarge public activity recognition datasets~(i.e. OPPORTUNITY, PAMAP2, and UCI\nDSADS), which demonstrates that STL significantly outperforms other\nstate-of-the-art methods w.r.t. classification accuracy (improvement of 7.68%).\nFurthermore, we extensively investigate the performance of STL across different\ndegrees of similarities and activity levels between domains. And we also\ndiscuss the potential of STL in other pervasive computing applications to\nprovide empirical experience for future research.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 06:18:31 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Hu", "Lisha", ""], ["Peng", "Xiaohui", ""], ["Yu", "Philip S.", ""]]}, {"id": "1801.00824", "submitter": "Chinmay Chinara", "authors": "Chinmay Chinara, Nishant Nath, Subhajeet Mishra, Sangram Keshari Sahoo\n  and Farida Ashraf Ali", "title": "A Novel Approach to Skew-Detection and Correction of English Alphabets\n  for OCR", "comments": "4 pages, 11 figures, 8 references, 2012 IEEE Student Conference on\n  Research and Development", "journal-ref": "IEEE SCOReD, 2012, Page 241-244", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition has been a challenging field in the advent of\ndigital computers. It is needed where information is to be readable both to\nhumans and machines. The process of OCR is composed of a set of pre and post\nprocessing steps that decide the level of accuracy of recognition. This paper\ndeals with one of the pre-processing steps involved in the OCR process i.e.\nSkew (Slant) Detection and Correction. The proposed algorithm implemented for\nskew-detection is termed as the COG (Centre of Gravity) method and for that of\nskew-correction is Sub-Pixel Shifting method. The algorithm has been kept\nsimple and optimized for efficient skew-detection and correction. The\nperformance analysis of the algorithm after testing has been aptly\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 20:25:59 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Chinara", "Chinmay", ""], ["Nath", "Nishant", ""], ["Mishra", "Subhajeet", ""], ["Sahoo", "Sangram Keshari", ""], ["Ali", "Farida Ashraf", ""]]}, {"id": "1801.00857", "submitter": "Alireza Karbalayghareh", "authors": "Alireza Karbalayghareh, Xiaoning Qian, and Edward R. Dougherty", "title": "Optimal Bayesian Transfer Learning", "comments": "IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions On Signal Processing, Vol. 66, No. 14, July 15,\n  2018", "doi": "10.1109/TSP.2018.2839583", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has recently attracted significant research attention, as\nit simultaneously learns from different source domains, which have plenty of\nlabeled data, and transfers the relevant knowledge to the target domain with\nlimited labeled data to improve the prediction performance. We propose a\nBayesian transfer learning framework where the source and target domains are\nrelated through the joint prior density of the model parameters. The modeling\nof joint prior densities enables better understanding of the \"transferability\"\nbetween domains. We define a joint Wishart density for the precision matrices\nof the Gaussian feature-label distributions in the source and target domains to\nact like a bridge that transfers the useful information of the source domain to\nhelp classification in the target domain by improving the target posteriors.\nUsing several theorems in multivariate statistics, the posteriors and posterior\npredictive densities are derived in closed forms with hypergeometric functions\nof matrix argument, leading to our novel closed-form and fast Optimal Bayesian\nTransfer Learning (OBTL) classifier. Experimental results on both synthetic and\nreal-world benchmark data confirm the superb performance of the OBTL compared\nto the other state-of-the-art transfer learning and domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 23:15:56 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 17:30:44 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Karbalayghareh", "Alireza", ""], ["Qian", "Xiaoning", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1801.00858", "submitter": "Varun Murali", "authors": "Varun Murali, Han-Pang Chiu, Supun Samarasekera, Rakesh (Teddy) Kumar", "title": "Utilizing Semantic Visual Landmarks for Precise Vehicle Navigation", "comments": "Published at IEEE ITSC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for integrating semantic information for\nvision-based vehicle navigation. Although vision-based vehicle navigation\nsystems using pre-mapped visual landmarks are capable of achieving submeter\nlevel accuracy in large-scale urban environment, a typical error source in this\ntype of systems comes from the presence of visual landmarks or features from\ntemporal objects in the environment, such as cars and pedestrians. We propose a\ngated factor graph framework to use semantic information associated with visual\nfeatures to make decisions on outlier/ inlier computation from three\nperspectives: the feature tracking process, the geo-referenced map building\nprocess, and the navigation system using pre-mapped landmarks. The class\ncategory that the visual feature belongs to is extracted from a pre-trained\ndeep learning network trained for semantic segmentation. The feasibility and\ngenerality of our approach is demonstrated by our implementations on top of two\nvision-based navigation systems. Experimental evaluations validate that the\ninjection of semantic information associated with visual landmarks using our\napproach achieves substantial improvements in accuracy on GPS-denied navigation\nsolutions for large-scale urban scenarios\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 23:18:48 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Murali", "Varun", "", "Teddy"], ["Chiu", "Han-Pang", "", "Teddy"], ["Samarasekera", "Supun", "", "Teddy"], ["Rakesh", "", "", "Teddy"], ["Kumar", "", ""]]}, {"id": "1801.00868", "submitter": "Alexander Kirillov", "authors": "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr\n  Doll\\'ar", "title": "Panoptic Segmentation", "comments": "accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a task we name panoptic segmentation (PS). Panoptic\nsegmentation unifies the typically distinct tasks of semantic segmentation\n(assign a class label to each pixel) and instance segmentation (detect and\nsegment each object instance). The proposed task requires generating a coherent\nscene segmentation that is rich and complete, an important step toward\nreal-world vision systems. While early work in computer vision addressed\nrelated image/scene parsing tasks, these are not currently popular, possibly\ndue to lack of appropriate metrics or associated recognition challenges. To\naddress this, we propose a novel panoptic quality (PQ) metric that captures\nperformance for all classes (stuff and things) in an interpretable and unified\nmanner. Using the proposed metric, we perform a rigorous study of both human\nand machine performance for PS on three existing datasets, revealing\ninteresting insights about the task. The aim of our work is to revive the\ninterest of the community in a more unified view of image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 00:21:31 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 09:50:50 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 18:17:40 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kirillov", "Alexander", ""], ["He", "Kaiming", ""], ["Girshick", "Ross", ""], ["Rother", "Carsten", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1801.00879", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Avirup Bhattacharyya, Prithaj Banerjee, Partha\n  Pratim Roy, Subrahmanyam Murala", "title": "A Novel Feature Descriptor for Image Retrieval by Combining Modified\n  Color Histogram and Diagonally Symmetric Co-occurrence Texture Pattern", "comments": "Preprint Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed a novel feature descriptors combining color\nand texture information collectively. In our proposed color descriptor\ncomponent, the inter-channel relationship between Hue (H) and Saturation (S)\nchannels in the HSV color space has been explored which was not done earlier.\nWe have quantized the H channel into a number of bins and performed the voting\nwith saturation values and vice versa by following a principle similar to that\nof the HOG descriptor, where orientation of the gradient is quantized into a\ncertain number of bins and voting is done with gradient magnitude. This helps\nus to study the nature of variation of saturation with variation in Hue and\nnature of variation of Hue with the variation in saturation. The texture\ncomponent of our descriptor considers the co-occurrence relationship between\nthe pixels symmetric about both the diagonals of a 3x3 window. Our work is\ninspired from the work done by Dubey et al.[1]. These two components, viz.\ncolor and texture information individually perform better than existing texture\nand color descriptors. Moreover, when concatenated the proposed descriptors\nprovide significant improvement over existing descriptors for content base\ncolor image retrieval. The proposed descriptor has been tested for image\nretrieval on five databases, including texture image databases - MIT VisTex\ndatabase and Salzburg texture database and natural scene databases Corel 1K,\nCorel 5K and Corel 10K. The precision and recall values experimented on these\ndatabases are compared with some state-of-art local patterns. The proposed\nmethod provided satisfactory results from the experiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 01:39:05 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Bhattacharyya", "Avirup", ""], ["Banerjee", "Prithaj", ""], ["Roy", "Partha Pratim", ""], ["Murala", "Subrahmanyam", ""]]}, {"id": "1801.00880", "submitter": "Mohammad Haft-Javaherian", "authors": "Mohammad Haft-Javaherian, Linjing Fang, Victorine Muse, Chris B.\n  Schaffer, Nozomi Nishimura, Mert R. Sabuncu", "title": "Deep convolutional neural networks for segmenting 3D in vivo multiphoton\n  images of vasculature in Alzheimer disease mouse models", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0213539", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The health and function of tissue rely on its vasculature network to provide\nreliable blood perfusion. Volumetric imaging approaches, such as multiphoton\nmicroscopy, are able to generate detailed 3D images of blood vessels that could\ncontribute to our understanding of the role of vascular structure in normal\nphysiology and in disease mechanisms. The segmentation of vessels, a core image\nanalysis problem, is a bottleneck that has prevented the systematic comparison\nof 3D vascular architecture across experimental populations. We explored the\nuse of convolutional neural networks to segment 3D vessels within volumetric in\nvivo images acquired by multiphoton microscopy. We evaluated different network\narchitectures and machine learning techniques in the context of this\nsegmentation problem. We show that our optimized convolutional neural network\narchitecture, which we call DeepVess, yielded a segmentation accuracy that was\nbetter than both the current state-of-the-art and a trained human annotator,\nwhile also being orders of magnitude faster. To explore the effects of aging\nand Alzheimer's disease on capillaries, we applied DeepVess to 3D images of\ncortical blood vessels in young and old mouse models of Alzheimer's disease and\nwild type littermates. We found little difference in the distribution of\ncapillary diameter or tortuosity between these groups, but did note a decrease\nin the number of longer capillary segments ($>75\\mu m$) in aged animals as\ncompared to young, in both wild type and Alzheimer's disease mouse models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 01:51:02 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 01:48:27 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 21:37:16 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2019 17:05:08 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Haft-Javaherian", "Mohammad", ""], ["Fang", "Linjing", ""], ["Muse", "Victorine", ""], ["Schaffer", "Chris B.", ""], ["Nishimura", "Nozomi", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1801.00881", "submitter": "He Lingxiao", "authors": "Lingxiao He, Jian Liang, Haiqing Li and Zhenan Sun", "title": "Deep Spatial Feature Reconstruction for Partial Person\n  Re-identification: Alignment-Free Approach", "comments": "8 pages, 11 figures, accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial person re-identification (re-id) is a challenging problem, where only\nseveral partial observations (images) of people are available for matching.\nHowever, few studies have provided flexible solutions to identifying a person\nin an image containing arbitrary part of the body. In this paper, we propose a\nfast and accurate matching method to address this problem. The proposed method\nleverages Fully Convolutional Network (FCN) to generate fix-sized spatial\nfeature maps such that pixel-level features are consistent. To match a pair of\nperson images of different sizes, a novel method called Deep Spatial feature\nReconstruction (DSR) is further developed to avoid explicit alignment.\nSpecifically, DSR exploits the reconstructing error from popular dictionary\nlearning models to calculate the similarity between different spatial feature\nmaps. In that way, we expect that the proposed FCN can decrease the similarity\nof coupled images from different persons and increase that from the same\nperson. Experimental results on two partial person datasets demonstrate the\nefficiency and effectiveness of the proposed method in comparison with several\nstate-of-the-art partial person re-id approaches. Additionally, DSR achieves\ncompetitive results on a benchmark person dataset Market1501 with 83.58\\%\nRank-1 accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 01:59:48 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 03:25:47 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 02:00:13 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["He", "Lingxiao", ""], ["Liang", "Jian", ""], ["Li", "Haiqing", ""], ["Sun", "Zhenan", ""]]}, {"id": "1801.00886", "submitter": "Sunrita Poddar", "authors": "Sunrita Poddar, Mathews Jacob", "title": "Recovery of Point Clouds on Surfaces: Application to Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for the recovery of points on a smooth surface in\nhigh-dimensional space, with application to dynamic imaging. We assume the\nsurface to be the zero-level set of a bandlimited function. We show that the\nexponential maps of the points on the surface satisfy annihilation relations,\nimplying that they lie in a finite dimensional subspace. We rely on nuclear\nnorm minimization of the maps to recover the points from noisy and undersampled\nmeasurements. Since this direct approach suffers from the curse of\ndimensionality, we introduce an iterative reweighted algorithm that uses the\n\"kernel trick\". The resulting algorithm has similarities to iterative\nalgorithms used in graph signal processing (GSP); this framework can be seen as\na continuous domain alternative to discrete GSP theory. The use of the\nalgorithm in recovering free breathing and ungated cardiac data shows the\npotential of this framework in practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 02:49:56 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Poddar", "Sunrita", ""], ["Jacob", "Mathews", ""]]}, {"id": "1801.00890", "submitter": "Sunrita Poddar", "authors": "Sunrita Poddar, Mathews Jacob", "title": "Recovery of Noisy Points on Band-limited Surfaces: Kernel Methods\n  Re-explained", "comments": "arXiv admin note: text overlap with arXiv:1801.00886", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a continuous domain framework for the recovery of points on a\nsurface in high dimensional space, represented as the zero-level set of a\nbandlimited function. We show that the exponential maps of the points on the\nsurface satisfy annihilation relations, implying that they lie in a finite\ndimensional subspace. The subspace properties are used to derive sampling\nconditions, which will guarantee the perfect recovery of the surface from\nfinite number of points. We rely on nuclear norm minimization to exploit the\nlow-rank structure of the maps to recover the points from noisy measurements.\nSince the direct estimation of the surface is computationally prohibitive in\nvery high dimensions, we propose an iterative reweighted algorithm using the\n\"kernel trick\". The iterative algorithm reveals deep links to Laplacian based\nalgorithms widely used in graph signal processing; the theory and the sampling\nconditions can serve as a basis for discrete-continuous domain processing of\nsignals on a graph.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 03:00:37 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 16:40:22 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Poddar", "Sunrita", ""], ["Jacob", "Mathews", ""]]}, {"id": "1801.00904", "submitter": "Jonghyun Choi", "authors": "Tae-Hoon Kim, Jonghyun Choi", "title": "ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks", "comments": "curricular learning, deep learning, deep q-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a curriculum or a syllabus for supervised learning and\ndeep reinforcement learning with deep neural networks by an attachable deep\nneural network, called ScreenerNet. Specifically, we learn a weight for each\nsample by jointly training the ScreenerNet and the main network in an\nend-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor\nrequires to remember the past learning history. We show the networks augmented\nwith the ScreenerNet achieve early convergence with better accuracy than the\nstate-of-the-art curricular learning methods in extensive experiments using\nthree popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a\nCart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend\nother curriculum learning methods such as Prioritized Experience Replay (PER)\nfor further accuracy improvement.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 05:49:37 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:48:12 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 22:51:01 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2018 06:46:02 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Kim", "Tae-Hoon", ""], ["Choi", "Jonghyun", ""]]}, {"id": "1801.00905", "submitter": "Mayank Singh", "authors": "Mayank Singh, Abhishek Sinha and Balaji Krishnamurthy", "title": "Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural networks have seen a huge surge in its adoption due to their\nability to provide high accuracy on various tasks. On the other hand, the\nexistence of adversarial examples have raised suspicions regarding the\ngeneralization capabilities of neural networks. In this work, we focus on the\nweight matrix learnt by the neural networks and hypothesize that ill\nconditioned weight matrix is one of the contributing factors in neural\nnetwork's susceptibility towards adversarial examples. For ensuring that the\nlearnt weight matrix's condition number remains sufficiently low, we suggest\nusing orthogonal regularizer. We show that this indeed helps in increasing the\nadversarial accuracy on MNIST and F-MNIST datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 05:52:52 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Singh", "Mayank", ""], ["Sinha", "Abhishek", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1801.00908", "submitter": "Siyang Li", "authors": "Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi, Qin Huang,\n  C.-C. Jay Kuo", "title": "Instance Embedding Transfer to Unsupervised Video Object Segmentation", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for unsupervised video object segmentation by\ntransferring the knowledge encapsulated in image-based instance embedding\nnetworks. The instance embedding network produces an embedding vector for each\npixel that enables identifying all pixels belonging to the same object. Though\ntrained on static images, the instance embeddings are stable over consecutive\nvideo frames, which allows us to link objects together over time. Thus, we\nadapt the instance networks trained on static images to video object\nsegmentation and incorporate the embeddings with objectness and optical flow\nfeatures, without model retraining or online fine-tuning. The proposed method\noutperforms state-of-the-art unsupervised segmentation methods in the DAVIS\ndataset and the FBMS dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 05:55:23 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 02:06:25 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Li", "Siyang", ""], ["Seybold", "Bryan", ""], ["Vorobyov", "Alexey", ""], ["Fathi", "Alireza", ""], ["Huang", "Qin", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1801.00926", "submitter": "Huazhu Fu", "authors": "Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang Liu, and\n  Xiaochun Cao", "title": "Joint Optic Disc and Cup Segmentation Based on Multi-label Deep Network\n  and Polar Transformation", "comments": "Project homepage: http://hzfu.github.io/proj_glaucoma_fundus.html ,\n  and Accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2791488", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is a chronic eye disease that leads to irreversible vision loss. The\ncup to disc ratio (CDR) plays an important role in the screening and diagnosis\nof glaucoma. Thus, the accurate and automatic segmentation of optic disc (OD)\nand optic cup (OC) from fundus images is a fundamental task. Most existing\nmethods segment them separately, and rely on hand-crafted visual feature from\nfundus images. In this paper, we propose a deep learning architecture, named\nM-Net, which solves the OD and OC segmentation jointly in a one-stage\nmulti-label system. The proposed M-Net mainly consists of multi-scale input\nlayer, U-shape convolutional network, side-output layer, and multi-label loss\nfunction. The multi-scale input layer constructs an image pyramid to achieve\nmultiple level receptive field sizes. The U-shape convolutional network is\nemployed as the main body network structure to learn the rich hierarchical\nrepresentation, while the side-output layer acts as an early classifier that\nproduces a companion local prediction map for different scale layers. Finally,\na multi-label loss function is proposed to generate the final segmentation map.\nFor improving the segmentation performance further, we also introduce the polar\ntransformation, which provides the representation of the original image in the\npolar coordinate system. The experiments show that our M-Net system achieves\nstate-of-the-art OD and OC segmentation result on ORIGA dataset.\nSimultaneously, the proposed method also obtains the satisfactory glaucoma\nscreening performances with calculated CDR value on both ORIGA and SCES\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 08:56:45 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 14:06:06 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 03:15:16 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Fu", "Huazhu", ""], ["Cheng", "Jun", ""], ["Xu", "Yanwu", ""], ["Wong", "Damon Wing Kee", ""], ["Liu", "Jiang", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1801.00939", "submitter": "Rocio Gonzalez-Diaz", "authors": "Rocio Gonzalez-Diaz, Maria-Jose Jimenez, Belen Medrano", "title": "Topological Tracking of Connected Components in Image Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology provides information about the lifetime of homology\nclasses along a filtration of cell complexes. Persistence barcode is a\ngraphical representation of such information. A filtration might be determined\nby time in a set of spatiotemporal data, but classical methods for computing\npersistent homology do not respect the fact that we can not move backwards in\ntime. In this paper, taking as input a time-varying sequence of two-dimensional\n(2D) binary digital images, we develop an algorithm for encoding, in the\nso-called {\\it spatiotemporal barcode}, lifetime of connected components (of\neither the foreground or background) that are moving in the image sequence over\ntime (this information may not coincide with the one provided by the\npersistence barcode). This way, given a connected component at a specific time\nin the sequence, we can track the component backwards in time until the moment\nit was born, by what we call a {\\it spatiotemporal path}. The main contribution\nof this paper with respect to our previous works lies in a new algorithm that\ncomputes spatiotemporal paths directly, valid for both foreground and\nbackground and developed in a general context, setting the ground for a future\nextension for tracking higher dimensional topological features in $nD$ binary\ndigital image sequences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 09:52:40 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Gonzalez-Diaz", "Rocio", ""], ["Jimenez", "Maria-Jose", ""], ["Medrano", "Belen", ""]]}, {"id": "1801.00968", "submitter": "Yi Xiao", "authors": "Yi Xiao, Xiang Cao, Xianyi Zhu, Renzhi Yang, Yan Zheng", "title": "Joint convolutional neural pyramid for depth map super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution depth map can be inferred from a low-resolution one with the\nguidance of an additional high-resolution texture map of the same scene.\nRecently, deep neural networks with large receptive fields are shown to benefit\napplications such as image completion. Our insight is that super resolution is\nsimilar to image completion, where only parts of the depth values are precisely\nknown. In this paper, we present a joint convolutional neural pyramid model\nwith large receptive fields for joint depth map super-resolution. Our model\nconsists of three sub-networks, two convolutional neural pyramids concatenated\nby a normal convolutional neural network. The convolutional neural pyramids\nextract information from large receptive fields of the depth map and guidance\nmap, while the convolutional neural network effectively transfers useful\nstructures of the guidance image to the depth image. Experimental results show\nthat our model outperforms existing state-of-the-art algorithms not only on\ndata pairs of RGB/depth images, but also on other data pairs like\ncolor/saliency and color-scribbles/colorized images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 11:53:34 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Xiao", "Yi", ""], ["Cao", "Xiang", ""], ["Zhu", "Xianyi", ""], ["Yang", "Renzhi", ""], ["Zheng", "Yan", ""]]}, {"id": "1801.01051", "submitter": "Junhui Wu", "authors": "Junhui Wu, Yun Ye, Yu Chen, Zhi Weng", "title": "Spot the Difference by Object Detection", "comments": "Tech Report, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet effective solution to a change\ndetection task that detects the difference between two images, which we call\n\"spot the difference\". Our approach uses CNN-based object detection by stacking\ntwo aligned images as input and considering the differences between the two\nimages as objects to detect. An early-merging architecture is used as the\nbackbone network. Our method is accurate, fast and robust while using very\ncheap annotation. We verify the proposed method on the task of change detection\nbetween the digital design and its photographic image of a book. Compared to\nverification based methods, our object detection based method outperforms other\nmethods by a large margin and gives extra information of location. We compress\nthe network and achieve 24 times acceleration while keeping the accuracy.\nBesides, as we synthesize the training data for detection using weakly labeled\nimages, our method does not need expensive bounding box annotation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 15:31:25 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Wu", "Junhui", ""], ["Ye", "Yun", ""], ["Chen", "Yu", ""], ["Weng", "Zhi", ""]]}, {"id": "1801.01054", "submitter": "Timur Khanipov", "authors": "Timur M. Khanipov", "title": "Computational complexity lower bounds of certain discrete Radon\n  transform approximations", "comments": "Created in ShareLaTeX, 11 pages, 2 PDF and 1 TikZ figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the computational model where only additions are allowed, the\n$\\Omega(n^2\\log n)$ lower bound on operations count with respect to image size\n$n\\times n$ is obtained for two types of the discrete Radon transform\nimplementations: the fast Hough transform and a generic strip pattern class\nwhich includes the classical Hough transform, implying the fast Hough transform\nalgorithm asymptotic optimality. The proofs are based on a specific result from\nthe boolean circuits complexity theory and are generalized for the case of\nboolean $\\vee$ binary operation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 15:41:08 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Khanipov", "Timur M.", ""]]}, {"id": "1801.01075", "submitter": "Abhimitra Meka", "authors": "Abhimitra Meka, Maxim Maximov, Michael Zollhoefer, Avishek Chatterjee,\n  Hans-Peter Seidel, Christian Richardt and Christian Theobalt", "title": "LIME: Live Intrinsic Material Estimation", "comments": "17 pages, Spotlight paper in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first end to end approach for real time material estimation\nfor general object shapes with uniform material that only requires a single\ncolor image as input. In addition to Lambertian surface properties, our\napproach fully automatically computes the specular albedo, material shininess,\nand a foreground segmentation. We tackle this challenging and ill posed inverse\nrendering problem using recent advances in image to image translation\ntechniques based on deep convolutional encoder decoder architectures. The\nunderlying core representations of our approach are specular shading, diffuse\nshading and mirror images, which allow to learn the effective and accurate\nseparation of diffuse and specular albedo. In addition, we propose a novel\nhighly efficient perceptual rendering loss that mimics real world image\nformation and obtains intermediate results even during run time. The estimation\nof material parameters at real time frame rates enables exciting mixed reality\napplications, such as seamless illumination consistent integration of virtual\nobjects into real world scenes, and virtual material cloning. We demonstrate\nour approach in a live setup, compare it to the state of the art, and\ndemonstrate its effectiveness through quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 16:55:31 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 15:43:19 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Meka", "Abhimitra", ""], ["Maximov", "Maxim", ""], ["Zollhoefer", "Michael", ""], ["Chatterjee", "Avishek", ""], ["Seidel", "Hans-Peter", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""]]}, {"id": "1801.01080", "submitter": "Pichao Wang", "authors": "Pichao Wang and Wanqing Li and Jun Wan and Philip Ogunbona and Xinwang\n  Liu", "title": "Cooperative Training of Deep Aggregation Networks for RGB-D Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel deep neural network training paradigm that exploits the conjoint\ninformation in multiple heterogeneous sources is proposed. Specifically, in a\nRGB-D based action recognition task, it cooperatively trains a single\nconvolutional neural network (named c-ConvNet) on both RGB visual features and\ndepth features, and deeply aggregates the two kinds of features for action\nrecognition. Differently from the conventional ConvNet that learns the deep\nseparable features for homogeneous modality-based classification with only one\nsoftmax loss function, the c-ConvNet enhances the discriminative power of the\ndeeply learned features and weakens the undesired modality discrepancy by\njointly optimizing a ranking loss and a softmax loss for both homogeneous and\nheterogeneous modalities. The ranking loss consists of intra-modality and\ncross-modality triplet losses, and it reduces both the intra-modality and\ncross-modality feature variations. Furthermore, the correlations between RGB\nand depth data are embedded in the c-ConvNet, and can be retrieved by either of\nthe modalities and contribute to the recognition in the case even only one of\nthe modalities is available. The proposed method was extensively evaluated on\ntwo large RGB-D action recognition datasets, ChaLearn LAP IsoGD and NTU RGB+D\ndatasets, and one small dataset, SYSU 3D HOI, and achieved state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 07:33:20 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Wan", "Jun", ""], ["Ogunbona", "Philip", ""], ["Liu", "Xinwang", ""]]}, {"id": "1801.01089", "submitter": "Gholamreza Anbarjafari", "authors": "Gholamreza Anbarjafari, Rain Eric Haamer, Iiris Lusi, Toomas Tikk, and\n  Lembit Valgma", "title": "3D Face Reconstruction with Region Based Best Fit Blending Using Mobile\n  Phone for Virtual Reality Based Social Media", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of virtual reality (VR) is exponentially increasing and due to that\nmany researchers has started to work on developing new VR based social media.\nFor this purpose it is important to have an avatar of the users which look like\nthem to be easily generated by the devices which are accessible, such as mobile\nphone. In this paper, we propose a novel method of recreating a 3D human face\nmodel captured with a phone camera image or video data. The method focuses more\non model shape than texture in order to make the face recognizable. We detect\n68 facial feature points and use them to separate a face into four regions. For\neach area the best fitting models are found and are further morphed combined to\nfind the best fitting models for each area. These are then combined and further\nmorphed in order to restore the original facial proportions. We also present a\nmethod of texturing the resulting model, where the aforementioned feature\npoints are used to generate a texture for the resulting model\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 07:46:17 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Anbarjafari", "Gholamreza", ""], ["Haamer", "Rain Eric", ""], ["Lusi", "Iiris", ""], ["Tikk", "Toomas", ""], ["Valgma", "Lembit", ""]]}, {"id": "1801.01198", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Hadi Kazemi, Seyed Mehdi Iranmanesh, Jeremi Dawson,\n  Nasser M. Nasrabadi", "title": "Fingerprint Distortion Rectification using Deep Convolutional Neural\n  Networks", "comments": "Accepted at ICB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastic distortion of fingerprints has a negative effect on the performance\nof fingerprint recognition systems. This negative effect brings inconvenience\nto users in authentication applications. However, in the negative recognition\nscenario where users may intentionally distort their fingerprints, this can be\na serious problem since distortion will prevent recognition system from\nidentifying malicious users. Current methods aimed at addressing this problem\nstill have limitations. They are often not accurate because they estimate\ndistortion parameters based on the ridge frequency map and orientation map of\ninput samples, which are not reliable due to distortion. Secondly, they are not\nefficient and requiring significant computation time to rectify samples. In\nthis paper, we develop a rectification model based on a Deep Convolutional\nNeural Network (DCNN) to accurately estimate distortion parameters from the\ninput image. Using a comprehensive database of synthetic distorted samples, the\nDCNN learns to accurately estimate distortion bases ten times faster than the\ndictionary search methods used in the previous approaches. Evaluating the\nproposed method on public databases of distorted samples shows that it can\nsignificantly improve the matching performance of distorted samples.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 22:43:35 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Dabouei", "Ali", ""], ["Kazemi", "Hadi", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Dawson", "Jeremi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1801.01235", "submitter": "Christopher Holder", "authors": "Christopher J. Holder, Toby P. Breckon, Xiong Wei", "title": "Depth Not Needed - An Evaluation of RGB-D Feature Encodings for Off-Road\n  Scene Understanding by Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding for autonomous vehicles is a challenging computer vision\ntask, with recent advances in convolutional neural networks (CNNs) achieving\nresults that notably surpass prior traditional feature driven approaches.\nHowever, limited work investigates the application of such methods either\nwithin the highly unstructured off-road environment or to RGBD input data. In\nthis work, we take an existing CNN architecture designed to perform semantic\nsegmentation of RGB images of urban road scenes, then adapt and retrain it to\nperform the same task with multichannel RGBD images obtained under a range of\nchallenging off-road conditions. We compare two different stereo matching\nalgorithms and five different methods of encoding depth information, including\ndisparity, local normal orientation and HHA (horizontal disparity, height above\nground plane, angle with gravity), to create a total of ten experimental\nvariations of our dataset, each of which is used to train and test a CNN so\nthat classification performance can be evaluated against a CNN trained using\nstandard RGB input.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 03:03:45 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Holder", "Christopher J.", ""], ["Breckon", "Toby P.", ""], ["Wei", "Xiong", ""]]}, {"id": "1801.01258", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Jingu Kang, and Jong Chul Ye", "title": "Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For homeland and transportation security applications, 2D X-ray explosive\ndetection system (EDS) have been widely used, but they have limitations in\nrecognizing 3D shape of the hidden objects. Among various types of 3D computed\ntomography (CT) systems to address this issue, this paper is interested in a\nstationary CT using fixed X-ray sources and detectors. However, due to the\nlimited number of projection views, analytic reconstruction algorithms produce\nsevere streaking artifacts. Inspired by recent success of deep learning\napproach for sparse view CT reconstruction, here we propose a novel image and\nsinogram domain deep learning architecture for 3D reconstruction from very\nsparse view measurement. The algorithm has been tested with the real data from\na prototype 9-view dual energy stationary CT EDS carry-on baggage scanner\ndeveloped by GEMSS Medical Systems, Korea, which confirms the superior\nreconstruction performance over the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:35:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Han", "Yoseob", ""], ["Kang", "Jingu", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1801.01260", "submitter": "Defa Zhu", "authors": "Si Liu, Yao Sun, Defa Zhu, Guanghui Ren, Yu Chen, Jiashi Feng and\n  Jizhong Han", "title": "Cross-domain Human Parsing via Adversarial Feature and Label Adaptation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human parsing has been extensively studied recently due to its wide\napplications in many important scenarios. Mainstream fashion parsing models\nfocus on parsing the high-resolution and clean images. However, directly\napplying the parsers trained on benchmarks to a particular application scenario\nin the wild, e.g., a canteen, airport or workplace, often gives\nnon-satisfactory performance due to domain shift. In this paper, we explore a\nnew and challenging cross-domain human parsing problem: taking the benchmark\ndataset with extensive pixel-wise labeling as the source domain, how to obtain\na satisfactory parser on a new target domain without requiring any additional\nmanual labeling? To this end, we propose a novel and efficient cross-domain\nhuman parsing model to bridge the cross-domain differences in terms of visual\nappearance and environment conditions and fully exploit commonalities across\ndomains. Our proposed model explicitly learns a feature compensation network,\nwhich is specialized for mitigating the cross-domain differences. A\ndiscriminative feature adversarial network is introduced to supervise the\nfeature compensation to effectively reduce the discrepancy between feature\ndistributions of two domains. Besides, our model also introduces a structured\nlabel adversarial network to guide the parsing results of the target domain to\nfollow the high-order relationships of the structured labels shared across\ndomains. The proposed framework is end-to-end trainable, practical and scalable\nin real applications. Extensive experiments are conducted where LIP dataset is\nthe source domain and 4 different datasets including surveillance videos,\nmovies and runway shows are evaluated as target domains. The results\nconsistently confirm data efficiency and performance advantages of the proposed\nmethod for the cross-domain human parsing problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:55:59 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 03:25:07 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Liu", "Si", ""], ["Sun", "Yao", ""], ["Zhu", "Defa", ""], ["Ren", "Guanghui", ""], ["Chen", "Yu", ""], ["Feng", "Jiashi", ""], ["Han", "Jizhong", ""]]}, {"id": "1801.01262", "submitter": "Yi Zhang", "authors": "Yi Zhang, Houjun Huang, Haifeng Zhang, Liao Ni, Wei Xu, Nasir Uddin\n  Ahmed, Md. Shakil Ahmed, Yilun Jin, Yingjie Chen, Jingxuan Wen and Wenxin Li", "title": "ICFVR 2017: 3rd International Competition on Finger Vein Recognition", "comments": "8 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, finger vein recognition has become an important sub-field in\nbiometrics and been applied to real-world applications. The development of\nfinger vein recognition algorithms heavily depends on large-scale real-world\ndata sets. In order to motivate research on finger vein recognition, we\nreleased the largest finger vein data set up to now and hold finger vein\nrecognition competitions based on our data set every year. In 2017,\nInternational Competition on Finger Vein Recognition(ICFVR) is held jointly\nwith IJCB 2017. 11 teams registered and 10 of them joined the final evaluation.\nThe winner of this year dramatically improved the EER from 2.64% to 0.483%\ncompared to the winner of last year. In this paper, we introduce the process\nand results of ICFVR 2017 and give insights on development of state-of-art\nfinger vein recognition algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 07:14:32 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Zhang", "Yi", ""], ["Huang", "Houjun", ""], ["Zhang", "Haifeng", ""], ["Ni", "Liao", ""], ["Xu", "Wei", ""], ["Ahmed", "Nasir Uddin", ""], ["Ahmed", "Md. Shakil", ""], ["Jin", "Yilun", ""], ["Chen", "Yingjie", ""], ["Wen", "Jingxuan", ""], ["Li", "Wenxin", ""]]}, {"id": "1801.01281", "submitter": "Matthieu Grard", "authors": "Matthieu Grard, Romain Br\\'egier, Florian Sella, Emmanuel\n  Dellandr\\'ea, Liming Chen", "title": "Object segmentation in depth maps with one user click and a\n  synthetically trained fully convolutional network", "comments": "This is a pre-print of an article published in Human Friendly\n  Robotics, 10th International Workshop, Springer Proceedings in Advanced\n  Robotics, vol 7. The final authenticated version is available online at:\n  https://doi.org/10.1007/978-3-319-89327-3\\_16, Springer Proceedings in\n  Advanced Robotics, Siciliano Bruno, Khatib Oussama, In press, Human Friendly\n  Robotics, 10th International Workshop, 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more and more household objects built on planned obsolescence and\nconsumed by a fast-growing population, hazardous waste recycling has become a\ncritical challenge. Given the large variability of household waste, current\nrecycling platforms mostly rely on human operators to analyze the scene,\ntypically composed of many object instances piled up in bulk. Helping them by\nrobotizing the unitary extraction is a key challenge to speed up this tedious\nprocess. Whereas supervised deep learning has proven very efficient for such\nobject-level scene understanding, e.g., generic object detection and\nsegmentation in everyday scenes, it however requires large sets of per-pixel\nlabeled images, that are hardly available for numerous application contexts,\nincluding industrial robotics. We thus propose a step towards a practical\ninteractive application for generating an object-oriented robotic grasp,\nrequiring as inputs only one depth map of the scene and one user click on the\nnext object to extract. More precisely, we address in this paper the middle\nissue of object seg-mentation in top views of piles of bulk objects given a\npixel location, namely seed, provided interactively by a human operator. We\npropose a twofold framework for generating edge-driven instance segments.\nFirst, we repurpose a state-of-the-art fully convolutional object contour\ndetector for seed-based instance segmentation by introducing the notion of\nedge-mask duality with a novel patch-free and contour-oriented loss function.\nSecond, we train one model using only synthetic scenes, instead of manually\nlabeled training data. Our experimental results show that considering edge-mask\nduality for training an encoder-decoder network, as we suggest, outperforms a\nstate-of-the-art patch-based network in the present application context.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 09:13:20 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 09:06:37 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Grard", "Matthieu", ""], ["Br\u00e9gier", "Romain", ""], ["Sella", "Florian", ""], ["Dellandr\u00e9a", "Emmanuel", ""], ["Chen", "Liming", ""]]}, {"id": "1801.01315", "submitter": "Dan Deng", "authors": "Dan Deng, Haifeng Liu, Xuelong Li, Deng Cai", "title": "PixelLink: Detecting Scene Text via Instance Segmentation", "comments": "AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art scene text detection algorithms are deep learning based\nmethods that depend on bounding box regression and perform at least two kinds\nof predictions: text/non-text classification and location regression.\nRegression plays a key role in the acquisition of bounding boxes in these\nmethods, but it is not indispensable because text/non-text prediction can also\nbe considered as a kind of semantic segmentation that contains full location\ninformation in itself. However, text instances in scene images often lie very\nclose to each other, making them very difficult to separate via semantic\nsegmentation. Therefore, instance segmentation is needed to address this\nproblem. In this paper, PixelLink, a novel scene text detection algorithm based\non instance segmentation, is proposed. Text instances are first segmented out\nby linking pixels within the same instance together. Text bounding boxes are\nthen extracted directly from the segmentation result without location\nregression. Experiments show that, compared with regression-based methods,\nPixelLink can achieve better or comparable performance on several benchmarks,\nwhile requiring many fewer training iterations and less training data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 11:48:21 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Deng", "Dan", ""], ["Liu", "Haifeng", ""], ["Li", "Xuelong", ""], ["Cai", "Deng", ""]]}, {"id": "1801.01316", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Mu Jung Cho, Anupriya Gagneja, Xiao Yang, Miriam\n  Brinberg, Katie Roehrick, Sagnik Ray Choudhury, Nilam Ram, Byron Reeves and\n  C. Lee Giles", "title": "Text Extraction and Retrieval from Smartphone Screenshots: Building a\n  Repository for Life in Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daily engagement in life experiences is increasingly interwoven with mobile\ndevice use. Screen capture at the scale of seconds is being used in behavioral\nstudies and to implement \"just-in-time\" health interventions. The increasing\npsychological breadth of digital information will continue to make the actual\nscreens that people view a preferred if not required source of data about life\nexperiences. Effective and efficient Information Extraction and Retrieval from\ndigital screenshots is a crucial prerequisite to successful use of screen data.\nIn this paper, we present the experimental workflow we exploited to: (i)\npre-process a unique collection of screen captures, (ii) extract unstructured\ntext embedded in the images, (iii) organize image text and metadata based on a\nstructured schema, (iv) index the resulting document collection, and (v) allow\nfor Image Retrieval through a dedicated vertical search engine application. The\nadopted procedure integrates different open source libraries for traditional\nimage processing, Optical Character Recognition (OCR), and Image Retrieval. Our\naim is to assess whether and how state-of-the-art methodologies can be applied\nto this novel data set. We show how combining OpenCV-based pre-processing\nmodules with a Long short-term memory (LSTM) based release of Tesseract OCR,\nwithout ad hoc training, led to a 74% character-level accuracy of the extracted\ntext. Further, we used the processed repository as baseline for a dedicated\nImage Retrieval system, for the immediate use and application for behavioral\nand prevention scientists. We discuss issues of Text Information Extraction and\nRetrieval that are particular to the screenshot image case and suggest\nimportant future work.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 11:51:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chiatti", "Agnese", ""], ["Cho", "Mu Jung", ""], ["Gagneja", "Anupriya", ""], ["Yang", "Xiao", ""], ["Brinberg", "Miriam", ""], ["Roehrick", "Katie", ""], ["Choudhury", "Sagnik Ray", ""], ["Ram", "Nilam", ""], ["Reeves", "Byron", ""], ["Giles", "C. Lee", ""]]}, {"id": "1801.01317", "submitter": "Tao Yang", "authors": "Tao Yang, Yan Wu, Junqiao Zhao, Linting Guan", "title": "Semantic Segmentation via Highly Fused Convolutional Network with\n  Multiple Soft Cost Functions", "comments": "16 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is one of the most challenged tasks in computer\nvision. In this paper, we propose a highly fused convolutional network, which\nconsists of three parts: feature downsampling, combined feature upsampling and\nmultiple predictions. We adopt a strategy of multiple steps of upsampling and\ncombined feature maps in pooling layers with its corresponding unpooling\nlayers. Then we bring out multiple pre-outputs, each pre-output is generated\nfrom an unpooling layer by one-step upsampling. Finally, we concatenate these\npre-outputs to get the final output. As a result, our proposed network makes\nhighly use of the feature information by fusing and reusing feature maps. In\naddition, when training our model, we add multiple soft cost functions on\npre-outputs and final outputs. In this way, we can reduce the loss reduction\nwhen the loss is back propagated. We evaluate our model on three major\nsegmentation datasets: CamVid, PASCAL VOC and ADE20K. We achieve a\nstate-of-the-art performance on CamVid dataset, as well as considerable\nimprovements on PASCAL VOC dataset and ADE20K dataset\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 11:55:55 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Yang", "Tao", ""], ["Wu", "Yan", ""], ["Zhao", "Junqiao", ""], ["Guan", "Linting", ""]]}, {"id": "1801.01397", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan", "title": "Implementation of Deep Convolutional Neural Network in Multi-class\n  Categorical Image Classification", "comments": "22 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1712.04711", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional Neural Networks has been implemented in many complex machine\nlearning takes such as image classification, object identification, autonomous\nvehicle and robotic vision tasks. However, ConvNet architecture efficiency and\naccuracy depend on a large number of fac- tors. Also, the complex architecture\nrequires a significant amount of data to train and involves with a large number\nof hyperparameters that increases the computational expenses and difficul-\nties. Hence, it is necessary to address the limitations and techniques to\novercome the barriers to ensure that the architecture performs well in complex\nvisual tasks. This article is intended to develop an efficient ConvNet\narchitecture for multi-class image categorical classification applica- tion. In\nthe development of the architecture, large pool of grey scale images are taken\nas input information images and split into training and test datasets. The\nnumerously available technique is implemented to reduce the overfitting and\npoor generalization of the network. The hyperpa- rameters of determined by\nBayesian Optimization with Gaussian Process prior algorithm. ReLu non-linear\nactivation function is implemented after the convolutional layers. Max pooling\nop- eration is carried out to downsampling the data points in pooling layers.\nCross-entropy loss function is used to measure the performance of the\narchitecture where the softmax is used in the classification layer. Mini-batch\ngradient descent with Adam optimizer algorithm is used for backpropagation.\nDeveloped architecture is validated with confusion matrix and classification\nreport.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 15:29:44 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Murugan", "Pushparaja", ""]]}, {"id": "1801.01402", "submitter": "Devesh Walawalkar", "authors": "Devesh Walawalkar", "title": "A fully automated framework for lung tumour detection, segmentation and\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early and correct diagnosis is a very important aspect of cancer treatment.\nDetection of tumour in Computed Tomography scan is a tedious and tricky task\nwhich requires expert knowledge and a lot of human working hours. As small\nhuman error is present in any work he does, it is possible that a CT scan could\nbe misdiagnosed causing the patient to become terminal. This paper introduces a\nnovel fully automated framework which helps to detect and segment tumour, if\npresent in a lung CT scan series. It also provides useful analysis of the\ndetected tumour such as its approximate volume, centre location and more. The\nframework provides a single click solution which analyses all CT images of a\nsingle patient series in one go. It helps to reduce the work of manually going\nthrough each CT slice and provides quicker and more accurate tumour diagnosis.\nIt makes use of customized image processing and image segmentation methods, to\ndetect and segment the prospective tumour region from the CT scan. It then uses\na trained ensemble classifier to correctly classify the segmented region as\nbeing tumour or not. Tumour analysis further computed can then be used to\ndetermine malignity of the tumour. With an accuracy of 98.14%, the implemented\nframework can be used in various practical scenarios, capable of eliminating\nneed of any expert pathologist intervention.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 15:26:24 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Walawalkar", "Devesh", ""]]}, {"id": "1801.01415", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes, Andrew\n  Zisserman", "title": "What have we learned from deep representations for action recognition?", "comments": "This document is best viewed in Adobe Reader where figures play on\n  click. Supplementary material can be downloaded at\n  http://feichtenhofer.github.io/action_vis.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the success of deep models has led to their deployment in all areas of\ncomputer vision, it is increasingly important to understand how these\nrepresentations work and what they are capturing. In this paper, we shed light\non deep spatiotemporal representations by visualizing what two-stream models\nhave learned in order to recognize actions in video. We show that local\ndetectors for appearance and motion objects arise to form distributed\nrepresentations for recognizing human actions. Key observations include the\nfollowing. First, cross-stream fusion enables the learning of true\nspatiotemporal features rather than simply separate appearance and motion\nfeatures. Second, the networks can learn local representations that are highly\nclass specific, but also generic representations that can serve a range of\nclasses. Third, throughout the hierarchy of the network, features become more\nabstract and show increasing invariance to aspects of the data that are\nunimportant to desired distinctions (e.g. motion patterns across various\nspeeds). Fourth, visualizations can be used not only to shed light on learned\nrepresentations, but also to reveal idiosyncracies of training data and to\nexplain failure cases of the system.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 15:47:47 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Pinz", "Axel", ""], ["Wildes", "Richard P.", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1801.01430", "submitter": "Anurag Ghosh", "authors": "Anurag Ghosh and C.V. Jawahar", "title": "SmartTennisTV: Automatic indexing of tennis videos", "comments": "10 pages, 4 figures, NCVPRIPG 2017 Accepted Paper (Best Paper Award\n  Winner)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a score based indexing approach for tennis\nvideos. Given a broadcast tennis video (BTV), we index all the video segments\nwith their scores to create a navigable and searchable match. Our approach\ntemporally segments the rallies in the video and then recognizes the scores\nfrom each of the segments, before refining the scores using the knowledge of\nthe tennis scoring system. We finally build an interface to effortlessly\nretrieve and view the relevant video segments by also automatically tagging the\nsegmented rallies with human accessible tags such as 'fault' and 'deuce'. The\nefficiency of our approach is demonstrated on BTV's from two major tennis\ntournaments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 16:38:55 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Ghosh", "Anurag", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1801.01442", "submitter": "Rithesh Kumar", "authors": "Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexandre de Brebisson,\n  Yoshua Bengio", "title": "ObamaNet: Photo-realistic lip-sync from text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ObamaNet, the first architecture that generates both audio and\nsynchronized photo-realistic lip-sync videos from any new text. Contrary to\nother published lip-sync approaches, ours is only composed of fully trainable\nneural modules and does not rely on any traditional computer graphics methods.\nMore precisely, we use three main modules: a text-to-speech network based on\nChar2Wav, a time-delayed LSTM to generate mouth-keypoints synced to the audio,\nand a network based on Pix2Pix to generate the video frames conditioned on the\nkeypoints.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 16:18:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kumar", "Rithesh", ""], ["Sotelo", "Jose", ""], ["Kumar", "Kundan", ""], ["de Brebisson", "Alexandre", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1801.01443", "submitter": "Wellington Pinheiro dos Santos", "authors": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel\n  Guilhermino da Silva Filho", "title": "A semi-supervised fuzzy GrowCut algorithm to segment and classify\n  regions of interest of mammographic images", "comments": null, "journal-ref": "Expert Systems With Applications, 65 (2016), 116-126", "doi": "10.1016/j.eswa.2016.08.016", "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, breast cancer is the most common\nform of cancer in women. It is the second leading cause of death among women\nround the world, becoming the most fatal form of cancer. Mammographic image\nsegmentation is a fundamental task to support image analysis and diagnosis,\ntaking into account shape analysis of mammary lesions and their borders.\nHowever, mammogram segmentation is a very hard process, once it is highly\ndependent on the types of mammary tissues. In this work we present a new\nsemi-supervised segmentation algorithm based on the modification of the GrowCut\nalgorithm to perform automatic mammographic image segmentation once a region of\ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian\nmembership functions to modify the evolution rule of the original GrowCut\nalgorithm, in order to estimate the uncertainty of a pixel being object or\nbackground. The main impact of the proposed method is the significant reduction\nof expert effort in the initialization of seed points of GrowCut to perform\naccurate segmentation, once it removes the need of selection of background\nseeds. We also constructed an automatic point selection process based on the\nsimulated annealing optimization method, avoiding the need of human\nintervention. The proposed approach was qualitatively compared with other\nstate-of-the-art segmentation techniques, considering the shape of segmented\nregions. In order to validate our proposal, we built an image classifier using\na classical multilayer perceptron. We used Zernike moments to extract segmented\nimage features. This analysis employed 685 mammograms from IRMA breast cancer\ndatabase, using fat and fibroid tissues. Results show that the proposed\ntechnique could achieve a classification rate of 91.28\\% for fat tissues,\nevidencing the feasibility of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 17:31:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Cordeiro", "Filipe Rolim", ""], ["Santos", "Wellington Pinheiro dos", ""], ["Filho", "Abel Guilhermino da Silva", ""]]}, {"id": "1801.01444", "submitter": "Shu Liu", "authors": "Guang Chen, Shu Liu, Kejia Ren, Zhongnan Qu, Changhong Fu, Gereon\n  Hinz, Alois Knoll", "title": "Deep Anticipation: Light Weight Intelligent Mobile Sensing in IoT by\n  Recurrent Architecture", "comments": "7 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of IoT era is shaping the future of mobile services.\nAdvanced communication technology enables a heterogeneous connectivity where\nmobile devices broadcast information to everything. Mobile applications such as\nrobotics and vehicles connecting to cloud and surroundings transfer the\nshort-range on-board sensor perception system to long-range mobile-sensing\nperception system. However, the mobile sensing perception brings new challenges\nfor how to efficiently analyze and intelligently interpret the deluge of IoT\ndata in mission- critical services. In this article, we model the challenges as\nlatency, packet loss and measurement noise which severely deteriorate the\nreliability and quality of IoT data. We integrate the artificial intelligence\ninto IoT to tackle these challenges. We propose a novel architecture that\nleverages recurrent neural networks (RNN) and Kalman filtering to anticipate\nmotions and interac- tions between objects. The basic idea is to learn\nenvironment dynamics by recurrent networks. To improve the robustness of IoT\ncommunication, we use the idea of Kalman filtering and deploy a prediction and\ncorrection step. In this way, the architecture learns to develop a biased\nbelief between prediction and measurement in the different situation. We\ndemonstrate our approach with synthetic and real-world datasets with noise that\nmimics the challenges of IoT communications. Our method brings a new level of\nIoT intelligence. It is also lightweight compared to other state-of-the-art\nconvolutional recurrent architecture and is ideally suitable for the\nresource-limited mobile applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 02:45:04 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 20:34:00 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Chen", "Guang", ""], ["Liu", "Shu", ""], ["Ren", "Kejia", ""], ["Qu", "Zhongnan", ""], ["Fu", "Changhong", ""], ["Hinz", "Gereon", ""], ["Knoll", "Alois", ""]]}, {"id": "1801.01446", "submitter": "Matthias Nie{\\ss}ner", "authors": "Justus Thies, Michael Zollh\\\"ofer, Matthias Nie{\\ss}ner", "title": "IMU2Face: Real-time Gesture-driven Facial Reenactment", "comments": "https://youtu.be/UXGodiDAqiE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present IMU2Face, a gesture-driven facial reenactment system. To this end,\nwe combine recent advances in facial motion capture and inertial measurement\nunits (IMUs) to control the facial expressions of a person in a target video\nbased on intuitive hand gestures. IMUs are omnipresent, since modern\nsmart-phones, smart-watches and drones integrate such sensors, e.g., for\nchanging the orientation of the screen content, counting steps, or for flight\nstabilization. Face tracking and reenactment is based on the state-of-the-art\nreal-time Face2Face facial reenactment system. Instead of transferring facial\nexpressions from a source to a target actor, we employ an IMU to track the hand\ngestures of a source actor and use its orientation to modify the target actor's\nexpressions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 16:26:45 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1801.01449", "submitter": "Takumi Moriya", "authors": "Takumi Moriya, Kazuyuki Saito, Hiroya Tanaka", "title": "3D Surface-to-Structure Translation using Deep Convolutional Networks", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our demonstration shows a system that estimates internal body structures from\n3D surface models using deep convolutional neural networks trained on CT\n(computed tomography) images of the human body. To take pictures of structures\ninside the body, we need to use a CT scanner or an MRI (Magnetic Resonance\nImaging) scanner. However, assuming that the mutual information between outer\nshape of the body and its inner structure is not zero, we can obtain an\napproximate internal structure from a 3D surface model based on MRI and CT\nimage database. This suggests that we could know where and what kind of disease\na person is likely to have in his/her body simply by 3D scanning surface of the\nbody. As a first prototype, we developed a system for estimating internal body\nstructures from surface models based on Visible Human Project DICOM CT Datasets\nfrom the University of Iowa Magnetic Resonance Research Facility.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 08:16:06 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Moriya", "Takumi", ""], ["Saito", "Kazuyuki", ""], ["Tanaka", "Hiroya", ""]]}, {"id": "1801.01450", "submitter": "Eric Kauderer-Abrams", "authors": "Eric Kauderer-Abrams", "title": "Quantifying Translation-Invariance in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in object recognition is the development of image\nrepresentations that are invariant to common transformations such as\ntranslation, rotation, and small deformations. There are multiple hypotheses\nregarding the source of translation invariance in CNNs. One idea is that\ntranslation invariance is due to the increasing receptive field size of neurons\nin successive convolution layers. Another possibility is that invariance is due\nto the pooling operation. We develop a simple a tool, the\ntranslation-sensitivity map, which we use to visualize and quantify the\ntranslation-invariance of various architectures. We obtain the surprising\nresult that architectural choices such as the number of pooling layers and the\nconvolution filter size have only a secondary effect on the\ntranslation-invariance of a network. Our analysis identifies training data\naugmentation as the most important factor in obtaining translation-invariant\nrepresentations of images using convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 21:05:13 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kauderer-Abrams", "Eric", ""]]}, {"id": "1801.01451", "submitter": "Andrew Kiruluta", "authors": "Andrew Kiruluta", "title": "Reducing Deep Network Complexity with Fourier Transform Methods", "comments": "mistake in tensorflow code with test data leakage into training set\n  leading to model over fitting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel way that uses shallow densely connected neuron network\narchitectures to achieve superior performance to convolution based neural\nnetworks (CNNs) approaches with the added benefits of lower computation burden\nrequiring dramatically less training examples to achieve high prediction\naccuracy ($>98\\%$). The advantages of our proposed method is demonstrated in\nresults on benchmark datasets which show significant performance gain over\nexisting state-of-the-art results on MNIST, CIFAR-10 and CIFAR-100. By Fourier\ntransforming the inputs, each point in the training sample then has a\nrepresentational energy of all the weighted information from every other point.\nThe consequence of using this input is a reduced complexity neuron network,\nreduced computation load and the lifting of the requirement for a large number\nof training examples to achieve high classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 20:30:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 12:09:37 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Kiruluta", "Andrew", ""]]}, {"id": "1801.01452", "submitter": "Weiwen Wu", "authors": "Weiwen Wu, Yanbo Zhang, Qian Wang, Fenglin Liu, Peijun Chen and\n  Hengyong Yu", "title": "Low-dose spectral CT reconstruction using L0 image gradient and tensor\n  dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral computed tomography (CT) has a great superiority in lesion\ndetection, tissue characterization and material decomposition. To further\nextend its potential clinical applications, in this work, we propose an\nimproved tensor dictionary learning method for low-dose spectral CT\nreconstruction with a constraint of image gradient L0-norm, which is named as\nL0TDL. The L0TDL method inherits the advantages of tensor dictionary learning\n(TDL) by employing the similarity of spectral CT images. On the other hand, by\nintroducing the L0-norm constraint in gradient image domain, the proposed\nmethod emphasizes the spatial sparsity to overcome the weakness of TDL on\npreserving edge information. The alternative direction minimization method\n(ADMM) is employed to solve the proposed method. Both numerical simulations and\nreal mouse studies are perform to evaluate the proposed method. The results\nshow that the proposed L0TDL method outperforms other competing methods, such\nas total variation (TV) minimization, TV with low rank (TV+LR), and TDL\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 00:24:04 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 15:34:00 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wu", "Weiwen", ""], ["Zhang", "Yanbo", ""], ["Wang", "Qian", ""], ["Liu", "Fenglin", ""], ["Chen", "Peijun", ""], ["Yu", "Hengyong", ""]]}, {"id": "1801.01453", "submitter": "Mark Kibanov", "authors": "Mark Kibanov, Martin Becker, Juergen Mueller, Martin Atzmueller,\n  Andreas Hotho, Gerd Stumme", "title": "Adaptive kNN using Expected Accuracy for Classification of Geo-Spatial\n  Data", "comments": null, "journal-ref": null, "doi": "10.1145/3167132.3167226", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The k-Nearest Neighbor (kNN) classification approach is conceptually simple -\nyet widely applied since it often performs well in practical applications.\nHowever, using a global constant k does not always provide an optimal solution,\ne.g., for datasets with an irregular density distribution of data points. This\npaper proposes an adaptive kNN classifier where k is chosen dynamically for\neach instance (point) to be classified, such that the expected accuracy of\nclassification is maximized. We define the expected accuracy as the accuracy of\na set of structurally similar observations. An arbitrary similarity function\ncan be used to find these observations. We introduce and evaluate different\nsimilarity functions. For the evaluation, we use five different classification\ntasks based on geo-spatial data. Each classification task consists of (tens of)\nthousands of items. We demonstrate, that the presented expected accuracy\nmeasures can be a good estimator for kNN performance, and the proposed adaptive\nkNN classifier outperforms common kNN and previously introduced adaptive kNN\nalgorithms. Also, we show that the range of considered k can be significantly\nreduced to speed up the algorithm without negative influence on classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 14:09:06 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kibanov", "Mark", ""], ["Becker", "Martin", ""], ["Mueller", "Juergen", ""], ["Atzmueller", "Martin", ""], ["Hotho", "Andreas", ""], ["Stumme", "Gerd", ""]]}, {"id": "1801.01454", "submitter": "Guillermo Gallego", "authors": "Guillermo Gallego, Elias Mueggler, Peter Sturm", "title": "Translation of \"Zur Ermittlung eines Objektes aus zwei Perspektiven mit\n  innerer Orientierung\" by Erwin Kruppa (1913)", "comments": "16 pages, 1 figure. Granted reproduction permission from the\n  publishing house of the Austrian Academy of Sciences\n  (https://verlag.oeaw.ac.at/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erwin Kruppa's 1913 paper, Erwin Kruppa, \"Zur Ermittlung eines Objektes aus\nzwei Perspektiven mit innerer Orientierung\", Sitzungsberichte der\nMathematisch-Naturwissenschaftlichen Kaiserlichen Akademie der Wissenschaften,\nVol. 122 (1913), pp. 1939-1948, which may be translated as \"To determine a 3D\nobject from two perspective views with known inner orientation\", is a landmark\npaper in Computer Vision because it provides the first five-point algorithm for\nrelative pose estimation. Kruppa showed that (a finite number of solutions for)\nthe relative pose between two calibrated images of a rigid object can be\ncomputed from five point matches between the images. Kruppa's work also gained\nattention in the topic of camera self-calibration, as presented in (Maybank and\nFaugeras, 1992). Since the paper is still relevant today (more than a hundred\ncitations within the last ten years) and the paper is not available online, we\nordered a copy from the German National Library in Frankfurt and provide an\nEnglish translation along with the German original. We also adapt the\nterminology to a modern jargon and provide some clarifications (highlighted in\nsans-serif font). For a historical review of geometric computer vision, the\nreader is referred to the recent survey paper (Sturm, 2011).\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 12:25:16 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Gallego", "Guillermo", ""], ["Mueggler", "Elias", ""], ["Sturm", "Peter", ""]]}, {"id": "1801.01466", "submitter": "Rahul Mitra", "authors": "Rahul Mitra and Nehal Doiphode and Utkarsh Gautam and Sanath Narayan\n  and Shuaib Ahmed and Sharat Chandran and Arjun Jain", "title": "A Large Dataset for Improving Patch Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new dataset for learning local image descriptors which can be\nused for significantly improved patch matching. Our proposed dataset consists\nof an order of magnitude more number of scenes, images, and positive and\nnegative correspondences compared to the currently available Multi-View Stereo\n(MVS) dataset from Brown et al. The new dataset also has better coverage of the\noverall viewpoint, scale, and lighting changes in comparison to the MVS\ndataset. Our dataset also provides supplementary information like RGB patches\nwith scale and rotations values, and intrinsic and extrinsic camera parameters\nwhich as shown later can be used to customize training data as per application.\nWe train an existing state-of-the-art model on our dataset and evaluate on\npublicly available benchmarks such as HPatches dataset and Strecha et\nal.\\cite{strecha} to quantify the image descriptor performance. Experimental\nevaluations show that the descriptors trained using our proposed dataset\noutperform the current state-of-the-art descriptors trained on MVS by 8%, 4%\nand 10% on matching, verification and retrieval tasks respectively on the\nHPatches dataset. Similarly on the Strecha dataset, we see an improvement of\n3-5% for the matching task in non-planar scenes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 17:37:45 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 05:53:21 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 14:31:04 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Mitra", "Rahul", ""], ["Doiphode", "Nehal", ""], ["Gautam", "Utkarsh", ""], ["Narayan", "Sanath", ""], ["Ahmed", "Shuaib", ""], ["Chandran", "Sharat", ""], ["Jain", "Arjun", ""]]}, {"id": "1801.01486", "submitter": "Seyed Mehdi Iranmanesh", "authors": "Seyed Mehdi Iranmanesh, Ali Dabouei, Hadi Kazemi, Nasser M. Nasrabadi", "title": "Deep Cross Polarimetric Thermal-to-visible Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep coupled learning frame- work to address the\nproblem of matching polarimetric ther- mal face photos against a gallery of\nvisible faces. Polariza- tion state information of thermal faces provides the\nmiss- ing textural and geometrics details in the thermal face im- agery which\nexist in visible spectrum. we propose a coupled deep neural network\narchitecture which leverages relatively large visible and thermal datasets to\novercome the problem of overfitting and eventually we train it by a\npolarimetric thermal face dataset which is the first of its kind. The pro-\nposed architecture is able to make full use of the polari- metric thermal\ninformation to train a deep model compared to the conventional shallow\nthermal-to-visible face recogni- tion methods. Proposed coupled deep neural\nnetwork also finds global discriminative features in a nonlinear embed- ding\nspace to relate the polarimetric thermal faces to their corresponding visible\nfaces. The results show the superior- ity of our method compared to the\nstate-of-the-art models in cross thermal-to-visible face recognition\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 18:41:27 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Iranmanesh", "Seyed Mehdi", ""], ["Dabouei", "Ali", ""], ["Kazemi", "Hadi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1801.01557", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Clayton P. Alexander, Mathias Unberath, Giacomo Taylor,\n  Sing Chun Lee, Bernhard Fuerst, Alex Johnson, Greg Osgood, Russell H. Taylor,\n  Harpal Khanuja, Mehran Armand, Nassir Navab", "title": "Plan in 2D, execute in 3D: An augmented reality solution for cup\n  placement in total hip arthroplasty", "comments": null, "journal-ref": "J. Med. Imag. 5(2), 021205 (2018)", "doi": "10.1117/1.JMI.5.2.021205", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibly achieving proper implant alignment is a critical step in total\nhip arthroplasty (THA) procedures that has been shown to substantially affect\npatient outcome. In current practice, correct alignment of the acetabular cup\nis verified in C-arm X-ray images that are acquired in an anterior-posterior\n(AP) view. Favorable surgical outcome is, therefore, heavily dependent on the\nsurgeon's experience in understanding the 3D orientation of a hemispheric\nimplant from 2D AP projection images. This work proposes an easy to use\nintra-operative component planning system based on two C-arm X-ray images that\nis combined with 3D augmented reality (AR) visualization that simplifies\nimpactor and cup placement according to the planning by providing a real-time\nRGBD data overlay. We evaluate the feasibility of our system in a user study\ncomprising four orthopedic surgeons at the Johns Hopkins Hospital, and also\nreport errors in translation, anteversion, and abduction as low as 1.98 mm,\n1.10 degrees, and 0.53 degrees, respectively. The promising performance of this\nAR solution shows that deploying this system could eliminate the need for\nexcessive radiation, simplify the intervention, and enable reproducibly\naccurate placement of acetabular implants.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 22:00:20 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Fotouhi", "Javad", ""], ["Alexander", "Clayton P.", ""], ["Unberath", "Mathias", ""], ["Taylor", "Giacomo", ""], ["Lee", "Sing Chun", ""], ["Fuerst", "Bernhard", ""], ["Johnson", "Alex", ""], ["Osgood", "Greg", ""], ["Taylor", "Russell H.", ""], ["Khanuja", "Harpal", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "1801.01560", "submitter": "Mathias Unberath", "authors": "Sebastian Andress, Alex Johnson, Mathias Unberath, Alexander Winkler,\n  Kevin Yu, Javad Fotouhi, Simon Weidert, Greg Osgood, Nassir Navab", "title": "On-the-fly Augmented Reality for Orthopaedic Surgery Using a Multi-Modal\n  Fiducial", "comments": "S. Andress, A. Johnson, M. Unberath, and A. Winkler have contributed\n  equally and are listed in alphabetical order", "journal-ref": "J. Med. Imag. 5(2), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluoroscopic X-ray guidance is a cornerstone for percutaneous orthopaedic\nsurgical procedures. However, two-dimensional observations of the\nthree-dimensional anatomy suffer from the effects of projective simplification.\nConsequently, many X-ray images from various orientations need to be acquired\nfor the surgeon to accurately assess the spatial relations between the\npatient's anatomy and the surgical tools. In this paper, we present an\non-the-fly surgical support system that provides guidance using augmented\nreality and can be used in quasi-unprepared operating rooms. The proposed\nsystem builds upon a multi-modality marker and simultaneous localization and\nmapping technique to co-calibrate an optical see-through head mounted display\nto a C-arm fluoroscopy system. Then, annotations on the 2D X-ray images can be\nrendered as virtual objects in 3D providing surgical guidance. We\nquantitatively evaluate the components of the proposed system, and finally,\ndesign a feasibility study on a semi-anthropomorphic phantom. The accuracy of\nour system was comparable to the traditional image-guided technique while\nsubstantially reducing the number of acquired X-ray images as well as procedure\ntime. Our promising results encourage further research on the interaction\nbetween virtual and real objects, that we believe will directly benefit the\nproposed method. Further, we would like to explore the capabilities of our\non-the-fly augmented reality support system in a larger study directed towards\ncommon orthopaedic interventions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 22:02:33 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Andress", "Sebastian", ""], ["Johnson", "Alex", ""], ["Unberath", "Mathias", ""], ["Winkler", "Alexander", ""], ["Yu", "Kevin", ""], ["Fotouhi", "Javad", ""], ["Weidert", "Simon", ""], ["Osgood", "Greg", ""], ["Navab", "Nassir", ""]]}, {"id": "1801.01572", "submitter": "YangQuan Chen Prof.", "authors": "Guoxiang Zhang and YangQuan Chen", "title": "LoopSmart: Smart Visual SLAM Through Surface Loop Closure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visual simultaneous localization and mapping (SLAM) framework of\nclosing surface loops. It combines both sparse feature matching and dense\nsurface alignment. Sparse feature matching is used for visual odometry and\nglobally camera pose fine-tuning when dense loops are detected, while dense\nsurface alignment is the way of closing large loops and solving surface\nmismatching problem. To achieve smart dense surface loop closure, a highly\nefficient CUDA-based global point cloud registration method and a map content\ndependent loop verification method are proposed. We run extensive experiments\non different datasets, our method outperforms state-of-the-art ones in terms of\nboth camera trajectory and surface reconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 22:53:07 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Zhang", "Guoxiang", ""], ["Chen", "YangQuan", ""]]}, {"id": "1801.01582", "submitter": "Dengxin Dai", "authors": "Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool", "title": "Object Referring in Videos with Language and Human Gaze", "comments": "Accepted to CVPR 2018, 10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of object referring (OR) i.e. to localize a target\nobject in a visual scene coming with a language description. Humans perceive\nthe world more as continued video snippets than as static images, and describe\nobjects not only by their appearance, but also by their spatio-temporal context\nand motion features. Humans also gaze at the object when they issue a referring\nexpression. Existing works for OR mostly focus on static images only, which\nfall short in providing many such cues. This paper addresses OR in videos with\nlanguage and human gaze. To that end, we present a new video dataset for OR,\nwith 30, 000 objects over 5, 000 stereo video sequences annotated for their\ndescriptions and gaze. We further propose a novel network model for OR in\nvideos, by integrating appearance, motion, gaze, and spatio-temporal context\ninto one network. Experimental results show that our method effectively\nutilizes motion cues, human gaze, and spatio-temporal context. Our method\noutperforms previousOR methods. For dataset and code, please refer\nhttps://people.ee.ethz.ch/~arunv/ORGaze.html.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 23:31:20 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 15:38:07 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1801.01596", "submitter": "Jiazhuo Wang", "authors": "Jiazhuo Wang, Jason Xu, Xuejun Wang", "title": "Combination of Hyperband and Bayesian Optimization for Hyperparameter\n  Optimization in Deep Learning", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved impressive results on many problems. However, it\nrequires high degree of expertise or a lot of experience to tune well the\nhyperparameters, and such manual tuning process is likely to be biased.\nMoreover, it is not practical to try out as many different hyperparameter\nconfigurations in deep learning as in other machine learning scenarios, because\nevaluating each single hyperparameter configuration in deep learning would mean\ntraining a deep neural network, which usually takes quite long time. Hyperband\nalgorithm achieves state-of-the-art performance on various hyperparameter\noptimization problems in the field of deep learning. However, Hyperband\nalgorithm does not utilize history information of previous explored\nhyperparameter configurations, thus the solution found is suboptimal. We\npropose to combine Hyperband algorithm with Bayesian optimization (which does\nnot ignore history when sampling next trial configuration). Experimental\nresults show that our combination approach is superior to other hyperparameter\noptimization approaches including Hyperband algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 01:00:03 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wang", "Jiazhuo", ""], ["Xu", "Jason", ""], ["Wang", "Xuejun", ""]]}, {"id": "1801.01615", "submitter": "Hanbyul Joo", "authors": "Hanbyul Joo, Tomas Simon, Yaser Sheikh", "title": "Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and\n  Bodies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified deformation model for the markerless capture of multiple\nscales of human movement, including facial expressions, body motion, and hand\ngestures. An initial model is generated by locally stitching together models of\nthe individual parts of the human body, which we refer to as the \"Frankenstein\"\nmodel. This model enables the full expression of part movements, including face\nand hands by a single seamless model. Using a large-scale capture of people\nwearing everyday clothes, we optimize the Frankenstein model to create \"Adam\".\nAdam is a calibrated model that shares the same skeleton hierarchy as the\ninitial model but can express hair and clothing geometry, making it directly\nusable for fitting people as they normally appear in everyday life. Finally, we\ndemonstrate the use of these models for total motion tracking, simultaneously\ncapturing the large-scale body movements and the subtle face and hand motion of\na social group of people.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 02:41:54 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Joo", "Hanbyul", ""], ["Simon", "Tomas", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1801.01627", "submitter": "Nibaran Das", "authors": "Soumya Ukil, Swarnendu Ghosh, Sk Md Obaidullah, K. C. Santosh, Kaushik\n  Roy, and Nibaran Das", "title": "Deep learning for word-level handwritten Indic script identification", "comments": "11 pages, 6 figures , 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method that uses convolutional neural networks (CNNs) for\nfeature extraction. Not just limited to conventional spatial domain\nrepresentation, we use multilevel 2D discrete Haar wavelet transform, where\nimage representations are scaled to a variety of different sizes. These are\nthen used to train different CNNs to select features. To be precise, we use 10\ndifferent CNNs that select a set of 10240 features, i.e. 1024/CNN. With this,\n11 different handwritten scripts are identified, where 1K words per script are\nused. In our test, we have achieved the maximum script identification rate of\n94.73% using multi-layer perceptron (MLP). Our results outperform the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 04:52:55 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Ukil", "Soumya", ""], ["Ghosh", "Swarnendu", ""], ["Obaidullah", "Sk Md", ""], ["Santosh", "K. C.", ""], ["Roy", "Kaushik", ""], ["Das", "Nibaran", ""]]}, {"id": "1801.01632", "submitter": "Songlin Zhai", "authors": "Guibing Guo, Songlin Zhai, Fajie Yuan, Yuan Liu, Xingwei Wang", "title": "VSE-ens: Visual-Semantic Embeddings with Efficient Negative Sampling", "comments": "Published by The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jointing visual-semantic embeddings (VSE) have become a research hotpot for\nthe task of image annotation, which suffers from the issue of semantic gap,\ni.e., the gap between images' visual features (low-level) and labels' semantic\nfeatures (high-level). This issue will be even more challenging if visual\nfeatures cannot be retrieved from images, that is, when images are only denoted\nby numerical IDs as given in some real datasets. The typical way of existing\nVSE methods is to perform a uniform sampling method for negative examples that\nviolate the ranking order against positive examples, which requires a\ntime-consuming search in the whole label space. In this paper, we propose a\nfast adaptive negative sampler that can work well in the settings of no figure\npixels available. Our sampling strategy is to choose the negative examples that\nare most likely to meet the requirements of violation according to the latent\nfactors of images. In this way, our approach can linearly scale up to large\ndatasets. The experiments demonstrate that our approach converges 5.02x faster\nthan the state-of-the-art approaches on OpenImages, 2.5x on IAPR-TCI2 and 2.06x\non NUS-WIDE datasets, as well as better ranking accuracy across datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 05:19:37 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 08:58:05 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Guo", "Guibing", ""], ["Zhai", "Songlin", ""], ["Yuan", "Fajie", ""], ["Liu", "Yuan", ""], ["Wang", "Xingwei", ""]]}, {"id": "1801.01671", "submitter": "Xuebo Liu", "authors": "Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan", "title": "FOTS: Fast Oriented Text Spotting with a Unified Network", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incidental scene text spotting is considered one of the most difficult and\nvaluable challenges in the document analysis community. Most existing methods\ntreat text detection and recognition as separate tasks. In this work, we\npropose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS)\nnetwork for simultaneous detection and recognition, sharing computation and\nvisual information among the two complementary tasks. Specially, RoIRotate is\nintroduced to share convolutional features between detection and recognition.\nBenefiting from convolution sharing strategy, our FOTS has little computation\noverhead compared to baseline text detection network, and the joint training\nmethod learns more generic features to make our method perform better than\nthese two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR\n2013 datasets demonstrate that the proposed method outperforms state-of-the-art\nmethods significantly, which further allows us to develop the first real-time\noriented text spotting system which surpasses all previous state-of-the-art\nresults by more than 5% on ICDAR 2015 text spotting task while keeping 22.6\nfps.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 08:41:57 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 11:30:21 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Liu", "Xuebo", ""], ["Liang", "Ding", ""], ["Yan", "Shi", ""], ["Chen", "Dagui", ""], ["Qiao", "Yu", ""], ["Yan", "Junjie", ""]]}, {"id": "1801.01687", "submitter": "Xingcheng Zhang", "authors": "Xingcheng Zhang, Lei Yang, Junjie Yan, Dahua Lin", "title": "Accelerated Training for Massive Classification via Dynamic Class\n  Selection", "comments": "8 pages, 6 figures, AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive classification, a classification task defined over a vast number of\nclasses (hundreds of thousands or even millions), has become an essential part\nof many real-world systems, such as face recognition. Existing methods,\nincluding the deep networks that achieved remarkable success in recent years,\nwere mostly devised for problems with a moderate number of classes. They would\nmeet with substantial difficulties, e.g. excessive memory demand and\ncomputational cost, when applied to massive problems. We present a new method\nto tackle this problem. This method can efficiently and accurately identify a\nsmall number of \"active classes\" for each mini-batch, based on a set of dynamic\nclass hierarchies constructed on the fly. We also develop an adaptive\nallocation scheme thereon, which leads to a better tradeoff between performance\nand cost. On several large-scale benchmarks, our method significantly reduces\nthe training cost and memory demand, while maintaining competitive performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:11:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Zhang", "Xingcheng", ""], ["Yang", "Lei", ""], ["Yan", "Junjie", ""], ["Lin", "Dahua", ""]]}, {"id": "1801.01693", "submitter": "Bernhard Kainz", "authors": "Keyang Zhou and Bernhard Kainz", "title": "Efficient Image Evidence Analysis of CNN Classification Results", "comments": "14 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) define the current state-of-the-art for\nimage recognition. With their emerging popularity, especially for critical\napplications like medical image analysis or self-driving cars, confirmability\nis becoming an issue. The black-box nature of trained predictors make it\ndifficult to trace failure cases or to understand the internal reasoning\nprocesses leading to results. In this paper we introduce a novel efficient\nmethod to visualise evidence that lead to decisions in CNNs. In contrast to\nnetwork fixation or saliency map methods, our method is able to illustrate the\nevidence for or against a classifier's decision in input pixel space\napproximately 10 times faster than previous methods. We also show that our\napproach is less prone to noise and can focus on the most relevant input\nregions, thus making it more accurate and interpretable. Moreover, by making\nsimplifications we link our method with other visualisation methods, providing\na general explanation for gradient-based visualisation techniques. We believe\nthat our work makes network introspection more feasible for debugging and\nunderstanding deep convolutional networks. This will increase trust between\nhumans and deep learning models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:31:21 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Zhou", "Keyang", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1801.01695", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin, Lucian Stefanita Grigore, Valentina Emilia\n  Balas, Cristina Madalina Noaica, Ionut Axenie, Justinian Popa, Cristian\n  Munteanu, Victor Stroescu, Ionut Manu, Alexandru Herea, Kartal Horasanli,\n  Iulia Maria Motoc (ACSTL Cross-Sensor Comparison Competition Team 2013)", "title": "Cross-Sensor Iris Recognition: LG4000-to-LG2200 Comparison", "comments": "Pages: 18; Figures: 21; Iris Codes Comparisons: O(1E9); Results\n  obtained by `ACSTL Cross-Sensor Comparison Competition Team 2013` during the\n  Cross-Sensor Comparison Competition 2013 organized within the IEEE-BTAS-2013\n  Conference", "journal-ref": null, "doi": null, "report-no": "Technical Report 460 / 24-07-2013, Rev. No. 4 / 30-09-2013,\n  University of South-East Europe Lumina, Bucharest, ROMANIA", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-sensor comparison experimental results reported here show that the\nprocedure defined and simulated during the Cross-Sensor Comparison Competition\n2013 by our team for migrating / upgrading LG2200 based to LG4000 based\nbiometric systems leads to better LG4000-to-LG2200 cross-sensor iris\nrecognition results than previously reported, both in terms of user comfort and\nin terms of system safety. On the other hand, LG2200-to-LG400 migration/upgrade\nprocedure defined and implemented by us is applicable to solve interoperability\nissues between LG2200 based and LG4000 based systems, but also to other pairs\nof systems having the same shift in the quality of acquired images.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:33:08 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Grigore", "Lucian Stefanita", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Balas", "Valentina Emilia", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Noaica", "Cristina Madalina", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Axenie", "Ionut", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Popa", "Justinian", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Munteanu", "Cristian", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Stroescu", "Victor", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Manu", "Ionut", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Herea", "Alexandru", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Horasanli", "Kartal", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"], ["Motoc", "Iulia Maria", "", "ACSTL Cross-Sensor Comparison Competition Team 2013"]]}, {"id": "1801.01698", "submitter": "Mohammad Kazem Moghimi", "authors": "Mohammad Mahdi Moghimi, Maryam Nayeri, Majid Pourahmadi, Mohammad\n  Kazem Moghimi", "title": "Moving Vehicle Detection Using AdaBoost and Haar-Like Feature in\n  Surveillance Videos", "comments": "13 pages", "journal-ref": "International Journal of Imaging and Robotics, vol. 18, no. 1, pp.\n  94-106 (2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection is a technology which its aim is to locate and show the\nvehicle size in digital images. In this technology, vehicles are detected in\npresence of other things like trees and buildings. It has an important role in\nmany computer vision applications such as vehicle tracking, analyzing the\ntraffic scene and efficient traffic management. In this paper, vehicles\ndetected based on the boosting technique by Viola Jones. Our proposed system is\ntested in some real scenes of surveillance videos with different light\nconditions. The experimental results show that the accuracy,completeness, and\nquality of the proposed vehicle detection method are better than the previous\ntechniques (about 94%, 92%, and 87%, respectively). Thus, our proposed approach\nis robust and efficient to detect vehicles in surveillance videos and their\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:41:02 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Moghimi", "Mohammad Mahdi", ""], ["Nayeri", "Maryam", ""], ["Pourahmadi", "Majid", ""], ["Moghimi", "Mohammad Kazem", ""]]}, {"id": "1801.01726", "submitter": "Peilun Li", "authors": "Peilun Li, Xiaodan Liang, Daoyuan Jia, Eric P. Xing", "title": "Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption", "comments": "In proceedings of BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in vision tasks (e.g., segmentation) highly depend on the\navailability of large-scale real-world image annotations obtained by cumbersome\nhuman labors. Moreover, the perception performance often drops significantly\nfor new scenarios, due to the poor generalization capability of models trained\non limited and biased annotations. In this work, we resort to transfer\nknowledge from automatically rendered scene annotations in virtual-world to\nfacilitate real-world visual tasks. Although virtual-world annotations can be\nideally diverse and unlimited, the discrepant data distributions between\nvirtual and real-world make it challenging for knowledge transferring. We thus\npropose a novel Semantic-aware Grad-GAN (SG-GAN) to perform virtual-to-real\ndomain adaption with the ability of retaining vital semantic information.\nBeyond the simple holistic color/texture transformation achieved by prior\nworks, SG-GAN successfully personalizes the appearance adaption for each\nsemantic region in order to preserve their key characteristic for better\nrecognition. It presents two main contributions to traditional GANs: 1) a soft\ngradient-sensitive objective for keeping semantic boundaries; 2) a\nsemantic-aware discriminator for validating the fidelity of personalized\nadaptions with respect to each semantic region. Qualitative and quantitative\nexperiments demonstrate the superiority of our SG-GAN in scene adaption over\nstate-of-the-art GANs. Further evaluations on semantic segmentation on\nCityscapes show using adapted virtual images by SG-GAN dramatically improves\nsegmentation performance than original virtual data. We release our code at\nhttps://github.com/Peilun-Li/SG-GAN.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 11:54:27 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 06:03:56 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Li", "Peilun", ""], ["Liang", "Xiaodan", ""], ["Jia", "Daoyuan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1801.01760", "submitter": "Yang Wang", "authors": "Chengyuan Zhang, Lin Wu, Yang Wang", "title": "Crossing Generative Adversarial Networks for Cross-View Person\n  Re-identification", "comments": "12 pages. arXiv admin note: text overlap with arXiv:1702.03431 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (\\textit{re-id}) refers to matching pedestrians\nacross disjoint yet non-overlapping camera views. The most effective way to\nmatch these pedestrians undertaking significant visual variations is to seek\nreliably invariant features that can describe the person of interest\nfaithfully. Most of existing methods are presented in a supervised manner to\nproduce discriminative features by relying on labeled paired images in\ncorrespondence. However, annotating pair-wise images is prohibitively expensive\nin labors, and thus not practical in large-scale networked cameras. Moreover,\nseeking comparable representations across camera views demands a flexible model\nto address the complex distributions of images. In this work, we study the\nco-occurrence statistic patterns between pairs of images, and propose to\ncrossing Generative Adversarial Network (Cross-GAN) for learning a joint\ndistribution for cross-image representations in a unsupervised manner. Given a\npair of person images, the proposed model consists of the variational\nauto-encoder to encode the pair into respective latent variables, a proposed\ncross-view alignment to reduce the view disparity, and an adversarial layer to\nseek the joint distribution of latent representations. The learned latent\nrepresentations are well-aligned to reflect the co-occurrence patterns of\npaired images. We empirically evaluate the proposed model against challenging\ndatasets, and our results show the importance of joint invariant features in\nimproving matching rates of person re-id with comparison to semi/unsupervised\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 03:52:15 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Wu", "Lin", ""], ["Wang", "Yang", ""]]}, {"id": "1801.01769", "submitter": "Suichan Li", "authors": "Suichan Li", "title": "3D-DETNet: a Single Stage Video-Based Vehicle Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based vehicle detection has received considerable attention over the\nlast ten years and there are many deep learning based detection methods which\ncan be applied to it. However, these methods are devised for still images and\napplying them for video vehicle detection directly always obtains poor\nperformance. In this work, we propose a new single-stage video-based vehicle\ndetector integrated with 3DCovNet and focal loss, called 3D-DETNet. Draw\nsupport from 3D Convolution network and focal loss, our method has ability to\ncapture motion information and is more suitable to detect vehicle in video than\nother single-stage methods devised for static images. The multiple video frames\nare initially fed to 3D-DETNet to generate multiple spatial feature maps, then\nsub-model 3DConvNet takes spatial feature maps as input to capture temporal\ninformation which is fed to final fully convolution model for predicting\nlocations of vehicles in video frames. We evaluate our method on UA-DETAC\nvehicle detection dataset and our 3D-DETNet yields best performance and keeps a\nhigher detection speed of 26 fps compared with other competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 14:38:14 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 09:06:07 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Li", "Suichan", ""]]}, {"id": "1801.01847", "submitter": "Cam Bermudez", "authors": "Camilo Bermudez, Andrew J. Plassard, Larry T. Davis, Allen T. Newton,\n  Susan M Resnick, Bennett A. Landman", "title": "Learning Implicit Brain MRI Manifolds with Deep Learning", "comments": "SPIE Medical Imaging 2018", "journal-ref": null, "doi": "10.1117/12.2293515", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in image processing and neuroimaging is to extract\nquantitative information from the acquired images in order to make observations\nabout the presence of disease or markers of development in populations. Having\na lowdimensional manifold of an image allows for easier statistical comparisons\nbetween groups and the synthesis of group representatives. Previous studies\nhave sought to identify the best mapping of brain MRI to a low-dimensional\nmanifold, but have been limited by assumptions of explicit similarity measures.\nIn this work, we use deep learning techniques to investigate implicit manifolds\nof normal brains and generate new, high-quality images. We explore implicit\nmanifolds by addressing the problems of image synthesis and image denoising as\nimportant tools in manifold learning. First, we propose the unsupervised\nsynthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN)\nby learning from 528 examples of 2D axial slices of brain MRI. Synthesized\nimages were first shown to be unique by performing a crosscorrelation with the\ntraining set. Real and synthesized images were then assessed in a blinded\nmanner by two imaging experts providing an image quality score of 1-5. The\nquality score of the synthetic image showed substantial overlap with that of\nthe real images. Moreover, we use an autoencoder with skip connections for\nimage denoising, showing that the proposed method results in higher PSNR than\nFSL SUSAN after denoising. This work shows the power of artificial networks to\nsynthesize realistic imaging data, which can be used to improve image\nprocessing techniques and provide a quantitative framework to structural\nchanges in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 17:24:37 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Bermudez", "Camilo", ""], ["Plassard", "Andrew J.", ""], ["Davis", "Larry T.", ""], ["Newton", "Allen T.", ""], ["Resnick", "Susan M", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1801.01849", "submitter": "Kai Zhao", "authors": "Kai Zhao, Wei Shen, Shanghua Gao, Dandan Li, Ming-Ming Cheng", "title": "Hi-Fi: Hierarchical Feature Integration for Skeleton Detection", "comments": "IJCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural images, the scales (thickness) of object skeletons may\ndramatically vary among objects and object parts, making object skeleton\ndetection a challenging problem. We present a new convolutional neural network\n(CNN) architecture by introducing a novel hierarchical feature integration\nmechanism, named Hi-Fi, to address the skeleton detection problem. The proposed\nCNN-based approach has a powerful multi-scale feature integration ability that\nintrinsically captures high-level semantics from deeper layers as well as\nlow-level details from shallower layers. % By hierarchically integrating\ndifferent CNN feature levels with bidirectional guidance, our approach (1)\nenables mutual refinement across features of different levels, and (2)\npossesses the strong ability to capture both rich object context and\nhigh-resolution details. Experimental results show that our method\nsignificantly outperforms the state-of-the-art methods in terms of effectively\nfusing features from very different scales, as evidenced by a considerable\nperformance improvement on several benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 17:34:18 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 13:50:32 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 08:12:08 GMT"}, {"version": "v4", "created": "Tue, 7 Aug 2018 08:07:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhao", "Kai", ""], ["Shen", "Wei", ""], ["Gao", "Shanghua", ""], ["Li", "Dandan", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1801.01933", "submitter": "Mao-Chuang Yeh", "authors": "Mao-Chuang Yeh and Shuai Tang", "title": "Improved Style Transfer by Respecting Inter-layer Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular series of style transfer methods apply a style to a content image\nby controlling mean and covariance of values in early layers of a feature\nstack. This is insufficient for transferring styles that have strong structure\nacross spatial scales like, e.g., textures where dots lie on long curves. This\npaper demonstrates that controlling inter-layer correlations yields visible\nimprovements in style transfer methods. We achieve this control by computing\ncross-layer, rather than within-layer, gram matrices. We find that (a)\ncross-layer gram matrices are sufficient to control within-layer statistics.\nInter-layer correlations improves style transfer and texture synthesis. The\npaper shows numerous examples on \"hard\" real style transfer problems (e.g. long\nscale and hierarchical patterns); (b) a fast approximate style transfer method\ncan control cross-layer gram matrices; (c) we demonstrate that multiplicative,\nrather than additive style and content loss, results in very good style\ntransfer. Multiplicative loss produces a visible emphasis on boundaries, and\nmeans that one hyper-parameter can be eliminated.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 22:41:11 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yeh", "Mao-Chuang", ""], ["Tang", "Shuai", ""]]}, {"id": "1801.01949", "submitter": "Di Tang", "authors": "Di Tang, Zhe Zhou, Yinqian Zhang, Kehuan Zhang", "title": "Face Flashing: a Secure Liveness Detection Protocol based on Light\n  Reflections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face authentication systems are becoming increasingly prevalent, especially\nwith the rapid development of Deep Learning technologies. However, human facial\ninformation is easy to be captured and reproduced, which makes face\nauthentication systems vulnerable to various attacks. Liveness detection is an\nimportant defense technique to prevent such attacks, but existing solutions did\nnot provide clear and strong security guarantees, especially in terms of time.\n  To overcome these limitations, we propose a new liveness detection protocol\ncalled Face Flashing that significantly increases the bar for launching\nsuccessful attacks on face authentication systems. By randomly flashing\nwell-designed pictures on a screen and analyzing the reflected light, our\nprotocol has leveraged physical characteristics of human faces: reflection\nprocessing at the speed of light, unique textual features, and uneven 3D\nshapes. Cooperating with working mechanism of the screen and digital cameras,\nour protocol is able to detect subtle traces left by an attacking process.\n  To demonstrate the effectiveness of Face Flashing, we implemented a prototype\nand performed thorough evaluations with large data set collected from\nreal-world scenarios. The results show that our Timing Verification can\neffectively detect the time gap between legitimate authentications and\nmalicious cases. Our Face Verification can also differentiate 2D plane from 3D\nobjects accurately. The overall accuracy of our liveness detection system is\n98.8\\%, and its robustness was evaluated in different scenarios. In the worst\ncase, our system's accuracy decreased to a still-high 97.3\\%.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 00:41:10 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 09:28:14 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Tang", "Di", ""], ["Zhou", "Zhe", ""], ["Zhang", "Yinqian", ""], ["Zhang", "Kehuan", ""]]}, {"id": "1801.01967", "submitter": "Amir Mazaheri", "authors": "Amir Mazaheri, Mubarak Shah", "title": "Visual Text Correction", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01261-8_10", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos, images, and sentences are mediums that can express the same\nsemantics. One can imagine a picture by reading a sentence or can describe a\nscene with some words. However, even small changes in a sentence can cause a\nsignificant semantic inconsistency with the corresponding video/image. For\nexample, by changing the verb of a sentence, the meaning may drastically\nchange. There have been many efforts to encode a video/sentence and decode it\nas a sentence/video. In this research, we study a new scenario in which both\nthe sentence and the video are given, but the sentence is inaccurate. A\nsemantic inconsistency between the sentence and the video or between the words\nof a sentence can result in an inaccurate description. This paper introduces a\nnew problem, called Visual Text Correction (VTC), i.e., finding and replacing\nan inaccurate word in the textual description of a video. We propose a deep\nnetwork that can simultaneously detect an inaccuracy in a sentence, and fix it\nby replacing the inaccurate word(s). Our method leverages the semantic\ninterdependence of videos and words, as well as the short-term and long-term\nrelations of the words in a sentence. In our formulation, part of a visual\nfeature vector for every single word is dynamically selected through a gating\nprocess. Furthermore, to train and evaluate our model, we propose an approach\nto automatically construct a large dataset for VTC problem. Our experiments and\nperformance analysis demonstrates that the proposed method provides very good\nresults and also highlights the general challenges in solving the VTC problem.\nTo the best of our knowledge, this work is the first of its kind for the Visual\nText Correction task.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 04:58:38 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 08:21:48 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 20:09:12 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Mazaheri", "Amir", ""], ["Shah", "Mubarak", ""]]}, {"id": "1801.01974", "submitter": "Fania Mokhayeri", "authors": "Fania Mokhayeri, Eric Granger, Guillaume-Alexandre Bilodeau", "title": "Domain-Specific Face Synthesis for Video Face Recognition from a Single\n  Sample Per Person", "comments": null, "journal-ref": "Transaction on Information Forensics and Security, Vol. 14, Issue\n  3, pp. 757-772, 2018", "doi": "10.1109/TIFS.2018.2866295", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The performance of still-to-video FR systems can decline significantly\nbecause faces captured in unconstrained operational domain (OD) over multiple\nvideo cameras have a different underlying data distribution compared to faces\ncaptured under controlled conditions in the enrollment domain (ED) with a still\ncamera. This is particularly true when individuals are enrolled to the system\nusing a single reference still. To improve the robustness of these systems, it\nis possible to augment the reference set by generating synthetic faces based on\nthe original still. However, without knowledge of the OD, many synthetic images\nmust be generated to account for all possible capture conditions. FR systems\nmay, therefore, require complex implementations and yield lower accuracy when\ntraining on many less relevant images. This paper introduces an algorithm for\ndomain-specific face synthesis (DSFS) that exploits the representative\nintra-class variation information available from the OD. Prior to operation, a\ncompact set of faces from unknown persons appearing in the OD is selected\nthrough clustering in the captured condition space. The domain-specific\nvariations of these face images are projected onto the reference stills by\nintegrating an image-based face relighting technique inside the 3D\nreconstruction framework. A compact set of synthetic faces is generated that\nresemble individuals of interest under the capture conditions relevant to the\nOD. In a particular implementation based on sparse representation\nclassification, the synthetic faces generated with the DSFS are employed to\nform a cross-domain dictionary that account for structured sparsity.\nExperimental results reveal that augmenting the reference gallery set of FR\nsystems using the proposed DSFS approach can provide a higher level of accuracy\ncompared to state-of-the-art approaches, with only a moderate increase in its\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 06:19:52 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 19:09:06 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Mokhayeri", "Fania", ""], ["Granger", "Eric", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1801.02021", "submitter": "Li Wang", "authors": "Li Wang, Ting Liu, Bing Wang, Xulei Yang and Gang Wang", "title": "Learning Hierarchical Features for Visual Object Tracking with Recursive\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has achieved very promising results in visual object\ntracking. Deep neural networks in existing tracking methods require a lot of\ntraining data to learn a large number of parameters. However, training data is\nnot sufficient for visual object tracking as annotations of a target object are\nonly available in the first frame of a test sequence. In this paper, we propose\nto learn hierarchical features for visual object tracking by using tree\nstructure based Recursive Neural Networks (RNN), which have fewer parameters\nthan other deep neural networks, e.g. Convolutional Neural Networks (CNN).\nFirst, we learn RNN parameters to discriminate between the target object and\nbackground in the first frame of a test sequence. Tree structure over local\npatches of an exemplar region is randomly generated by using a bottom-up greedy\nsearch strategy. Given the learned RNN parameters, we create two dictionaries\nregarding target regions and corresponding local patches based on the learned\nhierarchical features from both top and leaf nodes of multiple random trees. In\neach of the subsequent frames, we conduct sparse dictionary coding on all\ncandidates to select the best candidate as the new target location. In\naddition, we online update two dictionaries to handle appearance changes of\ntarget objects. Experimental results demonstrate that our feature learning\nalgorithm can significantly improve tracking performance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 14:39:29 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Wang", "Li", ""], ["Liu", "Ting", ""], ["Wang", "Bing", ""], ["Yang", "Xulei", ""], ["Wang", "Gang", ""]]}, {"id": "1801.02031", "submitter": "Ruichi Yu", "authors": "Ruichi Yu, Hongcheng Wang, Larry S. Davis", "title": "ReMotENet: Efficient Relevant Motion Event Detection for Large-scale\n  Home Surveillance Videos", "comments": "WACV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of detecting relevant motion caused by\nobjects of interest (e.g., person and vehicles) in large scale home\nsurveillance videos. The traditional method usually consists of two separate\nsteps, i.e., detecting moving objects with background subtraction running on\nthe camera, and filtering out nuisance motion events (e.g., trees, cloud,\nshadow, rain/snow, flag) with deep learning based object detection and tracking\nrunning on cloud. The method is extremely slow and therefore not cost\neffective, and does not fully leverage the spatial-temporal redundancies with a\npre-trained off-the-shelf object detector. To dramatically speedup relevant\nmotion event detection and improve its performance, we propose a novel network\nfor relevant motion event detection, ReMotENet, which is a unified, end-to-end\ndata-driven method using spatial-temporal attention-based 3D ConvNets to\njointly model the appearance and motion of objects-of-interest in a video.\nReMotENet parses an entire video clip in one forward pass of a neural network\nto achieve significant speedup. Meanwhile, it exploits the properties of home\nsurveillance videos, e.g., relevant motion is sparse both spatially and\ntemporally, and enhances 3D ConvNets with a spatial-temporal attention model\nand reference-frame subtraction to encourage the network to focus on the\nrelevant moving objects. Experiments demonstrate that our method can achieve\ncomparable or event better performance than the object detection based method\nbut with three to four orders of magnitude speedup (up to 20k times) on GPU\ndevices. Our network is efficient, compact and light-weight. It can detect\nrelevant motion on a 15s surveillance video clip within 4-8 milliseconds on a\nGPU and a fraction of second (0.17-0.39) on a CPU with a model size of less\nthan 1MB.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 15:19:06 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yu", "Ruichi", ""], ["Wang", "Hongcheng", ""], ["Davis", "Larry S.", ""]]}, {"id": "1801.02101", "submitter": "Mohammadhassan Izadyyazdanabadi", "authors": "Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Nikolay Martirosyan,\n  Jennifer Eschbacher, Peter Nakaji, Yezhou Yang, and Mark C. Preul", "title": "Improving utility of brain tumor confocal laser endomicroscopy:\n  objective value assessment and diagnostic frame detection with convolutional\n  neural networks", "comments": "SPIE Medical Imaging: Computer-Aided Diagnosis 2017", "journal-ref": "SPIE Proceedings Volume 10134, Medical Imaging 2017:\n  Computer-Aided Diagnosis; 101342J (2017);", "doi": "10.1117/12.2254902", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal laser endomicroscopy (CLE), although capable of obtaining images at\ncellular resolution during surgery of brain tumors in real time, creates as\nmany non-diagnostic as diagnostic images. Non-useful images are often distorted\ndue to relative motion between probe and brain or blood artifacts. Many images,\nhowever, simply lack diagnostic features immediately informative to the\nphysician. Examining all the hundreds or thousands of images from a single case\nto discriminate diagnostic images from nondiagnostic ones can be tedious.\nProviding a real-time diagnostic value assessment of images (fast enough to be\nused during the surgical acquisition process and accurate enough for the\npathologist to rely on) to automatically detect diagnostic frames would\nstreamline the analysis of images and filter useful images for the\npathologist/surgeon. We sought to automatically classify images as diagnostic\nor non-diagnostic. AlexNet, a deep-learning architecture, was used in a 4-fold\ncross validation manner. Our dataset includes 16,795 images (8572 nondiagnostic\nand 8223 diagnostic) from 74 CLE-aided brain tumor surgery patients. The ground\ntruth for all the images is provided by the pathologist. Average model accuracy\non test data was 91% overall (90.79 % accuracy, 90.94 % sensitivity and 90.87 %\nspecificity). To evaluate the model reliability we also performed receiver\noperating characteristic (ROC) analysis yielding 0.958 average for the area\nunder ROC curve (AUC). These results demonstrate that a deeply trained AlexNet\nnetwork can achieve a model that reliably and quickly recognizes diagnostic CLE\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 22:57:06 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Izadyyazdanabadi", "Mohammadhassan", ""], ["Belykh", "Evgenii", ""], ["Martirosyan", "Nikolay", ""], ["Eschbacher", "Jennifer", ""], ["Nakaji", "Peter", ""], ["Yang", "Yezhou", ""], ["Preul", "Mark C.", ""]]}, {"id": "1801.02108", "submitter": "Mengye Ren", "authors": "Mengye Ren, Andrei Pokrovsky, Bin Yang, Raquel Urtasun", "title": "SBNet: Sparse Blocks Network for Fast Inference", "comments": "10 pages, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional deep convolutional neural networks (CNNs) apply convolution\noperators uniformly in space across all feature maps for hundreds of layers -\nthis incurs a high computational cost for real-time applications. For many\nproblems such as object detection and semantic segmentation, we are able to\nobtain a low-cost computation mask, either from a priori problem knowledge, or\nfrom a low-resolution segmentation network. We show that such computation masks\ncan be used to reduce computation in the high-resolution main network. Variants\nof sparse activation CNNs have previously been explored on small-scale tasks\nand showed no degradation in terms of object classification accuracy, but often\nmeasured gains in terms of theoretical FLOPs without realizing a practical\nspeed-up when compared to highly optimized dense convolution implementations.\nIn this work, we leverage the sparsity structure of computation masks and\npropose a novel tiling-based sparse convolution algorithm. We verified the\neffectiveness of our sparse CNN on LiDAR-based 3D object detection, and we\nreport significant wall-clock speed-ups compared to dense convolution without\nnoticeable loss of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 01:03:25 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 14:44:00 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ren", "Mengye", ""], ["Pokrovsky", "Andrei", ""], ["Yang", "Bin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1801.02112", "submitter": "Luca Carlone", "authors": "Luca Carlone and Giuseppe C. Calafiore", "title": "Convex Relaxations for Pose Graph Optimization with Outliers", "comments": "10 pages, 5 figures, accepted for publication in the IEEE Robotics\n  and Automation Letters, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose Graph Optimization involves the estimation of a set of poses from\npairwise measurements and provides a formalization for many problems arising in\nmobile robotics and geometric computer vision. In this paper, we consider the\ncase in which a subset of the measurements fed to pose graph optimization is\nspurious. Our first contribution is to develop robust estimators that can cope\nwith heavy-tailed measurement noise, hence increasing robustness to the\npresence of outliers. Since the resulting estimators require solving nonconvex\noptimization problems, we further develop convex relaxations that approximately\nsolve those problems via semidefinite programming. We then provide conditions\nunder which the proposed relaxations are exact. Contrarily to existing\napproaches, our convex relaxations do not rely on the availability of an\ninitial guess for the unknown poses, hence they are more suitable for setups in\nwhich such guess is not available (e.g., multi-robot localization, recovery\nafter localization failure). We tested the proposed techniques in extensive\nsimulations, and we show that some of the proposed relaxations are indeed tight\n(i.e., they solve the original nonconvex problem 10 exactly) and ensure\naccurate estimation in the face of a large number of outliers.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 02:18:28 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Carlone", "Luca", ""], ["Calafiore", "Giuseppe C.", ""]]}, {"id": "1801.02121", "submitter": "Mahmoud Sadeghi HassanAbadi", "authors": "Mahmoud Sadeghi, Ali Zakerolhosseini, Ali Sonboli", "title": "Architecture Based Classification of Leaf Images", "comments": "28 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant classification and identification has so far been an important and\ndifficult task. In this paper, an efficient and systematic approach for\nextracting the leaf architecture characters from captured digital images is\nproposed. The input image is first pre-processed in five steps to be prepared\nfor feature extraction. In the second stage, methods for extracting different\narchitectural features are studied using various mathematical and computational\nmethods. Also, the classification rules for mapping the calculated values of\neach feature to semantic botanical terms in proposed. Compared with previous\nstudies, the proposed method combines extracted features of an image with\nspecific knowledge of leaf architecture in the domain of botany to provide a\ncomprehensive framework for both computer engineers and botanist. Finally,\nBased on the proposed method, experiments on the classification of the\nImagerCLEF 2012 dataset has been performed with promising results.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 03:26:30 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Sadeghi", "Mahmoud", ""], ["Zakerolhosseini", "Ali", ""], ["Sonboli", "Ali", ""]]}, {"id": "1801.02171", "submitter": "Alexandre Attia", "authors": "Alexandre Attia, Sharone Dayan", "title": "Detection and segmentation of the Left Ventricle in Cardiac MRI using\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous\ntask that can vary depending on the patient, the Magnetic Resonance Images\n(MRI) cuts and the experts. Still today, we consider manual delineation done by\nexperts as being the ground truth for cardiac diagnosticians. Thus, we are\nreviewing the paper - written by Avendi and al. - who presents a combined\napproach with Convolutional Neural Networks, Stacked Auto-Encoders and\nDeformable Models, to try and automate the segmentation while performing more\naccurately. Furthermore, we have implemented parts of the paper (around three\nquarts) and experimented both the original method and slightly modified\nversions when changing the architecture and the parameters.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 11:22:12 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Attia", "Alexandre", ""], ["Dayan", "Sharone", ""]]}, {"id": "1801.02190", "submitter": "Stylianos Venieris", "authors": "Michalis Rizakis, Stylianos I. Venieris, Alexandros Kouris and\n  Christos-Savvas Bouganis", "title": "Approximate FPGA-based LSTMs under Computation Time Constraints", "comments": "Accepted at the 14th International Symposium in Applied\n  Reconfigurable Computing (ARC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks and in particular Long Short-Term Memory (LSTM)\nnetworks have demonstrated state-of-the-art accuracy in several emerging\nArtificial Intelligence tasks. However, the models are becoming increasingly\ndemanding in terms of computational and memory load. Emerging latency-sensitive\napplications including mobile robots and autonomous vehicles often operate\nunder stringent computation time constraints. In this paper, we address the\nchallenge of deploying computationally demanding LSTMs at a constrained time\nbudget by introducing an approximate computing scheme that combines iterative\nlow-rank compression and pruning, along with a novel FPGA-based LSTM\narchitecture. Combined in an end-to-end framework, the approximation method's\nparameters are optimised and the architecture is configured to address the\nproblem of high-performance LSTM execution in time-constrained applications.\nQuantitative evaluation on a real-life image captioning application indicates\nthat the proposed methods required up to 6.5x less time to achieve the same\napplication-level accuracy compared to a baseline method, while achieving an\naverage of 25x higher accuracy under the same computation time constraints.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 13:46:03 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Rizakis", "Michalis", ""], ["Venieris", "Stylianos I.", ""], ["Kouris", "Alexandros", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1801.02200", "submitter": "Amanda Duarte", "authors": "Didac Sur\\'is, Amanda Duarte, Amaia Salvador, Jordi Torres and Xavier\n  Gir\\'o-i-Nieto", "title": "Cross-modal Embeddings for Video and Audio Retrieval", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of online videos brings several opportunities for\ntraining self-supervised neural networks. The creation of large scale datasets\nof videos such as the YouTube-8M allows us to deal with this large amount of\ndata in manageable way. In this work, we find new ways of exploiting this\ndataset by taking advantage of the multi-modal information it provides. By\nmeans of a neural network, we are able to create links between audio and visual\ndocuments, by projecting them into a common region of the feature space,\nobtaining joint audio-visual embeddings. These links are used to retrieve audio\nsamples that fit well to a given silent video, and also to retrieve images that\nmatch a given a query audio. The results in terms of Recall@K obtained over a\nsubset of YouTube-8M videos show the potential of this unsupervised approach\nfor cross-modal feature learning. We train embeddings for both scales and\nassess their quality in a retrieval problem, formulated as using the feature\nextracted from one modality to retrieve the most similar videos based on the\nfeatures computed in the other modality.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 15:43:22 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Sur\u00eds", "Didac", ""], ["Duarte", "Amanda", ""], ["Salvador", "Amaia", ""], ["Torres", "Jordi", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1801.02225", "submitter": "Hacer Yalim Keles", "authors": "Long Ang Lim and Hacer Yalim Keles", "title": "Foreground Segmentation Using a Triplet Convolutional Neural Network for\n  Multiscale Feature Encoding", "comments": "This paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2018.08.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach for moving objects segmentation in a scene is to perform a\nbackground subtraction. Several methods have been proposed in this domain.\nHowever, they lack the ability of handling various difficult scenarios such as\nillumination changes, background or camera motion, camouflage effect, shadow\netc. To address these issues, we propose a robust and flexible encoder-decoder\ntype neural network based approach. We adapt a pre-trained convolutional\nnetwork, i.e. VGG-16 Net, under a triplet framework in the encoder part to\nembed an image in multiple scales into the feature space and use a transposed\nconvolutional network in the decoder part to learn a mapping from feature space\nto image space. We train this network end-to-end by using only a few training\nsamples. Our network takes an RGB image in three different scales and produces\na foreground segmentation probability mask for the corresponding image. In\norder to evaluate our model, we entered the Change Detection 2014 Challenge\n(changedetection.net) and our method outperformed all the existing\nstate-of-the-art methods by an average F-Measure of 0.9770. Our source code\nwill be made publicly available at https://github.com/lim-anggun/FgSegNet.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 18:33:43 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Lim", "Long Ang", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "1801.02251", "submitter": "Siwei Feng", "authors": "Siwei Feng and Marco F.Duarte", "title": "Graph Autoencoder-Based Unsupervised Feature Selection with Broad and\n  Local Data Structure Preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a dimensionality reduction technique that selects a\nsubset of representative features from high dimensional data by eliminating\nirrelevant and redundant features. Recently, feature selection combined with\nsparse learning has attracted significant attention due to its outstanding\nperformance compared with traditional feature selection methods that ignores\ncorrelation between features. These works first map data onto a low-dimensional\nsubspace and then select features by posing a sparsity constraint on the\ntransformation matrix. However, they are restricted by design to linear data\ntransformation, a potential drawback given that the underlying correlation\nstructures of data are often non-linear. To leverage a more sophisticated\nembedding, we propose an autoencoder-based unsupervised feature selection\napproach that leverages a single-layer autoencoder for a joint framework of\nfeature selection and manifold learning. More specifically, we enforce column\nsparsity on the weight matrix connecting the input layer and the hidden layer,\nas in previous work. Additionally, we include spectral graph analysis on the\nprojected data into the learning process to achieve local data geometry\npreservation from the original data space to the low-dimensional feature space.\nExtensive experiments are conducted on image, audio, text, and biological data.\nThe promising experimental results validate the superiority of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 21:14:01 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2018 03:14:38 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Feng", "Siwei", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1801.02261", "submitter": "Avi Ben-Cohen", "authors": "Avi Ben-Cohen, Eyal Klang, Michal Marianne Amitai, Jacob Goldberger,\n  Hayit Greenspan", "title": "Anatomical Data Augmentation For CNN based Pixel-wise Classification", "comments": "To be presented at IEEE ISBI 2018", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363762", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method for anatomical data augmentation that is\nbased on using slices of computed tomography (CT) examinations that are\nadjacent to labeled slices as another resource of labeled data for training the\nnetwork. The extended labeled data is used to train a U-net network for a\npixel-wise classification into different hepatic lesions and normal liver\ntissues. Our dataset contains CT examinations from 140 patients with 333 CT\nimages annotated by an expert radiologist. We tested our approach and compared\nit to the conventional training process. Results indicate superiority of our\nmethod. Using the anatomical data augmentation we achieved an improvement of 3%\nin the success rate, 5% in the classification accuracy, and 4% in Dice.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 23:00:02 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ben-Cohen", "Avi", ""], ["Klang", "Eyal", ""], ["Amitai", "Michal Marianne", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1801.02279", "submitter": "Fatemeh Shiri", "authors": "Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, Piotr Koniusz", "title": "Identity-preserving Face Recovery from Portraits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the latent photorealistic faces from their artistic portraits aids\nhuman perception and facial analysis. However, a recovery process that can\npreserve identity is challenging because the fine details of real faces can be\ndistorted or lost in stylized images. In this paper, we present a new\nIdentity-preserving Face Recovery from Portraits (IFRP) to recover latent\nphotorealistic faces from unaligned stylized portraits. Our IFRP method\nconsists of two components: Style Removal Network (SRN) and Discriminative\nNetwork (DN). The SRN is designed to transfer feature maps of stylized images\nto the feature maps of the corresponding photorealistic faces. By embedding\nspatial transformer networks into the SRN, our method can compensate for\nmisalignments of stylized faces automatically and output aligned realistic face\nimages. The role of the DN is to enforce recovered faces to be similar to\nauthentic faces. To ensure the identity preservation, we promote the recovered\nand ground-truth faces to share similar visual features via a distance measure\nwhich compares features of recovered and ground-truth faces extracted from a\npre-trained VGG network. We evaluate our method on a large-scale synthesized\ndataset of real and stylized face pairs and attain state of the art results. In\naddition, our method can recover photorealistic faces from previously unseen\nstylized portraits, original paintings and human-drawn sketches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 00:25:35 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 23:17:49 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Shiri", "Fatemeh", ""], ["Yu", "Xin", ""], ["Porikli", "Fatih", ""], ["Hartley", "Richard", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1801.02325", "submitter": "Jie Lyu", "authors": "Jie Lyu, Zejian Yuan, Dapeng Chen", "title": "Long-term Multi-granularity Deep Framework for Driver Drowsiness\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real-world driver drowsiness detection from videos, the variation of head\npose is so large that the existing methods on global face is not capable of\nextracting effective features, such as looking aside and lowering head.\nTemporal dependencies with variable length are also rarely considered by the\nprevious approaches, e.g., yawning and speaking. In this paper, we propose a\nLong-term Multi-granularity Deep Framework to detect driver drowsiness in\ndriving videos containing the frontal faces. The framework includes two key\ncomponents: (1) Multi-granularity Convolutional Neural Network (MCNN), a novel\nnetwork utilizes a group of parallel CNN extractors on well-aligned facial\npatches of different granularities, and extracts facial representations\neffectively for large variation of head pose, furthermore, it can flexibly fuse\nboth detailed appearance clues of the main parts and local to global spatial\nconstraints; (2) a deep Long Short Term Memory network is applied on facial\nrepresentations to explore long-term relationships with variable length over\nsequential frames, which is capable to distinguish the states with temporal\ndependencies, such as blinking and closing eyes. Our approach achieves 90.05%\naccuracy and about 37 fps speed on the evaluation set of the public NTHU-DDD\ndataset, which is the state-of-the-art method on driver drowsiness detection.\nMoreover, we build a new dataset named FI-DDD, which is of higher precision of\ndrowsy locations in temporal dimension.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 07:21:46 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Lyu", "Jie", ""], ["Yuan", "Zejian", ""], ["Chen", "Dapeng", ""]]}, {"id": "1801.02385", "submitter": "Maayan Frid-Adar", "authors": "Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit\n  Greenspan", "title": "Synthetic Data Augmentation using GAN for Improved Liver Lesion\n  Classification", "comments": "To be presented at IEEE International Symposium on Biomedical Imaging\n  (ISBI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a data augmentation method that generates synthetic\nmedical images using Generative Adversarial Networks (GANs). We propose a\ntraining scheme that first uses classical data augmentation to enlarge the\ntraining set and then further enlarges the data size and its diversity by\napplying GAN techniques for synthetic data augmentation. Our method is\ndemonstrated on a limited dataset of computed tomography (CT) images of 182\nliver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification\nperformance using only classic data augmentation yielded 78.6% sensitivity and\n88.4% specificity. By adding the synthetic data augmentation the results\nsignificantly increased to 85.7% sensitivity and 92.4% specificity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 11:20:36 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Frid-Adar", "Maayan", ""], ["Klang", "Eyal", ""], ["Amitai", "Michal", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1801.02439", "submitter": "Yupei Wang", "authors": "Yupei Wang, Xin Zhao, Yin Li, Kaiqi Huang", "title": "Deep Crisp Boundaries: From Boundaries to Higher-level Tasks", "comments": "Accepted by IEEE Trans. on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2874279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection has made significant progress with the help of deep\nConvolutional Networks (ConvNet). These ConvNet based edge detectors have\napproached human level performance on standard benchmarks. We provide a\nsystematical study of these detectors' outputs. We show that the detection\nresults did not accurately localize edge pixels, which can be adversarial for\ntasks that require crisp edge inputs. As a remedy, we propose a novel\nrefinement architecture to address the challenging problem of learning a crisp\nedge detector using ConvNet. Our method leverages a top-down backward\nrefinement pathway, and progressively increases the resolution of feature maps\nto generate crisp edges. Our results achieve superior performance, surpassing\nhuman accuracy when using standard criteria on BSDS500, and largely\noutperforming state-of-the-art methods when using more strict criteria. More\nimportantly, we demonstrate the benefit of crisp edge maps for several\nimportant applications in computer vision, including optical flow estimation,\nobject proposal generation and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 14:30:21 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 09:06:32 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 03:51:08 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Wang", "Yupei", ""], ["Zhao", "Xin", ""], ["Li", "Yin", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1801.02472", "submitter": "Meysam Golmohammadi", "authors": "Vinit Shah, Meysam Golmohammadi, Saeedeh Ziyabari, Eva Von Weltin,\n  Iyad Obeid and Joseph Picone", "title": "Optimizing Channel Selection for Seizure Detection", "comments": "Published in Dec 2017 publication IEEE Signal Processing in Medicine\n  and Biology Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of electroencephalogram (EEG) signals can be complicated by\nobfuscating artifacts. Artifact detection plays an important role in the\nobservation and analysis of EEG signals. Spatial information contained in the\nplacement of the electrodes can be exploited to accurately detect artifacts.\nHowever, when fewer electrodes are used, less spatial information is available,\nmaking it harder to detect artifacts. In this study, we investigate the\nperformance of a deep learning algorithm, CNN-LSTM, on several channel\nconfigurations. Each configuration was designed to minimize the amount of\nspatial information lost compared to a standard 22-channel EEG. Systems using a\nreduced number of channels ranging from 8 to 20 achieved sensitivities between\n33% and 37% with false alarms in the range of [38, 50] per 24 hours. False\nalarms increased dramatically (e.g., over 300 per 24 hours) when the number of\nchannels was further reduced. Baseline performance of a system that used all 22\nchannels was 39% sensitivity with 23 false alarms. Since the 22-channel system\nwas the only system that included referential channels, the rapid increase in\nthe false alarm rate as the number of channels was reduced underscores the\nimportance of retaining referential channels for artifact reduction. This\ncautionary result is important because one of the biggest differences between\nvarious types of EEGs administered is the type of referential channel used.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 00:59:32 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Shah", "Vinit", ""], ["Golmohammadi", "Meysam", ""], ["Ziyabari", "Saeedeh", ""], ["Von Weltin", "Eva", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02475", "submitter": "Yangyang Xu", "authors": "Yangyang Xu and Lei Wang", "title": "Ensemble One-dimensional Convolution Neural Networks for Skeleton-based\n  Action Recognition", "comments": "the title of Table 3 has something wrong and the expermient is not\n  enough", "journal-ref": null, "doi": "10.1109/LSP.2018.2841649", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a effective but extensible residual\none-dimensional convolution neural network as base network, based on the this\nnetwork, we proposed four subnets to explore the features of skeleton sequences\nfrom each aspect. Given a skeleton sequences, the spatial information are\nencoded into the skeleton joints coordinate in a frame and the temporal\ninformation are present by multiple frames. Limited by the skeleton sequence\nrepresentations, two-dimensional convolution neural network cannot be used\ndirectly, we chose one-dimensional convolution layer as the basic layer. Each\nsub network could extract discriminative features from different aspects. Our\nfirst subnet is a two-stream network which could explore both temporal and\nspatial information. The second is a body-parted network, which could gain\nmicro spatial features and macro temporal features. The third one is an\nattention network, the main contribution of which is to focus the key frames\nand feature channels which high related with the action classes in a skeleton\nsequence. One frame-difference network, as the last subnet, mainly processes\nthe joints changes between the consecutive frames. Four subnets ensemble\ntogether by late fusion, the key problem of ensemble method is each subnet\nshould have a certain performance and between the subnets, there are diversity\nexisting. Each subnet shares a wellperformance basenet and differences between\nsubnets guaranteed the diversity. Experimental results show that the ensemble\nnetwork gets a state-of-the-art performance on three widely used datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 15:12:54 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 06:22:55 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Xu", "Yangyang", ""], ["Wang", "Lei", ""]]}, {"id": "1801.02477", "submitter": "Meysam Golmohammadi", "authors": "Amir Harati, Meysam Golmohammadi, Silvia Lopez, Iyad Obeid and Joseph\n  Picone", "title": "Improved EEG Event Classification Using Differential Energy", "comments": "Published in IEEE Signal Processing in Medicine and Biology\n  Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": "A. Harati, M. Golmohammadi, S. Lopez, I. Obeid and J. Picone,\n  \"Improved EEG event classification using differential energy,\" 2015 IEEE\n  Signal Processing in Medicine and Biology Symposium (SPMB), Philadelphia, PA,\n  2015, pp. 1-4", "doi": "10.1109/SPMB.2015.7405421", "report-no": null, "categories": "eess.SP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction for automatic classification of EEG signals typically\nrelies on time frequency representations of the signal. Techniques such as\ncepstral-based filter banks or wavelets are popular analysis techniques in many\nsignal processing applications including EEG classification. In this paper, we\npresent a comparison of a variety of approaches to estimating and\npostprocessing features. To further aid in discrimination of periodic signals\nfrom aperiodic signals, we add a differential energy term. We evaluate our\napproaches on the TUH EEG Corpus, which is the largest publicly available EEG\ncorpus and an exceedingly challenging task due to the clinical nature of the\ndata. We demonstrate that a variant of a standard filter bank-based approach,\ncoupled with first and second derivatives, provides a substantial reduction in\nthe overall error rate. The combination of differential energy and derivatives\nproduces a 24% absolute reduction in the error rate and improves our ability to\ndiscriminate between signal events and background noise. This relatively simple\napproach proves to be comparable to other popular feature extraction approaches\nsuch as wavelets, but is much more computationally efficient.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 03:55:55 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Harati", "Amir", ""], ["Golmohammadi", "Meysam", ""], ["Lopez", "Silvia", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02480", "submitter": "Manuel G\\\"unther", "authors": "Andras Rozsa, Manuel G\\\"unther, Ethan M. Rudd, Terrance E. Boult", "title": "Facial Attributes: Accuracy and Adversarial Robustness", "comments": "arXiv admin note: text overlap with arXiv:1605.05411", "journal-ref": "Pattern Recognition Letters, 2017, ISSN 0167-8655", "doi": "10.1016/j.patrec.2017.10.024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes, emerging soft biometrics, must be automatically and\nreliably extracted from images in order to be usable in stand-alone systems.\nWhile recent methods extract facial attributes using deep neural networks\n(DNNs) trained on labeled facial attribute data, the robustness of deep\nattribute representations has not been evaluated. In this paper, we examine the\nrepresentational stability of several approaches that recently advanced the\nstate of the art on the CelebA benchmark by generating adversarial examples\nformed by adding small, non-random perturbations to inputs yielding altered\nclassifications. We show that our fast flipping attribute (FFA) technique\ngenerates more adversarial examples than traditional algorithms, and that the\nadversarial robustness of DNNs varies highly between facial attributes. We also\ntest the correlation of facial attributes and find that only for related\nattributes do the formed adversarial perturbations change the classification of\nothers. Finally, we introduce the concept of natural adversarial samples, i.e.,\nmisclassified images where predictions can be corrected via small\nperturbations. We demonstrate that natural adversarial samples commonly occur\nand show that many of these images remain misclassified even with additional\ntraining epochs, even though their correct classification may require only a\nsmall adjustment to network parameters.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 00:53:16 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 16:11:40 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Rozsa", "Andras", ""], ["G\u00fcnther", "Manuel", ""], ["Rudd", "Ethan M.", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1801.02548", "submitter": "John McKay", "authors": "John McKay, Isaac Gerg, Vishal Monga", "title": "Bridging the Gap: Simultaneous Fine Tuning for Data Re-Balancing", "comments": "Submitted to IGARSS 2018, 4 Pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many real-world classification problems wherein the issue of data\nimbalance (the case when a data set contains substantially more samples for\none/many classes than the rest) is unavoidable. While under-sampling the\nproblematic classes is a common solution, this is not a compelling option when\nthe large data class is itself diverse and/or the limited data class is\nespecially small. We suggest a strategy based on recent work concerning limited\ndata problems which utilizes a supplemental set of images with similar\nproperties to the limited data class to aid in the training of a neural\nnetwork. We show results for our model against other typical methods on a\nreal-world synthetic aperture sonar data set. Code can be found at\ngithub.com/JohnMcKay/dataImbalance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 16:44:38 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["McKay", "John", ""], ["Gerg", "Isaac", ""], ["Monga", "Vishal", ""]]}, {"id": "1801.02591", "submitter": "Mojtaba Sedigh Fazli", "authors": "Mojtaba S. Fazli, Stephen A.Vella, Silvia N.J. Moreno and Shannon\n  Quinn", "title": "Unsupervised Discovery of Toxoplasma gondii Motility Phenotypes", "comments": "4 pages, Accepted to 2018 IEEE International Symposium on Biomedical\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxoplasma gondii is a parasitic protozoan that causes dis- seminated\ntoxoplasmosis, a disease that afflicts roughly a third of the worlds\npopulation. Its virulence is predicated on its motility and ability to enter\nand exit nucleated cells; therefore, studies elucidating its mechanism of\nmotility and in particular, its motility patterns in the context of its lytic\ncycle, are critical to the eventual development of therapeutic strate- gies.\nHere, we present an end-to-end computational pipeline for identifying T. gondii\nmotility phenotypes in a completely unsupervised, data-driven way. We track the\nparasites before and after addition of extracellular Ca2+ to study its effects\non the parasite motility patterns and use this information to parameterize the\nmotion and group it according to similarity of spatiotemporal dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:04:00 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 19:04:18 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Fazli", "Mojtaba S.", ""], ["Vella", "Stephen A.", ""], ["Moreno", "Silvia N. J.", ""], ["Quinn", "Shannon", ""]]}, {"id": "1801.02608", "submitter": "Danny Karmon", "authors": "Danny Karmon, Daniel Zoran and Yoav Goldberg", "title": "LaVAN: Localized and Visible Adversarial Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most works on adversarial examples for deep-learning based image classifiers\nuse noise that, while small, covers the entire image. We explore the case where\nthe noise is allowed to be visible but confined to a small, localized patch of\nthe image, without covering any of the main object(s) in the image. We show\nthat it is possible to generate localized adversarial noises that cover only 2%\nof the pixels in the image, none of them over the main object, and that are\ntransferable across images and locations, and successfully fool a\nstate-of-the-art Inception v3 model with very high success rates.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:44:23 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 12:49:11 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Karmon", "Danny", ""], ["Zoran", "Daniel", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1801.02610", "submitter": "Chaowei Xiao", "authors": "Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song", "title": "Generating Adversarial Examples with Adversarial Networks", "comments": "Accepted to IJCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been found to be vulnerable to adversarial\nexamples resulting from adding small-magnitude perturbations to inputs. Such\nadversarial examples can mislead DNNs to produce adversary-selected results.\nDifferent attack strategies have been proposed to generate adversarial\nexamples, but how to produce them with high perceptual quality and more\nefficiently requires more research efforts. In this paper, we propose AdvGAN to\ngenerate adversarial examples with generative adversarial networks (GANs),\nwhich can learn and approximate the distribution of original instances. For\nAdvGAN, once the generator is trained, it can generate adversarial\nperturbations efficiently for any instance, so as to potentially accelerate\nadversarial training as defenses. We apply AdvGAN in both semi-whitebox and\nblack-box attack settings. In semi-whitebox attacks, there is no need to access\nthe original target model after the generator is trained, in contrast to\ntraditional white-box attacks. In black-box attacks, we dynamically train a\ndistilled model for the black-box model and optimize the generator accordingly.\nAdversarial examples generated by AdvGAN on different target models have high\nattack success rate under state-of-the-art defenses compared to other attacks.\nOur attack has placed the first with 92.76% accuracy on a public MNIST\nblack-box attack challenge.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:50:13 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 19:04:37 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 22:08:13 GMT"}, {"version": "v4", "created": "Tue, 12 Feb 2019 06:19:35 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2019 15:53:22 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Xiao", "Chaowei", ""], ["Li", "Bo", ""], ["Zhu", "Jun-Yan", ""], ["He", "Warren", ""], ["Liu", "Mingyan", ""], ["Song", "Dawn", ""]]}, {"id": "1801.02612", "submitter": "Chaowei Xiao", "authors": "Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song", "title": "Spatially Transformed Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that widely used deep neural networks (DNNs) are\nvulnerable to carefully crafted adversarial examples. Many advanced algorithms\nhave been proposed to generate adversarial examples by leveraging the\n$\\mathcal{L}_p$ distance for penalizing perturbations. Researchers have\nexplored different defense methods to defend against such adversarial attacks.\nWhile the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual\nquality remains an active research area, in this paper we will instead focus on\na different type of perturbation, namely spatial transformation, as opposed to\nmanipulating the pixel values directly as in prior works. Perturbations\ngenerated through spatial transformation could result in large $\\mathcal{L}_p$\ndistance measures, but our extensive experiments show that such spatially\ntransformed adversarial examples are perceptually realistic and more difficult\nto defend against with existing defense systems. This potentially provides a\nnew direction in adversarial example generation and the design of corresponding\ndefenses. We visualize the spatial transformation based perturbation for\ndifferent examples and show that our technique can produce realistic\nadversarial examples with smooth image deformation. Finally, we visualize the\nattention of deep networks with different types of adversarial examples to\nbetter understand how these examples are interpreted.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:51:59 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 19:03:32 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Xiao", "Chaowei", ""], ["Zhu", "Jun-Yan", ""], ["Li", "Bo", ""], ["He", "Warren", ""], ["Liu", "Mingyan", ""], ["Song", "Dawn", ""]]}, {"id": "1801.02613", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema,\n  Grant Schoenebeck, Dawn Song, Michael E. Houle, James Bailey", "title": "Characterizing Adversarial Subspaces Using Local Intrinsic\n  Dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently been shown to be vulnerable against\nadversarial examples, which are carefully crafted instances that can mislead\nDNNs to make errors during prediction. To better understand such attacks, a\ncharacterization is needed of the properties of regions (the so-called\n'adversarial subspaces') in which adversarial examples lie. We tackle this\nchallenge by characterizing the dimensional properties of adversarial regions,\nvia the use of Local Intrinsic Dimensionality (LID). LID assesses the\nspace-filling capability of the region surrounding a reference example, based\non the distance distribution of the example to its neighbors. We first provide\nexplanations about how adversarial perturbation can affect the LID\ncharacteristic of adversarial regions, and then show empirically that LID\ncharacteristics can facilitate the distinction of adversarial examples\ngenerated using state-of-the-art attacks. As a proof-of-concept, we show that a\npotential application of LID is to distinguish adversarial examples, and the\npreliminary results show that it can outperform several state-of-the-art\ndetection measures by large margins for five attack strategies considered in\nthis paper across three benchmark datasets. Our analysis of the LID\ncharacteristic for adversarial regions not only motivates new directions of\neffective adversarial defense, but also opens up more challenges for developing\nnew attacks to better understand the vulnerabilities of DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:54:40 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 14:53:21 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 07:24:10 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Ma", "Xingjun", ""], ["Li", "Bo", ""], ["Wang", "Yisen", ""], ["Erfani", "Sarah M.", ""], ["Wijewickrema", "Sudanthi", ""], ["Schoenebeck", "Grant", ""], ["Song", "Dawn", ""], ["Houle", "Michael E.", ""], ["Bailey", "James", ""]]}, {"id": "1801.02642", "submitter": "Akshay Pai", "authors": "Marco Singh and Akshay Pai", "title": "Boundary Optimizing Network (BON)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite all the success that deep neural networks have seen in classifying\ncertain datasets, the challenge of finding optimal solutions that generalize\nstill remains. In this paper, we propose the Boundary Optimizing Network (BON),\na new approach to generalization for deep neural networks when used for\nsupervised learning. Given a classification network, we propose to use a\ncollaborative generative network that produces new synthetic data points in the\nform of perturbations of original data points. In this way, we create a data\nsupport around each original data point which prevents decision boundaries from\npassing too close to the original data points, i.e. prevents overfitting. We\nshow that BON improves convergence on CIFAR-10 using the state-of-the-art\nDensenet. We do however observe that the generative network suffers from\ncatastrophic forgetting during training, and we therefore propose to use a\nvariation of Memory Aware Synapses to optimize the generative network (called\nBON++). On the Iris dataset, we visualize the effect of BON++ when the\ngenerator does not suffer from catastrophic forgetting and conclude that the\napproach has the potential to create better boundaries in a higher dimensional\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 19:02:44 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 21:32:05 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 10:24:09 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Singh", "Marco", ""], ["Pai", "Akshay", ""]]}, {"id": "1801.02684", "submitter": "Lina Karam", "authors": "Lina Karam and Tejas Borkar and Yu Cao and Junseok Chae", "title": "Generative Sensing: Transforming Unreliable Sensor Data for Reliable\n  Recognition", "comments": "5 pages, Submitted to IEEE MIPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a deep learning enabled generative sensing framework\nwhich integrates low-end sensors with computational intelligence to attain a\nhigh recognition accuracy on par with that attained with high-end sensors. The\nproposed generative sensing framework aims at transforming low-end, low-quality\nsensor data into higher quality sensor data in terms of achieved classification\naccuracy. The low-end data can be transformed into higher quality data of the\nsame modality or into data of another modality. Different from existing methods\nfor image generation, the proposed framework is based on discriminative models\nand targets to maximize the recognition accuracy rather than a similarity\nmeasure. This is achieved through the introduction of selective feature\nregeneration in a deep neural network (DNN). The proposed generative sensing\nwill essentially transform low-quality sensor data into high-quality\ninformation for robust perception. Results are presented to illustrate the\nperformance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 20:57:34 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Karam", "Lina", ""], ["Borkar", "Tejas", ""], ["Cao", "Yu", ""], ["Chae", "Junseok", ""]]}, {"id": "1801.02686", "submitter": "Arya Senna Abdul Rachman", "authors": "Achim Kampker, Mohsen Sefati, Arya Abdul Rachman, Kai Kreisk\\\"other\n  and Pascual Campoy", "title": "Towards Multi-Object Detection and Tracking in Urban Scenario under\n  Uncertainties", "comments": "Some significant editorial/editing issues are found upon review.\n  Paper will undergo language re-proofing before resubmitted", "journal-ref": "4th.VEHITS.Proc. 109 (2018) 156-167", "doi": "10.5220/0006706101560167", "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban-oriented autonomous vehicles require a reliable perception technology\nto tackle the high amount of uncertainties. The recently introduced compact 3D\nLIDAR sensor offers a surround spatial information that can be exploited to\nenhance the vehicle perception. We present a real-time integrated framework of\nmulti-target object detection and tracking using 3D LIDAR geared toward urban\nuse. Our approach combines sensor occlusion-aware detection method with\ncomputationally efficient heuristics rule-based filtering and adaptive\nprobabilistic tracking to handle uncertainties arising from sensing limitation\nof 3D LIDAR and complexity of the target object movement. The evaluation\nresults using real-world pre-recorded 3D LIDAR data and comparison with\nstate-of-the-art works shows that our framework is capable of achieving\npromising tracking performance in the urban situation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 21:05:10 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 17:51:59 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Kampker", "Achim", ""], ["Sefati", "Mohsen", ""], ["Rachman", "Arya Abdul", ""], ["Kreisk\u00f6ther", "Kai", ""], ["Campoy", "Pascual", ""]]}, {"id": "1801.02722", "submitter": "Zichen Zhang", "authors": "Zichen Zhang, Min Tang, Dana Cobzas, Dornoosh Zonoobi, Martin\n  Jagersand, Jacob L. Jaremko", "title": "End-to-end detection-segmentation network with ROI convolution", "comments": "ISBI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an end-to-end neural network that improves the segmentation\naccuracy of fully convolutional networks by incorporating a localization unit.\nThis network performs object localization first, which is then used as a cue to\nguide the training of the segmentation network. We test the proposed method on\na segmentation task of small objects on a clinical dataset of ultrasound\nimages. We show that by jointly learning for detection and segmentation, the\nproposed network is able to improve the segmentation accuracy compared to only\nlearning for segmentation. Code is publicly available at\nhttps://github.com/vincentzhang/roi-fcn.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 23:34:45 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 22:46:51 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhang", "Zichen", ""], ["Tang", "Min", ""], ["Cobzas", "Dana", ""], ["Zonoobi", "Dornoosh", ""], ["Jagersand", "Martin", ""], ["Jaremko", "Jacob L.", ""]]}, {"id": "1801.02728", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Yibin Xie, Zhengwei Zhou, Feng Shi, Anthony G.\n  Christodoulou, Debiao Li", "title": "Brain MRI Super Resolution Using 3D Deep Densely Connected Neural\n  Networks", "comments": "Accepted by ISBI'18", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363679", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance image (MRI) in high spatial resolution provides detailed\nanatomical information and is often necessary for accurate quantitative\nanalysis. However, high spatial resolution typically comes at the expense of\nlonger scan time, less spatial coverage, and lower signal to noise ratio (SNR).\nSingle Image Super-Resolution (SISR), a technique aimed to restore\nhigh-resolution (HR) details from one single low-resolution (LR) input image,\nhas been improved dramatically by recent breakthroughs in deep learning. In\nthis paper, we introduce a new neural network architecture, 3D Densely\nConnected Super-Resolution Networks (DCSRN) to restore HR features of\nstructural brain MR images. Through experiments on a dataset with 1,113\nsubjects, we demonstrate that our network outperforms bicubic interpolation as\nwell as other deep learning methods in restoring 4x resolution-reduced images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 23:56:32 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Chen", "Yuhua", ""], ["Xie", "Yibin", ""], ["Zhou", "Zhengwei", ""], ["Shi", "Feng", ""], ["Christodoulou", "Anthony G.", ""], ["Li", "Debiao", ""]]}, {"id": "1801.02730", "submitter": "Mario Michael Krell", "authors": "Mario Michael Krell, Anett Seeland, Su Kyoung Kim", "title": "Data Augmentation for Brain-Computer Interfaces: Analysis on\n  Event-Related Potentials Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On image data, data augmentation is becoming less relevant due to the large\namount of available training data and regularization techniques. Common\napproaches are moving windows (cropping), scaling, affine distortions, random\nnoise, and elastic deformations. For electroencephalographic data, the lack of\nsufficient training data is still a major issue. We suggest and evaluate\ndifferent approaches to generate augmented data using temporal and\nspatial/rotational distortions. Our results on the perception of rare stimuli\n(P300 data) and movement prediction (MRCP data) show that these approaches are\nfeasible and can significantly increase the performance of signal processing\nchains for brain-computer interfaces by 1% to 6%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:34:34 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Krell", "Mario Michael", ""], ["Seeland", "Anett", ""], ["Kim", "Su Kyoung", ""]]}, {"id": "1801.02753", "submitter": "Wengling Chen", "authors": "Wengling Chen, James Hays", "title": "SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic images from human drawn sketches is a challenging\nproblem in computer graphics and vision. Existing approaches either need exact\nedge maps, or rely on retrieval of existing photographs. In this work, we\npropose a novel Generative Adversarial Network (GAN) approach that synthesizes\nplausible images from 50 categories including motorcycles, horses and couches.\nWe demonstrate a data augmentation technique for sketches which is fully\nautomatic, and we show that the augmented data is helpful to our task. We\nintroduce a new network building block suitable for both the generator and\ndiscriminator which improves the information flow by injecting the input image\nat multiple scales. Compared to state-of-the-art image translation methods, our\napproach generates more realistic images and achieves significantly higher\nInception Scores.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 02:07:26 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 22:18:29 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Chen", "Wengling", ""], ["Hays", "James", ""]]}, {"id": "1801.02765", "submitter": "Minghui Liao", "authors": "Minghui Liao, Baoguang Shi, Xiang Bai", "title": "TextBoxes++: A Single-Shot Oriented Scene Text Detector", "comments": "15 pages", "journal-ref": "IEEE Transactions on Image Processing 27 (2018) 3676-3690", "doi": "10.1109/TIP.2018.2825107", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection is an important step of scene text recognition system\nand also a challenging problem. Different from general object detection, the\nmain challenges of scene text detection lie on arbitrary orientations, small\nsizes, and significantly variant aspect ratios of text in natural images. In\nthis paper, we present an end-to-end trainable fast scene text detector, named\nTextBoxes++, which detects arbitrary-oriented scene text with both high\naccuracy and efficiency in a single network forward pass. No post-processing\nother than an efficient non-maximum suppression is involved. We have evaluated\nthe proposed TextBoxes++ on four public datasets. In all experiments,\nTextBoxes++ outperforms competing methods in terms of text localization\naccuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of\n0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an\nf-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore,\ncombined with a text recognizer, TextBoxes++ significantly outperforms the\nstate-of-the-art approaches for word spotting and end-to-end text recognition\ntasks on popular benchmarks. Code is available at:\nhttps://github.com/MhLiao/TextBoxes_plusplus\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 02:40:49 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 14:16:19 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 09:02:20 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Liao", "Minghui", ""], ["Shi", "Baoguang", ""], ["Bai", "Xiang", ""]]}, {"id": "1801.02774", "submitter": "Justin Gilmer", "authors": "Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz,\n  Maithra Raghu, Martin Wattenberg, and Ian Goodfellow", "title": "Adversarial Spheres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art computer vision models have been shown to be vulnerable to\nsmall adversarial perturbations of the input. In other words, most images in\nthe data distribution are both correctly classified by the model and are very\nclose to a visually similar misclassified image. Despite substantial research\ninterest, the cause of the phenomenon is still poorly understood and remains\nunsolved. We hypothesize that this counter intuitive behavior is a naturally\noccurring result of the high dimensional geometry of the data manifold. As a\nfirst step towards exploring this hypothesis, we study a simple synthetic\ndataset of classifying between two concentric high dimensional spheres. For\nthis dataset we show a fundamental tradeoff between the amount of test error\nand the average distance to nearest error. In particular, we prove that any\nmodel which misclassifies a small constant fraction of a sphere will be\nvulnerable to adversarial perturbations of size $O(1/\\sqrt{d})$. Surprisingly,\nwhen we train several different architectures on this dataset, all of their\nerror sets naturally approach this theoretical bound. As a result of the\ntheory, the vulnerability of neural networks to small adversarial perturbations\nis a logical consequence of the amount of test error observed. We hope that our\ntheoretical analysis of this very simple case will point the way forward to\nexplore how the geometry of complex real-world data sets leads to adversarial\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 03:24:53 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 21:35:38 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 17:08:27 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Gilmer", "Justin", ""], ["Metz", "Luke", ""], ["Faghri", "Fartash", ""], ["Schoenholz", "Samuel S.", ""], ["Raghu", "Maithra", ""], ["Wattenberg", "Martin", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1801.02892", "submitter": "Kunal Swami", "authors": "Kunal Swami and Saikat Kumar Das", "title": "CANDY: Conditional Adversarial Networks based Fully End-to-End System\n  for Single Image Haze Removal", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image haze removal is a very challenging and ill-posed problem. The\nexisting haze removal methods in literature, including the recently introduced\ndeep learning methods, model the problem of haze removal as that of estimating\nintermediate parameters, viz., scene transmission map and atmospheric light.\nThese are used to compute the haze-free image from the hazy input image. Such\nan approach only focuses on accurate estimation of intermediate parameters,\nwhile the aesthetic quality of the haze-free image is unaccounted for in the\noptimization framework. Thus, errors in the estimation of intermediate\nparameters often lead to generation of inferior quality haze-free images. In\nthis paper, we present CANDY (Conditional Adversarial Networks based Dehazing\nof hazY images), a fully end-to-end model which directly generates a clean\nhaze-free image from a hazy input image. CANDY also incorporates the visual\nquality of haze-free image into the optimization function; thus, generating a\nsuperior quality haze-free image. To the best of our knowledge, this is the\nfirst work in literature to propose a fully end-to-end model for single image\nhaze removal. Also, this is the first work to explore the newly introduced\nconcept of generative adversarial networks for the problem of single image haze\nremoval. The proposed model CANDY was trained on a synthetically created haze\nimage dataset, while evaluation was performed on challenging synthetic as well\nas real haze image datasets. The extensive evaluation and comparison results of\nCANDY reveal that it significantly outperforms existing state-of-the-art haze\nremoval methods in literature, both quantitatively as well as qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 11:35:55 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 11:36:49 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Swami", "Kunal", ""], ["Das", "Saikat Kumar", ""]]}, {"id": "1801.02929", "submitter": "Hiroshi Inoue", "authors": "Hiroshi Inoue", "title": "Data Augmentation by Pairing Samples for Images Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a widely used technique in many machine learning tasks,\nsuch as image classification, to virtually enlarge the training dataset size\nand avoid overfitting. Traditional data augmentation techniques for image\nclassification tasks create new samples from the original training data by, for\nexample, flipping, distorting, adding a small amount of noise to, or cropping a\npatch from an original image. In this paper, we introduce a simple but\nsurprisingly effective data augmentation technique for image classification\ntasks. With our technique, named SamplePairing, we synthesize a new sample from\none image by overlaying another image randomly chosen from the training data\n(i.e., taking an average of two images for each pixel). By using two images\nrandomly selected from the training set, we can generate $N^2$ new samples from\n$N$ training samples. This simple data augmentation technique significantly\nimproved classification accuracy for all the tested datasets; for example, the\ntop-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset\nwith GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show\nthat our SamplePairing technique largely improved accuracy when the number of\nsamples in the training set was very small. Therefore, our technique is more\nvaluable for tasks with a limited amount of training data, such as medical\nimaging tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 13:37:11 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 13:28:07 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Inoue", "Hiroshi", ""]]}, {"id": "1801.03002", "submitter": "Ivona Tautkute", "authors": "Ivona Tautkute, Tomasz Trzcinski, Aleksander Skorupa, Lukasz Brocki\n  and Krzysztof Marasek", "title": "DeepStyle: Multimodal Search Engine for Fashion and Interior Design", "comments": "Copyright held by IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "IEEE Access 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multimodal search engine that combines visual and\ntextual cues to retrieve items from a multimedia database aesthetically similar\nto the query. The goal of our engine is to enable intuitive retrieval of\nfashion merchandise such as clothes or furniture. Existing search engines treat\ntextual input only as an additional source of information about the query image\nand do not correspond to the real-life scenario where the user looks for 'the\nsame shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those\nshortcomings by using a joint neural network architecture to model contextual\ndependencies between features of different modalities. We prove the robustness\nof this approach on two different challenging datasets of fashion items and\nfurniture where our DeepStyle engine outperforms baseline methods by 18-21% on\nthe tested datasets. Our search engine is commercially deployed and available\nthrough a Web-based application.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 14:08:55 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 12:44:10 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Tautkute", "Ivona", ""], ["Trzcinski", "Tomasz", ""], ["Skorupa", "Aleksander", ""], ["Brocki", "Lukasz", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1801.03039", "submitter": "Patryk Orzechowski", "authors": "Patryk Orzechowski, Moshe Sipper, Xiuzhen Huang, and Jason H. Moore", "title": "EBIC: an evolutionary-based parallel biclustering algorithm for pattern\n  discover", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bty401", "report-no": null, "categories": "cs.LG cs.CV cs.IR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel biclustering algorithm based on artificial intelligence\n(AI) is introduced. The method called EBIC aims to detect biologically\nmeaningful, order-preserving patterns in complex data. The proposed algorithm\nis probably the first one capable of discovering with accuracy exceeding 50%\nmultiple complex patterns in real gene expression datasets. It is also one of\nthe very few biclustering methods designed for parallel environments with\nmultiple graphics processing units (GPUs). We demonstrate that EBIC outperforms\nstate-of-the-art biclustering methods, in terms of recovery and relevance, on\nboth synthetic and genetic datasets. EBIC also yields results over 12 times\nfaster than the most accurate reference algorithms. The proposed algorithm is\nanticipated to be added to the repertoire of unsupervised machine learning\nalgorithms for the analysis of datasets, including those from large-scale\ngenomic studies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:13:07 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 14:08:11 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Orzechowski", "Patryk", ""], ["Sipper", "Moshe", ""], ["Huang", "Xiuzhen", ""], ["Moore", "Jason H.", ""]]}, {"id": "1801.03049", "submitter": "Eunbyung Park", "authors": "Eunbyung Park, Alexander C. Berg", "title": "Meta-Tracker: Fast and Robust Online Adaptation for Visual Object\n  Trackers", "comments": "Code: https://github.com/silverbottlep/meta_trackers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper improves state-of-the-art visual object trackers that use online\nadaptation. Our core contribution is an offline meta-learning-based method to\nadjust the initial deep networks used in online adaptation-based tracking. The\nmeta learning is driven by the goal of deep networks that can quickly be\nadapted to robustly model a particular target in future frames. Ideally the\nresulting models focus on features that are useful for future frames, and avoid\noverfitting to background clutter, small parts of the target, or noise. By\nenforcing a small number of update iterations during meta-learning, the\nresulting networks train significantly faster. We demonstrate this approach on\ntop of the high performance tracking approaches: tracking-by-detection based\nMDNet and the correlation based CREST. Experimental results on standard\nbenchmarks, OTB2015 and VOT2016, show that our meta-learned versions of both\ntrackers improve speed, accuracy, and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:38:10 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 19:48:00 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Park", "Eunbyung", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1801.03127", "submitter": "Gabriel Schwartz", "authors": "Gabriel Schwartz and Ko Nishino", "title": "Recognizing Material Properties from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans rely on properties of the materials that make up objects to guide our\ninteractions with them. Grasping smooth materials, for example, requires care,\nand softness is an ideal property for fabric used in bedding. Even when these\nproperties are not visual (e.g. softness is a physical property), we may still\ninfer their presence visually. We refer to such material properties as visual\nmaterial attributes. Recognizing these attributes in images can contribute\nvaluable information for general scene understanding and material recognition.\nUnlike well-known object and scene attributes, visual material attributes are\nlocal properties with no fixed shape or spatial extent. We show that given a\nset of images annotated with known material attributes, we may accurately\nrecognize the attributes from small local image patches. Obtaining such\nannotations in a consistent fashion at scale, however, is challenging. To\naddress this, we introduce a method that allows us to probe the human visual\nperception of materials by asking simple yes/no questions comparing pairs of\nimage patches. This provides sufficient weak supervision to build a set of\nattributes and associated classifiers that, while unnamed, serve the same\nfunction as the named attributes we use to describe materials. Doing so allows\nus to recognize visual material attributes without resorting to exhaustive\nmanual annotation of a fixed set of named attributes. Furthermore, we show that\nthis method may be integrated in the end-to-end learning of a material\nclassification CNN to simultaneously recognize materials and discover their\nvisual attributes. Our experimental results show that visual material\nattributes, whether named or automatically discovered, provide a useful\nintermediate representation for known material categories themselves as well as\na basis for transfer learning when recognizing previously-unseen categories.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:22:41 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Schwartz", "Gabriel", ""], ["Nishino", "Ko", ""]]}, {"id": "1801.03145", "submitter": "Yuxing Tang", "authors": "Yuxing Tang, Josiah Wang, Xiaofang Wang, Boyang Gao, Emmanuel\n  Dellandrea, Robert Gaizauskas, Liming Chen", "title": "Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised\n  Object Detection", "comments": "TPAMI. correct some typos", "journal-ref": "Published in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, November 2017", "doi": "10.1109/TPAMI.2017.2771779", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CNN-based object detection systems have achieved remarkable success on\nseveral large-scale object detection benchmarks. However, training such\ndetectors requires a large number of labeled bounding boxes, which are more\ndifficult to obtain than image-level annotations. Previous work addresses this\nissue by transforming image-level classifiers into object detectors. This is\ndone by modeling the differences between the two on categories with both\nimage-level and bounding box annotations, and transferring this information to\nconvert classifiers to detectors for categories without bounding box\nannotations. We improve this previous work by incorporating knowledge about\nobject similarities from visual and semantic domains during the transfer\nprocess. The intuition behind our proposed method is that visually and\nsemantically similar categories should exhibit more common transferable\nproperties than dissimilar categories, e.g. a better detector would result by\ntransforming the differences between a dog classifier and a dog detector onto\nthe cat class, than would by transforming from the violin class. Experimental\nresults on the challenging ILSVRC2013 detection dataset demonstrate that each\nof our proposed object similarity based knowledge transfer methods outperforms\nthe baseline methods. We found strong evidence that visual similarity and\nsemantic relatedness are complementary for the task, and when combined notably\nimprove detection, achieving state-of-the-art detection performance in a\nsemi-supervised setting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:29:12 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 16:09:50 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Tang", "Yuxing", ""], ["Wang", "Josiah", ""], ["Wang", "Xiaofang", ""], ["Gao", "Boyang", ""], ["Dellandrea", "Emmanuel", ""], ["Gaizauskas", "Robert", ""], ["Chen", "Liming", ""]]}, {"id": "1801.03149", "submitter": "Bangalore Ravi Kiran", "authors": "B Ravi Kiran, Dilip Mathew Thomas, Ranjith Parakkal", "title": "An overview of deep learning based methods for unsupervised and\n  semi-supervised anomaly detection in videos", "comments": "15 pages, double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos represent the primary source of information for surveillance\napplications and are available in large amounts but in most cases contain\nlittle or no annotation for supervised learning. This article reviews the\nstate-of-the-art deep learning based methods for video anomaly detection and\ncategorizes them based on the type of model and criteria of detection. We also\nperform simple studies to understand the different approaches and provide the\ncriteria of evaluation for spatio-temporal anomaly detection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:44:26 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 10:50:11 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Kiran", "B Ravi", ""], ["Thomas", "Dilip Mathew", ""], ["Parakkal", "Ranjith", ""]]}, {"id": "1801.03150", "submitter": "Mathew Monfort", "authors": "Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah\n  Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick,\n  Aude Oliva", "title": "Moments in Time Dataset: one million videos for event understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Moments in Time Dataset, a large-scale human-annotated\ncollection of one million short videos corresponding to dynamic events\nunfolding within three seconds. Modeling the spatial-audio-temporal dynamics\neven for actions occurring in 3 second videos poses many challenges: meaningful\nevents do not include only people, but also objects, animals, and natural\nphenomena; visual and auditory events can be symmetrical in time (\"opening\" is\n\"closing\" in reverse), and either transient or sustained. We describe the\nannotation process of our dataset (each video is tagged with one action or\nactivity label among 339 different classes), analyze its scale and diversity in\ncomparison to other large-scale video datasets for action recognition, and\nreport results of several baseline models addressing separately, and jointly,\nthree modalities: spatial, temporal and auditory. The Moments in Time dataset,\ndesigned to have a large coverage and diversity of events in both visual and\nauditory modalities, can serve as a new challenge to develop models that scale\nto the level of complexity and abstract reasoning that a human processes on a\ndaily basis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:46:38 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 15:25:18 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 13:20:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Monfort", "Mathew", ""], ["Andonian", "Alex", ""], ["Zhou", "Bolei", ""], ["Ramakrishnan", "Kandan", ""], ["Bargal", "Sarah Adel", ""], ["Yan", "Tom", ""], ["Brown", "Lisa", ""], ["Fan", "Quanfu", ""], ["Gutfruend", "Dan", ""], ["Vondrick", "Carl", ""], ["Oliva", "Aude", ""]]}, {"id": "1801.03182", "submitter": "Min Xian", "authors": "Min Xian, Yingtao Zhang, H. D. Cheng, Fei Xu, Kuan Huang, Boyu Zhang,\n  Jianrui Ding, Chunping Ning, Ying Wang", "title": "A Benchmark for Breast Ultrasound Image Segmentation (BUSIS)", "comments": "9 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast ultrasound (BUS) image segmentation is challenging and critical for\nBUS Computer-Aided Diagnosis (CAD) systems. Many BUS segmentation approaches\nhave been proposed in the last two decades, but the performances of most\napproaches have been assessed using relatively small private datasets with\ndiffer-ent quantitative metrics, which result in discrepancy in performance\ncomparison. Therefore, there is a pressing need for building a benchmark to\ncompare existing methods using a public dataset objectively, and to determine\nthe performance of the best breast tumor segmentation algorithm available today\nand to investigate what segmentation strategies are valuable in clinical\npractice and theoretical study. In this work, we will publish a B-mode BUS\nimage segmentation benchmark (BUSIS) with 562 images and compare the\nperformance of five state-of-the-art BUS segmentation methods quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 23:19:00 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Xian", "Min", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Xu", "Fei", ""], ["Huang", "Kuan", ""], ["Zhang", "Boyu", ""], ["Ding", "Jianrui", ""], ["Ning", "Chunping", ""], ["Wang", "Ying", ""]]}, {"id": "1801.03228", "submitter": "Nilavra Bhattacharya", "authors": "Swalpa Kumar Roy, Nilavra Bhattacharya, Bhabatosh Chanda, Bidyut B.\n  Chaudhuri and Dipak Kumar Ghosh", "title": "FWLBP: A Scale Invariant Descriptor for Texture Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel texture descriptor called Fractal Weighted\nLocal Binary Pattern (FWLBP). The fractal dimension (FD) measure is relatively\ninvariant to scale-changes, and presents a good correlation with human\nviewpoint of surface roughness. We have utilized this property to construct a\nscale-invariant descriptor. Here, the input image is sampled using an augmented\nform of the local binary pattern (LBP) over three different radii, and then\nused an indexing operation to assign FD weights to the collected samples. The\nfinal histogram of the descriptor has its features calculated using LBP, and\nits weights computed from the FD image. The proposed descriptor is scale\ninvariant, and is also robust in rotation or reflection, and partially tolerant\nto noise and illumination changes. In addition, the local fractal dimension is\nrelatively insensitive to the bi-Lipschitz transformations, whereas its\nextension is adequate to precisely discriminate the fundamental of texture\nprimitives. Experiment results carried out on standard texture databases show\nthat the proposed descriptor achieved better classification rates compared to\nthe state-of-the-art descriptors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 03:32:31 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 10:36:24 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Roy", "Swalpa Kumar", ""], ["Bhattacharya", "Nilavra", ""], ["Chanda", "Bhabatosh", ""], ["Chaudhuri", "Bidyut B.", ""], ["Ghosh", "Dipak Kumar", ""]]}, {"id": "1801.03230", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Pujan Kandel, Candice W. Bolan, Michael B. Wallace,\n  and Ulas Bagci", "title": "Lung and Pancreatic Tumor Characterization in the Deep Learning Era:\n  Novel Supervised and Unsupervised Learning Approaches", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging 2019", "journal-ref": null, "doi": "10.1109/TMI.2019.2894349", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk stratification (characterization) of tumors from radiology images can be\nmore accurate and faster with computer-aided diagnosis (CAD) tools. Tumor\ncharacterization through such tools can also enable non-invasive cancer\nstaging, prognosis, and foster personalized treatment planning as a part of\nprecision medicine. In this study, we propose both supervised and unsupervised\nmachine learning strategies to improve tumor characterization. Our first\napproach is based on supervised learning for which we demonstrate significant\ngains with deep learning algorithms, particularly by utilizing a 3D\nConvolutional Neural Network and Transfer Learning. Motivated by the\nradiologists' interpretations of the scans, we then show how to incorporate\ntask dependent feature representations into a CAD system via a\ngraph-regularized sparse Multi-Task Learning (MTL) framework. In the second\napproach, we explore an unsupervised learning algorithm to address the limited\navailability of labeled training data, a common problem in medical imaging\napplications. Inspired by learning from label proportion (LLP) approaches in\ncomputer vision, we propose to use proportion-SVM for characterizing tumors. We\nalso seek the answer to the fundamental question about the goodness of \"deep\nfeatures\" for unsupervised tumor classification. We evaluate our proposed\nsupervised and unsupervised learning algorithms on two different tumor\ndiagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans,\nrespectively, and obtain the state-of-the-art sensitivity and specificity\nresults in both problems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 03:47:07 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 05:30:33 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 13:25:51 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Kandel", "Pujan", ""], ["Bolan", "Candice W.", ""], ["Wallace", "Michael B.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1801.03252", "submitter": "Ziqiang Zheng", "authors": "Ziqiang Zheng and Chao Wang and Zhibin Yu and Haiyong Zheng and Bing\n  Zheng", "title": "Instance Map based Image Synthesis with a Denoising Generative\n  Adversarial Network", "comments": "10 pages, 16figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic layouts based Image synthesizing, which has benefited from the\nsuccess of Generative Adversarial Network (GAN), has drawn much attention in\nthese days. How to enhance the synthesis image equality while keeping the\nstochasticity of the GAN is still a challenge. We propose a novel denoising\nframework to handle this problem. The overlapped objects generation is another\nchallenging task when synthesizing images from a semantic layout to a realistic\nRGB photo. To overcome this deficiency, we include a one-hot semantic label map\nto force the generator paying more attention on the overlapped objects\ngeneration. Furthermore, we improve the loss function of the discriminator by\nconsidering perturb loss and cascade layer loss to guide the generation\nprocess. We applied our methods on the Cityscapes, Facades and NYU datasets and\ndemonstrate the image generation ability of our model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 07:16:46 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Wang", "Chao", ""], ["Yu", "Zhibin", ""], ["Zheng", "Haiyong", ""], ["Zheng", "Bing", ""]]}, {"id": "1801.03299", "submitter": "Tatsuya Yokota", "authors": "Tatsuya Yokota and Hidekata Hontani", "title": "Simultaneous Tensor Completion and Denoising by Noise Inequality\n  Constrained Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion is a technique of filling missing elements of the\nincomplete data tensors. It being actively studied based on the convex\noptimization scheme such as nuclear-norm minimization. When given data tensors\ninclude some noises, the nuclear-norm minimization problem is usually converted\nto the nuclear-norm `regularization' problem which simultaneously minimize\npenalty and error terms with some trade-off parameter. However, the good value\nof trade-off is not easily determined because of the difference of two units\nand the data dependence. In the sense of trade-off tuning, the noisy tensor\ncompletion problem with the `noise inequality constraint' is better choice than\nthe `regularization' because the good noise threshold can be easily bounded\nwith noise standard deviation. In this study, we tackle to solve the convex\ntensor completion problems with two types of noise inequality constraints:\nGaussian and Laplace distributions. The contributions of this study are\nfollows: (1) New tensor completion and denoising models using tensor total\nvariation and nuclear-norm are proposed which can be characterized as a\ngeneralization/extension of many past matrix and tensor completion models, (2)\nproximal mappings for noise inequalities are derived which are analytically\ncomputable with low computational complexity, (3) convex optimization algorithm\nis proposed based on primal-dual splitting framework, (4) new step-size\nadaptation method is proposed to accelerate the optimization, and (5) extensive\nexperiments demonstrated the advantages of the proposed method for visual data\nretrieval such as for color images, movies, and 3D-volumetric data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 10:45:26 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Yokota", "Tatsuya", ""], ["Hontani", "Hidekata", ""]]}, {"id": "1801.03318", "submitter": "Deepak Mishra", "authors": "Deepak Mishra, Santanu Chaudhury, Mukul Sarkar and Arvinder Singh Soin", "title": "Unsupervised Despeckling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast and quality of ultrasound images are adversely affected by the\nexcessive presence of speckle. However, being an inherent imaging property,\nspeckle helps in tissue characterization and tracking. Thus, despeckling of the\nultrasound images requires the reduction of speckle extent without any\noversmoothing. In this letter, we aim to address the despeckling problem using\nan unsupervised deep adversarial approach. A despeckling residual neural\nnetwork (DRNN) is trained with an adversarial loss imposed by a discriminator.\nThe discriminator tries to differentiate between the despeckled images\ngenerated by the DRNN and the set of high-quality images. Further to prevent\nthe developed DRNN from oversmoothing, a structural loss term is used along\nwith the adversarial loss. Experimental evaluations show that the proposed DRNN\nis able to outperform the state-of-the-art despeckling approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 11:44:48 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Mishra", "Deepak", ""], ["Chaudhury", "Santanu", ""], ["Sarkar", "Mukul", ""], ["Soin", "Arvinder Singh", ""]]}, {"id": "1801.03399", "submitter": "Chi Li", "authors": "Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D.Hager, and\n  Manmohan Chandraker", "title": "Deep Supervision with Intermediate Concepts", "comments": "Submitted to TPAMI, first revision. arXiv admin note: text overlap\n  with arXiv:1612.02699", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent data-driven approaches to scene interpretation predominantly pose\ninference as an end-to-end black-box mapping, commonly performed by a\nConvolutional Neural Network (CNN). However, decades of work on perceptual\norganization in both human and machine vision suggests that there are often\nintermediate representations that are intrinsic to an inference task, and which\nprovide essential structure to improve generalization. In this work, we explore\nan approach for injecting prior domain structure into neural network training\nby supervising hidden layers of a CNN with intermediate concepts that normally\nare not observed in practice. We formulate a probabilistic framework which\nformalizes these notions and predicts improved generalization via this deep\nsupervision method. One advantage of this approach is that we are able to train\nonly from synthetic CAD renderings of cluttered scenes, where concept values\ncan be extracted, but apply the results to real images. Our implementation\nachieves the state-of-the-art performance of 2D/3D keypoint localization and\nimage classification on real image benchmarks, including KITTI, PASCAL VOC,\nPASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach\noutperforms alternative forms of supervision, such as multi-task networks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 22:00:48 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 04:10:35 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Li", "Chi", ""], ["Zia", "M. Zeeshan", ""], ["Tran", "Quoc-Huy", ""], ["Yu", "Xiang", ""], ["Hager", "Gregory D.", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1801.03431", "submitter": "Maxime Lafarge", "authors": "Maxime W. Lafarge, Josien P.W. Pluim, Koen A.J. Eppenhof, Pim\n  Moeskops, Mitko Veta", "title": "Inferring a Third Spatial Dimension from 2D Histological Images", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histological images are obtained by transmitting light through a tissue\nspecimen that has been stained in order to produce contrast. This process\nresults in 2D images of the specimen that has a three-dimensional structure. In\nthis paper, we propose a method to infer how the stains are distributed in the\ndirection perpendicular to the surface of the slide for a given 2D image in\norder to obtain a 3D representation of the tissue. This inference is achieved\nby decomposition of the staining concentration maps under constraints that\nensure realistic decomposition and reconstruction of the original 2D images.\nOur study shows that it is possible to generate realistic 3D images making this\nmethod a potential tool for data augmentation when training deep learning\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 15:59:12 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Lafarge", "Maxime W.", ""], ["Pluim", "Josien P. W.", ""], ["Eppenhof", "Koen A. J.", ""], ["Moeskops", "Pim", ""], ["Veta", "Mitko", ""]]}, {"id": "1801.03454", "submitter": "Ruth Fong", "authors": "Ruth Fong and Andrea Vedaldi", "title": "Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters\n  in Deep Neural Networks", "comments": "Camera-Ready for CVPR18; supplementary materials:\n  http://ruthcfong.github.io/files/net2vec_supps.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to understand the meaning of the intermediate representations\ncaptured by deep networks, recent papers have tried to associate specific\nsemantic concepts to individual neural network filter responses, where\ninteresting correlations are often found, largely by focusing on extremal\nfilter responses. In this paper, we show that this approach can favor\neasy-to-interpret cases that are not necessarily representative of the average\nbehavior of a representation.\n  A more realistic but harder-to-study hypothesis is that semantic\nrepresentations are distributed, and thus filters must be studied in\nconjunction. In order to investigate this idea while enabling systematic\nvisualization and quantification of multiple filter responses, we introduce the\nNet2Vec framework, in which semantic concepts are mapped to vectorial\nembeddings based on corresponding filter responses. By studying such\nembeddings, we are able to show that 1., in most cases, multiple filters are\nrequired to code for a concept, that 2., often filters are not concept specific\nand help encode multiple concepts, and that 3., compared to single filter\nactivations, filter embeddings are able to better characterize the meaning of a\nrepresentation and its relationship to other concepts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 17:01:36 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 11:00:02 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1801.03458", "submitter": "Luona Yang", "authors": "Luona Yang, Xiaodan Liang, Tairui Wang, Eric Xing", "title": "Real-to-Virtual Domain Unification for End-to-End Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the spectrum of vision-based autonomous driving, vanilla end-to-end models\nare not interpretable and suboptimal in performance, while mediated perception\nmodels require additional intermediate representations such as segmentation\nmasks or detection bounding boxes, whose annotation can be prohibitively\nexpensive as we move to a larger scale. More critically, all prior works fail\nto deal with the notorious domain shift if we were to merge data collected from\ndifferent sources, which greatly hinders the model generalization ability. In\nthis work, we address the above limitations by taking advantage of virtual data\ncollected from driving simulators, and present DU-drive, an unsupervised\nreal-to-virtual domain unification framework for end-to-end autonomous driving.\nIt first transforms real driving data to its less complex counterpart in the\nvirtual domain and then predicts vehicle control commands from the generated\nvirtual image. Our framework has three unique advantages: 1) it maps driving\ndata collected from a variety of source distributions into a unified domain,\neffectively eliminating domain shift; 2) the learned virtual representation is\nsimpler than the input real image and closer in form to the \"minimum sufficient\nstatistic\" for the prediction task, which relieves the burden of the\ncompression phase while optimizing the information bottleneck tradeoff and\nleads to superior prediction performance; 3) it takes advantage of annotated\nvirtual data which is unlimited and free to obtain. Extensive experiments on\ntwo public driving datasets and two driving simulators demonstrate the\nperformance superiority and interpretive capability of DU-drive.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 17:08:54 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 05:11:13 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Yang", "Luona", ""], ["Liang", "Xiaodan", ""], ["Wang", "Tairui", ""], ["Xing", "Eric", ""]]}, {"id": "1801.03493", "submitter": "Kevin Hsieh", "authors": "Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Paramvir Bahl,\n  Matthai Philipose, Phillip B. Gibbons, Onur Mutlu", "title": "Focus: Querying Large Video Datasets with Low Latency and Low Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of videos are continuously recorded from cameras deployed for\ntraffic control and surveillance with the goal of answering \"after the fact\"\nqueries: identify video frames with objects of certain classes (cars, bags)\nfrom many days of recorded video. While advancements in convolutional neural\nnetworks (CNNs) have enabled answering such queries with high accuracy, they\nare too expensive and slow. We build Focus, a system for low-latency and\nlow-cost querying on large video datasets. Focus uses cheap ingestion\ntechniques to index the videos by the objects occurring in them. At\ningest-time, it uses compression and video-specific specialization of CNNs.\nFocus handles the lower accuracy of the cheap CNNs by judiciously leveraging\nexpensive CNNs at query-time. To reduce query time latency, we cluster similar\nobjects and hence avoid redundant processing. Using experiments on video\nstreams from traffic, surveillance and news channels, we see that Focus uses\n58X fewer GPU cycles than running expensive ingest processors and is 37X faster\nthan processing all the video at query time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 18:52:25 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Hsieh", "Kevin", ""], ["Ananthanarayanan", "Ganesh", ""], ["Bodik", "Peter", ""], ["Bahl", "Paramvir", ""], ["Philipose", "Matthai", ""], ["Gibbons", "Phillip B.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1801.03530", "submitter": "Jianshu Zhang", "authors": "Jianshu Zhang and Jun Du and Lirong Dai", "title": "Multi-Scale Attention with Dense Encoder for Handwritten Mathematical\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten mathematical expression recognition is a challenging problem due\nto the complicated two-dimensional structures, ambiguous handwriting input and\nvariant scales of handwritten math symbols. To settle this problem, we utilize\nthe attention based encoder-decoder model that recognizes mathematical\nexpression images from two-dimensional layouts to one-dimensional LaTeX\nstrings. We improve the encoder by employing densely connected convolutional\nnetworks as they can strengthen feature extraction and facilitate gradient\npropagation especially on a small training set. We also present a novel\nmulti-scale attention model which is employed to deal with the recognition of\nmath symbols in different scales and save the fine-grained details that will be\ndropped by pooling operations. Validated on the CROHME competition task, the\nproposed method significantly outperforms the state-of-the-art methods with an\nexpression recognition accuracy of 52.8% on CROHME 2014 and 50.1% on CROHME\n2016, by only using the official training dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 09:22:42 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 01:52:21 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Zhang", "Jianshu", ""], ["Du", "Jun", ""], ["Dai", "Lirong", ""]]}, {"id": "1801.03546", "submitter": "Upal Mahbub", "authors": "Upal Mahbub and Sayantan Sarkar and Rama Chellappa", "title": "Segment-based Methods for Facial Attribute Detection from Partial Faces", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing, 2018", "doi": "10.1109/TAFFC.2018.2820048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods of attribute detection from faces almost always\nassume the presence of a full, unoccluded face. Hence, their performance\ndegrades for partially visible and occluded faces. In this paper, we introduce\nSPLITFACE, a deep convolutional neural network-based method that is explicitly\ndesigned to perform attribute detection in partially occluded faces. Taking\nseveral facial segments and the full face as input, the proposed method takes a\ndata driven approach to determine which attributes are localized in which\nfacial segments. The unique architecture of the network allows each attribute\nto be predicted by multiple segments, which permits the implementation of\ncommittee machine techniques for combining local and global decisions to boost\nperformance. With access to segment-based predictions, SPLITFACE can predict\nwell those attributes which are localized in the visible parts of the face,\nwithout having to rely on the presence of the whole face. We use the CelebA and\nLFWA facial attribute datasets for standard evaluations. We also modify both\ndatasets, to occlude the faces, so that we can evaluate the performance of\nattribute detection algorithms on partial faces. Our evaluation shows that\nSPLITFACE significantly outperforms other recent methods especially for partial\nfaces.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 20:32:35 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mahbub", "Upal", ""], ["Sarkar", "Sayantan", ""], ["Chellappa", "Rama", ""]]}, {"id": "1801.03551", "submitter": "Farnoosh Ghadiri", "authors": "Farnoosh Ghadiri, Robert Bergevin, Guillaume-Alexandre Bilodeau", "title": "From Superpixel to Human Shape Modelling for Carried Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting carried objects is one of the requirements for developing systems\nto reason about activities involving people and objects. We present an approach\nto detect carried objects from a single video frame with a novel method that\nincorporates features from multiple scales. Initially, a foreground mask in a\nvideo frame is segmented into multi-scale superpixels. Then the human-like\nregions in the segmented area are identified by matching a set of extracted\nfeatures from superpixels against learned features in a codebook. A carried\nobject probability map is generated using the complement of the matching\nprobabilities of superpixels to human-like regions and background information.\nA group of superpixels with high carried object probability and strong edge\nsupport is then merged to obtain the shape of the carried object. We applied\nour method to two challenging datasets, and results show that our method is\ncompetitive with or better than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 21:07:13 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Ghadiri", "Farnoosh", ""], ["Bergevin", "Robert", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1801.03754", "submitter": "Cigdem Turan", "authors": "Cigdem Turan, Kin-Man Lam, and Xiangjian He", "title": "Soft Locality Preserving Map (SLPM) for Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For image recognition, an extensive number of methods have been proposed to\novercome the high-dimensionality problem of feature vectors being used. These\nmethods vary from unsupervised to supervised, and from statistics to\ngraph-theory based. In this paper, the most popular and the state-of-the-art\nmethods for dimensionality reduction are firstly reviewed, and then a new and\nmore efficient manifold-learning method, named Soft Locality Preserving Map\n(SLPM), is presented. Furthermore, feature generation and sample selection are\nproposed to achieve better manifold learning. SLPM is a graph-based\nsubspace-learning method, with the use of k-neighbourhood information and the\nclass information. The key feature of SLPM is that it aims to control the level\nof spread of the different classes, because the spread of the classes in the\nunderlying manifold is closely connected to the generalizability of the learned\nsubspace. Our proposed manifold-learning method can be applied to various\npattern recognition applications, and we evaluate its performances on facial\nexpression recognition. Experiments on databases, such as the Bahcesehir\nUniversity Multilingual Affective Face Database (BAUM-2), the Extended\nCohn-Kanade (CK+) Database, the Japanese Female Facial Expression (JAFFE)\nDatabase, and the Taiwanese Facial Expression Image Database (TFEID), show that\nSLPM can effectively reduce the dimensionality of the feature vectors and\nenhance the discriminative power of the extracted features for expression\nrecognition. Furthermore, the proposed feature-generation method can improve\nthe generalizability of the underlying manifolds for facial expression\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 13:39:24 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Turan", "Cigdem", ""], ["Lam", "Kin-Man", ""], ["He", "Xiangjian", ""]]}, {"id": "1801.03800", "submitter": "Dario Prandi", "authors": "Ugo Boscain, Roman Chertovskih, Jean-Paul Gauthier, Dario Prandi and\n  Alexey Remizov", "title": "Cortical-inspired image reconstruction via sub-Riemannian geometry and\n  hypoelliptic diffusion", "comments": null, "journal-ref": "ESAIM: ProcS. 64 (2018), pp. 37-53", "doi": "10.1051/proc/201864037", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review several algorithms for image inpainting based on the\nhypoelliptic diffusion naturally associated with a mathematical model of the\nprimary visual cortex. In particular, we present one algorithm that does not\nexploit the information of where the image is corrupted, and others that do it.\nWhile the first algorithm is able to reconstruct only images that our visual\nsystem is still capable of recognize, we show that those of the second type\ncompletely transcend such limitation providing reconstructions at the\nstate-of-the-art in image inpainting. This can be interpreted as a validation\nof the fact that our visual cortex actually encodes the first type of\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 14:59:54 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Boscain", "Ugo", ""], ["Chertovskih", "Roman", ""], ["Gauthier", "Jean-Paul", ""], ["Prandi", "Dario", ""], ["Remizov", "Alexey", ""]]}, {"id": "1801.03910", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Alexei A. Efros, Jitendra Malik", "title": "Multi-view Consistency as Supervisory Signal for Learning Shape and Pose\n  Prediction", "comments": "Project url with code: https://shubhtuls.github.io/mvcSnP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning single-view shape and pose prediction\nwithout using direct supervision for either. Our approach allows leveraging\nmulti-view observations from unknown poses as supervisory signal during\ntraining. Our proposed training setup enforces geometric consistency between\nthe independently predicted shape and pose from two views of the same instance.\nWe consequently learn to predict shape in an emergent canonical (view-agnostic)\nframe along with a corresponding pose predictor. We show empirical and\nqualitative results using the ShapeNet dataset and observe encouragingly\ncompetitive performance to previous techniques which rely on stronger forms of\nsupervision. We also demonstrate the applicability of our framework in a\nrealistic setting which is beyond the scope of existing techniques: using a\ntraining dataset comprised of online product images where the underlying shape\nand pose are unknown.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 18:15:47 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 17:27:36 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Efros", "Alexei A.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1801.03924", "submitter": "Richard Zhang", "authors": "Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver\n  Wang", "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "comments": "Accepted to CVPR 2018; Code and data available at\n  https://www.github.com/richzhang/PerceptualSimilarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is nearly effortless for humans to quickly assess the perceptual\nsimilarity between two images, the underlying processes are thought to be quite\ncomplex. Despite this, the most widely used perceptual metrics today, such as\nPSNR and SSIM, are simple, shallow functions, and fail to account for many\nnuances of human perception. Recently, the deep learning community has found\nthat features of the VGG network trained on ImageNet classification has been\nremarkably useful as a training loss for image synthesis. But how perceptual\nare these so-called \"perceptual losses\"? What elements are critical for their\nsuccess? To answer these questions, we introduce a new dataset of human\nperceptual similarity judgments. We systematically evaluate deep features\nacross different architectures and tasks and compare them with classic metrics.\nWe find that deep features outperform all previous metrics by large margins on\nour dataset. More surprisingly, this result is not restricted to\nImageNet-trained VGG features, but holds across different deep architectures\nand levels of supervision (supervised, self-supervised, or even unsupervised).\nOur results suggest that perceptual similarity is an emergent property shared\nacross deep visual representations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 18:54:17 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 19:25:07 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zhang", "Richard", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""], ["Shechtman", "Eli", ""], ["Wang", "Oliver", ""]]}, {"id": "1801.03983", "submitter": "Mingze Xu", "authors": "Mingze Xu, Aidean Sharghi, Xin Chen, David J Crandall", "title": "Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low\n  Resolution Action Recognition", "comments": "9 pagers, 5 figures, published in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major emerging challenge is how to protect people's privacy as cameras and\ncomputer vision are increasingly integrated into our daily lives, including in\nsmart devices inside homes. A potential solution is to capture and record just\nthe minimum amount of information needed to perform a task of interest. In this\npaper, we propose a fully-coupled two-stream spatiotemporal architecture for\nreliable human action recognition on extremely low resolution (e.g., 12x16\npixel) videos. We provide an efficient method to extract spatial and temporal\nfeatures and to aggregate them into a robust feature representation for an\nentire action video sequence. We also consider how to incorporate high\nresolution videos during training in order to build better low resolution\naction recognition models. We evaluate on two publicly-available datasets,\nshowing significant improvements over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 20:39:30 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Xu", "Mingze", ""], ["Sharghi", "Aidean", ""], ["Chen", "Xin", ""], ["Crandall", "David J", ""]]}, {"id": "1801.03986", "submitter": "Mingze Xu", "authors": "Mingze Xu, Chenyou Fan, John D Paden, Geoffrey C Fox, David J Crandall", "title": "Multi-Task Spatiotemporal Neural Networks for Structured Surface\n  Reconstruction", "comments": "10 pages, 7 figures, published in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have surpassed the performance of traditional\ntechniques on a wide range of problems in computer vision, but nearly all of\nthis work has studied consumer photos, where precisely correct output is often\nnot critical. It is less clear how well these techniques may apply on\nstructured prediction problems where fine-grained output with high precision is\nrequired, such as in scientific imaging domains. Here we consider the problem\nof segmenting echogram radar data collected from the polar ice sheets, which is\nchallenging because segmentation boundaries are often very weak and there is a\nhigh degree of noise. We propose a multi-task spatiotemporal neural network\nthat combines 3D ConvNets and Recurrent Neural Networks (RNNs) to estimate ice\nsurface boundaries from sequences of tomographic radar images. We show that our\nmodel outperforms the state-of-the-art on this problem by (1) avoiding the need\nfor hand-tuned parameters, (2) extracting multiple surfaces (ice-air and\nice-bed) simultaneously, (3) requiring less non-visual metadata, and (4) being\nabout 6 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 20:47:33 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 21:30:25 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Xu", "Mingze", ""], ["Fan", "Chenyou", ""], ["Paden", "John D", ""], ["Fox", "Geoffrey C", ""], ["Crandall", "David J", ""]]}, {"id": "1801.04011", "submitter": "Cameron Fabbri", "authors": "Cameron Fabbri, Md Jahidul Islam, Junaed Sattar", "title": "Enhancing Underwater Imagery using Generative Adversarial Networks", "comments": "Submitted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous underwater vehicles (AUVs) rely on a variety of sensors -\nacoustic, inertial and visual - for intelligent decision making. Due to its\nnon-intrusive, passive nature, and high information content, vision is an\nattractive sensing modality, particularly at shallower depths. However, factors\nsuch as light refraction and absorption, suspended particles in the water, and\ncolor distortion affect the quality of visual data, resulting in noisy and\ndistorted images. AUVs that rely on visual sensing thus face difficult\nchallenges, and consequently exhibit poor performance on vision-driven tasks.\nThis paper proposes a method to improve the quality of visual underwater scenes\nusing Generative Adversarial Networks (GANs), with the goal of improving input\nto vision-driven behaviors further down the autonomy pipeline. Furthermore, we\nshow how recently proposed methods are able to generate a dataset for the\npurpose of such underwater image restoration. For any visually-guided\nunderwater robots, this improvement can result in increased safety and\nreliability through robust visual perception. To that effect, we present\nquantitative and qualitative data which demonstrates that images corrected\nthrough the proposed approach generate more visually appealing images, and also\nprovide increased accuracy for a diver tracking algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 22:58:42 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Fabbri", "Cameron", ""], ["Islam", "Md Jahidul", ""], ["Sattar", "Junaed", ""]]}, {"id": "1801.04012", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "Non-Rigid Image Registration Using Self-Supervised Fully Convolutional\n  Networks without Training Data", "comments": "Accepted by IEEE International Symposium on Biomedical Imaging 2018\n  (ISBI'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel non-rigid image registration algorithm is built upon fully\nconvolutional networks (FCNs) to optimize and learn spatial transformations\nbetween pairs of images to be registered in a self-supervised learning\nframework. Different from most existing deep learning based image registration\nmethods that learn spatial transformations from training data with known\ncorresponding spatial transformations, our method directly estimates spatial\ntransformations between pairs of images by maximizing an image-wise similarity\nmetric between fixed and deformed moving images, similar to conventional image\nregistration algorithms. The image registration is implemented in a\nmulti-resolution image registration framework to jointly optimize and learn\nspatial transformations and FCNs at different spatial resolutions with deep\nself-supervision through typical feedforward and backpropagation computation.\nThe proposed method has been evaluated for registering 3D structural brain\nmagnetic resonance (MR) images and obtained better performance than\nstate-of-the-art image registration algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 22:58:48 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "1801.04013", "submitter": "Hongming Li", "authors": "Hongming Li, Theodore D. Satterthwaite, Yong Fan", "title": "Brain Age Prediction Based on Resting-State Functional Connectivity\n  Patterns Using Convolutional Neural Networks", "comments": "Accepted by IEEE International Symposium on Biomedical Imaging 2018\n  (ISBI'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain age prediction based on neuroimaging data could help characterize both\nthe typical brain development and neuropsychiatric disorders. Pattern\nrecognition models built upon functional connectivity (FC) measures derived\nfrom resting state fMRI (rsfMRI) data have been successfully used to predict\nthe brain age. However, most existing studies focus on coarse-grained FC\nmeasures between brain regions or intrinsic connectivity networks (ICNs), which\nmay sacrifice fine-grained FC information of the rsfMRI data. Whole brain\nvoxel-wise FC measures could provide fine-grained FC information of the brain\nand may improve the prediction performance. In this study, we develop a deep\nlearning method to use convolutional neural networks (CNNs) to learn\ninformative features from the fine-grained whole brain FC measures for the\nbrain age prediction. Experimental results on a large dataset of resting-state\nfMRI demonstrate that the deep learning model with fine-grained FC measures\ncould better predict the brain age.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 23:09:38 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Li", "Hongming", ""], ["Satterthwaite", "Theodore D.", ""], ["Fan", "Yong", ""]]}, {"id": "1801.04018", "submitter": "Jordan Malof", "authors": "Joseph Camilo, Rui Wang, Leslie M. Collins, Kyle Bradbury, Jordan M.\n  Malof", "title": "Application of a semantic segmentation convolutional neural network for\n  accurate automatic detection and mapping of solar photovoltaic arrays in\n  aerial imagery", "comments": "Accepted for publication at the 2017 IEEE Applied Imagery Pattern\n  Recognition (AIPR) Workshop. Presented at the conference in Washington D.C.,\n  October 10-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automatically detecting small-scale solar\nphotovoltaic arrays for behind-the-meter energy resource assessment in high\nresolution aerial imagery. Such algorithms offer a faster and more\ncost-effective solution to collecting information on distributed solar\nphotovoltaic (PV) arrays, such as their location, capacity, and generated\nenergy. The surface area of PV arrays, a characteristic which can be estimated\nfrom aerial imagery, provides an important proxy for array capacity and energy\ngeneration. In this work, we employ a state-of-the-art convolutional neural\nnetwork architecture, called SegNet (Badrinarayanan et. al., 2015), to\nsemantically segment (or map) PV arrays in aerial imagery. This builds on\nprevious work focused on identifying the locations of PV arrays, as opposed to\ntheir specific shapes and sizes. We measure the ability of our SegNet\nimplementation to estimate the surface area of PV arrays on a large, publicly\navailable, dataset that has been employed in several previous studies. The\nresults indicate that the SegNet model yields substantial performance\nimprovements with respect to estimating shape and size as compared to a\nrecently proposed convolutional neural network PV detection algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 23:40:41 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Camilo", "Joseph", ""], ["Wang", "Rui", ""], ["Collins", "Leslie M.", ""], ["Bradbury", "Kyle", ""], ["Malof", "Jordan M.", ""]]}, {"id": "1801.04065", "submitter": "Lidong Yu", "authors": "Lidong Yu, Yucheng Wang, Yuwei Wu and Yunde Jia", "title": "Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture", "comments": "Accepted by AAAI18 as a spotlight poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown excellent performance for stereo matching.\nMany efforts focus on the feature extraction and similarity measurement of the\nmatching cost computation step while less attention is paid on cost aggregation\nwhich is crucial for stereo matching. In this paper, we present a\nlearning-based cost aggregation method for stereo matching by a novel\nsub-architecture in the end-to-end trainable pipeline. We reformulate the cost\naggregation as a learning process of the generation and selection of cost\naggregation proposals which indicate the possible cost aggregation results. The\ncost aggregation sub-architecture is realized by a two-stream network: one for\nthe generation of cost aggregation proposals, the other for the selection of\nthe proposals. The criterion for the selection is determined by the low-level\nstructure information obtained from a light convolutional network. The\ntwo-stream network offers a global view guidance for the cost aggregation to\nrectify the mismatching value stemming from the limited view of the matching\ncost computation. The comprehensive experiments on challenge datasets such as\nKITTI and Scene Flow show that our method outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 05:58:27 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Yu", "Lidong", ""], ["Wang", "Yucheng", ""], ["Wu", "Yuwei", ""], ["Jia", "Yunde", ""]]}, {"id": "1801.04076", "submitter": "Marc Chaumont", "authors": "Mehdi Yedroudj, Marc Chaumont, Fr\\'ed\\'eric Comby", "title": "How to augment a small learning set for improving the performances of a\n  CNN-based steganalyzer?", "comments": "EI'2018, in Proceedings of Media Watermarking, Security, and\n  Forensics, Part of IS&T International Symposium on Electronic Imaging, San\n  Francisco, California, USA, 28 Jan. -2 Feb. 2018, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and convolutional neural networks (CNN) have been intensively\nused in many image processing topics during last years. As far as steganalysis\nis concerned, the use of CNN allows reaching the state-of-the-art results. The\nperformances of such networks often rely on the size of their learning\ndatabase. An obvious preliminary assumption could be considering that \"the\nbigger a database is, the better the results are\". However, it appears that\ncautions have to be taken when increasing the database size if one desire to\nimprove the classification accuracy i.e. enhance the steganalysis efficiency.\nTo our knowledge, no study has been performed on the enrichment impact of a\nlearning database on the steganalysis performance. What kind of images can be\nadded to the initial learning set? What are the sensitive criteria: the camera\nmodels used for acquiring the images, the treatments applied to the images, the\ncameras proportions in the database, etc? This article continues the work\ncarried out in a previous paper, and explores the ways to improve the\nperformances of CNN. It aims at studying the effects of \"base augmentation\" on\nthe performance of steganalysis using a CNN. We present the results of this\nstudy using various experimental protocols and various databases to define the\ngood practices in base augmentation for steganalysis.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 07:30:22 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 14:48:30 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yedroudj", "Mehdi", ""], ["Chaumont", "Marc", ""], ["Comby", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1801.04085", "submitter": "Patrick Trampert", "authors": "Patrick Trampert, Faysal Bourghorbel, Pavel Potocek, Maurice Peemen,\n  Christian Schlinkmann, Tim Dahmen and Philipp Slusallek", "title": "How should a fixed budget of dwell time be spent in scanning electron\n  microscopy to optimize image quality?", "comments": "submitted to Ultramicroscopy as a Full Length Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scanning electron microscopy, the achievable image quality is often\nlimited by a maximum feasible acquisition time per dataset. Particularly with\nregard to three-dimensional or large field-of-view imaging, a compromise must\nbe found between a high amount of shot noise, which leads to a low\nsignal-to-noise ratio, and excessive acquisition times. Assuming a fixed\nacquisition time per frame, we compared three different strategies for\nalgorithm-assisted image acquisition in scanning electron microscopy. We\nevaluated (1) raster scanning with a reduced dwell time per pixel followed by a\nstate-of-the-art Denoising algorithm, (2) raster scanning with a decreased\nresolution in conjunction with a state-of-the-art Super Resolution algorithm,\nand (3) a sparse scanning approach where a fixed percentage of pixels is\nvisited by the beam in combination with state-of-the-art inpainting algorithms.\nAdditionally, we considered increased beam currents for each of the strategies.\nThe experiments showed that sparse scanning using an appropriate reconstruction\ntechnique was superior to the other strategies.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 08:28:49 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Trampert", "Patrick", ""], ["Bourghorbel", "Faysal", ""], ["Potocek", "Pavel", ""], ["Peemen", "Maurice", ""], ["Schlinkmann", "Christian", ""], ["Dahmen", "Tim", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1801.04096", "submitter": "San Jiang", "authors": "San Jiang, Wanshou Jiang", "title": "Hierarchical Motion Consistency Constraint for Efficient Geometrical\n  Verification in UAV Image Matching", "comments": "31 pages; 11104 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategy for efficient geometrical verification in\nunmanned aerial vehicle (UAV) image matching. First, considering the complex\ntransformation model between correspondence set in the image-space, feature\npoints of initial candidate matches are projected onto an elevation plane in\nthe object-space, with assistant of UAV flight control data and camera mounting\nangles. Spatial relationships are simplified as a 2D-translation in which a\nmotion establishes the relation of two correspondence points. Second, a\nhierarchical motion consistency constraint, termed HMCC, is designed to\neliminate outliers from initial candidate matches, which includes three major\nsteps, namely the global direction consistency constraint, the local\ndirection-change consistency constraint and the global length consistency\nconstraint. To cope with scenarios with high outlier ratios, the HMCC is\nachieved by using a voting scheme. Finally, an efficient geometrical\nverification strategy is proposed by using the HMCC as a pre-processing step to\nincrease inlier ratios before the consequent application of the basic RANSAC\nalgorithm. The performance of the proposed strategy is verified through\ncomprehensive comparison and analysis by using real UAV datasets captured with\ndifferent photogrammetric systems. Experimental results demonstrate that the\ngenerated motions have noticeable separation ability, and the HMCC-RANSAC\nalgorithm can efficiently eliminate outliers based on the motion consistency\nconstraint, with a speedup ratio reaching to 6 for oblique UAV images. Even\nthough the completeness sacrifice of approximately 7 percent of points is\nobserved from image orientation tests, competitive orientation accuracy is\nachieved from all used datasets. For geometrical verification of both nadir and\noblique UAV images, the proposed method can be a more efficient solution.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:15:29 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Jiang", "San", ""], ["Jiang", "Wanshou", ""]]}, {"id": "1801.04102", "submitter": "Donghoon Lee", "authors": "Donghoon Lee, Ming-Hsuan Yang, and Songhwai Oh", "title": "Generative Single Image Reflection Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image reflection separation is an ill-posed problem since two scenes,\na transmitted scene and a reflected scene, need to be inferred from a single\nobservation. To make the problem tractable, in this work we assume that\ncategories of two scenes are known. It allows us to address the problem by\ngenerating both scenes that belong to the categories while their contents are\nconstrained to match with the observed image. A novel network architecture is\nproposed to render realistic images of both scenes based on adversarial\nlearning. The network can be trained in a weakly supervised manner, i.e., it\nlearns to separate an observed image without corresponding ground truth images\nof transmission and reflection scenes which are difficult to collect in\npractice. Experimental results on real and synthetic datasets demonstrate that\nthe proposed algorithm performs favorably against existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:36:17 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Lee", "Donghoon", ""], ["Yang", "Ming-Hsuan", ""], ["Oh", "Songhwai", ""]]}, {"id": "1801.04134", "submitter": "Jonas Rothfuss", "authors": "Jonas Rothfuss, Fabio Ferreira, Eren Erdal Aksoy, You Zhou, and Tamim\n  Asfour", "title": "Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic\n  Experiences for Robot Action Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel deep neural network architecture for representing robot\nexperiences in an episodic-like memory which facilitates encoding, recalling,\nand predicting action experiences. Our proposed unsupervised deep episodic\nmemory model 1) encodes observed actions in a latent vector space and, based on\nthis latent encoding, 2) infers most similar episodes previously experienced,\n3) reconstructs original episodes, and 4) predicts future frames in an\nend-to-end fashion. Results show that conceptually similar actions are mapped\ninto the same region of the latent vector space. Based on these results, we\nintroduce an action matching and retrieval mechanism, benchmark its performance\non two large-scale action datasets, 20BN-something-something and ActivityNet\nand evaluate its generalization capability in a real-world scenario on a\nhumanoid robot.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 11:22:55 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:26:38 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 21:20:44 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Rothfuss", "Jonas", ""], ["Ferreira", "Fabio", ""], ["Aksoy", "Eren Erdal", ""], ["Zhou", "You", ""], ["Asfour", "Tamim", ""]]}, {"id": "1801.04161", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Sailesh Conjeti, Nassir Navab and Christian\n  Wachinger", "title": "QuickNAT: A Fully Convolutional Network for Quick and Accurate\n  Segmentation of Neuroanatomy", "comments": "Accepted for Publication at NeuroImage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation from structural magnetic resonance imaging (MRI) is\na prerequisite for most morphological analyses, but is computationally intense\nand can therefore delay the availability of image markers after scan\nacquisition. We introduce QuickNAT, a fully convolutional, densely connected\nneural network that segments a \\revision{MRI brain scan} in 20 seconds. To\nenable training of the complex network with millions of learnable parameters\nusing limited annotated data, we propose to first pre-train on auxiliary labels\ncreated from existing segmentation software. Subsequently, the pre-trained\nmodel is fine-tuned on manual labels to rectify errors in auxiliary labels.\nWith this learning strategy, we are able to use large neuroimaging repositories\nwithout manual annotations for training. In an extensive set of evaluations on\neight datasets that cover a wide age range, pathology, and different scanners,\nwe demonstrate that QuickNAT achieves superior segmentation accuracy and\nreliability in comparison to state-of-the-art methods, while being orders of\nmagnitude faster. The speed up facilitates processing of large data\nrepositories and supports translation of imaging biomarkers by making them\navailable within seconds for fast clinical decision making.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 13:37:34 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 09:12:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Conjeti", "Sailesh", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1801.04187", "submitter": "Fen Xiao", "authors": "Fen Xiao, Wenzheng Deng, Liangchan Peng, Chunhong Cao, Kai Hu, Xieping\n  Gao", "title": "MSDNN: Multi-Scale Deep Neural Network for Salient Object Detection", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection is a fundamental problem and has been received a\ngreat deal of attentions in computer vision. Recently deep learning model\nbecame a powerful tool for image feature extraction. In this paper, we propose\na multi-scale deep neural network (MSDNN) for salient object detection. The\nproposed model first extracts global high-level features and context\ninformation over the whole source image with recurrent convolutional neural\nnetwork (RCNN). Then several stacked deconvolutional layers are adopted to get\nthe multi-scale feature representation and obtain a series of saliency maps.\nFinally, we investigate a fusion convolution module (FCM) to build a final\npixel level saliency map. The proposed model is extensively evaluated on four\nsalient object detection benchmark datasets. Results show that our deep model\nsignificantly outperforms other 12 state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 14:54:36 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Xiao", "Fen", ""], ["Deng", "Wenzheng", ""], ["Peng", "Liangchan", ""], ["Cao", "Chunhong", ""], ["Hu", "Kai", ""], ["Gao", "Xieping", ""]]}, {"id": "1801.04260", "submitter": "Fabian Mentzer", "authors": "Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte,\n  Luc Van Gool", "title": "Conditional Probability Models for Deep Image Compression", "comments": "CVPR 2018. Code available at https://github.com/fab-jul/imgcomp-cvpr\n  . The first two authors contributed equally. Minor revision: fixed Fig. 2,\n  added page numbers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks trained as image auto-encoders have recently emerged as\na promising direction for advancing the state-of-the-art in image compression.\nThe key challenge in learning such networks is twofold: To deal with\nquantization, and to control the trade-off between reconstruction error\n(distortion) and entropy (rate) of the latent image representation. In this\npaper, we focus on the latter challenge and propose a new technique to navigate\nthe rate-distortion trade-off for an image compression auto-encoder. The main\nidea is to directly model the entropy of the latent representation by using a\ncontext model: A 3D-CNN which learns a conditional probability model of the\nlatent distribution of the auto-encoder. During training, the auto-encoder\nmakes use of the context model to estimate the entropy of its representation,\nand the context model is concurrently updated to learn the dependencies between\nthe symbols in the latent representation. Our experiments show that this\napproach, when measured in MS-SSIM, yields a state-of-the-art image compression\nsystem based on a simple convolutional auto-encoder.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 18:29:05 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 17:07:05 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 08:01:25 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 14:38:14 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Mentzer", "Fabian", ""], ["Agustsson", "Eirikur", ""], ["Tschannen", "Michael", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1801.04261", "submitter": "Sen He", "authors": "Sen He, Nicolas Pugeault", "title": "Deep saliency: What is learnt by a deep network about saliency?", "comments": "Accepted paper in 2nd Workshop on Visualisation for Deep Learning in\n  the 34th International Conference On Machine Learning", "journal-ref": "2nd Workshop on Visualisation for Deep Learning, ICML 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks have achieved impressive performance on a\nbroad range of problems, beating prior art on established benchmarks, but it\noften remains unclear what are the representations learnt by those systems and\nhow they achieve such performance. This article examines the specific problem\nof saliency detection, where benchmarks are currently dominated by CNN-based\napproaches, and investigates the properties of the learnt representation by\nvisualizing the artificial neurons' receptive fields.\n  We demonstrate that fine tuning a pre-trained network on the saliency\ndetection task lead to a profound transformation of the network's deeper\nlayers. Moreover we argue that this transformation leads to the emergence of\nreceptive fields conceptually similar to the centre-surround filters\nhypothesized by early research on visual saliency.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 18:32:15 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 16:48:49 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["He", "Sen", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1801.04264", "submitter": "Chen Chen", "authors": "Waqas Sultani, Chen Chen, Mubarak Shah", "title": "Real-world Anomaly Detection in Surveillance Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance videos are able to capture a variety of realistic anomalies. In\nthis paper, we propose to learn anomalies by exploiting both normal and\nanomalous videos. To avoid annotating the anomalous segments or clips in\ntraining videos, which is very time consuming, we propose to learn anomaly\nthrough the deep multiple instance ranking framework by leveraging weakly\nlabeled training videos, i.e. the training labels (anomalous or normal) are at\nvideo-level instead of clip-level. In our approach, we consider normal and\nanomalous videos as bags and video segments as instances in multiple instance\nlearning (MIL), and automatically learn a deep anomaly ranking model that\npredicts high anomaly scores for anomalous video segments. Furthermore, we\nintroduce sparsity and temporal smoothness constraints in the ranking loss\nfunction to better localize anomaly during training. We also introduce a new\nlarge-scale first of its kind dataset of 128 hours of videos. It consists of\n1900 long and untrimmed real-world surveillance videos, with 13 realistic\nanomalies such as fighting, road accident, burglary, robbery, etc. as well as\nnormal activities. This dataset can be used for two tasks. First, general\nanomaly detection considering all anomalies in one group and all normal\nactivities in another group. Second, for recognizing each of 13 anomalous\nactivities. Our experimental results show that our MIL method for anomaly\ndetection achieves significant improvement on anomaly detection performance as\ncompared to the state-of-the-art approaches. We provide the results of several\nrecent deep learning baselines on anomalous activity recognition. The low\nrecognition performance of these baselines reveals that our dataset is very\nchallenging and opens more opportunities for future work. The dataset is\navailable at: https://webpages.uncc.edu/cchen62/dataset.html\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 18:46:23 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 23:34:43 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 16:00:30 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Sultani", "Waqas", ""], ["Chen", "Chen", ""], ["Shah", "Mubarak", ""]]}, {"id": "1801.04314", "submitter": "Reuben Farrugia", "authors": "Reuben A. Farrugia and Christine Guillemot", "title": "Light Field Super-Resolution using a Low-Rank Prior and Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging has recently known a regain of interest due to the\navailability of practical light field capturing systems that offer a wide range\nof applications in the field of computer vision. However, capturing\nhigh-resolution light fields remains technologically challenging since the\nincrease in angular resolution is often accompanied by a significant reduction\nin spatial resolution. This paper describes a learning-based spatial light\nfield super-resolution method that allows the restoration of the entire light\nfield with consistency across all sub-aperture images. The algorithm first uses\noptical flow to align the light field and then reduces its angular dimension\nusing low-rank approximation. We then consider the linearly independent columns\nof the resulting low-rank model as an embedding, which is restored using a deep\nconvolutional neural network (DCNN). The super-resolved embedding is then used\nto reconstruct the remaining sub-aperture images. The original disparities are\nrestored using inverse warping where missing pixels are approximated using a\nnovel light field inpainting algorithm. Experimental results show that the\nproposed method outperforms existing light field super-resolution algorithms,\nachieving PSNR gains of 0.23 dB over the second best performing method. This\nperformance can be further improved using iterative back-projection as a\npost-processing step.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 21:02:24 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Farrugia", "Reuben A.", ""], ["Guillemot", "Christine", ""]]}, {"id": "1801.04331", "submitter": "Omar Vidal Pino", "authors": "Omar Vidal Pino, Erickson Rangel Nascimento and Mario Fernando\n  Montenegro Campos", "title": "Prototypicality effects in global semantic description of objects", "comments": "Paper accepted in IEEE Winter Conference on Applications of Computer\n  Vision 2019 (WACV2019). Content: 10 pages (8 + 2 reference) with 7 figures", "journal-ref": null, "doi": "10.1109/WACV.2019.00136", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel approach for semantic description of\nobject features based on the prototypicality effects of the Prototype Theory.\nOur prototype-based description model encodes and stores the semantic meaning\nof an object, while describing its features using the semantic prototype\ncomputed by CNN-classifications models. Our method uses semantic prototypes to\ncreate discriminative descriptor signatures that describe an object\nhighlighting its most distinctive features within the category. Our experiments\nshow that: i) our descriptor preserves the semantic information used by the\nCNN-models in classification tasks; ii) our distance metric can be used as the\nobject's typicality score; iii) our descriptor signatures are semantically\ninterpretable and enables the simulation of the prototypical organization of\nobjects within a category.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 21:58:40 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 22:56:46 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 20:27:35 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Pino", "Omar Vidal", ""], ["Nascimento", "Erickson Rangel", ""], ["Campos", "Mario Fernando Montenegro", ""]]}, {"id": "1801.04334", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Ronald M. Summers", "title": "TieNet: Text-Image Embedding Network for Common Thorax Disease\n  Classification and Reporting in Chest X-rays", "comments": "v1: Main paper + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays are one of the most common radiological examinations in daily\nclinical routines. Reporting thorax diseases using chest X-rays is often an\nentry-level task for radiologist trainees. Yet, reading a chest X-ray image\nremains a challenging job for learning-oriented machine intelligence, due to\n(1) shortage of large-scale machine-learnable medical image datasets, and (2)\nlack of techniques that can mimic the high-level reasoning of human\nradiologists that requires years of knowledge accumulation and professional\ntraining. In this paper, we show the clinical free-text radiological reports\ncan be utilized as a priori knowledge for tackling these two key problems. We\npropose a novel Text-Image Embedding network (TieNet) for extracting the\ndistinctive image and text representations. Multi-level attention models are\nintegrated into an end-to-end trainable CNN-RNN architecture for highlighting\nthe meaningful text words and image regions. We first apply TieNet to classify\nthe chest X-rays by using both image features and text embeddings extracted\nfrom associated reports. The proposed auto-annotation framework achieves high\naccuracy (over 0.9 on average in AUCs) in assigning disease labels for our\nhand-label evaluation dataset. Furthermore, we transform the TieNet into a\nchest X-ray reporting system. It simulates the reporting process and can output\ndisease classification and a preliminary report together. The classification\nresults are significantly improved (6% increase on average in AUCs) compared to\nthe state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:04:30 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Wang", "Xiaosong", ""], ["Peng", "Yifan", ""], ["Lu", "Le", ""], ["Lu", "Zhiyong", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1801.04356", "submitter": "Bo Liu", "authors": "Bo Liu, Xudong Wang, Mandar Dixit, Roland Kwitt, Nuno Vasconcelos", "title": "Feature Space Transfer for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of data augmentation in feature space is considered. A new\narchitecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for\nthe modeling of feature trajectories induced by variations of object pose. This\narchitecture exploits a parametrization of the pose manifold in terms of pose\nand appearance. This leads to a deep encoder/decoder network architecture,\nwhere the encoder factors into an appearance and a pose predictor. Unlike\nprevious attempts at trajectory transfer, FATTEN can be efficiently trained\nend-to-end, with no need to train separate feature transfer functions. This is\nrealized by supplying the decoder with information about a target pose and the\nuse of a multi-task loss that penalizes category- and pose-mismatches. In\nresult, FATTEN discourages discontinuous or non-smooth trajectories that fail\nto capture the structure of the pose manifold, and generalizes well on object\nrecognition tasks involving large pose variation. Experimental results on the\nartificial ModelNet database show that it can successfully learn to map source\nfeatures to target features of a desired pose, while preserving class identity.\nMost notably, by using feature space transfer for data augmentation (w.r.t.\npose and depth) on SUN-RGBD objects, we demonstrate considerable performance\nimprovements on one/few-shot object recognition in a transfer learning setup,\ncompared to current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 01:02:28 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 19:48:37 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 18:23:13 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Liu", "Bo", ""], ["Wang", "Xudong", ""], ["Dixit", "Mandar", ""], ["Kwitt", "Roland", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1801.04381", "submitter": "Mark Sandler", "authors": "Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov\n  and Liang-Chieh Chen", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2018, pp. 4510-4520", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 04:46:26 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 01:59:36 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 19:35:28 GMT"}, {"version": "v4", "created": "Thu, 21 Mar 2019 19:44:34 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Sandler", "Mark", ""], ["Howard", "Andrew", ""], ["Zhu", "Menglong", ""], ["Zhmoginov", "Andrey", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1801.04438", "submitter": "Petar Palasek", "authors": "Petar Palasek, Ioannis Patras", "title": "Semi-supervised Fisher vector network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore how the architecture proposed in [8], which expresses\nthe processing steps of the classical Fisher vector pipeline approaches, i.e.\ndimensionality reduction by principal component analysis (PCA) projection,\nGaussian mixture model (GMM) and Fisher vector descriptor extraction as network\nlayers, can be modified into a hybrid network that combines the benefits of\nboth unsupervised and supervised training methods, resulting in a model that\nlearns a semi-supervised Fisher vector descriptor of the input data. We\nevaluate the proposed model at image classification and action recognition\nproblems and show how the model's classification performance improves as the\namount of unlabeled data increases during training.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 13:42:02 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Palasek", "Petar", ""], ["Patras", "Ioannis", ""]]}, {"id": "1801.04461", "submitter": "Yiran Wu", "authors": "Yiran Wu, Sihao Ying, Lianmin Zheng", "title": "Size-to-depth: A New Perspective for Single Image Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of single monocular image depth\nestimation. It is a challenging problem due to its ill-posedness nature and has\nfound wide application in industry. Previous efforts belongs roughly to two\nfamilies: learning-based method and interactive method. Learning-based method,\nin which deep convolutional neural network (CNN) is widely used, can achieve\ngood result. But they suffer low generalization ability and typically perform\npoorly for unfamiliar scenes. Besides, data-hungry nature for such method makes\ndata aquisition expensive and time-consuming. Interactive method requires human\nannotation of depth which, however, is errorneous and of large variance.\n  To overcome these problems, we propose a new perspective for single monocular\nimage depth estimation problem: size to depth. Our method require sparse label\nfor real-world size of object rather than raw depth. A Coarse depth map is then\ninferred following geometric relationships according to size labels. Then we\nrefine the depth map by doing energy function optimization on conditional\nrandom field(CRF). We experimentally demonstrate that our method outperforms\ntraditional depth-labeling methods and can produce satisfactory depth maps.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 16:14:42 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Wu", "Yiran", ""], ["Ying", "Sihao", ""], ["Zheng", "Lianmin", ""]]}, {"id": "1801.04486", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Can Computers Create Art?", "comments": "to appear in Arts, special issue on Machine as Artist (21st Century)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This essay discusses whether computers, using Artificial Intelligence (AI),\ncould create art. First, the history of technologies that automated aspects of\nart is surveyed, including photography and animation. In each case, there were\ninitial fears and denial of the technology, followed by a blossoming of new\ncreative and professional opportunities for artists. The current hype and\nreality of Artificial Intelligence (AI) tools for art making is then discussed,\ntogether with predictions about how AI tools will be used. It is then\nspeculated about whether it could ever happen that AI systems could be credited\nwith authorship of artwork. It is theorized that art is something created by\nsocial agents, and so computers cannot be credited with authorship of art in\nour current understanding. A few ways that this could change are also\nhypothesized.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 21:04:13 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 03:37:22 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 06:25:17 GMT"}, {"version": "v4", "created": "Thu, 8 Feb 2018 19:23:07 GMT"}, {"version": "v5", "created": "Mon, 19 Mar 2018 06:32:02 GMT"}, {"version": "v6", "created": "Tue, 8 May 2018 03:45:56 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "1801.04520", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Marios Savvides", "title": "Non-Parametric Transformation Networks", "comments": "Preprint only", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ConvNets, through their architecture, only enforce invariance to translation.\nIn this paper, we introduce a new class of deep convolutional architectures\ncalled Non-Parametric Transformation Networks (NPTNs) which can learn\n\\textit{general} invariances and symmetries directly from data. NPTNs are a\nnatural generalization of ConvNets and can be optimized directly using gradient\ndescent. Unlike almost all previous works in deep architectures, they make no\nassumption regarding the structure of the invariances present in the data and\nin that aspect are flexible and powerful. We also model ConvNets and NPTNs\nunder a unified framework called Transformation Networks (TN), which yields a\nbetter understanding of the connection between the two. We demonstrate the\nefficacy of NPTNs on data such as MNIST with extreme transformations and\nCIFAR10 where they outperform baselines, and further outperform several recent\nalgorithms on ETH-80. They do so while having the same number of parameters. We\nalso show that they are more effective than ConvNets in modelling symmetries\nand invariances from data, without the explicit knowledge of the added\narbitrary nuisance transformations. Finally, we replace ConvNets with NPTNs\nwithin Capsule Networks and show that this enables Capsule Nets to perform even\nbetter.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 06:48:45 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 05:10:50 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 20:34:23 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 15:57:39 GMT"}, {"version": "v5", "created": "Sat, 19 May 2018 13:32:14 GMT"}, {"version": "v6", "created": "Sat, 8 Sep 2018 22:45:23 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1801.04540", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Daniel Soudry", "title": "Fix your classifier: the marginal value of training the last weight\n  layer", "comments": "https://openreview.net/forum?id=S1Dh8Tg0-", "journal-ref": "International Conference on Learning Representations 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are commonly used as models for classification for a wide\nvariety of tasks. Typically, a learned affine transformation is placed at the\nend of such models, yielding a per-class value used for classification. This\nclassifier can have a vast number of parameters, which grows linearly with the\nnumber of possible classes, thus requiring increasingly more resources. In this\nwork we argue that this classifier can be fixed, up to a global scale constant,\nwith little or no loss of accuracy for most tasks, allowing memory and\ncomputational benefits. Moreover, we show that by initializing the classifier\nwith a Hadamard matrix we can speed up inference as well. We discuss the\nimplications for current understanding of neural network models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 12:00:43 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 08:56:25 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Soudry", "Daniel", ""]]}, {"id": "1801.04590", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Raviteja Vemulapalli and Matthew Brown", "title": "Frame-Recurrent Video Super-Resolution", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in video super-resolution have shown that convolutional\nneural networks combined with motion compensation are able to merge information\nfrom multiple low-resolution (LR) frames to generate high-quality images.\nCurrent state-of-the-art methods process a batch of LR frames to generate a\nsingle high-resolution (HR) frame and run this scheme in a sliding window\nfashion over the entire video, effectively treating the problem as a large\nnumber of separate multi-frame super-resolution tasks. This approach has two\nmain weaknesses: 1) Each input frame is processed and warped multiple times,\nincreasing the computational cost, and 2) each output frame is estimated\nindependently conditioned on the input frames, limiting the system's ability to\nproduce temporally consistent results.\n  In this work, we propose an end-to-end trainable frame-recurrent video\nsuper-resolution framework that uses the previously inferred HR estimate to\nsuper-resolve the subsequent frame. This naturally encourages temporally\nconsistent results and reduces the computational cost by warping only one image\nin each step. Furthermore, due to its recurrent nature, the proposed method has\nthe ability to assimilate a large number of previous frames without increased\ncomputational demands. Extensive evaluations and comparisons with previous\nmethods validate the strengths of our approach and demonstrate that the\nproposed framework is able to significantly outperform the current state of the\nart.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 17:53:53 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 12:28:58 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 00:38:35 GMT"}, {"version": "v4", "created": "Sun, 25 Mar 2018 17:24:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Vemulapalli", "Raviteja", ""], ["Brown", "Matthew", ""]]}, {"id": "1801.04651", "submitter": "Theodore Nowak", "authors": "Theodore S. Nowak, Jason J. Corso", "title": "Deep Net Triage: Analyzing the Importance of Network Layers via\n  Structural Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their prevalence, deep networks are poorly understood. This is due,\nat least in part, to their highly parameterized nature. As such, while certain\nstructures have been found to work better than others, the significance of a\nmodel's unique structure, or the importance of a given layer, and how these\ntranslate to overall accuracy, remains unclear. In this paper, we analyze these\nproperties of deep neural networks via a process we term deep net triage. Like\nmedical triage---the assessment of the importance of various wounds---we assess\nthe importance of layers in a neural network, or as we call it, their\ncriticality. We do this by applying structural compression, whereby we reduce a\nblock of layers to a single layer. After compressing a set of layers, we apply\na combination of initialization and training schemes, and look at network\naccuracy, convergence, and the layer's learned filters to assess the\ncriticality of the layer. We apply this analysis across four data sets of\nvarying complexity. We find that the accuracy of the model does not depend on\nwhich layer was compressed; that accuracy can be recovered or exceeded after\ncompression by fine-tuning across the entire model; and, lastly, that Knowledge\nDistillation can be used to hasten convergence of a compressed network, but\nconstrains the accuracy attainable to that of the base model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 03:04:18 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 03:31:00 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Nowak", "Theodore S.", ""], ["Corso", "Jason J.", ""]]}, {"id": "1801.04654", "submitter": "Naveed Akhtar Dr.", "authors": "Naveed Akhtar and Ajmal Mian", "title": "Hyperspectral recovery from RGB images using Gaussian Processes", "comments": "Revision submitted to IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to recover spectral details from RGB images of known spectral\nquantization by modeling natural spectra under Gaussian Processes and combining\nthem with the RGB images. Our technique exploits Process Kernels to model the\nrelative smoothness of reflectance spectra, and encourages non-negativity in\nthe resulting signals for better estimation of the reflectance values. The\nGaussian Processes are inferred in sets using clusters of spatio-spectrally\ncorrelated hyperspectral training patches. Each set is transformed to match the\nspectral quantization of the test RGB image. We extract overlapping patches\nfrom the RGB image and match them to the hyperspectral training patches by\nspectrally transforming the latter. The RGB patches are encoded over the\ntransformed Gaussian Processes related to those hyperspectral patches and the\nresulting image is constructed by combining the codes with the original\nProcesses. Our approach infers the desired Gaussian Processes under a fully\nBayesian model inspired by Beta-Bernoulli Process, for which we also present\nthe inference procedure. A thorough evaluation using three hyperspectral\ndatasets demonstrates the effective extraction of spectral details from RGB\nimages by the proposed technique.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 03:26:09 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 02:05:11 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1801.04662", "submitter": "Mu Li", "authors": "Mu Li and Shuhang Gu and David Zhang and Wangmeng Zuo", "title": "Enlarging Context with Low Cost: Efficient Arithmetic Coding with\n  Trimmed Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arithmetic coding is an essential class of coding techniques. One key issue\nof arithmetic encoding method is to predict the probability of the current\ncoding symbol from its context, i.e., the preceding encoded symbols, which\nusually can be executed by building a look-up table (LUT). However, the\ncomplexity of LUT increases exponentially with the length of context. Thus,\nsuch solutions are limited to modeling large context, which inevitably\nrestricts the compression performance. Several recent deep neural network-based\nsolutions have been developed to account for large context, but are still\ncostly in computation. The inefficiency of the existing methods are mainly\nattributed to that probability prediction is performed independently for the\nneighboring symbols, which actually can be efficiently conducted by shared\ncomputation. To this end, we propose a trimmed convolutional network for\narithmetic encoding (TCAE) to model large context while maintaining\ncomputational efficiency. As for trimmed convolution, the convolutional kernels\nare specially trimmed to respect the compression order and context dependency\nof the input symbols. Benefited from trimmed convolution, the probability\nprediction of all symbols can be efficiently performed in one single forward\npass via a fully convolutional network. Furthermore, to speed up the decoding\nprocess, a slope TCAE model is presented to divide the codes from a 3D code map\ninto several blocks and remove the dependency between the codes inner one block\nfor parallel decoding, which can 60x speed up the decoding process. Experiments\nshow that our TCAE and slope TCAE attain better compression ratio in lossless\ngray image compression, and can be adopted in CNN-based lossy image compression\nto achieve state-of-the-art rate-distortion performance with real-time encoding\nspeed.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 04:28:32 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 14:54:20 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Li", "Mu", ""], ["Gu", "Shuhang", ""], ["Zhang", "David", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1801.04720", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Christian Bailer, Oliver Wasenm\\\"uller, Didier\n  Stricker", "title": "Combining Stereo Disparity and Optical Flow for Basic Scene Flow", "comments": "Commercial Vehicle Technology Symposium (CVTS), 2018", "journal-ref": null, "doi": "10.1007/978-3-658-21300-8_8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow is a description of real world motion in 3D that contains more\ninformation than optical flow. Because of its complexity there exists no\napplicable variant for real-time scene flow estimation in an automotive or\ncommercial vehicle context that is sufficiently robust and accurate. Therefore,\nmany applications estimate the 2D optical flow instead. In this paper, we\nexamine the combination of top-performing state-of-the-art optical flow and\nstereo disparity algorithms in order to achieve a basic scene flow. On the\npublic KITTI Scene Flow Benchmark we demonstrate the reasonable accuracy of the\ncombination approach and show its speed in computation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 10:21:08 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Bailer", "Christian", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1801.04751", "submitter": "Fatih Nar", "authors": "Fatih Nar", "title": "SAR Image Despeckling Using Quadratic-Linear Approximated L1-Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speckle noise, inherent in synthetic aperture radar (SAR) images, degrades\nthe performance of the various SAR image analysis tasks. Thus, speckle noise\nreduction is a critical preprocessing step for smoothing homogeneous regions\nwhile preserving details. This letter proposes a variational despeckling\napproach where L1-norm total variation regularization term is approximated in a\nquadratic and linear manner to increase accuracy while decreasing the\ncomputation time. Despeckling performance and computational efficiency of the\nproposed method are shown using synthetic and real-world SAR images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 11:47:44 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Nar", "Fatih", ""]]}, {"id": "1801.04815", "submitter": "Michael Opitz", "authors": "Michael Opitz, Georg Waltner, Horst Possegger and Horst Bischof", "title": "Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly", "comments": "Extension to our paper BIER: Boosting Independent Embeddings Robustly\n  (ICCV 2017 oral) - submitted to PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning similarity functions between image pairs with deep neural networks\nyields highly correlated activations of embeddings. In this work, we show how\nto improve the robustness of such embeddings by exploiting the independence\nwithin ensembles. To this end, we divide the last embedding layer of a deep\nnetwork into an embedding ensemble and formulate training this ensemble as an\nonline gradient boosting problem. Each learner receives a reweighted training\nsample from the previous learners. Further, we propose two loss functions which\nincrease the diversity in our ensemble. These loss functions can be applied\neither for weight initialization or during training. Together, our\ncontributions leverage large embedding sizes more effectively by significantly\nreducing correlation of the embedding and consequently increase retrieval\naccuracy of the embedding. Our method works with any differentiable loss\nfunction and does not introduce any additional parameters during test time. We\nevaluate our metric learning method on image retrieval tasks and show that it\nimproves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford\nOnline Products, In-Shop Clothes Retrieval and VehicleID datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 14:22:30 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Opitz", "Michael", ""], ["Waltner", "Georg", ""], ["Possegger", "Horst", ""], ["Bischof", "Horst", ""]]}, {"id": "1801.04880", "submitter": "Subhankar Chattoraj", "authors": "Subhankar Chattoraj, Karan Vishwakarma", "title": "Classification of histopathological breast cancer images using iterative\n  VMD aided Zernike moments & textural signatures", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel method for an automated diagnosis of breast\ncarcinoma through multilevel iterative variational mode decomposition (VMD) and\ntextural features encompassing Zernaike moments, fractal dimension and entropy\nfeatures namely, Kapoor entropy, Renyi entropy, Yager entropy features are\nextracted from VMD components. The proposed method considers the\nhistopathological image as a set of multidimensional spatially-evolving\nsignals. ReliefF algorithm is used to select the discriminatory features and\nstatistically most significant features are fed to squares support vector\nmachine (SVM) for classification. We evaluate the efficiency of the proposed\nmethodology on publicly available Breakhis dataset containing 7,909 breast\ncancer histological images, collected from 82 patients, of both benign and\nmalignant cases. Experimental results shows the efficacy of the proposed method\nin outperforming the state of the art while achieving an average classification\nrates of 89.61% and 88:23% using three-fold and ten-fold cross-validation\nstrategies, respectively. This system can aid the pathologist in accurate and\nreliable diagnosis of biopsy samples. BreaKHis, a publicly dataset available at\nhttp://web.inf.ufpr.br/vri/breast-cancer-database.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 17:17:28 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Chattoraj", "Subhankar", ""], ["Vishwakarma", "Karan", ""]]}, {"id": "1801.04929", "submitter": "Mario Michael Krell", "authors": "Mario Michael Krell", "title": "Generalizing, Decoding, and Optimizing Support Vector Machine\n  Classification", "comments": null, "journal-ref": "PhD Thesis, University of Bremen, Bremen, 1-236, 2015", "doi": null, "report-no": "urn:nbn:de:gbv:46-00104380-12", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of complex data usually requires the composition of\nprocessing steps. Here, a major challenge is the selection of optimal\nalgorithms for preprocessing and classification (including parameterizations).\nNowadays, parts of the optimization process are automized but expert knowledge\nand manual work are still required. We present three steps to face this process\nand ease the optimization. Namely, we take a theoretical view on classical\nclassifiers, provide an approach to interpret the classifier together with the\npreprocessing, and integrate both into one framework which enables a\nsemiautomatic optimization of the processing chain and which interfaces\nnumerous algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 16:49:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Krell", "Mario Michael", ""]]}, {"id": "1801.05030", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu, Sorina Smeureanu, Marius Popescu, Bogdan Alexe", "title": "Detecting abnormal events in video using Narrowed Normality Clusters", "comments": "Accepted at WACV 2019. arXiv admin note: text overlap with\n  arXiv:1705.08182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the abnormal event detection problem as an outlier detection\ntask and we propose a two-stage algorithm based on k-means clustering and\none-class Support Vector Machines (SVM) to eliminate outliers. In the feature\nextraction stage, we propose to augment spatio-temporal cubes with deep\nappearance features extracted from the last convolutional layer of a\npre-trained neural network. After extracting motion and appearance features\nfrom the training video containing only normal events, we apply k-means\nclustering to find clusters representing different types of normal motion and\nappearance features. In the first stage, we consider that clusters with fewer\nsamples (with respect to a given threshold) contain mostly outliers, and we\neliminate these clusters altogether. In the second stage, we shrink the borders\nof the remaining clusters by training a one-class SVM model on each cluster. To\ndetected abnormal events in the test video, we analyze each test sample and\nconsider its maximum normality score provided by the trained one-class SVM\nmodels, based on the intuition that a test sample can belong to only one\ncluster of normality. If the test sample does not fit well in any narrowed\nnormality cluster, then it is labeled as abnormal. We compare our method with\nseveral state-of-the-art methods on three benchmark data sets. The empirical\nresults indicate that our abnormal event detection framework can achieve better\nresults in most cases, while processing the test video in real-time at 24\nframes per second on a single CPU.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 10:57:08 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:58:07 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 15:04:10 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 08:59:19 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Smeureanu", "Sorina", ""], ["Popescu", "Marius", ""], ["Alexe", "Bogdan", ""]]}, {"id": "1801.05038", "submitter": "R\\'emi Cura", "authors": "Remi Cura, Julien Perret, Nicolas Paparoditis", "title": "An octree cells occupancy geometric dimensionality descriptor for\n  massive on-server point cloud visualisation and classification", "comments": "extracted from article arXiv:1602.06920 ( arXiv:1602.06920 was split\n  into 2 articles because it was to long and to hard to read)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar datasets are becoming more and more common. They are appreciated for\ntheir precise 3D nature, and have a wide range of applications, such as surface\nreconstruction, object detection, visualisation, etc. For all this\napplications, having additional semantic information per point has potential of\nincreasing the quality and the efficiency of the application. In the last\ndecade the use of Machine Learning and more specifically classification methods\nhave proved to be successful to create this semantic information. In this\nparadigm, the goal is to classify points into a set of given classes (for\ninstance tree, building, ground, other). Some of these methods use descriptors\n(also called feature) of a point to learn and predict its class. Designing the\ndescriptors is then the heart of these methods. Descriptors can be based on\npoints geometry and attributes, use contextual information, etc. Furthermore,\ndescriptors can be used by humans for easier visual understanding and sometimes\nfiltering. In this work we propose a new simple geometric descriptor that gives\ninformation about the implicit local dimensionality of the point cloud at\nvarious scale. For instance a tree seen from afar is more volumetric in nature\n(3D), yet locally each leaves is rather planar (2D). To do so we build an\noctree centred on the point to consider, and compare the variation of the\noccupancy of the cells across the levels of the octree. We compare this\ndescriptor with the state of the art dimensionality descriptor and show its\ninterest. We further test the descriptor for classification within the Point\nCloud Server, and demonstrate efficiency and correctness results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 21:40:24 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Cura", "Remi", ""], ["Perret", "Julien", ""], ["Paparoditis", "Nicolas", ""]]}, {"id": "1801.05040", "submitter": "Jonas Teuwen", "authors": "Mohsen Ghafoorian, Jonas Teuwen, Rashindra Manniesing, Frank-Erik de\n  Leeuw, Bram van Ginneken, Nico Karssemeijer, Bram Platel", "title": "Student Beats the Teacher: Deep Neural Networks for Lateral Ventricles\n  Segmentation in Brain MR", "comments": "7 pages, 4 figures, SPIE Medical Imaging 2018 Conference paper", "journal-ref": "Proc. SPIE 10574, 105742U (2 March 2018)", "doi": "10.1117/12.2293569", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ventricular volume and its progression are known to be linked to several\nbrain diseases such as dementia and schizophrenia. Therefore accurate\nmeasurement of ventricle volume is vital for longitudinal studies on these\ndisorders, making automated ventricle segmentation algorithms desirable. In the\npast few years, deep neural networks have shown to outperform the classical\nmodels in many imaging domains. However, the success of deep networks is\ndependent on manually labeled data sets, which are expensive to acquire\nespecially for higher dimensional data in the medical domain. In this work, we\nshow that deep neural networks can be trained on much-cheaper-to-acquire\npseudo-labels (e.g., generated by other automated less accurate methods) and\nstill produce more accurate segmentations compared to the quality of the\nlabels. To show this, we use noisy segmentation labels generated by a\nconventional region growing algorithm to train a deep network for lateral\nventricle segmentation. Then on a large manually annotated test set, we show\nthat the network significantly outperforms the conventional region growing\nalgorithm which was used to produce the training labels for the network. Our\nexperiments report a Dice Similarity Coefficient (DSC) of $0.874$ for the\ntrained network compared to $0.754$ for the conventional region growing\nalgorithm ($p < 0.001$).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 21:43:48 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 11:54:32 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ghafoorian", "Mohsen", ""], ["Teuwen", "Jonas", ""], ["Manniesing", "Rashindra", ""], ["de Leeuw", "Frank-Erik", ""], ["van Ginneken", "Bram", ""], ["Karssemeijer", "Nico", ""], ["Platel", "Bram", ""]]}, {"id": "1801.05068", "submitter": "Jaouhar Fattahi", "authors": "Kalthoum Ouerghi, Najib Fadlallah, Amor Smida, Ridha Ghayoula, Jaouhar\n  Fattahi, Noureddine Boulejfen", "title": "Circular Antenna Array Design for Breast Cancer Detection", "comments": null, "journal-ref": "Sensors Networks Smart and Emerging Technologies (SENSET), 2017,\n  pp. 1-4", "doi": "10.1109/SENSET.2017.8125016", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microwave imaging for breast cancer detection is based on the contrast in the\nelectrical properties of healthy fatty breast tissues. This paper presents an\nindustrial, scientific and medical (ISM) bands comparative study of five\nmicrostrip patch antennas for microwave imaging at a frequency of 2.45 GHz. The\nchoice of one antenna is made for an antenna array composed of 8 antennas for a\nmicrowave breast imaging system. Each antenna element is arranged in a circular\nconfiguration so that it can be directly faced to the breast phantom for better\ntumor detection. This choice is made by putting each antenna alone on the\nBreast skin to study the electric field, magnetic fields and current density in\nthe healthy tissue of the breast phantom designed and simulated in Ansoft High\nFrequency Simulation Software (HFSS).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 23:42:27 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ouerghi", "Kalthoum", ""], ["Fadlallah", "Najib", ""], ["Smida", "Amor", ""], ["Ghayoula", "Ridha", ""], ["Fattahi", "Jaouhar", ""], ["Boulejfen", "Noureddine", ""]]}, {"id": "1801.05091", "submitter": "Seunghoon Hong", "authors": "Seunghoon Hong and Dingdong Yang and Jongwook Choi and Honglak Lee", "title": "Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hierarchical approach for text-to-image synthesis by\ninferring semantic layout. Instead of learning a direct mapping from text to\nimage, our algorithm decomposes the generation process into multiple steps, in\nwhich it first constructs a semantic layout from the text by the layout\ngenerator and converts the layout to an image by the image generator. The\nproposed layout generator progressively constructs a semantic layout in a\ncoarse-to-fine manner by generating object bounding boxes and refining each box\nby estimating object shapes inside the box. The image generator synthesizes an\nimage conditioned on the inferred semantic layout, which provides a useful\nsemantic structure of an image matching with the text description. Our model\nnot only generates semantically more meaningful images, but also allows\nautomatic annotation of generated images and user-controlled generation process\nby modifying the generated scene layout. We demonstrate the capability of the\nproposed model on challenging MS-COCO dataset and show that the model can\nsubstantially improve the image quality, interpretability of output and\nsemantic alignment to input text over existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 01:49:29 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 02:44:17 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Hong", "Seunghoon", ""], ["Yang", "Dingdong", ""], ["Choi", "Jongwook", ""], ["Lee", "Honglak", ""]]}, {"id": "1801.05117", "submitter": "Jinwei Gu", "authors": "Huaijin Chen, Jinwei Gu, Orazio Gallo, Ming-Yu Liu, Ashok\n  Veeraraghavan, Jan Kautz", "title": "Reblur2Deblur: Deblurring Videos via Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur is a fundamental problem in computer vision as it impacts image\nquality and hinders inference. Traditional deblurring algorithms leverage the\nphysics of the image formation model and use hand-crafted priors: they usually\nproduce results that better reflect the underlying scene, but present\nartifacts. Recent learning-based methods implicitly extract the distribution of\nnatural images directly from the data and use it to synthesize plausible\nimages. Their results are impressive, but they are not always faithful to the\ncontent of the latent image. We present an approach that bridges the two. Our\nmethod fine-tunes existing deblurring neural networks in a self-supervised\nfashion by enforcing that the output, when blurred based on the optical flow\nbetween subsequent frames, matches the input blurry image. We show that our\nmethod significantly improves the performance of existing methods on several\ndatasets both visually and in terms of image quality metrics. The supplementary\nmaterial is https://goo.gl/nYPjEQ\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 05:02:09 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Chen", "Huaijin", ""], ["Gu", "Jinwei", ""], ["Gallo", "Orazio", ""], ["Liu", "Ming-Yu", ""], ["Veeraraghavan", "Ashok", ""], ["Kautz", "Jan", ""]]}, {"id": "1801.05124", "submitter": "Teng-Yok Lee", "authors": "Chieh-Chi Kao, Teng-Yok Lee, Pradeep Sen, Ming-Yu Liu", "title": "Localization-Aware Active Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning - a class of algorithms that iteratively searches for the\nmost informative samples to include in a training dataset - has been shown to\nbe effective at annotating data for image classification. However, the use of\nactive learning for object detection is still largely unexplored as determining\ninformativeness of an object-location hypothesis is more difficult. In this\npaper, we address this issue and present two metrics for measuring the\ninformativeness of an object hypothesis, which allow us to leverage active\nlearning to reduce the amount of annotated data needed to achieve a target\nobject detection performance. Our first metric measures 'localization\ntightness' of an object hypothesis, which is based on the overlapping ratio\nbetween the region proposal and the final prediction. Our second metric\nmeasures 'localization stability' of an object hypothesis, which is based on\nthe variation of predicted object locations when input images are corrupted by\nnoise. Our experimental results show that by augmenting a conventional\nactive-learning algorithm designed for classification with the proposed\nmetrics, the amount of labeled training data required can be reduced up to 25%.\nMoreover, on PASCAL 2007 and 2012 datasets our localization-stability method\nhas an average relative improvement of 96.5% and 81.9% over the baseline method\nusing classification only.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 05:43:53 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Kao", "Chieh-Chi", ""], ["Lee", "Teng-Yok", ""], ["Sen", "Pradeep", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "1801.05141", "submitter": "Kazi Nazmul Haque", "authors": "Kazi Nazmul Haque, Mohammad Abu Yousuf, Rajib Rana", "title": "Image denoising and restoration with CNN-LSTM Encoder Decoder with\n  Direct Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is always a challenging task in the field of computer vision\nand image processing. In this paper, we have proposed an encoder-decoder model\nwith direct attention, which is capable of denoising and reconstruct highly\ncorrupted images. Our model consists of an encoder and a decoder, where the\nencoder is a convolutional neural network and decoder is a multilayer Long\nShort-Term memory network. In the proposed model, the encoder reads an image\nand catches the abstraction of that image in a vector, where decoder takes that\nvector as well as the corrupted image to reconstruct a clean image. We have\ntrained our model on MNIST handwritten digit database after making lower half\nof every image as black as well as adding noise top of that. After a massive\ndestruction of the images where it is hard for a human to understand the\ncontent of those images, our model can retrieve that image with minimal error.\nOur proposed model has been compared with convolutional encoder-decoder, where\nour model has performed better at generating missing part of the images than\nconvolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 07:27:46 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Haque", "Kazi Nazmul", ""], ["Yousuf", "Mohammad Abu", ""], ["Rana", "Rajib", ""]]}, {"id": "1801.05143", "submitter": "Lei Chu", "authors": "Zenan Ling, Robert C. Qiu, Zhijian Jin, Yuhang Zhang, Xing He, Haichun\n  Liu, Lei Chu", "title": "An Accurate and Real-time Self-blast Glass Insulator Location Method\n  Based On Faster R-CNN and U-net with Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The location of broken insulators in aerial images is a challenging task.\nThis paper, focusing on the self-blast glass insulator, proposes a deep\nlearning solution. We address the broken insulators location problem as a low\nsignal-noise-ratio image location framework with two modules: 1) object\ndetection based on Fast R-CNN, and 2) classification of pixels based on U-net.\nA diverse aerial image set of some grid in China is tested to validated the\nproposed approach. Furthermore, a comparison is made among different methods\nand the result shows that our approach is accurate and real-time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 07:56:58 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ling", "Zenan", ""], ["Qiu", "Robert C.", ""], ["Jin", "Zhijian", ""], ["Zhang", "Yuhang", ""], ["He", "Xing", ""], ["Liu", "Haichun", ""], ["Chu", "Lei", ""]]}, {"id": "1801.05151", "submitter": "Kai Qiao", "authors": "Chi Zhang, Kai Qiao, Linyuan Wang, Li Tong, Ying Zeng, Bin Yan", "title": "Constraint-free Natural Image Reconstruction from fMRI Signals Based on\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research on decoding brain activity based on functional\nmagnetic resonance imaging (fMRI) has made remarkable achievements. However,\nconstraint-free natural image reconstruction from brain activity is still a\nchallenge. The existing methods simplified the problem by using semantic prior\ninformation or just reconstructing simple images such as letters and digitals.\nWithout semantic prior information, we present a novel method to reconstruct\nnature images from fMRI signals of human visual cortex based on the computation\nmodel of convolutional neural network (CNN). Firstly, we extracted the units\noutput of viewed natural images in each layer of a pre-trained CNN as CNN\nfeatures. Secondly, we transformed image reconstruction from fMRI signals into\nthe problem of CNN feature visualizations by training a sparse linear\nregression to map from the fMRI patterns to CNN features. By iteratively\noptimization to find the matched image, whose CNN unit features become most\nsimilar to those predicted from the brain activity, we finally achieved the\npromising results for the challenging constraint-free natural image\nreconstruction. As there was no use of semantic prior information of the\nstimuli when training decoding model, any category of images (not constraint by\nthe training set) could be reconstructed theoretically. We found that the\nreconstructed images resembled the natural stimuli, especially in position and\nshape. The experimental results suggest that hierarchical visual features can\neffectively express the visual perception process of human brain.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:34:18 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Zhang", "Chi", ""], ["Qiao", "Kai", ""], ["Wang", "Linyuan", ""], ["Tong", "Li", ""], ["Zeng", "Ying", ""], ["Yan", "Bin", ""]]}, {"id": "1801.05156", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Empirical Explorations in Training Networks with Discrete Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present extensive experiments training and testing hidden units in deep\nnetworks that emit only a predefined, static, number of discretized values.\nThese units provide benefits in real-world deployment in systems in which\nmemory and/or computation may be limited. Additionally, they are particularly\nwell suited for use in large recurrent network models that require the\nmaintenance of large amounts of internal state in memory. Surprisingly, we find\nthat despite reducing the number of values that can be represented in the\noutput activations from $2^{32}-2^{64}$ to between 64 and 256, there is little\nto no degradation in network performance across a variety of different\nsettings. We investigate simple classification and regression tasks, as well as\nmemorization and compression problems. We compare the results with more\nstandard activations, such as tanh and relu. Unlike previous discretization\nstudies which often concentrate only on binary units, we examine the effects of\nvarying the number of allowed activation levels. Compared to existing\napproaches for discretization, the approach presented here is both conceptually\nand programatically simple, has no stochastic component, and allows the\ntraining, testing, and usage phases to be treated in exactly the same manner.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:47:18 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "1801.05171", "submitter": "Nati Ofir", "authors": "Nati Ofir, Shai Silberstein, Hila Levi, Dani Rozenbaum, Yosi Keller\n  and Sharon Duvdevani Bar", "title": "Deep Multi-Spectral Registration Using Invariant Descriptor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel deep-learning method to align\ncross-spectral images. Our approach relies on a learned descriptor which is\ninvariant to different spectra. Multi-modal images of the same scene capture\ndifferent signals and therefore their registration is challenging and it is not\nsolved by classic approaches. To that end, we developed a feature-based\napproach that solves the visible (VIS) to Near-Infra-Red (NIR) registration\nproblem. Our algorithm detects corners by Harris and matches them by a\npatch-metric learned on top of CIFAR-10 network descriptor. As our experiments\ndemonstrate we achieve a high-quality alignment of cross-spectral images with a\nsub-pixel accuracy. Comparing to other existing methods, our approach is more\naccurate in the task of VIS to NIR registration.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 09:27:42 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 12:01:39 GMT"}, {"version": "v3", "created": "Sun, 21 Jan 2018 05:57:25 GMT"}, {"version": "v4", "created": "Wed, 24 Jan 2018 11:58:09 GMT"}, {"version": "v5", "created": "Thu, 8 Feb 2018 07:12:35 GMT"}, {"version": "v6", "created": "Wed, 23 May 2018 07:26:34 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Ofir", "Nati", ""], ["Silberstein", "Shai", ""], ["Levi", "Hila", ""], ["Rozenbaum", "Dani", ""], ["Keller", "Yosi", ""], ["Bar", "Sharon Duvdevani", ""]]}, {"id": "1801.05173", "submitter": "Varghese Alex Kollerathu Mr.", "authors": "Mahendra Khened, Varghese Alex Kollerathu and Ganapathy Krishnamurthi", "title": "Fully Convolutional Multi-scale Residual DenseNets for Cardiac\n  Segmentation and Automated Cardiac Diagnosis using Ensemble of Classifiers", "comments": "59 Pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep fully convolutional neural network (FCN) based architectures have shown\ngreat potential in medical image segmentation. However, such architectures\nusually have millions of parameters and inadequate number of training samples\nleading to over-fitting and poor generalization. In this paper, we present a\nnovel highly parameter and memory efficient FCN based architecture for medical\nimage analysis. We propose a novel up-sampling path which incorporates long\nskip and short-cut connections to overcome the feature map explosion in FCN\nlike architectures. In order to processes the input images at multiple scales\nand view points simultaneously, we propose to incorporate Inception module's\nparallel structures. We also propose a novel dual loss function whose weighting\nscheme allows to combine advantages of cross-entropy and dice loss. We have\nvalidated our proposed network architecture on two publicly available datasets,\nnamely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)\nLeft Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017\nchallenge stands second place for segmentation and first place in automated\ncardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011\nchallenge our approach attained 0.74 Jaccard index, which is so far the highest\npublished result in fully automated algorithms. From the segmentation we\nextracted clinically relevant cardiac parameters and hand-crafted features\nwhich reflected the clinical diagnostic analysis to train an ensemble system\nfor cardiac disease classification. Our approach combined both cardiac\nsegmentation and disease diagnosis into a fully automated framework which is\ncomputational efficient and hence has the potential to be incorporated in\ncomputer-aided diagnosis (CAD) tools for clinical application.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 09:32:32 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Khened", "Mahendra", ""], ["Kollerathu", "Varghese Alex", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "1801.05269", "submitter": "Erik Stenborg", "authors": "Erik Stenborg and Carl Toft and Lars Hammarstrand", "title": "Long-term Visual Localization using Semantically Segmented Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust cross-seasonal localization is one of the major challenges in\nlong-term visual navigation of autonomous vehicles. In this paper, we exploit\nrecent advances in semantic segmentation of images, i.e., where each pixel is\nassigned a label related to the type of object it represents, to attack the\nproblem of long-term visual localization. We show that semantically labeled 3-D\npoint maps of the environment, together with semantically segmented images, can\nbe efficiently used for vehicle localization without the need for detailed\nfeature descriptors (SIFT, SURF, etc.). Thus, instead of depending on\nhand-crafted feature descriptors, we rely on the training of an image\nsegmenter. The resulting map takes up much less storage space compared to a\ntraditional descriptor based map. A particle filter based semantic localization\nsolution is compared to one based on SIFT-features, and even with large\nseasonal variations over the year we perform on par with the larger and more\ndescriptive SIFT-features, and are able to localize with an error below 1 m\nmost of the time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 14:43:41 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 13:57:35 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Stenborg", "Erik", ""], ["Toft", "Carl", ""], ["Hammarstrand", "Lars", ""]]}, {"id": "1801.05278", "submitter": "Qilu Zhao", "authors": "Qilu Zhao and Zongmin Li", "title": "Unsupervised Representation Learning with Laplacian Pyramid\n  Auto-encoders", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale-space representation has been popular in computer vision community due\nto its theoretical foundation. The motivation for generating a scale-space\nrepresentation of a given data set originates from the basic observation that\nreal-world objects are composed of different structures at different scales.\nHence, it's reasonable to consider learning features with image pyramids\ngenerated by smoothing and down-sampling operations. In this paper we propose\nLaplacian pyramid auto-encoders, a straightforward modification of the deep\nconvolutional auto-encoder architecture, for unsupervised representation\nlearning. The method uses multiple encoding-decoding sub-networks within a\nLaplacian pyramid framework to reconstruct the original image and the low pass\nfiltered images. The last layer of each encoding sub-network also connects to\nan encoding layer of the sub-network in the next level, which aims to reverse\nthe process of Laplacian pyramid generation. Experimental results showed that\nLaplacian pyramid benefited the classification and reconstruction performance\nof deep auto-encoder approaches, and batch normalization is critical to get\ndeep auto-encoders approaches to begin learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 14:59:05 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 13:05:29 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhao", "Qilu", ""], ["Li", "Zongmin", ""]]}, {"id": "1801.05284", "submitter": "Juan Eugenio Iglesias", "authors": "Juan Eugenio Iglesias and Marc Modat and Loic Peter and Allison\n  Stevens and Roberto Annunziata and Tom Vercauteren and Ed Lein and Bruce\n  Fischl and Sebastien Ourselin", "title": "Joint registration and synthesis using a probabilistic model for\n  alignment of MRI and histological sections", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2018.09.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear registration of 2D histological sections with corresponding slices\nof MRI data is a critical step of 3D histology reconstruction. This task is\ndifficult due to the large differences in image contrast and resolution, as\nwell as the complex nonrigid distortions produced when sectioning the sample\nand mounting it on the glass slide. It has been shown in brain MRI registration\nthat better spatial alignment across modalities can be obtained by synthesizing\none modality from the other and then using intra-modality registration metrics,\nrather than by using mutual information (MI) as metric. However, such an\napproach typically requires a database of aligned images from the two\nmodalities, which is very difficult to obtain for histology/MRI.\n  Here, we overcome this limitation with a probabilistic method that\nsimultaneously solves for registration and synthesis directly on the target\nimages, without any training data. In our model, the MRI slice is assumed to be\na contrast-warped, spatially deformed version of the histological section. We\nuse approximate Bayesian inference to iteratively refine the probabilistic\nestimate of the synthesis and the registration, while accounting for each\nother's uncertainty. Moreover, manually placed landmarks can be seamlessly\nintegrated in the framework for increased performance.\n  Experiments on a synthetic dataset show that, compared with MI, the proposed\nmethod makes it possible to use a much more flexible deformation model in the\nregistration to improve its accuracy, without compromising robustness.\nMoreover, our framework also exploits information in manually placed landmarks\nmore efficiently than MI, since landmarks inform both synthesis and\nregistration - as opposed to registration alone. Finally, we show qualitative\nresults on the public Allen atlas, in which the proposed method provides a\nclear improvement over MI based registration.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 15:16:05 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Iglesias", "Juan Eugenio", ""], ["Modat", "Marc", ""], ["Peter", "Loic", ""], ["Stevens", "Allison", ""], ["Annunziata", "Roberto", ""], ["Vercauteren", "Tom", ""], ["Lein", "Ed", ""], ["Fischl", "Bruce", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1801.05299", "submitter": "Bowen Tan", "authors": "Nayun Xu, Bowen Tan, Bingyu Kong", "title": "Autonomous Driving in Reality with Reinforcement Learning and Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning is widely used in training autonomous driving vehicle.\nHowever, it is trained with large amount of supervised labeled data.\nReinforcement learning can be trained without abundant labeled data, but we\ncannot train it in reality because it would involve many unpredictable\naccidents. Nevertheless, training an agent with good performance in virtual\nenvironment is relatively much easier. Because of the huge difference between\nvirtual and real, how to fill the gap between virtual and real is challenging.\nIn this paper, we proposed a novel framework of reinforcement learning with\nimage semantic segmentation network to make the whole model adaptable to\nreality. The agent is trained in TORCS, a car racing simulator.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 06:55:23 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 17:28:52 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Xu", "Nayun", ""], ["Tan", "Bowen", ""], ["Kong", "Bingyu", ""]]}, {"id": "1801.05302", "submitter": "Wenda Qiu", "authors": "Wenda Qiu, Yueyang Xianzang, Zhekai Zhang", "title": "Benchmark Visual Question Answer Models by using Focus Map", "comments": "A group project paper for course CS348. arXiv admin note: text\n  overlap with arXiv:1705.03633 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring and Executing Programs for Visual Reasoning proposes a model for\nvisual reasoning that consists of a program generator and an execution engine\nto avoid end-to-end models. To show that the model actually learns which\nobjects to focus on to answer the questions, the authors give a visualization\nof the norm of the gradient of the sum of the predicted answer scores with\nrespect to the final feature map. However, the authors do not evaluate the\nefficiency of focus map. This paper purposed a method for evaluating it. We\ngenerate several kinds of questions to test different keywords. We infer focus\nmaps from the model by asking these questions and evaluate them by comparing\nwith the segmentation graph. Furthermore, this method can be applied to any\nmodel if focus maps can be inferred from it. By evaluating focus map of\ndifferent models on the CLEVR dataset, we will show that CLEVR-iep model has\nlearned where to focus more than end-to-end models.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 09:09:33 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Qiu", "Wenda", ""], ["Xianzang", "Yueyang", ""], ["Zhang", "Zhekai", ""]]}, {"id": "1801.05339", "submitter": "Jon Almaz\\'an", "authors": "Jon Almazan, Bojana Gajic, Naila Murray, Diane Larlus", "title": "Re-ID done right: towards good practices for person re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep architecture using a ranking loss has become standard for the\nperson re-identification task. Increasingly, these deep architectures include\nadditional components that leverage part detections, attribute predictions,\npose estimators and other auxiliary information, in order to more effectively\nlocalize and align discriminative image regions. In this paper we adopt a\ndifferent approach and carefully design each component of a simple deep\narchitecture and, critically, the strategy for training it effectively for\nperson re-identification. We extensively evaluate each design choice, leading\nto a list of good practices for person re-identification. By following these\npractices, our approach outperforms the state of the art, including more\ncomplex methods with auxiliary components, by large margins on four benchmark\ndatasets. We also provide a qualitative analysis of our trained representation\nwhich indicates that, while compact, it is able to capture information from\nlocalized and discriminative regions, in a manner akin to an implicit attention\nmechanism.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 16:29:11 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Almazan", "Jon", ""], ["Gajic", "Bojana", ""], ["Murray", "Naila", ""], ["Larlus", "Diane", ""]]}, {"id": "1801.05365", "submitter": "Pramuditha Perera", "authors": "Pramuditha Perera, Vishal M. Patel", "title": "Learning Deep Features for One-Class Classification", "comments": "Accepted to appear in Transactions in Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2917862", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning-based solution for the problem of feature learning\nin one-class classification. The proposed method operates on top of a\nConvolutional Neural Network (CNN) of choice and produces descriptive features\nwhile maintaining a low intra-class variance in the feature space for the given\nclass. For this purpose two loss functions, compactness loss and\ndescriptiveness loss are proposed along with a parallel CNN architecture. A\ntemplate matching-based framework is introduced to facilitate the testing\nprocess. Extensive experiments on publicly available anomaly detection, novelty\ndetection and mobile active authentication datasets show that the proposed Deep\nOne-Class (DOC) classification method achieves significant improvements over\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 17:01:48 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 16:40:12 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Perera", "Pramuditha", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1801.05367", "submitter": "Ekta Vats", "authors": "Anders Hast, Per Cullhed, and Ekta Vats", "title": "TexT - Text Extractor Tool for Handwritten Document Transcription and\n  Annotation", "comments": null, "journal-ref": "Digital Libraries and Multimedia Archives. IRCDL 2018.\n  Communications in Computer and Information Science, vol 806. Springer, Cham", "doi": "10.1007/978-3-319-73165-0_8", "report-no": null, "categories": "cs.DL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for semi-automatic transcription of\nlarge-scale historical handwritten documents and proposes a simple\nuser-friendly text extractor tool, TexT for transcription. The proposed\napproach provides a quick and easy transcription of text using computer\nassisted interactive technique. The algorithm finds multiple occurrences of the\nmarked text on-the-fly using a word spotting system. TexT is also capable of\nperforming on-the-fly annotation of handwritten text with automatic generation\nof ground truth labels, and dynamic adjustment and correction of user generated\nbounding box annotations with the word being perfectly encapsulated. The user\ncan view the document and the found words in the original form or with\nbackground noise removed for easier visualization of transcription results. The\neffectiveness of TexT is demonstrated on an archival manuscript collection from\nwell-known publicly available dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 12:05:33 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Hast", "Anders", ""], ["Cullhed", "Per", ""], ["Vats", "Ekta", ""]]}, {"id": "1801.05387", "submitter": "Mohammad Javad Shafiee", "authors": "Mohammad Javad Shafiee, Brendan Chwyl, Francis Li, Rongyan Chen,\n  Michelle Karg, Christian Scharfenberger, Alexander Wong", "title": "StressedNets: Efficient Feature Representations via Stress-induced\n  Evolutionary Synthesis of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of leveraging deep neural networks for\nextracting deep feature representations is a significant barrier to its\nwidespread adoption, particularly for use in embedded devices. One particularly\npromising strategy to addressing the complexity issue is the notion of\nevolutionary synthesis of deep neural networks, which was demonstrated to\nsuccessfully produce highly efficient deep neural networks while retaining\nmodeling performance. Here, we further extend upon the evolutionary synthesis\nstrategy for achieving efficient feature extraction via the introduction of a\nstress-induced evolutionary synthesis framework, where stress signals are\nimposed upon the synapses of a deep neural network during training to induce\nstress and steer the synthesis process towards the production of more efficient\ndeep neural networks over successive generations and improved model fidelity at\na greater efficiency. The proposed stress-induced evolutionary synthesis\napproach is evaluated on a variety of different deep neural network\narchitectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object\nclassification and object detection) to synthesize efficient StressedNets over\nmultiple generations. Experimental results demonstrate the efficacy of the\nproposed framework to synthesize StressedNets with significant improvement in\nnetwork architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and\nspeed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra\nX1 mobile processor).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 17:47:13 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chwyl", "Brendan", ""], ["Li", "Francis", ""], ["Chen", "Rongyan", ""], ["Karg", "Michelle", ""], ["Scharfenberger", "Christian", ""], ["Wong", "Alexander", ""]]}, {"id": "1801.05401", "submitter": "Yu-Xiong Wang", "authors": "Yu-Xiong Wang, Ross Girshick, Martial Hebert, Bharath Hariharan", "title": "Low-Shot Learning from Imaginary Data", "comments": "CVPR 2018 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can quickly learn new visual concepts, perhaps because they can easily\nvisualize or imagine what novel objects look like from different views.\nIncorporating this ability to hallucinate novel instances of new concepts might\nhelp machine vision systems perform better low-shot learning, i.e., learning\nconcepts from few examples. We present a novel approach to low-shot learning\nthat uses this idea. Our approach builds on recent progress in meta-learning\n(\"learning to learn\") by combining a meta-learner with a \"hallucinator\" that\nproduces additional training examples, and optimizing both models jointly. Our\nhallucinator can be incorporated into a variety of meta-learners and provides\nsignificant gains: up to a 6 point boost in classification accuracy when only a\nsingle training example is available, yielding state-of-the-art performance on\nthe challenging ImageNet low-shot classification benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:38:35 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 02:53:28 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Yu-Xiong", ""], ["Girshick", "Ross", ""], ["Hebert", "Martial", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1801.05412", "submitter": "Muhammad Hussain", "authors": "Ihsan Ullah, Muhammad Hussain, Emad-ul-Haq Qazi and Hatim Aboalsamh", "title": "An Automated System for Epilepsy Detection using EEG Brain Signals based\n  on Deep Learning Approach", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a neurological disorder and for its detection, encephalography\n(EEG) is a commonly used clinical approach. Manual inspection of EEG brain\nsignals is a time-consuming and laborious process, which puts heavy burden on\nneurologists and affects their performance. Several automatic techniques have\nbeen proposed using traditional approaches to assist neurologists in detecting\nbinary epilepsy scenarios e.g. seizure vs. non-seizure or normal vs. ictal.\nThese methods do not perform well when classifying ternary case e.g. ictal vs.\nnormal vs. inter-ictal; the maximum accuracy for this case by the\nstate-of-the-art-methods is 97+-1%. To overcome this problem, we propose a\nsystem based on deep learning, which is an ensemble of pyramidal\none-dimensional convolutional neural network (P-1D-CNN) models. In a CNN model,\nthe bottleneck is the large number of learnable parameters. P-1D-CNN works on\nthe concept of refinement approach and it results in 60% fewer parameters\ncompared to traditional CNN models. Further to overcome the limitations of\nsmall amount of data, we proposed augmentation schemes for learning P-1D-CNN\nmodel. In almost all the cases concerning epilepsy detection, the proposed\nsystem gives an accuracy of 99.1+-0.9% on the University of Bonn dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:49:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ullah", "Ihsan", ""], ["Hussain", "Muhammad", ""], ["Qazi", "Emad-ul-Haq", ""], ["Aboalsamh", "Hatim", ""]]}, {"id": "1801.05449", "submitter": "Muhammad Hussain", "authors": "Amani Alahmadi, Muhammad Hussain, Hatim Aboalsamh and Mansour Zuair", "title": "ConvSRC: SmartPhone based Periocular Recognition using Deep\n  Convolutional Neural Network and Sparsity Augmented Collaborative\n  Representation", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone based periocular recognition has gained significant attention from\nbiometric research community because of the limitations of biometric modalities\nlike face, iris etc. Most of the existing methods for periocular recognition\nemploy hand-crafted features. Recently, learning based image representation\ntechniques like deep Convolutional Neural Network (CNN) have shown outstanding\nperformance in many visual recognition tasks. CNN needs a huge volume of data\nfor its learning, but for periocular recognition only limited amount of data is\navailable. The solution is to use CNN pre-trained on the dataset from the\nrelated domain, in this case the challenge is to extract efficiently the\ndiscriminative features. Using a pertained CNN model (VGG-Net), we propose a\nsimple, efficient and compact image representation technique that takes into\naccount the wealth of information and sparsity existing in the activations of\nthe convolutional layers and employs principle component analysis. For\nrecognition, we use an efficient and robust Sparse Augmented Collaborative\nRepresentation based Classification (SA-CRC) technique. For thorough evaluation\nof ConvSRC (the proposed system), experiments were carried out on the VISOB\nchallenging database which was presented for periocular recognition competition\nin ICIP2016. The obtained results show the superiority of ConvSRC over the\nstate-of-the-art methods; it obtains a GMR of more than 99% at FMR = 10-3 and\noutperforms the first winner of ICIP2016 challenge by 10%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 19:06:07 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Alahmadi", "Amani", ""], ["Hussain", "Muhammad", ""], ["Aboalsamh", "Hatim", ""], ["Zuair", "Mansour", ""]]}, {"id": "1801.05458", "submitter": "Tiep H. Vu", "authors": "Tiep Vu, Lam Nguyen, Tiantong Guo, Vishal Monga", "title": "Deep Network for Simultaneous Decomposition and Classification in\n  UWB-SAR Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying buried and obscured targets of interest from other natural and\nmanmade clutter objects in the scene is an important problem for the U.S. Army.\nTargets of interest are often represented by signals captured using\nlow-frequency (UHF to L-band) ultra-wideband (UWB) synthetic aperture radar\n(SAR) technology. This technology has been used in various applications,\nincluding ground penetration and sensing-through-the-wall. However, the\ntechnology still faces a significant issues regarding low-resolution SAR\nimagery in this particular frequency band, low radar cross sections (RCS),\nsmall objects compared to radar signal wavelengths, and heavy interference. The\nclassification problem has been firstly, and partially, addressed by sparse\nrepresentation-based classification (SRC) method which can extract noise from\nsignals and exploit the cross-channel information. Despite providing potential\nresults, SRC-related methods have drawbacks in representing nonlinear relations\nand dealing with larger training sets. In this paper, we propose a Simultaneous\nDecomposition and Classification Network (SDCN) to alleviate noise inferences\nand enhance classification accuracy. The network contains two jointly trained\nsub-networks: the decomposition sub-network handles denoising, while the\nclassification sub-network discriminates targets from confusers. Experimental\nresults show significant improvements over a network without decomposition and\nSRC-related methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 19:32:06 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 17:30:17 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Vu", "Tiep", ""], ["Nguyen", "Lam", ""], ["Guo", "Tiantong", ""], ["Monga", "Vishal", ""]]}, {"id": "1801.05525", "submitter": "Antonio Rueda-Toicen", "authors": "Wuilan Torres and Antonio Rueda-Toicen", "title": "Identification of Seed Cells in Multispectral Images for GrowCut\n  Segmentation", "comments": "10 pages, in Spanish, originally presented at CIMENICS 2016, accepted\n  to the Journal of the Faculty of Engineering UCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of satellite images is a necessary step to perform\nobject-oriented image classification, which has become relevant due to its\napplicability on images with a high spatial resolution. To perform\nobject-oriented image classification, the studied image must first be segmented\nin uniform regions. This segmentation requires manual work by an expert user,\nwho must exhaustively explore the image to establish thresholds that generate\nuseful and representative segments without oversegmenting and without\ndiscarding representative segments. We propose a technique that automatically\nsegments the multispectral image while facing these issues. We identify in the\nimage homogenous zones according to their spectral signatures through the use\nof morphological filters. These homogenous zones are representatives of\ndifferent types of land coverings in the image and are used as seeds for the\nGrowCut multispectral segmentation algorithm. GrowCut is a cellular automaton\nwith competitive region growth, its cells are linked to every pixel in the\nimage through three parameters: the pixel's spectral signature, a label, and a\nstrength factor that represents the strength with which a cell defends its\nlabel. The seed cells possess maximum strength and maintain their state\nthroughout the automaton's evolution. Starting from seed cells, each cell in\nthe image is iteratively attacked by its neighboring cells. When the automaton\nstops updating its states, we obtain a segmented image where each pixel has\ntaken the label of one of its cells. In this paper the algorithm was applied in\nan image acquired by Landsat8 on agricultural land of Calabozo, Guarico,\nVenezuela where there are different types of land coverings: agriculture, urban\nregions, water bodies, and savannas with different degrees of human\nintervention. The segmentation obtained is presented as irregular polygons\nenclosing geographical objects.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 01:57:27 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Torres", "Wuilan", ""], ["Rueda-Toicen", "Antonio", ""]]}, {"id": "1801.05527", "submitter": "Kei Fong Lam Prof", "authors": "Harald Garcke, Kei Fong Lam, Vanessa Styles", "title": "Cahn--Hilliard inpainting with the double obstacle potential", "comments": "26 pages, 8 figures, accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inpainting of damaged images has a wide range of applications, and many\ndifferent mathematical methods have been proposed to solve this problem.\nInpainting with the help of Cahn--Hilliard models has been particularly\nsuccessful, and it turns out that Cahn--Hilliard inpainting with the double\nobstacle potential can lead to better results compared to inpainting with a\nsmooth double well potential. However, a mathematical analysis of this approach\nis missing so far. In this paper we give first analytical results for a\nCahn--Hilliard double obstacle inpainting model regarding existence of global\nsolutions to the time-dependent problem and stationary solutions to the\ntime-independent problem without constraints on the parameters involved. With\nthe help of numerical results we show the effectiveness of the approach for\nbinary and grayscale images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 02:20:18 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 03:30:04 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Garcke", "Harald", ""], ["Lam", "Kei Fong", ""], ["Styles", "Vanessa", ""]]}, {"id": "1801.05551", "submitter": "Navaneeth Bodla", "authors": "Navaneeth Bodla, Gang Hua, Rama Chellappa", "title": "Semi-supervised FusedGAN for Conditional Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FusedGAN, a deep network for conditional image synthesis with\ncontrollable sampling of diverse images. Fidelity, diversity and controllable\nsampling are the main quality measures of a good image generation model. Most\nexisting models are insufficient in all three aspects. The FusedGAN can perform\ncontrollable sampling of diverse images with very high fidelity. We argue that\ncontrollability can be achieved by disentangling the generation process into\nvarious stages. In contrast to stacked GANs, where multiple stages of GANs are\ntrained separately with full supervision of labeled intermediate images, the\nFusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlike\nexisting methods, which requires full supervision with paired conditions and\nimages, the FusedGAN can effectively leverage more abundant images without\ncorresponding conditions in training, to produce more diverse samples with high\nfidelity. We achieve this by fusing two generators: one for unconditional image\ngeneration, and the other for conditional image generation, where the two\npartly share a common latent space thereby disentangling the generation. We\ndemonstrate the efficacy of the FusedGAN in fine grained image generation tasks\nsuch as text-to-image, and attribute-to-face generation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 05:07:49 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Bodla", "Navaneeth", ""], ["Hua", "Gang", ""], ["Chellappa", "Rama", ""]]}, {"id": "1801.05558", "submitter": "Yoonho Lee", "authors": "Yoonho Lee and Seungjin Choi", "title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based meta-learning methods leverage gradient descent to learn the\ncommonalities among various tasks. While previous such methods have been\nsuccessful in meta-learning tasks, they resort to simple gradient descent\nduring meta-testing. Our primary contribution is the {\\em MT-net}, which\nenables the meta-learner to learn on each layer's activation space a subspace\nthat the task-specific learner performs gradient descent on. Additionally, a\ntask-specific learner of an {\\em MT-net} performs gradient descent with respect\nto a meta-learned distance metric, which warps the activation space to be more\nsensitive to task identity. We demonstrate that the dimension of this learned\nsubspace reflects the complexity of the task-specific learner's adaptation\ntask, and also that our model is less sensitive to the choice of initial\nlearning rates than previous gradient-based meta-learning methods. Our method\nachieves state-of-the-art or comparable performance on few-shot classification\nand regression tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 05:34:08 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 07:40:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 12:33:23 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Lee", "Yoonho", ""], ["Choi", "Seungjin", ""]]}, {"id": "1801.05560", "submitter": "Chris McCool", "authors": "M. Halstead and C. McCool and S. Denman and T. Perez and C. Fookes", "title": "Fruit Quantity and Quality Estimation using a Robotic Vision System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localisation of crop remains highly challenging in unstructured\nenvironments such as farms. Many of the developed systems still rely on the use\nof hand selected features for crop identification and often neglect the\nestimation of crop quantity and quality, which is key to assigning labor during\nfarming processes. To alleviate these limitations we present a robotic vision\nsystem that can accurately estimate the quantity and quality of sweet pepper\n(Capsicum annuum L), a key horticultural crop. This system consists of three\nparts: detection, quality estimation, and tracking. Efficient detection is\nachieved using the FasterRCNN framework. Quality is then estimated in the same\nframework by learning a parallel layer which we show experimentally results in\nsuperior performance than treating quality as extra classes in the traditional\nFaster-RCNN framework. Evaluation of these two techniques outlines the improved\nperformance of the parallel layer, where we achieve an F1 score of 77.3 for the\nparallel technique yet only 72.5 for the best scoring (red) of the multi-class\nimplementation. To track the crop we present a tracking via detection approach,\nwhich uses the FasterRCNN with parallel layers, that is also a vision-only\nsolution. This approach is cheap to implement as it only requires a camera and\nin experiments across 2 days we show that our proposed system can accurately\nestimate the number of sweet pepper present, within 4.1% of the ground truth.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 05:53:26 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Halstead", "M.", ""], ["McCool", "C.", ""], ["Denman", "S.", ""], ["Perez", "T.", ""], ["Fookes", "C.", ""]]}, {"id": "1801.05568", "submitter": "Parth Shah", "authors": "Parth Shah, Vishvajit Bakarola, Supriya Pati", "title": "Image Captioning using Deep Neural Architectures", "comments": "Pre-print version of paper accepted at 2017 International Conference\n  on Innovations in information Embedded and Communication Systems (ICIIECS)", "journal-ref": null, "doi": "10.1109/ICIIECS.2017.8276124", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically creating the description of an image using any natural\nlanguages sentence like English is a very challenging task. It requires\nexpertise of both image processing as well as natural language processing. This\npaper discuss about different available models for image captioning task. We\nhave also discussed about how the advancement in the task of object recognition\nand machine translation has greatly improved the performance of image\ncaptioning model in recent years. In addition to that we have discussed how\nthis model can be implemented. In the end, we have also evaluated the\nperformance of model using standard evaluation matrices.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 06:24:44 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Shah", "Parth", ""], ["Bakarola", "Vishvajit", ""], ["Pati", "Supriya", ""]]}, {"id": "1801.05574", "submitter": "Ying Lu", "authors": "Ying Lu, Liming Chen, Alexandre Saidi, Xianfeng Gu", "title": "Brenier approach for optimal transportation between a quasi-discrete\n  measure and a discrete measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly estimating the discrepancy between two data distributions has\nalways been an important task in Machine Learning. Recently, Cuturi proposed\nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost\nbetween two distributions as a distance to describe distribution discrepancy.\nAlthough it has been successfully adopted in various machine learning\napplications (e.g. in Natural Language Processing and Computer Vision) since\nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The\nfirst one is that the Sinkhorn distance only gives an approximation of the real\nWasserstein distance, the second one is the `divide by zero' problem which\noften occurs during matrix scaling when setting the entropy regularization\ncoefficient to a small value. In this paper, we introduce a new Brenier\napproach for calculating a more accurate Wasserstein distance between two\ndiscrete distributions, this approach successfully avoids the two limitations\nshown above for Sinkhorn distance and gives an alternative way for estimating\ndistribution discrepancy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 07:06:21 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Lu", "Ying", ""], ["Chen", "Liming", ""], ["Saidi", "Alexandre", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1801.05585", "submitter": "Nanne Van Noord", "authors": "Nanne van Noord, Eric Postma", "title": "Light-weight pixel context encoders for image inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose Pixel Content Encoders (PCE), a light-weight image\ninpainting model, capable of generating novel con-tent for large missing\nregions in images. Unlike previously presented convolutional neural network\nbased models, our PCE model has an order of magnitude fewer trainable\nparameters. Moreover, by incorporating dilated convolutions we are able to\npreserve fine grained spatial information, achieving state-of-the-art\nperformance on benchmark datasets of natural images and paintings. Besides\nimage inpainting, we show that without changing the architecture, PCE can be\nused for image extrapolation, generating novel content beyond existing image\nboundaries.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 08:19:41 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["van Noord", "Nanne", ""], ["Postma", "Eric", ""]]}, {"id": "1801.05599", "submitter": "Feng Wang", "authors": "Feng Wang, Weiyang Liu, Haijun Liu, Jian Cheng", "title": "Additive Margin Softmax for Face Verification", "comments": "Published in Signal Processing Letters, Volume: 25 Issue: 7 Pages:\n  926-930", "journal-ref": null, "doi": "10.1109/LSP.2018.2822810", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a conceptually simple and geometrically\ninterpretable objective function, i.e. additive margin Softmax (AM-Softmax),\nfor deep face verification. In general, the face verification task can be\nviewed as a metric learning problem, so learning large-margin face features\nwhose intra-class variation is small and inter-class difference is large is of\ngreat importance in order to achieve good performance. Recently, Large-margin\nSoftmax and Angular Softmax have been proposed to incorporate the angular\nmargin in a multiplicative manner. In this work, we introduce a novel additive\nangular margin for the Softmax loss, which is intuitively appealing and more\ninterpretable than the existing works. We also emphasize and discuss the\nimportance of feature normalization in the paper. Most importantly, our\nexperiments on LFW BLUFR and MegaFace show that our additive margin softmax\nloss consistently performs better than the current state-of-the-art methods\nusing the same network architecture and training dataset. Our code has also\nbeen made available at https://github.com/happynear/AMSoftmax\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 09:13:05 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 13:39:44 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 12:22:44 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 12:35:03 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Wang", "Feng", ""], ["Liu", "Weiyang", ""], ["Liu", "Haijun", ""], ["Cheng", "Jian", ""]]}, {"id": "1801.05606", "submitter": "Andrea Romanoni", "authors": "Andrea Bignoli and Andrea Romanoni and Matteo Matteucci", "title": "Multi-View Stereo 3D Edge Reconstruction", "comments": "Accepted for WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for the reconstruction of 3D edges in\nmulti-view stereo scenarios. Previous research in the field typically relied on\nvideo sequences and limited the reconstruction process to either straight\nline-segments, or edge-points, i.e., 3D points that correspond to image edges.\nWe instead propose a system, denoted as EdgeGraph3D, able to recover both\nstraight and curved 3D edges from an unordered image sequence. A second\ncontribution of this work is a graph-based representation for 2D edges that\nallows the identification of the most structurally significant edges detected\nin an image. We integrate EdgeGraph3D in a multi-view stereo reconstruction\npipeline and analyze the benefits provided by 3D edges to the accuracy of the\nrecovered surfaces. We evaluate the effectiveness of our approach on multiple\ndatasets from two different collections in the multi-view stereo literature.\nExperimental results demonstrate the ability of EdgeGraph3D to work in presence\nof strong illumination changes and reflections, which are usually detrimental\nto the effectiveness of classical photometric reconstruction systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 09:46:01 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Bignoli", "Andrea", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1801.05678", "submitter": "Xianbiao Qi", "authors": "Xianbiao Qi and Lei Zhang", "title": "Face Recognition via Centralized Coordinate Learning", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owe to the rapid development of deep neural network (DNN) techniques and the\nemergence of large scale face databases, face recognition has achieved a great\nsuccess in recent years. During the training process of DNN, the face features\nand classification vectors to be learned will interact with each other, while\nthe distribution of face features will largely affect the convergence status of\nnetwork and the face similarity computing in test stage. In this work, we\nformulate jointly the learning of face features and classification vectors, and\npropose a simple yet effective centralized coordinate learning (CCL) method,\nwhich enforces the features to be dispersedly spanned in the coordinate space\nwhile ensuring the classification vectors to lie on a hypersphere. An adaptive\nangular margin is further proposed to enhance the discrimination capability of\nface features. Extensive experiments are conducted on six face benchmarks,\nincluding those have large age gap and hard negative samples. Trained only on\nthe small-scale CASIA Webface dataset with 460K face images from about 10K\nsubjects, our CCL model demonstrates high effectiveness and generality, showing\nconsistently competitive performance across all the six benchmark databases.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 14:32:40 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Qi", "Xianbiao", ""], ["Zhang", "Lei", ""]]}, {"id": "1801.05694", "submitter": "Awais Ashfaq", "authors": "Awais Ashfaq, Jonas Adler", "title": "A modified fuzzy C means algorithm for shading correction in\n  craniofacial CBCT images", "comments": "15 pages, published in CMBEBIH 2017", "journal-ref": "Proceedings of the International Conference on Medical and\n  Biological Engineering 2017", "doi": "10.1007/978-981-10-4166-2_81", "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CBCT images suffer from acute shading artifacts primarily due to scatter.\nNumerous image-domain correction algorithms have been proposed in the\nliterature that use patient-specific planning CT images to estimate shading\ncontributions in CBCT images. However, in the context of radiosurgery\napplications such as gamma knife, planning images are often acquired through\nMRI which impedes the use of polynomial fitting approaches for shading\ncorrection. We present a new shading correction approach that is independent of\nplanning CT images. Our algorithm is based on the assumption that true CBCT\nimages follow a uniform volumetric intensity distribution per material, and\nscatter perturbs this uniform texture by contributing cupping and shading\nartifacts in the image domain. The framework is a combination of fuzzy C-means\ncoupled with a neighborhood regularization term and Otsu's method. Experimental\nresults on artificially simulated craniofacial CBCT images are provided to\ndemonstrate the effectiveness of our algorithm. Spatial non-uniformity is\nreduced from 16% to 7% in soft tissue and from 44% to 8% in bone regions. With\nshading-correction, thresholding based segmentation accuracy for bone pixels is\nimproved from 85% to 91% when compared to thresholding without\nshading-correction. The proposed algorithm is thus practical and qualifies as a\nplug and play extension into any CBCT reconstruction software for shading\ncorrection.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 14:54:39 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Ashfaq", "Awais", ""], ["Adler", "Jonas", ""]]}, {"id": "1801.05746", "submitter": "Alexey Shvets", "authors": "Vladimir Iglovikov and Alexey Shvets", "title": "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image\n  Segmentation", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pixel-wise image segmentation is demanding task in computer vision. Classical\nU-Net architectures composed of encoders and decoders are very popular for\nsegmentation of medical images, satellite images etc. Typically, neural network\ninitialized with weights from a network pre-trained on a large data set like\nImageNet shows better performance than those trained from scratch on a small\ndataset. In some practical applications, particularly in medicine and traffic\nsafety, the accuracy of the models is of utmost importance. In this paper, we\ndemonstrate how the U-Net type architecture can be improved by the use of the\npre-trained encoder. Our code and corresponding pre-trained weights are\npublicly available at https://github.com/ternaus/TernausNet. We compare three\nweight initialization schemes: LeCun uniform, the encoder with weights from\nVGG11 and full network trained on the Carvana dataset. This network\narchitecture was a part of the winning solution (1st out of 735) in the Kaggle:\nCarvana Image Masking Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 16:49:10 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Iglovikov", "Vladimir", ""], ["Shvets", "Alexey", ""]]}, {"id": "1801.05787", "submitter": "Lucas Theis", "authors": "Lucas Theis, Iryna Korshunova, Alykhan Tejani, Ferenc Husz\\'ar", "title": "Faster gaze prediction with dense networks and Fisher pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human fixations from images has recently seen large improvements\nby leveraging deep representations which were pretrained for object\nrecognition. However, as we show in this paper, these networks are highly\noverparameterized for the task of fixation prediction. We first present a\nsimple yet principled greedy pruning method which we call Fisher pruning.\nThrough a combination of knowledge distillation and Fisher pruning, we obtain\nmuch more runtime-efficient architectures for saliency prediction, achieving a\n10x speedup for the same AUC performance as a state of the art network on the\nCAT2000 dataset. Speeding up single-image gaze prediction is important for many\nreal-world applications, but it is also a crucial step in the development of\nvideo saliency models, where the amount of data to be processed is\nsubstantially larger.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 18:34:33 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 10:38:35 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Theis", "Lucas", ""], ["Korshunova", "Iryna", ""], ["Tejani", "Alykhan", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1801.05800", "submitter": "R\\'emi Cura", "authors": "Remi Cura, Julien Perret, Nicolas Paparoditis", "title": "Interactive in-base street model edit: how common GIS software and a\n  database can serve as a custom Graphical User Interface", "comments": "this article is an extract from PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our modern world produces an increasing quantity of data, and especially\ngeospatial data, with advance of sensing technologies, and growing complexity\nand organisation of vector data. Tools are needed to efficiently create and\nedit those vector geospatial data. Procedural generation has been a tool of\nchoice to generate strongly organised data, yet it may be hard to control.\nBecause those data may be involved to take consequence-full real life\ndecisions, user interactions are required to check data and edit it. The\nclassical process to do so would be to build an adhoc Graphical User Interface\n(GUI) tool adapted for the model and method being used. This task is difficult,\ntakes a large amount of resources, and is very specific to one model, making it\nhard to share and re-use.\n  Besides, many common generic GUI already exists to edit vector data, each\nhaving its specialities. We propose a change of paradigm; instead of building a\nspecific tool for one task, we use common GIS software as GUIs, and deport the\nspecific interactions from the software to within the database. In this\nparadigm, GIS software simply modify geometry and attributes of database\nlayers, and those changes are used by the database to perform automated task.\n  This new paradigm has many advantages. The first one is genericity. With\nin-base interaction, any GIS software can be used to perform edition, whatever\nthe software is a Desktop sofware or a web application. The second is\nconcurrency and coherency. Because interaction is in-base, use of database\nfeatures allows seamless multi-user work, and can guarantee that the data is in\na coherent state. Last we propose tools to facilitate multi-user edits, both\nduring the edit phase (each user knows what areas are edited by other users),\nand before and after edit (planning of edit, analyse of edited areas).\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 18:55:43 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Cura", "Remi", ""], ["Perret", "Julien", ""], ["Paparoditis", "Nicolas", ""]]}, {"id": "1801.05895", "submitter": "Ruizhi Deng", "authors": "Ligeng Zhu and Ruizhi Deng and Michael Maire and Zhiwei Deng and Greg\n  Mori and Ping Tan", "title": "Sparsely Aggregated Convolutional Networks", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a key architectural aspect of deep convolutional neural networks:\nthe pattern of internal skip connections used to aggregate outputs of earlier\nlayers for consumption by deeper layers. Such aggregation is critical to\nfacilitate training of very deep networks in an end-to-end manner. This is a\nprimary reason for the widespread adoption of residual networks, which\naggregate outputs via cumulative summation. While subsequent works investigate\nalternative aggregation operations (e.g. concatenation), we focus on an\northogonal question: which outputs to aggregate at a particular point in the\nnetwork. We propose a new internal connection structure which aggregates only a\nsparse set of previous outputs at any given depth. Our experiments demonstrate\nthis simple design change offers superior performance with fewer parameters and\nlower computational requirements. Moreover, we show that sparse aggregation\nallows networks to scale more robustly to 1000+ layers, thereby opening future\navenues for training long-running visual processes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 01:02:58 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 04:12:15 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 07:51:35 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Zhu", "Ligeng", ""], ["Deng", "Ruizhi", ""], ["Maire", "Michael", ""], ["Deng", "Zhiwei", ""], ["Mori", "Greg", ""], ["Tan", "Ping", ""]]}, {"id": "1801.05912", "submitter": "Chen Shen", "authors": "Chen Shen, Holger R. Roth, Hirohisa Oda, Masahiro Oda, Yuichiro\n  Hayashi, Kazunari Misawa, Kensaku Mori", "title": "On the influence of Dice loss function in multi-class organ segmentation\n  of abdominal CT using 3D fully convolutional networks", "comments": "presented at MI-ken, November 2017, Takamatsu, Japan\n  (http://www.ieice.org/iss/mi/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods achieved impressive results for the segmentation\nof medical images. With the development of 3D fully convolutional networks\n(FCNs), it has become feasible to produce improved results for multi-organ\nsegmentation of 3D computed tomography (CT) images. The results of multi-organ\nsegmentation using deep learning-based methods not only depend on the choice of\nnetworks architecture, but also strongly rely on the choice of loss function.\nIn this paper, we present a discussion on the influence of Dice-based loss\nfunctions for multi-class organ segmentation using a dataset of abdominal CT\nvolumes. We investigated three different types of weighting the Dice loss\nfunctions based on class label frequencies (uniform, simple and square) and\nevaluate their influence on segmentation accuracies. Furthermore, we compared\nthe influence of different initial learning rates. We achieved average Dice\nscores of 81.3%, 59.5% and 31.7% for uniform, simple and square types of\nweighting when the learning rate is 0.001, and 78.2%, 81.0% and 58.5% for each\nweighting when the learning rate is 0.01. Our experiments indicated a strong\nrelationship between class balancing weights and initial learning rate in\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 02:46:06 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Shen", "Chen", ""], ["Roth", "Holger R.", ""], ["Oda", "Hirohisa", ""], ["Oda", "Masahiro", ""], ["Hayashi", "Yuichiro", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""]]}, {"id": "1801.05918", "submitter": "Liwen Zheng", "authors": "Liwen Zheng, Canmiao Fu, Yong Zhao", "title": "Extend the shallow part of Single Shot MultiBox Detector via\n  Convolutional Neural Network", "comments": "7 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Shot MultiBox Detector (SSD) is one of the fastest algorithms in the\ncurrent object detection field, which uses fully convolutional neural network\nto detect all scaled objects in an image. Deconvolutional Single Shot Detector\n(DSSD) is an approach which introduces more context information by adding the\ndeconvolution module to SSD. And the mean Average Precision (mAP) of DSSD on\nPASCAL VOC2007 is improved from SSD's 77.5% to 78.6%. Although DSSD obtains\nhigher mAP than SSD by 1.1%, the frames per second (FPS) decreases from 46 to\n11.8. In this paper, we propose a single stage end-to-end image detection model\ncalled ESSD to overcome this dilemma. Our solution to this problem is to\ncleverly extend better context information for the shallow layers of the best\nsingle stage (e.g. SSD) detectors. Experimental results show that our model can\nreach 79.4% mAP, which is higher than DSSD and SSD by 0.8 and 1.9 points\nrespectively. Meanwhile, our testing speed is 25 FPS in Titan X GPU which is\nmore than double the original DSSD.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 03:03:38 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Zheng", "Liwen", ""], ["Fu", "Canmiao", ""], ["Zhao", "Yong", ""]]}, {"id": "1801.05944", "submitter": "Zhenyu He", "authors": "Qiao Liu and Zhenyu He and Xin Li and Yuan Zheng", "title": "PTB-TIR: A Thermal Infrared Pedestrian Tracking Benchmark", "comments": "10 pages,IEEE Transactions on Multimedia (2019)", "journal-ref": null, "doi": "10.1109/TMM.2019.2932615", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal infrared (TIR) pedestrian tracking is one of the important components\namong numerous applications of computer vision, which has a major advantage: it\ncan track pedestrians in total darkness. The ability to evaluate the TIR\npedestrian tracker fairly, on a benchmark dataset, is significant for the\ndevelopment of this field. However, there is not a benchmark dataset. In this\npaper, we develop a TIR pedestrian tracking dataset for the TIR pedestrian\ntracker evaluation. The dataset includes 60 thermal sequences with manual\nannotations. Each sequence has nine attribute labels for the attribute based\nevaluation. In addition to the dataset, we carry out the large-scale evaluation\nexperiments on our benchmark dataset using nine publicly available trackers.\nThe experimental results help us understand the strengths and weaknesses of\nthese trackers.In addition, in order to gain more insight into the TIR\npedestrian tracker, we divide its functions into three components: feature\nextractor, motion model, and observation model. Then, we conduct three\ncomparison experiments on our benchmark dataset to validate how each component\naffects the tracker's performance. The findings of these experiments provide\nsome guidelines for future research. The dataset and evaluation toolkit can be\ndownloaded at {https://github.com/QiaoLiuHit/PTB-TIR_Evaluation_toolkit}.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 05:44:32 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:57:25 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 14:29:44 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Liu", "Qiao", ""], ["He", "Zhenyu", ""], ["Li", "Xin", ""], ["Zheng", "Yuan", ""]]}, {"id": "1801.05968", "submitter": "Alexander Khvostikov", "authors": "Alexander Khvostikov, Karim Aderghal, Jenny Benois-Pineau, Andrey\n  Krylov, Gwenaelle Catheline", "title": "3D CNN-based classification using sMRI and MD-DTI images for Alzheimer\n  disease studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided early diagnosis of Alzheimers Disease (AD) and its prodromal\nform, Mild Cognitive Impairment (MCI), has been the subject of extensive\nresearch in recent years. Some recent studies have shown promising results in\nthe AD and MCI determination using structural and functional Magnetic Resonance\nImaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor\nImaging (DTI) modalities. Furthermore, fusion of imaging modalities in a\nsupervised machine learning framework has shown promising direction of\nresearch.\n  In this paper we first review major trends in automatic classification\nmethods such as feature extraction based methods as well as deep learning\napproaches in medical image analysis applied to the field of Alzheimer's\nDisease diagnostics. Then we propose our own algorithm for Alzheimer's Disease\ndiagnostics based on a convolutional neural network and sMRI and DTI modalities\nfusion on hippocampal ROI using data from the Alzheimers Disease Neuroimaging\nInitiative (ADNI) database (http://adni.loni.usc.edu). Comparison with a single\nmodality approach shows promising results. We also propose our own method of\ndata augmentation for balancing classes of different size and analyze the\nimpact of the ROI size on the classification results as well.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 08:42:28 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Khvostikov", "Alexander", ""], ["Aderghal", "Karim", ""], ["Benois-Pineau", "Jenny", ""], ["Krylov", "Andrey", ""], ["Catheline", "Gwenaelle", ""]]}, {"id": "1801.06066", "submitter": "Xi Peng", "authors": "Xi Peng, Rogerio S. Feris, Xiaoyu Wang, Dimitris N. Metaxas", "title": "RED-Net: A Recurrent Encoder-Decoder Network for Video-based Face\n  Alignment", "comments": "International Journal of Computer Vision. arXiv admin note: text\n  overlap with arXiv:1608.05477", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for real-time face alignment in videos based on a\nrecurrent encoder-decoder network model. Our proposed model predicts 2D facial\npoint heat maps regularized by both detection and regression loss, while\nuniquely exploiting recurrent learning at both spatial and temporal dimensions.\nAt the spatial level, we add a feedback loop connection between the combined\noutput response map and the input, in order to enable iterative coarse-to-fine\nface alignment using a single network model, instead of relying on traditional\ncascaded model ensembles. At the temporal level, we first decouple the features\nin the bottleneck of the network into temporal-variant factors, such as pose\nand expression, and temporal-invariant factors, such as identity information.\nTemporal recurrent learning is then applied to the decoupled temporal-variant\nfeatures. We show that such feature disentangling yields better generalization\nand significantly more accurate results at test time. We perform a\ncomprehensive experimental analysis, showing the importance of each component\nof our proposed model, as well as superior results over the state of the art\nand several variations of our method in standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 04:29:44 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Peng", "Xi", ""], ["Feris", "Rogerio S.", ""], ["Wang", "Xiaoyu", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1801.06104", "submitter": "Joscha Diehl", "authors": "Joscha Diehl and Jeremy Reizenstein", "title": "Invariants of multidimensional time series based on their\n  iterated-integral signature", "comments": "complete rewrite of Section 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of features for multidimensional time series, that\nare invariant with respect to transformations of the ambient space. The general\nlinear group, the group of rotations and the group of permutations of the axes\nare considered. The starting point for their construction is Chen's\niterated-integral signature.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 15:53:00 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 12:18:09 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Diehl", "Joscha", ""], ["Reizenstein", "Jeremy", ""]]}, {"id": "1801.06277", "submitter": "Siyeong Lee", "authors": "Siyeong Lee, Gwon Hwan An, Suk-Ju Kang", "title": "Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single\n  Low Dynamic Range Image", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2868246", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep neural network model that reconstructs\na high dynamic range (HDR) image from a single low dynamic range (LDR) image.\nThe proposed model is based on a convolutional neural network composed of\ndilated convolutional layers, and infers LDR images with various exposures and\nillumination from a single LDR image of the same scene. Then, the final HDR\nimage can be formed by merging these inference results. It is relatively easy\nfor the proposed method to find the mapping between the LDR and an HDR with a\ndifferent bit depth because of the chaining structure inferring the\nrelationship between the LDR images with brighter (or darker) exposures from a\ngiven LDR image. The method not only extends the range, but also has the\nadvantage of restoring the light information of the actual physical world. For\nthe HDR images obtained by the proposed method, the HDR-VDP2 Q score, which is\nthe most popular evaluation metric for HDR images, was 56.36 for a display with\na 1920$\\times$1200 resolution, which is an improvement of 6 compared with the\nscores of conventional algorithms. In addition, when comparing the peak\nsignal-to-noise ratio values for tone mapped HDR images generated by the\nproposed and conventional algorithms, the average value obtained by the\nproposed algorithm is 30.86 dB, which is 10 dB higher than those obtained by\nthe conventional algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 03:03:13 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lee", "Siyeong", ""], ["An", "Gwon Hwan", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "1801.06288", "submitter": "Jingxin Liu", "authors": "Jingxin Liu, Bolei Xu, Chi Zheng, Yuanhao Gong, Jon Garibaldi, Daniele\n  Soria, Andew Green, Ian O. Ellis, Wenbin Zou, Guoping Qiu", "title": "An End-to-End Deep Learning Histochemical Scoring System for Breast\n  Cancer Tissue Microarray", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging 03 September 2018", "doi": "10.1109/TMI.2018.2868333", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the methods for stratifying different molecular classes of breast\ncancer is the Nottingham Prognostic Index Plus (NPI+) which uses breast cancer\nrelevant biomarkers to stain tumour tissues prepared on tissue microarray\n(TMA). To determine the molecular class of the tumour, pathologists will have\nto manually mark the nuclei activity biomarkers through a microscope and use a\nsemi-quantitative assessment method to assign a histochemical score (H-Score)\nto each TMA core. Manually marking positively stained nuclei is a time\nconsuming, imprecise and subjective process which will lead to inter-observer\nand intra-observer discrepancies. In this paper, we present an end-to-end deep\nlearning system which directly predicts the H-Score automatically. Our system\nimitates the pathologists' decision process and uses one fully convolutional\nnetwork (FCN) to extract all nuclei region (tumour and non-tumour), a second\nFCN to extract tumour nuclei region, and a multi-column convolutional neural\nnetwork which takes the outputs of the first two FCNs and the stain intensity\ndescription image as input and acts as the high-level decision making mechanism\nto directly output the H-Score of the input TMA image. To the best of our\nknowledge, this is the first end-to-end system that takes a TMA image as input\nand directly outputs a clinical score. We will present experimental results\nwhich demonstrate that the H-Scores predicted by our model have very high and\nstatistically significant correlation with experienced pathologists' scores and\nthat the H-Score discrepancy between our algorithm and the pathologists is on\npar with the inter-subject discrepancy between the pathologists.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 04:04:50 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Liu", "Jingxin", ""], ["Xu", "Bolei", ""], ["Zheng", "Chi", ""], ["Gong", "Yuanhao", ""], ["Garibaldi", "Jon", ""], ["Soria", "Daniele", ""], ["Green", "Andew", ""], ["Ellis", "Ian O.", ""], ["Zou", "Wenbin", ""], ["Qiu", "Guoping", ""]]}, {"id": "1801.06302", "submitter": "Jing Zhang", "authors": "Jing Zhang, Yang Cao, Yang Wang, Chenglin Wen, Chang Wen Chen", "title": "Fully Point-wise Convolutional Neural Network for Modeling Statistical\n  Regularities in Natural Images", "comments": "9 pages, 7 figures. To appear in ACM MM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling statistical regularity plays an essential role in ill-posed image\nprocessing problems. Recently, deep learning based methods have been presented\nto implicitly learn statistical representation of pixel distributions in\nnatural images and leverage it as a constraint to facilitate subsequent tasks,\nsuch as color constancy and image dehazing. However, the existing CNN\narchitecture is prone to variability and diversity of pixel intensity within\nand between local regions, which may result in inaccurate statistical\nrepresentation. To address this problem, this paper presents a novel fully\npoint-wise CNN architecture for modeling statistical regularities in natural\nimages. Specifically, we propose to randomly shuffle the pixels in the origin\nimages and leverage the shuffled image as input to make CNN more concerned with\nthe statistical properties. Moreover, since the pixels in the shuffled image\nare independent identically distributed, we can replace all the large\nconvolution kernels in CNN with point-wise ($1*1$) convolution kernels while\nmaintaining the representation ability. Experimental results on two\napplications: color constancy and image dehazing, demonstrate the superiority\nof our proposed network over the existing architectures, i.e., using\n1/10$\\sim$1/100 network parameters and computational cost while achieving\ncomparable performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:32:33 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 09:06:26 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 00:06:22 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Zhang", "Jing", ""], ["Cao", "Yang", ""], ["Wang", "Yang", ""], ["Wen", "Chenglin", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1801.06313", "submitter": "Penghang Yin", "authors": "Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi,\n  Jack Xin", "title": "BinaryRelax: A Relaxation Approach For Training Deep Neural Networks\n  With Quantized Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BinaryRelax, a simple two-phase algorithm, for training deep\nneural networks with quantized weights. The set constraint that characterizes\nthe quantization of weights is not imposed until the late stage of training,\nand a sequence of \\emph{pseudo} quantized weights is maintained. Specifically,\nwe relax the hard constraint into a continuous regularizer via Moreau envelope,\nwhich turns out to be the squared Euclidean distance to the set of quantized\nweights. The pseudo quantized weights are obtained by linearly interpolating\nbetween the float weights and their quantizations. A continuation strategy is\nadopted to push the weights towards the quantized state by gradually increasing\nthe regularization parameter. In the second phase, exact quantization scheme\nwith a small learning rate is invoked to guarantee fully quantized weights. We\ntest BinaryRelax on the benchmark CIFAR and ImageNet color image datasets to\ndemonstrate the superiority of the relaxed quantization approach and the\nimproved accuracy over the state-of-the-art training methods. Finally, we prove\nthe convergence of BinaryRelax under an approximate orthogonality condition.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 06:35:23 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 07:39:45 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 00:01:37 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yin", "Penghang", ""], ["Zhang", "Shuai", ""], ["Lyu", "Jiancheng", ""], ["Osher", "Stanley", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "1801.06345", "submitter": "Luojun Lin", "authors": "Lingyu Liang, Luojun Lin, Lianwen Jin, Duorui Xie and Mengru Li", "title": "SCUT-FBP5500: A Diverse Benchmark Dataset for Multi-Paradigm Facial\n  Beauty Prediction", "comments": "6 pages, 14 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial beauty prediction (FBP) is a significant visual recognition problem to\nmake assessment of facial attractiveness that is consistent to human\nperception. To tackle this problem, various data-driven models, especially\nstate-of-the-art deep learning techniques, were introduced, and benchmark\ndataset become one of the essential elements to achieve FBP. Previous works\nhave formulated the recognition of facial beauty as a specific supervised\nlearning problem of classification, regression or ranking, which indicates that\nFBP is intrinsically a computation problem with multiple paradigms. However,\nmost of FBP benchmark datasets were built under specific computation\nconstrains, which limits the performance and flexibility of the computational\nmodel trained on the dataset. In this paper, we argue that FBP is a\nmulti-paradigm computation problem, and propose a new diverse benchmark\ndataset, called SCUT-FBP5500, to achieve multi-paradigm facial beauty\nprediction. The SCUT-FBP5500 dataset has totally 5500 frontal faces with\ndiverse properties (male/female, Asian/Caucasian, ages) and diverse labels\n(face landmarks, beauty scores within [1,~5], beauty score distribution), which\nallows different computational models with different FBP paradigms, such as\nappearance-based/shape-based facial beauty classification/regression model for\nmale/female of Asian/Caucasian. We evaluated the SCUT-FBP5500 dataset for FBP\nusing different combinations of feature and predictor, and various deep\nlearning methods. The results indicates the improvement of FBP and the\npotential applications based on the SCUT-FBP5500.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 09:53:19 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Liang", "Lingyu", ""], ["Lin", "Luojun", ""], ["Jin", "Lianwen", ""], ["Xie", "Duorui", ""], ["Li", "Mengru", ""]]}, {"id": "1801.06349", "submitter": "Matei Mancas", "authors": "Matei Mancas, Christian Frisson, Jo\\\"elle Tilmanne, Nicolas\n  d'Alessandro, Petr Barborka, Furkan Bayansar, Francisco Bernard, Rebecca\n  Fiebrink, Alexis Heloir, Edgar Hemery, Sohaib Laraba, Alexis Moinet, Fabrizio\n  Nunnari, Thierry Ravet, Lo\\\"ic Reboursi\\`ere, Alvaro Sarasua, Micka\\\"el Tits,\n  No\\'e Tits, Fran\\c{c}ois Zaj\\'ega, Paolo Alborno, Ksenia Kolykhalova, Emma\n  Frid, Damiano Malafronte, Lisanne Huis in't Veld, H\\\"useyin Cakmak, Kevin El\n  Haddad, Nicolas Riche, Julien Leroy, Pierre Marighetto, Bekir Berker\n  T\\\"urker, Hossein Khaki, Roberto Pulisci, Emer Gilmartin, Fasih Haider,\n  K\\\"ubra Cengiz, Martin Sulir, Ilaria Torre, Shabbir Marzban, Ramazan\n  Yaz{\\i}c{\\i}, Furkan Burak B\\^agc{\\i}, Vedat Gazi K{\\i}l{\\i}, Hilal Sezer,\n  Sena B\\\"usra Yenge, Charles-Alexandre Delestage, Sylvie Leleu-Merviel, Muriel\n  Meyer-Chemenska, Daniel Schmitt, Willy Yvart, St\\'ephane Dupont, Ozan Can\n  Altiok, Ayseg\\\"ul Bumin, Ceren Dikmen, Ivan Giangreco, Silvan Heller, Emre\n  K\\\"ulah, Gueorgui Pironkov, Luca Rossetto, Yusuf Sahillioglu, Heiko Schuldt,\n  Omar Seddati, Yusuf Setinkaya, Metin Sezgin, Claudiu Tanase, Emre Toyan, Sean\n  Wood, Doguhan Yeke, Fran\\c{c}cois Rocca, Pierre-Henri De Deken, Alessandra\n  Bandrabur, Fabien Grisard, Axel Jean-Caurant, Vincent Courboulay, Radhwan Ben\n  Madhkour, Ambroise Moreau", "title": "Proceedings of eNTERFACE 2015 Workshop on Intelligent Interfaces", "comments": "159 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted\nby the Numediart Institute of Creative Technologies of the University of Mons\nfrom August 10th to September 2015. During the four weeks, students and\nresearchers from all over the world came together in the Numediart Institute of\nthe University of Mons to work on eight selected projects structured around\nintelligent interfaces. Eight projects were selected and their reports are\nshown here.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 10:03:35 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Mancas", "Matei", ""], ["Frisson", "Christian", ""], ["Tilmanne", "Jo\u00eblle", ""], ["d'Alessandro", "Nicolas", ""], ["Barborka", "Petr", ""], ["Bayansar", "Furkan", ""], ["Bernard", "Francisco", ""], ["Fiebrink", "Rebecca", ""], ["Heloir", "Alexis", ""], ["Hemery", "Edgar", ""], ["Laraba", "Sohaib", ""], ["Moinet", "Alexis", ""], ["Nunnari", "Fabrizio", ""], ["Ravet", "Thierry", ""], ["Reboursi\u00e8re", "Lo\u00efc", ""], ["Sarasua", "Alvaro", ""], ["Tits", "Micka\u00ebl", ""], ["Tits", "No\u00e9", ""], ["Zaj\u00e9ga", "Fran\u00e7ois", ""], ["Alborno", "Paolo", ""], ["Kolykhalova", "Ksenia", ""], ["Frid", "Emma", ""], ["Malafronte", "Damiano", ""], ["Veld", "Lisanne Huis in't", ""], ["Cakmak", "H\u00fcseyin", ""], ["Haddad", "Kevin El", ""], ["Riche", "Nicolas", ""], ["Leroy", "Julien", ""], ["Marighetto", "Pierre", ""], ["T\u00fcrker", "Bekir Berker", ""], ["Khaki", "Hossein", ""], ["Pulisci", "Roberto", ""], ["Gilmartin", "Emer", ""], ["Haider", "Fasih", ""], ["Cengiz", "K\u00fcbra", ""], ["Sulir", "Martin", ""], ["Torre", "Ilaria", ""], ["Marzban", "Shabbir", ""], ["Yaz\u0131c\u0131", "Ramazan", ""], ["B\u00e2gc\u0131", "Furkan Burak", ""], ["K\u0131l\u0131", "Vedat Gazi", ""], ["Sezer", "Hilal", ""], ["Yenge", "Sena B\u00fcsra", ""], ["Delestage", "Charles-Alexandre", ""], ["Leleu-Merviel", "Sylvie", ""], ["Meyer-Chemenska", "Muriel", ""], ["Schmitt", "Daniel", ""], ["Yvart", "Willy", ""], ["Dupont", "St\u00e9phane", ""], ["Altiok", "Ozan Can", ""], ["Bumin", "Ayseg\u00fcl", ""], ["Dikmen", "Ceren", ""], ["Giangreco", "Ivan", ""], ["Heller", "Silvan", ""], ["K\u00fclah", "Emre", ""], ["Pironkov", "Gueorgui", ""], ["Rossetto", "Luca", ""], ["Sahillioglu", "Yusuf", ""], ["Schuldt", "Heiko", ""], ["Seddati", "Omar", ""], ["Setinkaya", "Yusuf", ""], ["Sezgin", "Metin", ""], ["Tanase", "Claudiu", ""], ["Toyan", "Emre", ""], ["Wood", "Sean", ""], ["Yeke", "Doguhan", ""], ["Rocca", "Fran\u00e7cois", ""], ["De Deken", "Pierre-Henri", ""], ["Bandrabur", "Alessandra", ""], ["Grisard", "Fabien", ""], ["Jean-Caurant", "Axel", ""], ["Courboulay", "Vincent", ""], ["Madhkour", "Radhwan Ben", ""], ["Moreau", "Ambroise", ""]]}, {"id": "1801.06353", "submitter": "Siddique Latif", "authors": "Siddique Latif, Rajib Rana, Shahzad Younis, Junaid Qadir, and Julien\n  Epps", "title": "Transfer Learning for Improving Speech Emotion Classification Accuracy", "comments": "Proc. Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing speech emotion recognition research focuses on\nautomatic emotion detection using training and testing data from same corpus\ncollected under the same conditions. The performance of such systems has been\nshown to drop significantly in cross-corpus and cross-language scenarios. To\naddress the problem, this paper exploits a transfer learning technique to\nimprove the performance of speech emotion recognition systems that is novel in\ncross-language and cross-corpus scenarios. Evaluations on five different\ncorpora in three different languages show that Deep Belief Networks (DBNs)\noffer better accuracy than previous approaches on cross-corpus emotion\nrecognition, relative to a Sparse Autoencoder and SVM baseline system. Results\nalso suggest that using a large number of languages for training and using a\nsmall fraction of the target data in training can significantly boost accuracy\ncompared with baseline also for the corpus with limited training examples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 10:16:11 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 07:51:51 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 13:39:49 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 01:36:53 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Latif", "Siddique", ""], ["Rana", "Rajib", ""], ["Younis", "Shahzad", ""], ["Qadir", "Junaid", ""], ["Epps", "Julien", ""]]}, {"id": "1801.06397", "submitter": "Nikolaus Mayer", "authors": "Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel\n  Cremers, Alexey Dosovitskiy, Thomas Brox", "title": "What Makes Good Synthetic Training Data for Learning Disparity and\n  Optical Flow Estimation?", "comments": "added references (UCL dataset); added IJCV copyright information", "journal-ref": null, "doi": "10.1007/s11263-018-1082-6", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finding that very large networks can be trained efficiently and reliably\nhas led to a paradigm shift in computer vision from engineered solutions to\nlearning formulations. As a result, the research challenge shifts from devising\nalgorithms to creating suitable and abundant training data for supervised\nlearning. How to efficiently create such training data? The dominant data\nacquisition method in visual recognition is based on web data and manual\nannotation. Yet, for many computer vision problems, such as stereo or optical\nflow estimation, this approach is not feasible because humans cannot manually\nenter a pixel-accurate flow field. In this paper, we promote the use of\nsynthetically generated data for the purpose of training deep networks on such\ntasks.We suggest multiple ways to generate such data and evaluate the influence\nof dataset properties on the performance and generalization properties of the\nresulting networks. We also demonstrate the benefit of learning schedules that\nuse different types of data at selected stages of the training process.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 13:21:07 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 10:28:01 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 10:26:58 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Mayer", "Nikolaus", ""], ["Ilg", "Eddy", ""], ["Fischer", "Philipp", ""], ["Hazirbas", "Caner", ""], ["Cremers", "Daniel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1801.06432", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Yannis Panagakis, Stefanos Zafeiriou", "title": "Robust Kronecker Component Analysis", "comments": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Special Issue on Compact and Efficient Feature Representation and Learning in\n  Computer Vision, 2018. Contains appendices. arXiv admin note: text overlap\n  with arXiv:1703.07886", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning and component analysis models are fundamental for\nlearning compact representations that are relevant to a given task (feature\nextraction, dimensionality reduction, denoising, etc.). The model complexity is\nencoded by means of specific structure, such as sparsity, low-rankness, or\nnonnegativity. Unfortunately, approaches like K-SVD - that learn dictionaries\nfor sparse coding via Singular Value Decomposition (SVD) - are hard to scale to\nhigh-volume and high-dimensional visual data, and fragile in the presence of\noutliers. Conversely, robust component analysis methods such as the Robust\nPrincipal Component Analysis (RPCA) are able to recover low-complexity (e.g.,\nlow-rank) representations from data corrupted with noise of unknown magnitude\nand support, but do not provide a dictionary that respects the structure of the\ndata (e.g., images), and also involve expensive computations. In this paper, we\npropose a novel Kronecker-decomposable component analysis model, coined as\nRobust Kronecker Component Analysis (RKCA), that combines ideas from sparse\ndictionary learning and robust component analysis. RKCA has several appealing\nproperties, including robustness to gross corruption; it can be used for\nlow-rank modeling, and leverages separability to solve significantly smaller\nproblems. We design an efficient learning algorithm by drawing links with a\nrestricted form of tensor factorization, and analyze its optimality and\nlow-rankness properties. The effectiveness of the proposed approach is\ndemonstrated on real-world applications, namely background subtraction and\nimage denoising and completion, by performing a thorough comparison with the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 18:01:50 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 20:55:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Bahri", "Mehdi", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1801.06434", "submitter": "Lutz Roese-Koerner", "authors": "Ido Freeman, Lutz Roese-Koerner, Anton Kummert", "title": "EffNet: An Efficient Structure for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever increasing application of Convolutional Neural Networks to\ncustomer products the need emerges for models to efficiently run on embedded,\nmobile hardware. Slimmer models have therefore become a hot research topic with\nvarious approaches which vary from binary networks to revised convolution\nlayers. We offer our contribution to the latter and propose a novel convolution\nblock which significantly reduces the computational burden while surpassing the\ncurrent state-of-the-art. Our model, dubbed EffNet, is optimised for models\nwhich are slim to begin with and is created to tackle issues in existing models\nsuch as MobileNet and ShuffleNet.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 14:57:23 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 12:50:36 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 22:25:23 GMT"}, {"version": "v4", "created": "Thu, 1 Mar 2018 16:10:33 GMT"}, {"version": "v5", "created": "Wed, 14 Mar 2018 08:52:59 GMT"}, {"version": "v6", "created": "Tue, 5 Jun 2018 12:10:12 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Freeman", "Ido", ""], ["Roese-Koerner", "Lutz", ""], ["Kummert", "Anton", ""]]}, {"id": "1801.06445", "submitter": "Fei Yang", "authors": "Fei Yang, Qian Zhang, Miaohui Wang and Guoping Qiu", "title": "Quality Classified Image Analysis with Application to Face Detection and\n  Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur, out of focus, insufficient spatial resolution, lossy compression\nand many other factors can all cause an image to have poor quality. However,\nimage quality is a largely ignored issue in traditional pattern recognition\nliterature. In this paper, we use face detection and recognition as case\nstudies to show that image quality is an essential factor which will affect the\nperformances of traditional algorithms. We demonstrated that it is not the\nimage quality itself that is the most important, but rather the quality of the\nimages in the training set should have similar quality as those in the testing\nset. To handle real-world application scenarios where images with different\nkinds and severities of degradation can be presented to the system, we have\ndeveloped a quality classified image analysis framework to deal with images of\nmixed qualities adaptively. We use deep neural networks first to classify\nimages based on their quality classes and then design a separate face detector\nand recognizer for images in each quality class. We will present experimental\nresults to show that our quality classified framework can accurately classify\nimages based on the type and severity of image degradations and can\nsignificantly boost the performances of state-of-the-art face detector and\nrecognizer in dealing with image datasets containing mixed quality images.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 15:18:21 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Yang", "Fei", ""], ["Zhang", "Qian", ""], ["Wang", "Miaohui", ""], ["Qiu", "Guoping", ""]]}, {"id": "1801.06452", "submitter": "Qinbing Fu", "authors": "Qinbing Fu and Cheng Hu and Shigang Yue", "title": "Collision Selective Visual Neural Network Inspired by LGMD2 Neurons in\n  Juvenile Locusts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous robots in dynamic environments mixed with human, it is vital\nto detect impending collision quickly and robustly. The biological visual\nsystems evolved over millions of years may provide us efficient solutions for\ncollision detection in complex environments. In the cockpit of locusts, two\nLobula Giant Movement Detectors, i.e. LGMD1 and LGMD2, have been identified\nwhich respond to looming objects rigorously with high firing rates. Compared to\nLGMD1, LGMD2 matures early in the juvenile locusts with specific selectivity to\ndark moving objects against bright background in depth while not responding to\nlight objects embedded in dark background - a similar situation which ground\nvehicles and robots are facing with. However, little work has been done on\nmodeling LGMD2, let alone its potential in robotics and other vision-based\napplications. In this article, we propose a novel way of modeling LGMD2 neuron,\nwith biased ON and OFF pathways splitting visual streams into parallel channels\nencoding brightness increments and decrements separately to fulfill its\nselectivity. Moreover, we apply a biophysical mechanism of spike frequency\nadaptation to shape the looming selectivity in such a collision-detecting\nneuron model. The proposed visual neural network has been tested with\nsystematic experiments, challenged against synthetic and real physical stimuli,\nas well as image streams from the sensor of a miniature robot. The results\ndemonstrated this framework is able to detect looming dark objects embedded in\nbright backgrounds selectively, which make it ideal for ground mobile\nplatforms. The robotic experiments also showed its robustness in collision\ndetection - it performed well for near range navigation in an arena with many\nobstacles. Its enhanced collision selectivity to dark approaching objects\nversus receding and translating ones has also been verified via systematic\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 00:34:55 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Fu", "Qinbing", ""], ["Hu", "Cheng", ""], ["Yue", "Shigang", ""]]}, {"id": "1801.06457", "submitter": "Jose Bernal Moyano", "authors": "Jose Bernal, Kaisar Kushibar, Mariano Cabezas, Sergi Valverde, Arnau\n  Oliver, Xavier Llad\\'o", "title": "Quantitative analysis of patch-based fully convolutional neural networks\n  for tissue segmentation on brain magnetic resonance imaging", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2926697", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate brain tissue segmentation in Magnetic Resonance Imaging (MRI) has\nattracted the attention of medical doctors and researchers since variations in\ntissue volume help in diagnosing and monitoring neurological diseases. Several\nproposals have been designed throughout the years comprising conventional\nmachine learning strategies as well as convolutional neural networks (CNN)\napproaches. In particular, in this paper, we analyse a sub-group of deep\nlearning methods producing dense predictions. This branch, referred in the\nliterature as Fully CNN (FCNN), is of interest as these architectures can\nprocess an input volume in less time than CNNs and local spatial dependencies\nmay be encoded since several voxels are classified at once. Our study focuses\non understanding architectural strengths and weaknesses of literature-like\napproaches. Hence, we implement eight FCNN architectures inspired by robust\nstate-of-the-art methods on brain segmentation related tasks. We evaluate them\nusing the IBSR18, MICCAI2012 and iSeg2017 datasets as they contain infant and\nadult data and exhibit varied voxel spacing, image quality, number of scans and\navailable imaging modalities. The discussion is driven in three directions:\ncomparison between 2D and 3D approaches, the importance of multiple modalities\nand overlapping as a sampling strategy for training and testing models. To\nencourage other researchers to explore the evaluation framework, a public\nversion is accessible to download from our research website.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 15:24:15 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 11:43:59 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bernal", "Jose", ""], ["Kushibar", "Kaisar", ""], ["Cabezas", "Mariano", ""], ["Valverde", "Sergi", ""], ["Oliver", "Arnau", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1801.06490", "submitter": "Pankaj Pansari", "authors": "Pankaj Pansari, Chris Russell, M.Pawan Kumar", "title": "Worst-case Optimal Submodular Extensions for Marginal Estimation", "comments": "Accepted to AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular extensions of an energy function can be used to efficiently\ncompute approximate marginals via variational inference. The accuracy of the\nmarginals depends crucially on the quality of the submodular extension. To\nidentify the best possible extension, we show an equivalence between the\nsubmodular extensions of the energy and the objective functions of linear\nprogramming (LP) relaxations for the corresponding MAP estimation problem. This\nallows us to (i) establish the worst-case optimality of the submodular\nextension for Potts model used in the literature; (ii) identify the worst-case\noptimal submodular extension for the more general class of metric labeling; and\n(iii) efficiently compute the marginals for the widely used dense CRF model\nwith the help of a recently proposed Gaussian filtering method. Using synthetic\nand real data, we show that our approach provides comparable upper bounds on\nthe log-partition function to those obtained using tree-reweighted message\npassing (TRW) in cases where the latter is computationally feasible.\nImportantly, unlike TRW, our approach provides the first practical algorithm to\ncompute an upper bound on the dense CRF model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 14:36:57 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Pansari", "Pankaj", ""], ["Russell", "Chris", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1801.06504", "submitter": "Alexandre Attia", "authors": "Alexandre Attia, Sharone Dayan", "title": "Detecting and counting tiny faces", "comments": "4 pages, 10 figures, 2 appendix page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding Tiny Faces (by Hu and Ramanan) proposes a novel approach to find\nsmall objects in an image. Our contribution consists in deeply understanding\nthe choices of the paper together with applying and extending a similar method\nto a real world subject which is the counting of people in a public\ndemonstration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 17:41:12 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 16:04:15 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Attia", "Alexandre", ""], ["Dayan", "Sharone", ""]]}, {"id": "1801.06510", "submitter": "Joel Brogan Joel R Brogan", "authors": "Daniel Moreira, Aparna Bharati, Joel Brogan, Allan Pinto, Michael\n  Parowski, Kevin W. Bowyer, Patrick J. Flynn, Anderson Rocha, Walter J.\n  Scheirer", "title": "Image Provenance Analysis at Scale", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior art has shown it is possible to estimate, through image processing and\ncomputer vision techniques, the types and parameters of transformations that\nhave been applied to the content of individual images to obtain new images.\nGiven a large corpus of images and a query image, an interesting further step\nis to retrieve the set of original images whose content is present in the query\nimage, as well as the detailed sequences of transformations that yield the\nquery image given the original images. This is a problem that recently has\nreceived the name of image provenance analysis. In these times of public media\nmanipulation ( e.g., fake news and meme sharing), obtaining the history of\nimage transformations is relevant for fact checking and authorship\nverification, among many other applications. This article presents an\nend-to-end processing pipeline for image provenance analysis, which works at\nreal-world scale. It employs a cutting-edge image filtering solution that is\ncustom-tailored for the problem at hand, as well as novel techniques for\nobtaining the provenance graph that expresses how the images, as nodes, are\nancestrally connected. A comprehensive set of experiments for each stage of the\npipeline is provided, comparing the proposed solution with state-of-the-art\nresults, employing previously published datasets. In addition, this work\nintroduces a new dataset of real-world provenance cases from the social media\nsite Reddit, along with baseline results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 17:54:22 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 14:41:34 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Moreira", "Daniel", ""], ["Bharati", "Aparna", ""], ["Brogan", "Joel", ""], ["Pinto", "Allan", ""], ["Parowski", "Michael", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""], ["Rocha", "Anderson", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1801.06519", "submitter": "Arun Mallya", "authors": "Arun Mallya, Dillon Davis, Svetlana Lazebnik", "title": "Piggyback: Adapting a Single Network to Multiple Tasks by Learning to\n  Mask Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method for adapting a single, fixed deep neural network\nto multiple tasks without affecting performance on already learned tasks. By\nbuilding upon ideas from network quantization and pruning, we learn binary\nmasks that piggyback on an existing network, or are applied to unmodified\nweights of that network to provide good performance on a new task. These masks\nare learned in an end-to-end differentiable fashion, and incur a low overhead\nof 1 bit per network parameter, per task. Even though the underlying network is\nfixed, the ability to mask individual weights allows for the learning of a\nlarge number of filters. We show performance comparable to dedicated fine-tuned\nnetworks for a variety of classification tasks, including those with large\ndomain shifts from the initial task (ImageNet), and a variety of network\narchitectures. Unlike prior work, we do not suffer from catastrophic forgetting\nor competition between tasks, and our performance is agnostic to task ordering.\nCode available at https://github.com/arunmallya/piggyback.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 18:25:59 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 21:29:28 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Mallya", "Arun", ""], ["Davis", "Dillon", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1801.06523", "submitter": "Nachiket Deo", "authors": "Nachiket Deo, Akshay Rangesh, Mohan M. Trivedi", "title": "How would surround vehicles move? A Unified Framework for Maneuver\n  Classification and Motion Prediction", "comments": "Accepted for publication in IEEE transactions on Intelligent Vehicles", "journal-ref": "IEEE Transactions on Intelligent Vehicles ( Volume: 3, Issue: 2,\n  June 2018 )", "doi": "10.1109/TIV.2018.2804159", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable prediction of surround vehicle motion is a critical requirement for\npath planning for autonomous vehicles. In this paper we propose a unified\nframework for surround vehicle maneuver classification and motion prediction\nthat exploits multiple cues, namely, the estimated motion of vehicles, an\nunderstanding of typical motion patterns of freeway traffic and inter-vehicle\ninteraction. We report our results in terms of maneuver classification accuracy\nand mean and median absolute error of predicted trajectories against the ground\ntruth for real traffic data collected using vehicle mounted sensors on\nfreeways. An ablative analysis is performed to analyze the relative importance\nof each cue for trajectory prediction. Additionally, an analysis of execution\ntime for the components of the framework is presented. Finally, we present\nmultiple case studies analyzing the outputs of our model for complex traffic\nscenarios\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 18:31:27 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Deo", "Nachiket", ""], ["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1801.06593", "submitter": "Thangarajah Akilan Mr", "authors": "Thangarajah Akilan", "title": "A Foreground Inference Network for Video Surveillance Using Multi-View\n  Receptive Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground (FG) pixel labelling plays a vital role in video surveillance.\nRecent engineering solutions have attempted to exploit the efficacy of deep\nlearning (DL) models initially targeted for image classification to deal with\nFG pixel labelling. One major drawback of such strategy is the lacking\ndelineation of visual objects when training samples are limited. To grapple\nwith this issue, we introduce a multi-view receptive field fully convolutional\nneural network (MV-FCN) that harness recent seminal ideas, such as, fully\nconvolutional structure, inception modules, and residual networking. Therefrom,\nwe implement a system in an encoder-decoder fashion that subsumes a core and\ntwo complementary feature flow paths. The model exploits inception modules at\nearly and late stages with three different sizes of receptive fields to capture\ninvariance at various scales. The features learned in the encoding phase are\nfused with appropriate feature maps in the decoding phase through residual\nconnections for achieving enhanced spatial representation. These multi-view\nreceptive fields and residual feature connections are expected to yield highly\ngeneralized features for an accurate pixel-wise FG region identification. It\nis, then, trained with database specific exemplary segmentations to predict\ndesired FG objects.\n  The comparative experimental results on eleven benchmark datasets validate\nthat the proposed model achieves very competitive performance with the prior-\nand state-of-the-art algorithms. We also report that how well a transfer\nlearning approach can be useful to enhance the performance of our proposed\nMV-FCN.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 23:01:16 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Akilan", "Thangarajah", ""]]}, {"id": "1801.06611", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Multiple Description Convolutional Neural Networks for Image Compression", "comments": "13 pages, 3 tables, and 6 figures. Accepted by IEEE Trans. CSVT. Our\n  another MDC paper based on deep learning is \"Deep Multiple Description Coding\n  by Learning Scalar Quantization\", which was accepted by DCC-2019.\n  (arXiv:1811.01504)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple description coding (MDC) is able to stably transmit the signal in\nthe un-reliable and non-prioritized networks, which has been broadly studied\nfor several decades. However, the traditional MDC doesn't well leverage image's\ncontext features to generate multiple descriptions. In this paper, we propose a\nnovel standard-compliant convolutional neural network-based MDC framework in\nterm of image's context features. Firstly, multiple description generator\nnetwork (MDGN) is designed to produce appearance-similar yet feature-different\nmultiple descriptions automatically according to image's content, which are\ncompressed by standard codec. Secondly, we present multiple description\nreconstruction network (MDRN) including side reconstruction network (SRN) and\ncentral reconstruction network (CRN). When any one of two lossy descriptions is\nreceived at the decoder, SRN network is used to improve the quality of this\ndecoded lossy description by removing the compression artifact and up-sampling\nsimultaneously. Meanwhile, we utilize CRN network with two decoded descriptions\nas inputs for better reconstruction, if both of lossy descriptions are\navailable. Thirdly, multiple description virtual codec network (MDVCN) is\nproposed to bridge the gap between MDGN network and MDRN network in order to\ntrain an end-to-end MDC framework. Here, two learning algorithms are provided\nto train our whole framework. In addition to structural similarity loss\nfunction, the produced descriptions are used as opposing labels with multiple\ndescription distance loss function to regularize the training of MDGN network.\nThese losses guarantee that the generated description images are structurally\nsimilar yet finely diverse. Experimental results show a great deal of objective\nand subjective quality measurements to validate the efficiency of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 01:28:20 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 02:47:01 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1801.06635", "submitter": "Danping Liao", "authors": "Danping Liao, Siyu Chen, Yuntao Qian", "title": "Visualization of Hyperspectral Images Using Moving Least Squares", "comments": "arXiv admin note: text overlap with arXiv:1712.01657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying the large number of bands in a hyper spectral image on a\ntrichromatic monitor has been an active research topic. The visualized image\nshall convey as much information as possible form the original data and\nfacilitate image interpretation. Most existing methods display HSIs in false\ncolors which contradict with human's experience and expectation. In this paper,\nwe propose a nonlinear approach to visualize an input HSI with natural colors\nby taking advantage of a corresponding RGB image. Our approach is based on\nMoving Least Squares, an interpolation scheme for reconstructing a surface from\na set of control points, which in our case is a set of matching pixels between\nthe HSI and the corresponding RGB image. Based on MLS, the proposed method\nsolves for each spectral signature a unique transformation so that the non\nlinear structure of the HSI can be preserved. The matching pixels between a\npair of HSI and RGB image can be reused to display other HSIs captured b the\nsame imaging sensor with natural colors. Experiments show that the output image\nof the proposed method no only have natural colors but also maintain the visual\ninformation necessary for human analysis.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 07:01:27 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Liao", "Danping", ""], ["Chen", "Siyu", ""], ["Qian", "Yuntao", ""]]}, {"id": "1801.06642", "submitter": "Hefeng Wu", "authors": "Hanhui Li, Xiangjian He, Hefeng Wu, Saeed Amirgholipour Kasmani,\n  Ruomei Wang, Xiaonan Luo, Liang Lin", "title": "Structured Inhomogeneous Density Map Learning for Crowd Counting", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at tackling the problem of crowd counting in extremely\nhigh-density scenes, which contain hundreds, or even thousands of people. We\nbegin by a comprehensive analysis of the most widely used density map-based\nmethods, and demonstrate how easily existing methods are affected by the\ninhomogeneous density distribution problem, e.g., causing them to be sensitive\nto outliers, or be hard to optimized. We then present an extremely simple\nsolution to the inhomogeneous density distribution problem, which can be\nintuitively summarized as extending the density map from 2D to 3D, with the\nextra dimension implicitly indicating the density level. Such solution can be\nimplemented by a single Density-Aware Network, which is not only easy to train,\nbut also can achieve the state-of-art performance on various challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 09:21:52 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Hanhui", ""], ["He", "Xiangjian", ""], ["Wu", "Hefeng", ""], ["Kasmani", "Saeed Amirgholipour", ""], ["Wang", "Ruomei", ""], ["Luo", "Xiaonan", ""], ["Lin", "Liang", ""]]}, {"id": "1801.06665", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Yannis Panagakis, Stefanos Zafeiriou", "title": "Visual Data Augmentation through Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid progress in machine learning methods has been empowered by i) huge\ndatasets that have been collected and annotated, ii) improved engineering (e.g.\ndata pre-processing/normalization). The existing datasets typically include\nseveral million samples, which constitutes their extension a colossal task. In\naddition, the state-of-the-art data-driven methods demand a vast amount of\ndata, hence a standard engineering trick employed is artificial data\naugmentation for instance by adding into the data cropped and (affinely)\ntransformed images. However, this approach does not correspond to any change in\nthe natural 3D scene.\n  We propose instead to perform data augmentation through learning realistic\nlocal transformations. We learn a forward and an inverse transformation that\nmaps an image from the high-dimensional space of pixel intensities to a latent\nspace which varies (approximately) linearly with the latent space of a\nrealistically transformed version of the image. Such transformed images can be\nconsidered two successive frames in a video. Next, we utilize these\ntransformations to learn a linear model that modifies the latent spaces and\nthen use the inverse transformation to synthesize a new image. We argue that\nthe this procedure produces powerful invariant representations. We perform both\nqualitative and quantitative experiments that demonstrate our proposed method\ncreates new realistic images.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 12:08:59 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1801.06687", "submitter": "Hongxin Wang", "authors": "Hongxin Wang, Jigen Peng and Shigang Yue", "title": "A Directionally Selective Small Target Motion Detecting Visual Neural\n  Network in Cluttered Backgrounds", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": "10.1109/TCYB.2018.2869384", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminating targets moving against a cluttered background is a huge\nchallenge, let alone detecting a target as small as one or a few pixels and\ntracking it in flight. In the fly's visual system, a class of specific neurons,\ncalled small target motion detectors (STMDs), have been identified as showing\nexquisite selectivity for small target motion. Some of the STMDs have also\ndemonstrated directional selectivity which means these STMDs respond strongly\nonly to their preferred motion direction. Directional selectivity is an\nimportant property of these STMD neurons which could contribute to tracking\nsmall targets such as mates in flight. However, little has been done on\nsystematically modeling these directional selective STMD neurons. In this\npaper, we propose a directional selective STMD-based neural network (DSTMD) for\nsmall target detection in a cluttered background. In the proposed neural\nnetwork, a new correlation mechanism is introduced for direction selectivity\nvia correlating signals relayed from two pixels. Then, a lateral inhibition\nmechanism is implemented on the spatial field for size selectivity of STMD\nneurons. Extensive experiments showed that the proposed neural network not only\nis in accord with current biological findings, i.e. showing directional\npreferences, but also worked reliably in detecting small targets against\ncluttered backgrounds.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 15:11:07 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 22:12:33 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 22:08:56 GMT"}, {"version": "v4", "created": "Sat, 11 Aug 2018 16:51:54 GMT"}, {"version": "v5", "created": "Sat, 29 Sep 2018 10:37:55 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wang", "Hongxin", ""], ["Peng", "Jigen", ""], ["Yue", "Shigang", ""]]}, {"id": "1801.06694", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas and Mar\\'ia Elena Algorri", "title": "Determination of Digital Straight Segments Using the Slope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for the recognition of digital straight lines based\non the slope. This method combines the Freeman's chain coding scheme and new\ndiscovered properties of the digital slope introduced in this paper. We also\npresent the efficiency of our method from a testbed.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 16:08:12 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Cartas", "Alejandro", ""], ["Algorri", "Mar\u00eda Elena", ""]]}, {"id": "1801.06710", "submitter": "Anil Kumar Vadathya Mr", "authors": "Anil Kumar Vadathya, Saikiran Cholleti, Gautham Ramajayam,\n  Vijayalakshmi Kanchana, Kaushik Mitra", "title": "Learning Light Field Reconstruction from a Single Coded Image", "comments": "accepted at ACPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging is a rich way of representing the 3D world around us.\nHowever, due to limited sensor resolution capturing light field data inherently\nposes spatio-angular resolution trade-off. In this paper, we propose a deep\nlearning based solution to tackle the resolution trade-off. Specifically, we\nreconstruct full sensor resolution light field from a single coded image. We\npropose to do this in three stages 1) reconstruction of center view from the\ncoded image 2) estimating disparity map from the coded image and center view 3)\nwarping center view using the disparity to generate light field. We propose\nthree neural networks for these stages. Our disparity estimation network is\ntrained in an unsupervised manner alleviating the need for ground truth\ndisparity. Our results demonstrate better recovery of parallax from the coded\nimage. Also, we get better results than dictionary learning based approaches\nboth qualitatively and quatitatively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 18:17:19 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 18:06:58 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Vadathya", "Anil Kumar", ""], ["Cholleti", "Saikiran", ""], ["Ramajayam", "Gautham", ""], ["Kanchana", "Vijayalakshmi", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1801.06724", "submitter": "Eli Schwartz", "authors": "Eli Schwartz, Raja Giryes and Alex M. Bronstein", "title": "DeepISP: Towards Learning an End-to-End Image Processing Pipeline", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 28.2 (2019): 912-923", "doi": "10.1109/TIP.2018.2872858", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepISP, a full end-to-end deep neural model of the camera image\nsignal processing (ISP) pipeline. Our model learns a mapping from the raw\nlow-light mosaiced image to the final visually compelling image and encompasses\nlow-level tasks such as demosaicing and denoising as well as higher-level tasks\nsuch as color correction and image adjustment. The training and evaluation of\nthe pipeline were performed on a dedicated dataset containing pairs of\nlow-light and well-lit images captured by a Samsung S7 smartphone camera in\nboth raw and processed JPEG formats. The proposed solution achieves\nstate-of-the-art performance in objective evaluation of PSNR on the subtask of\njoint denoising and demosaicing. For the full end-to-end pipeline, it achieves\nbetter visual quality compared to the manufacturer ISP, in both a subjective\nhuman assessment and when rated by a deep model trained for assessing image\nquality.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 20:41:05 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 12:32:36 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Schwartz", "Eli", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1801.06729", "submitter": "Burak Uzkent", "authors": "Burak Uzkent, YoungWoo Seo", "title": "EnKCF: Ensemble of Kernelized Correlation Filters for High-Speed Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision technologies are very attractive for practical applications\nrunning on embedded systems. For such an application, it is desirable for the\ndeployed algorithms to run in high-speed and require no offline training. To\ndevelop a single-target tracking algorithm with these properties, we propose an\nensemble of the kernelized correlation filters (KCF), we call it EnKCF. A\ncommittee of KCFs is specifically designed to address the variations in scale\nand translation of moving objects. To guarantee a high-speed run-time\nperformance, we deploy each of KCFs in turn, instead of applying multiple KCFs\nto each frame. To minimize any potential drifts between individual KCFs\ntransition, we developed a particle filter. Experimental results showed that\nthe performance of ours is, on average, 70.10% for precision at 20 pixels,\n53.00% for success rate for the OTB100 data, and 54.50% and 40.2% for the\nUAV123 data. Experimental results showed that our method is better than other\nhigh-speed trackers over 5% on precision on 20 pixels and 10-20% on AUC on\naverage. Moreover, our implementation ran at 340 fps for the OTB100 and at 416\nfps for the UAV123 dataset that is faster than DCF (292 fps) for the OTB100 and\nKCF (292 fps) for the UAV123. To increase flexibility of the proposed EnKCF\nrunning on various platforms, we also explored different levels of deep\nconvolutional features.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 21:22:46 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Uzkent", "Burak", ""], ["Seo", "YoungWoo", ""]]}, {"id": "1801.06732", "submitter": "Yixuan Zhang", "authors": "Zhongping Zhang, Yixuan Zhang, Zheng Zhou, Jiebo Luo", "title": "Boundary-based Image Forgery Detection by Fast Shallow CNN", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image forgery detection is the task of detecting and localizing forged parts\nin tampered images. Previous works mostly focus on high resolution images using\ntraces of resampling features, demosaicing features or sharpness of edges.\nHowever, a good detection method should also be applicable to low resolution\nimages because compressed or resized images are common these days. To this end,\nwe propose a Shallow Convolutional Neural Network(SCNN), capable of\ndistinguishing the boundaries of forged regions from original edges in low\nresolution images. SCNN is designed to utilize the information of chroma and\nsaturation. Based on SCNN, two approaches that are named Sliding Windows\nDetection (SWD) and Fast SCNN, respectively, are developed to detect and\nlocalize image forgery region. In this paper, we substantiate that Fast SCNN\ncan detect drastic change of chroma and saturation. In image forgery detection\nexperiments Our model is evaluated on the CASIA 2.0 dataset. The results show\nthat Fast SCNN performs well on low resolution images and achieves significant\nimprovements over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 21:33:12 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 00:45:33 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Zhang", "Zhongping", ""], ["Zhang", "Yixuan", ""], ["Zhou", "Zheng", ""], ["Luo", "Jiebo", ""]]}, {"id": "1801.06734", "submitter": "Yixuan Zhang", "authors": "Zhengyuan Yang, Yixuan Zhang, Jerry Yu, Junjie Cai, Jiebo Luo", "title": "End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars\n  with Visual Perception", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been successfully applied to\nautonomous driving tasks, many in an end-to-end manner. Previous end-to-end\nsteering control methods take an image or an image sequence as the input and\ndirectly predict the steering angle with CNN. Although single task learning on\nsteering angles has reported good performances, the steering angle alone is not\nsufficient for vehicle control. In this work, we propose a multi-task learning\nframework to predict the steering angle and speed control simultaneously in an\nend-to-end manner. Since it is nontrivial to predict accurate speed values with\nonly visual inputs, we first propose a network to predict discrete speed\ncommands and steering angles with image sequences. Moreover, we propose a\nmulti-modal multi-task network to predict speed values and steering angles by\ntaking previous feedback speeds and visual recordings as inputs. Experiments\nare conducted on the public Udacity dataset and a newly collected SAIC dataset.\nResults show that the proposed model predicts steering angles and speed values\naccurately. Furthermore, we improve the failure data synthesis methods to solve\nthe problem of error accumulation in real road tests.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 21:59:08 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 22:13:57 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Zhang", "Yixuan", ""], ["Yu", "Jerry", ""], ["Cai", "Junjie", ""], ["Luo", "Jiebo", ""]]}, {"id": "1801.06742", "submitter": "Yan Huang", "authors": "Yan Huang, Jinsong Xu, Qiang Wu, Zhedong Zheng, Zhaoxiang Zhang, Jian\n  Zhang", "title": "Multi-pseudo Regularized Label for Generated Data in Person\n  Re-Identification", "comments": "To appear on IEEE Transaction on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2874715", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient training data normally is required to train deeply learned models.\nHowever, due to the expensive manual process for labelling large number of\nimages, the amount of available training data is always limited. To produce\nmore data for training a deep network, Generative Adversarial Network (GAN) can\nbe used to generate artificial sample data. However, the generated data usually\ndoes not have annotation labels. To solve this problem, in this paper, we\npropose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign\nit to the generated data. With MpRL, the generated data will be used as the\nsupplementary of real training data to train a deep neural network in a\nsemi-supervised learning fashion. To build the corresponding relationship\nbetween the real data and generated data, MpRL assigns each generated data a\nproper virtual label which reflects the likelihood of the affiliation of the\ngenerated data to pre-defined training classes in the real data domain. Unlike\nthe traditional label which usually is a single integral number, the virtual\nlabel proposed in this work is a set of weight-based values each individual of\nwhich is a number in (0,1] called multi-pseudo label and reflects the degree of\nrelation between each generated data to every pre-defined class of real data. A\ncomprehensive evaluation is carried out by adopting two state-of-the-art\nconvolutional neural networks (CNNs) in our experiments to verify the\neffectiveness of MpRL. Experiments demonstrate that by assigning MpRL to\ngenerated data, we can further improve the person re-ID performance on five\nre-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01.\nThe proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48%\nimprovements in rank-1 accuracy over a strong CNN baseline on the five datasets\nrespectively, and outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 00:20:30 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 09:17:01 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 05:52:12 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Huang", "Yan", ""], ["Xu", "Jinsong", ""], ["Wu", "Qiang", ""], ["Zheng", "Zhedong", ""], ["Zhang", "Zhaoxiang", ""], ["Zhang", "Jian", ""]]}, {"id": "1801.06756", "submitter": "Peiyao Wang", "authors": "Weisheng Dong, Peiyao Wang, Wotao Yin, Guangming Shi, Fangfang Wu,\n  Xiaotong Lu", "title": "Denoising Prior Driven Deep Neural Network for Image Restoration", "comments": "14page, 11 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2873610", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown very promising results for various\nimage restoration (IR) tasks. However, the design of network architectures\nremains a major challenging for achieving further improvements. While most\nexisting DNN-based methods solve the IR problems by directly mapping low\nquality images to desirable high-quality images, the observation models\ncharacterizing the image degradation processes have been largely ignored. In\nthis paper, we first propose a denoising-based IR algorithm, whose iterative\nsteps can be computed efficiently. Then, the iterative process is unfolded into\na deep neural network, which is composed of multiple denoisers modules\ninterleaved with back-projection (BP) modules that ensure the observation\nconsistencies. A convolutional neural network (CNN) based denoiser that can\nexploit the multi-scale redundancies of natural images is proposed. As such,\nthe proposed network not only exploits the powerful denoising ability of DNNs,\nbut also leverages the prior of the observation model. Through end-to-end\ntraining, both the denoisers and the BP modules can be jointly optimized.\nExperimental results on several IR tasks, e.g., image denoisig,\nsuper-resolution and deblurring show that the proposed method can lead to very\ncompetitive and often state-of-the-art results on several IR tasks, including\nimage denoising, deblurring and super-resolution.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 03:08:11 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 07:35:32 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Dong", "Weisheng", ""], ["Wang", "Peiyao", ""], ["Yin", "Wotao", ""], ["Shi", "Guangming", ""], ["Wu", "Fangfang", ""], ["Lu", "Xiaotong", ""]]}, {"id": "1801.06761", "submitter": "Lequan Yu", "authors": "Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng", "title": "PU-Net: Point Cloud Upsampling Network", "comments": "accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and analyzing 3D point clouds with deep networks is challenging due\nto the sparseness and irregularity of the data. In this paper, we present a\ndata-driven point cloud upsampling technique. The key idea is to learn\nmulti-level features per point and expand the point set via a multi-branch\nconvolution unit implicitly in feature space. The expanded feature is then\nsplit to a multitude of features, which are then reconstructed to an upsampled\npoint set. Our network is applied at a patch-level, with a joint loss function\nthat encourages the upsampled points to remain on the underlying surface with a\nuniform distribution. We conduct various experiments using synthesis and scan\ndata to evaluate our method and demonstrate its superiority over some baseline\nmethods and an optimization-based method. Results show that our upsampled\npoints have better uniformity and are located closer to the underlying\nsurfaces.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 04:10:52 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 06:20:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yu", "Lequan", ""], ["Li", "Xianzhi", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1801.06769", "submitter": "Liang Shen", "authors": "Liang Shen, Zihan Yue, Quan Chen, Fan Feng and Jie Ma", "title": "Deep joint rain and haze removal from single images", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain removal from a single image is a challenge which has been studied for a\nlong time. In this paper, a novel convolutional neural network based on wavelet\nand dark channel is proposed. On one hand, we think that rain streaks\ncorrespond to high frequency component of the image. Therefore, haar wavelet\ntransform is a good choice to separate the rain streaks and background to some\nextent. More specifically, the LL subband of a rain image is more inclined to\nexpress the background information, while LH, HL, HH subband tend to represent\nthe rain streaks and the edges. On the other hand, the accumulation of rain\nstreaks from long distance makes the rain image look like haze veil. We extract\ndark channel of rain image as a feature map in network. By increasing this\nmapping between the dark channel of input and output images, we achieve haze\nremoval in an indirect way. All of the parameters are optimized by\nback-propagation. Experiments on both synthetic and real- world datasets reveal\nthat our method outperforms other state-of- the-art methods from a qualitative\nand quantitative perspective.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 05:09:58 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Shen", "Liang", ""], ["Yue", "Zihan", ""], ["Chen", "Quan", ""], ["Feng", "Fan", ""], ["Ma", "Jie", ""]]}, {"id": "1801.06790", "submitter": "Zhifei Zhang", "authors": "Zhifei Zhang, Yang Song, Hairong Qi", "title": "Decoupled Learning for Conditional Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating encoding-decoding nets with adversarial nets has been widely\nadopted in image generation tasks. We observe that the state-of-the-art\nachievements were obtained by carefully balancing the reconstruction loss and\nadversarial loss, and such balance shifts with different network structures,\ndatasets, and training strategies. Empirical studies have demonstrated that an\ninappropriate weight between the two losses may cause instability, and it is\ntricky to search for the optimal setting, especially when lacking prior\nknowledge on the data and network.\n  This paper gives the first attempt to relax the need of manual balancing by\nproposing the concept of \\textit{decoupled learning}, where a novel network\nstructure is designed that explicitly disentangles the backpropagation paths of\nthe two losses.\n  Experimental results demonstrate the effectiveness, robustness, and\ngenerality of the proposed method. The other contribution of the paper is the\ndesign of a new evaluation metric to measure the image quality of generative\nmodels. We propose the so-called \\textit{normalized relative discriminative\nscore} (NRDS), which introduces the idea of relative comparison, rather than\nproviding absolute estimates like existing metrics.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 08:48:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Zhang", "Zhifei", ""], ["Song", "Yang", ""], ["Qi", "Hairong", ""]]}, {"id": "1801.06797", "submitter": "Luis Herranz", "authors": "Xinhang Song, Luis Herranz, Shuqiang Jiang", "title": "Depth CNNs for RGB-D scene recognition: learning from scratch better\n  than transferring from RGB-CNNs", "comments": "AAAI Conference on Artificial Intelligence 2017", "journal-ref": "AAAI Conference on Artificial Intelligence 2017, 4271-4277", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene recognition with RGB images has been extensively studied and has\nreached very remarkable recognition levels, thanks to convolutional neural\nnetworks (CNN) and large scene datasets. In contrast, current RGB-D scene data\nis much more limited, so often leverages RGB large datasets, by transferring\npretrained RGB CNN models and fine-tuning with the target RGB-D dataset.\nHowever, we show that this approach has the limitation of hardly reaching\nbottom layers, which is key to learn modality-specific features. In contrast,\nwe focus on the bottom layers, and propose an alternative strategy to learn\ndepth features combining local weakly supervised training from patches followed\nby global fine tuning with images. This strategy is capable of learning very\ndiscriminative depth-specific features with limited depth images, without\nresorting to Places-CNN. In addition we propose a modified CNN architecture to\nfurther match the complexity of the model and the amount of data available. For\nRGB-D scene recognition, depth and RGB features are combined by projecting them\nin a common space and further leaning a multilayer classifier, which is jointly\noptimized in an end-to-end network. Our framework achieves state-of-the-art\naccuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 09:38:50 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Song", "Xinhang", ""], ["Herranz", "Luis", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1801.06801", "submitter": "Huan Long", "authors": "Tao Yu, Huan Long, John E. Hopcroft", "title": "Curvature-based Comparison of Two Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we show the similarities and differences of two deep neural\nnetworks by comparing the manifolds composed of activation vectors in each\nfully connected layer of them. The main contribution of this paper includes 1)\na new data generating algorithm which is crucial for determining the dimension\nof manifolds; 2) a systematic strategy to compare manifolds. Especially, we\ntake Riemann curvature and sectional curvature as part of criterion, which can\nreflect the intrinsic geometric properties of manifolds. Some interesting\nresults and phenomenon are given, which help in specifying the similarities and\ndifferences between the features extracted by two networks and demystifying the\nintrinsic mechanism of deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 10:17:33 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Yu", "Tao", ""], ["Long", "Huan", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1801.06831", "submitter": "Heng Fan", "authors": "Heng Fan, Haibin Ling", "title": "Dense Recurrent Neural Networks for Scene Labeling", "comments": "Tech. Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently recurrent neural networks (RNNs) have demonstrated the ability to\nimprove scene labeling through capturing long-range dependencies among image\nunits. In this paper, we propose dense RNNs for scene labeling by exploring\nvarious long-range semantic dependencies among image units. In comparison with\nexisting RNN based approaches, our dense RNNs are able to capture richer\ncontextual dependencies for each image unit via dense connections between each\npair of image units, which significantly enhances their discriminative power.\nBesides, to select relevant and meanwhile restrain irrelevant dependencies for\neach unit from dense connections, we introduce an attention model into dense\nRNNs. The attention model enables automatically assigning more importance to\nhelpful dependencies while less weight to unconcerned dependencies. Integrating\nwith convolutional neural networks (CNNs), our method achieves state-of-the-art\nperformances on the PASCAL Context, MIT ADE20K and SiftFlow benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 14:43:22 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Fan", "Heng", ""], ["Ling", "Haibin", ""]]}, {"id": "1801.06867", "submitter": "Luis Herranz", "authors": "Luis Herranz, Shuqiang Jiang, Xiangyang Li", "title": "Scene recognition with CNNs: objects, scales and dataset bias", "comments": "CVPR 2016", "journal-ref": "L. Herranz, S. Jiang, X. Li, \"Scene recognition with CNNs:\n  objects, scales and dataset bias\", Proc. International Conference on Computer\n  Vision and Pattern Recognition (CVPR16), Las Vegas, Nevada, USA, June 2016", "doi": "10.1109/CVPR.2016.68", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since scenes are composed in part of objects, accurate recognition of scenes\nrequires knowledge about both scenes and objects. In this paper we address two\nrelated problems: 1) scale induced dataset bias in multi-scale convolutional\nneural network (CNN) architectures, and 2) how to combine effectively\nscene-centric and object-centric knowledge (i.e. Places and ImageNet) in CNNs.\nAn earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help\nmuch. Here we propose an alternative method taking the scale into account,\nresulting in significant recognition gains. By analyzing the response of\nImageNet-CNNs and Places-CNNs at different scales we find that both operate in\ndifferent scale ranges, so using the same network for all the scales induces\ndataset bias resulting in limited performance. Thus, adapting the feature\nextractor to each particular scale (i.e. scale-specific CNNs) is crucial to\nimprove recognition, since the objects in the scenes have their specific range\nof scales. Experimental results show that the recognition accuracy highly\ndepends on the scale, and that simple yet carefully chosen multi-scale\ncombinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art\nrecognition accuracy in SUN397 up to 66.26% (and even 70.17% with deeper\narchitectures, comparable to human performance).\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 18:18:47 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Herranz", "Luis", ""], ["Jiang", "Shuqiang", ""], ["Li", "Xiangyang", ""]]}, {"id": "1801.06879", "submitter": "Yinhao Zhu", "authors": "Yinhao Zhu, Nicholas Zabaras", "title": "Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate\n  Modeling and Uncertainty Quantification", "comments": "52 pages, 28 figures, submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2018.04.018", "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the development of surrogate models for uncertainty\nquantification and propagation in problems governed by stochastic PDEs using a\ndeep convolutional encoder-decoder network in a similar fashion to approaches\nconsidered in deep learning for image-to-image regression tasks. Since normal\nneural networks are data intensive and cannot provide predictive uncertainty,\nwe propose a Bayesian approach to convolutional neural nets. A recently\nintroduced variational gradient descent algorithm based on Stein's method is\nscaled to deep convolutional networks to perform approximate Bayesian inference\non millions of uncertain network parameters. This approach achieves state of\nthe art performance in terms of predictive accuracy and uncertainty\nquantification in comparison to other approaches in Bayesian neural networks as\nwell as techniques that include Gaussian processes and ensemble methods even\nwhen the training data size is relatively small. To evaluate the performance of\nthis approach, we consider standard uncertainty quantification benchmark\nproblems including flow in heterogeneous media defined in terms of limited\ndata-driven permeability realizations. The performance of the surrogate model\ndeveloped is very good even though there is no underlying structure shared\nbetween the input (permeability) and output (flow/pressure) fields as is often\nthe case in the image-to-image regression models used in computer vision\nproblems. Studies are performed with an underlying stochastic input\ndimensionality up to $4,225$ where most other uncertainty quantification\nmethods fail. Uncertainty propagation tasks are considered and the predictive\noutput Bayesian statistics are compared to those obtained with Monte Carlo\nestimates.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 19:18:13 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhu", "Yinhao", ""], ["Zabaras", "Nicholas", ""]]}, {"id": "1801.06940", "submitter": "Qianye Yang", "authors": "Qianye Yang, Nannan Li, Zixu Zhao, Xingyu Fan, Eric I-Chao Chang, Yan\n  Xu", "title": "MRI Cross-Modality NeuroImage-to-NeuroImage Translation", "comments": "46 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a cross-modality generation framework that learns to generate\ntranslated modalities from given modalities in MR images without real\nacquisition. Our proposed method performs NeuroImage-to-NeuroImage translation\n(abbreviated as N2N) by means of a deep learning model that leverages\nconditional generative adversarial networks (cGANs). Our framework jointly\nexploits the low-level features (pixel-wise information) and high-level\nrepresentations (e.g. brain tumors, brain structure like gray matter, etc.)\nbetween cross modalities which are important for resolving the challenging\ncomplexity in brain structures. Our framework can serve as an auxiliary method\nin clinical diagnosis and has great application potential. Based on our\nproposed framework, we first propose a method for cross-modality registration\nby fusing the deformation fields to adopt the cross-modality information from\ntranslated modalities. Second, we propose an approach for MRI segmentation,\ntranslated multichannel segmentation (TMS), where given modalities, along with\ntranslated modalities, are segmented by fully convolutional networks (FCN) in a\nmultichannel manner. Both of these two methods successfully adopt the\ncross-modality information to improve the performance without adding any extra\ndata. Experiments demonstrate that our proposed framework advances the\nstate-of-the-art on five brain MRI datasets. We also observe encouraging\nresults in cross-modality registration and segmentation on some widely adopted\nbrain datasets. Overall, our work can serve as an auxiliary method in clinical\ndiagnosis and be applied to various tasks in medical fields.\n  Keywords: image-to-image, cross-modality, registration, segmentation, brain\nMRI\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 02:53:28 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 07:17:13 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Yang", "Qianye", ""], ["Li", "Nannan", ""], ["Zhao", "Zixu", ""], ["Fan", "Xingyu", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""]]}, {"id": "1801.06976", "submitter": "Hongxin Wang", "authors": "Hongxin Wang, Jigen Peng and Shigang Yue", "title": "An Improved LPTC Neural Model for Background Motion Direction Estimation", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of specialized neurons, called lobula plate tangential cells (LPTCs)\nhas been shown to respond strongly to wide-field motion. The classic model,\nelementary motion detector (EMD) and its improved model, two-quadrant detector\n(TQD) have been proposed to simulate LPTCs. Although EMD and TQD can percept\nbackground motion, their outputs are so cluttered that it is difficult to\ndiscriminate actual motion direction of the background. In this paper, we\npropose a max operation mechanism to model a newly-found transmedullary neuron\nTm9 whose physiological properties do not map onto EMD and TQD. This proposed\nmax operation mechanism is able to improve the detection performance of TQD in\ncluttered background by filtering out irrelevant motion signals. We will\ndemonstrate the functionality of this proposed mechanism in wide-field motion\nperception.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 06:57:05 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Wang", "Hongxin", ""], ["Peng", "Jigen", ""], ["Yue", "Shigang", ""]]}, {"id": "1801.07080", "submitter": "Muktabh Mayank Srivastava", "authors": "Sonaal Kant, Muktabh Mayank Srivastava", "title": "Towards Automated Tuberculosis detection using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tuberculosis(TB) in India is the world's largest TB epidemic. TB leads to\n480,000 deaths every year. Between the years 2006 and 2014, Indian economy lost\nUS$340 Billion due to TB. This combined with the emergence of drug resistant\nbacteria in India makes the problem worse. The government of India has hence\ncome up with a new strategy which requires a high-sensitivity microscopy based\nTB diagnosis mechanism. We propose a new Deep Neural Network based drug\nsensitive TB detection methodology with recall and precision of 83.78% and\n67.55% respectively for bacillus detection. This method takes a microscopy\nimage with proper zoom level as input and returns location of suspected TB\ngerms as output. The high accuracy of our method gives it the potential to\nevolve into a high sensitivity system to diagnose TB when trained at scale.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 13:21:06 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Kant", "Sonaal", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1801.07110", "submitter": "Alessandro Betti", "authors": "Alessandro Betti and Marco Gori", "title": "Convolutional Networks in Visual Environments", "comments": "49 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The puzzle of computer vision might find new challenging solutions when we\nrealize that most successful methods are working at image level, which is\nremarkably more difficult than processing directly visual streams. In this\npaper, we claim that their processing naturally leads to formulate the motion\ninvariance principle, which enables the construction of a new theory of\nlearning with convolutional networks. The theory addresses a number of\nintriguing questions that arise in natural vision, and offers a well-posed\ncomputational scheme for the discovery of convolutional filters over the\nretina. They are driven by differential equations derived from the principle of\nleast cognitive action. Unlike traditional convolutional networks, which need\nmassive supervision, the proposed theory offers a truly new scenario in which\nfeature learning takes place by unsupervised processing of video signals. It is\npointed out that an opportune blurring of the video, along the interleaving of\nsegments of null signal, make it possible to conceive a novel learning\nmechanism that yields the minimum of the cognitive action. Basically, while the\ntheory enables the implementation of novel computer vision systems, it is also\nprovides an intriguing explanation of the solution that evolution has\ndiscovered for humans, where it looks like that the video blurring in newborns\nand the day-night rhythm seem to emerge in a general computational framework,\nregardless of biology.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 21:35:40 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""]]}, {"id": "1801.07141", "submitter": "Ayan Kumar Bhunia", "authors": "Aishik Konwer, Ayan Kumar Bhunia, Abir Bhowmick, Ankan Kumar Bhunia,\n  Prithaj Banerjee, Partha Pratim Roy, Umapada Pal", "title": "Staff line Removal using Generative Adversarial Networks", "comments": "To be appeared in ICPR 2018, 2018 International Conference on Pattern\n  Recognition(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Staff line removal is a crucial pre-processing step in Optical Music\nRecognition. It is a challenging task to simultaneously reduce the noise and\nalso retain the quality of music symbol context in ancient degraded music score\nimages. In this paper we propose a novel approach for staff line removal, based\non Generative Adversarial Networks. We convert staff line images into patches\nand feed them into a U-Net, used as Generator. The Generator intends to produce\nstaff-less images at the output. Then the Discriminator does binary\nclassification and differentiates between the generated fake staff-less image\nand real ground truth staff less image. For training, we use a Loss function\nwhich is a weighted combination of L2 loss and Adversarial loss. L2 loss\nminimizes the difference between real and fake staff-less image. Adversarial\nloss helps to retrieve more high quality textures in generated images. Thus our\narchitecture supports solutions which are closer to ground truth and it\nreflects in our results. For evaluation we consider the ICDAR/GREC 2013 staff\nremoval database. Our method achieves superior performance in comparison to\nother conventional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:35:51 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 02:41:42 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 18:29:31 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Konwer", "Aishik", ""], ["Bhunia", "Ayan Kumar", ""], ["Bhowmick", "Abir", ""], ["Bhunia", "Ankan Kumar", ""], ["Banerjee", "Prithaj", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1801.07145", "submitter": "Eric Alcaide", "authors": "Eric Alcaide", "title": "E-swish: Adjusting Activations to Different Network Depths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions have a notorious impact on neural networks on both\ntraining and testing the models against the desired problem. Currently, the\nmost used activation function is the Rectified Linear Unit (ReLU). This paper\nintroduces a new and novel activation function, closely related with the new\nactivation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which\ngeneralizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We\nshow that E-swish outperforms many other well-known activations including both\nReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy\nimprovements on Cifar10 and Cifar100 respectively for the WRN 10-2 when\ncompared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The\ncode to reproduce all our experiments can be found at\nhttps://github.com/EricAlcaide/E-swish\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:40:29 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Alcaide", "Eric", ""]]}, {"id": "1801.07156", "submitter": "Ayan Kumar Bhunia", "authors": "Ankan Kumar Bhunia, Ayan Kumar Bhunia, Prithaj Banerjee, Aishik\n  Konwer, Abir Bhowmick, Partha Pratim Roy, Umapada Pal", "title": "Word Level Font-to-Font Image Translation using Convolutional Recurrent\n  Generative Adversarial Networks", "comments": "To be appeared in ICPR 2018, 2018 International Conference on Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversion of one font to another font is very useful in real life\napplications. In this paper, we propose a Convolutional Recurrent Generative\nmodel to solve the word level font transfer problem. Our network is able to\nconvert the font style of any printed text images from its current font to the\nrequired font. The network is trained end-to-end for the complete word images.\nThus it eliminates the necessary pre-processing steps, like character\nsegmentations. We extend our model to conditional setting that helps to learn\none-to-many mapping function. We employ a novel convolutional recurrent model\narchitecture in the Generator that efficiently deals with the word images of\narbitrary width. It also helps to maintain the consistency of the final images\nafter concatenating the generated image patches of target font. Besides, the\nGenerator and the Discriminator network, we employ a Classification network to\nclassify the generated word images of converted font style to their subsequent\nfont categories. Most of the earlier works related to image translation are\nperformed on square images. Our proposed architecture is the first work which\ncan handle images of varying widths. Word images generally have varying width\ndepending on the number of characters present. Hence, we test our model on a\nsynthetically generated font dataset. We compare our method with some of the\nstate-of-the-art methods for image translation. The superior performance of our\nnetwork on the same dataset proves the ability of our model to learn the font\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:58:20 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 02:40:20 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 01:45:56 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Bhunia", "Ankan Kumar", ""], ["Bhunia", "Ayan Kumar", ""], ["Banerjee", "Prithaj", ""], ["Konwer", "Aishik", ""], ["Bhowmick", "Abir", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1801.07198", "submitter": "Chichen Fu", "authors": "Chichen Fu and Soonam Lee and David Joon Ho and Shuo Han and Paul\n  Salama and Kenneth W. Dunn and Edward J. Delp", "title": "Three Dimensional Fluorescence Microscopy Image Synthesis and\n  Segmentation", "comments": "Accepted by CVPR Workshop on Computer Vision for Microscopy Image\n  Analysis (CVMI)", "journal-ref": null, "doi": "10.1109/CVPRW.2018.00298", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in fluorescence microscopy enable acquisition of 3D image volumes\nwith better image quality and deeper penetration into tissue. Segmentation is a\nrequired step to characterize and analyze biological structures in the images\nand recent 3D segmentation using deep learning has achieved promising results.\nOne issue is that deep learning techniques require a large set of groundtruth\ndata which is impractical to annotate manually for large 3D microscopy volumes.\nThis paper describes a 3D deep learning nuclei segmentation method using\nsynthetic 3D volumes for training. A set of synthetic volumes and the\ncorresponding groundtruth are generated using spatially constrained\ncycle-consistent adversarial networks. Segmentation results demonstrate that\nour proposed method is capable of segmenting nuclei successfully for various\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 17:08:13 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2018 03:46:50 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fu", "Chichen", ""], ["Lee", "Soonam", ""], ["Ho", "David Joon", ""], ["Han", "Shuo", ""], ["Salama", "Paul", ""], ["Dunn", "Kenneth W.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1801.07211", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Abir Bhowmick, Ankan Kumar Bhunia, Aishik Konwer,\n  Prithaj Banerjee, Partha Pratim Roy, Umapada Pal", "title": "Handwriting Trajectory Recovery using End-to-End Deep Encoder-Decoder\n  Network", "comments": "To be appeared in ICPR 2018, 2018 International Conference on Pattern\n  Recognition, Code Link:\n  https://drive.google.com/file/d/1clT-UuXgPp6uFn1tmIXx481qvPUcY0fV/view", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel technique to recover the pen trajectory\nof offline characters which is a crucial step for handwritten character\nrecognition. Generally, online acquisition approach has more advantage than its\noffline counterpart as the online technique keeps track of the pen movement.\nHence, pen tip trajectory retrieval from offline text can bridge the gap\nbetween online and offline methods. Our proposed framework employs sequence to\nsequence model which consists of an encoder-decoder LSTM module. Our encoder\nmodule consists of Convolutional LSTM network, which takes an offline character\nimage as the input and encodes the feature sequence to a hidden representation.\nThe output of the encoder is fed to a decoder LSTM and we get the successive\ncoordinate points from every time step of the decoder LSTM. Although the\nsequence to sequence model is a popular paradigm in various computer vision and\nlanguage translation tasks, the main contribution of our work lies in designing\nan end-to-end network for a decade old popular problem in Document Image\nAnalysis community. Tamil, Telugu and Devanagari characters of LIPI Toolkit\ndataset are used for our experiments. Our proposed method has achieved superior\nperformance compared to the other conventional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 17:25:05 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 02:39:07 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 01:33:51 GMT"}, {"version": "v4", "created": "Sun, 3 Jun 2018 14:00:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Bhowmick", "Abir", ""], ["Bhunia", "Ankan Kumar", ""], ["Konwer", "Aishik", ""], ["Banerjee", "Prithaj", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1801.07230", "submitter": "Unaiza Ahsan", "authors": "Unaiza Ahsan, Chen Sun, Irfan Essa", "title": "DiscrimNet: Semi-Supervised Action Recognition from Videos using\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an action recognition framework using Gen- erative Adversarial\nNetworks. Our model involves train- ing a deep convolutional generative\nadversarial network (DCGAN) using a large video activity dataset without la-\nbel information. Then we use the trained discriminator from the GAN model as an\nunsupervised pre-training step and fine-tune the trained discriminator model on\na labeled dataset to recognize human activities. We determine good network\narchitectural and hyperparameter settings for us- ing the discriminator from\nDCGAN as a trained model to learn useful representations for action\nrecognition. Our semi-supervised framework using only appearance infor- mation\nachieves superior or comparable performance to the current state-of-the-art\nsemi-supervised action recog- nition methods on two challenging video activity\ndatasets: UCF101 and HMDB51.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:26:14 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Ahsan", "Unaiza", ""], ["Sun", "Chen", ""], ["Essa", "Irfan", ""]]}, {"id": "1801.07239", "submitter": "Luis Herranz", "authors": "Luis Herranz, Weiqing Min, Shuqiang Jiang", "title": "Food recognition and recipe analysis: integrating visual content,\n  context and external knowledge", "comments": "Survey about contextual food recognition and multimodal recipe\n  analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central role of food in our individual and social life, combined with\nrecent technological advances, has motivated a growing interest in applications\nthat help to better monitor dietary habits as well as the exploration and\nretrieval of food-related information. We review how visual content, context\nand external knowledge can be integrated effectively into food-oriented\napplications, with special focus on recipe analysis and retrieval, food\nrecommendation, and the restaurant context as emerging directions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:45:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Herranz", "Luis", ""], ["Min", "Weiqing", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1801.07339", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Wentong Liao, Xinbo Li, Bodo Rosenhahn", "title": "Vehicle Detection in Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of vehicles in aerial images is widely applied in many\napplications. Comparing with object detection in the ground view images,\nvehicle detection in aerial images remains a challenging problem because of\nsmall vehicle size, monotone appearance and the complex background. In this\npaper, we propose a novel double focal loss convolutional neural network\nframework (DFL-CNN). In the proposed framework, the skip connection is used in\nthe CNN structure to enhance the feature learning. Also, the focal loss\nfunction is used to substitute for conventional cross entropy loss function in\nboth of the region proposed network and the final classifier. We further\nintroduce the first large-scale vehicle detection dataset ITCVD with ground\ntruth annotations for all the vehicles in the scene. We demonstrate the\nperformance of our model on the existing benchmark DLR 3K dataset as well as\nthe ITCVD dataset. The experimental results show that our DFL-CNN outperforms\nthe baselines on vehicle detection.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 22:08:28 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 13:07:43 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yang", "Michael Ying", ""], ["Liao", "Wentong", ""], ["Li", "Xinbo", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1801.07365", "submitter": "Qiangui Huang", "authors": "Qiangui Huang, Kevin Zhou, Suya You, Ulrich Neumann", "title": "Learning to Prune Filters in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art computer vision algorithms use large scale\nconvolutional neural networks (CNNs) as basic building blocks. These CNNs are\nknown for their huge number of parameters, high redundancy in weights, and\ntremendous computing resource consumptions. This paper presents a learning\nalgorithm to simplify and speed up these CNNs. Specifically, we introduce a\n\"try-and-learn\" algorithm to train pruning agents that remove unnecessary CNN\nfilters in a data-driven way. With the help of a novel reward function, our\nagents removes a significant number of filters in CNNs while maintaining\nperformance at a desired level. Moreover, this method provides an easy control\nof the tradeoff between network performance and its scale. Per- formance of our\nalgorithm is validated with comprehensive pruning experiments on several\npopular CNNs for visual recognition and semantic segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 01:30:34 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Huang", "Qiangui", ""], ["Zhou", "Kevin", ""], ["You", "Suya", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1801.07372", "submitter": "Aiden Nibali", "authors": "Aiden Nibali, Zhen He, Stuart Morgan, Luke Prendergast", "title": "Numerical Coordinate Regression with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deep learning approaches to inferring numerical coordinates for\npoints of interest in an input image. Existing convolutional neural\nnetwork-based solutions to this problem either take a heatmap matching approach\nor regress to coordinates with a fully connected output layer. Neither of these\napproaches is ideal, since the former is not entirely differentiable, and the\nlatter lacks inherent spatial generalization. We propose our differentiable\nspatial to numerical transform (DSNT) to fill this gap. The DSNT layer adds no\ntrainable parameters, is fully differentiable, and exhibits good spatial\ngeneralization. Unlike heatmap matching, DSNT works well with low heatmap\nresolutions, so it can be dropped in as an output layer for a wide range of\nexisting fully convolutional architectures. Consequently, DSNT offers a better\ntrade-off between inference speed and prediction accuracy compared to existing\ntechniques. When used to replace the popular heatmap matching approach used in\nalmost all state-of-the-art methods for pose estimation, DSNT gives better\nprediction accuracy for all model architectures tested.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 02:18:04 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 22:19:39 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Nibali", "Aiden", ""], ["He", "Zhen", ""], ["Morgan", "Stuart", ""], ["Prendergast", "Luke", ""]]}, {"id": "1801.07388", "submitter": "Daniel Castro Chin", "authors": "Daniel Castro, Steven Hickson, Patsorn Sangkloy, Bhavishya Mittal,\n  Sean Dai, James Hays, Irfan Essa", "title": "Let's Dance: Learning From Online Dance Videos", "comments": "first submitted November 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, deep neural network approaches have naturally extended to\nthe video domain, in their simplest case by aggregating per-frame\nclassifications as a baseline for action recognition. A majority of the work in\nthis area extends from the imaging domain, leading to visual-feature heavy\napproaches on temporal data. To address this issue we introduce \"Let's Dance\",\na 1000 video dataset (and growing) comprised of 10 visually overlapping dance\ncategories that require motion for their classification. We stress the\nimportant of human motion as a key distinguisher in our work given that, as we\nshow in this work, visual information is not sufficient to classify\nmotion-heavy categories. We compare our datasets' performance using imaging\ntechniques with UCF-101 and demonstrate this inherent difficulty. We present a\ncomparison of numerous state-of-the-art techniques on our dataset using three\ndifferent representations (video, optical flow and multi-person pose data) in\norder to analyze these approaches. We discuss the motion parameterization of\neach of them and their value in learning to categorize online dance videos.\nLastly, we release this dataset (and its three representations) for the\nresearch community to use.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 03:48:17 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Castro", "Daniel", ""], ["Hickson", "Steven", ""], ["Sangkloy", "Patsorn", ""], ["Mittal", "Bhavishya", ""], ["Dai", "Sean", ""], ["Hays", "James", ""], ["Essa", "Irfan", ""]]}, {"id": "1801.07424", "submitter": "Wenguan Wang", "authors": "Wenguan Wang and Jianbing Shen and Fang Guo and Ming-Ming Cheng and\n  Ali Borji", "title": "Revisiting Video Saliency: A Large-scale Benchmark and a New Model", "comments": "CVPR2018 paper. Website: https://github.com/wenguanwang/DHF1K We have\n  corrected some statistics of our results (baseline training setting (iii)) on\n  UCF sports dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we contribute to video saliency research in two ways. First, we\nintroduce a new benchmark for predicting human eye movements during dynamic\nscene free-viewing, which is long-time urged in this field. Our dataset, named\nDHF1K (Dynamic Human Fixation), consists of 1K high-quality, elaborately\nselected video sequences spanning a large range of scenes, motions, object\ntypes and background complexity. Existing video saliency datasets lack variety\nand generality of common dynamic scenes and fall short in covering challenging\nsituations in unconstrained environments. In contrast, DHF1K makes a\nsignificant leap in terms of scalability, diversity and difficulty, and is\nexpected to boost video saliency modeling. Second, we propose a novel video\nsaliency model that augments the CNN-LSTM network architecture with an\nattention mechanism to enable fast, end-to-end saliency learning. The attention\nmechanism explicitly encodes static saliency information, thus allowing LSTM to\nfocus on learning more flexible temporal saliency representation across\nsuccessive frames. Such a design fully leverages existing large-scale static\nfixation datasets, avoids overfitting, and significantly improves training\nefficiency and testing performance. We thoroughly examine the performance of\nour model, with respect to state-of-the-art saliency models, on three\nlarge-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental\nresults over more than 1.2K testing videos containing 400K frames demonstrate\nthat our model outperforms other competitors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 08:01:50 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 22:35:33 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 05:07:41 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""], ["Guo", "Fang", ""], ["Cheng", "Ming-Ming", ""], ["Borji", "Ali", ""]]}, {"id": "1801.07451", "submitter": "Korsuk Sirinukunwattana", "authors": "Korsuk Sirinukunwattana, David Snead, David Epstein, Zia Aftab, Imaad\n  Mujeeb, Yee Wah Tsang, Ian Cree, Nasir Rajpoot", "title": "Novel digital tissue phenotypic signatures of distant metastasis in\n  colorectal cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant metastasis is the major cause of death in colorectal cancer (CRC).\nPatients at high risk of developing distant metastasis could benefit from\nappropriate adjuvant and follow-up treatments if stratified accurately at an\nearly stage of the disease. Studies have increasingly recognized the role of\ndiverse cellular components within the tumor microenvironment in the\ndevelopment and progression of CRC tumors. In this paper, we show that a new\nmethod of automated analysis of digitized images from colorectal cancer tissue\nslides can provide important estimates of distant metastasis-free survival\n(DMFS, the time before metastasis is first observed) on the basis of details of\nthe microenvironment. Specifically, we determine what cell types are found in\nthe vicinity of other cell types, and in what numbers, rather than\nconcentrating exclusively on the cancerous cells. We then extract novel tissue\nphenotypic signatures using statistical measurements about tissue composition.\nSuch signatures can underpin clinical decisions about the advisability of\nvarious types of adjuvant therapy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 09:30:23 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Sirinukunwattana", "Korsuk", ""], ["Snead", "David", ""], ["Epstein", "David", ""], ["Aftab", "Zia", ""], ["Mujeeb", "Imaad", ""], ["Tsang", "Yee Wah", ""], ["Cree", "Ian", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1801.07455", "submitter": "Sijie Yan", "authors": "Sijie Yan, Yuanjun Xiong and Dahua Lin", "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action\n  Recognition", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamics of human body skeletons convey significant information for human\naction recognition. Conventional approaches for modeling skeletons usually rely\non hand-crafted parts or traversal rules, thus resulting in limited expressive\npower and difficulties of generalization. In this work, we propose a novel\nmodel of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks\n(ST-GCN), which moves beyond the limitations of previous methods by\nautomatically learning both the spatial and temporal patterns from data. This\nformulation not only leads to greater expressive power but also stronger\ngeneralization capability. On two large datasets, Kinetics and NTU-RGBD, it\nachieves substantial improvements over mainstream methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 09:48:47 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 07:17:02 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Yan", "Sijie", ""], ["Xiong", "Yuanjun", ""], ["Lin", "Dahua", ""]]}, {"id": "1801.07459", "submitter": "Yuechao Gao", "authors": "Yuechao Gao, Nianhong Liu, Sheng Zhang", "title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of\n  Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address memory and computation resource limitations for hardware-oriented\nacceleration of deep convolutional neural networks (CNNs), we present a\ncomputation flow, stacked filters stationary flow (SFS), and a corresponding\ndata encoding format, relative indexed compressed sparse filter format (CSF),\nto make the best of data sparsity, and simplify data handling at execution\ntime. And we also propose a three dimensional Single Instruction Multiple Data\n(3D-SIMD) processor architecture to illustrate how to accelerate deep CNNs by\ntaking advantage of SFS flow and CSF format. Comparing with the\nstate-of-the-art result (Han et al., 2016b), our methods achieve 1.11x\nimprovement in reducing the storage required by AlexNet, and 1.09x improvement\nin reducing the storage required by SqueezeNet, without loss of accuracy on the\nImageNet dataset. Moreover, using these approaches, chip area for logics\nhandling irregular sparse data access can be saved. Comparing with the 2D-SIMD\nprocessor structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\nprocessing element (PE) array utilization rate improvement (from 26.4\\% to\n96.5\\%) on the data from Deep Compression on AlexNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 09:57:10 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 13:54:33 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 04:08:45 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Gao", "Yuechao", ""], ["Liu", "Nianhong", ""], ["Zhang", "Sheng", ""]]}, {"id": "1801.07481", "submitter": "Ciprian Corneanu", "authors": "Fatemeh Noroozi and Ciprian Adrian Corneanu and Dorota Kami\\'nska and\n  Tomasz Sapi\\'nski and Sergio Escalera and Gholamreza Anbarjafari", "title": "Survey on Emotional Body Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic emotion recognition has become a trending research topic in the\npast decade. While works based on facial expressions or speech abound,\nrecognizing affect from body gestures remains a less explored topic. We present\na new comprehensive survey hoping to boost research in the field. We first\nintroduce emotional body gestures as a component of what is commonly known as\n\"body language\" and comment general aspects as gender differences and culture\ndependence. We then define a complete framework for automatic emotional body\ngesture recognition. We introduce person detection and comment static and\ndynamic body pose estimation methods both in RGB and 3D. We then comment the\nrecent literature related to representation learning and emotion recognition\nfrom images of emotionally expressive gestures. We also discuss multi-modal\napproaches that combine speech or face with body gestures for improved emotion\nrecognition. While pre-processing methodologies (e.g. human detection and pose\nestimation) are nowadays mature technologies fully developed for robust large\nscale analysis, we show that for emotion recognition the quantity of labelled\ndata is scarce, there is no agreement on clearly defined output spaces and the\nrepresentations are shallow and largely based on naive geometrical\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 11:03:37 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Noroozi", "Fatemeh", ""], ["Corneanu", "Ciprian Adrian", ""], ["Kami\u0144ska", "Dorota", ""], ["Sapi\u0144ski", "Tomasz", ""], ["Escalera", "Sergio", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1801.07492", "submitter": "Kaicheng Yu", "authors": "Kaicheng Yu, Mathieu Salzmann", "title": "Statistically Motivated Second Order Pooling", "comments": "Accepted to ECCV 2018. Camera ready version. 14 page, 5 figures, 3\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Second-order pooling, a.k.a.~bilinear pooling, has proven effective for deep\nlearning based visual recognition. However, the resulting second-order networks\nyield a final representation that is orders of magnitude larger than that of\nstandard, first-order ones, making them memory-intensive and cumbersome to\ndeploy. Here, we introduce a general, parametric compression strategy that can\nproduce more compact representations than existing compression techniques, yet\noutperform both compressed and uncompressed second-order models. Our approach\nis motivated by a statistical analysis of the network's activations, relying on\noperations that lead to a Gaussian-distributed final representation, as\ninherently used by first-order deep networks. As evidenced by our experiments,\nthis lets us outperform the state-of-the-art first-order and second-order\nmodels on several benchmark recognition datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 11:39:19 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 13:48:56 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 11:11:25 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Yu", "Kaicheng", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1801.07580", "submitter": "Niannan Xue", "authors": "Niannan Xue, Jiankang Deng, Shiyang Cheng, Yannis Panagakis, Stefanos\n  Zafeiriou", "title": "Side Information for Face Completion: a Robust PCA Approach", "comments": "arXiv admin note: text overlap with arXiv:1702.00648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) is a powerful method for learning\nlow-rank feature representation of various visual data. However, for certain\ntypes as well as significant amount of error corruption, it fails to yield\nsatisfactory results; a drawback that can be alleviated by exploiting\ndomain-dependent prior knowledge or information. In this paper, we propose two\nmodels for the RPCA that take into account such side information, even in the\npresence of missing values. We apply this framework to the task of UV\ncompletion which is widely used in pose-invariant face recognition. Moreover,\nwe construct a generative adversarial network (GAN) to extract side information\nas well as subspaces. These subspaces not only assist in the recovery but also\nspeed up the process in case of large-scale data. We quantitatively and\nqualitatively evaluate the proposed approaches through both synthetic data and\nfive real-world datasets to verify their effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 02:00:06 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Xue", "Niannan", ""], ["Deng", "Jiankang", ""], ["Cheng", "Shiyang", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1801.07632", "submitter": "Zeyuan Chen", "authors": "Zeyuan Chen, Shaoliang Nie, Tianfu Wu, and Christopher G. Healey", "title": "High Resolution Face Completion with Multiple Controllable Attributes\n  via Fully End-to-End Progressive Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning approach for high resolution face completion with\nmultiple controllable attributes (e.g., male and smiling) under arbitrary\nmasks. Face completion entails understanding both structural meaningfulness and\nappearance consistency locally and globally to fill in \"holes\" whose content do\nnot appear elsewhere in an input image. It is a challenging task with the\ndifficulty level increasing significantly with respect to high resolution, the\ncomplexity of \"holes\" and the controllable attributes of filled-in fragments.\nOur system addresses the challenges by learning a fully end-to-end framework\nthat trains generative adversarial networks (GANs) progressively from low\nresolution to high resolution with conditional vectors encoding controllable\nattributes.\n  We design novel network architectures to exploit information across multiple\nscales effectively and efficiently. We introduce new loss functions encouraging\nsharp completion. We show that our system can complete faces with large\nstructural and appearance variations using a single feed-forward pass of\ncomputation with mean inference time of 0.007 seconds for images at 1024 x 1024\nresolution. We also perform a pilot human study that shows our approach\noutperforms state-of-the-art face completion methods in terms of rank analysis.\nThe code will be released upon publication.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:12:26 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Chen", "Zeyuan", ""], ["Nie", "Shaoliang", ""], ["Wu", "Tianfu", ""], ["Healey", "Christopher G.", ""]]}, {"id": "1801.07633", "submitter": "Iyiola E. Olatunji", "authors": "Iyiola E. Olatunji", "title": "Human Activity Recognition for Mobile Robot", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1069/1/012148", "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the increasing number of mobile robots including domestic robots for\ncleaning and maintenance in developed countries, human activity recognition is\ninevitable for congruent human-robot interaction. Needless to say that this is\nindeed a challenging task for robots, it is expedient to learn human activities\nfor autonomous mobile robots (AMR) for navigating in an uncontrolled\nenvironment without any guidance. Building a correct classifier for complex\nhuman action is non-trivial since simple actions can be combined to recognize a\ncomplex human activity. In this paper, we trained a model for human activity\nrecognition using convolutional neural network. We trained and validated the\nmodel using the Vicon physical action dataset and also tested the model on our\ngenerated dataset (VMCUHK). Our experiment shows that our method performs with\nhigh accuracy, human activity recognition task both on the Vicon physical\naction dataset and VMCUHK dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:14:43 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Olatunji", "Iyiola E.", ""]]}, {"id": "1801.07637", "submitter": "Yaron Gurovich", "authors": "Yaron Gurovich, Yair Hanani, Omri Bar, Nicole Fleischer, Dekel\n  Gelbman, Lina Basel-Salmon, Peter Krawitz, Susanne B Kamphausen, Martin\n  Zenker, Lynne M. Bird, Karen W. Gripp", "title": "DeepGestalt - Identifying Rare Genetic Syndromes Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial analysis technologies have recently measured up to the capabilities of\nexpert clinicians in syndrome identification. To date, these technologies could\nonly identify phenotypes of a few diseases, limiting their role in clinical\nsettings where hundreds of diagnoses must be considered.\n  We developed a facial analysis framework, DeepGestalt, using computer vision\nand deep learning algorithms, that quantifies similarities to hundreds of\ngenetic syndromes based on unconstrained 2D images. DeepGestalt is currently\ntrained with over 26,000 patient cases from a rapidly growing\nphenotype-genotype database, consisting of tens of thousands of validated\nclinical cases, curated through a community-driven platform. DeepGestalt\ncurrently achieves 91% top-10-accuracy in identifying over 215 different\ngenetic syndromes and has outperformed clinical experts in three separate\nexperiments.\n  We suggest that this form of artificial intelligence is ready to support\nmedical genetics in clinical and laboratory practices and will play a key role\nin the future of precision medicine.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:18:24 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Gurovich", "Yaron", ""], ["Hanani", "Yair", ""], ["Bar", "Omri", ""], ["Fleischer", "Nicole", ""], ["Gelbman", "Dekel", ""], ["Basel-Salmon", "Lina", ""], ["Krawitz", "Peter", ""], ["Kamphausen", "Susanne B", ""], ["Zenker", "Martin", ""], ["Bird", "Lynne M.", ""], ["Gripp", "Karen W.", ""]]}, {"id": "1801.07648", "submitter": "Elie Aljalbout", "authors": "Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Maximilian Strobel,\n  Daniel Cremers", "title": "Clustering with Deep Learning: Taxonomy and New Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering methods based on deep neural networks have proven promising for\nclustering real-world data because of their high representational power. In\nthis paper, we propose a systematic taxonomy of clustering methods that utilize\ndeep neural networks. We base our taxonomy on a comprehensive review of recent\nwork and validate the taxonomy in a case study. In this case study, we show\nthat the taxonomy enables researchers and practitioners to systematically\ncreate new clustering methods by selectively recombining and replacing distinct\naspects of previous methods with the goal of overcoming their individual\nlimitations. The experimental evaluation confirms this and shows that the\nmethod created for the case study achieves state-of-the-art clustering quality\nand surpasses it in some cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:41:03 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 19:41:22 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Aljalbout", "Elie", ""], ["Golkov", "Vladimir", ""], ["Siddiqui", "Yawar", ""], ["Strobel", "Maximilian", ""], ["Cremers", "Daniel", ""]]}, {"id": "1801.07674", "submitter": "James Davis", "authors": "James W. Davis, Christopher Menart, Muhammad Akbar, Roman Ilin", "title": "A Classification Refinement Strategy for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the observation that semantic segmentation errors are partially\npredictable, we propose a compact formulation using confusion statistics of the\ntrained classifier to refine (re-estimate) the initial pixel label hypotheses.\nThe proposed strategy is contingent upon computing the classifier confusion\nprobabilities for a given dataset and estimating a relevant prior on the object\nclasses present in the image to be classified. We provide a procedure to\nrobustly estimate the confusion probabilities and explore multiple prior\ndefinitions. Experiments are shown comparing performances on multiple\nchallenging datasets using different priors to improve a state-of-the-art\nsemantic segmentation classifier. This study demonstrates the potential to\nsignificantly improve semantic labeling and motivates future work for reliable\nlabel prior estimation from images.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 17:45:54 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Davis", "James W.", ""], ["Menart", "Christopher", ""], ["Akbar", "Muhammad", ""], ["Ilin", "Roman", ""]]}, {"id": "1801.07698", "submitter": "Jiankang Deng", "authors": "Jiankang Deng and Jia Guo and Niannan Xue and Stefanos Zafeiriou", "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", "comments": "ArcFace with parallel acceleration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in feature learning using Deep Convolutional\nNeural Networks (DCNNs) for large-scale face recognition is the design of\nappropriate loss functions that enhance discriminative power. Centre loss\npenalises the distance between the deep features and their corresponding class\ncentres in the Euclidean space to achieve intra-class compactness. SphereFace\nassumes that the linear transformation matrix in the last fully connected layer\ncan be used as a representation of the class centres in an angular space and\npenalises the angles between the deep features and their corresponding weights\nin a multiplicative way. Recently, a popular line of research is to incorporate\nmargins in well-established loss functions in order to maximise face class\nseparability. In this paper, we propose an Additive Angular Margin Loss\n(ArcFace) to obtain highly discriminative features for face recognition. The\nproposed ArcFace has a clear geometric interpretation due to the exact\ncorrespondence to the geodesic distance on the hypersphere. We present arguably\nthe most extensive experimental evaluation of all the recent state-of-the-art\nface recognition methods on over 10 face recognition benchmarks including a new\nlarge-scale image database with trillion level of pairs and a large-scale video\ndataset. We show that ArcFace consistently outperforms the state-of-the-art and\ncan be easily implemented with negligible computational overhead. We release\nall refined training data, training codes, pre-trained models and training\nlogs, which will help reproduce the results in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 18:39:19 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 17:36:29 GMT"}, {"version": "v3", "created": "Sat, 9 Feb 2019 15:14:58 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Deng", "Jiankang", ""], ["Guo", "Jia", ""], ["Xue", "Niannan", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1801.07729", "submitter": "Ahmed Elgammal", "authors": "Ahmed Elgammal and Marian Mazzone and Bingchen Liu and Diana Kim and\n  Mohamed Elhoseiny", "title": "The Shape of Art History in the Eyes of the Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How does the machine classify styles in art? And how does it relate to art\nhistorians' methods for analyzing style? Several studies have shown the ability\nof the machine to learn and predict style categories, such as Renaissance,\nBaroque, Impressionism, etc., from images of paintings. This implies that the\nmachine can learn an internal representation encoding discriminative features\nthrough its visual analysis. However, such a representation is not necessarily\ninterpretable. We conducted a comprehensive study of several of the\nstate-of-the-art convolutional neural networks applied to the task of style\nclassification on 77K images of paintings, and analyzed the learned\nrepresentation through correlation analysis with concepts derived from art\nhistory. Surprisingly, the networks could place the works of art in a smooth\ntemporal arrangement mainly based on learning style labels, without any a\npriori knowledge of time of creation, the historical time and context of\nstyles, or relations between styles. The learned representations showed that\nthere are few underlying factors that explain the visual variations of style in\nart. Some of these factors were found to correlate with style patterns\nsuggested by Heinrich W\\\"olfflin (1846-1945). The learned representations also\nconsistently highlighted certain artists as the extreme distinctive\nrepresentative of their styles, which quantitatively confirms art historian\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:05:21 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 10:00:28 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Mazzone", "Marian", ""], ["Liu", "Bingchen", ""], ["Kim", "Diana", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "1801.07740", "submitter": "Mykhail Uss Ph.D.", "authors": "Mykhail Uss, Benoit Vozel, Vladimir Lukin, Kacem Chehdi", "title": "Estimation of Variance and Spatial Correlation Width for Fine-scale\n  Measurement Error in Digital Elevation Model", "comments": "15 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we borrow from blind noise parameter estimation (BNPE)\nmethodology early developed in the image processing field an original and\ninnovative no-reference approach to estimate Digital Elevation Model (DEM)\nvertical error parameters without resorting to a reference DEM. The challenges\nassociated with the proposed approach related to the physical nature of the\nerror and its multifactor structure in DEM are discussed in detail. A suitable\nmultivariate method is then developed for estimating the error in gridded DEM.\nIt is built on a recently proposed vectorial BNPE method for estimating\nspatially correlated noise using Noise Informative areas and Fractal Brownian\nMotion. The newly multivariate method is derived to estimate the effect of the\nstacking procedure and that of the epipolar line error on local (fine-scale)\nstandard deviation and autocorrelation function width of photogrammetric DEM\nmeasurement error. Applying the new estimator to ASTER GDEM2 and ALOS World 3D\nDEMs, good agreement of derived estimates with results available in the\nliterature is evidenced. In future works, the proposed no-reference method for\nanalyzing DEM error can be extended to a larger number of predictors for\naccounting for other factors influencing remote sensing (RS) DEM accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:29:53 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Uss", "Mykhail", ""], ["Vozel", "Benoit", ""], ["Lukin", "Vladimir", ""], ["Chehdi", "Kacem", ""]]}, {"id": "1801.07779", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "The WiLI benchmark dataset for written language identification", "comments": "{\"pages\": 12, \"figures\": 4, \"language\": \"English\", \"author-ORCiD\":\n  [\"https://orcid.org/0000-0002-6517-1690\"]}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes the WiLI-2018 benchmark dataset for monolingual written\nnatural language identification. WiLI-2018 is a publicly available, free of\ncharge dataset of short text extracts from Wikipedia. It contains 1000\nparagraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a\nclassification dataset: Given an unknown paragraph written in one dominant\nlanguage, it has to be decided which language it is.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 21:40:53 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1801.07791", "submitter": "Yangyan Li", "authors": "Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen", "title": "PointCNN: Convolution On $\\mathcal{X}$-Transformed Points", "comments": "To be published in NIPS 2018, code available at\n  https://github.com/yangyanli/PointCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and general framework for feature learning from point\nclouds. The key to the success of CNNs is the convolution operator that is\ncapable of leveraging spatially-local correlation in data represented densely\nin grids (e.g. images). However, point clouds are irregular and unordered, thus\ndirectly convolving kernels against features associated with the points, will\nresult in desertion of shape information and variance to point ordering. To\naddress these problems, we propose to learn an $\\mathcal{X}$-transformation\nfrom the input points, to simultaneously promote two causes. The first is the\nweighting of the input features associated with the points, and the second is\nthe permutation of the points into a latent and potentially canonical order.\nElement-wise product and sum operations of the typical convolution operator are\nsubsequently applied on the $\\mathcal{X}$-transformed features. The proposed\nmethod is a generalization of typical CNNs to feature learning from point\nclouds, thus we call it PointCNN. Experiments show that PointCNN achieves on\npar or better performance than state-of-the-art methods on multiple challenging\nbenchmark datasets and tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 22:07:21 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 11:45:08 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 02:11:12 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 01:33:31 GMT"}, {"version": "v5", "created": "Mon, 5 Nov 2018 09:31:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Yangyan", ""], ["Bu", "Rui", ""], ["Sun", "Mingchao", ""], ["Wu", "Wei", ""], ["Di", "Xinhan", ""], ["Chen", "Baoquan", ""]]}, {"id": "1801.07829", "submitter": "Yue Wang", "authors": "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M.\n  Bronstein, Justin M. Solomon", "title": "Dynamic Graph CNN for Learning on Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds provide a flexible geometric representation suitable for\ncountless applications in computer graphics; they also comprise the raw output\nof most 3D data acquisition devices. While hand-designed features on point\nclouds have long been proposed in graphics and vision, however, the recent\noverwhelming success of convolutional neural networks (CNNs) for image analysis\nsuggests the value of adapting insight from CNN to the point cloud world. Point\nclouds inherently lack topological information so designing a model to recover\ntopology can enrich the representation power of point clouds. To this end, we\npropose a new neural network module dubbed EdgeConv suitable for CNN-based\nhigh-level tasks on point clouds including classification and segmentation.\nEdgeConv acts on graphs dynamically computed in each layer of the network. It\nis differentiable and can be plugged into existing architectures. Compared to\nexisting modules operating in extrinsic space or treating each point\nindependently, EdgeConv has several appealing properties: It incorporates local\nneighborhood information; it can be stacked applied to learn global shape\nproperties; and in multi-layer systems affinity in feature space captures\nsemantic characteristics over potentially long distances in the original\nembedding. We show the performance of our model on standard benchmarks\nincluding ModelNet40, ShapeNetPart, and S3DIS.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 01:14:04 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 06:11:21 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Wang", "Yue", ""], ["Sun", "Yongbin", ""], ["Liu", "Ziwei", ""], ["Sarma", "Sanjay E.", ""], ["Bronstein", "Michael M.", ""], ["Solomon", "Justin M.", ""]]}, {"id": "1801.07848", "submitter": "Sepidehsadat Hosseini", "authors": "Sepidehsadat Hosseini and Seok Hee Lee and Nam Ik Cho", "title": "Feeding Hand-Crafted Features for Enhancing the Performance of\n  Convolutional Neural Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the convolutional neural network (CNN) is be- lieved to find right\nfeatures for a given problem, the study of hand-crafted features is somewhat\nneglected these days. In this paper, we show that finding an appropriate\nfeature for the given problem may be still important as they can en- hance the\nperformance of CNN-based algorithms. Specif- ically, we show that feeding an\nappropriate feature to the CNN enhances its performance in some face related\nworks such as age/gender estimation, face detection and emotion recognition. We\nuse Gabor filter bank responses for these tasks, feeding them to the CNN along\nwith the input image. The stack of image and Gabor responses can be fed to the\nCNN as a tensor input, or as a fused image which is a weighted sum of image and\nGabor responses. The Gabor filter parameters can also be tuned depending on the\ngiven problem, for increasing the performance. From the extensive experiments,\nit is shown that the proposed methods provide better performance than the\nconventional CNN-based methods that use only the input images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 03:26:07 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Hosseini", "Sepidehsadat", ""], ["Lee", "Seok Hee", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1801.07853", "submitter": "Zhe Wang", "authors": "Zhe Wang, Xiaoyi Liu, Liangjian Chen, Limin Wang, Yu Qiao, Xiaohui\n  Xie, Charless Fowlkes", "title": "Structured Triplet Learning with POS-tag Guided Attention for Visual\n  Question Answering", "comments": "8 pages, 5 figures, state-of-the-art VQA system;\n  https://github.com/wangzheallen/STL-VQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) is of significant interest due to its\npotential to be a strong test of image understanding systems and to probe the\nconnection between language and vision. Despite much recent progress, general\nVQA is far from a solved problem. In this paper, we focus on the VQA\nmultiple-choice task, and provide some good practices for designing an\neffective VQA model that can capture language-vision interactions and perform\njoint reasoning. We explore mechanisms of incorporating part-of-speech (POS)\ntag guided attention, convolutional n-grams, triplet attention interactions\nbetween the image, question and candidate answer, and structured learning for\ntriplets based on image-question pairs. We evaluate our models on two popular\ndatasets: Visual7W and VQA Real Multiple Choice. Our final model achieves the\nstate-of-the-art performance of 68.2% on Visual7W, and a very competitive\nperformance of 69.6% on the test-standard split of VQA Real Multiple Choice.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 03:58:51 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Wang", "Zhe", ""], ["Liu", "Xiaoyi", ""], ["Chen", "Liangjian", ""], ["Wang", "Limin", ""], ["Qiao", "Yu", ""], ["Xie", "Xiaohui", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1801.07892", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang", "title": "Generative Image Inpainting with Contextual Attention", "comments": "Accepted in CVPR 2018; add CelebA-HQ results; open sourced;\n  interactive demo available: http://jhyu.me/demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based approaches have shown promising results for the\nchallenging task of inpainting large missing regions in an image. These methods\ncan generate visually plausible image structures and textures, but often create\ndistorted structures or blurry textures inconsistent with surrounding areas.\nThis is mainly due to ineffectiveness of convolutional neural networks in\nexplicitly borrowing or copying information from distant spatial locations. On\nthe other hand, traditional texture and patch synthesis approaches are\nparticularly suitable when it needs to borrow textures from the surrounding\nregions. Motivated by these observations, we propose a new deep generative\nmodel-based approach which can not only synthesize novel image structures but\nalso explicitly utilize surrounding image features as references during network\ntraining to make better predictions. The model is a feed-forward, fully\nconvolutional neural network which can process images with multiple holes at\narbitrary locations and with variable sizes during the test time. Experiments\non multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and\nnatural images (ImageNet, Places2) demonstrate that our proposed approach\ngenerates higher-quality inpainting results than existing ones. Code, demo and\nmodels are available at: https://github.com/JiahuiYu/generative_inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 08:04:55 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 21:46:22 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Yu", "Jiahui", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Shen", "Xiaohui", ""], ["Lu", "Xin", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1801.07939", "submitter": "Fazil Altinel", "authors": "Fazil Altinel, Mete Ozay, Takayuki Okatani", "title": "Deep Structured Energy-Based Image Inpainting", "comments": "Accepted to 24th International Conference on Pattern Recognition\n  (ICPR 2018). 6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a structured image inpainting method employing an\nenergy based model. In order to learn structural relationship between patterns\nobserved in images and missing regions of the images, we employ an energy-based\nstructured prediction method. The structural relationship is learned by\nminimizing an energy function which is defined by a simple convolutional neural\nnetwork. The experimental results on various benchmark datasets show that our\nproposed method significantly outperforms the state-of-the-art methods which\nuse Generative Adversarial Networks (GANs). We obtained 497.35 mean squared\nerror (MSE) on the Olivetti face dataset compared to 833.0 MSE provided by the\nstate-of-the-art method. Moreover, we obtained 28.4 dB peak signal to noise\nratio (PSNR) on the SVHN dataset and 23.53 dB on the CelebA dataset, compared\nto 22.3 dB and 21.3 dB, provided by the state-of-the-art methods, respectively.\nThe code is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 11:46:14 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 05:57:43 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Altinel", "Fazil", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1801.07987", "submitter": "Xi Zhang", "authors": "Xi Zhang and Xiaolin Wu", "title": "Near-lossless $\\ell_\\infty$-constrained Image Decompression via Deep\n  Neural Network", "comments": "Accepted by DCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a number of CNN-based techniques were proposed to remove image\ncompression artifacts. As in other restoration applications, these techniques\nall learn a mapping from decompressed patches to the original counterparts\nunder the ubiquitous $\\ell_\\infty$ metric. However, this approach is incapable\nof restoring distinctive image details which may be statistical outliers but\nhave high semantic importance (e.g., tiny lesions in medical images). To\novercome this weakness, we propose to incorporate an $\\ell_\\infty$ fidelity\ncriterion in the design of neural network so that no small, distinctive\nstructures of the original image can be dropped or distorted. Experimental\nresults demonstrate that the proposed method outperforms the state-of-the-art\nmethods in $\\ell_\\infty$ error metric and perceptual quality, while being\ncompetitive in $\\ell_2$ error metric as well. It can restore subtle image\ndetails that are otherwise destroyed or missed by other algorithms. Our\nresearch suggests a new machine learning paradigm of ultra high fidelity image\ncompression that is ideally suited for applications in medicine, space, and\nsciences.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 12:25:42 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 10:44:51 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 11:28:36 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 21:30:01 GMT"}, {"version": "v5", "created": "Fri, 17 Jan 2020 22:25:45 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1801.08092", "submitter": "Aditya Ganeshan Master", "authors": "Konda Reddy Mopuri, Aditya Ganeshan and R. Venkatesh Babu", "title": "Generalizable Data-free Objective for Crafting Universal Adversarial\n  Perturbations", "comments": "TPAMI | Repository: https://github.com/val-iisc/GD-UAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are susceptible to adversarial perturbations: small\nchanges to input that can cause large changes in output. It is also\ndemonstrated that there exist input-agnostic perturbations, called universal\nadversarial perturbations, which can change the inference of target model on\nmost of the data samples. However, existing methods to craft universal\nperturbations are (i) task specific, (ii) require samples from the training\ndata distribution, and (iii) perform complex optimizations. Additionally,\nbecause of the data dependence, fooling ability of the crafted perturbations is\nproportional to the available training data. In this paper, we present a novel,\ngeneralizable and data-free approaches for crafting universal adversarial\nperturbations. Independent of the underlying task, our objective achieves\nfooling via corrupting the extracted features at multiple layers. Therefore,\nthe proposed objective is generalizable to craft image-agnostic perturbations\nacross multiple vision tasks such as object recognition, semantic segmentation,\nand depth estimation. In the practical setting of black-box attack scenario\n(when the attacker does not have access to the target model and it's training\ndata), we show that our objective outperforms the data dependent objectives to\nfool the learned models. Further, via exploiting simple priors related to the\ndata distribution, our objective remarkably boosts the fooling ability of the\ncrafted perturbations. Significant fooling rates achieved by our objective\nemphasize that the current deep learning models are now at an increased risk,\nsince our objective generalizes across multiple tasks without the requirement\nof training data for crafting the perturbations. To encourage reproducible\nresearch, we have released the codes for our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:36:57 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 12:43:10 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 08:19:43 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Ganeshan", "Aditya", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1801.08100", "submitter": "Roberto J. L\\'opez-Sastre", "authors": "Carolina Redondo-Cabrera and Roberto J. L\\'opez-Sastre", "title": "Unsupervised learning from videos using temporal coherency deep networks", "comments": null, "journal-ref": "Computer Vision and Image Understanding, 2018", "doi": "10.1016/j.cviu.2018.08.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the challenging problem of unsupervised learning from\nvideos. Existing methods utilize the spatio-temporal continuity in contiguous\nvideo frames as regularization for the learning process. Typically, this\ntemporal coherence of close frames is used as a free form of annotation,\nencouraging the learned representations to exhibit small differences between\nthese frames. But this type of approach fails to capture the dissimilarity\nbetween videos with different content, hence learning less discriminative\nfeatures. We here propose two Siamese architectures for Convolutional Neural\nNetworks, and their corresponding novel loss functions, to learn from unlabeled\nvideos, which jointly exploit the local temporal coherence between contiguous\nframes, and a global discriminative margin used to separate representations of\ndifferent videos. An extensive experimental evaluation is presented, where we\nvalidate the proposed models on various tasks. First, we show how the learned\nfeatures can be used to discover actions and scenes in video collections.\nSecond, we show the benefits of such an unsupervised learning from just\nunlabeled videos, which can be directly used as a prior for the supervised\nrecognition tasks of actions and objects in images, where our results further\nshow that our features can even surpass a traditional and heavily supervised\npre-training plus fine-tunning strategy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:51:32 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 13:28:13 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Redondo-Cabrera", "Carolina", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1801.08110", "submitter": "Roberto J. L\\'opez-Sastre", "authors": "Daniel O\\~noro-Rubio and Roberto J. L\\'opez-Sastre and Carolina\n  Redondo-Cabrera and Pedro Gil-Jim\\'enez", "title": "The challenge of simultaneous object detection and pose estimation: a\n  comparative study", "comments": null, "journal-ref": "Image and Vision Computing, 2018", "doi": "10.1016/j.imavis.2018.09.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects and estimating their pose remains as one of the major\nchallenges of the computer vision research community. There exists a compromise\nbetween localizing the objects and estimating their viewpoints. The detector\nideally needs to be view-invariant, while the pose estimation process should be\nable to generalize towards the category-level. This work is an exploration of\nusing deep learning models for solving both problems simultaneously. For doing\nso, we propose three novel deep learning architectures, which are able to\nperform a joint detection and pose estimation, where we gradually decouple the\ntwo tasks. We also investigate whether the pose estimation problem should be\nsolved as a classification or regression problem, being this still an open\nquestion in the computer vision community. We detail a comparative analysis of\nall our solutions and the methods that currently define the state of the art\nfor this problem. We use PASCAL3D+ and ObjectNet3D datasets to present the\nthorough experimental evaluation and main results. With the proposed models we\nachieve the state-of-the-art performance in both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 18:21:38 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["O\u00f1oro-Rubio", "Daniel", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""], ["Redondo-Cabrera", "Carolina", ""], ["Gil-Jim\u00e9nez", "Pedro", ""]]}, {"id": "1801.08163", "submitter": "Kushal Kafle", "authors": "Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan", "title": "DVQA: Understanding Data Visualizations via Question Answering", "comments": "CVPR 2018 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bar charts are an effective way to convey numeric information, but today's\nalgorithms cannot parse them. Existing methods fail when faced with even minor\nvariations in appearance. Here, we present DVQA, a dataset that tests many\naspects of bar chart understanding in a question answering framework. Unlike\nvisual question answering (VQA), DVQA requires processing words and answers\nthat are unique to a particular bar chart. State-of-the-art VQA algorithms\nperform poorly on DVQA, and we propose two strong baselines that perform\nconsiderably better. Our work will enable algorithms to automatically extract\nnumeric and semantic information from vast quantities of bar charts found in\nscientific publications, Internet articles, business reports, and many other\nareas.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 19:47:04 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:42:15 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Kafle", "Kushal", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Kanan", "Christopher", ""]]}, {"id": "1801.08186", "submitter": "Licheng Yu", "authors": "Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal,\n  Tamara L.Berg", "title": "MAttNet: Modular Attention Network for Referring Expression\n  Comprehension", "comments": "Equation of word attention fixed; MAttNet+Grabcut results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address referring expression comprehension: localizing an\nimage region described by a natural language expression. While most recent work\ntreats expressions as a single unit, we propose to decompose them into three\nmodular components related to subject appearance, location, and relationship to\nother objects. This allows us to flexibly adapt to expressions containing\ndifferent types of information in an end-to-end framework. In our model, which\nwe call the Modular Attention Network (MAttNet), two types of attention are\nutilized: language-based attention that learns the module weights as well as\nthe word/phrase attention that each module should focus on; and visual\nattention that allows the subject and relationship modules to focus on relevant\nimage components. Module weights combine scores from all three modules\ndynamically to output an overall score. Experiments show that MAttNet\noutperforms previous state-of-art methods by a large margin on both\nbounding-box-level and pixel-level comprehension tasks. Demo and code are\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 20:54:26 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 15:40:51 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 02:45:55 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yu", "Licheng", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jimei", ""], ["Lu", "Xin", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1801.08234", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh and Mohan M. Trivedi", "title": "When Vehicles See Pedestrians with Phones:A Multi-Cue Framework for\n  Recognizing Phone-based Activities of Pedestrians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intelligent vehicle community has devoted considerable efforts to model\ndriver behavior, and in particular to detect and overcome driver distraction in\nan effort to reduce accidents caused by driver negligence. However, as the\ndomain increasingly shifts towards autonomous and semi-autonomous solutions,\nthe driver is no longer integral to the decision making process, indicating a\nneed to refocus efforts elsewhere. To this end, we propose to study pedestrian\ndistraction instead. In particular, we focus on detecting pedestrians who are\nengaged in secondary activities involving their cellphones and similar handheld\nmultimedia devices from a purely vision-based standpoint. To achieve this\nobjective, we propose a pipeline incorporating articulated human pose\nestimation, followed by a soft object label transfer from an ensemble of\nexemplar SVMs trained on the nearest neighbors in pose feature space. We\nadditionally incorporate head gaze features and prior pose information to carry\nout cellphone related pedestrian activity recognition. Finally, we offer a\nmethod to reliably track the articulated pose of a pedestrian through a\nsequence of images using a particle filter with a Gaussian Process Dynamical\nModel (GPDM), which can then be used to estimate sequentially varying activity\nscores at a very low computational cost. The entire framework is fast\n(especially for sequential data) and accurate, and easily extensible to include\nother secondary activities and sources of distraction.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 23:14:58 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1801.08252", "submitter": "Seyed Ali Rokni", "authors": "Seyed Ali Rokni, Marjan Nourollahi, Hassan Ghasemzadeh", "title": "Personalized Human Activity Recognition Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major barrier to the personalized Human Activity Recognition using wearable\nsensors is that the performance of the recognition model drops significantly\nupon adoption of the system by new users or changes in physical/ behavioral\nstatus of users. Therefore, the model needs to be retrained by collecting new\nlabeled data in the new context. In this study, we develop a transfer learning\nframework using convolutional neural networks to build a personalized activity\nrecognition model with minimal user supervision.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 01:35:25 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Rokni", "Seyed Ali", ""], ["Nourollahi", "Marjan", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "1801.08267", "submitter": "Wei-Ta Chu", "authors": "Wei-Ta Chu, Kai-Chia Ho, and Ali Borji", "title": "Visual Weather Temperature Prediction", "comments": "8 pages, accepted to WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to employ convolutional recurrent neural networks\nfor weather temperature estimation using only image data. We study ambient\ntemperature estimation based on deep neural networks in two scenarios a)\nestimating temperature of a single outdoor image, and b) predicting temperature\nof the last image in an image sequence. In the first scenario, visual features\nare extracted by a convolutional neural network trained on a large-scale image\ndataset. We demonstrate that promising performance can be obtained, and analyze\nhow volume of training data influences performance. In the second scenario, we\nconsider the temporal evolution of visual appearance, and construct a recurrent\nneural network to predict the temperature of the last image in a given image\nsequence. We obtain better prediction accuracy compared to the state-of-the-art\nmodels. Further, we investigate how performance varies when information is\nextracted from different scene regions, and when images are captured in\ndifferent daytime hours. Our approach further reinforces the idea of using only\nvisual information for cost efficient weather prediction in the future.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 03:37:02 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Chu", "Wei-Ta", ""], ["Ho", "Kai-Chia", ""], ["Borji", "Ali", ""]]}, {"id": "1801.08268", "submitter": "Utsav Gewali", "authors": "Utsav B. Gewali and Sildomar T. Monteiro", "title": "A Tutorial on Modeling and Inference in Undirected Graphical Models for\n  Hyperspectral Image Analysis", "comments": null, "journal-ref": null, "doi": "10.1080/01431161.2018.1465614", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models have been successfully used to jointly model the\nspatial and the spectral dependencies in earth observing hyperspectral images.\nThey produce less noisy, smooth, and spatially coherent land cover maps and\ngive top accuracies on many datasets. Moreover, they can easily be combined\nwith other state-of-the-art approaches, such as deep learning. This has made\nthem an essential tool for remote sensing researchers and practitioners.\nHowever, graphical models have not been easily accessible to the larger remote\nsensing community as they are not discussed in standard remote sensing\ntextbooks and not included in the popular remote sensing software and\ntoolboxes. In this tutorial, we provide a theoretical introduction to Markov\nrandom fields and conditional random fields based spatial-spectral\nclassification for land cover mapping along with a detailed step-by-step\npractical guide on applying these methods using freely available software.\nFurthermore, the discussed methods are benchmarked on four public hyperspectral\ndatasets for a fair comparison among themselves and easy comparison with the\nvast number of methods in literature which use the same datasets. The source\ncode necessary to reproduce all the results in the paper is published on-line\nto make it easier for the readers to apply these techniques to different remote\nsensing problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 03:44:43 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Gewali", "Utsav B.", ""], ["Monteiro", "Sildomar T.", ""]]}, {"id": "1801.08297", "submitter": "Yuan Gao", "authors": "Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, Alan L. Yuille", "title": "NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural\n  Discriminative Dimensionality Reduction", "comments": "11 pages, 3 figures, 9 tables", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Convolutional Neural Network (CNN)\nstructure for general-purpose multi-task learning (MTL), which enables\nautomatic feature fusing at every layer from different tasks. This is in\ncontrast with the most widely used MTL CNN structures which empirically or\nheuristically share features on some specific layers (e.g., share all the\nfeatures except the last convolutional layer). The proposed layerwise feature\nfusing scheme is formulated by combining existing CNN components in a novel\nway, with clear mathematical interpretability as discriminative dimensionality\nreduction, which is referred to as Neural Discriminative Dimensionality\nReduction (NDDR). Specifically, we first concatenate features with the same\nspatial resolution from different tasks according to their channel dimension.\nThen, we show that the discriminative dimensionality reduction can be fulfilled\nby 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use\nof existing CNN components ensures the end-to-end training and the\nextensibility of the proposed NDDR layer to various state-of-the-art CNN\narchitectures in a \"plug-and-play\" manner. The detailed ablation analysis shows\nthat the proposed NDDR layer is easy to train and also robust to different\nhyperparameters. Experiments on different task sets with various base network\narchitectures demonstrate the promising performance and desirable\ngeneralizability of our proposed method. The code of our paper is available at\nhttps://github.com/ethanygao/NDDR-CNN.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 07:38:52 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 11:57:07 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 19:15:47 GMT"}, {"version": "v4", "created": "Thu, 4 Apr 2019 23:24:48 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Gao", "Yuan", ""], ["Ma", "Jiayi", ""], ["Zhao", "Mingbo", ""], ["Liu", "Wei", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1801.08301", "submitter": "Guangfeng Lin", "authors": "Guangfeng Lin, Caixia Fan, Wanjun Chen, Yajun Chen, Fan Zhao", "title": "Class label autoencoder for zero-shot learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing zero-shot learning (ZSL) methods usually learn a projection function\nbetween a feature space and a semantic embedding space(text or attribute space)\nin the training seen classes or testing unseen classes. However, the projection\nfunction cannot be used between the feature space and multi-semantic embedding\nspaces, which have the diversity characteristic for describing the different\nsemantic information of the same class. To deal with this issue, we present a\nnovel method to ZSL based on learning class label autoencoder (CLA). CLA can\nnot only build a uniform framework for adapting to multi-semantic embedding\nspaces, but also construct the encoder-decoder mechanism for constraining the\nbidirectional projection between the feature space and the class label space.\nMoreover, CLA can jointly consider the relationship of feature classes and the\nrelevance of the semantic classes for improving zero-shot classification. The\nCLA solution can provide both unseen class labels and the relation of the\ndifferent classes representation(feature or semantic information) that can\nencode the intrinsic structure of classes. Extensive experiments demonstrate\nthe CLA outperforms state-of-art methods on four benchmark datasets, which are\nAwA, CUB, Dogs and ImNet-2.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 08:00:36 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Lin", "Guangfeng", ""], ["Fan", "Caixia", ""], ["Chen", "Wanjun", ""], ["Chen", "Yajun", ""], ["Zhao", "Fan", ""]]}, {"id": "1801.08322", "submitter": "Siddique Latif", "authors": "Siddique Latif, Muhammad Usman, Rajib Rana, and Junaid Qadir", "title": "Phonocardiographic Sensing using Deep Learning for Abnormal Heartbeat\n  Detection", "comments": null, "journal-ref": "IEEE Sensors Journal 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac auscultation involves expert interpretation of abnormalities in heart\nsounds using stethoscope. Deep learning based cardiac auscultation is of\nsignificant interest to the healthcare community as it can help reducing the\nburden of manual auscultation with automated detection of abnormal heartbeats.\nHowever, the problem of automatic cardiac auscultation is complicated due to\nthe requirement of reliability and high accuracy, and due to the presence of\nbackground noise in the heartbeat sound. In this work, we propose a Recurrent\nNeural Networks (RNNs) based automated cardiac auscultation solution. Our\nchoice of RNNs is motivated by the great success of deep learning in medical\napplications and by the observation that RNNs represent the deep learning\nconfiguration most suitable for dealing with sequential or temporal data even\nin the presence of noise. We explore the use of various RNN models, and\ndemonstrate that these models deliver the abnormal heartbeat classification\nscore with significant improvement. Our proposed approach using RNNs can be\npotentially be used for real-time abnormal heartbeat detection in the Internet\nof Medical Things for remote monitoring applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 09:25:41 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 07:38:24 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 13:02:35 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 01:48:35 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Latif", "Siddique", ""], ["Usman", "Muhammad", ""], ["Rana", "Rajib", ""], ["Qadir", "Junaid", ""]]}, {"id": "1801.08329", "submitter": "Siddique Latif", "authors": "Muhammad Usman, Siddique Latif and Junaid Qadir", "title": "Using Deep Autoencoders for Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature descriptors involved in image processing are generally manually\nchosen and high dimensional in nature. Selecting the most important features is\na very crucial task for systems like facial expression recognition. This paper\ninvestigates the performance of deep autoencoders for feature selection and\ndimension reduction for facial expression recognition on multiple levels of\nhidden layers. The features extracted from the stacked autoencoder outperformed\nwhen compared to other state-of-the-art feature selection and dimension\nreduction techniques.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 10:06:01 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Usman", "Muhammad", ""], ["Latif", "Siddique", ""], ["Qadir", "Junaid", ""]]}, {"id": "1801.08360", "submitter": "Jinxing Li", "authors": "Jinxing Li, Bob Zhang, Guangming Lu, David Zhang", "title": "Dual Asymmetric Deep Hashing Learning", "comments": "12 pages, 6 figures, 7 tables, 37 conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the impressive learning power, deep learning has achieved a remarkable\nperformance in supervised hash function learning. In this paper, we propose a\nnovel asymmetric supervised deep hashing method to preserve the semantic\nstructure among different categories and generate the binary codes\nsimultaneously. Specifically, two asymmetric deep networks are constructed to\nreveal the similarity between each pair of images according to their semantic\nlabels. The deep hash functions are then learned through two networks by\nminimizing the gap between the learned features and discrete codes.\nFurthermore, since the binary codes in the Hamming space also should keep the\nsemantic affinity existing in the original space, another asymmetric pairwise\nloss is introduced to capture the similarity between the binary codes and\nreal-value features. This asymmetric loss not only improves the retrieval\nperformance, but also contributes to a quick convergence at the training phase.\nBy taking advantage of the two-stream deep structures and two types of\nasymmetric pairwise functions, an alternating algorithm is designed to optimize\nthe deep features and high-quality binary codes efficiently. Experimental\nresults on three real-world datasets substantiate the effectiveness and\nsuperiority of our approach as compared with state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 11:31:29 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Li", "Jinxing", ""], ["Zhang", "Bob", ""], ["Lu", "Guangming", ""], ["Zhang", "David", ""]]}, {"id": "1801.08361", "submitter": "Stuart Golodetz", "authors": "Stuart Golodetz, Tommaso Cavallari, Nicholas A Lord, Victor A\n  Prisacariu, David W Murray and Philip H S Torr", "title": "Collaborative Large-Scale Dense 3D Reconstruction with Online\n  Inter-Agent Pose Optimisation", "comments": "Stuart Golodetz, Tommaso Cavallari and Nicholas Lord assert joint\n  first authorship", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics\n  24(11):2895-2905, 2018", "doi": "10.1109/TVCG.2018.2868533", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing dense, volumetric models of real-world 3D scenes is important\nfor many tasks, but capturing large scenes can take significant time, and the\nrisk of transient changes to the scene goes up as the capture time increases.\nThese are good reasons to want instead to capture several smaller sub-scenes\nthat can be joined to make the whole scene. Achieving this has traditionally\nbeen difficult: joining sub-scenes that may never have been viewed from the\nsame angle requires a high-quality camera relocaliser that can cope with novel\nposes, and tracking drift in each sub-scene can prevent them from being joined\nto make a consistent overall scene. Recent advances, however, have\nsignificantly improved our ability to capture medium-sized sub-scenes with\nlittle to no tracking drift: real-time globally consistent reconstruction\nsystems can close loops and re-integrate the scene surface on the fly, whilst\nnew visual-inertial odometry approaches can significantly reduce tracking drift\nduring live reconstruction. Moreover, high-quality regression forest-based\nrelocalisers have recently been made more practical by the introduction of a\nmethod to allow them to be trained and used online. In this paper, we leverage\nthese advances to present what to our knowledge is the first system to allow\nmultiple users to collaborate interactively to reconstruct dense, voxel-based\nmodels of whole buildings using only consumer-grade hardware, a task that has\ntraditionally been both time-consuming and dependent on the availability of\nspecialised hardware. Using our system, an entire house or lab can be\nreconstructed in under half an hour and at a far lower cost than was previously\npossible.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 11:31:44 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 15:00:40 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Golodetz", "Stuart", ""], ["Cavallari", "Tommaso", ""], ["Lord", "Nicholas A", ""], ["Prisacariu", "Victor A", ""], ["Murray", "David W", ""], ["Torr", "Philip H S", ""]]}, {"id": "1801.08388", "submitter": "Alessio Del Bue", "authors": "Sebastian Hoppe Nesgaard Jensen, Mads Emil Brix Doest, Henrik Aanaes,\n  Alessio Del Bue", "title": "A Benchmark and Evaluation of Non-Rigid Structure from Motion", "comments": "Accepted, International Journal of Computer Vision (IJCV)", "journal-ref": "International Journal of Computer Vision (IJCV), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Rigid structure from motion (NRSfM), is a long standing and central\nproblem in computer vision and its solution is necessary for obtaining 3D\ninformation from multiple images when the scene is dynamic. A main issue\nregarding the further development of this important computer vision topic, is\nthe lack of high quality data sets. We here address this issue by presenting a\ndata set created for this purpose, which is made publicly available, and\nconsiderably larger than the previous state of the art. To validate the\napplicability of this data set, and provide an investigation into the state of\nthe art of NRSfM, including potential directions forward, we here present a\nbenchmark and a scrupulous evaluation using this data set. This benchmark\nevaluates 18 different methods with available code that reasonably spans the\nstate of the art in sparse NRSfM. This new public data set and evaluation\nprotocol will provide benchmark tools for further development in this\nchallenging field.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 12:59:09 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 08:41:03 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 15:03:42 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Jensen", "Sebastian Hoppe Nesgaard", ""], ["Doest", "Mads Emil Brix", ""], ["Aanaes", "Henrik", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1801.08390", "submitter": "Peipei Li", "authors": "Peipei Li, Yibo Hu, Qi Li, Ran He and Zhenan Sun", "title": "Global and Local Consistent Age Generative Adversarial Networks", "comments": "6 pages, 8 figures, submitted to ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age progression/regression is a challenging task due to the complicated and\nnon-linear transformation in human aging process. Many researches have shown\nthat both global and local facial features are essential for face\nrepresentation, but previous GAN based methods mainly focused on the global\nfeature in age synthesis. To utilize both global and local facial information,\nwe propose a Global and Local Consistent Age Generative Adversarial Network\n(GLCA-GAN). In our generator, a global network learns the whole facial\nstructure and simulates the aging trend of the whole face, while three crucial\nfacial patches are progressed or regressed by three local networks aiming at\nimitating subtle changes of crucial facial subregions. To preserve most of the\ndetails in age-attribute-irrelevant areas, our generator learns the residual\nface. Moreover, we employ an identity preserving loss to better preserve the\nidentity information, as well as age preserving loss to enhance the accuracy of\nage synthesis. A pixel loss is also adopted to preserve detailed facial\ninformation of the input face. Our proposed method is evaluated on three face\naging datasets, i.e., CACD dataset, Morph dataset and FG-NET dataset.\nExperimental results show appealing performance of the proposed method by\ncomparing with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 13:04:13 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Li", "Peipei", ""], ["Hu", "Yibo", ""], ["Li", "Qi", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1801.08391", "submitter": "Haosheng Zou", "authors": "Haosheng Zou, Hang Su, Shihong Song, Jun Zhu", "title": "Understanding Human Behaviors in Crowds by Imitating the Decision-Making\n  Process", "comments": "accepted to AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd behavior understanding is crucial yet challenging across a wide range\nof applications, since crowd behavior is inherently determined by a sequential\ndecision-making process based on various factors, such as the pedestrians' own\ndestinations, interaction with nearby pedestrians and anticipation of upcoming\nevents. In this paper, we propose a novel framework of Social-Aware Generative\nAdversarial Imitation Learning (SA-GAIL) to mimic the underlying\ndecision-making process of pedestrians in crowds. Specifically, we infer the\nlatent factors of human decision-making process in an unsupervised manner by\nextending the Generative Adversarial Imitation Learning framework to anticipate\nfuture paths of pedestrians. Different factors of human decision making are\ndisentangled with mutual information maximization, with the process modeled by\ncollision avoidance regularization and Social-Aware LSTMs. Experimental results\ndemonstrate the potential of our framework in disentangling the latent\ndecision-making factors of pedestrians and stronger abilities in predicting\nfuture trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 13:08:43 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Zou", "Haosheng", ""], ["Su", "Hang", ""], ["Song", "Shihong", ""], ["Zhu", "Jun", ""]]}, {"id": "1801.08406", "submitter": "Subrahmanyam Murala", "authors": "Akshay Dudhane and Subrahmanyam Murala", "title": "C2MSNet: A Novel approach for single image haze removal", "comments": "Accepted in Winter Conference on Applications of Computer Vision\n  (WACV-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degradation of image quality due to the presence of haze is a very common\nphenomenon. Existing DehazeNet [3], MSCNN [11] tackled the drawbacks of hand\ncrafted haze relevant features. However, these methods have the problem of\ncolor distortion in gloomy (poor illumination) environment. In this paper, a\ncardinal (red, green and blue) color fusion network for single image haze\nremoval is proposed. In first stage, network fusses color information present\nin hazy images and generates multi-channel depth maps. The second stage\nestimates the scene transmission map from generated dark channels using multi\nchannel multi scale convolutional neural network (McMs-CNN) to recover the\noriginal scene. To train the proposed network, we have used two standard\ndatasets namely: ImageNet [5] and D-HAZY [1]. Performance evaluation of the\nproposed approach has been carried out using structural similarity index\n(SSIM), mean square error (MSE) and peak signal to noise ratio (PSNR).\nPerformance analysis shows that the proposed approach outperforms the existing\nstate-of-the-art methods for single image dehazing.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 14:16:14 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Dudhane", "Akshay", ""], ["Murala", "Subrahmanyam", ""]]}, {"id": "1801.08467", "submitter": "Xiaoxiang Zhu", "authors": "Lloyd H. Hughes, Michael Schmitt, Lichao Mou, Yuanyuan Wang, and Xiao\n  Xiang Zhu", "title": "Identifying Corresponding Patches in SAR and Optical Images with a\n  Pseudo-Siamese CNN", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2018.2799232", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose a pseudo-siamese convolutional neural network\n(CNN) architecture that enables to solve the task of identifying corresponding\npatches in very-high-resolution (VHR) optical and synthetic aperture radar\n(SAR) remote sensing imagery. Using eight convolutional layers each in two\nparallel network streams, a fully connected layer for the fusion of the\nfeatures learned in each stream, and a loss function based on binary\ncross-entropy, we achieve a one-hot indication if two patches correspond or\nnot. The network is trained and tested on an automatically generated dataset\nthat is based on a deterministic alignment of SAR and optical imagery via\npreviously reconstructed and subsequently co-registered 3D point clouds. The\nsatellite images, from which the patches comprising our dataset are extracted,\nshow a complex urban scene containing many elevated objects (i.e. buildings),\nthus providing one of the most difficult experimental environments. The\nachieved results show that the network is able to predict corresponding patches\nwith high accuracy, thus indicating great potential for further development\ntowards a generalized multi-sensor key-point matching procedure. Index\nTerms-synthetic aperture radar (SAR), optical imagery, data fusion, deep\nlearning, convolutional neural networks (CNN), image matching, deep matching\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 16:12:38 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hughes", "Lloyd H.", ""], ["Schmitt", "Michael", ""], ["Mou", "Lichao", ""], ["Wang", "Yuanyuan", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1801.08468", "submitter": "Ling Zhang", "authors": "Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao", "title": "Convolutional Invasion and Expansion Networks for Tumor Growth\n  Prediction", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 15 November 2017, Volume:PP\n  Issue: 99", "doi": "10.1109/TMI.2017.2774044", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor growth is associated with cell invasion and mass-effect, which are\ntraditionally formulated by mathematical models, namely reaction-diffusion\nequations and biomechanics. Such models can be personalized based on clinical\nmeasurements to build the predictive models for tumor growth. In this paper, we\ninvestigate the possibility of using deep convolutional neural networks\n(ConvNets) to directly represent and learn the cell invasion and mass-effect,\nand to predict the subsequent involvement regions of a tumor. The invasion\nnetwork learns the cell invasion from information related to metabolic rate,\ncell density and tumor boundary derived from multimodal imaging data. The\nexpansion network models the mass-effect from the growing motion of tumor mass.\nWe also study different architectures that fuse the invasion and expansion\nnetworks, in order to exploit the inherent correlations among them. Our network\ncan easily be trained on population data and personalized to a target patient,\nunlike most previous mathematical modeling methods that fail to incorporate\npopulation data. Quantitative experiments on a pancreatic tumor data set show\nthat the proposed method substantially outperforms a state-of-the-art\nmathematical model-based approach in both accuracy and efficiency, and that the\ninformation captured by each of the two subnetworks are complementary.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 16:13:29 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Zhang", "Ling", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""], ["Kebebew", "Electron", ""], ["Yao", "Jianhua", ""]]}, {"id": "1801.08480", "submitter": "Prem Sewak Sudhish", "authors": "Prem Sewak Sudhish", "title": "An Integrated Soft Computing Approach to a Multi-biometric Security\n  Model", "comments": "Ph.D. thesis of Prem Sewak Sudhish, Dayalbagh Educational Institute.\n  The thesis has been formatted for duplex printing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abstract of the thesis consists of three sections, videlicet,\n  Motivation\n  Chapter Organization\n  Salient Contributions.\n  The complete abstract is included with the thesis. The final section on\nSalient Contributions is reproduced below.\n  Salient Contributions\n  The research presents the following salient contributions:\n  i. A novel technique has been developed for comparing biographical\ninformation, by combining the average impact of Levenshtein,\nDamerau-Levenshtein, and editor distances. The impact is calculated as the\nratio of the edit distance to the maximum possible edit distance between two\nstrings of the same lengths as the given pair of strings. This impact lies in\nthe range [0, 1] and can easily be converted to a similarity (matching) score\nby subtracting the impact from unity.\n  ii. A universal soft computing framework is proposed for adaptively fusing\nbiometric and biographical information by making real-time decisions to\ndetermine after consideration of each individual identifier whether computation\nof matching scores and subsequent fusion of additional identifiers, including\nbiographical information is required. This proposed framework not only improves\nthe accuracy of the system by fusing less reliable information (e.g.\nbiographical information) only for instances where such a fusion is required,\nbut also improves the efficiency of the system by computing matching scores for\nvarious available identifiers only when this computation is considered\nnecessary.\n  iii. A scientific method for comparing efficiency of fusion strategies\nthrough a predicted effort to error trade-off curve.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 16:54:16 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Sudhish", "Prem Sewak", ""]]}, {"id": "1801.08486", "submitter": "Ling Zhang", "authors": "Ling Zhang, Vissagan Gopalakrishnan, Le Lu, Ronald M. Summers, Joel\n  Moss, Jianhua Yao", "title": "Self-Learning to Detect and Segment Cysts in Lung CT Images without\n  Manual Annotation", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a fundamental problem in medical image analysis. In\nrecent years, deep neural networks achieve impressive performances on many\nmedical image segmentation tasks by supervised learning on large manually\nannotated data. However, expert annotations on big medical datasets are\ntedious, expensive or sometimes unavailable. Weakly supervised learning could\nreduce the effort for annotation but still required certain amounts of\nexpertise. Recently, deep learning shows a potential to produce more accurate\npredictions than the original erroneous labels. Inspired by this, we introduce\na very weakly supervised learning method, for cystic lesion detection and\nsegmentation in lung CT images, without any manual annotation. Our method works\nin a self-learning manner, where segmentation generated in previous steps\n(first by unsupervised segmentation then by neural networks) is used as ground\ntruth for the next level of network learning. Experiments on a cystic lung\nlesion dataset show that the deep learning could perform better than the\ninitial unsupervised annotation, and progressively improve itself after\nself-learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 17:02:29 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Zhang", "Ling", ""], ["Gopalakrishnan", "Vissagan", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""], ["Moss", "Joel", ""], ["Yao", "Jianhua", ""]]}, {"id": "1801.08513", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Erin B. Wetherley and Paul D. Gader", "title": "Unmixing urban hyperspectral imagery with a Gaussian mixture model on\n  endmember variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model a pixel as a linear combination of endmembers sampled\nfrom probability distributions of Gaussian mixture models (GMM). The parameters\nof the GMM distributions are estimated using spectral libraries. Abundances are\nestimated based on the distribution parameters. The advantage of this algorithm\nis that the model size grows very slowly as a function of the library size. To\nvalidate this method, we used data collected by the AVIRIS sensor over the\nSanta Barbara region: two 16 m spatial resolution and two 4 m spatial\nresolution images. 64 validated regions of interest (ROI) (180 m by 180 m) were\nused to assess estimate accuracy. Ground truth was obtained using 1 m images\nleading to the following 6 classes: turfgrass, non-photosynthetic vegetation\n(NPV), paved, roof, soil, and tree. Spectral libraries were built by manually\nidentifying and extracting pure spectra from both resolution images, resulting\nin 3,287 spectra at 16 m and 15,426 spectra at 4 m. We then unmixed ROIs of\neach resolution using the following unmixing algorithms: the set-based\nalgorithms MESMA and AAM, and the distribution-based algorithms GMM, NCM, and\nBCM. The original libraries were used for the distribution-based algorithms\nwhereas set-based methods required a sophisticated reduction method, resulting\nin reduced libraries of 61 spectra at 16 m and 95 spectra at 4 m. The results\nshow that GMM performs best among the distribution-based methods, producing\ncomparable accuracy to MESMA, and may be more robust across datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 18:22:22 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Zhou", "Yuan", ""], ["Wetherley", "Erin B.", ""], ["Gader", "Paul D.", ""]]}, {"id": "1801.08558", "submitter": "Hidetoshi Furukawa", "authors": "Hidetoshi Furukawa", "title": "Deep Learning for End-to-End Automatic Target Recognition from Synthetic\n  Aperture Radar Imagery", "comments": "Technical Report, 6 pages, 7 figures, 7 tables, Copyright(C)2018\n  IEICE", "journal-ref": "IEICE Technical Report, vol.117, no.403, SANE2017-92, pp.35-40,\n  Jan. 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard architecture of synthetic aperture radar (SAR) automatic target\nrecognition (ATR) consists of three stages: detection, discrimination, and\nclassification. In recent years, convolutional neural networks (CNNs) for SAR\nATR have been proposed, but most of them classify target classes from a target\nchip extracted from SAR imagery, as a classification for the third stage of SAR\nATR. In this report, we propose a novel CNN for end-to-end ATR from SAR\nimagery. The CNN named verification support network (VersNet) performs all\nthree stages of SAR ATR end-to-end. VersNet inputs a SAR image of arbitrary\nsizes with multiple classes and multiple targets, and outputs a SAR ATR image\nrepresenting the position, class, and pose of each detected target. This report\ndescribes the evaluation results of VersNet which trained to output scores of\nall 12 classes: 10 target classes, a target front class, and a background\nclass, for each pixel using the moving and stationary target acquisition and\nrecognition (MSTAR) public dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:05:38 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Furukawa", "Hidetoshi", ""]]}, {"id": "1801.08577", "submitter": "Jayanta Dutta", "authors": "Jayanta K Dutta, Jiayi Liu, Unmesh Kurup and Mohak Shah", "title": "Effective Building Block Design for Deep Convolutional Neural Networks\n  using Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown promising results on many machine learning tasks but\nDL models are often complex networks with large number of neurons and layers,\nand recently, complex layer structures known as building blocks. Finding the\nbest deep model requires a combination of finding both the right architecture\nand the correct set of parameters appropriate for that architecture. In\naddition, this complexity (in terms of layer types, number of neurons, and\nnumber of layers) also present problems with generalization since larger\nnetworks are easier to overfit to the data. In this paper, we propose a search\nframework for finding effective architectural building blocks for convolutional\nneural networks (CNN). Our approach is much faster at finding models that are\nclose to state-of-the-art in performance. In addition, the models discovered by\nour approach are also smaller than models discovered by similar techniques. We\nachieve these twin advantages by designing our search space in such a way that\nit searches over a reduced set of state-of-the-art building blocks for CNNs\nincluding residual block, inception block, inception-residual block, ResNeXt\nblock and many others. We apply this technique to generate models for multiple\nimage datasets and show that these models achieve performance comparable to\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\nalso show that learned models are transferable between datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:40:44 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Dutta", "Jayanta K", ""], ["Liu", "Jiayi", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "1801.08599", "submitter": "Ling Zhang", "authors": "Zhihui Guo, Ling Zhang, Le Lu, Mohammadhadi Bagheri, Ronald M.\n  Summers, Milan Sonka, Jianhua Yao", "title": "Deep LOGISMOS: Deep Learning Graph-based 3D Segmentation of Pancreatic\n  Tumors on CT scans", "comments": "4 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports Deep LOGISMOS approach to 3D tumor segmentation by\nincorporating boundary information derived from deep contextual learning to\nLOGISMOS - layered optimal graph image segmentation of multiple objects and\nsurfaces. Accurate and reliable tumor segmentation is essential to tumor growth\nanalysis and treatment selection. A fully convolutional network (FCN), UNet, is\nfirst trained using three adjacent 2D patches centered at the tumor, providing\ncontextual UNet segmentation and probability map for each 2D patch. The UNet\nsegmentation is then refined by Gaussian Mixture Model (GMM) and morphological\noperations. The refined UNet segmentation is used to provide the initial shape\nboundary to build a segmentation graph. The cost for each node of the graph is\ndetermined by the UNet probability maps. Finally, a max-flow algorithm is\nemployed to find the globally optimal solution thus obtaining the final\nsegmentation. For evaluation, we applied the method to pancreatic tumor\nsegmentation on a dataset of 51 CT scans, among which 30 scans were used for\ntraining and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient\n(DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4%\nrespectively, both are significantly improved (p<0.05) compared with contextual\nUNet and/or LOGISMOS alone.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 21:34:44 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Guo", "Zhihui", ""], ["Zhang", "Ling", ""], ["Lu", "Le", ""], ["Bagheri", "Mohammadhadi", ""], ["Summers", "Ronald M.", ""], ["Sonka", "Milan", ""], ["Yao", "Jianhua", ""]]}, {"id": "1801.08613", "submitter": "David Hall", "authors": "David Hall, Feras Dayoub, Tristan Perez and Chris McCool", "title": "A Rapidly Deployable Classification System using Visual Data for the\n  Application of Precision Weed Management", "comments": "36 pages, 14 figures, published Computers and Electronics in\n  Agriculture Vol. 148", "journal-ref": "D. Hall, F. Dayoub, T. Perez, and C. McCool, \"A Rapidly Deployable\n  Classification System using Visual Data for the Application of Precision Weed\n  Management,\" Computers and Electronics in Agriculture, Vol. 148, pp. 107-120,\n  May 2018", "doi": "10.1016/j.compag.2018.02.023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we demonstrate a rapidly deployable weed classification system\nthat uses visual data to enable autonomous precision weeding without making\nprior assumptions about which weed species are present in a given field.\nPrevious work in this area relies on having prior knowledge of the weed species\npresent in the field. This assumption cannot always hold true for every field,\nand thus limits the use of weed classification systems based on this\nassumption. In this work, we obviate this assumption and introduce a rapidly\ndeployable approach able to operate on any field without any weed species\nassumptions prior to deployment. We present a three stage pipeline for the\nimplementation of our weed classification system consisting of initial field\nsurveillance, offline processing and selective labelling, and automated\nprecision weeding. The key characteristic of our approach is the combination of\nplant clustering and selective labelling which is what enables our system to\noperate without prior weed species knowledge. Testing using field data we are\nable to label 12.3 times fewer images than traditional full labelling whilst\nreducing classification accuracy by only 14%.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 22:00:00 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 07:36:23 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Hall", "David", ""], ["Dayoub", "Feras", ""], ["Perez", "Tristan", ""], ["McCool", "Chris", ""]]}, {"id": "1801.08614", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Youbao Tang, Le Lu, Adam P. Harrison, Ke Yan, Jing Xiao,\n  Lin Yang, Ronald M. Summers", "title": "Accurate Weakly Supervised Deep Lesion Segmentation on CT Scans:\n  Self-Paced 3D Mask Generation from RECIST", "comments": "v1: Main paper + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric lesion segmentation via medical imaging is a powerful means to\nprecisely assess multiple time-point lesion/tumor changes. Because manual 3D\nsegmentation is prohibitively time consuming and requires radiological\nexperience, current practices rely on an imprecise surrogate called response\nevaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST\nmarks are commonly found in current hospital picture and archiving systems\n(PACS), meaning they can provide a potentially powerful, yet extraordinarily\nchallenging, source of weak supervision for full 3D segmentation. Toward this\nend, we introduce a convolutional neural network based weakly supervised\nself-paced segmentation (WSSS) method to 1) generate the initial lesion\nsegmentation on the axial RECIST-slice; 2) learn the data distribution on\nRECIST-slices; 3) adapt to segment the whole volume slice by slice to finally\nobtain a volumetric segmentation. In addition, we explore how super-resolution\nimages (2~5 times beyond the physical CT imaging), generated from a proposed\nstacked generative adversarial network, can aid the WSSS performance. We employ\nthe DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735\nPACS-bookmarked findings, which include lesions, tumors, and lymph nodes of\nvarying sizes, categories, body regions and surrounding contexts. These are\ndrawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node\ndataset, where 3D ground truth masks are available for all images. For the\nDeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices\nand 76% in 3D lesion volumes. We further validate using a subjective user\nstudy, where an experienced radiologist accepted our WSSS-generated lesion\nsegmentation results with a high probability of 92.4%.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 22:05:19 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Cai", "Jinzheng", ""], ["Tang", "Youbao", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""], ["Yan", "Ke", ""], ["Xiao", "Jing", ""], ["Yang", "Lin", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1801.08616", "submitter": "Ling Zhang", "authors": "Ling Zhang, Le Lu, Isabella Nogues, Ronald M. Summers, Shaoxiong Liu,\n  Jianhua Yao", "title": "DeepPap: Deep Convolutional Networks for Cervical Cell Classification", "comments": null, "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 19 May 2017,\n  Volume: 21 Issue: 6", "doi": "10.1109/JBHI.2017.2705583", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation-assisted cervical screening via Pap smear or liquid-based cytology\n(LBC) is a highly effective cell imaging based cancer detection tool, where\ncells are partitioned into \"abnormal\" and \"normal\" categories. However, the\nsuccess of most traditional classification methods relies on the presence of\naccurate cell segmentations. Despite sixty years of research in this field,\naccurate segmentation remains a challenge in the presence of cell clusters and\npathologies. Moreover, previous classification methods are only built upon the\nextraction of hand-crafted features, such as morphology and texture. This paper\naddresses these limitations by proposing a method to directly classify cervical\ncells - without prior segmentation - based on deep features, using\nconvolutional neural networks (ConvNets). First, the ConvNet is pre-trained on\na natural image dataset. It is subsequently fine-tuned on a cervical cell\ndataset consisting of adaptively re-sampled image patches coarsely centered on\nthe nuclei. In the testing phase, aggregation is used to average the prediction\nscores of a similar set of image patches. The proposed method is evaluated on\nboth Pap smear and LBC datasets. Results show that our method outperforms\nprevious algorithms in classification accuracy (98.3%), area under the curve\n(AUC) (0.99) values, and especially specificity (98.3%), when applied to the\nHerlev benchmark Pap smear dataset and evaluated using five-fold\ncross-validation. Similar superior performances are also achieved on the HEMLBC\n(H&E stained manual LBC) dataset. Our method is promising for the development\nof automation-assisted reading systems in primary cervical screening.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 22:12:29 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Zhang", "Ling", ""], ["Lu", "Le", ""], ["Nogues", "Isabella", ""], ["Summers", "Ronald M.", ""], ["Liu", "Shaoxiong", ""], ["Yao", "Jianhua", ""]]}, {"id": "1801.08624", "submitter": "Bo Chang", "authors": "Bo Chang, Qiong Zhang, Shenyi Pan, Lili Meng", "title": "Generating Handwritten Chinese Characters using CycleGAN", "comments": "Accepted at WACV 2018", "journal-ref": null, "doi": "10.1109/WACV.2018.00028", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting of Chinese has long been an important skill in East Asia.\nHowever, automatic generation of handwritten Chinese characters poses a great\nchallenge due to the large number of characters. Various machine learning\ntechniques have been used to recognize Chinese characters, but few works have\nstudied the handwritten Chinese character generation problem, especially with\nunpaired training data. In this work, we formulate the Chinese handwritten\ncharacter generation as a problem that learns a mapping from an existing\nprinted font to a personalized handwritten style. We further propose DenseNet\nCycleGAN to generate Chinese handwritten characters. Our method is applied not\nonly to commonly used Chinese characters but also to calligraphy work with\naesthetic values. Furthermore, we propose content accuracy and style\ndiscrepancy as the evaluation metrics to assess the quality of the handwritten\ncharacters generated. We then use our proposed metrics to evaluate the\ngenerated characters from CASIA dataset as well as our newly introduced Lanting\ncalligraphy dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 22:36:05 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Chang", "Bo", ""], ["Zhang", "Qiong", ""], ["Pan", "Shenyi", ""], ["Meng", "Lili", ""]]}, {"id": "1801.08676", "submitter": "Rodrigo Santa Cruz", "authors": "Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould", "title": "Neural Algebra of Classifiers", "comments": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is fundamentally compositional, so it is natural to think of visual\nrecognition as the recognition of basic visually primitives that are composed\naccording to well-defined rules. This strategy allows us to recognize unseen\ncomplex concepts from simple visual primitives. However, the current trend in\nvisual recognition follows a data greedy approach where huge amounts of data\nare required to learn models for any desired visual concept. In this paper, we\nbuild on the compositionality principle and develop an \"algebra\" to compose\nclassifiers for complex visual concepts. To this end, we learn neural network\nmodules to perform boolean algebra operations on simple visual classifiers.\nSince these modules form a complete functional set, a classifier for any\ncomplex visual concept defined as a boolean expression of primitives can be\nobtained by recursively applying the learned modules, even if we do not have a\nsingle training sample. As our experiments show, using such a framework, we can\ncompose classifiers for complex visual concepts outperforming standard\nbaselines on two well-known visual recognition benchmarks. Finally, we present\na qualitative analysis of our method and its properties.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 05:13:10 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Cruz", "Rodrigo Santa", ""], ["Fernando", "Basura", ""], ["Cherian", "Anoop", ""], ["Gould", "Stephen", ""]]}, {"id": "1801.08706", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Mehmet Efendioglu, Caner Demirpolat", "title": "Cloud Detection From RGB Color Remote Sensing Images With Deep Pyramid\n  Networks", "comments": "Submitted to IGARSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud detection from remotely observed data is a critical pre-processing step\nfor various remote sensing applications. In particular, this problem becomes\neven harder for RGB color images, since there is no distinct spectral pattern\nfor clouds, which is directly separable from the Earth surface. In this paper,\nwe adapt a deep pyramid network (DPN) to tackle this problem. For this purpose,\nthe network is enhanced with a pre-trained parameter model at the encoder\nlayer. Moreover, the method is able to obtain accurate pixel-level segmentation\nand classification results from a set of noisy labeled RGB color images. In\norder to demonstrate the superiority of the method, we collect and label data\nwith the corresponding cloud/non-cloudy masks acquired from low-orbit Gokturk-2\nand RASAT satellites. The experimental results validates that the proposed\nmethod outperforms several baselines even for hard cases (e.g. snowy mountains)\nthat are perceptually difficult to distinguish by human eyes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 08:15:46 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Ozkan", "Savas", ""], ["Efendioglu", "Mehmet", ""], ["Demirpolat", "Caner", ""]]}, {"id": "1801.08747", "submitter": "Sebastian Sudholt", "authors": "Rene Grzeszick, Sebastian Sudholt, Gernot A. Fink", "title": "Weakly Supervised Object Detection with Pointwise Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a novel approach for weakly supervised object detection that\nincorporates pointwise mutual information is presented. A fully convolutional\nneural network architecture is applied in which the network learns one filter\nper object class. The resulting feature map indicates the location of objects\nin an image, yielding an intuitive representation of a class activation map.\nWhile traditionally such networks are learned by a softmax or binary logistic\nregression (sigmoid cross-entropy loss), a learning approach based on a cosine\nloss is introduced. A pointwise mutual information layer is incorporated in the\nnetwork in order to project predictions and ground truth presence labels in a\nnon-categorical embedding space. Thus, the cosine loss can be employed in this\nnon-categorical representation. Besides integrating image level annotations, it\nis shown how to integrate point-wise annotations using a Spatial Pyramid\nPooling layer. The approach is evaluated on the VOC2012 dataset for\nclassification, point localization and weakly supervised bounding box\nlocalization. It is shown that the combination of pointwise mutual information\nand a cosine loss eases the learning process and thus improves the accuracy.\nThe integration of coarse point-wise localizations further improves the results\nat minimal annotation costs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 10:46:43 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Grzeszick", "Rene", ""], ["Sudholt", "Sebastian", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1801.08839", "submitter": "Wenqiang Xu", "authors": "Wenqiang Xu, Yonglu Li, Cewu Lu", "title": "SRDA: Generating Instance Segmentation Annotation Via Scanning,\n  Reasoning And Domain Adaptation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is a problem of significance in computer vision.\nHowever, preparing annotated data for this task is extremely time-consuming and\ncostly. By combining the advantages of 3D scanning, reasoning, and GAN-based\ndomain adaptation techniques, we introduce a novel pipeline named SRDA to\nobtain large quantities of training samples with very minor effort. Our\npipeline is well-suited to scenes that can be scanned, i.e. most indoor and\nsome outdoor scenarios. To evaluate our performance, we build three\nrepresentative scenes and a new dataset, with 3D models of various common\nobjects categories and annotated real-world scene images. Extensive experiments\nshow that our pipeline can achieve decent instance segmentation performance\ngiven very low human labor cost.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 15:10:15 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 21:41:48 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 05:48:23 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Xu", "Wenqiang", ""], ["Li", "Yonglu", ""], ["Lu", "Cewu", ""]]}, {"id": "1801.08863", "submitter": "Gholamreza Anbarjafari", "authors": "Morteza Daneshmand, Ahmed Helmi, Egils Avots, Fatemeh Noroozi, Fatih\n  Alisinanoglu, Hasan Sait Arslan, Jelena Gorbova, Rain Eric Haamer, Cagri\n  Ozcinar, Gholamreza Anbarjafari", "title": "3D Scanning: A Comprehensive Survey", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of 3D scanning methodologies and technologies\nproposed in the existing scientific and industrial literature. Throughout the\npaper, various types of the related techniques are reviewed, which consist,\nmainly, of close-range, aerial, structure-from-motion and terrestrial\nphotogrammetry, and mobile, terrestrial and airborne laser scanning, as well as\ntime-of-flight, structured-light and phase-comparison methods, along with\ncomparative and combinational studies, the latter being intended to help make a\nclearer distinction on the relevance and reliability of the possible choices.\nMoreover, outlier detection and surface fitting procedures are discussed\nconcisely, which are necessary post-processing stages.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 00:13:15 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Daneshmand", "Morteza", ""], ["Helmi", "Ahmed", ""], ["Avots", "Egils", ""], ["Noroozi", "Fatemeh", ""], ["Alisinanoglu", "Fatih", ""], ["Arslan", "Hasan Sait", ""], ["Gorbova", "Jelena", ""], ["Haamer", "Rain Eric", ""], ["Ozcinar", "Cagri", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1801.08925", "submitter": "Mikhail Startsev", "authors": "Mikhail Startsev, Michael Dorr", "title": "Supersaliency: A Novel Pipeline for Predicting Smooth Pursuit-Based\n  Attention Improves Generalizability of Video Saliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting attention is a popular topic at the intersection of human and\ncomputer vision. However, even though most of the available video saliency data\nsets and models claim to target human observers' fixations, they fail to\ndifferentiate them from smooth pursuit (SP), a major eye movement type that is\nunique to perception of dynamic scenes. In this work, we highlight the\nimportance of SP and its prediction (which we call supersaliency, due to\ngreater selectivity compared to fixations), and aim to make its distinction\nfrom fixations explicit for computational models. To this end, we (i) use\nalgorithmic and manual annotations of SP and fixations for two well-established\nvideo saliency data sets, (ii) train Slicing Convolutional Neural Networks for\nsaliency prediction on either fixation- or SP-salient locations, and (iii)\nevaluate our and 26 publicly available dynamic saliency models on three data\nsets against traditional saliency and supersaliency ground truth. Overall, our\nmodels outperform the state of the art in both the new supersaliency and the\ntraditional saliency problem settings, for which literature models are\noptimized. Importantly, on two independent data sets, our supersaliency model\nshows greater generalization ability and outperforms all other models, even for\nfixation prediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 18:24:45 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 09:15:53 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 10:40:07 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Startsev", "Mikhail", ""], ["Dorr", "Michael", ""]]}, {"id": "1801.08926", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James\n  Storer", "title": "Deflecting Adversarial Attacks with Pixel Deflection", "comments": "Accepted to IEEE CVPR 2018 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs are poised to become integral parts of many critical systems. Despite\ntheir robustness to natural variations, image pixel values can be manipulated,\nvia small, carefully crafted, imperceptible perturbations, to cause a model to\nmisclassify images. We present an algorithm to process an image so that\nclassification accuracy is significantly preserved in the presence of such\nadversarial manipulations. Image classifiers tend to be robust to natural\nnoise, and adversarial attacks tend to be agnostic to object location. These\nobservations motivate our strategy, which leverages model robustness to defend\nagainst adversarial perturbations by forcing the image to match natural image\nstatistics. Our algorithm locally corrupts the image by redistributing pixel\nvalues via a process we term pixel deflection. A subsequent wavelet-based\ndenoising operation softens this corruption, as well as some of the adversarial\nchanges. We demonstrate experimentally that the combination of these techniques\nenables the effective recovery of the true class, against a variety of robust\nattacks. Our results compare favorably with current state-of-the-art defenses,\nwithout requiring retraining or modifying the CNN.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 18:24:59 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 16:51:42 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 20:36:53 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Prakash", "Aaditya", ""], ["Moran", "Nick", ""], ["Garber", "Solomon", ""], ["DiLillo", "Antonella", ""], ["Storer", "James", ""]]}, {"id": "1801.08981", "submitter": "Steven Hickson", "authors": "Steven Hickson, Stan Birchfield, Irfan Essa, Henrik Christensen", "title": "Efficient Hierarchical Graph-Based Segmentation of RGBD Videos", "comments": "CVPR 2014", "journal-ref": null, "doi": "10.1109/CVPR.2014.51", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient and scalable algorithm for segmenting 3D RGBD point\nclouds by combining depth, color, and temporal information using a multistage,\nhierarchical graph-based approach. Our algorithm processes a moving window over\nseveral point clouds to group similar regions over a graph, resulting in an\ninitial over-segmentation. These regions are then merged to yield a dendrogram\nusing agglomerative clustering via a minimum spanning tree algorithm. Bipartite\ngraph matching at a given level of the hierarchical tree yields the final\nsegmentation of the point clouds by maintaining region identities over\narbitrarily long periods of time. We show that a multistage segmentation with\ndepth then color yields better results than a linear combination of depth and\ncolor. Due to its incremental processing, our algorithm can process videos of\nany length and in a streaming pipeline. The algorithm's ability to produce\nrobust, efficient segmentation is demonstrated with numerous experimental\nresults on challenging sequences from our own as well as public RGBD data sets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 21:21:25 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Hickson", "Steven", ""], ["Birchfield", "Stan", ""], ["Essa", "Irfan", ""], ["Christensen", "Henrik", ""]]}, {"id": "1801.08985", "submitter": "Steven Hickson", "authors": "Steven Hickson, Anelia Angelova, Irfan Essa, Rahul Sukthankar", "title": "Object category learning and retrieval with weak supervision", "comments": "Camera-ready version for NIPS 2017 workshop Learning with Limited\n  Labeled Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of retrieving objects from image data and learning to\nclassify them into meaningful semantic categories with minimal supervision. To\nthat end, we propose a fully differentiable unsupervised deep clustering\napproach to learn semantic classes in an end-to-end fashion without individual\nclass labeling using only unlabeled object proposals. The key contributions of\nour work are 1) a kmeans clustering objective where the clusters are learned as\nparameters of the network and are represented as memory units, and 2)\nsimultaneously building a feature representation, or embedding, while learning\nto cluster it. This approach shows promising results on two popular computer\nvision datasets: on CIFAR10 for clustering objects, and on the more complex and\nchallenging Cityscapes dataset for semantically discovering classes which\nvisually correspond to cars, people, and bicycles. Currently, the only\nsupervision provided is segmentation objectness masks, but this method can be\nextended to use an unsupervised objectness-based object generation mechanism\nwhich will make the approach completely unsupervised.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 21:47:59 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 20:22:15 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Hickson", "Steven", ""], ["Angelova", "Anelia", ""], ["Essa", "Irfan", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1801.09005", "submitter": "Jianhui Chen Mr", "authors": "Jianhui Chen, Fangrui Zhu and James J. Little", "title": "A Two-point Method for PTZ Camera Calibration in Sports", "comments": "WACV 2018 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrating narrow field of view soccer cameras is challenging because there\nare very few field markings in the image. Unlike previous solutions, we propose\na two-point method, which requires only two point correspondences given the\nprior knowledge of base location and orientation of a pan-tilt-zoom (PTZ)\ncamera. We deploy this new calibration method to annotate pan-tilt-zoom data\nfrom soccer videos. The collected data are used as references for new images.\nWe also propose a fast random forest method to predict pan-tilt angles without\nimage-to-image feature matching, leading to an efficient calibration method for\nnew images. We demonstrate our system on synthetic data and two real soccer\ndatasets. Our two-point approach achieves superior performance over the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 23:37:03 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chen", "Jianhui", ""], ["Zhu", "Fangrui", ""], ["Little", "James J.", ""]]}, {"id": "1801.09041", "submitter": "Qing Li", "authors": "Qing Li, Jianlong Fu, Dongfei Yu, Tao Mei, Jiebo Luo", "title": "Tell-and-Answer: Towards Explainable Visual Question Answering using\n  Attributes and Captions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has attracted attention from both computer\nvision and natural language processing communities. Most existing approaches\nadopt the pipeline of representing an image via pre-trained CNNs, and then\nusing the uninterpretable CNN features in conjunction with the question to\npredict the answer. Although such end-to-end models might report promising\nperformance, they rarely provide any insight, apart from the answer, into the\nVQA process. In this work, we propose to break up the end-to-end VQA into two\nsteps: explaining and reasoning, in an attempt towards a more explainable VQA\nby shedding light on the intermediate results between these two steps. To that\nend, we first extract attributes and generate descriptions as explanations for\nan image using pre-trained attribute detectors and image captioning models,\nrespectively. Next, a reasoning module utilizes these explanations in place of\nthe image to infer an answer to the question. The advantages of such a\nbreakdown include: (1) the attributes and captions can reflect what the system\nextracts from the image, thus can provide some explanations for the predicted\nanswer; (2) these intermediate results can help us identify the inabilities of\nboth the image understanding part and the answer inference part when the\npredicted answer is wrong. We conduct extensive experiments on a popular VQA\ndataset and dissect all results according to several measurements of the\nexplanation quality. Our system achieves comparable performance with the\nstate-of-the-art, yet with added benefits of explainability and the inherent\nability to further improve with higher quality explanations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 05:34:37 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Li", "Qing", ""], ["Fu", "Jianlong", ""], ["Yu", "Dongfei", ""], ["Mei", "Tao", ""], ["Luo", "Jiebo", ""]]}, {"id": "1801.09042", "submitter": "Yipin Zhou", "authors": "Yipin Zhou and Yale Song and Tamara L. Berg", "title": "Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks", "comments": "WACV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a still photograph, one can imagine how dynamic objects might move\nagainst a static background. This idea has been actualized in the form of\ncinemagraphs, where the motion of particular objects within a still image is\nrepeated, giving the viewer a sense of animation. In this paper, we learn\ncomputational models that can generate cinemagraph sequences automatically\ngiven a single image. To generate cinemagraphs, we explore combining generative\nmodels with a recurrent neural network and deep Q-networks to enhance the power\nof sequence generation. To enable and evaluate these models we make use of two\ndatasets, one synthetically generated and the other containing real video\ngenerated cinemagraphs. Both qualitative and quantitative evaluations\ndemonstrate the effectiveness of our models on the synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 05:48:20 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Zhou", "Yipin", ""], ["Song", "Yale", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1801.09054", "submitter": "Umit Kacar", "authors": "Umit Kacar and Murvet Kirci", "title": "Ear Recognition With Score-Level Fusion Based On CMC In Long-Wave\n  Infrared Spectrum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Only a few studies have been reported regarding human ear recognition in long\nwave infrared band. Thus, we have created ear database based on long wave\ninfrared band. We have called that the database is long wave infrared band\nMIDAS consisting of 2430 records of 81 subjects. Thermal band provides seamless\noperation both night and day, robust against spoofing with understanding live\near and invariant to illumination conditions for human ear recognition. We have\nproposed to use different algorithms to reveal the distinctive features. Then,\nwe have reduced the number of dimensions using subspace methods. Finally, the\ndimension of data is reduced in accordance with the classifier methods. After\nthis, the decision is determined by the best sores or combining some of the\nbest scores with matching fusion. The results have showed that the fusion\ntechnique was successful. We have reached 97.71% for rank-1 with 567 test\nprobes. Furthermore, we have defined the perfect rank which is rank number when\nrecognition rate reaches 100% in cumulative matching curve. This evaluation is\nimportant for especially forensics, for example corpse identification, criminal\ninvestigation etc.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 08:41:54 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kacar", "Umit", ""], ["Kirci", "Murvet", ""]]}, {"id": "1801.09056", "submitter": "Umit Kacar", "authors": "Cihan Akin, Umit Kacar, Murvet Kirci", "title": "A Multi-Biometrics for Twins Identification Based Speech and Ear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The development of technology biometrics becomes crucial more. To define\nhuman characteristic biometric systems are used but because of inability of\ntraditional biometric systems to recognize twins, multimodal biometric systems\nare developed. In this study a multimodal biometric recognition system is\nproposed to recognize twins from each other and from the other people by using\nimage and speech data. The speech or image data can be enough to recognize\npeople from each other but twins cannot be distinguished with one of these\ndata. Therefore a robust recognition system with the combine of speech and ear\nimages is needed. As database, the photos and speech data of 39 twins are used.\nFor speech recognition MFCC and DTW algorithms are used. Also, Gabor filter and\nDCVA algorithms are used for ear identification. Multi-biometrics success rate\nis increased by making matching score level fusion. Especially, rank-5 is\nreached 100%. We think that speech and ear can be complementary. Therefore, it\nis result that multi-biometrics based speech and ear is effective for human\nidentifications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 08:47:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Akin", "Cihan", ""], ["Kacar", "Umit", ""], ["Kirci", "Murvet", ""]]}, {"id": "1801.09057", "submitter": "Pei Guo", "authors": "Pei Guo, Ryan Farrell", "title": "Aligned to the Object, not to the Image: A Unified Pose-aligned\n  Representation for Fine-grained Recognition", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic appearance variation due to pose constitutes a great challenge in\nfine-grained recognition, one which recent methods using attention mechanisms\nor second-order statistics fail to adequately address. Modern CNNs typically\nlack an explicit understanding of object pose and are instead confused by\nentangled pose and appearance. In this paper, we propose a unified object\nrepresentation built from a hierarchy of pose-aligned regions. Rather than\nrepresenting an object by regions aligned to image axes, the proposed\nrepresentation characterizes appearance relative to the object's pose using\npose-aligned patches whose features are robust to variations in pose, scale and\nrotation. We propose an algorithm that performs pose estimation and forms the\nunified object representation as the concatenation of hierarchical pose-aligned\nregions features, which is then fed into a classification network. The proposed\nalgorithm surpasses the performance of other approaches, increasing the\nstate-of-the-art by nearly 2% on the widely-used CUB-200 dataset and by more\nthan 8% on the much larger NABirds dataset. The effectiveness of this paradigm\nrelative to competing methods suggests the critical importance of disentangling\npose and appearance for continued progress in fine-grained recognition.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 08:53:29 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:50:59 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 06:10:19 GMT"}, {"version": "v4", "created": "Tue, 11 Sep 2018 03:06:32 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Guo", "Pei", ""], ["Farrell", "Ryan", ""]]}, {"id": "1801.09083", "submitter": "Yi Xiao", "authors": "Yi Xiao, Peiyao Zhou, Yan Zheng", "title": "Interactive Deep Colorization With Simultaneous Global and Local Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorization methods using deep neural networks have become a recent trend.\nHowever, most of them do not allow user inputs, or only allow limited user\ninputs (only global inputs or only local inputs), to control the output\ncolorful images. The possible reason is that it's difficult to differentiate\nthe influence of different kind of user inputs in network training. To solve\nthis problem, we present a novel deep colorization method, which allows\nsimultaneous global and local inputs to better control the output colorized\nimages. The key step is to design an appropriate loss function that can\ndifferentiate the influence of input data, global inputs and local inputs. With\nthis design, our method accepts no inputs, or global inputs, or local inputs,\nor both global and local inputs, which is not supported in previous deep\ncolorization methods. In addition, we propose a global color theme\nrecommendation system to help users determine global inputs. Experimental\nresults shows that our methods can better control the colorized images and\ngenerate state-of-art results.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 12:36:31 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Xiao", "Yi", ""], ["Zhou", "Peiyao", ""], ["Zheng", "Yan", ""]]}, {"id": "1801.09086", "submitter": "Ashish Mishra", "authors": "Ashish Mishra, Vinay Kumar Verma, M Shiva Krishna Reddy, Arulkumar S,\n  Piyush Rai, and Anurag Mittal", "title": "A Generative Approach to Zero-Shot and Few-Shot Action Recognition", "comments": "Accepted in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a generative framework for zero-shot action recognition where some\nof the possible action classes do not occur in the training data. Our approach\nis based on modeling each action class using a probability distribution whose\nparameters are functions of the attribute vector representing that action\nclass. In particular, we assume that the distribution parameters for any action\nclass in the visual space can be expressed as a linear combination of a set of\nbasis vectors where the combination weights are given by the attributes of the\naction class. These basis vectors can be learned solely using labeled data from\nthe known (i.e., previously seen) action classes, and can then be used to\npredict the parameters of the probability distributions of unseen action\nclasses. We consider two settings: (1) Inductive setting, where we use only the\nlabeled examples of the seen action classes to predict the unseen action class\nparameters; and (2) Transductive setting which further leverages unlabeled data\nfrom the unseen action classes. Our framework also naturally extends to\nfew-shot action recognition where a few labeled examples from unseen classes\nare available. Our experiments on benchmark datasets (UCF101, HMDB51 and\nOlympic) show significant performance improvements as compared to various\nbaselines, in both standard zero-shot (disjoint seen and unseen classes) and\ngeneralized zero-shot learning settings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 13:13:29 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Mishra", "Ashish", ""], ["Verma", "Vinay Kumar", ""], ["Reddy", "M Shiva Krishna", ""], ["S", "Arulkumar", ""], ["Rai", "Piyush", ""], ["Mittal", "Anurag", ""]]}, {"id": "1801.09092", "submitter": "Behnaz Nojavanasghari", "authors": "Behnaz Nojavanasghari, Yuchi Huang, Saad Khan", "title": "Interactive Generative Adversarial Networks for Facial Expression\n  Generation in Dyadic Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A social interaction is a social exchange between two or more\nindividuals,where individuals modify and adjust their behaviors in response to\ntheir interaction partners. Our social interactions are one of most fundamental\naspects of our lives and can profoundly affect our mood, both positively and\nnegatively. With growing interest in virtual reality and avatar-mediated\ninteractions,it is desirable to make these interactions natural and human like\nto promote positive effect in the interactions and applications such as\nintelligent tutoring systems, automated interview systems and e-learning. In\nthis paper, we propose a method to generate facial behaviors for an agent.\nThese behaviors include facial expressions and head pose and they are generated\nconsidering the users affective state. Our models learn semantically meaningful\nrepresentations of the face and generate appropriate and temporally smooth\nfacial behaviors in dyadic interactions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 14:01:17 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 19:02:43 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Nojavanasghari", "Behnaz", ""], ["Huang", "Yuchi", ""], ["Khan", "Saad", ""]]}, {"id": "1801.09097", "submitter": "Yifei Fan", "authors": "Yifei Fan and Anthony Yezzi", "title": "Towards an Understanding of Neural Networks in Natural-Image Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major uncertainties, dataset bias and adversarial examples, prevail in\nstate-of-the-art AI algorithms with deep neural networks. In this paper, we\npresent an intuitive explanation for these issues as well as an interpretation\nof the performance of deep networks in a natural-image space. The explanation\nconsists of two parts: the philosophy of neural networks and a hypothetical\nmodel of natural-image spaces. Following the explanation, we 1) demonstrate\nthat the values of training samples differ, 2) provide incremental boost to the\naccuracy of a CIFAR-10 classifier by introducing an additional \"random-noise\"\ncategory during training, 3) alleviate over-fitting thereby enhancing the\nrobustness against adversarial examples by detecting and excluding illusive\ntraining samples that are consistently misclassified. Our overall contribution\nis therefore twofold. First, while most existing algorithms treat data equally\nand have a strong appetite for more data, we demonstrate in contrast that an\nindividual datum can sometimes have disproportionate and counterproductive\ninfluence and that it is not always better to train neural networks with more\ndata. Next, we consider more thoughtful strategies by taking into account the\ngeometric and topological properties of natural-image spaces to which deep\nnetworks are applied.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 14:41:59 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 05:18:00 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Fan", "Yifei", ""], ["Yezzi", "Anthony", ""]]}, {"id": "1801.09103", "submitter": "Marco Godi", "authors": "Marco Carletti, Marco Godi, Maedeh Aghaei, Francesco Giuliari, Marco\n  Cristani", "title": "Understanding Deep Architectures by Visual Summaries", "comments": "Project page and code available at\n  http://marcocarletti.altervista.org/publications/understanding-visual-summaries/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, visualization techniques extract the salient patterns\nexploited by deep networks for image classification, focusing on single images;\nno effort has been spent in investigating whether these patterns are\nsystematically related to precise semantic entities over multiple images\nbelonging to a same class, thus failing to capture the very understanding of\nthe image class the network has realized. This paper goes in this direction,\npresenting a visualization framework which produces a group of clusters or\nsummaries, each one formed by crisp salient image regions focusing on a\nparticular part that the network has exploited with high regularity to decide\nfor a given class. The approach is based on a sparse optimization step\nproviding sharp image saliency masks that are clustered together by means of a\nsemantic flow similarity measure. The summaries communicate clearly what a\nnetwork has exploited of a particular image class, and this is proved through\nautomatic image tagging and with a user study. Beyond the deep network\nunderstanding, summaries are also useful for many quantitative reasons: their\nnumber is correlated with ability of a network to classify (more summaries,\nbetter performances), and they can be used to improve the classification\naccuracy of a network through summary-driven specializations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 15:26:07 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 14:49:35 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 11:32:25 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Carletti", "Marco", ""], ["Godi", "Marco", ""], ["Aghaei", "Maedeh", ""], ["Giuliari", "Francesco", ""], ["Cristani", "Marco", ""]]}, {"id": "1801.09108", "submitter": "Chengjiang Long", "authors": "Chengjiang Long, Roddy Collins, Eran Swears, Anthony Hoogs", "title": "Deep Neural Networks In Fully Connected CRF For Image Labeling With\n  Social Network Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for predicting image labels by fusing image content\ndescriptors with the social media context of each image. An image uploaded to a\nsocial media site such as Flickr often has meaningful, associated information,\nsuch as comments and other images the user has uploaded, that is complementary\nto pixel content and helpful in predicting labels. Prediction challenges such\nas ImageNet~\\cite{imagenet_cvpr09} and MSCOCO~\\cite{LinMBHPRDZ:ECCV14} use only\npixels, while other methods make predictions purely from social media context\n\\cite{McAuleyECCV12}. Our method is based on a novel fully connected\nConditional Random Field (CRF) framework, where each node is an image, and\nconsists of two deep Convolutional Neural Networks (CNN) and one Recurrent\nNeural Network (RNN) that model both textual and visual node/image information.\nThe edge weights of the CRF graph represent textual similarity and link-based\nmetadata such as user sets and image groups. We model the CRF as an RNN for\nboth learning and inference, and incorporate the weighted ranking loss and\ncross entropy loss into the CRF parameter optimization to handle the training\ndata imbalance issue. Our proposed approach is evaluated on the MIR-9K dataset\nand experimentally outperforms current state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 16:13:22 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Long", "Chengjiang", ""], ["Collins", "Roddy", ""], ["Swears", "Eran", ""], ["Hoogs", "Anthony", ""]]}, {"id": "1801.09111", "submitter": "Binghui Wang", "authors": "Binghui Wang, Chuang Lin", "title": "Robust Multi-subspace Analysis Using Novel Column L0-norm Constrained\n  Matrix Factorization", "comments": "13 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the underlying structure of data (approximately) generated from a\nunion of independent subspaces. Traditional methods learn only one subspace,\nfailing to discover the multi-subspace structure, while state-of-the-art\nmethods analyze the multi-subspace structure using data themselves as the\ndictionary, which cannot offer the explicit basis to span each subspace and are\nsensitive to errors via an indirect representation. Additionally, they also\nsuffer from a high computational complexity, being quadratic or cubic to the\nsample size. To tackle all these problems, we propose a method, called Matrix\nFactorization with Column L0-norm constraint (MFC0), that can simultaneously\nlearn the basis for each subspace, generate a direct sparse representation for\neach data sample, as well as removing errors in the data in an efficient way.\nFurthermore, we develop a first-order alternating direction algorithm, whose\ncomputational complexity is linear to the sample size, to stably and\neffectively solve the nonconvex objective function and non- smooth l0-norm\nconstraint of MFC0. Experimental results on both synthetic and real-world\ndatasets demonstrate that besides the superiority over traditional and\nstate-of-the-art methods for subspace clustering, data reconstruction, error\ncorrection, MFC0 also shows its uniqueness for multi-subspace basis learning\nand direct sparse representation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 17:00:02 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Wang", "Binghui", ""], ["Lin", "Chuang", ""]]}, {"id": "1801.09128", "submitter": "?tefan S\\u{a}ftescu", "authors": "Michael Tanner, Stefan Saftescu, Alex Bewley, Paul Newman (Oxford\n  Robotics Institute)", "title": "Meshed Up: Learnt Error Correction in 3D Reconstructions", "comments": "Accepted for the International Conference on Robotics and Automation\n  (ICRA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense reconstructions often contain errors that prior work has so far\nminimised using high quality sensors and regularising the output. Nevertheless,\nerrors still persist. This paper proposes a machine learning technique to\nidentify errors in three dimensional (3D) meshes. Beyond simply identifying\nerrors, our method quantifies both the magnitude and the direction of depth\nestimate errors when viewing the scene. This enables us to improve the\nreconstruction accuracy.\n  We train a suitably deep network architecture with two 3D meshes: a\nhigh-quality laser reconstruction, and a lower quality stereo image\nreconstruction. The network predicts the amount of error in the lower quality\nreconstruction with respect to the high-quality one, having only view the\nformer through its input. We evaluate our approach by correcting\ntwo-dimensional (2D) inverse-depth images extracted from the 3D model, and show\nthat our method improves the quality of these depth reconstructions by up to a\nrelative 10% RMSE.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 19:38:21 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Tanner", "Michael", "", "Oxford\n  Robotics Institute"], ["Saftescu", "Stefan", "", "Oxford\n  Robotics Institute"], ["Bewley", "Alex", "", "Oxford\n  Robotics Institute"], ["Newman", "Paul", "", "Oxford\n  Robotics Institute"]]}, {"id": "1801.09184", "submitter": "Yancheng Bai", "authors": "Yancheng Bai, Huijuan Xu, Kate Saenko, Bernard Ghanem", "title": "Contextual Multi-Scale Region Convolutional 3D Network for Activity\n  Detection", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity detection is a fundamental problem in computer vision. Detecting\nactivities of different temporal scales is particularly challenging. In this\npaper, we propose the contextual multi-scale region convolutional 3D network\n(CMS-RC3D) for activity detection. To deal with the inherent temporal scale\nvariability of activity instances, the temporal feature pyramid is used to\nrepresent activities of different temporal scales. On each level of the\ntemporal feature pyramid, an activity proposal detector and an activity\nclassifier are learned to detect activities of specific temporal scales.\nTemporal contextual information is fused into activity classifiers for better\nrecognition. More importantly, the entire model at all levels can be trained\nend-to-end. Our CMS-RC3D detector can deal with activities at all temporal\nscale ranges with only a single pass through the backbone network. We test our\ndetector on two public activity detection benchmarks, THUMOS14 and ActivityNet.\nExtensive experiments show that the proposed CMS-RC3D detector outperforms\nstate-of-the-art methods on THUMOS14 by a substantial margin and achieves\ncomparable results on ActivityNet despite using a shallow feature extractor.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 05:46:01 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Bai", "Yancheng", ""], ["Xu", "Huijuan", ""], ["Saenko", "Kate", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1801.09195", "submitter": "Hyunjung Shim Dr.", "authors": "Duhyeon Bang, Hyunjung Shim", "title": "Improved Training of Generative Adversarial Networks Using\n  Representative Features", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of generative adversarial networks (GANs) for image\ngeneration, the trade-off between visual quality and image diversity remains a\nsignificant issue. This paper achieves both aims simultaneously by improving\nthe stability of training GANs. The key idea of the proposed approach is to\nimplicitly regularize the discriminator using representative features. Focusing\non the fact that standard GAN minimizes reverse Kullback-Leibler (KL)\ndivergence, we transfer the representative feature, which is extracted from the\ndata distribution using a pre-trained autoencoder (AE), to the discriminator of\nstandard GANs. Because the AE learns to minimize forward KL divergence, our GAN\ntraining with representative features is influenced by both reverse and forward\nKL divergence. Consequently, the proposed approach is verified to improve\nvisual quality and diversity of state of the art GANs using extensive\nevaluations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 08:28:20 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 06:18:25 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 01:27:09 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Bang", "Duhyeon", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1801.09238", "submitter": "Saptarshi Das", "authors": "Saptarshi Das, Kaushik Halder, and Amitava Gupta", "title": "Performance Analysis of Robust Stable PID Controllers Using Dominant\n  Pole Placement for SOPTD Process Models", "comments": "50 pages, 42 figures, Knowledge-Based Systems, 2018", "journal-ref": null, "doi": "10.1016/j.knosys.2018.01.030", "report-no": null, "categories": "cs.SY cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives new formulations for designing dominant pole placement\nbased proportional-integral-derivative (PID) controllers to handle second order\nprocesses with time delays (SOPTD). Previously, similar attempts have been made\nfor pole placement in delay-free systems. The presence of the time delay term\nmanifests itself as a higher order system with variable number of interlaced\npoles and zeros upon Pade approximation, which makes it difficult to achieve\nprecise pole placement control. We here report the analytical expressions to\nconstrain the closed loop dominant and non-dominant poles at the desired\nlocations in the complex s-plane, using a third order Pade approximation for\nthe delay term. However, invariance of the closed loop performance with\ndifferent time delay approximation has also been verified using increasing\norder of Pade, representing a closed to reality higher order delay dynamics.\nThe choice of the nature of non-dominant poles e.g. all being complex, real or\na combination of them modifies the characteristic equation and influences the\nachievable stability regions. The effect of different types of non-dominant\npoles and the corresponding stability regions are obtained for nine test-bench\nprocesses indicating different levels of open-loop damping and lag to delay\nratio. Next, we investigate which expression yields a wider stability region in\nthe design parameter space by using Monte Carlo simulations while uniformly\nsampling a chosen design parameter space. Various time and frequency domain\ncontrol performance parameters are investigated next, as well as their\ndeviations with uncertain process parameters, using thousands of Monte Carlo\nsimulations, around the robust stable solution for each of the nine test-bench\nprocesses.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 14:32:28 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Das", "Saptarshi", ""], ["Halder", "Kaushik", ""], ["Gupta", "Amitava", ""]]}, {"id": "1801.09242", "submitter": "Hongwen Zhang", "authors": "Hongwen Zhang, Qi Li, Zhenan Sun", "title": "Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark\n  Localization", "comments": "Code available at https://github.com/HongwenZhang/JVCR-3Dlandmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face shape is more expressive and viewpoint-consistent than its 2D\ncounterpart. However, 3D facial landmark localization in a single image is\nchallenging due to the ambiguous nature of landmarks under 3D perspective.\nExisting approaches typically adopt a suboptimal two-step strategy, performing\n2D landmark localization followed by depth estimation. In this paper, we\npropose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial\nlandmark localization, addressing it more effectively in an end-to-end fashion.\nFirst, a compact volumetric representation is proposed to encode the per-voxel\nlikelihood of positions being the 3D landmarks. The dimensionality of such a\nrepresentation is fixed regardless of the number of target landmarks, so that\nthe curse of dimensionality could be avoided. Then, a stacked hourglass network\nis adopted to estimate the volumetric representation from coarse to fine,\nfollowed by a 3D convolution network that takes the estimated volume as input\nand regresses 3D coordinates of the face shape. In this way, the 3D structural\nconstraints between landmarks could be learned by the neural network in a more\nefficient manner. Moreover, the proposed pipeline enables end-to-end training\nand improves the robustness and accuracy of 3D facial landmark localization.\nThe effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D\ndatasets. Experimental results show that the proposed method achieves\nstate-of-the-art performance in comparison with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 15:34:16 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Zhang", "Hongwen", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""]]}, {"id": "1801.09321", "submitter": "Saikat Roy", "authors": "Arindam Das, Saikat Roy, Ujjwal Bhattacharya, Swapan Kumar Parui", "title": "Document Image Classification with Intra-Domain Transfer Learning and\n  Stacked Generalization of Deep Convolutional Neural Networks", "comments": "Accepted in 24th International Conference in Pattern Recognition\n  (ICPR), Beijing, China, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a region-based Deep Convolutional Neural Network framework is\nproposed for document structure learning. The contribution of this work\ninvolves efficient training of region based classifiers and effective\nensembling for document image classification. A primary level of `inter-domain'\ntransfer learning is used by exporting weights from a pre-trained VGG16\narchitecture on the ImageNet dataset to train a document classifier on whole\ndocument images. Exploiting the nature of region based influence modelling, a\nsecondary level of `intra-domain' transfer learning is used for rapid training\nof deep learning models for image segments. Finally, stacked generalization\nbased ensembling is utilized for combining the predictions of the base deep\nneural network models. The proposed method achieves state-of-the-art accuracy\nof 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks\nset by existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 00:03:54 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 17:03:21 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 21:54:44 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Das", "Arindam", ""], ["Roy", "Saikat", ""], ["Bhattacharya", "Ujjwal", ""], ["Parui", "Swapan Kumar", ""]]}, {"id": "1801.09335", "submitter": "Jason Kuen", "authors": "Jason Kuen, Xiangfei Kong, Zhe Lin, Gang Wang, Jianxiong Yin, Simon\n  See, Yap-Peng Tan", "title": "Stochastic Downsampling for Cost-Adjustable Inference and Improved\n  Regularization in Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable to train convolutional networks (CNNs) to run more\nefficiently during inference. In many cases however, the computational budget\nthat the system has for inference cannot be known beforehand during training,\nor the inference budget is dependent on the changing real-time resource\navailability. Thus, it is inadequate to train just inference-efficient CNNs,\nwhose inference costs are not adjustable and cannot adapt to varied inference\nbudgets. We propose a novel approach for cost-adjustable inference in CNNs -\nStochastic Downsampling Point (SDPoint). During training, SDPoint applies\nfeature map downsampling to a random point in the layer hierarchy, with a\nrandom downsampling ratio. The different stochastic downsampling configurations\nknown as SDPoint instances (of the same model) have computational costs\ndifferent from each other, while being trained to minimize the same prediction\nloss. Sharing network parameters across different instances provides\nsignificant regularization boost. During inference, one may handpick a SDPoint\ninstance that best fits the inference budget. The effectiveness of SDPoint, as\nboth a cost-adjustable inference approach and a regularizer, is validated\nthrough extensive experiments on image classification.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 01:16:03 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kuen", "Jason", ""], ["Kong", "Xiangfei", ""], ["Lin", "Zhe", ""], ["Wang", "Gang", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""], ["Tan", "Yap-Peng", ""]]}, {"id": "1801.09343", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Aswin C. Sankaranarayanan", "title": "KRISM --- Krylov Subspace-based Optical Computing of Hyperspectral\n  Images", "comments": "14 pages of main paper and 15 pages of supplementary material", "journal-ref": "Vishwanath Saragadam and Aswin C. Sankaranarayanan, \"KRISM ---\n  Krylov Subspace-based Optical Computing of Hyperspectral Images\", ACM Trans.\n  Graphics 38, 5 (2019), 148:1-14", "doi": "10.1145/3345553", "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive imaging technique that optically computes a low-rank\napproximation of a scene's hyperspectral image, conceptualized as a matrix.\nCentral to the proposed technique is the optical implementation of two\nmeasurement operators: a spectrally-coded imager and a spatially-coded\nspectrometer. By iterating between the two operators, we show that the top\nsingular vectors and singular values of a hyperspectral image can be adaptively\nand optically computed with only a few iterations. We present an optical design\nthat uses pupil plane coding for implementing the two operations and show\nseveral compelling results using a lab prototype to demonstrate the\neffectiveness of the proposed hyperspectral imager.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 12:10:38 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 03:11:34 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 05:38:39 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 18:17:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1801.09356", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Shiv Surya, Trisha Mittal and Venkatesh\n  Babu Radhakrishnan", "title": "Game of Sketches: Deep Recurrent Models of Pictionary-style Word\n  Guessing", "comments": "To be presented at AAAI-2018. Code, pre-trained models and dataset at\n  github.com/val-iisc/sketchguess", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of intelligent agents to play games in human-like fashion is\npopularly considered a benchmark of progress in Artificial Intelligence.\nSimilarly, performance on multi-disciplinary tasks such as Visual Question\nAnswering (VQA) is considered a marker for gauging progress in Computer Vision.\nIn our work, we bring games and VQA together. Specifically, we introduce the\nfirst computational model aimed at Pictionary, the popular word-guessing social\ngame. We first introduce Sketch-QA, an elementary version of Visual Question\nAnswering task. Styled after Pictionary, Sketch-QA uses incrementally\naccumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves\nasking a fixed question (\"What object is being drawn?\") and gathering\nopen-ended guess-words from human guessers. We analyze the resulting dataset\nand present many interesting findings therein. To mimic Pictionary-style\nguessing, we subsequently propose a deep neural model which generates\nguess-words in response to temporally evolving human-drawn sketches. Our model\neven makes human-like mistakes while guessing, thus amplifying the human\nmimicry factor. We evaluate our model on the large-scale guess-word dataset\ngenerated via Sketch-QA task and compare with various baselines. We also\nconduct a Visual Turing Test to obtain human impressions of the guess-words\ngenerated by humans and our model. Experimental results demonstrate the promise\nof our approach for Pictionary and similarly themed games.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 03:54:03 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Surya", "Shiv", ""], ["Mittal", "Trisha", ""], ["Radhakrishnan", "Venkatesh Babu", ""]]}, {"id": "1801.09360", "submitter": "Mustansar Fiaz", "authors": "Mustansar Fiaz, Sajid Javed, Arif Mahmood, Soon Ki Jung", "title": "Comparative Study of ECO and CFNet Trackers in Noisy Environment", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is one of the most challenging task and has secured\nsignificant attention of computer vision researchers in the past two decades.\nRecent deep learning based trackers have shown good performance on various\ntracking challenges. A tracking method should track objects in sequential\nframes accurately in challenges such as deformation, low resolution, occlusion,\nscale and light variations. Most trackers achieve good performance on specific\nchallenges instead of all tracking problems, hence there is a lack of general\npurpose tracking algorithms that can perform well in all conditions. Moreover,\nperformance of tracking techniques has not been evaluated in noisy\nenvironments. Visual object tracking has real world applications and there is\ngood chance that noise may get added during image acquisition in surveillance\ncameras. We aim to study the robustness of two state of the art trackers in the\npresence of noise including Efficient Convolutional Operators (ECO) and\nCorrelation Filter Network (CFNet). Our study demonstrates that the performance\nof these trackers degrades as the noise level increases, which demonstrate the\nneed to design more robust tracking algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 04:56:32 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Fiaz", "Mustansar", ""], ["Javed", "Sajid", ""], ["Mahmood", "Arif", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1801.09392", "submitter": "Xiaoming Li", "authors": "Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, Shiguang Shan", "title": "Shift-Net: Image Inpainting via Deep Feature Rearrangement", "comments": "25 pages, 17 figures, 1 table, main paper + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks (CNNs) have exhibited their potential in image\ninpainting for producing plausible results. However, in most existing methods,\ne.g., context encoder, the missing parts are predicted by propagating the\nsurrounding convolutional features through a fully connected layer, which\nintends to produce semantically plausible but blurry result. In this paper, we\nintroduce a special shift-connection layer to the U-Net architecture, namely\nShift-Net, for filling in missing regions of any shape with sharp structures\nand fine-detailed textures. To this end, the encoder feature of the known\nregion is shifted to serve as an estimation of the missing parts. A guidance\nloss is introduced on decoder feature to minimize the distance between the\ndecoder feature after fully connected layer and the ground-truth encoder\nfeature of the missing parts. With such constraint, the decoder feature in\nmissing region can be used to guide the shift of encoder feature in known\nregion. An end-to-end learning algorithm is further developed to train the\nShift-Net. Experiments on the Paris StreetView and Places datasets demonstrate\nthe efficiency and effectiveness of our Shift-Net in producing sharper,\nfine-detailed, and visually plausible results. The codes and pre-trained models\nare available at https://github.com/Zhaoyi-Yan/Shift-Net.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 08:13:49 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 12:43:24 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Yan", "Zhaoyi", ""], ["Li", "Xiaoming", ""], ["Li", "Mu", ""], ["Zuo", "Wangmeng", ""], ["Shan", "Shiguang", ""]]}, {"id": "1801.09414", "submitter": "Zhifeng Li", "authors": "Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao\n  Zhou, Zhifeng Li, Wei Liu", "title": "CosFace: Large Margin Cosine Loss for Deep Face Recognition", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has made extraordinary progress owing to the advancement of\ndeep convolutional neural networks (CNNs). The central task of face\nrecognition, including face verification and identification, involves face\nfeature discrimination. However, the traditional softmax loss of deep CNNs\nusually lacks the power of discrimination. To address this problem, recently\nseveral loss functions such as center loss, large margin softmax loss, and\nangular softmax loss have been proposed. All these improved losses share the\nsame idea: maximizing inter-class variance and minimizing intra-class variance.\nIn this paper, we propose a novel loss function, namely large margin cosine\nloss (LMCL), to realize this idea from a different perspective. More\nspecifically, we reformulate the softmax loss as a cosine loss by $L_2$\nnormalizing both features and weight vectors to remove radial variations, based\non which a cosine margin term is introduced to further maximize the decision\nmargin in the angular space. As a result, minimum intra-class variance and\nmaximum inter-class variance are achieved by virtue of normalization and cosine\ndecision margin maximization. We refer to our model trained with LMCL as\nCosFace. Extensive experimental evaluations are conducted on the most popular\npublic-domain face recognition datasets such as MegaFace Challenge, Youtube\nFaces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art\nperformance on these benchmarks, which confirms the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 09:23:55 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 11:15:57 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Yitong", ""], ["Zhou", "Zheng", ""], ["Ji", "Xing", ""], ["Gong", "Dihong", ""], ["Zhou", "Jingchao", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""]]}, {"id": "1801.09436", "submitter": "Mohammad Amin Shabani", "authors": "Mohammad Amin Shabani, Laleh Samadfam, Mohammad Amin Sadeghi", "title": "Local Visual Microphones: Improved Sound Extraction from Silent Video", "comments": "Accepted to BMVC 2017", "journal-ref": null, "doi": "10.5244/C.31.102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sound waves cause small vibrations in nearby objects. A few techniques exist\nin the literature that can extract sound from video. In this paper we study\nlocal vibration patterns at different image locations. We show that different\nlocations in the image vibrate differently. We carefully aggregate local\nvibrations and produce a sound quality that improves state-of-the-art. We show\nthat local vibrations could have a time delay because sound waves take time to\ntravel through the air. We use this phenomenon to estimate sound direction. We\nalso present a novel algorithm that speeds up sound extraction by two to three\norders of magnitude and reaches real-time performance in a 20KHz video.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 10:29:43 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Shabani", "Mohammad Amin", ""], ["Samadfam", "Laleh", ""], ["Sadeghi", "Mohammad Amin", ""]]}, {"id": "1801.09449", "submitter": "Mattias Heinrich", "authors": "Mattias P. Heinrich, Max Blendowski and Ozan Oktay", "title": "TernaryNet: Faster Deep Model Inference without GPUs for Medical 3D\n  Segmentation using Sparse and Binary Convolutions", "comments": null, "journal-ref": "International Journal of Computer Assisted Radiology and Surgery\n  2018", "doi": "10.1007/s11548-018-1797-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) are currently ubiquitous in medical\nimaging. While their versatility and high quality results for common image\nanalysis tasks including segmentation, localisation and prediction is\nastonishing, the large representational power comes at the cost of highly\ndemanding computational effort. This limits their practical applications for\nimage guided interventions and diagnostic (point-of-care) support using mobile\ndevices without graphics processing units (GPU). We propose a new scheme that\napproximates both trainable weights and neural activations in deep networks by\nternary values and tackles the open question of backpropagation when dealing\nwith non-differentiable functions. Our solution enables the removal of the\nexpensive floating-point matrix multiplications throughout any convolutional\nneural network and replaces them by energy and time preserving binary operators\nand population counts. Our approach, which is demonstrated using a\nfully-convolutional network (FCN) for CT pancreas segmentation leads to more\nthan 10-fold reduced memory requirements and we provide a concept for\nsub-second inference without GPUs. Our ternary approximation obtains high\naccuracies (without any post-processing) with a Dice overlap of 71.0% that are\nstatistically equivalent to using networks with high-precision weights and\nactivations. We further demonstrate the significant improvements reached in\ncomparison to binary quantisation and without our proposed ternary hyperbolic\ntangent continuation. We present a key enabling technique for highly efficient\nDCNN inference without GPUs that will help to bring the advances of deep\nlearning to practical clinical applications. It has also great promise for\nimproving accuracies in large-scale medical data retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 11:13:43 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Heinrich", "Mattias P.", ""], ["Blendowski", "Max", ""], ["Oktay", "Ozan", ""]]}, {"id": "1801.09454", "submitter": "Hiroya Maeda", "authors": "Hiroya Maeda, Yoshihide Sekimoto, Toshikazu Seto, Takehiro Kashiyama,\n  Hiroshi Omata", "title": "Road Damage Detection Using Deep Neural Networks with Images Captured\n  Through a Smartphone", "comments": "14 pages, 7 figures", "journal-ref": "Computer Aided Civil and Infrastructure Engineering, 2018", "doi": "10.1111/mice.12387", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research on damage detection of road surfaces using image processing\ntechniques has been actively conducted, achieving considerably high detection\naccuracies. Many studies only focus on the detection of the presence or absence\nof damage. However, in a real-world scenario, when the road managers from a\ngoverning body need to repair such damage, they need to clearly understand the\ntype of damage in order to take effective action. In addition, in many of these\nprevious studies, the researchers acquire their own data using different\nmethods. Hence, there is no uniform road damage dataset available openly,\nleading to the absence of a benchmark for road damage detection. This study\nmakes three contributions to address these issues. First, to the best of our\nknowledge, for the first time, a large-scale road damage dataset is prepared.\nThis dataset is composed of 9,053 road damage images captured with a smartphone\ninstalled on a car, with 15,435 instances of road surface damage included in\nthese road images. In order to generate this dataset, we cooperated with 7\nmunicipalities in Japan and acquired road images for more than 40 hours. These\nimages were captured in a wide variety of weather and illuminance conditions.\nIn each image, we annotated the bounding box representing the location and type\nof damage. Next, we used a state-of-the-art object detection method using\nconvolutional neural networks to train the damage detection model with our\ndataset, and compared the accuracy and runtime speed on both, using a GPU\nserver and a smartphone. Finally, we demonstrate that the type of damage can be\nclassified into eight types with high accuracy by applying the proposed object\ndetection method. The road damage dataset, our experimental results, and the\ndeveloped smartphone application used in this study are publicly available\n(https://github.com/sekilab/RoadDamageDetector/).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 11:25:20 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 03:23:24 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Maeda", "Hiroya", ""], ["Sekimoto", "Yoshihide", ""], ["Seto", "Toshikazu", ""], ["Kashiyama", "Takehiro", ""], ["Omata", "Hiroshi", ""]]}, {"id": "1801.09467", "submitter": "Chang Shu", "authors": "Chang Shu, Xi Chen, Qiwei Xie, Hua Han", "title": "Hierarchical Spatial Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision researchers have been expecting that neural networks have\nspatial transformation ability to eliminate the interference caused by\ngeometric distortion for a long time. Emergence of spatial transformer network\nmakes dream come true. Spatial transformer network and its variants can handle\nglobal displacement well, but lack the ability to deal with local spatial\nvariance. Hence how to achieve a better manner of deformation in the neural\nnetwork has become a pressing matter of the moment. To address this issue, we\nanalyze the advantages and disadvantages of approximation theory and optical\nflow theory, then we combine them to propose a novel way to achieve image\ndeformation and implement it with a hierarchical convolutional neural network.\nThis new approach solves for a linear deformation along with an optical flow\nfield to model image deformation. In the experiments of cluttered MNIST\nhandwritten digits classification and image plane alignment, our method\noutperforms baseline methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:09:58 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 02:33:41 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Shu", "Chang", ""], ["Chen", "Xi", ""], ["Xie", "Qiwei", ""], ["Han", "Hua", ""]]}, {"id": "1801.09468", "submitter": "Sihui Luo", "authors": "Sihui Luo, Yezhou Yang, Mingli Song", "title": "DeepSIC: Deep Semantic Image Compression", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating semantic information into the codecs during image compression\ncan significantly reduce the repetitive computation of fundamental semantic\nanalysis (such as object recognition) in client-side applications. The same\npractice also enable the compressed code to carry the image semantic\ninformation during storage and transmission. In this paper, we propose a\nconcept called Deep Semantic Image Compression (DeepSIC) and put forward two\nnovel architectures that aim to reconstruct the compressed image and generate\ncorresponding semantic representations at the same time. The first architecture\nperforms semantic analysis in the encoding process by reserving a portion of\nthe bits from the compressed code to store the semantic representations. The\nsecond performs semantic analysis in the decoding step with the feature maps\nthat are embedded in the compressed code. In both architectures, the feature\nmaps are shared by the compression and the semantic analytics modules. To\nvalidate our approaches, we conduct experiments on the publicly available\nbenchmarking datasets and achieve promising results. We also provide a thorough\nanalysis of the advantages and disadvantages of the proposed technique.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:10:04 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Luo", "Sihui", ""], ["Yang", "Yezhou", ""], ["Song", "Mingli", ""]]}, {"id": "1801.09472", "submitter": "AmirAbbas Davari", "authors": "AmirAbbas Davari, Nikolaos Sakaltras, Armin Haeberle, Sulaiman Vesal,\n  Vincent Christlein, Andreas Maier, Christian Riess", "title": "Hyper-Hue and EMAP on Hyperspectral Images for Supervised Layer\n  Decomposition of Old Master Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Old master drawings were mostly created step by step in several layers using\ndifferent materials. To art historians and restorers, examination of these\nlayers brings various insights into the artistic work process and helps to\nanswer questions about the object, its attribution and its authenticity.\nHowever, these layers typically overlap and are oftentimes difficult to\ndifferentiate with the unaided eye. For example, a common layer combination is\nred chalk under ink.\n  In this work, we propose an image processing pipeline that operates on\nhyperspectral images to separate such layers. Using this pipeline, we show that\nhyperspectral images enable better layer separation than RGB images, and that\nspectral focus stacking aids the layer separation. In particular, we propose to\nuse two descriptors in hyperspectral historical document analysis, namely\nhyper-hue and extended multi-attribute profile (EMAP). Our comparative results\nwith other features underline the efficacy of the three proposed improvements.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:29:44 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 12:30:48 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Davari", "AmirAbbas", ""], ["Sakaltras", "Nikolaos", ""], ["Haeberle", "Armin", ""], ["Vesal", "Sulaiman", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "1801.09477", "submitter": "Nachwa Abou Bakr", "authors": "Nachwa Abou Bakr (PERVASIVE), James Crowley", "title": "Histogram of Oriented Depth Gradients for Action Recognition", "comments": "ORASIS 2017, Jun 2017, Colleville-sur-Mer, France. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report on experiments with the use of local measures for\ndepth motion for visual action recognition from MPEG encoded RGBD video\nsequences. We show that such measures can be combined with local space-time\nvideo descriptors for appearance to provide a computationally efficient method\nfor recognition of actions. Fisher vectors are used for encoding and\nconcatenating a depth descriptor with existing RGB local descriptors. We then\nemploy a linear SVM for recognizing manipulation actions using such vectors. We\nevaluate the effectiveness of such measures by comparison to the\nstate-of-the-art using two recent datasets for action recognition in kitchen\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:38:31 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Bakr", "Nachwa Abou", "", "PERVASIVE"], ["Crowley", "James", ""]]}, {"id": "1801.09518", "submitter": "Ulugbek Kamilov", "authors": "Emrah Bostan, Ulugbek S. Kamilov, Laura Waller", "title": "Learning-based Image Reconstruction via Parallel Proximal Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2018.2833812", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, sparsity-driven regularization has led to advancement of\nimage reconstruction algorithms. Traditionally, such regularizers rely on\nanalytical models of sparsity (e.g. total variation (TV)). However, more recent\nmethods are increasingly centered around data-driven arguments inspired by deep\nlearning. In this letter, we propose to generalize TV regularization by\nreplacing the l1-penalty with an alternative prior that is trainable.\nSpecifically, our method learns the prior via extending the recently proposed\nfast parallel proximal algorithm (FPPA) to incorporate data-adaptive proximal\noperators. The proposed framework does not require additional inner iterations\nfor evaluating the proximal mappings of the corresponding learned prior.\nMoreover, our formalism ensures that the training and reconstruction processes\nshare the same algorithmic structure, making the end-to-end implementation\nintuitive. As an example, we demonstrate our algorithm on the problem of\ndeconvolution in a fluorescence microscope.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 14:17:28 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Bostan", "Emrah", ""], ["Kamilov", "Ulugbek S.", ""], ["Waller", "Laura", ""]]}, {"id": "1801.09530", "submitter": "Ruth Davidson", "authors": "Chuan Du, Christopher Szul, Adarsh Manawa, Nima Rasekh, Rosemary\n  Guzman, and Ruth Davidson", "title": "RGB image-based data analysis via discrete Morse theory and persistent\n  homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and comparing images for the purposes of data analysis is\ncurrently a very computationally demanding task. A group at Australian National\nUniversity (ANU) recently developed open-source code that can detect\nfundamental topological features of a grayscale image in a computationally\nfeasible manner. This is made possible by the fact that computers store\ngrayscale images as cubical cellular complexes. These complexes can be studied\nusing the techniques of discrete Morse theory. We expand the functionality of\nthe ANU code by introducing methods and software for analyzing images encoded\nin red, green, and blue (RGB), because this image encoding is very popular for\npublicly available data. Our methods allow the extraction of key topological\ninformation from RGB images via informative persistence diagrams by introducing\nnovel methods for transforming RGB-to-grayscale. This paradigm allows us to\nperform data analysis directly on RGB images representing water scarcity\nvariability as well as crime variability. We introduce software enabling a a\nuser to predict future image properties, towards the eventual aim of more rapid\nimage-based data behavior prediction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:57:04 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Du", "Chuan", ""], ["Szul", "Christopher", ""], ["Manawa", "Adarsh", ""], ["Rasekh", "Nima", ""], ["Guzman", "Rosemary", ""], ["Davidson", "Ruth", ""]]}, {"id": "1801.09555", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Chaochun Liu, Wei Fan, Xiaohui Xie", "title": "DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule\n  Detection and Classification", "comments": "9 pages, 8 figures, IEEE WACV conference. arXiv admin note:\n  substantial text overlap with arXiv:1709.05538", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a fully automated lung computed tomography (CT)\ncancer diagnosis system, DeepLung. DeepLung consists of two components, nodule\ndetection (identifying the locations of candidate nodules) and classification\n(classifying candidate nodules into benign or malignant). Considering the 3D\nnature of lung CT data and the compactness of dual path networks (DPN), two\ndeep 3D DPN are designed for nodule detection and classification respectively.\nSpecifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is\ndesigned for nodule detection with 3D dual path blocks and a U-net-like\nencoder-decoder structure to effectively learn nodule features. For nodule\nclassification, gradient boosting machine (GBM) with 3D dual path network\nfeatures is proposed. The nodule classification subnetwork was validated on a\npublic dataset from LIDC-IDRI, on which it achieved better performance than\nstate-of-the-art approaches and surpassed the performance of experienced\ndoctors based on image modality. Within the DeepLung system, candidate nodules\nare detected first by the nodule detection subnetwork, and nodule diagnosis is\nconducted by the classification subnetwork. Extensive experimental results\ndemonstrate that DeepLung has performance comparable to experienced doctors\nboth for the nodule-level and patient-level diagnosis on the LIDC-IDRI\ndataset.\\footnote{https://github.com/uci-cbcl/DeepLung.git}\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 23:22:00 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Zhu", "Wentao", ""], ["Liu", "Chaochun", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1801.09571", "submitter": "Effrosyni Mavroudi", "authors": "Effrosyni Mavroudi, Divya Bhaskara, Shahin Sefati, Haider Ali, Ren\\'e\n  Vidal", "title": "End-to-End Fine-Grained Action Segmentation and Recognition Using\n  Conditional Random Field Models and Discriminative Sparse Coding", "comments": "Camera ready version accepted at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained action segmentation and recognition is an important yet\nchallenging task. Given a long, untrimmed sequence of kinematic data, the task\nis to classify the action at each time frame and segment the time series into\nthe correct sequence of actions. In this paper, we propose a novel framework\nthat combines a temporal Conditional Random Field (CRF) model with a powerful\nframe-level representation based on discriminative sparse coding. We introduce\nan end-to-end algorithm for jointly learning the weights of the CRF model,\nwhich include action classification and action transition costs, as well as an\novercomplete dictionary of mid-level action primitives. This results in a CRF\nmodel that is driven by sparse coding features obtained using a discriminative\ndictionary that is shared among different actions and adapted to the task of\nstructured output learning. We evaluate our method on three surgical tasks\nusing kinematic data from the JIGSAWS dataset, as well as on a food preparation\ntask using accelerometer data from the 50 Salads dataset. Our results show that\nthe proposed method performs on par or better than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 15:24:56 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Mavroudi", "Effrosyni", ""], ["Bhaskara", "Divya", ""], ["Sefati", "Shahin", ""], ["Ali", "Haider", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1801.09646", "submitter": "David-Alexandre Beaupr\\'e", "authors": "David-Alexandre Beaupr\\'e, Guillaume-Alexandre Bilodeau and Nicolas\n  Saunier", "title": "Improving Multiple Object Tracking with Optical Flow and Edge\n  Preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new method for detecting road users in an urban\nenvironment which leads to an improvement in multiple object tracking. Our\nmethod takes as an input a foreground image and improves the object detection\nand segmentation. This new image can be used as an input to trackers that use\nforeground blobs from background subtraction. The first step is to create\nforeground images for all the frames in an urban video. Then, starting from the\noriginal blobs of the foreground image, we merge the blobs that are close to\none another and that have similar optical flow. The next step is extracting the\nedges of the different objects to detect multiple objects that might be very\nclose (and be merged in the same blob) and to adjust the size of the original\nblobs. At the same time, we use the optical flow to detect occlusion of objects\nthat are moving in opposite directions. Finally, we make a decision on which\ninformation we keep in order to construct a new foreground image with blobs\nthat can be used for tracking. The system is validated on four videos of an\nurban traffic dataset. Our method improves the recall and precision metrics for\nthe object detection task compared to the vanilla background subtraction method\nand improves the CLEAR MOT metrics in the tracking tasks for most videos.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:42:33 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Beaupr\u00e9", "David-Alexandre", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "1801.09672", "submitter": "Ze Wang", "authors": "Danfeng Xie, Li Bai, Ze Wang", "title": "Denoising Arterial Spin Labeling Cerebral Blood Flow Images Using Deep\n  Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arterial spin labeling perfusion MRI is a noninvasive technique for measuring\nquantitative cerebral blood flow (CBF), but the measurement is subject to a low\nsignal-to-noise-ratio(SNR). Various post-processing methods have been proposed\nto denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is\nan emerging technique that can learn the most representative signal from data\nwithout prior modeling which can be highly complex and analytically\nindescribable. The purpose of this study was to assess whether the record\nbreaking performance of DL can be translated into ASL MRI denoising. We used\nconvolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL)\nto inherently consider the inter-voxel correlations. To better guide DL-ASL\ntraining, we incorporated prior knowledge about ASL MRI: the structural\nsimilarity between ASL CBF map and grey matter probability map. A relatively\nlarge sample data were used to train the model which was subsequently applied\nto a new set of data for testing. Experimental results showed that DL-ASL\nachieved state-of-the-art denoising performance for ASL MRI as compared to\ncurrent routine methods in terms of higher SNR, keeping CBF quantification\nquality while shorten the acquisition time by 75%, and automatic partial volume\ncorrection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 20:54:29 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Xie", "Danfeng", ""], ["Bai", "Li", ""], ["Wang", "Ze", ""]]}, {"id": "1801.09718", "submitter": "Tomasz Kornuta", "authors": "Mikyas T. Desta and Larry Chen and Tomasz Kornuta", "title": "Object-based reasoning in VQA", "comments": "10 pages, 15 figures, published as a conference paper at 2018 IEEE\n  Winter Conf. on Applications of Computer Vision (WACV'2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a novel problem domain where multi-modal\ninputs must be processed in order to solve the task given in the form of a\nnatural language. As the solutions inherently require to combine visual and\nnatural language processing with abstract reasoning, the problem is considered\nas AI-complete. Recent advances indicate that using high-level, abstract facts\nextracted from the inputs might facilitate reasoning. Following that direction\nwe decided to develop a solution combining state-of-the-art object detection\nand reasoning modules. The results, achieved on the well-balanced CLEVR\ndataset, confirm the promises and show significant, few percent improvements of\naccuracy on the complex \"counting\" task.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 19:24:51 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Desta", "Mikyas T.", ""], ["Chen", "Larry", ""], ["Kornuta", "Tomasz", ""]]}, {"id": "1801.09749", "submitter": "Philippe Burlina", "authors": "Mike Pekala, Neil Joshi, David E. Freund, Neil M. Bressler, Delia\n  Cabrera DeBuc and Philippe M Burlina", "title": "Deep Learning based Retinal OCT Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to evaluate the efficacy of methods that use deep learning\n(DL) for the automatic fine-grained segmentation of optical coherence\ntomography (OCT) images of the retina. OCT images from 10 patients with mild\nnon-proliferative diabetic retinopathy were used from a public (U. of Miami)\ndataset. For each patient, five images were available: one image of the fovea\ncenter, two images of the perifovea, and two images of the parafovea. For each\nimage, two expert graders each manually annotated five retinal surfaces (i.e.\nboundaries between pairs of retinal layers). The first grader's annotations\nwere used as ground truth and the second grader's annotations to compute\ninter-operator agreement. The proposed automated approach segments images using\nfully convolutional networks (FCNs) together with Gaussian process (GP)-based\nregression as a post-processing step to improve the quality of the estimates.\nUsing 10-fold cross validation, the performance of the algorithms is determined\nby computing the per-pixel unsigned error (distance) between the automated\nestimates and the ground truth annotations generated by the first manual\ngrader. We compare the proposed method against five state of the art automatic\nsegmentation techniques. The results show that the proposed methods compare\nfavorably with state of the art techniques, resulting in the smallest mean\nunsigned error values and associated standard deviations, and performance is\ncomparable with human annotation of retinal layers from OCT when there is only\nmild retinopathy. The results suggest that semantic segmentation using FCNs,\ncoupled with regression-based post-processing, can effectively solve the OCT\nsegmentation problem on par with human capabilities with mild retinopathy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 20:44:40 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Pekala", "Mike", ""], ["Joshi", "Neil", ""], ["Freund", "David E.", ""], ["Bressler", "Neil M.", ""], ["DeBuc", "Delia Cabrera", ""], ["Burlina", "Philippe M", ""]]}, {"id": "1801.09804", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Jessi Bustos, Thomas Lu", "title": "Predicting Rapid Fire Growth (Flashover) Using Conditional Generative\n  Adversarial Networks", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flashover occurs when a fire spreads very rapidly through crevices due to\nintense heat. Flashovers present one of the most frightening and challenging\nfire phenomena to those who regularly encounter them: firefighters.\nFirefighters' safety and lives often depend on their ability to predict\nflashovers before they occur. Typical pre-flashover fire characteristics\ninclude dark smoke, high heat, and rollover (\"angel fingers\") and can be\nquantified by color, size, and shape. Using a color video stream from a\nfirefighter's body camera, we applied generative adversarial neural networks\nfor image enhancement. The neural networks were trained to enhance very dark\nfire and smoke patterns in videos and monitor dynamic changes in smoke and fire\nareas. Preliminary tests with limited flashover training videos showed that we\npredicted a flashover as early as 55 seconds before it occurred.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:09:48 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Yun", "Kyongsik", ""], ["Bustos", "Jessi", ""], ["Lu", "Thomas", ""]]}, {"id": "1801.09823", "submitter": "Peng Tang", "authors": "Peng Tang, Chunyu Wang, Xinggang Wang, Wenyu Liu, Wenjun Zeng,\n  Jingdong Wang", "title": "Object Detection in Videos by High Quality Object Linking", "comments": "accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with object detection in static images, object detection in videos\nis more challenging due to degraded image qualities. An effective way to\naddress this problem is to exploit temporal contexts by linking the same object\nacross video to form tubelets and aggregating classification scores in the\ntubelets. In this paper, we focus on obtaining high quality object linking\nresults for better classification. Unlike previous methods that link objects by\nchecking boxes between neighboring frames, we propose to link in the same\nframe. To achieve this goal, we extend prior methods in following aspects: (1)\na cuboid proposal network that extracts spatio-temporal candidate cuboids which\nbound the movement of objects; (2) a short tubelet detection network that\ndetects short tubelets in short video segments; (3) a short tubelet linking\nalgorithm that links temporally-overlapping short tubelets to form long\ntubelets. Experiments on the ImageNet VID dataset show that our method\noutperforms both the static image detector and the previous state of the art.\nIn particular, our method improves results by 8.8% over the static image\ndetector for fast moving objects.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 01:59:50 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 23:46:22 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 07:56:01 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tang", "Peng", ""], ["Wang", "Chunyu", ""], ["Wang", "Xinggang", ""], ["Liu", "Wenyu", ""], ["Zeng", "Wenjun", ""], ["Wang", "Jingdong", ""]]}, {"id": "1801.09847", "submitter": "Qian-Yi Zhou", "authors": "Qian-Yi Zhou and Jaesik Park and Vladlen Koltun", "title": "Open3D: A Modern Library for 3D Data Processing", "comments": "http://www.open3d.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open3D is an open-source library that supports rapid development of software\nthat deals with 3D data. The Open3D frontend exposes a set of carefully\nselected data structures and algorithms in both C++ and Python. The backend is\nhighly optimized and is set up for parallelization. Open3D was developed from a\nclean slate with a small and carefully considered set of dependencies. It can\nbe set up on different platforms and compiled from source with minimal effort.\nThe code is clean, consistently styled, and maintained via a clear code review\nmechanism. Open3D has been used in a number of published research projects and\nis actively deployed in the cloud. We welcome contributions from the\nopen-source community.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 04:33:20 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhou", "Qian-Yi", ""], ["Park", "Jaesik", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1801.09859", "submitter": "Pratik Brahma", "authors": "Pratik Prabhanjan Brahma, Qiuyuan Huang, Dapeng Wu", "title": "Structured Memory based Deep Model to Detect as well as Characterize\n  Novel Inputs", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has pushed the boundaries in various machine learning\ntasks, the current models are still far away from replicating many functions\nthat a normal human brain can do. Explicit memorization based deep architecture\nhave been recently proposed with the objective to understand and predict\nbetter. In this work, we design a system that involves a primary learner and an\nadjacent representational memory bank which is organized using a comparative\nlearner. This spatially forked deep architecture with a structured memory can\nsimultaneously predict and reason about the nature of an input, which may even\nbelong to a category never seen in the training data, by relating it with the\nmemorized past representations at the higher layers. Characterizing images of\nunseen object classes in both synthetic and real world datasets is used as an\nexample to showcase the operational success of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 06:04:11 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Brahma", "Pratik Prabhanjan", ""], ["Huang", "Qiuyuan", ""], ["Wu", "Dapeng", ""]]}, {"id": "1801.09919", "submitter": "Yash Patel", "authors": "Michal Bu\\v{s}ta, Yash Patel, Jiri Matas", "title": "E2E-MLT - an Unconstrained End-to-End Method for Multi-Language Scene\n  Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An end-to-end trainable (fully differentiable) method for multi-language\nscene text localization and recognition is proposed. The approach is based on a\nsingle fully convolutional network (FCN) with shared layers for both tasks.\n  E2E-MLT is the first published multi-language OCR for scene text. While\ntrained in multi-language setup, E2E-MLT demonstrates competitive performance\nwhen compared to other methods trained for English scene text alone. The\nexperiments show that obtaining accurate multi-language multi-script\nannotations is a challenging problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 10:09:15 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 21:49:16 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Bu\u0161ta", "Michal", ""], ["Patel", "Yash", ""], ["Matas", "Jiri", ""]]}, {"id": "1801.09927", "submitter": "Qingji Guan", "authors": "Qingji Guan, Yaping Huang, Zhun Zhong, Zhedong Zheng, Liang Zheng and\n  Yi Yang", "title": "Diagnose like a Radiologist: Attention Guided Convolutional Neural\n  Network for Thorax Disease Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of thorax disease classification on chest X-ray\nimages. Existing methods generally use the global image as input for network\nlearning. Such a strategy is limited in two aspects. 1) A thorax disease\nusually happens in (small) localized areas which are disease specific. Training\nCNNs using global image may be affected by the (excessive) irrelevant noisy\nareas. 2) Due to the poor alignment of some CXR images, the existence of\nirregular borders hinders the network performance. In this paper, we address\nthe above problems by proposing a three-branch attention guided convolution\nneural network (AG-CNN). AG-CNN 1) learns from disease-specific regions to\navoid noise and improve alignment, 2) also integrates a global branch to\ncompensate the lost discriminative cues by local branch. Specifically, we first\nlearn a global CNN branch using global images. Then, guided by the attention\nheat map generated from the global branch, we inference a mask to crop a\ndiscriminative region from the global image. The local region is used for\ntraining a local CNN branch. Lastly, we concatenate the last pooling layers of\nboth the global and local branches for fine-tuning the fusion branch. The\nComprehensive experiment is conducted on the ChestX-ray14 dataset. We first\nreport a strong global baseline producing an average AUC of 0.841 with\nResNet-50 as backbone. After combining the local cues with the global\ninformation, AG-CNN improves the average AUC to 0.868. While DenseNet-121 is\nused, the average AUC achieves 0.871, which is a new state of the art in the\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 10:55:23 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Guan", "Qingji", ""], ["Huang", "Yaping", ""], ["Zhong", "Zhun", ""], ["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""]]}, {"id": "1801.09969", "submitter": "Yixing Zhu", "authors": "Yixing Zhu and Jun Du", "title": "Sliding Line Point Regression for Shape Robust Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional text detection methods mostly focus on quadrangle text. In this\nstudy we propose a novel method named sliding line point regression (SLPR) in\norder to detect arbitrary-shape text in natural scene. SLPR regresses multiple\npoints on the edge of text line and then utilizes these points to sketch the\noutlines of the text. The proposed SLPR can be adapted to many object detection\narchitectures such as Faster R-CNN and R-FCN. Specifically, we first generate\nthe smallest rectangular box including the text with region proposal network\n(RPN), then isometrically regress the points on the edge of text by using the\nvertically and horizontally sliding lines. To make full use of information and\nreduce redundancy, we calculate x-coordinate or y-coordinate of target point by\nthe rectangular box position, and just regress the remaining y-coordinate or\nx-coordinate. Accordingly we can not only reduce the parameters of system, but\nalso restrain the points which will generate more regular polygon. Our approach\nachieved competitive results on traditional ICDAR2015 Incidental Scene Text\nbenchmark and curve text detection dataset CTW1500.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 12:58:10 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhu", "Yixing", ""], ["Du", "Jun", ""]]}, {"id": "1801.10031", "submitter": "arXiv Admin", "authors": "Suman Kunwar", "title": "Malaria Detection Using Image Processing and Machine Learning", "comments": "This paper has been withdrawn by arXiv. arXiv admin note: author list\n  truncated due to disputed authorship and content", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Malaria is mosquito-borne blood disease caused by parasites of the genus\nPlasmodium. Conventional diagnostic tool for malaria is the examination of\nstained blood cell of patient in microscope. The blood to be tested is placed\nin a slide and is observed under a microscope to count the number of infected\nRBC. An expert technician is involved in the examination of the slide with\nintense visual and mental concentration. This is tiresome and time consuming\nprocess.\n  In this paper, we construct a new mage processing system for detection and\nquantification of plasmodium parasites in blood smear slide, later we develop\nMachine Learning algorithm to learn, detect and determine the types of infected\ncells according to its features.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 06:26:59 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:54:13 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Kunwar", "Suman", ""]]}, {"id": "1801.10041", "submitter": "John Edgar Vargas Mu\\~noz", "authors": "John E. Vargas-Mu\\~noz, Ananda S. Chowdhury, Eduardo B. Alexandre,\n  Felipe L. Galv\\~ao, Paulo A. Vechiatto Miranda and Alexandre X. Falc\\~ao", "title": "An Iterative Spanning Forest Framework for Superpixel Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2897941", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel segmentation has become an important research problem in image\nprocessing. In this paper, we propose an Iterative Spanning Forest (ISF)\nframework, based on sequences of Image Foresting Transforms, where one can\nchoose i) a seed sampling strategy, ii) a connectivity function, iii) an\nadjacency relation, and iv) a seed pixel recomputation procedure to generate\nimproved sets of connected superpixels (supervoxels in 3D) per iteration. The\nsuperpixels in ISF structurally correspond to spanning trees rooted at those\nseeds. We present five ISF methods to illustrate different choices of its\ncomponents. These methods are compared with approaches from the\nstate-of-the-art in effectiveness and efficiency. The experiments involve 2D\nand 3D datasets with distinct characteristics, and a high level application,\nnamed sky image segmentation. The theoretical properties of ISF are\ndemonstrated in the supplementary material and the results show that some of\nits methods are competitive with or superior to the best baselines in\neffectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 15:05:38 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Vargas-Mu\u00f1oz", "John E.", ""], ["Chowdhury", "Ananda S.", ""], ["Alexandre", "Eduardo B.", ""], ["Galv\u00e3o", "Felipe L.", ""], ["Miranda", "Paulo A. Vechiatto", ""], ["Falc\u00e3o", "Alexandre X.", ""]]}, {"id": "1801.10068", "submitter": "Guoliang Kang", "authors": "Guoliang Kang, Liang Zheng, Yan Yan, Yi Yang", "title": "Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation:\n  the Benefit of Target Expectation Maximization", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we make two contributions to unsupervised domain adaptation\n(UDA) using the convolutional neural network (CNN). First, our approach\ntransfers knowledge in all the convolutional layers through attention\nalignment. Most previous methods align high-level representations, e.g.,\nactivations of the fully connected (FC) layers. In these methods, however, the\nconvolutional layers which underpin critical low-level domain knowledge cannot\nbe updated directly towards reducing domain discrepancy. Specifically, we\nassume that the discriminative regions in an image are relatively invariant to\nimage style changes. Based on this assumption, we propose an attention\nalignment scheme on all the target convolutional layers to uncover the\nknowledge shared by the source domain. Second, we estimate the posterior label\ndistribution of the unlabeled data for target network training. Previous\nmethods, which iteratively update the pseudo labels by the target network and\nrefine the target network by the updated pseudo labels, are vulnerable to label\nestimation errors. Instead, our approach uses category distribution to\ncalculate the cross-entropy loss for training, thereby ameliorating the error\naccumulation of the estimated labels. The two contributions allow our approach\nto outperform the state-of-the-art methods by +2.6% on the Office-31 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 15:55:57 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 16:20:10 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 15:39:26 GMT"}, {"version": "v4", "created": "Sat, 11 Aug 2018 16:44:04 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Kang", "Guoliang", ""], ["Zheng", "Liang", ""], ["Yan", "Yan", ""], ["Yang", "Yi", ""]]}, {"id": "1801.10100", "submitter": "Pavani Tripathi", "authors": "Aditya Lakra, Pavani Tripathi, Rohit Keshari, Mayank Vatsa, Richa\n  Singh", "title": "SegDenseNet: Iris Segmentation for Pre and Post Cataract Surgery", "comments": "Corrected diagrams. Results remain the same!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cataract is caused due to various factors such as age, trauma, genetics,\nsmoking and substance consumption, and radiation. It is one of the major common\nophthalmic diseases worldwide which can potentially affect iris-based biometric\nsystems. India, which hosts the largest biometrics project in the world, has\nabout 8 million people undergoing cataract surgery annually. While existing\nresearch shows that cataract does not have a major impact on iris recognition,\nour observations suggest that the iris segmentation approaches are not well\nequipped to handle cataract or post cataract surgery cases. Therefore, failure\nin iris segmentation affects the overall recognition performance. This paper\npresents an efficient iris segmentation algorithm with variations due to\ncataract and post cataract surgery. The proposed algorithm, termed as\nSegDenseNet, is a deep learning algorithm based on DenseNets. The experiments\non the IIITD Cataract database show that improving the segmentation enhances\nthe identification by up to 25% across different sensors and matchers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 17:09:23 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 09:27:38 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lakra", "Aditya", ""], ["Tripathi", "Pavani", ""], ["Keshari", "Rohit", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "1801.10109", "submitter": "Jianshu Zhang", "authors": "Jianshu Zhang and Yixing Zhu and Jun Du and Lirong Dai", "title": "Trajectory-based Radical Analysis Network for Online Handwritten Chinese\n  Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, great progress has been made for online handwritten Chinese\ncharacter recognition due to the emergence of deep learning techniques.\nHowever, previous research mostly treated each Chinese character as one class\nwithout explicitly considering its inherent structure, namely the radical\ncomponents with complicated geometry. In this study, we propose a novel\ntrajectory-based radical analysis network (TRAN) to firstly identify radicals\nand analyze two-dimensional structures among radicals simultaneously, then\nrecognize Chinese characters by generating captions of them based on the\nanalysis of their internal radicals. The proposed TRAN employs recurrent neural\nnetworks (RNNs) as both an encoder and a decoder. The RNN encoder makes full\nuse of online information by directly transforming handwriting trajectory into\nhigh-level features. The RNN decoder aims at generating the caption by\ndetecting radicals and spatial structures through an attention model. The\nmanner of treating a Chinese character as a two-dimensional composition of\nradicals can reduce the size of vocabulary and enable TRAN to possess the\ncapability of recognizing unseen Chinese character classes, only if the\ncorresponding radicals have been seen. Evaluated on CASIA-OLHWDB database, the\nproposed approach significantly outperforms the state-of-the-art\nwhole-character modeling approach with a relative character error rate (CER)\nreduction of 10%. Meanwhile, for the case of recognition of 500 unseen Chinese\ncharacters, TRAN can achieve a character accuracy of about 60% while the\ntraditional whole-character method has no capability to handle them.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 02:42:32 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhang", "Jianshu", ""], ["Zhu", "Yixing", ""], ["Du", "Jun", ""], ["Dai", "Lirong", ""]]}, {"id": "1801.10111", "submitter": "Qilin Zhang", "authors": "Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, Weiping Li", "title": "Video-based Sign Language Recognition without Temporal Segmentation", "comments": "32nd AAAI Conference on Artificial Intelligence (AAAI-18), Feb. 2-7,\n  2018, New Orleans, Louisiana, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of hearing impaired people around the world routinely use some\nvariants of sign languages to communicate, thus the automatic translation of a\nsign language is meaningful and important. Currently, there are two\nsub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that\nrecognizes word by word and continuous SLR that translates entire sentences.\nExisting continuous SLR methods typically utilize isolated SLRs as building\nblocks, with an extra layer of preprocessing (temporal segmentation) and\nanother layer of post-processing (sentence synthesis). Unfortunately, temporal\nsegmentation itself is non-trivial and inevitably propagates errors into\nsubsequent steps. Worse still, isolated SLR methods typically require strenuous\nlabeling of each word separately in a sentence, severely limiting the amount of\nattainable training data. To address these challenges, we propose a novel\ncontinuous sign recognition framework, the Hierarchical Attention Network with\nLatent Space (LS-HAN), which eliminates the preprocessing of temporal\nsegmentation. The proposed LS-HAN consists of three components: a two-stream\nConvolutional Neural Network (CNN) for video feature representation generation,\na Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention\nNetwork (HAN) for latent space based recognition. Experiments are carried out\non two large scale datasets. Experimental results demonstrate the effectiveness\nof the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 17:37:42 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Huang", "Jie", ""], ["Zhou", "Wengang", ""], ["Zhang", "Qilin", ""], ["Li", "Houqiang", ""], ["Li", "Weiping", ""]]}, {"id": "1801.10112", "submitter": "Puneet Kumar Dokania", "authors": "Arslan Chaudhry, Puneet K. Dokania, Thalaiyasingam Ajanthan, Philip H.\n  S. Torr", "title": "Riemannian Walk for Incremental Learning: Understanding Forgetting and\n  Intransigence", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01252-6_33", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning (IL) has received a lot of attention recently, however,\nthe literature lacks a precise problem definition, proper evaluation settings,\nand metrics tailored specifically for the IL problem. One of the main\nobjectives of this work is to fill these gaps so as to provide a common ground\nfor better understanding of IL. The main challenge for an IL algorithm is to\nupdate the classifier whilst preserving existing knowledge. We observe that, in\naddition to forgetting, a known issue while preserving knowledge, IL also\nsuffers from a problem we call intransigence, inability of a model to update\nits knowledge. We introduce two metrics to quantify forgetting and\nintransigence that allow us to understand, analyse, and gain better insights\ninto the behaviour of IL algorithms. We present RWalk, a generalization of\nEWC++ (our efficient version of EWC [Kirkpatrick2016EWC]) and Path Integral\n[Zenke2017Continual] with a theoretically grounded KL-divergence based\nperspective. We provide a thorough analysis of various IL algorithms on MNIST\nand CIFAR-100 datasets. In these experiments, RWalk obtains superior results in\nterms of accuracy, and also provides a better trade-off between forgetting and\nintransigence.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 17:47:35 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 19:28:38 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 16:10:46 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chaudhry", "Arslan", ""], ["Dokania", "Puneet K.", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1801.10121", "submitter": "Quanzeng You", "authors": "Quanzeng You, Hailin Jin, Jiebo Luo", "title": "Image Captioning at Will: A Versatile Scheme for Effectively Injecting\n  Sentiments into Image Descriptions", "comments": "8 pages, 5 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image captioning has recently approached human-level performance\ndue to the latest advances in computer vision and natural language\nunderstanding. However, most of the current models can only generate plain\nfactual descriptions about the content of a given image. However, for human\nbeings, image caption writing is quite flexible and diverse, where additional\nlanguage dimensions, such as emotion, humor and language styles, are often\nincorporated to produce diverse, emotional, or appealing captions. In\nparticular, we are interested in generating sentiment-conveying image\ndescriptions, which has received little attention. The main challenge is how to\neffectively inject sentiments into the generated captions without altering the\nsemantic matching between the visual content and the generated descriptions. In\nthis work, we propose two different models, which employ different schemes for\ninjecting sentiments into image captions. Compared with the few existing\napproaches, the proposed models are much simpler and yet more effective. The\nexperimental results show that our model outperform the state-of-the-art models\nin generating sentimental (i.e., sentiment-bearing) image captions. In\naddition, we can also easily manipulate the model by assigning different\nsentiments to the testing image to generate captions with the corresponding\nsentiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 18:13:14 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["You", "Quanzeng", ""], ["Jin", "Hailin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1801.10281", "submitter": "Guangyu Zhong", "authors": "Guangyu Zhong, Yi-Hsuan Tsai, Sifei Liu, Zhixun Su, Ming-Hsuan Yang", "title": "Learning Video-Story Composition via Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning-based method to compose a video-story\nfrom a group of video clips that describe an activity or experience. We learn\nthe coherence between video clips from real videos via the Recurrent Neural\nNetwork (RNN) that jointly incorporates the spatial-temporal semantics and\nmotion dynamics to generate smooth and relevant compositions. We further\nrearrange the results generated by the RNN to make the overall video-story\ncompatible with the storyline structure via a submodular ranking optimization\nprocess. Experimental results on the video-story dataset show that the proposed\nalgorithm outperforms the state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 02:35:30 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Zhong", "Guangyu", ""], ["Tsai", "Yi-Hsuan", ""], ["Liu", "Sifei", ""], ["Su", "Zhixun", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1801.10300", "submitter": "Kuan-Ting Chen", "authors": "Wen Hua Lin, Kuan-Ting Chen, Hung Yueh Chiang and Winston Hsu", "title": "Netizen-Style Commenting on Fashion Photos: Dataset and Diversity\n  Measures", "comments": "The Web Conference (WWW) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural network models have achieved promising results in image\ncaptioning task. Yet, \"vanilla\" sentences, only describing shallow appearances\n(e.g., types, colors), generated by current works are not satisfied netizen\nstyle resulting in lacking engagements, contexts, and user intentions. To\ntackle this problem, we propose Netizen Style Commenting (NSC), to\nautomatically generate characteristic comments to a user-contributed fashion\nphoto. We are devoted to modulating the comments in a vivid \"netizen\" style\nwhich reflects the culture in a designated social community and hopes to\nfacilitate more engagement with users. In this work, we design a novel\nframework that consists of three major components: (1) We construct a\nlarge-scale clothing dataset named NetiLook, which contains 300K posts (photos)\nwith 5M comments to discover netizen-style comments. (2) We propose three\nunique measures to estimate the diversity of comments. (3) We bring diversity\nby marrying topic models with neural networks to make up the insufficiency of\nconventional image captioning works. Experimenting over Flickr30k and our\nNetiLook datasets, we demonstrate our proposed approaches benefit fashion photo\ncommenting and improve image captioning tasks both in accuracy and diversity.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 05:08:58 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Lin", "Wen Hua", ""], ["Chen", "Kuan-Ting", ""], ["Chiang", "Hung Yueh", ""], ["Hsu", "Winston", ""]]}, {"id": "1801.10304", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Yuncheng Li, Jianchao Yang, Jiebo Luo", "title": "Action Recognition with Spatio-Temporal Visual Attention on Skeleton\n  Image Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition with 3D skeleton sequences is becoming popular due to its\nspeed and robustness. The recently proposed Convolutional Neural Networks (CNN)\nbased methods have shown good performance in learning spatio-temporal\nrepresentations for skeleton sequences. Despite the good recognition accuracy\nachieved by previous CNN based methods, there exist two problems that\npotentially limit the performance. First, previous skeleton representations are\ngenerated by chaining joints with a fixed order. The corresponding semantic\nmeaning is unclear and the structural information among the joints is lost.\nSecond, previous models do not have an ability to focus on informative joints.\nThe attention mechanism is important for skeleton based action recognition\nbecause there exist spatio-temporal key stages while the joint predictions can\nbe inaccurate. To solve these two problems, we propose a novel CNN based method\nfor skeleton based action recognition. We first redesign the skeleton\nrepresentations with a depth-first tree traversal order, which enhances the\nsemantic meaning of skeleton images and better preserves the associated\nstructural information. We then propose the idea of a two-branch attention\narchitecture that focuses on spatio-temporal key stages and filters out\nunreliable joint predictions. A base attention model with the simplest\nstructure is first introduced. By improving the structures in both branches, we\nfurther propose a Global Long-sequence Attention Network (GLAN). Furthermore,\nin order to adjust the kernel's spatio-temporal aspect ratios and better\ncapture long term dependencies, we propose a Sub-Sequence Attention Network\n(SSAN) that takes sub-image sequences as inputs. Our experiment results on NTU\nRGB+D and SBU Kinetic Interaction outperforms the state-of-the-art. The model\nis further validated on noisy estimated poses from UCF101 and Kinetics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 05:23:05 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 15:09:25 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Li", "Yuncheng", ""], ["Yang", "Jianchao", ""], ["Luo", "Jiebo", ""]]}, {"id": "1801.10312", "submitter": "Youngjae Yu", "authors": "Youngjae Yu, Sangho Lee, Joonil Na, Jaeyun Kang, Gunhee Kim", "title": "A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360\n  Video", "comments": "In AAAI 2018, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of highlight detection from a 360 degree video by\nsummarizing it both spatially and temporally. Given a long 360 degree video, we\nspatially select pleasantly-looking normal field-of-view (NFOV) segments from\nunlimited field of views (FOV) of the 360 degree video, and temporally\nsummarize it into a concise and informative highlight as a selected subset of\nsubshots. We propose a novel deep ranking model named as Composition View Score\n(CVS) model, which produces a spherical score map of composition per video\nsegment, and determines which view is suitable for highlight via a sliding\nwindow kernel at inference. To evaluate the proposed framework, we perform\nexperiments on the Pano2Vid benchmark dataset and our newly collected 360\ndegree video highlight dataset from YouTube and Vimeo. Through evaluation using\nboth quantitative summarization metrics and user studies via Amazon Mechanical\nTurk, we demonstrate that our approach outperforms several state-of-the-art\nhighlight detection methods. We also show that our model is 16 times faster at\ninference than AutoCam, which is one of the first summarization algorithms of\n360 degree videos\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 06:29:11 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Yu", "Youngjae", ""], ["Lee", "Sangho", ""], ["Na", "Joonil", ""], ["Kang", "Jaeyun", ""], ["Kim", "Gunhee", ""]]}, {"id": "1801.10319", "submitter": "Xi Cheng", "authors": "Xi Cheng, Xiang Li, Ying Tai, Jian Yang", "title": "SESR: Single Image Super Resolution with Recursive Squeeze and\n  Excitation Networks", "comments": "Preprint version with 6 pages for ICPR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Single image super resolution is a very important computer vision task, with\na wide range of applications. In recent years, the depth of the\nsuper-resolution model has been constantly increasing, but with a small\nincrease in performance, it has brought a huge amount of computation and memory\nconsumption. In this work, in order to make the super resolution models more\neffective, we proposed a novel single image super resolution method via\nrecursive squeeze and excitation networks (SESR). By introducing the squeeze\nand excitation module, our SESR can model the interdependencies and\nrelationships between channels and that makes our model more efficiency. In\naddition, the recursive structure and progressive reconstruction method in our\nmodel minimized the layers and parameters and enabled SESR to simultaneously\ntrain multi-scale super resolution in a single model. After evaluating on four\nbenchmark test sets, our model is proved to be above the state-of-the-art\nmethods in terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 06:50:49 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Cheng", "Xi", ""], ["Li", "Xiang", ""], ["Tai", "Ying", ""], ["Yang", "Jian", ""]]}, {"id": "1801.10324", "submitter": "Li Liu", "authors": "Li Liu, Jie Chen, Paul Fieguth, Guoying Zhao, Rama Chellappa, Matti\n  Pietikainen", "title": "From BoW to CNN: Two Decades of Texture Representation for Texture\n  Classification", "comments": "Accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is a fundamental characteristic of many types of images, and texture\nrepresentation is one of the essential and challenging problems in computer\nvision and pattern recognition which has attracted extensive research\nattention. Since 2000, texture representations based on Bag of Words (BoW) and\non Convolutional Neural Networks (CNNs) have been extensively studied with\nimpressive performance. Given this period of remarkable evolution, this paper\naims to present a comprehensive survey of advances in texture representation\nover the last two decades. More than 200 major publications are cited in this\nsurvey covering different aspects of the research, which includes (i) problem\ndescription; (ii) recent advances in the broad categories of BoW-based,\nCNN-based and attribute-based methods; and (iii) evaluation issues,\nspecifically benchmark datasets and state of the art results. In retrospect of\nwhat has been achieved so far, the survey discusses open challenges and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 07:06:12 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 20:55:57 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Liu", "Li", ""], ["Chen", "Jie", ""], ["Fieguth", "Paul", ""], ["Zhao", "Guoying", ""], ["Chellappa", "Rama", ""], ["Pietikainen", "Matti", ""]]}, {"id": "1801.10341", "submitter": "Stefan Sommer", "authors": "Stefan Sommer", "title": "An Infinitesimal Probabilistic Model for Principal Component Analysis of\n  Manifold Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a probabilistic and infinitesimal view of how the principal\ncomponent analysis procedure (PCA) can be generalized to analysis of nonlinear\nmanifold valued data. Starting with the probabilistic PCA interpretation of the\nEuclidean PCA procedure, we show how PCA can be generalized to manifolds in an\nintrinsic way that does not resort to linearization of the data space. The\nunderlying probability model is constructed by mapping a Euclidean stochastic\nprocess to the manifold using stochastic development of Euclidean\nsemimartingales. The construction uses a connection and bundles of covariant\ntensors to allow global transport of principal eigenvectors, and the model is\nthereby an example of how principal fiber bundles can be used to handle the\nlack of global coordinate system and orientations that characterizes manifold\nvalued statistics. We show how curvature implies non-integrability of the\nequivalent of Euclidean principal subspaces, and how the stochastic flows\nprovide an alternative to explicit construction of such subspaces. We describe\nestimation procedures for inference of parameters and prediction of principal\ncomponents, and we give examples of properties of the model on embedded\nsurfaces.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 08:16:16 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 19:21:08 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Sommer", "Stefan", ""]]}, {"id": "1801.10342", "submitter": "Peiyao Wang", "authors": "Xiaotong Lu, Weisheng Dong, Peiyao Wang, Guangming Shi, Xuemei Xie", "title": "ConvCSNet: A Convolutional Compressive Sensing Framework Based on Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS), aiming to reconstruct an image/signal from a small\nset of random measurements has attracted considerable attentions in recent\nyears. Due to the high dimensionality of images, previous CS methods mainly\nwork on image blocks to avoid the huge requirements of memory and computation,\ni.e., image blocks are measured with Gaussian random matrices, and the whole\nimages are recovered from the reconstructed image blocks. Though efficient,\nsuch methods suffer from serious blocking artifacts. In this paper, we propose\na convolutional CS framework that senses the whole image using a set of\nconvolutional filters. Instead of reconstructing individual blocks, the whole\nimage is reconstructed from the linear convolutional measurements.\nSpecifically, the convolutional CS is implemented based on a convolutional\nneural network (CNN), which performs both the convolutional CS and nonlinear\nreconstruction. Through end-to-end training, the sensing filters and the\nreconstruction network can be jointly optimized. To facilitate the design of\nthe CS reconstruction network, a novel two-branch CNN inspired from a\nsparsity-based CS reconstruction model is developed. Experimental results show\nthat the proposed method substantially outperforms previous state-of-the-art CS\nmethods in term of both PSNR and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 08:22:53 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Lu", "Xiaotong", ""], ["Dong", "Weisheng", ""], ["Wang", "Peiyao", ""], ["Shi", "Guangming", ""], ["Xie", "Xuemei", ""]]}, {"id": "1801.10351", "submitter": "Ofir Nabati", "authors": "Ofir Nabati, David Mendlovic and Raja Giryes", "title": "Fast and Accurate Reconstruction of Compressed Color Light Field", "comments": null, "journal-ref": "ICCP 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field photography has been studied thoroughly in recent years. One of\nits drawbacks is the need for multi-lens in the imaging. To compensate that,\ncompressed light field photography has been proposed to tackle the trade-offs\nbetween the spatial and angular resolutions. It obtains by only one lens, a\ncompressed version of the regular multi-lens system. The acquisition system\nconsists of a dedicated hardware followed by a decompression algorithm, which\nusually suffers from high computational time. In this work, we propose a\ncomputationally efficient neural network that recovers a high-quality color\nlight field from a single coded image. Unlike previous works, we compress the\ncolor channels as well, removing the need for a CFA in the imaging system. Our\napproach outperforms existing solutions in terms of recovery quality and\ncomputational complexity. We propose also a neural network for depth map\nextraction based on the decompressed light field, which is trained in an\nunsupervised manner without the ground truth depth map.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 08:44:02 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 10:19:48 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Nabati", "Ofir", ""], ["Mendlovic", "David", ""], ["Giryes", "Raja", ""]]}, {"id": "1801.10355", "submitter": "Alan JiaXiang Guo", "authors": "Alan J.X. Guo, Fei Zhu", "title": "A CNN-based Spatial Feature Fusion Algorithm for Hyperspectral Imagery\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2911993", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortage of training samples remains one of the main obstacles in\napplying the artificial neural networks (ANN) to the hyperspectral images\nclassification. To fuse the spatial and spectral information, pixel patches are\noften utilized to train a model, which may further aggregate this problem. In\nthe existing works, an ANN model supervised by center-loss (ANNC) was\nintroduced. Training merely with spectral information, the ANNC yields\ndiscriminative spectral features suitable for the subsequent classification\ntasks. In this paper, a CNN-based spatial feature fusion (CSFF) algorithm is\nproposed, which allows a smart fusion of the spatial information to the\nspectral features extracted by ANNC. As a critical part of CSFF, a CNN-based\ndiscriminant model is introduced to estimate whether two paring pixels belong\nto the same class. At the testing stage, by applying the discriminant model to\nthe pixel-pairs generated by the test pixel and its neighbors, the local\nstructure is estimated and represented as a customized convolutional kernel.\nThe spectral-spatial feature is obtained by a convolutional operation between\nthe estimated kernel and the corresponding spectral features within a\nneighborhood. At last, the label of the test pixel is predicted by classifying\nthe resulting spectral-spatial feature. Without increasing the number of\ntraining samples or involving pixel patches at the training stage, the CSFF\nframework achieves the state-of-the-art by declining $20\\%-50\\%$ classification\nfailures in experiments on three well-known hyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 08:57:10 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 07:50:02 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Guo", "Alan J. X.", ""], ["Zhu", "Fei", ""]]}, {"id": "1801.10365", "submitter": "Haichao Shi", "authors": "Haichao Shi, Xiao-Yu Zhang", "title": "Synchronized Detection and Recovery of Steganographic Messages with\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we mainly study the mechanism of learning the steganographic\nalgorithm as well as combining the learning process with adversarial learning\nto learn a good steganographic algorithm. To handle the problem of embedding\nsecret messages into the specific medium, we design a novel adversarial modules\nto learn the steganographic algorithm, and simultaneously train three modules\ncalled generator, discriminator and steganalyzer. Different from existing\nmethods, the three modules are formalized as a game to communicate with each\nother. In the game, the generator and discriminator attempt to communicate with\neach other using secret messages hidden in an image. While the steganalyzer\nattempts to analyze whether there is a transmission of confidential\ninformation. We show that through unsupervised adversarial training, the\nadversarial model can produce robust steganographic solutions, which act like\nan encryption. Furthermore, we propose to utilize supervised adversarial\ntraining method to train a robust steganalyzer, which is utilized to\ndiscriminate whether an image contains secret information. Numerous experiments\nare conducted on publicly available dataset to demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 09:11:18 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 08:01:03 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 07:15:26 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Shi", "Haichao", ""], ["Zhang", "Xiao-Yu", ""]]}, {"id": "1801.10434", "submitter": "Zhong Li", "authors": "Zhong Li, Yu Ji, Wei Yang, Jinwei Ye, Jingyi Yu", "title": "Robust 3D Human Motion Reconstruction Via Dynamic Template Construction", "comments": "3DV 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-view human body capture systems, the recovered 3D geometry or even\nthe acquired imagery data can be heavily corrupted due to occlusions, noise,\nlimited field of- view, etc. Direct estimation of 3D pose, body shape or motion\non these low-quality data has been traditionally challenging.In this paper, we\npresent a graph-based non-rigid shape registration framework that can\nsimultaneously recover 3D human body geometry and estimate pose/motion at high\nfidelity.Our approach first generates a global full-body template by\nregistering all poses in the acquired motion sequence.We then construct a\ndeformable graph by utilizing the rigid components in the global template. We\ndirectly warp the global template graph back to each motion frame in order to\nfill in missing geometry. Specifically, we combine local rigidity and temporal\ncoherence constraints to maintain geometry and motion consistencies.\nComprehensive experiments on various scenes show that our method is accurate\nand robust even in the presence of drastic motions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:03:29 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Li", "Zhong", ""], ["Ji", "Yu", ""], ["Yang", "Wei", ""], ["Ye", "Jinwei", ""], ["Yu", "Jingyi", ""]]}, {"id": "1801.10441", "submitter": "Haohan Li", "authors": "Haohan Li, Zuoqiang Shi and Xiaoping Wang", "title": "Weighted Nonlocal Total Variation in Image Processing", "comments": "15 pages, 49 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel weighted nonlocal total variation (WNTV) method is\nproposed. Compared to the classical nonlocal total variation methods, our\nmethod modifies the energy functional to introduce a weight to balance between\nthe labeled sets and unlabeled sets. With extensive numerical examples in\nsemi-supervised clustering, image inpainting and image colorization, we\ndemonstrate that WNTV provides an effective and efficient method in many image\nprocessing and machine learning problems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:24:34 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Li", "Haohan", ""], ["Shi", "Zuoqiang", ""], ["Wang", "Xiaoping", ""]]}, {"id": "1801.10442", "submitter": "Arsha Nagrani", "authors": "Arsha Nagrani, Andrew Zisserman", "title": "From Benedict Cumberbatch to Sherlock Holmes: Character Identification\n  in TV series without a Script", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is the automatic identification of characters in TV\nand feature film material. In contrast to standard approaches to this task,\nwhich rely on the weak supervision afforded by transcripts and subtitles, we\npropose a new method requiring only a cast list. This list is used to obtain\nimages of actors from freely available sources on the web, providing a form of\npartial supervision for this task. In using images of actors to recognize\ncharacters, we make the following three contributions: (i) We demonstrate that\nan automated semi-supervised learning approach is able to adapt from the\nactor's face to the character's face, including the face context of the hair;\n(ii) By building voice models for every character, we provide a bridge between\nfrontal faces (for which there is plenty of actor-level supervision) and\nprofile (for which there is very little or none); and (iii) by combining face\ncontext and speaker identification, we are able to identify characters with\npartially occluded faces and extreme facial poses. Results are presented on the\nTV series 'Sherlock' and the feature film 'Casablanca'. We achieve the\nstate-of-the-art on the Casablanca benchmark, surpassing previous methods that\nhave used the stronger supervision available from transcripts.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:25:29 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Nagrani", "Arsha", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1801.10443", "submitter": "Alexander G\\'omez Villa", "authors": "Alexander Gomez Villa, Augusto Salazar, Igor Stefanini", "title": "Counting Cells in Time-Lapse Microscopy using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic approach to counting any kind of cells could alleviate work of\nthe experts and boost the research in fields such as regenerative medicine. In\nthis paper, a method for microscopy cell counting using multiple frames (hence\ntemporal information) is proposed. Unlike previous approaches where the cell\ncounting is done independently in each frame (static cell counting), in this\nwork the cell counting prediction is done using multiple frames (dynamic cell\ncounting). A spatiotemporal model using ConvNets and long short term memory\n(LSTM) recurrent neural networks is proposed to overcome temporal variations.\nThe model outperforms static cell counting in a publicly available dataset of\nstem cells. The advantages, working conditions and limitations of the\nConvNet-LSTM method are discussed. Although our method is tested in cell\ncounting, it can be extrapolated to quantify in video (or correlated image\nseries) any kind of objects or volumes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:27:45 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Villa", "Alexander Gomez", ""], ["Salazar", "Augusto", ""], ["Stefanini", "Igor", ""]]}, {"id": "1801.10447", "submitter": "Deepak Mittal", "authors": "Deepak Mittal, Shweta Bhardwaj, Mitesh M. Khapra, Balaraman Ravindran", "title": "Recovering from Random Pruning: On the Plasticity of Deep Convolutional\n  Neural Networks", "comments": "Accepted at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations. The key idea is to rank the filters based on a certain criterion\n(say, $l_1$-norm, average percentage of zeros, etc) and retain only the top\nranked filters. Once the low scoring filters are pruned away the remainder of\nthe network is fine tuned and is shown to give performance comparable to the\noriginal unpruned network. In this work, we report experiments which suggest\nthat the comparable performance of the pruned network is not due to the\nspecific criterion chosen but due to the inherent plasticity of deep neural\nnetworks which allows them to recover from the loss of pruned filters once the\nrest of the filters are fine-tuned. Specifically, we show counter-intuitive\nresults wherein by randomly pruning 25-50\\% filters from deep CNNs we are able\nto obtain the same performance as obtained by using state of the art pruning\nmethods. We empirically validate our claims by doing an exhaustive evaluation\nwith VGG-16 and ResNet-50. Further, we also evaluate a real world scenario\nwhere a CNN trained on all 1000 ImageNet classes needs to be tested on only a\nsmall set of classes at test time (say, only animals). We create a new\nbenchmark dataset from ImageNet to evaluate such class specific pruning and\nshow that even here a random pruning strategy gives close to state of the art\nperformance. Lastly, unlike existing approaches which mainly focus on the task\nof image classification, in this work we also report results on object\ndetection. We show that using a simple random pruning strategy we can achieve\nsignificant speed up in object detection (74$\\%$ improvement in fps) while\nretaining the same accuracy as that of the original Faster RCNN model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:40:47 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Mittal", "Deepak", ""], ["Bhardwaj", "Shweta", ""], ["Khapra", "Mitesh M.", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1801.10496", "submitter": "Heng Fan", "authors": "Heng Fan, Haibin Ling", "title": "Parallel Tracking and Verifying", "comments": "Project is available at\n  http://www.dabi.temple.edu/~hbling/code/PTAV/ptav.htm. arXiv admin note: text\n  overlap with arXiv:1708.00153", "journal-ref": null, "doi": "10.1109/TIP.2019.2904789", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being intensively studied, visual object tracking has witnessed great\nadvances in either speed (e.g., with correlation filters) or accuracy (e.g.,\nwith deep features). Real-time and high accuracy tracking algorithms, however,\nremain scarce. In this paper we study the problem from a new perspective and\npresent a novel parallel tracking and verifying (PTAV) framework, by taking\nadvantage of the ubiquity of multi-thread techniques and borrowing ideas from\nthe success of parallel tracking and mapping in visual SLAM. The proposed PTAV\nframework is typically composed of two components, a (base) tracker T and a\nverifier V, working in parallel on two separate threads. The tracker T aims to\nprovide a super real-time tracking inference and is expected to perform well\nmost of the time; by contrast, the verifier V validates the tracking results\nand corrects T when needed. The key innovation is that, V does not work on\nevery frame but only upon the requests from T; on the other end, T may adjust\nthe tracking according to the feedback from V. With such collaboration, PTAV\nenjoys both the high efficiency provided by T and the strong discriminative\npower by V. Meanwhile, to adapt V to object appearance changes over time, we\nmaintain a dynamic target template pool for adaptive verification, resulting in\nfurther performance improvements. In our extensive experiments on popular\nbenchmarks including OTB2015, TC128, UAV20L and VOT2016, PTAV achieves the best\ntracking accuracy among all real-time trackers, and in fact even outperforms\nmany deep learning based algorithms. Moreover, as a general framework, PTAV is\nvery flexible with great potentials for future improvement and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:18:22 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Fan", "Heng", ""], ["Ling", "Haibin", ""]]}, {"id": "1801.10517", "submitter": "Min Fu", "authors": "Qiuhua Liu and Min Fu and Xinqi Gong and Hao Jiang", "title": "Densely Dilated Spatial Pooling Convolutional Network using benign loss\n  functions for imbalanced volumetric prostate segmentation", "comments": "14pages, 5 figures, anonymous review in IJACAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high incidence rate of prostate disease poses a requirement in early\ndetection for diagnosis. As one of the main imaging methods used for prostate\ncancer detection, Magnetic Resonance Imaging (MRI) has wide range of appearance\nand imbalance problems, making automated prostate segmentation fundamental but\nchallenging. Here we propose a novel Densely Dilated Spatial Pooling\nConvolutional Network (DDSP ConNet) in encoder-decoder structure. It employs\ndense structure to combine dilated convolution and global pooling, thus\nsupplies coarse segmentation results from encoder and decoder subnet and\npreserves more contextual information. To obtain richer hierarchical feature\nmaps, residual long connection is furtherly adopted to fuse contexture\nfeatures. Meanwhile, we adopt DSC loss and Jaccard loss functions to train our\nDDSP ConNet. We surprisingly found and proved that, in contrast to re-weighted\ncross entropy, DSC loss and Jaccard loss have a lot of benign properties in\ntheory, including symmetry, continuity and differentiability about the\nparameters of network. Extensive experiments on the MICCAI PROMISE12 challenge\ndataset have been done to corroborate the effectiveness of our DDSP ConNet with\nDSC loss and Jaccard loss. Totally, our method achieves a score of 85.78 in the\ntest dataset, outperforming most of other competitors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 16:01:36 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 11:41:28 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Liu", "Qiuhua", ""], ["Fu", "Min", ""], ["Gong", "Xinqi", ""], ["Jiang", "Hao", ""]]}, {"id": "1801.10562", "submitter": "Min Xu", "authors": "Bo Zhou, Qiang Guo, Xiangrui Zeng, Min Xu", "title": "Feature Decomposition Based Saliency Detection in Electron\n  Cryo-Tomograms", "comments": "14 pages", "journal-ref": "IEEE International Conference on Bioinformatics & Biomedicine,\n  Workshop on Machine Learning in High Resolution Microscopy (BIBM-MLHRM 2018)", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular\nstructures at the submolecular resolution in close to the native state.\nHowever, due to the high degree of structural complexity and imaging limits,\nthe automatic segmentation of cellular components from ECT images is very\ndifficult. To complement and speed up existing segmentation methods, it is\ndesirable to develop a generic cell component segmentation method that is 1)\nnot specific to particular types of cellular components, 2) able to segment\nunknown cellular components, 3) fully unsupervised and does not rely on the\navailability of training data. As an important step towards this goal, in this\npaper, we propose a saliency detection method that computes the likelihood that\na subregion in a tomogram stands out from the background. Our method consists\nof four steps: supervoxel over-segmentation, feature extraction, feature matrix\ndecomposition, and computation of saliency. The method produces a distribution\nmap that represents the regions' saliency in tomograms. Our experiments show\nthat our method can successfully label most salient regions detected by a human\nobserver, and able to filter out regions not containing cellular components.\nTherefore, our method can remove the majority of the background region, and\nsignificantly speed up the subsequent processing of segmentation and\nrecognition of cellular components captured by ECT.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:25:14 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Zhou", "Bo", ""], ["Guo", "Qiang", ""], ["Zeng", "Xiangrui", ""], ["Xu", "Min", ""]]}, {"id": "1801.10585", "submitter": "Timo Hackel", "authors": "Timo Hackel, Mikhail Usvyatsov, Silvano Galliani, Jan D. Wegner,\n  Konrad Schindler", "title": "Inference, Learning and Attention Mechanisms that Exploit and Preserve\n  Sparsity in Convolutional Networks", "comments": "Updated to IJCV version", "journal-ref": null, "doi": "10.1007/s11263-020-01302-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While CNNs naturally lend themselves to densely sampled data, and\nsophisticated implementations are available, they lack the ability to\nefficiently process sparse data. In this work we introduce a suite of tools\nthat exploit sparsity in both the feature maps and the filter weights, and\nthereby allow for significantly lower memory footprints and computation times\nthan the conventional dense framework when processing data with a high degree\nof sparsity. Our scheme provides (i) an efficient GPU implementation of a\nconvolution layer based on direct, sparse convolution; (ii) a filter step\nwithin the convolution layer, which we call attention, that prevents fill-in,\ni.e., the tendency of convolution to rapidly decrease sparsity, and guarantees\nan upper bound on the computational resources; and (iii) an adaptation of the\nback-propagation algorithm, which makes it possible to combine our approach\nwith standard learning frameworks, while still exploiting sparsity in the data\nand the model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 18:12:24 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 09:33:19 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 13:44:04 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Hackel", "Timo", ""], ["Usvyatsov", "Mikhail", ""], ["Galliani", "Silvano", ""], ["Wegner", "Jan D.", ""], ["Schindler", "Konrad", ""]]}, {"id": "1801.10597", "submitter": "Min Xu", "authors": "Jialiang Guo, Bo Zhou, Xiangrui Zeng, Zachary Freyberg, Min Xu", "title": "Model compression for faster structural separation of macromolecules\n  captured by Cellular Electron Cryo-Tomography", "comments": "8 pages", "journal-ref": "International Conference on Image Analysis and Recognition (ICIAR)\n  2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule\nstructure inside single cells. Macromolecule classification approaches based on\nconvolutional neural networks (CNN) were developed to separate millions of\nmacromolecules captured from ECT systematically. However, given the fast\naccumulation of ECT data, it will soon become necessary to use CNN models to\nefficiently and accurately separate substantially more macromolecules at the\nprediction stage, which requires additional computational costs. To speed up\nthe prediction, we compress classification models into compact neural networks\nwith little in accuracy for deployment. Specifically, we propose to perform\nmodel compression through knowledge distillation. Firstly, a complex teacher\nnetwork is trained to generate soft labels with better classification\nfeasibility followed by training of customized student networks with simple\narchitectures using the soft label to compress model complexity. Our tests\ndemonstrate that our compressed models significantly reduce the number of\nparameters and time cost while maintaining similar classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 18:39:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Guo", "Jialiang", ""], ["Zhou", "Bo", ""], ["Zeng", "Xiangrui", ""], ["Freyberg", "Zachary", ""], ["Xu", "Min", ""]]}]