[{"id": "1610.00029", "submitter": "Kardi Teknomo", "authors": "Kardi Teknomo", "title": "Microscopic Pedestrian Flow Characteristics: Development of an Image\n  Processing Data Collection and Simulation Model", "comments": "140 pages, Teknomo, Kardi, Microscopic Pedestrian Flow\n  Characteristics: Development of an Image Processing Data Collection and\n  Simulation Model, Ph.D. Dissertation, Tohoku University Japan, Sendai, 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopic pedestrian studies consider detailed interaction of pedestrians\nto control their movement in pedestrian traffic flow. The tools to collect the\nmicroscopic data and to analyze microscopic pedestrian flow are still very much\nin its infancy. The microscopic pedestrian flow characteristics need to be\nunderstood. Manual, semi manual and automatic image processing data collection\nsystems were developed. It was found that the microscopic speed resemble a\nnormal distribution with a mean of 1.38 m/second and standard deviation of 0.37\nm/second. The acceleration distribution also bear a resemblance to the normal\ndistribution with an average of 0.68 m/ square second. A physical based\nmicroscopic pedestrian simulation model was also developed. Both Microscopic\nVideo Data Collection and Microscopic Pedestrian Simulation Model generate a\ndatabase called NTXY database. The formulations of the flow performance or\nmicroscopic pedestrian characteristics are explained. Sensitivity of the\nsimulation and relationship between the flow performances are described.\nValidation of the simulation using real world data is then explained through\nthe comparison between average instantaneous speed distributions of the real\nworld data with the result of the simulations. The simulation model is then\napplied for some experiments on a hypothetical situation to gain more\nunderstanding of pedestrian behavior in one way and two way situations, to know\nthe behavior of the system if the number of elderly pedestrian increases and to\nevaluate a policy of lane-like segregation toward pedestrian crossing and\ninspects the performance of the crossing. It was revealed that the microscopic\npedestrian studies have been successfully applied to give more understanding to\nthe behavior of microscopic pedestrians flow, predict the theoretical and\npractical situation and evaluate some design policies before its\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 09:46:23 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Teknomo", "Kardi", ""]]}, {"id": "1610.00070", "submitter": "Li Xiao", "authors": "Jia Xu, Zu-Zhen Huang, Zhi-Rui Wang, Li Xiao, Xiang-Gen Xia, and Teng\n  Long", "title": "Radial Velocity Retrieval for Multichannel SAR Moving Targets with\n  Time-Space Doppler De-ambiguity", "comments": "14 double-column pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, with respect to multichannel synthetic aperture radars (SAR),\nwe first formulate the problems of Doppler ambiguities on the radial velocity\n(RV) estimation of a ground moving target in range-compressed domain,\nrange-Doppler domain and image domain, respectively. It is revealed that in\nthese problems, a cascaded time-space Doppler ambiguity (CTSDA) may encounter,\ni.e., time domain Doppler ambiguity (TDDA) in each channel arises first and\nthen spatial domain Doppler ambiguity (SDDA) among multi-channels arises\nsecond. Accordingly, the multichannel SAR systems with different parameters are\ninvestigated in three different cases with diverse Doppler ambiguity\nproperties, and a multi-frequency SAR is then proposed to obtain the RV\nestimation by solving the ambiguity problem based on Chinese remainder theorem\n(CRT). In the first two cases, the ambiguity problem can be solved by the\nexisting closed-form robust CRT. In the third case, it is found that the\nproblem is different from the conventional CRT problems and we call it a double\nremaindering problem in this paper. We then propose a sufficient condition\nunder which the double remaindering problem, i.e., the CTSDA, can also be\nsolved by the closed-form robust CRT. When the sufficient condition is not\nsatisfied for a multi-channel SAR, a searching based method is proposed.\nFinally, some results of numerical experiments are provided to demonstrate the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 01:39:15 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 18:17:19 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 19:58:44 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Xu", "Jia", ""], ["Huang", "Zu-Zhen", ""], ["Wang", "Zhi-Rui", ""], ["Xiao", "Li", ""], ["Xia", "Xiang-Gen", ""], ["Long", "Teng", ""]]}, {"id": "1610.00134", "submitter": "G\\\"okhan \\\"Ozbulak", "authors": "G\\\"okhan \\\"Ozbulak, Yusuf Aytar, Haz{\\i}m Kemal Ekenel", "title": "How Transferable are CNN-based Features for Age and Gender\n  Classification?", "comments": "12 pages, 3 figures, 2 tables, International Conference of the\n  Biometrics Special Interest Group (BIOSIG) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age and gender are complementary soft biometric traits for face recognition.\nSuccessful estimation of age and gender from facial images taken under\nreal-world conditions can contribute improving the identification results in\nthe wild. In this study, in order to achieve robust age and gender\nclassification in the wild, we have benefited from Deep Convolutional Neural\nNetworks based representation. We have explored transferability of existing\ndeep convolutional neural network (CNN) models for age and gender\nclassification. The generic AlexNet-like architecture and domain specific\nVGG-Face CNN model are employed and fine-tuned with the Adience dataset\nprepared for age and gender classification in uncontrolled environments. In\naddition, task specific GilNet CNN model has also been utilized and used as a\nbaseline method in order to compare with transferred models. Experimental\nresults show that both transferred deep CNN models outperform the GilNet CNN\nmodel, which is the state-of-the-art age and gender classification approach on\nthe Adience dataset, by an absolute increase of 7% and 4.5% in accuracy,\nrespectively. This outcome indicates that transferring a deep CNN model can\nprovide better classification performance than a task specific CNN model, which\nhas a limited number of layers and trained from scratch using a limited amount\nof data as in the case of GilNet. Domain specific VGG-Face CNN model has been\nfound to be more useful and provided better performance for both age and gender\nclassification tasks, when compared with generic AlexNet-like model, which\nshows that transfering from a closer domain is more useful.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 13:28:39 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["\u00d6zbulak", "G\u00f6khan", ""], ["Aytar", "Yusuf", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "1610.00163", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Duo Wang, Nicholas D. Lane and Pietro Li\\`o", "title": "X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets", "comments": "To appear in the 7th IEEE Symposium Series on Computational\n  Intelligence (IEEE SSCI 2016), 8 pages, 6 figures. Minor revisions, in\n  response to reviewers' comments", "journal-ref": null, "doi": "10.1109/SSCI.2016.7849978", "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose cross-modal convolutional neural networks (X-CNNs),\na novel biologically inspired type of CNN architectures, treating gradient\ndescent-specialised CNNs as individual units of processing in a larger-scale\nnetwork topology, while allowing for unconstrained information flow and/or\nweight sharing between analogous hidden layers of the network---thus\ngeneralising the already well-established concept of neural network ensembles\n(where information typically may flow only between the output layers of the\nindividual networks). The constituent networks are individually designed to\nlearn the output function on their own subset of the input data, after which\ncross-connections between them are introduced after each pooling operation to\nperiodically allow for information exchange between them. This injection of\nknowledge into a model (by prior partition of the input data through domain\nknowledge or unsupervised methods) is expected to yield greatest returns in\nsparse data environments, which are typically less suitable for training CNNs.\nFor evaluation purposes, we have compared a standard four-layer CNN as well as\na sophisticated FitNet4 architecture against their cross-modal variants on the\nCIFAR-10 and CIFAR-100 datasets with differing percentages of the training data\nbeing removed, and find that at lower levels of data availability, the X-CNNs\nsignificantly outperform their baselines (typically providing a 2--6% benefit,\ndepending on the dataset size and whether data augmentation is used), while\nstill maintaining an edge on all of the full dataset tests.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 18:01:35 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 14:51:36 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Wang", "Duo", ""], ["Lane", "Nicholas D.", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1610.00175", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son, Xiao-Ping Zhang", "title": "Near-Infrared Image Dehazing Via Color Regularization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-infrared imaging can capture haze-free near-infrared gray images and\nvisible color images, according to physical scattering models, e.g., Rayleigh\nor Mie models. However, there exist serious discrepancies in brightness and\nimage structures between the near-infrared gray images and the visible color\nimages. The direct use of the near-infrared gray images brings about another\ncolor distortion problem in the dehazed images. Therefore, the color distortion\nshould also be considered for near-infrared dehazing. To reflect this point,\nthis paper presents an approach of adding a new color regularization to\nconventional dehazing framework. The proposed color regularization can model\nthe color prior for unknown haze-free images from two captured images. Thus,\nnatural-looking colors and fine details can be induced on the dehazed images.\nThe experimental results show that the proposed color regularization model can\nhelp remove the color distortion and the haze at the same time. Also, the\neffectiveness of the proposed color regularization is verified by comparing\nwith other conventional regularizations. It is also shown that the proposed\ncolor regularization can remove the edge artifacts which arise from the use of\nthe conventional dark prior model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 19:46:24 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Son", "Chang-Hwan", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "1610.00279", "submitter": "Andrey Makarenko", "authors": "A.V. Makarenko", "title": "Deep Learning Algorithms for Signal Recognition in Long Perimeter\n  Monitoring Distributed Fiber Optic Sensors", "comments": "11 pages, 7 figures, 2 tables. Slightly extended preprint of paper\n  accepted for IEEE MLSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show an approach to build deep learning algorithms for\nrecognizing signals in distributed fiber optic monitoring and security systems\nfor long perimeters. Synthesizing such detection algorithms poses a non-trivial\nresearch and development challenge, because these systems face stringent error\n(type I and II) requirements and operate in difficult signal-jamming\nenvironments, with intensive signal-like jamming and a variety of changing\npossible signal portraits of possible recognized events. To address these\nissues, we have developed a twolevel event detection architecture, where the\nprimary classifier is based on an ensemble of deep convolutional networks, can\nrecognize 7 classes of signals and receives time-space data frames as input.\nUsing real-life data, we have shown that the applied methods result in\nefficient and robust multiclass detection algorithms that have a high degree of\nadaptability.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 13:46:47 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Makarenko", "A. V.", ""]]}, {"id": "1610.00291", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Linlin Shen, Ke Sun, Guoping Qiu", "title": "Deep Feature Consistent Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for constructing Variational Autoencoder (VAE).\nInstead of using pixel-by-pixel loss, we enforce deep feature consistency\nbetween the input and the output of a VAE, which ensures the VAE's output to\npreserve the spatial correlation characteristics of the input, thus leading the\noutput to have a more natural visual appearance and better perceptual quality.\nBased on recent deep learning works such as style transfer, we employ a\npre-trained deep convolutional neural network (CNN) and use its hidden features\nto define a feature perceptual loss for VAE training. Evaluated on the CelebA\nface dataset, we show that our model produces better results than other methods\nin the literature. We also show that our method can produce latent vectors that\ncan capture the semantic information of face expressions and can be used to\nachieve state-of-the-art performance in facial attribute prediction.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 15:48:36 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Hou", "Xianxu", ""], ["Shen", "Linlin", ""], ["Sun", "Ke", ""], ["Qiu", "Guoping", ""]]}, {"id": "1610.00307", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Moin Nabi, Hossein Mousavi, Enver Sangineto, Nicu\n  Sebe", "title": "Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal\n  Event Detection", "comments": "To appear at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the crowd abnormal event detection methods rely on complex\nhand-crafted features to represent the crowd motion and appearance.\nConvolutional Neural Networks (CNN) have shown to be a powerful tool with\nexcellent representational capacities, which can leverage the need for\nhand-crafted features. In this paper, we show that keeping track of the changes\nin the CNN feature across time can facilitate capturing the local abnormality.\nWe specifically propose a novel measure-based method which allows measuring the\nlocal abnormality in a video by combining semantic information (inherited from\nexisting CNN models) with low-level Optical-Flow. One of the advantage of this\nmethod is that it can be used without the fine-tuning costs. The proposed\nmethod is validated on challenging abnormality detection datasets and the\nresults show the superiority of our method compared to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 16:39:35 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 12:17:02 GMT"}, {"version": "v3", "created": "Sat, 27 Jan 2018 00:35:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Nabi", "Moin", ""], ["Mousavi", "Hossein", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "1610.00318", "submitter": "Hamid Tizhoosh", "authors": "H.R. Tizhoosh, Shujin Zhu, Hanson Lo, Varun Chaudhari, Tahmid Mehdi", "title": "MinMax Radon Barcodes for Medical Image Retrieval", "comments": "To appear in proceedings of the 12th International Symposium on\n  Visual Computing, December 12-14, 2016, Las Vegas, Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based medical image retrieval can support diagnostic decisions by\nclinical experts. Examining similar images may provide clues to the expert to\nremove uncertainties in his/her final diagnosis. Beyond conventional feature\ndescriptors, binary features in different ways have been recently proposed to\nencode the image content. A recent proposal is \"Radon barcodes\" that employ\nbinarized Radon projections to tag/annotate medical images with content-based\nbinary vectors, called barcodes. In this paper, MinMax Radon barcodes are\nintroduced which are superior to \"local thresholding\" scheme suggested in the\nliterature. Using IRMA dataset with 14,410 x-ray images from 193 different\nclasses, the advantage of using MinMax Radon barcodes over \\emph{thresholded}\nRadon barcodes are demonstrated. The retrieval error for direct search drops by\nmore than 15\\%. As well, SURF, as a well-established non-binary approach, and\nBRISK, as a recent binary method are examined to compare their results with\nMinMax Radon barcodes when retrieving images from IRMA dataset. The results\ndemonstrate that MinMax Radon barcodes are faster and more accurate when\napplied on IRMA images.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 17:29:01 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Tizhoosh", "H. R.", ""], ["Zhu", "Shujin", ""], ["Lo", "Hanson", ""], ["Chaudhari", "Varun", ""], ["Mehdi", "Tahmid", ""]]}, {"id": "1610.00320", "submitter": "Hamid Tizhoosh", "authors": "S. Sharma, I. Umar, L. Ospina, D. Wong, H.R. Tizhoosh", "title": "Stacked Autoencoders for Medical Image Search", "comments": "To appear in proceedings of the 12th International Symposium on\n  Visual Computing, December 12-14, 2016, Las Vegas, Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images can be a valuable resource for reliable information to support\nmedical diagnosis. However, the large volume of medical images makes it\nchallenging to retrieve relevant information given a particular scenario. To\nsolve this challenge, content-based image retrieval (CBIR) attempts to\ncharacterize images (or image regions) with invariant content information in\norder to facilitate image search. This work presents a feature extraction\ntechnique for medical images using stacked autoencoders, which encode images to\nbinary vectors. The technique is applied to the IRMA dataset, a collection of\n14,410 x-ray images in order to demonstrate the ability of autoencoders to\nretrieve similar x-rays given test queries. Using IRMA dataset as a benchmark,\nit was found that stacked autoencoders gave excellent results with a retrieval\nerror of 376 for 1,733 test images with a compression of 74.61%.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 17:34:02 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Sharma", "S.", ""], ["Umar", "I.", ""], ["Ospina", "L.", ""], ["Wong", "D.", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1610.00321", "submitter": "Yi Zhang", "authors": "Hu Chen, Yi Zhang, Weihua Zhang, Peixi Liao, Ke Li, Jiliu Zhou and Ge\n  Wang", "title": "Low-dose CT denoising with convolutional neural network", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.08508", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the potential radiation risk, low-dose CT has attracted much\nattention. However, simply lowering the radiation dose will lead to significant\ndeterioration of the image quality. In this paper, we propose a noise reduction\nmethod for low-dose CT via deep neural network without accessing original\nprojection data. A deep convolutional neural network is trained to transform\nlow-dose CT images towards normal-dose CT images, patch by patch. Visual and\nquantitative evaluation demonstrates a competing performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 17:35:58 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Chen", "Hu", ""], ["Zhang", "Yi", ""], ["Zhang", "Weihua", ""], ["Liao", "Peixi", ""], ["Li", "Ke", ""], ["Zhou", "Jiliu", ""], ["Wang", "Ge", ""]]}, {"id": "1610.00382", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son, Xiao-Ping Zhang", "title": "Near-Infrared Coloring via a Contrast-Preserving Mapping Model", "comments": "12 pages", "journal-ref": "IEEE Transactions on Image Processing, vol. 26, no. 11, pp.\n  5381-5394, Nov. 2017", "doi": "10.1109/TIP.2017.2724241", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-infrared gray images captured together with corresponding visible color\nimages have recently proven useful for image restoration and classification.\nThis paper introduces a new coloring method to add colors to near-infrared gray\nimages based on a contrast-preserving mapping model. A naive coloring method\ndirectly adds the colors from the visible color image to the near-infrared gray\nimage; however, this method results in an unrealistic image because of the\ndiscrepancies in brightness and image structure between the captured\nnear-infrared gray image and the visible color image. To solve the discrepancy\nproblem, first we present a new contrast-preserving mapping model to create a\nnew near-infrared gray image with a similar appearance in the luminance plane\nto the visible color image, while preserving the contrast and details of the\ncaptured near-infrared gray image. Then based on the proposed\ncontrast-preserving mapping model, we develop a method to derive realistic\ncolors that can be added to the newly created near-infrared gray image.\nExperimental results show that the proposed method can not only preserve the\nlocal contrasts and details of the captured near-infrared gray image, but\ntransfers the realistic colors from the visible color image to the newly\ncreated near-infrared gray image. Experimental results also show that the\nproposed approach can be applied to near-infrared denoising.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 01:08:20 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Son", "Chang-Hwan", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "1610.00386", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son, Xiao-Ping Zhang", "title": "Rain Removal via Shrinkage-Based Sparse Coding and Learned Rain\n  Dictionary", "comments": "17 pages", "journal-ref": "Journal of Imaging Science and Technology, vol. 64, no. 3, pp.\n  30501-1-30501-17(17), May 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new rain removal model based on the shrinkage of the\nsparse codes for a single image. Recently, dictionary learning and sparse\ncoding have been widely used for image restoration problems. These methods can\nalso be applied to the rain removal by learning two types of rain and non-rain\ndictionaries and forcing the sparse codes of the rain dictionary to be zero\nvectors. However, this approach can generate unwanted edge artifacts and detail\nloss in the non-rain regions. Based on this observation, a new approach for\nshrinking the sparse codes is presented in this paper. To effectively shrink\nthe sparse codes in the rain and non-rain regions, an error map between the\ninput rain image and the reconstructed rain image is generated by using the\nlearned rain dictionary. Based on this error map, both the sparse codes of rain\nand non-rain dictionaries are used jointly to represent the image structures of\nobjects and avoid the edge artifacts in the non-rain regions. In the rain\nregions, the correlation matrix between the rain and non-rain dictionaries is\ncalculated. Then, the sparse codes corresponding to the highly correlated\nsignal-atoms in the rain and non-rain dictionaries are shrunk jointly to\nimprove the removal of the rain structures. The experimental results show that\nthe proposed shrinkage-based sparse coding can preserve image structures and\navoid the edge artifacts in the non-rain regions, and it can remove the rain\nstructures in the rain regions. Also, visual quality evaluation confirms that\nthe proposed method outperforms the conventional texture and rain removal\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 02:03:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Son", "Chang-Hwan", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "1610.00405", "submitter": "Bo Chen", "authors": "Bo Chen and Pietro Perona", "title": "Seeing into Darkness: Scotopic Visual Recognition", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are formed by counting how many photons traveling from a given set of\ndirections hit an image sensor during a given time interval. When photons are\nfew and far in between, the concept of `image' breaks down and it is best to\nconsider directly the flow of photons. Computer vision in this regime, which we\ncall `scotopic', is radically different from the classical image-based paradigm\nin that visual computations (classification, control, search) have to take\nplace while the stream of photons is captured and decisions may be taken as\nsoon as enough information is available. The scotopic regime is important for\nbiomedical imaging, security, astronomy and many other fields. Here we develop\na framework that allows a machine to classify objects with as few photons as\npossible, while maintaining the error rate below an acceptable threshold. A\ndynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of\nthis framework. We propose and study an algorithm to optimize the tradeoff of a\nconvolutional network directly from lowlight images and evaluate on simulated\nimages from standard datasets. Surprisingly, scotopic systems can achieve\ncomparable classification performance as traditional vision systems while using\nless than 0.1% of the photons in a conventional image. In addition, we\ndemonstrate that our algorithms work even when the illuminance of the\nenvironment is unknown and varying. Last, we outline a spiking neural network\ncoupled with photon-counting sensors as a power-efficient hardware realization\nof scotopic algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 05:03:45 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Chen", "Bo", ""], ["Perona", "Pietro", ""]]}, {"id": "1610.00410", "submitter": "Patrick Virtue", "authors": "Patrick Virtue and Michael Lustig", "title": "On the Empirical Effect of Gaussian Noise in Under-sampled MRI\n  Reconstruction", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Fourier-based medical imaging, sampling below the Nyquist rate results in\nan underdetermined system, in which linear reconstructions will exhibit\nartifacts. Another consequence of under-sampling is lower signal to noise ratio\n(SNR) due to fewer acquired measurements. Even if an oracle provided the\ninformation to perfectly disambiguate the underdetermined system, the\nreconstructed image could still have lower image quality than a corresponding\nfully sampled acquisition because of the reduced measurement time. The effects\nof lower SNR and the underdetermined system are coupled during reconstruction,\nmaking it difficult to isolate the impact of lower SNR on image quality. To\nthis end, we present an image quality prediction process that reconstructs\nfully sampled, fully determined data with noise added to simulate the loss of\nSNR induced by a given under-sampling pattern. The resulting prediction image\nempirically shows the effect of noise in under-sampled image reconstruction\nwithout any effect from an underdetermined system.\n  We discuss how our image quality prediction process can simulate the\ndistribution of noise for a given under-sampling pattern, including variable\ndensity sampling that produces colored noise in the measurement data. An\ninteresting consequence of our prediction model is that we can show that\nrecovery from underdetermined non-uniform sampling is equivalent to a weighted\nleast squares optimization that accounts for heterogeneous noise levels across\nmeasurements.\n  Through a series of experiments with synthetic and in vivo datasets, we\ndemonstrate the efficacy of the image quality prediction process and show that\nit provides a better estimation of reconstruction image quality than the\ncorresponding fully-sampled reference image.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 05:28:06 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Virtue", "Patrick", ""], ["Lustig", "Michael", ""]]}, {"id": "1610.00427", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son, Xiao-Ping Zhang", "title": "Rain structure transfer using an exemplar rain image for synthetic rain\n  image generation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a simple method of transferring rain structures of a\ngiven exemplar rain image into a target image. Given the exemplar rain image\nand its corresponding masked rain image, rain patches including rain structures\nare extracted randomly, and then residual rain patches are obtained by\nsubtracting those rain patches from their mean patches. Next, residual rain\npatches are selected randomly, and then added to the given target image along a\nraster scanning direction. To decrease boundary artifacts around the added\npatches on the target image, minimum error boundary cuts are found using\ndynamic programming, and then blending is conducted between overlapping\npatches. Our experiment shows that the proposed method can generate realistic\nrain images that have similar rain structures in the exemplar images. Moreover,\nit is expected that the proposed method can be used for rain removal. More\nspecifically, natural images and synthetic rain images generated via the\nproposed method can be used to learn classifiers, for example, deep neural\nnetworks, in a supervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 06:58:43 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Son", "Chang-Hwan", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "1610.00527", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka,\n  Oriol Vinyals, Alex Graves, Koray Kavukcuoglu", "title": "Video Pixel Networks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic video model, the Video Pixel Network (VPN), that\nestimates the discrete joint distribution of the raw pixel values in a video.\nThe model and the neural architecture reflect the time, space and color\nstructure of video tensors and encode it as a four-dimensional dependency\nchain. The VPN approaches the best possible performance on the Moving MNIST\nbenchmark, a leap over the previous state of the art, and the generated videos\nshow only minor deviations from the ground truth. The VPN also produces\ndetailed samples on the action-conditional Robotic Pushing benchmark and\ngeneralizes to the motion of novel objects.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 13:06:40 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Oord", "Aaron van den", ""], ["Simonyan", "Karen", ""], ["Danihelka", "Ivo", ""], ["Vinyals", "Oriol", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1610.00660", "submitter": "Samik Banerjee", "authors": "Samik Banerjee, Sukhendu Das", "title": "Kernel Selection using Multiple Kernel Learning and Domain Adaptation in\n  Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance\n  Scenario", "comments": "13 pages, 15 figures, 4 tables. Kernel Selection, Surveillance,\n  Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Recognition (FR) has been the interest to several researchers over the\npast few decades due to its passive nature of biometric authentication. Despite\nhigh accuracy achieved by face recognition algorithms under controlled\nconditions, achieving the same performance for face images obtained in\nsurveillance scenarios, is a major hurdle. Some attempts have been made to\nsuper-resolve the low-resolution face images and improve the contrast, without\nconsiderable degree of success. The proposed technique in this paper tries to\ncope with the very low resolution and low contrast face images obtained from\nsurveillance cameras, for FR under surveillance conditions. For Support Vector\nMachine classification, the selection of appropriate kernel has been a widely\ndiscussed issue in the research community. In this paper, we propose a novel\nkernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to\nobtain the best feature-kernel pairing. Our proposed technique employs a\neffective kernel selection by Multiple Kernel Learning (MKL) method, to choose\nthe optimal kernel to be used along with unsupervised domain adaptation method\nin the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem.\nRigorous experimentation has been performed on three real-world surveillance\nface datasets : FR\\_SURV, SCface and ChokePoint. Results have been shown using\nRank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method\noutperforms all other recent state-of-the-art techniques by a considerable\nmargin.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 18:22:03 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Banerjee", "Samik", ""], ["Das", "Sukhendu", ""]]}, {"id": "1610.00696", "submitter": "Chelsea Finn", "authors": "Chelsea Finn and Sergey Levine", "title": "Deep Visual Foresight for Planning Robot Motion", "comments": "ICRA 2017. Supplementary video:\n  https://sites.google.com/site/robotforesight/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in scaling up robot learning to many skills and environments\nis removing the need for human supervision, so that robots can collect their\nown data and improve their own performance without being limited by the cost of\nrequesting human feedback. Model-based reinforcement learning holds the promise\nof enabling an agent to learn to predict the effects of its actions, which\ncould provide flexible predictive models for a wide range of tasks and\nenvironments, without detailed human supervision. We develop a method for\ncombining deep action-conditioned video prediction models with model-predictive\ncontrol that uses entirely unlabeled training data. Our approach does not\nrequire a calibrated camera, an instrumented training set-up, nor precise\nsensing and actuation. Our results show that our method enables a real robot to\nperform nonprehensile manipulation -- pushing objects -- and can handle novel\nobjects not seen during training.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 19:54:17 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 00:18:49 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.00731", "submitter": "Michael Ying Yang", "authors": "Siva Karthik Mustikovela, Michael Ying Yang, Carsten Rother", "title": "Can Ground Truth Label Propagation from Video help Semantic\n  Segmentation?", "comments": "To appear at ECCV 2016 Workshop on Video Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For state-of-the-art semantic segmentation task, training convolutional\nneural networks (CNNs) requires dense pixelwise ground truth (GT) labeling,\nwhich is expensive and involves extensive human effort. In this work, we study\nthe possibility of using auxiliary ground truth, so-called \\textit{pseudo\nground truth} (PGT) to improve the performance. The PGT is obtained by\npropagating the labels of a GT frame to its subsequent frames in the video\nusing a simple CRF-based, cue integration framework. Our main contribution is\nto demonstrate the use of noisy PGT along with GT to improve the performance of\na CNN. We perform a systematic analysis to find the right kind of PGT that\nneeds to be added along with the GT for training a CNN. In this regard, we\nexplore three aspects of PGT which influence the learning of a CNN: i) the PGT\nlabeling has to be of good quality; ii) the PGT images have to be different\ncompared to the GT images; iii) the PGT has to be trusted differently than GT.\nWe conclude that PGT which is diverse from GT images and has good quality of\nlabeling can indeed help improve the performance of a CNN. Also, when PGT is\nmultiple folds larger than GT, weighing down the trust on PGT helps in\nimproving the accuracy. Finally, We show that using PGT along with GT, the\nperformance of Fully Convolutional Network (FCN) on Camvid data is increased by\n$2.7\\%$ on IoU accuracy. We believe such an approach can be used to train CNNs\nfor semantic video segmentation where sequentially labeled image frames are\nneeded. To this end, we provide recommendations for using PGT strategically for\nsemantic segmentation and hence bypass the need for extensive human efforts in\nlabeling.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 20:29:12 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Mustikovela", "Siva Karthik", ""], ["Yang", "Michael Ying", ""], ["Rother", "Carsten", ""]]}, {"id": "1610.00748", "submitter": "Michael Ying Yang", "authors": "Omid Hosseini jafari and Michael Ying Yang", "title": "Real-Time RGB-D based Template Matching Pedestrian Detection", "comments": "published in ICRA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is one of the most popular topics in computer vision and\nrobotics. Considering challenging issues in multiple pedestrian detection, we\npresent a real-time depth-based template matching people detector. In this\npaper, we propose different approaches for training the depth-based template.\nWe train multiple templates for handling issues due to various upper-body\norientations of the pedestrians and different levels of detail in depth-map of\nthe pedestrians with various distances from the camera. And, we take into\naccount the degree of reliability for different regions of sliding window by\nproposing the weighted template approach. Furthermore, we combine the\ndepth-detector with an appearance based detector as a verifier to take\nadvantage of the appearance cues for dealing with the limitations of depth\ndata. We evaluate our method on the challenging ETH dataset sequence. We show\nthat our method outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 20:57:17 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["jafari", "Omid Hosseini", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1610.00759", "submitter": "Cornelia Fermuller Cornelia Fermuller", "authors": "Cornelia Ferm\\\"uller, Fang Wang, Yezhou Yang, Konstantinos\n  Zampogiannis, Yi Zhang, Francisco Barranco, Michael Pfeiffer", "title": "Prediction of Manipulation Actions", "comments": "15 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking at a person's hands one often can tell what the person is going to do\nnext, how his/her hands are moving and where they will be, because an actor's\nintentions shape his/her movement kinematics during action execution.\nSimilarly, active systems with real-time constraints must not simply rely on\npassive video-segment classification, but they have to continuously update\ntheir estimates and predict future actions. In this paper, we study the\nprediction of dexterous actions. We recorded from subjects performing different\nmanipulation actions on the same object, such as \"squeezing\", \"flipping\",\n\"washing\", \"wiping\" and \"scratching\" with a sponge. In psychophysical\nexperiments, we evaluated human observers' skills in predicting actions from\nvideo sequences of different length, depicting the hand movement in the\npreparation and execution of actions before and after contact with the object.\nWe then developed a recurrent neural network based method for action prediction\nusing as input patches around the hand. We also used the same formalism to\npredict the forces on the finger tips using for training synchronized video and\nforce data streams. Evaluations on two new datasets showed that our system\nclosely matches human performance in the recognition task, and demonstrate the\nability of our algorithm to predict what and how a dexterous action is\nperformed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 21:23:13 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Ferm\u00fcller", "Cornelia", ""], ["Wang", "Fang", ""], ["Yang", "Yezhou", ""], ["Zampogiannis", "Konstantinos", ""], ["Zhang", "Yi", ""], ["Barranco", "Francisco", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1610.00824", "submitter": "Dacheng Tao", "authors": "Shaoli Huang and Dacheng Tao", "title": "Real Time Fine-Grained Categorization with Accuracy and Interpretability", "comments": "arXiv admin note: text overlap with arXiv:1512.08086", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-designed fine-grained categorization system usually has three\ncontradictory requirements: accuracy (the ability to identify objects among\nsubordinate categories); interpretability (the ability to provide\nhuman-understandable explanation of recognition system behavior); and\nefficiency (the speed of the system). To handle the trade-off between accuracy\nand interpretability, we propose a novel \"Deeper Part-Stacked CNN\" architecture\narmed with interpretability by modeling subtle differences between object\nparts. The proposed architecture consists of a part localization network, a\ntwo-stream classification network that simultaneously encodes object-level and\npart-level cues, and a feature vectors fusion component. Specifically, the part\nlocalization network is implemented by exploring a new paradigm for key point\nlocalization that first samples a small number of representable pixels and then\ndetermine their labels via a convolutional layer followed by a softmax layer.\nWe also use a cropping layer to extract part features and propose a scale\nmean-max layer for feature fusion learning. Experimentally, our proposed method\noutperform state-of-the-art approaches both in part localization task and\nclassification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set\nof sharing strategies between the computation of multiple object parts, our\nsingle model is fairly efficient running at 32 frames/sec.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 02:20:18 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Huang", "Shaoli", ""], ["Tao", "Dacheng", ""]]}, {"id": "1610.00838", "submitter": "Yubin Deng", "authors": "Yubin Deng, Chen Change Loy, Xiaoou Tang", "title": "Image Aesthetic Assessment: An Experimental Survey", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2017.2696576", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey aims at reviewing recent computer vision techniques used in the\nassessment of image aesthetic quality. Image aesthetic assessment aims at\ncomputationally distinguishing high-quality photos from low-quality ones based\non photographic rules, typically in the form of binary classification or\nquality scoring. A variety of approaches has been proposed in the literature\ntrying to solve this challenging problem. In this survey, we present a\nsystematic listing of the reviewed approaches based on visual feature types\n(hand-crafted features and deep features) and evaluation criteria (dataset\ncharacteristics and evaluation metrics). Main contributions and novelties of\nthe reviewed approaches are highlighted and discussed. In addition, following\nthe emergence of deep learning techniques, we systematically evaluate recent\ndeep learning settings that are useful for developing a robust deep model for\naesthetic scoring. Experiments are conducted using simple yet solid baselines\nthat are competitive with the current state-of-the-arts. Moreover, we discuss\nthe possibility of manipulating the aesthetics of images through computational\napproaches. We hope that our survey could serve as a comprehensive reference\nsource for future research on the study of image aesthetic assessment.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 04:09:41 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 03:32:41 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Deng", "Yubin", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1610.00889", "submitter": "Jiayu Shu", "authors": "Jiayu Shu, Rui Zheng, and Pan Hui", "title": "Cardea: Context-Aware Visual Privacy Protection from Pervasive Cameras", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of mobile and wearable devices with built-in cameras,\nthe bright prospect of camera related applications such as augmented reality\nand life-logging system, the increased ease of taking and sharing photos, and\nadvances in computer vision techniques have greatly facilitated people's lives\nin many aspects, but have also inevitably raised people's concerns about visual\nprivacy at the same time. Motivated by recent user studies that people's\nprivacy concerns are dependent on the context, in this paper, we propose\nCardea, a context-aware and interactive visual privacy protection framework\nthat enforces privacy protection according to people's privacy preferences. The\nframework provides people with fine-grained visual privacy protection using: i)\npersonal privacy profiles, with which people can define their context-dependent\nprivacy preferences; and ii) visual indicators: face features, for devices to\nautomatically locate individuals who request privacy protection; and iii) hand\ngestures, for people to flexibly interact with cameras to temporarily change\ntheir privacy preferences. We design and implement the framework consisting of\nthe client app on Android devices and the cloud server. Our evaluation results\nconfirm this framework is practical and effective with 86% overall accuracy,\nshowing promising future for context-aware visual privacy protection from\npervasive cameras.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 08:01:27 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Shu", "Jiayu", ""], ["Zheng", "Rui", ""], ["Hui", "Pan", ""]]}, {"id": "1610.00893", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, Nauman Shahid, Ulf Skoglund and Pierre Vandergheynst", "title": "Adaptive Graph-based Total Variation for Tomographic Reconstructions", "comments": "8 Pages, 5 page letter, 3 page supplement, 8 Figures, Accepted for\n  publication: IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity exploiting image reconstruction (SER) methods have been extensively\nused with Total Variation (TV) regularization for tomographic reconstructions.\nLocal TV methods fail to preserve texture details and often create additional\nartefacts due to over-smoothing. Non-Local TV (NLTV) methods have been proposed\nas a solution to this but they either lack continuous updates due to\ncomputational constraints or limit the locality to a small region. In this\npaper, we propose Adaptive Graph-based TV (AGTV). The proposed method goes\nbeyond spatial similarity between different regions of an image being\nreconstructed by establishing a connection between similar regions in the\nentire image regardless of spatial distance. As compared to NLTV the proposed\nmethod is computationally efficient and involves updating the graph prior\nduring every iteration making the connection between similar regions stronger.\nMoreover, it promotes sparsity in the wavelet and graph gradient domains. Since\nTV is a special case of graph TV the proposed method can also be seen as a\ngeneralization of SER and TV methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 08:13:07 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 09:50:34 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 12:18:34 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Mahmood", "Faisal", ""], ["Shahid", "Nauman", ""], ["Skoglund", "Ulf", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1610.01052", "submitter": "Md Momin Al Aziz", "authors": "Rezaul Karim, Md. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman", "title": "A novel and effective scoring scheme for structure classification and\n  pairwise similarity measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein tertiary structure defines its functions, classification and binding\nsites. Similar structural characteristics between two proteins often lead to\nthe similar characteristics thereof. Determining structural similarity\naccurately in real time is a crucial research issue. In this paper, we present\na novel and effective scoring scheme that is dependent on novel features\nextracted from protein alpha carbon distance matrices. Our scoring scheme is\ninspired from pattern recognition and computer vision. Our method is\nsignificantly better than the current state of the art methods in terms of\nfamily match of pairs of protein structures and other statistical measurements.\nThe effectiveness of our method is tested on standard benchmark structures. A\nweb service is available at http://research.buet.ac.bd:8080/Comograd/score.html\nwhere you can get the similarity measurement score between two protein\nstructures based on our method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 15:47:45 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Karim", "Rezaul", ""], ["Aziz", "Md. Momin Al", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1610.01066", "submitter": "Hojjat Seyed Mousavi", "authors": "Hojjat S. Mousavi and Vishal Monga", "title": "Sparsity-based Color Image Super Resolution via Exploiting Cross Channel\n  Constraints", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2704443", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity constrained single image super-resolution (SR) has been of much\nrecent interest. A typical approach involves sparsely representing patches in a\nlow-resolution (LR) input image via a dictionary of example LR patches, and\nthen using the coefficients of this representation to generate the\nhigh-resolution (HR) output via an analogous HR dictionary. However, most\nexisting sparse representation methods for super resolution focus on the\nluminance channel information and do not capture interactions between color\nchannels. In this work, we extend sparsity based super-resolution to multiple\ncolor channels by taking color information into account. Edge similarities\namongst RGB color bands are exploited as cross channel correlation constraints.\nThese additional constraints lead to a new optimization problem which is not\neasily solvable; however, a tractable solution is proposed to solve it\nefficiently. Moreover, to fully exploit the complementary information among\ncolor channels, a dictionary learning method is also proposed specifically to\nlearn color dictionaries that encourage edge similarities. Merits of the\nproposed method over state of the art are demonstrated both visually and\nquantitatively using image quality metrics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 16:15:36 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""]]}, {"id": "1610.01068", "submitter": "Rafal Scherer", "authors": "Marcin Korytkowski, Leszek Rutkowski, Rafa{\\l} Scherer", "title": "Fast Image Classification by Boosting Fuzzy Classifiers", "comments": "1 figure", "journal-ref": "Inf. Sci. (327) 2016 175-182", "doi": "10.1016/j.ins.2015.08.030", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to visual objects classification based\non generating simple fuzzy classifiers using local image features to\ndistinguish between one known class and other classes. Boosting meta learning\nis used to find the most representative local features. The proposed approach\nis tested on a state-of-the-art image dataset and compared with the\nbag-of-features image representation model combined with the Support Vector\nMachine classification. The novel method gives better classification accuracy\nand the time of learning and testing process is more than 30% shorter.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 16:18:57 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Korytkowski", "Marcin", ""], ["Rutkowski", "Leszek", ""], ["Scherer", "Rafa\u0142", ""]]}, {"id": "1610.01076", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Tutorial on Answering Questions about Images with Deep Learning", "comments": "The tutorial was presented at '2nd Summer School on Integrating\n  Vision and Language: Deep Learning' in Malta, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Together with the development of more accurate methods in Computer Vision and\nNatural Language Understanding, holistic architectures that answer on questions\nabout the content of real-world images have emerged. In this tutorial, we build\na neural-based approach to answer questions about images. We base our tutorial\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\nmodels that we present here can achieve a competitive performance on both\ndatasets, in fact, they are among the best methods that use a combination of\nLSTM with a global, full frame CNN representation of an image. We hope that\nafter reading this tutorial, the reader will be able to use Deep Learning\nframeworks, such as Keras and introduced Kraino, to build various architectures\nthat will lead to a further performance improvement on this challenging task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 16:29:28 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1610.01119", "submitter": "Limin Wang", "authors": "Limin Wang, Sheng Guo, Weilin Huang, Yuanjun Xiong, Yu Qiao", "title": "Knowledge Guided Disambiguation for Large-Scale Scene Classification\n  with Multi-Resolution CNNs", "comments": "To appear in IEEE Transactions on Image Processing. Code and models\n  are available at https://github.com/wanglimin/MRCNN-Scene-Recognition", "journal-ref": null, "doi": "10.1109/TIP.2017.2675339", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have made remarkable progress on scene\nrecognition, partially due to these recent large-scale scene datasets, such as\nthe Places and Places2. Scene categories are often defined by multi-level\ninformation, including local objects, global layout, and background\nenvironment, thus leading to large intra-class variations. In addition, with\nthe increasing number of scene categories, label ambiguity has become another\ncrucial issue in large-scale classification. This paper focuses on large-scale\nscene recognition and makes two major contributions to tackle these issues.\nFirst, we propose a multi-resolution CNN architecture that captures visual\ncontent and structure at multiple levels. The multi-resolution CNNs are\ncomposed of coarse resolution CNNs and fine resolution CNNs, which are\ncomplementary to each other. Second, we design two knowledge guided\ndisambiguation techniques to deal with the problem of label ambiguity. (i) We\nexploit the knowledge from the confusion matrix computed on validation data to\nmerge ambiguous classes into a super category. (ii) We utilize the knowledge of\nextra networks to produce a soft label for each image. Then the super\ncategories or soft labels are employed to guide CNN training on the Places2. We\nconduct extensive experiments on three large-scale image datasets (ImageNet,\nPlaces, and Places2), demonstrating the effectiveness of our approach.\nFurthermore, our method takes part in two major scene recognition challenges,\nand achieves the second place at the Places2 challenge in ILSVRC 2015, and the\nfirst place at the LSUN challenge in CVPR 2016. Finally, we directly test the\nlearned representations on other scene benchmarks, and obtain the new\nstate-of-the-art results on the MIT Indoor67 (86.7\\%) and SUN397 (72.0\\%). We\nrelease the code and models\nat~\\url{https://github.com/wanglimin/MRCNN-Scene-Recognition}.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 18:33:32 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 21:00:55 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Wang", "Limin", ""], ["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Xiong", "Yuanjun", ""], ["Qiao", "Yu", ""]]}, {"id": "1610.01206", "submitter": "Yingming Li", "authors": "Yingming Li, Ming Yang, and Zhongfei Zhang", "title": "A Survey of Multi-View Representation Learning", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": "10.1109/TKDE.2018.2872063", "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 17:14:15 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 03:11:53 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 08:08:22 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 05:52:06 GMT"}, {"version": "v5", "created": "Wed, 24 Oct 2018 02:34:53 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Li", "Yingming", ""], ["Yang", "Ming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1610.01223", "submitter": "Marc-Andr\\'e Carbonneau", "authors": "Marc-Andr\\'e Carbonneau, Eric Granger, Yazid Attabi and Ghyslain\n  Gagnon", "title": "Feature Learning from Spectrograms for Assessment of Personality Traits", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": "10.1109/TAFFC.2017.2763132", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have recently been proposed to analyze speech and\nautomatically infer the personality of the speaker. These methods often rely on\nprosodic and other hand crafted speech processing features extracted with\noff-the-shelf toolboxes. To achieve high accuracy, numerous features are\ntypically extracted using complex and highly parameterized algorithms. In this\npaper, a new method based on feature learning and spectrogram analysis is\nproposed to simplify the feature extraction process while maintaining a high\nlevel of accuracy. The proposed method learns a dictionary of discriminant\nfeatures from patches extracted in the spectrogram representations of training\nspeech segments. Each speech segment is then encoded using the dictionary, and\nthe resulting feature set is used to perform classification of personality\ntraits. Experiments indicate that the proposed method achieves state-of-the-art\nresults with a significant reduction in complexity when compared to the most\nrecent reference methods. The number of features, and difficulties linked to\nthe feature extraction process are greatly reduced as only one type of\ndescriptors is used, for which the 6 parameters can be tuned automatically. In\ncontrast, the simplest reference method uses 4 types of descriptors to which 6\nfunctionals are applied, resulting in over 20 parameters to be tuned.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 22:38:35 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Carbonneau", "Marc-Andr\u00e9", ""], ["Granger", "Eric", ""], ["Attabi", "Yazid", ""], ["Gagnon", "Ghyslain", ""]]}, {"id": "1610.01238", "submitter": "Dan Barnes", "authors": "Dan Barnes, Will Maddern and Ingmar Posner", "title": "Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for\n  Urban Autonomy", "comments": "International Conference on Robotics and Automation (ICRA), 2017.\n  Video summary: http://youtu.be/rbZ8ck_1nZk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weakly-supervised approach to segmenting proposed drivable paths\nin images with the goal of autonomous driving in complex urban environments.\nUsing recorded routes from a data collection vehicle, our proposed method\ngenerates vast quantities of labelled images containing proposed paths and\nobstacles without requiring manual annotation, which we then use to train a\ndeep semantic segmentation network. With the trained network we can segment\nproposed paths and obstacles at run-time using a vehicle equipped with only a\nmonocular camera without relying on explicit modelling of road or lane\nmarkings. We evaluate our method on the large-scale KITTI and Oxford RobotCar\ndatasets and demonstrate reliable path proposal and obstacle segmentation in a\nwide variety of environments under a range of lighting, weather and traffic\nconditions. We illustrate how the method can generalise to multiple path\nproposals at intersections and outline plans to incorporate the system into a\nframework for autonomous urban driving.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 00:44:49 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 22:05:10 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 16:54:44 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Barnes", "Dan", ""], ["Maddern", "Will", ""], ["Posner", "Ingmar", ""]]}, {"id": "1610.01247", "submitter": "Nikhil Krishnaswamy", "authors": "Tuan Do, Nikhil Krishnaswamy, James Pustejovsky", "title": "ECAT: Event Capture Annotation Tool", "comments": "4 pages, 4 figures, ISA workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Event Capture Annotation Tool (ECAT), a\nuser-friendly, open-source interface tool for annotating events and their\nparticipants in video, capable of extracting the 3D positions and orientations\nof objects in video captured by Microsoft's Kinect(R) hardware. The modeling\nlanguage VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object,\nprogram, and attribute representations, although ECAT uses its own spec for\nexplicit labeling of motion instances. The demonstration will show the tool's\nworkflow and the options available for capturing event-participant relations\nand browsing visual data. Mapping ECAT's output to VoxML will also be\naddressed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 01:24:42 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Do", "Tuan", ""], ["Krishnaswamy", "Nikhil", ""], ["Pustejovsky", "James", ""]]}, {"id": "1610.01326", "submitter": "Nicolo' Genesio", "authors": "Nicol\\`o Genesio, Tariq Abuhashim, Fabio Solari, Manuela Chessa and\n  Lorenzo Natale", "title": "Mobility Map Computations for Autonomous Navigation using an RGBD Sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the numbers of life-size humanoids as well as their mobile\ncapabilities have steadily grown. Stable walking motion and control for\nhumanoid robots are active fields of research. In this scenario an open\nquestion is how to model and analyse the scene so that a motion planning\nalgorithm can generate an appropriate walking pattern. This paper presents the\ncurrent work towards scene modelling and understanding, using an RGBD sensor.\nThe main objective is to provide the humanoid robot iCub with capabilities to\nnavigate safely and interact with various parts of the environment. In this\nsense we address the problem of traversability analysis of the scene, focusing\non classification of point clouds as a function of mobility, and hence walking\nsafety.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 09:22:32 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Genesio", "Nicol\u00f2", ""], ["Abuhashim", "Tariq", ""], ["Solari", "Fabio", ""], ["Chessa", "Manuela", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1610.01374", "submitter": "Samik Banerjee", "authors": "Samik Banerjee, Sukhendu Das", "title": "Domain Adaptation with Soft-margin multiple feature-kernel learning\n  beats Deep Learning for surveillance face recognition", "comments": "This is an extended version of the paper accepted in CVPR Biometric\n  Workshop, 2016. arXiv admin note: text overlap with arXiv:1610.00660", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) is the most preferred mode for biometric-based\nsurveillance, due to its passive nature of detecting subjects, amongst all\ndifferent types of biometric traits. FR under surveillance scenario does not\ngive satisfactory performance due to low contrast, noise and poor illumination\nconditions on probes, as compared to the training samples. A state-of-the-art\ntechnology, Deep Learning, even fails to perform well in these scenarios. We\npropose a novel soft-margin based learning method for multiple feature-kernel\ncombinations, followed by feature transformed using Domain Adaptation, which\noutperforms many recent state-of-the-art techniques, when tested using three\nreal-world surveillance face datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 11:48:56 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 13:14:49 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Banerjee", "Samik", ""], ["Das", "Sukhendu", ""]]}, {"id": "1610.01376", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana, Rita Cucchiara", "title": "Recognizing and Presenting the Storytelling Video Structure with Deep\n  Multimodal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for temporal and semantic segmentation\nof edited videos into meaningful segments, from the point of view of the\nstorytelling structure. The objective is to decompose a long video into more\nmanageable sequences, which can in turn be used to retrieve the most\nsignificant parts of it given a textual query and to provide an effective\nsummarization. Previous video decomposition methods mainly employed perceptual\ncues, tackling the problem either as a story change detection, or as a\nsimilarity grouping task, and the lack of semantics limited their ability to\nidentify story boundaries. Our proposal connects together perceptual, audio and\nsemantic cues in a specialized deep network architecture designed with a\ncombination of CNNs which generate an appropriate embedding, and clusters shots\ninto connected sequences of semantic scenes, i.e. stories. A retrieval\npresentation strategy is also proposed, by selecting the semantically and\naesthetically \"most valuable\" thumbnails to present, considering the query in\norder to improve the storytelling presentation. Finally, the subjective nature\nof the task is considered, by conducting experiments with different annotators\nand by proposing an algorithm to maximize the agreement between automatic\nresults and human annotators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 11:55:33 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 14:09:38 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1610.01390", "submitter": "Mathieu Hatt", "authors": "Marie-Charlotte Desseroit (CHU Poitiers - D\\'epartement de m\\'edecine\n  nucl\\'eaire), Florent Tixier (CHU Poitiers - D\\'epartement de m\\'edecine\n  nucl\\'eaire), Wolfgang Weber, Barry A Siegel, Catherine Cheze Le Rest (CHU\n  Poitiers - D\\'epartement de m\\'edecine nucl\\'eaire), Dimitris Visvikis\n  (LaTIM), Mathieu Hatt (LaTIM)", "title": "Reliability of PET/CT shape and heterogeneity features in functional and\n  morphological components of Non-Small Cell Lung Cancer tumors: a\n  repeatability analysis in a prospective multi-center cohort", "comments": "Journal of Nuclear Medicine, Society of Nuclear Medicine, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The main purpose of this study was to assess the reliability of\nshape and heterogeneity features in both Positron Emission Tomography (PET) and\nlow-dose Computed Tomography (CT) components of PET/CT. A secondary objective\nwas to investigate the impact of image quantization.Material and methods: A\nHealth Insurance Portability and Accountability Act -compliant secondary\nanalysis of deidentified prospectively acquired PET/CT test-retest datasets of\n74 patients from multi-center Merck and ACRIN trials was performed.\nMetabolically active volumes were automatically delineated on PET with Fuzzy\nLocally Adaptive Bayesian algorithm. 3DSlicerTM was used to semi-automatically\ndelineate the anatomical volumes on low-dose CT components. Two quantization\nmethods were considered: a quantization into a set number of bins\n(quantizationB) and an alternative quantization with bins of fixed width\n(quantizationW). Four shape descriptors, ten first-order metrics and 26\ntextural features were computed. Bland-Altman analysis was used to quantify\nrepeatability. Features were subsequently categorized as very reliable,\nreliable, moderately reliable and poorly reliable with respect to the\ncorresponding volume variability. Results: Repeatability was highly variable\namongst features. Numerous metrics were identified as poorly or moderately\nreliable. Others were (very) reliable in both modalities, and in all categories\n(shape, 1st-, 2nd- and 3rd-order metrics). Image quantization played a major\nrole in the features repeatability. Features were more reliable in PET with\nquantizationB, whereas quantizationW showed better results in CT.Conclusion:\nThe test-retest repeatability of shape and heterogeneity features in PET and\nlow-dose CT varied greatly amongst metrics. The level of repeatability also\ndepended strongly on the quantization step, with different optimal choices for\neach modality. The repeatability of PET and low-dose CT features should be\ncarefully taken into account when selecting metrics to build multiparametric\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 12:51:09 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Desseroit", "Marie-Charlotte", "", "CHU Poitiers - D\u00e9partement de m\u00e9decine\n  nucl\u00e9aire"], ["Tixier", "Florent", "", "CHU Poitiers - D\u00e9partement de m\u00e9decine\n  nucl\u00e9aire"], ["Weber", "Wolfgang", "", "CHU\n  Poitiers - D\u00e9partement de m\u00e9decine nucl\u00e9aire"], ["Siegel", "Barry A", "", "CHU\n  Poitiers - D\u00e9partement de m\u00e9decine nucl\u00e9aire"], ["Rest", "Catherine Cheze Le", "", "CHU\n  Poitiers - D\u00e9partement de m\u00e9decine nucl\u00e9aire"], ["Visvikis", "Dimitris", "", "LaTIM"], ["Hatt", "Mathieu", "", "LaTIM"]]}, {"id": "1610.01394", "submitter": "Shaofei Wang", "authors": "Shaofei Wang and Charless C. Fowlkes", "title": "Learning Optimal Parameters for Multi-target Tracking with Contextual\n  Interactions", "comments": "arXiv admin note: text overlap with arXiv:1412.2066", "journal-ref": null, "doi": "10.1007/s11263-016-0960-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an end-to-end framework for learning parameters of min-cost flow\nmulti-target tracking problem with quadratic trajectory interactions including\nsuppression of overlapping tracks and contextual cues about cooccurrence of\ndifferent objects. Our approach utilizes structured prediction with a\ntracking-specific loss function to learn the complete set of model parameters.\nIn this learning framework, we evaluate two different approaches to finding an\noptimal set of tracks under a quadratic model objective, one based on an LP\nrelaxation and the other based on novel greedy variants of dynamic programming\nthat handle pairwise interactions. We find the greedy algorithms achieve almost\nequivalent accuracy to the LP relaxation while being up to 10x faster than a\ncommercial LP solver. We evaluate trained models on three challenging\nbenchmarks. Surprisingly, we find that with proper parameter learning, our\nsimple data association model without explicit appearance/motion reasoning is\nable to achieve comparable or better accuracy than many state-of-the-art\nmethods that use far more complex motion features or appearance affinity metric\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 13:04:48 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Wang", "Shaofei", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1610.01400", "submitter": "Julien Rabin", "authors": "Nicolas Papadakis and Julien Rabin", "title": "Convex Histogram-Based Joint Image Segmentation with Regularized Optimal\n  Transport Cost", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate in this work a versatile convex framework for multiple image\nsegmentation, relying on the regularized optimal mass transport theory. In this\nsetting, several transport cost functions are considered and used to match\nstatistical distributions of features. In practice, global multidimensional\nhistograms are estimated from the segmented image regions, and are compared to\nreferring models that are either fixed histograms given a priori, or directly\ninferred in the non-supervised case. The different convex problems studied are\nsolved efficiently using primal-dual algorithms. The proposed approach is\ngeneric and enables multi-phase segmentation as well as co-segmentation of\nmultiple images.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 13:13:21 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Papadakis", "Nicolas", ""], ["Rabin", "Julien", ""]]}, {"id": "1610.01444", "submitter": "Davide Alinovi", "authors": "Davide Alinovi, Gianluigi Ferrari, Francesco Pisani, Riccardo Raheli", "title": "Markov Chain Modeling and Simulation of Breathing Patterns", "comments": "submitted for publication; 19 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of large video databases obtained from real patients with\nrespiratory disorders makes the design and optimization of video-based\nmonitoring systems quite critical. The purpose of this study is the development\nof suitable models and simulators of breathing behaviors and disorders, such as\nrespiratory pauses and apneas, in order to allow efficient design and test of\nvideo-based monitoring systems. More precisely, a novel Continuous-Time Markov\nChain (CTMC) statistical model of breathing patterns is presented. The\nRespiratory Rate (RR) pattern, estimated by measured vital signs of\nhospital-monitored patients, is approximated as a CTMC, whose states and\nparameters are selected through an appropriate statistical analysis. Then, two\nsimulators, software- and hardware-based, are proposed. After validation of the\nCTMC model, the proposed simulators are tested with previously developed\nvideo-based algorithms for the estimation of the RR and the detection of apnea\nevents. Examples of application to assess the performance of systems for\nvideo-based RR estimation and apnea detection are presented. The results, in\nterms of Kullback-Leibler divergence, show that realistic breathing patterns,\nincluding specific respiratory disorders, can be accurately described by the\nproposed model; moreover, the simulators are able to reproduce practical\nbreathing patterns for video analysis. The presented CTMC statistical model can\nbe strategic to describe realistic breathing patterns and devise simulators\nuseful to develop and test novel and effective video processing-based\nmonitoring systems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:32:21 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Alinovi", "Davide", ""], ["Ferrari", "Gianluigi", ""], ["Pisani", "Francesco", ""], ["Raheli", "Riccardo", ""]]}, {"id": "1610.01465", "submitter": "Kushal Kafle", "authors": "Kushal Kafle, Christopher Kanan", "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2017.06.005", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a recent problem in computer vision and\nnatural language processing that has garnered a large amount of interest from\nthe deep learning, computer vision, and natural language processing\ncommunities. In VQA, an algorithm needs to answer text-based questions about\nimages. Since the release of the first VQA dataset in 2014, additional datasets\nhave been released and many algorithms have been proposed. In this review, we\ncritically examine the current state of VQA in terms of problem formulation,\nexisting datasets, evaluation metrics, and algorithms. In particular, we\ndiscuss the limitations of current datasets with regard to their ability to\nproperly train and assess VQA algorithms. We then exhaustively review existing\nalgorithms for VQA. Finally, we discuss possible future directions for VQA and\nimage understanding research.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:58:36 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 01:39:40 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 05:39:21 GMT"}, {"version": "v4", "created": "Thu, 15 Jun 2017 01:52:59 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "1610.01502", "submitter": "Nina Miolane", "authors": "Nina Miolane (ASCLEPIOS), Susan Holmes, Xavier Pennec (ASCLEPIOS)", "title": "Template shape estimation: correcting an asymptotic bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use tools from geometric statistics to analyze the usual estimation\nprocedure of a template shape. This applies to shapes from landmarks, curves,\nsurfaces, images etc. We demonstrate the asymptotic bias of the template shape\nestimation using the stratified geometry of the shape space. We give a Taylor\nexpansion of the bias with respect to a parameter $\\sigma$ describing the\nmeasurement error on the data. We propose two bootstrap procedures that\nquantify the bias and correct it, if needed. They are applicable for any type\nof shape data. We give a rule of thumb to provide intuition on whether the bias\nhas to be corrected. This exhibits the parameters that control the bias'\nmagnitude. We illustrate our results on simulated and real shape data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 12:45:26 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 12:54:28 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Miolane", "Nina", "", "ASCLEPIOS"], ["Holmes", "Susan", "", "ASCLEPIOS"], ["Pennec", "Xavier", "", "ASCLEPIOS"]]}, {"id": "1610.01563", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Thomas S. A. Wallis and Matthias Bethge", "title": "DeepGaze II: Reading fixations from deep features trained on object\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present DeepGaze II, a model that predicts where people look in\nimages. The model uses the features from the VGG-19 deep neural network trained\nto identify objects in images. Contrary to other saliency models that use deep\nfeatures, here we use the VGG features for saliency prediction with no\nadditional fine-tuning (rather, a few readout layers are trained on top of the\nVGG features to predict saliency). The model is therefore a strong test of\ntransfer learning. After conservative cross-validation, DeepGaze II explains\nabout 87% of the explainable information gain in the patterns of fixations and\nachieves top performance in area under the curve metrics on the MIT300 hold-out\nbenchmark. These results corroborate the finding from DeepGaze I (which\nexplained 56% of the explainable information gain), that deep features trained\non object recognition provide a versatile feature space for performing related\nvisual tasks. We explore the factors that contribute to this success and\npresent several informative image examples. A web service is available to\ncompute model predictions at http://deepgaze.bethgelab.org.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 18:47:28 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1610.01578", "submitter": "Krzysztof Cpa{\\l}ka", "authors": "Krzysztof Cpalka, Marcin Zalasinski, Leszek Rutkowski", "title": "A new algorithm for identity verification based on the analysis of a\n  handwritten dynamic signature", "comments": "34 pages, 7 figures", "journal-ref": "Applied Soft Computing, vol. 43, pp. 47-56, 2016", "doi": "10.1016/j.asoc.2016.02.017", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity verification based on authenticity assessment of a handwritten\nsignature is an important issue in biometrics. There are many effective methods\nfor signature verification taking into account dynamics of a signing process.\nMethods based on partitioning take a very important place among them. In this\npaper we propose a new approach to signature partitioning. Its most important\nfeature is the possibility of selecting and processing of hybrid partitions in\norder to increase a precision of the test signature analysis. Partitions are\nformed by a combination of vertical and horizontal sections of the signature.\nVertical sections correspond to the initial, middle, and final time moments of\nthe signing process. In turn, horizontal sections correspond to the signature\nareas associated with high and low pen velocity and high and low pen pressure\non the surface of a graphics tablet. Our previous research on vertical and\nhorizontal sections of the dynamic signature (created independently) led us to\ndevelop the algorithm presented in this paper. Selection of sections, among\nothers, allows us to define the stability of the signing process in the\npartitions, promoting signature areas of greater stability (and vice versa). In\nthe test of the proposed method two databases were used: public MCYT-100 and\npaid BioSecure.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 19:32:55 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Cpalka", "Krzysztof", ""], ["Zalasinski", "Marcin", ""], ["Rutkowski", "Leszek", ""]]}, {"id": "1610.01685", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto, James Davidson and Abhinav Gupta", "title": "Supervision via Competition: Robot Adversaries for Learning Tasks", "comments": "Submission to ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent paradigm shift in robotics to data-driven learning\nfor planning and control. Due to large number of experiences required for\ntraining, most of these approaches use a self-supervised paradigm: using\nsensors to measure success/failure. However, in most cases, these sensors\nprovide weak supervision at best. In this work, we propose an adversarial\nlearning framework that pits an adversary against the robot learning the task.\nIn an effort to defeat the adversary, the original robot learns to perform the\ntask with more robustness leading to overall improved performance. We show that\nthis adversarial framework forces the the robot to learn a better grasping\nmodel in order to overcome the adversary. By grasping 82% of presented novel\nobjects compared to 68% without an adversary, we demonstrate the utility of\ncreating adversaries. We also demonstrate via experiments that having robots in\nadversarial setting might be a better learning strategy as compared to having\ncollaborative multiple robots.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:28:12 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Pinto", "Lerrel", ""], ["Davidson", "James", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1610.01706", "submitter": "Chunhua Shen", "authors": "Yuanzhouhan Cao, Chunhua Shen, Heng Tao Shen", "title": "Exploiting Depth from Single Monocular Images for Object Detection and\n  Semantic Segmentation", "comments": "14 pages. Accepted to IEEE T. Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2621673", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting RGB data with measured depth has been shown to improve the\nperformance of a range of tasks in computer vision including object detection\nand semantic segmentation. Although depth sensors such as the Microsoft Kinect\nhave facilitated easy acquisition of such depth information, the vast majority\nof images used in vision tasks do not contain depth information. In this paper,\nwe show that augmenting RGB images with estimated depth can also improve the\naccuracy of both object detection and semantic segmentation. Specifically, we\nfirst exploit the recent success of depth estimation from monocular images and\nlearn a deep depth estimation model. Then we learn deep depth features from the\nestimated depth and combine with RGB features for object detection and semantic\nsegmentation. Additionally, we propose an RGB-D semantic segmentation method\nwhich applies a multi-task training scheme: semantic label prediction and depth\nvalue regression. We test our methods on several datasets and demonstrate that\nincorporating information from estimated depth improves the performance of\nobject detection and semantic segmentation remarkably.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 01:30:46 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Cao", "Yuanzhouhan", ""], ["Shen", "Chunhua", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1610.01708", "submitter": "Nian Liu", "authors": "Nian Liu and Junwei Han", "title": "A Deep Spatial Contextual Long-term Recurrent Convolutional Network for\n  Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional saliency models usually adopt hand-crafted image features and\nhuman-designed mechanisms to calculate local or global contrast. In this paper,\nwe propose a novel computational saliency model, i.e., deep spatial contextual\nlong-term recurrent convolutional network (DSCLRCN) to predict where people\nlooks in natural scenes. DSCLRCN first automatically learns saliency related\nlocal features on each image location in parallel. Then, in contrast with most\nother deep network based saliency models which infer saliency in local\ncontexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human\nvisual system to incorporate global contexts to assess the saliency of each\nimage location by leveraging the deep spatial long short-term memory (DSLSTM)\nmodel. Moreover, we also integrate scene context modulation in DSLSTM for\nsaliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM)\nmodel. The whole network can be trained end-to-end and works efficiently when\ntesting. Experimental results on two benchmark datasets show that DSCLRCN can\nachieve state-of-the-art performance on saliency detection. Furthermore, the\nproposed DSCLSTM model can significantly boost the saliency detection\nperformance by incorporating both global spatial interconnections and scene\ncontext modulation, which may uncover novel inspirations for studies on them in\ncomputational saliency models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 01:38:13 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Liu", "Nian", ""], ["Han", "Junwei", ""]]}, {"id": "1610.01732", "submitter": "Lei Tai", "authors": "Lei Tai, Haoyang Ye, Qiong Ye, Ming Liu", "title": "PCA-aided Fully Convolutional Networks for Semantic Segmentation of\n  Multi-channel fMRI", "comments": "ICAR 2017 - 18th International Conference on Advanced Robotics, Best\n  Student Paper Award, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of functional magnetic resonance imaging (fMRI) makes\ngreat sense for pathology diagnosis and decision system of medical robots. The\nmulti-channel fMRI provides more information of the pathological features. But\nthe increased amount of data causes complexity in feature detections. This\npaper proposes a principal component analysis (PCA)-aided fully convolutional\nnetwork to particularly deal with multi-channel fMRI. We transfer the learned\nweights of contemporary classification networks to the segmentation task by\nfine-tuning. The results of the convolutional network are compared with various\nmethods e.g. k-NN. A new labeling strategy is proposed to solve the semantic\nsegmentation problem with unclear boundaries. Even with a small-sized training\ndataset, the test results demonstrate that our model outperforms other\npathological feature detection methods. Besides, its forward inference only\ntakes 90 milliseconds for a single set of fMRI data. To our knowledge, this is\nthe first time to realize pixel-wise labeling of multi-channel magnetic\nresonance image using FCN.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 05:08:15 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 15:44:09 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 12:50:09 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 15:52:08 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Tai", "Lei", ""], ["Ye", "Haoyang", ""], ["Ye", "Qiong", ""], ["Liu", "Ming", ""]]}, {"id": "1610.01795", "submitter": "Mohamad Ivan Fanany", "authors": "Ines Heidieni Ikasari, Vina Ayumi, Mohamad Ivan Fanany, Sidik Mulyono", "title": "Multiple Regularizations Deep Learning for Paddy Growth Stages\n  Classification from LANDSAT-8", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses remote sensing technology that can provide information about\nthe condition of the earth's surface area, fast, and spatially. The study area\nwas in Karawang District, lying in the Northern part of West Java-Indonesia. We\naddress a paddy growth stages classification using LANDSAT 8 image data\nobtained from multi-sensor remote sensing image taken in October 2015 to August\n2016. This study pursues a fast and accurate classification of paddy growth\nstages by employing multiple regularizations learning on some deep learning\nmethods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional\nNeural Networks). The used regularizations are Fast Dropout, Dropout, and Batch\nNormalization. To evaluate the effectiveness, we also compared our method with\nother machine learning methods such as (Logistic Regression, SVM, Random\nForest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data\nsamples that correspond to paddy growth stages data obtained from i-Sky (eye in\nthe sky) Innovation system. The growth stages are determined based on paddy\ncrop phenology profile from time series of LANDSAT-8 images. The classification\nresults show that MLP using multiple regularization Dropout and Batch\nNormalization achieves the highest accuracy for this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 09:46:08 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Ikasari", "Ines Heidieni", ""], ["Ayumi", "Vina", ""], ["Fanany", "Mohamad Ivan", ""], ["Mulyono", "Sidik", ""]]}, {"id": "1610.01801", "submitter": "Svetlana Kordumova", "authors": "Svetlana Kordumova, Jan C. van Gemert, Cees G. M. Snoek, Arnold W. M.\n  Smeulders", "title": "Searching Scenes by Abstracting Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to represent a scene as an abstraction of 'things'.\nWe start from 'things' as generated by modern object proposals, and we\ninvestigate their immediately observable properties: position, size, aspect\nratio and color, and those only. Where the recent successes and excitement of\nthe field lie in object identification, we represent the scene composition\nindependent of object identities. We make three contributions in this work.\nFirst, we study simple observable properties of 'things', and call it things\nsyntax. Second, we propose translating the things syntax in linguistic abstract\nstatements and study their descriptive effect to retrieve scenes. Thirdly, we\npropose querying of scenes with abstract block illustrations and study their\neffectiveness to discriminate among different types of scenes. The benefit of\nabstract statements and block illustrations is that we generate them directly\nfrom the images, without any learning beforehand as in the standard attribute\nlearning. Surprisingly, we show that even though we use the simplest of\nfeatures from 'things' layout and no learning at all, we can still retrieve\nscenes reasonably well.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 10:03:45 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Kordumova", "Svetlana", ""], ["van Gemert", "Jan C.", ""], ["Snoek", "Cees G. M.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1610.01852", "submitter": "Ulugbek Kamilov", "authors": "Hsiou-Yuan Liu and Ulugbek S. Kamilov and Dehong Liu and Hassan\n  Mansour and Petros T. Boufounos", "title": "Compressive Imaging with Iterative Forward Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new compressive imaging method for reconstructing 2D or 3D\nobjects from their scattered wave-field measurements. Our method relies on a\nnovel, nonlinear measurement model that can account for the multiple scattering\nphenomenon, which makes the method preferable in applications where linear\nmeasurement models are inaccurate. We construct the measurement model by\nexpanding the scattered wave-field with an accelerated-gradient method, which\nis guaranteed to converge and is suitable for large-scale problems. We provide\nexplicit formulas for computing the gradient of our measurement model with\nrespect to the unknown image, which enables image formation with a sparsity-\ndriven numerical optimization algorithm. We validate the method both\nanalytically and with numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:44:24 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Liu", "Hsiou-Yuan", ""], ["Kamilov", "Ulugbek S.", ""], ["Liu", "Dehong", ""], ["Mansour", "Hassan", ""], ["Boufounos", "Petros T.", ""]]}, {"id": "1610.01854", "submitter": "Yu Wang", "authors": "Yu Wang, Haofu Liao, Yang Feng, Xiangyang Xu, Jiebo Luo", "title": "Do They All Look the Same? Deciphering Chinese, Japanese and Koreans by\n  Fine-Grained Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study to what extend Chinese, Japanese and Korean faces can be classified\nand which facial attributes offer the most important cues. First, we propose a\nnovel way of obtaining large numbers of facial images with nationality labels.\nThen we train state-of-the-art neural networks with these labeled images. We\nare able to achieve an accuracy of 75.03% in the classification task, with\nchances being 33.33% and human accuracy 38.89% . Further, we train multiple\nfacial attribute classifiers to identify the most distinctive features for each\ngroup. We find that Chinese, Japanese and Koreans do exhibit substantial\ndifferences in certain attributes, such as bangs, smiling, and bushy eyebrows.\nAlong the way, we uncover several gender-related cross-country patterns as\nwell. Our work, which complements existing APIs such as Microsoft Cognitive\nServices and Face++, could find potential applications in tourism, e-commerce,\nsocial media marketing, criminal justice and even counter-terrorism.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:14:34 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 01:26:37 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Wang", "Yu", ""], ["Liao", "Haofu", ""], ["Feng", "Yang", ""], ["Xu", "Xiangyang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1610.01860", "submitter": "Bernd Sturmfels", "authors": "Joe Kileel, Zuzana Kukelova, Tomas Pajdla and Bernd Sturmfels", "title": "Distortion Varieties", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distortion varieties of a given projective variety are parametrized by\nduplicating coordinates and multiplying them with monomials. We study their\ndegrees and defining equations. Exact formulas are obtained for the case of\none-parameter distortions. These are based on Chow polytopes and Gr\\\"obner\nbases. Multi-parameter distortions are studied using tropical geometry. The\nmotivation for distortion varieties comes from multi-view geometry in computer\nvision. Our theory furnishes a new framework for formulating and solving\nminimal problems for camera models with image distortion.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:22:30 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Kileel", "Joe", ""], ["Kukelova", "Zuzana", ""], ["Pajdla", "Tomas", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1610.01906", "submitter": "Ziwei Xu", "authors": "Ziwei Xu, Haitian Zheng, Minjian Pang, Yangchun Zhu, Xiongfei Su,\n  Guyue Zhou, Lu Fang", "title": "Utilizing High-level Visual Feature for Indoor Shopping Mall Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards robust and convenient indoor shopping mall navigation, we propose a\nnovel learning-based scheme to utilize the high-level visual information from\nthe storefront images captured by personal devices of users. Specifically, we\ndecompose the visual navigation problem into localization and map generation\nrespectively. Given a storefront input image, a novel feature fusion scheme\n(denoted as FusionNet) is proposed by fusing the distinguishing DNN-based\nappearance feature and text feature for robust recognition of store brands,\nwhich serves for accurate localization. Regarding the map generation, we\nconvert the user-captured indicator map of the shopping mall into a topological\nmap by parsing the stores and their connectivity. Experimental results\nconducted on the real shopping malls demonstrate that the proposed system\nachieves robust localization and precise map generation, enabling accurate\nnavigation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 15:14:47 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 13:30:03 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 14:55:07 GMT"}, {"version": "v4", "created": "Sat, 18 Feb 2017 11:50:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Xu", "Ziwei", ""], ["Zheng", "Haitian", ""], ["Pang", "Minjian", ""], ["Zhu", "Yangchun", ""], ["Su", "Xiongfei", ""], ["Zhou", "Guyue", ""], ["Fang", "Lu", ""]]}, {"id": "1610.01925", "submitter": "Mohamad Ivan Fanany", "authors": "L. M. Rasdi Rere, Mohamad Ivan Fanany, and Aniati Murni Arymurthy", "title": "Metaheuristic Algorithms for Convolution Neural Network", "comments": "Article ID 1537325, 13 pages. Received 29 January 2016; Revised 15\n  April 2016; Accepted 10 May 2016. Academic Editor: Martin Hagan. in Hindawi\n  Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016)", "journal-ref": "Computational Intelligence and Neuroscience Volume 2016 (2016),\n  Article ID 1537325, 13 pages", "doi": "10.1155/2016/1537325", "report-no": "1537325", "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical modern optimization technique is usually either heuristic or\nmetaheuristic. This technique has managed to solve some optimization problems\nin the research area of science, engineering, and industry. However,\nimplementation strategy of metaheuristic for accuracy improvement on\nconvolution neural networks (CNN), a famous deep learning method, is still\nrarely investigated. Deep learning relates to a type of machine learning\ntechnique, where its aim is to move closer to the goal of artificial\nintelligence of creating a machine that could successfully perform any\nintellectual tasks that can be carried out by a human. In this paper, we\npropose the implementation strategy of three popular metaheuristic approaches,\nthat is, simulated annealing, differential evolution, and harmony search, to\noptimize CNN. The performances of these metaheuristic methods in optimizing CNN\non classifying MNIST and CIFAR dataset were evaluated and compared.\nFurthermore, the proposed methods are also compared with the original CNN.\nAlthough the proposed methods show an increase in the computation time, their\naccuracy has also been improved (up to 7.14 percent).\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 16:11:06 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Rere", "L. M. Rasdi", ""], ["Fanany", "Mohamad Ivan", ""], ["Arymurthy", "Aniati Murni", ""]]}, {"id": "1610.01944", "submitter": "Georg Poier", "authors": "Georg Poier, Markus Seidl, Matthias Zeppelzauer, Christian Reinbacher,\n  Martin Schaich, Giovanna Bellandi, Alberto Marretta, and Horst Bischof", "title": "PetroSurf3D - A Dataset for high-resolution 3D Surface Segmentation", "comments": "CBMI Submission; Dataset and more information can be found at\n  http://lrs.icg.tugraz.at/research/petroglyphsegmentation/", "journal-ref": null, "doi": "10.1145/3095713.3095719", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of powerful 3D scanning hardware and reconstruction\nalgorithms has strongly promoted the generation of 3D surface reconstructions\nin different domains. An area of special interest for such 3D reconstructions\nis the cultural heritage domain, where surface reconstructions are generated to\ndigitally preserve historical artifacts. While reconstruction quality nowadays\nis sufficient in many cases, the robust analysis (e.g. segmentation, matching,\nand classification) of reconstructed 3D data is still an open topic. In this\npaper, we target the automatic and interactive segmentation of high-resolution\n3D surface reconstructions from the archaeological domain. To foster research\nin this field, we introduce a fully annotated and publicly available\nlarge-scale 3D surface dataset including high-resolution meshes, depth maps and\npoint clouds as a novel benchmark dataset to the community. We provide baseline\nresults for our existing random forest-based approach and for the first time\ninvestigate segmentation with convolutional neural networks (CNNs) on the data.\nResults show that both approaches have complementary strengths and weaknesses\nand that the provided dataset represents a challenge for future research.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 16:55:07 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 14:15:26 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 13:40:08 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Poier", "Georg", ""], ["Seidl", "Markus", ""], ["Zeppelzauer", "Matthias", ""], ["Reinbacher", "Christian", ""], ["Schaich", "Martin", ""], ["Bellandi", "Giovanna", ""], ["Marretta", "Alberto", ""], ["Bischof", "Horst", ""]]}, {"id": "1610.01983", "submitter": "Matthew Johnson-Roberson", "authors": "Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur\n  Sridhar, Karl Rosaen, and Ram Vasudevan", "title": "Driving in the Matrix: Can Virtual Worlds Replace Human-Generated\n  Annotations for Real World Tasks?", "comments": "Proceedings of International Conference on Robotics and Automation\n  (ICRA) 2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has rapidly transformed the state of the art algorithms used to\naddress a variety of problems in computer vision and robotics. These\nbreakthroughs have relied upon massive amounts of human annotated training\ndata. This time consuming process has begun impeding the progress of these deep\nlearning efforts. This paper describes a method to incorporate photo-realistic\ncomputer images from a simulation engine to rapidly generate annotated data\nthat can be used for the training of machine learning algorithms. We\ndemonstrate that a state of the art architecture, which is trained only using\nthese synthetic annotations, performs better than the identical architecture\ntrained on human annotated real-world data, when tested on the KITTI data set\nfor vehicle detection. By training machine learning algorithms on a rich\nvirtual world, real objects in real scenes can be learned and classified using\nsynthetic data. This approach offers the possibility of accelerating deep\nlearning's application to sensor-based classification problems like those that\nappear in self-driving cars. The source code and data to train and validate the\nnetworks described in this paper are made available for researchers.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 18:26:43 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 13:20:49 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Johnson-Roberson", "Matthew", ""], ["Barto", "Charles", ""], ["Mehta", "Rounak", ""], ["Sridhar", "Sharath Nittur", ""], ["Rosaen", "Karl", ""], ["Vasudevan", "Ram", ""]]}, {"id": "1610.02055", "submitter": "Bolei Zhou", "authors": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, Aude\n  Oliva", "title": "Places: An Image Database for Deep Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of multi-million-item dataset initiatives has enabled data-hungry\nmachine learning algorithms to reach near-human semantic classification at\ntasks such as object and scene recognition. Here we describe the Places\nDatabase, a repository of 10 million scene photographs, labeled with scene\nsemantic categories and attributes, comprising a quasi-exhaustive list of the\ntypes of environments encountered in the world. Using state of the art\nConvolutional Neural Networks, we provide impressive baseline performances at\nscene classification. With its high-coverage and high-diversity of exemplars,\nthe Places Database offers an ecosystem to guide future progress on currently\nintractable visual recognition problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 20:14:13 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Zhou", "Bolei", ""], ["Khosla", "Aditya", ""], ["Lapedriza", "Agata", ""], ["Torralba", "Antonio", ""], ["Oliva", "Aude", ""]]}, {"id": "1610.02136", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Kevin Gimpel", "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples\n  in Neural Networks", "comments": "Published as a conference paper at ICLR 2017. 1 Figure in 1 Appendix.\n  Minor changes from the previous version", "journal-ref": "International Conference on Learning Representations 2017", "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two related problems of detecting if an example is\nmisclassified or out-of-distribution. We present a simple baseline that\nutilizes probabilities from softmax distributions. Correctly classified\nexamples tend to have greater maximum softmax probabilities than erroneously\nclassified and out-of-distribution examples, allowing for their detection. We\nassess performance by defining several tasks in computer vision, natural\nlanguage processing, and automatic speech recognition, showing the\neffectiveness of this baseline across all. We then show the baseline can\nsometimes be surpassed, demonstrating the room for future research on these\nunderexplored detection tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 04:06:01 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:11:25 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 07:32:57 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1610.02177", "submitter": "Patrick Christ", "authors": "Patrick Ferdinand Christ, Mohamed Ezzeldin A. Elshaer, Florian\n  Ettlinger, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler,\n  Marco Armbruster, Felix Hofmann, Melvin D'Anastasi, Wieland H. Sommer,\n  Seyed-Ahmad Ahmadi and Bjoern H. Menze", "title": "Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully\n  Convolutional Neural Networks and 3D Conditional Random Fields", "comments": "Accepted at MICCAI 2016. Source code available on\n  https://github.com/IBBM/Cascaded-FCN", "journal-ref": null, "doi": "10.1007/978-3-319-46723-8_48", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of the liver and its lesion is an important step\ntowards deriving quantitative biomarkers for accurate clinical diagnosis and\ncomputer-aided decision support systems. This paper presents a method to\nautomatically segment liver and lesions in CT abdomen images using cascaded\nfully convolutional neural networks (CFCNs) and dense 3D conditional random\nfields (CRFs). We train and cascade two FCNs for a combined segmentation of the\nliver and its lesions. In the first step, we train a FCN to segment the liver\nas ROI input for a second FCN. The second FCN solely segments lesions from the\npredicted liver ROIs of step 1. We refine the segmentations of the CFCN using a\ndense 3D CRF that accounts for both spatial coherence and appearance. CFCN\nmodels were trained in a 2-fold cross-validation on the abdominal CT dataset\n3DIRCAD comprising 15 hepatic tumor volumes. Our results show that CFCN-based\nsemantic liver and lesion segmentation achieves Dice scores over 94% for liver\nwith computation times below 100s per volume. We experimentally demonstrate the\nrobustness of the proposed method as a decision support system with a high\naccuracy and speed for usage in daily clinical routine.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 08:23:32 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Christ", "Patrick Ferdinand", ""], ["Elshaer", "Mohamed Ezzeldin A.", ""], ["Ettlinger", "Florian", ""], ["Tatavarty", "Sunil", ""], ["Bickel", "Marc", ""], ["Bilic", "Patrick", ""], ["Rempfler", "Markus", ""], ["Armbruster", "Marco", ""], ["Hofmann", "Felix", ""], ["D'Anastasi", "Melvin", ""], ["Sommer", "Wieland H.", ""], ["Ahmadi", "Seyed-Ahmad", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1610.02237", "submitter": "Hilde Kuehne", "authors": "Hilde Kuehne, Alexander Richard, Juergen Gall", "title": "Weakly supervised learning of actions from transcripts", "comments": "33 pages, 9 figures, to appear in CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for weakly supervised learning of human actions from\nvideo transcriptions. Our system is based on the idea that, given a sequence of\ninput data and a transcript, i.e. a list of the order the actions occur in the\nvideo, it is possible to infer the actions within the video stream, and thus,\nlearn the related action models without the need for any frame-based\nannotation. Starting from the transcript information at hand, we split the\ngiven data sequences uniformly based on the number of expected actions. We then\nlearn action models for each class by maximizing the probability that the\ntraining video sequences are generated by the action models given the sequence\norder as defined by the transcripts. The learned model can be used to\ntemporally segment an unseen video with or without transcript. We evaluate our\napproach on four distinct activity datasets, namely Hollywood Extended, MPII\nCooking, Breakfast and CRIM13. We show that our system is able to align the\nscripted actions with the video data and that the learned models localize and\nclassify actions competitively in comparison to models trained with full\nsupervision, i.e. with frame level annotations, and that they outperform any\ncurrent state-of-the-art approach for aligning transcripts with video data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:00:08 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 09:25:13 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Kuehne", "Hilde", ""], ["Richard", "Alexander", ""], ["Gall", "Juergen", ""]]}, {"id": "1610.02251", "submitter": "Gustavo Carneiro", "authors": "Zhi Lu, Gustavo Carneiro, Neeraj Dhungel, Andrew P. Bradley", "title": "Automated Detection of Individual Micro-calcifications from Mammograms\n  using a Multi-stage Cascade Approach", "comments": "5 Pages, ISBI 2017 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mammography, the efficacy of computer-aided detection methods depends, in\npart, on the robust localisation of micro-calcifications ($\\mu$C). Currently,\nthe most effective methods are based on three steps: 1) detection of individual\n$\\mu$C candidates, 2) clustering of individual $\\mu$C candidates, and 3)\nclassification of $\\mu$C clusters. Where the second step is motivated both to\nreduce the number of false positive detections from the first step and on the\nevidence that malignancy depends on a relatively large number of $\\mu$C\ndetections within a certain area. In this paper, we propose a novel approach to\n$\\mu$C detection, consisting of the detection \\emph{and} classification of\nindividual $\\mu$C candidates, using shape and appearance features, using a\ncascade of boosting classifiers. The final step in our approach then clusters\nthe remaining individual $\\mu$C candidates. The main advantage of this approach\nlies in its ability to reject a significant number of false positive $\\mu$C\ncandidates compared to previously proposed methods. Specifically, on the\nINbreast dataset, we show that our approach has a true positive rate (TPR) for\nindividual $\\mu$Cs of 40\\% at one false positive per image (FPI) and a TPR of\n80\\% at 10 FPI. These results are significantly more accurate than the current\nstate of the art, which has a TPR of less than 1\\% at one FPI and a TPR of 10\\%\nat 10 FPI. Our results are competitive with the state of the art at the\nsubsequent stage of detecting clusters of $\\mu$Cs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:36:21 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Lu", "Zhi", ""], ["Carneiro", "Gustavo", ""], ["Dhungel", "Neeraj", ""], ["Bradley", "Andrew P.", ""]]}, {"id": "1610.02255", "submitter": "Samuel Albanie", "authors": "Samuel Albanie and Andrea Vedaldi", "title": "Learning Grimaces by Watching TV", "comments": "British Machine Vision Conference (BMVC) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differently from computer vision systems which require explicit supervision,\nhumans can learn facial expressions by observing people in their environment.\nIn this paper, we look at how similar capabilities could be developed in\nmachine vision. As a starting point, we consider the problem of relating facial\nexpressions to objectively measurable events occurring in videos. In\nparticular, we consider a gameshow in which contestants play to win significant\nsums of money. We extract events affecting the game and corresponding facial\nexpressions objectively and automatically from the videos, obtaining large\nquantities of labelled data for our study. We also develop, using benchmarks\nsuch as FER and SFEW 2.0, state-of-the-art deep neural networks for facial\nexpression recognition, showing that pre-training on face verification data can\nbe highly beneficial for this task. Then, we extend these models to use facial\nexpressions to predict events in videos and learn nameable expressions from\nthem. The dataset and emotion recognition models are available at\nhttp://www.robots.ox.ac.uk/~vgg/data/facevalue\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:42:47 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Albanie", "Samuel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1610.02256", "submitter": "Xin Jin", "authors": "Xin Jin, Le Wu, Xiaodong Li, Xiaokun Zhang, Jingying Chi, Siwei Peng,\n  Shiming Ge, Geng Zhao, Shuying Li", "title": "ILGNet: Inception Modules with Connected Local and Global Features for\n  Efficient Image Aesthetic Quality Classification using Domain Adaptation", "comments": "under review, IET-Computer Vision, Previous WCSP2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a challenging problem of aesthetic image\nclassification, which is to label an input image as high or low aesthetic\nquality. We take both the local and global features of images into\nconsideration. A novel deep convolutional neural network named ILGNet is\nproposed, which combines both the Inception modules and an connected layer of\nboth Local and Global features. The ILGnet is based on GoogLeNet. Thus, it is\neasy to use a pre-trained GoogLeNet for large-scale image classification\nproblem and fine tune our connected layers on an large scale database of\naesthetic related images: AVA, i.e. \\emph{domain adaptation}. The experiments\nreveal that our model achieves the state of the arts in AVA database. Both the\ntraining and testing speeds of our model are higher than those of the original\nGoogLeNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:46:45 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 02:23:09 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 23:43:04 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Jin", "Xin", ""], ["Wu", "Le", ""], ["Li", "Xiaodong", ""], ["Zhang", "Xiaokun", ""], ["Chi", "Jingying", ""], ["Peng", "Siwei", ""], ["Ge", "Shiming", ""], ["Zhao", "Geng", ""], ["Li", "Shuying", ""]]}, {"id": "1610.02306", "submitter": "Mohamad Ivan Fanany", "authors": "Vina Ayumi, L.M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni\n  Arymurthy", "title": "Optimization of Convolutional Neural Network using Microcanonical\n  Annealing Algorithm", "comments": "Accepted to be published at IEEE ICACSIS 2016. arXiv admin note: text\n  overlap with arXiv:1610.01925", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is one of the most prominent architectures\nand algorithm in Deep Learning. It shows a remarkable improvement in the\nrecognition and classification of objects. This method has also been proven to\nbe very effective in a variety of computer vision and machine learning\nproblems. As in other deep learning, however, training the CNN is interesting\nyet challenging. Recently, some metaheuristic algorithms have been used to\noptimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated\nAnnealing and Harmony Search. In this paper, another type of metaheuristic\nalgorithms with different strategy has been proposed, i.e. Microcanonical\nAnnealing to optimize Convolutional Neural Network. The performance of the\nproposed method is tested using the MNIST and CIFAR-10 datasets. Although\nexperiment results of MNIST dataset indicate the increase in computation time\n(1.02x - 1.38x), nevertheless this proposed method can considerably enhance the\nperformance of the original CNN (up to 4.60\\%). On the CIFAR10 dataset,\ncurrently, state of the art is 96.53\\% using fractional pooling, while this\nproposed method achieves 99.14\\%.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 14:39:50 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Ayumi", "Vina", ""], ["Rere", "L. M. Rasdi", ""], ["Fanany", "Mohamad Ivan", ""], ["Arymurthy", "Aniati Murni", ""]]}, {"id": "1610.02357", "submitter": "Francois Chollet", "authors": "Fran\\c{c}ois Chollet", "title": "Xception: Deep Learning with Depthwise Separable Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 17:51:51 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 17:37:25 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 18:40:27 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chollet", "Fran\u00e7ois", ""]]}, {"id": "1610.02373", "submitter": "Mohamad Ivan Fanany", "authors": "Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin", "title": "Distributed Averaging CNN-ELM for Big Data", "comments": "Submitted to IEEE Transactions on Systems, Man and Cybernetics:\n  Systems", "journal-ref": null, "doi": null, "report-no": "SMCA-16-09-1039", "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the scalability of machine learning to handle big volume of data\nis a challenging task. The scale up approach has some limitations. In this\npaper, we proposed a scale out approach for CNN-ELM based on MapReduce on\nclassifier level. Map process is the CNN-ELM training for certain partition of\ndata. It involves many CNN-ELM models that can be trained asynchronously.\nReduce process is the averaging of all CNN-ELM weights as final training\nresult. This approach can save a lot of training time than single CNN-ELM\nmodels trained alone. This approach also increased the scalability of machine\nlearning by combining scale out and scale up approaches. We verified our method\nin extended MNIST data set and not-MNIST data set experiment. However, it has\nsome drawbacks by additional iteration learning parameters that need to be\ncarefully taken and training data distribution that need to be carefully\nselected. Further researches to use more complex image data set are required.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 18:59:23 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Budiman", "Arif", ""], ["Fanany", "Mohamad Ivan", ""], ["Basaruddin", "Chan", ""]]}, {"id": "1610.02391", "submitter": "Ramprasaath R. Selvaraju", "authors": "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna\n  Vedantam, Devi Parikh, Dhruv Batra", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based\n  Localization", "comments": "This version was published in International Journal of Computer\n  Vision (IJCV) in 2019; A previous version of the paper was published at\n  International Conference on Computer Vision (ICCV'17)", "journal-ref": null, "doi": "10.1007/s11263-019-01228-7", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for producing \"visual explanations\" for decisions from\na large class of CNN-based models, making them more transparent. Our approach -\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of\nany target concept, flowing into the final convolutional layer to produce a\ncoarse localization map highlighting important regions in the image for\npredicting the concept. Grad-CAM is applicable to a wide variety of CNN\nmodel-families: (1) CNNs with fully-connected layers, (2) CNNs used for\nstructured outputs, (3) CNNs used in tasks with multimodal inputs or\nreinforcement learning, without any architectural changes or re-training. We\ncombine Grad-CAM with fine-grained visualizations to create a high-resolution\nclass-discriminative visualization and apply it to off-the-shelf image\nclassification, captioning, and visual question answering (VQA) models,\nincluding ResNet-based architectures. In the context of image classification\nmodels, our visualizations (a) lend insights into their failure modes, (b) are\nrobust to adversarial images, (c) outperform previous methods on localization,\n(d) are more faithful to the underlying model and (e) help achieve\ngeneralization by identifying dataset bias. For captioning and VQA, we show\nthat even non-attention based models can localize inputs. We devise a way to\nidentify important neurons through Grad-CAM and combine it with neuron names to\nprovide textual explanations for model decisions. Finally, we design and\nconduct human studies to measure if Grad-CAM helps users establish appropriate\ntrust in predictions from models and show that Grad-CAM helps untrained users\nsuccessfully discern a 'stronger' nodel from a 'weaker' one even when both make\nidentical predictions. Our code is available at\nhttps://github.com/ramprs/grad-cam/, along with a demo at\nhttp://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 19:54:24 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 07:19:35 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 23:48:00 GMT"}, {"version": "v4", "created": "Tue, 3 Dec 2019 02:13:03 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Selvaraju", "Ramprasaath R.", ""], ["Cogswell", "Michael", ""], ["Das", "Abhishek", ""], ["Vedantam", "Ramakrishna", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1610.02414", "submitter": "Fan Zhang", "authors": "Fan Zhang, Fabio Duarte, Ruixian Ma, Dimitrios Milioris, Hui Lin,\n  Carlo Ratti", "title": "Indoor Space Recognition using Deep Convolutional Neural Network: A Case\n  Study at MIT Campus", "comments": "22 pages; 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust and parsimonious approach using Deep\nConvolutional Neural Network (DCNN) to recognize and interpret interior space.\nDCNN has achieved incredible success in object and scene recognition. In this\nstudy we design and train a DCNN to classify a pre-zoning indoor space, and\nfrom a single phone photo to recognize the learned space features, with no need\nof additional assistive technology. We collect more than 600,000 images inside\nMIT campus buildings to train our DCNN model, and achieved 97.9% accuracy in\nvalidation dataset and 81.7% accuracy in test dataset based on spatial-scale\nfixed model. Furthermore, the recognition accuracy and spatial resolution can\nbe potentially improved through multiscale classification model. We identify\nthe discriminative image regions through Class Activating Mapping (CAM)\ntechnique, to observe the model's behavior in how to recognize space and\ninterpret it in an abstract way. By evaluating the results with\nmisclassification matrix, we investigate the visual spatial feature of interior\nspace by looking into its visual similarity and visual distinctiveness, giving\ninsights into interior design and human indoor perception and wayfinding\nresearch. The contribution of this paper is threefold. First, we propose a\nrobust and parsimonious approach for indoor navigation using DCNN. Second, we\ndemonstrate that DCNN also has a potential capability in space feature learning\nand recognition, even under severe appearance changes. Third, we introduce a\nDCNN based approach to look into the visual similarity and visual\ndistinctiveness of interior space.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 20:24:04 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhang", "Fan", ""], ["Duarte", "Fabio", ""], ["Ma", "Ruixian", ""], ["Milioris", "Dimitrios", ""], ["Lin", "Hui", ""], ["Ratti", "Carlo", ""]]}, {"id": "1610.02424", "submitter": "Ashwin Kalyan", "authors": "Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing\n  Sun, Stefan Lee, David Crandall, Dhruv Batra", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\n  Models", "comments": "16 pages; accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence models are widely used to model time-series data. Equally\nubiquitous is the usage of beam search (BS) as an approximate inference\nalgorithm to decode output sequences from these models. BS explores the search\nspace in a greedy left-right fashion retaining only the top-B candidates -\nresulting in sequences that differ only slightly from each other. Producing\nlists of nearly identical sequences is not only computationally wasteful but\nalso typically fails to capture the inherent ambiguity of complex AI tasks. To\novercome this problem, we propose Diverse Beam Search (DBS), an alternative to\nBS that decodes a list of diverse outputs by optimizing for a\ndiversity-augmented objective. We observe that our method finds better top-1\nsolutions by controlling for the exploration and exploitation of the search\nspace - implying that DBS is a better search algorithm. Moreover, these gains\nare achieved with minimal computational or memory over- head as compared to\nbeam search. To demonstrate the broad applicability of our method, we present\nresults on image captioning, machine translation and visual question generation\nusing both standard quantitative metrics and qualitative human studies.\nFurther, we study the role of diversity for image-grounded language generation\ntasks as the complexity of the image changes. We observe that our method\nconsistently outperforms BS and previously proposed techniques for diverse\ndecoding from neural sequence models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 20:56:47 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 13:48:32 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Vijayakumar", "Ashwin K", ""], ["Cogswell", "Michael", ""], ["Selvaraju", "Ramprasath R.", ""], ["Sun", "Qing", ""], ["Lee", "Stefan", ""], ["Crandall", "David", ""], ["Batra", "Dhruv", ""]]}, {"id": "1610.02431", "submitter": "Andrea Vedaldi", "authors": "A. Mahendran and H. Bilen and J. F. Henriques and A. Vedaldi", "title": "ResearchDoom and CocoDoom: Learning Computer Vision with Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we introduce ResearchDoom, an implementation of the Doom\nfirst-person shooter that can extract detailed metadata from the game. We also\nintroduce the CocoDoom dataset, a collection of pre-recorded data extracted\nfrom Doom gaming sessions along with annotations in the MS Coco format.\nResearchDoom and CocoDoom can be used to train and evaluate a variety of\ncomputer vision methods such as object recognition, detection and segmentation\nat the level of instances and categories, tracking, ego-motion estimation,\nmonocular depth estimation and scene segmentation. The code and data are\navailable at http://www.robots.ox.ac.uk/~vgg/research/researchdoom.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 21:35:02 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Mahendran", "A.", ""], ["Bilen", "H.", ""], ["Henriques", "J. F.", ""], ["Vedaldi", "A.", ""]]}, {"id": "1610.02454", "submitter": "Scott Reed", "authors": "Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele,\n  Honglak Lee", "title": "Learning What and Where to Draw", "comments": "In NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently demonstrated the\ncapability to synthesize compelling real-world images, such as room interiors,\nalbum covers, manga, faces, birds, and flowers. While existing models can\nsynthesize images based on global constraints such as a class label or caption,\nthey do not provide control over pose or object location. We propose a new\nmodel, the Generative Adversarial What-Where Network (GAWWN), that synthesizes\nimages given instructions describing what content to draw in which location. We\nshow high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset,\nconditioned on both informal text descriptions and also object location. Our\nsystem exposes control over both the bounding box around the bird and its\nconstituent parts. By modeling the conditional distributions over part\nlocations, our system also enables conditioning on arbitrary subsets of parts\n(e.g. only the beak and tail), yielding an efficient interface for picking part\nlocations. We also show preliminary results on the more challenging domain of\ntext- and location-controllable synthesis of images of human actions on the\nMPII Human Pose dataset.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 00:27:57 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Reed", "Scott", ""], ["Akata", "Zeynep", ""], ["Mohan", "Santosh", ""], ["Tenka", "Samuel", ""], ["Schiele", "Bernt", ""], ["Lee", "Honglak", ""]]}, {"id": "1610.02478", "submitter": "Liane Gabora", "authors": "Graeme McCaig, Steve DiPaola, and Liane Gabora", "title": "Deep Convolutional Networks as Models of Generalization and Blending\n  Within Visual Creativity", "comments": "8 pages, In Proceedings of the 7th International Conference on\n  Computational Creativity. Palo Alto: Association for the Advancement of\n  Artificial Intelligence (AAAI) Press (2016)", "journal-ref": "In Proceedings of the 7th International Conference on\n  Computational Creativity (pp. 156-163). Palo Alto, CA: Association for the\n  Advancement of Artificial Intelligence (AAAI) Press. (2016)", "doi": null, "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine two recent artificial intelligence (AI) based deep learning\nalgorithms for visual blending in convolutional neural networks (Mordvintsev et\nal. 2015, Gatys et al. 2015). To investigate the potential value of these\nalgorithms as tools for computational creativity research, we explain and\nschematize the essential aspects of the algorithms' operation and give visual\nexamples of their output. We discuss the relationship of the two algorithms to\nhuman cognitive science theories of creativity such as conceptual blending\ntheory and honing theory, and characterize the algorithms with respect to\ngeneration of novelty and aesthetic quality.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 04:15:26 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 21:02:30 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["McCaig", "Graeme", ""], ["DiPaola", "Steve", ""], ["Gabora", "Liane", ""]]}, {"id": "1610.02482", "submitter": "Jing Dong", "authors": "Jing Dong, John Gary Burnham, Byron Boots, Glen C. Rains, Frank\n  Dellaert", "title": "4D Crop Monitoring: Spatio-Temporal Reconstruction for Agriculture", "comments": "Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous crop monitoring at high spatial and temporal resolution is a\ncritical problem in precision agriculture. While Structure from Motion and\nMulti-View Stereo algorithms can finely reconstruct the 3D structure of a field\nwith low-cost image sensors, these algorithms fail to capture the dynamic\nnature of continuously growing crops. In this paper we propose a 4D\nreconstruction approach to crop monitoring, which employs a spatio-temporal\nmodel of dynamic scenes that is useful for precision agriculture applications.\nAdditionally, we provide a robust data association algorithm to address the\nproblem of large appearance changes due to scenes being viewed from different\nangles at different points in time, which is critical to achieving 4D\nreconstruction. Finally, we collected a high quality dataset with ground truth\nstatistics to evaluate the performance of our method. We demonstrate that our\n4D reconstruction approach provides models that are qualitatively correct with\nrespect to visual appearance and quantitatively accurate when measured against\nthe ground truth geometric properties of the monitored crops.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 04:34:25 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Dong", "Jing", ""], ["Burnham", "John Gary", ""], ["Boots", "Byron", ""], ["Rains", "Glen C.", ""], ["Dellaert", "Frank", ""]]}, {"id": "1610.02483", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Cheng-Hao Deng, Chong-Wah Ngo", "title": "Boost K-Means", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its simplicity and versatility, k-means remains popular since it was\nproposed three decades ago. The performance of k-means has been enhanced from\ndifferent perspectives over the years. Unfortunately, a good trade-off between\nquality and efficiency is hardly reached. In this paper, a novel k-means\nvariant is presented. Different from most of k-means variants, the clustering\nprocedure is driven by an explicit objective function, which is feasible for\nthe whole l2-space. The classic egg-chicken loop in k-means has been simplified\nto a pure stochastic optimization procedure. The procedure of k-means becomes\nsimpler and converges to a considerably better local optima. The effectiveness\nof this new variant has been studied extensively in different contexts, such as\ndocument clustering, nearest neighbor search and image clustering. Superior\nperformance is observed across different scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 04:36:42 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 07:32:37 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Deng", "Cheng-Hao", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1610.02509", "submitter": "Kareem Ahmed", "authors": "I. M. El-Henawy, Kareem Ahmed", "title": "Content-Based Image Retrieval Using Multiresolution Analysis Of\n  Shape-Based Classified Images", "comments": null, "journal-ref": "Global Journal of Computers & Technology 1, no. 1 (2014)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-Based Image Retrieval (CBIR) systems have been widely used for a wide\nrange of applications such as Art collections, Crime prevention and\nIntellectual property. In this paper, a novel CBIR system, which utilizes\nvisual contents (color, texture and shape) of an image to retrieve images, is\nproposed. The proposed system builds three feature vectors and stores them into\nMySQL database. The first feature vector uses descriptive statistics to\ndescribe the distribution of data in each channel of RGB channels of the image.\nThe second feature vector describes the texture using eigenvalues of the 39\nsub-bands that are generated after applying four levels 2D DWT in each channel\n(red, green and blue channels) of the image. These wavelets sub-bands perfectly\ndescribes the horizontal, vertical and diagonal edges that exist in the\nmulti-resolution analysis of the image. The third feature vector describes the\nbasic shapes that exist in the skeletonization version of the black and white\nrepresentation of the image. Experimental results on a private MYSQL database\nthat consists of 10000 images, using color, texture, shape and stored relevance\nfeedbacks, showed 96.4% average correct retrieval rate in an efficient recovery\ntime.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 10:27:58 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["El-Henawy", "I. M.", ""], ["Ahmed", "Kareem", ""]]}, {"id": "1610.02579", "submitter": "Wanli Ouyang", "authors": "Xingyu Zeng and Wanli Ouyang and Junjie Yan and Hongsheng Li and Tong\n  Xiao and Kun Wang and Yu Liu and Yucong Zhou and Bin Yang and Zhe Wang and\n  Hui Zhou and Xiaogang Wang", "title": "Crafting GBD-Net for Object Detection", "comments": "This paper shows the details of our approach in wining the ImageNet\n  object detection challenge of 2016, with source code provided on\n  \\url{https://github.com/craftGBD/craftGBD}. The preliminary version of this\n  paper is presented at the ECCV. Xingyu Zeng and Wanli Ouyang contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual cues from multiple support regions of different sizes and\nresolutions are complementary in classifying a candidate box in object\ndetection. Effective integration of local and contextual visual cues from these\nregions has become a fundamental problem in object detection.\n  In this paper, we propose a gated bi-directional CNN (GBD-Net) to pass\nmessages among features from different support regions during both feature\nlearning and feature extraction. Such message passing can be implemented\nthrough convolution between neighboring support regions in two directions and\ncan be conducted in various layers. Therefore, local and contextual visual\npatterns can validate the existence of each other by learning their nonlinear\nrelationships and their close interactions are modeled in a more complex way.\nIt is also shown that message passing is not always helpful but dependent on\nindividual samples. Gated functions are therefore needed to control message\ntransmission, whose on-or-offs are controlled by extra visual evidence from the\ninput sample. The effectiveness of GBD-Net is shown through experiments on\nthree object detection datasets, ImageNet, Pascal VOC2007 and Microsoft COCO.\nThis paper also shows the details of our approach in wining the ImageNet object\ndetection challenge of 2016, with source code provided on\n\\url{https://github.com/craftGBD/craftGBD}.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 20:36:20 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zeng", "Xingyu", ""], ["Ouyang", "Wanli", ""], ["Yan", "Junjie", ""], ["Li", "Hongsheng", ""], ["Xiao", "Tong", ""], ["Wang", "Kun", ""], ["Liu", "Yu", ""], ["Zhou", "Yucong", ""], ["Yang", "Bin", ""], ["Wang", "Zhe", ""], ["Zhou", "Hui", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1610.02610", "submitter": "Connor Schenck", "authors": "Connor Schenck and Dieter Fox", "title": "Visual Closed-Loop Control for Pouring Liquids", "comments": "To appear at ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pouring a specific amount of liquid is a challenging task. In this paper we\ndevelop methods for robots to use visual feedback to perform closed-loop\ncontrol for pouring liquids. We propose both a model-based and a model-free\nmethod utilizing deep learning for estimating the volume of liquid in a\ncontainer. Our results show that the model-free method is better able to\nestimate the volume. We combine this with a simple PID controller to pour\nspecific amounts of liquid, and show that the robot is able to achieve an\naverage 38ml deviation from the target amount. To our knowledge, this is the\nfirst use of raw visual feedback to pour liquids in robotics.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 01:28:01 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 02:05:13 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 23:05:47 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Schenck", "Connor", ""], ["Fox", "Dieter", ""]]}, {"id": "1610.02616", "submitter": "Zecheng Xie", "authors": "Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, Terry Lyons", "title": "Learning Spatial-Semantic Context with Fully Convolutional Recurrent\n  Network for Online Handwritten Chinese Text Recognition", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online handwritten Chinese text recognition (OHCTR) is a challenging problem\nas it involves a large-scale character set, ambiguous segmentation, and\nvariable-length input sequences. In this paper, we exploit the outstanding\ncapability of path signature to translate online pen-tip trajectories into\ninformative signature feature maps using a sliding window-based method,\nsuccessfully capturing the analytic and geometric properties of pen strokes\nwith strong local invariance and robustness. A multi-spatial-context fully\nconvolutional recurrent network (MCFCRN) is proposed to exploit the multiple\nspatial contexts from the signature feature maps and generate a prediction\nsequence while completely avoiding the difficult segmentation problem.\nFurthermore, an implicit language model is developed to make predictions based\non semantic context within a predicting feature sequence, providing a new\nperspective for incorporating lexicon constraints and prior knowledge about a\ncertain language in the recognition procedure. Experiments on two standard\nbenchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with\ncorrect rates of 97.10% and 97.15%, respectively, which are significantly\nbetter than the best result reported thus far in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 02:39:07 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 15:33:19 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Xie", "Zecheng", ""], ["Sun", "Zenghui", ""], ["Jin", "Lianwen", ""], ["Ni", "Hao", ""], ["Lyons", "Terry", ""]]}, {"id": "1610.02651", "submitter": "Shanmuganathan Raman", "authors": "Shubham Pachori and Shanmuganathan Raman", "title": "Zero Shot Hashing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a framework to hash images containing instances of\nunknown object classes. In many object recognition problems, we might have\naccess to huge amount of data. It may so happen that even this huge data\ndoesn't cover the objects belonging to classes that we see in our day to day\nlife. Zero shot learning exploits auxiliary information (also called as\nsignatures) in order to predict the labels corresponding to unknown classes. In\nthis work, we attempt to generate the hash codes for images belonging to unseen\nclasses, information of which is available only through the textual corpus. We\nformulate this as an unsupervised hashing formulation as the exact labels are\nnot available for the instances of unseen classes. We show that the proposed\nsolution is able to generate hash codes which can predict labels corresponding\nto unseen classes with appreciably good precision.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 09:46:02 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Pachori", "Shubham", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1610.02692", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Issey Masuda, Santiago Pascual de la Puente and Xavier Giro-i-Nieto", "title": "Open-Ended Visual Question-Answering", "comments": "Bachelor thesis report graded with A with honours at ETSETB Telecom\n  BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016. Source\n  code and models are publicly available at\n  http://imatge-upc.github.io/vqa-2016-cvprw/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis report studies methods to solve Visual Question-Answering (VQA)\ntasks with a Deep Learning framework. As a preliminary step, we explore Long\nShort-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to\ntackle Question-Answering (text based). We then modify the previous model to\naccept an image as an input in addition to the question. For this purpose, we\nexplore the VGG-16 and K-CNN convolutional neural networks to extract visual\nfeatures from the image. These are merged with the word embedding or with a\nsentence embedding of the question to predict the answer. This work was\nsuccessfully submitted to the Visual Question Answering Challenge 2016, where\nit achieved a 53,62% of accuracy in the test dataset. The developed software\nhas followed the best programming practices and Python code style, providing a\nconsistent baseline in Keras for different configurations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 16:38:31 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Masuda", "Issey", ""], ["de la Puente", "Santiago Pascual", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1610.02714", "submitter": "Jessica Finocchiaro", "authors": "Jessica Finocchiaro, Aisha Urooj Khan, Ali Borji", "title": "Egocentric Height Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric, or first-person vision which became popular in recent years with\nan emerge in wearable technology, is different than exocentric (third-person)\nvision in some distinguishable ways, one of which being that the camera wearer\nis generally not visible in the video frames. Recent work has been done on\naction and object recognition in egocentric videos, as well as work on\nbiometric extraction from first-person videos. Height estimation can be a\nuseful feature for both soft-biometrics and object tracking. Here, we propose a\nmethod of estimating the height of an egocentric camera without any calibration\nor reference points. We used both traditional computer vision approaches and\ndeep learning in order to determine the visual cues that results in best height\nestimation. Here, we introduce a framework inspired by two stream networks\ncomprising of two Convolutional Neural Networks, one based on spatial\ninformation, and one based on information given by optical flow in a frame.\nGiven an egocentric video as an input to the framework, our model yields a\nheight estimate as an output. We also incorporate late fusion to learn a\ncombination of temporal and spatial cues. Comparing our model with other\nmethods we used as baselines, we achieve height estimates for videos with a\nMean Average Error of 14.04 cm over a range of 103 cm of data, and\nclassification accuracy for relative height (tall, medium or short) up to\n93.75% where chance level is 33%.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 20:08:17 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Finocchiaro", "Jessica", ""], ["Khan", "Aisha Urooj", ""], ["Borji", "Ali", ""]]}, {"id": "1610.02760", "submitter": "Xiaodong Zhuang", "authors": "Xiaodong Zhuang, N. E. Mastorakis, Jieru Chi, Hanping Wang", "title": "Image Segmentation Based on the Self-Balancing Mechanism in Virtual 3D\n  Elastic Mesh", "comments": "14 pages, 21 figures", "journal-ref": "WSEAS Transactions on Computers, pp. 805-818, Volume 14, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel model of 3D elastic mesh is presented for image\nsegmentation. The model is inspired by stress and strain in physical elastic\nobjects, while the repulsive force and elastic force in the model are defined\nslightly different from the physical force to suit the segmentation problem\nwell. The self-balancing mechanism in the model guarantees the stability of the\nmethod in segmentation. The shape of the elastic mesh at balance state is used\nfor region segmentation, in which the sign distribution of the points'z\ncoordinate values is taken as the basis for segmentation. The effectiveness of\nthe proposed method is proved by analysis and experimental results for both\ntest images and real world images.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 03:13:29 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhuang", "Xiaodong", ""], ["Mastorakis", "N. E.", ""], ["Chi", "Jieru", ""], ["Wang", "Hanping", ""]]}, {"id": "1610.02762", "submitter": "Xiaodong Zhuang", "authors": "Xiaodong Zhuang, N. E. Mastorakis", "title": "Matching of Images with Rotation Transformation Based on the Virtual\n  Electromagnetic Interaction", "comments": "19 pages, 26 figures", "journal-ref": "WSEAS Transactions On Computers, pp. 679-697, Volume 14, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach of image matching for rotating transformation is presented\nand studied. The approach is inspired by electromagnetic interaction force\nbetween physical currents. The virtual current in images is proposed based on\nthe significant edge lines extracted as the fundamental structural feature of\nimages. The virtual electromagnetic force and the corresponding moment is\nstudied between two images after the extraction of the virtual currents in the\nimages. Then image matching for rotating transformation is implemented by\nexploiting the interaction between the virtual currents in the two images to be\nmatched. The experimental results prove the effectiveness of the novel idea,\nwhich indicates the promising application of the proposed method in image\nregistration.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 03:21:35 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhuang", "Xiaodong", ""], ["Mastorakis", "N. E.", ""]]}, {"id": "1610.02850", "submitter": "Manuel Amthor", "authors": "Manuel Amthor, Erik Rodner, Joachim Denzler", "title": "Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets", "comments": "British Machine Vision Conference (BMVC) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Impatient Deep Neural Networks (DNNs) which deal with dynamic time\nbudgets during application. They allow for individual budgets given a priori\nfor each test example and for anytime prediction, i.e., a possible interruption\nat multiple stages during inference while still providing output estimates. Our\napproach can therefore tackle the computational costs and energy demands of\nDNNs in an adaptive manner, a property essential for real-time applications.\nOur Impatient DNNs are based on a new general framework of learning dynamic\nbudget predictors using risk minimization, which can be applied to current DNN\narchitectures by adding early prediction and additional loss layers. A key\naspect of our method is that all of the intermediate predictors are learned\njointly. In experiments, we evaluate our approach for different budget\ndistributions, architectures, and datasets. Our results show a significant gain\nin expected accuracy compared to common baselines.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 11:11:06 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Amthor", "Manuel", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1610.02902", "submitter": "Vania Estrela Dr.", "authors": "Albany E. Herrmann, Vania Vieira Estrela", "title": "Content Based Image Retrieval (CBIR) in Remote Clinical Diagnosis and\n  Healthcare", "comments": "28 pages, 6 figures, Book Chapter from \"Encyclopedia of E-Health and\n  Telemedicine\"", "journal-ref": "Encyclopedia of E-Health and Telemedicine. IGI Global, 2016.\n  495-520. Web. 10 Oct. 2016", "doi": "10.4018/978-1-4666-9978-6.ch039", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-Based Image Retrieval (CBIR) locates, retrieves and displays images\nalike to one given as a query, using a set of features. It demands accessible\ndata in medical archives and from medical equipment, to infer meaning after\nsome processing. A problem similar in some sense to the target image can aid\nclinicians. CBIR complements text-based retrieval and improves evidence-based\ndiagnosis, administration, teaching, and research in healthcare. It facilitates\nvisual/automatic diagnosis and decision-making in real-time remote\nconsultation/screening, store-and-forward tests, home care assistance and\noverall patient surveillance. Metrics help comparing visual data and improve\ndiagnostic. Specially designed architectures can benefit from the application\nscenario. CBIR use calls for file storage standardization, querying procedures,\nefficient image transmission, realistic databases, global availability, access\nsimplicity, and Internet-based structures. This chapter recommends important\nand complex aspects required to handle visual content in healthcare.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 13:22:28 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Herrmann", "Albany E.", ""], ["Estrela", "Vania Vieira", ""]]}, {"id": "1610.02915", "submitter": "Dongyoon Han", "authors": "Dongyoon Han, Jiwhan Kim, and Junmo Kim", "title": "Deep Pyramidal Residual Networks", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have shown remarkable performance\nin image classification tasks in recent years. Generally, deep neural network\narchitectures are stacks consisting of a large number of convolutional layers,\nand they perform downsampling along the spatial dimension via pooling to reduce\nmemory usage. Concurrently, the feature map dimension (i.e., the number of\nchannels) is sharply increased at downsampling locations, which is essential to\nensure effective performance because it increases the diversity of high-level\nattributes. This also applies to residual networks and is very closely related\nto their performance. In this research, instead of sharply increasing the\nfeature map dimension at units that perform downsampling, we gradually increase\nthe feature map dimension at all units to involve as many locations as\npossible. This design, which is discussed in depth together with our new\ninsights, has proven to be an effective means of improving generalization\nability. Furthermore, we propose a novel residual unit capable of further\nimproving the classification accuracy with our new network architecture.\nExperiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown\nthat our network architecture has superior generalization ability compared to\nthe original residual networks. Code is available at\nhttps://github.com/jhkim89/PyramidNet}\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 13:47:13 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 08:42:56 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 08:38:08 GMT"}, {"version": "v4", "created": "Wed, 6 Sep 2017 10:18:53 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Han", "Dongyoon", ""], ["Kim", "Jiwhan", ""], ["Kim", "Junmo", ""]]}, {"id": "1610.02923", "submitter": "Vania Estrela Dr.", "authors": "Alessandra Martins Coelho, Vania V. Estrela", "title": "EM-Based Mixture Models Applied to Video Event Detection", "comments": "25 pages, 8 figures, Available from:\n  http://www.intechopen.com/books/principal-component-analysis-engineering-applications/em-based-mixture-models-applied-to-video-event-detection,\n  Chapter from book \"Principal Component Analysis - Engineering Applications\",\n  Dr. Parinya Sanguansat (Ed.), InTech, 2012. arXiv admin note: text overlap\n  with arXiv:1404.1100 by other authors", "journal-ref": null, "doi": "10.5772/38129", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance system (SS) development requires hi-tech support to prevail over\nthe shortcomings related to the massive quantity of visual information from\nSSs. Anything but reduced human monitoring became impossible by means of its\nphysical and economic implications, and an advance towards an automated\nsurveillance becomes the only way out. When it comes to a computer vision\nsystem, automatic video event comprehension is a challenging task due to motion\nclutter, event understanding under complex scenes, multilevel semantic event\ninference, contextualization of events and views obtained from multiple\ncameras, unevenness of motion scales, shape changes, occlusions and object\ninteractions among lots of other impairments. In recent years, state-of-the-art\nmodels for video event classification and recognition include modeling events\nto discern context, detecting incidents with only one camera, low-level feature\nextraction and description, high-level semantic event classification, and\nrecognition. Even so, it is still very burdensome to recuperate or label a\nspecific video part relying solely on its content. Principal component analysis\n(PCA) has been widely known and used, but when combined with other techniques\nsuch as the expectation-maximization (EM) algorithm its computation becomes\nmore efficient. This chapter introduces advances associated with the concept of\nProbabilistic PCA (PPCA) analysis of video event and it also aims at looking\nclosely to ways and metrics to evaluate these less intensive EM implementations\nof PCA and KPCA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 14:07:49 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Coelho", "Alessandra Martins", ""], ["Estrela", "Vania V.", ""]]}, {"id": "1610.02947", "submitter": "Jongwook Choi", "authors": "Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim", "title": "End-to-end Concept Word Detection for Video Captioning, Retrieval, and\n  Question Answering", "comments": "In CVPR 2017. Winner of three (fill-in-the-blank, multiple-choice\n  test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge. 22\n  pages", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2017, pp. 3165-3173", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high-level concept word detector that can be integrated with any\nvideo-to-language models. It takes a video as input and generates a list of\nconcept words as useful semantic priors for language generation models. The\nproposed word detector has two important properties. First, it does not require\nany external knowledge sources for training. Second, the proposed word detector\nis trainable in an end-to-end manner jointly with any video-to-language models.\nTo maximize the values of detected words, we also develop a semantic attention\nmechanism that selectively focuses on the detected concept words and fuse them\nwith the word encoding and decoding in the language model. In order to\ndemonstrate that the proposed approach indeed improves the performance of\nmultiple video-to-language tasks, we participate in four tasks of LSMDC 2016.\nOur approach achieves the best accuracies in three of them, including\nfill-in-the-blank, multiple-choice test, and movie retrieval. We also attain\ncomparable performance for the other task, movie description.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 15:03:15 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 14:27:20 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 10:12:31 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Yu", "Youngjae", ""], ["Ko", "Hyungjin", ""], ["Choi", "Jongwook", ""], ["Kim", "Gunhee", ""]]}, {"id": "1610.02984", "submitter": "Liang Zheng", "authors": "Liang Zheng, Yi Yang, and Alexander G. Hauptmann", "title": "Person Re-identification: Past, Present and Future", "comments": "20 pages, 5 tables, 10 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) has become increasingly popular in the\ncommunity due to its application and research significance. It aims at spotting\na person of interest in other cameras. In the early days, hand-crafted\nalgorithms and small-scale evaluation were predominantly reported. Recent years\nhave witnessed the emergence of large-scale datasets and deep learning systems\nwhich make use of large data volumes. Considering different tasks, we classify\nmost current re-ID methods into two classes, i.e., image-based and video-based;\nin both tasks, hand-crafted and deep learning systems will be reviewed.\nMoreover, two new re-ID tasks which are much closer to real-world applications\nare described and discussed, i.e., end-to-end re-ID and fast re-ID in very\nlarge galleries. This paper: 1) introduces the history of person re-ID and its\nrelationship with image classification and instance retrieval; 2) surveys a\nbroad selection of the hand-crafted systems and the large-scale methods in both\nimage- and video-based re-ID; 3) describes critical future directions in\nend-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs\nsome important yet under-developed issues.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 16:19:21 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zheng", "Liang", ""], ["Yang", "Yi", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1610.03023", "submitter": "Weixun Zhou", "authors": "Weixun Zhou, Shawn Newsam, Congmin Li, Zhenfeng Shao", "title": "Learning Low Dimensional Convolutional Neural Networks for\n  High-Resolution Remote Sensing Image Retrieval", "comments": null, "journal-ref": "Remote Sens., 9(5), 489 (2017)", "doi": "10.3390/rs9050489", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning powerful feature representations for image retrieval has always been\na challenging task in the field of remote sensing. Traditional methods focus on\nextracting low-level hand-crafted features which are not only time-consuming\nbut also tend to achieve unsatisfactory performance due to the content\ncomplexity of remote sensing images. In this paper, we investigate how to\nextract deep feature representations based on convolutional neural networks\n(CNN) for high-resolution remote sensing image retrieval (HRRSIR). To this end,\ntwo effective schemes are proposed to generate powerful feature representations\nfor HRRSIR. In the first scheme, the deep features are extracted from the\nfully-connected and convolutional layers of the pre-trained CNN models,\nrespectively; in the second scheme, we propose a novel CNN architecture based\non conventional convolution layers and a three-layer perceptron. The novel CNN\nmodel is then trained on a large remote sensing dataset to learn low\ndimensional features. The two schemes are evaluated on several public and\nchallenging datasets, and the results indicate that the proposed schemes and in\nparticular the novel CNN are able to achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 18:45:30 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 19:04:58 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Zhou", "Weixun", ""], ["Newsam", "Shawn", ""], ["Li", "Congmin", ""], ["Shao", "Zhenfeng", ""]]}, {"id": "1610.03129", "submitter": "Aditya Tatu Dr.", "authors": "Aditya Tatu", "title": "Tangled Splines", "comments": "12 pages, To be sent to a Journal/Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting shape information from object bound- aries is a well studied\nproblem in vision, and has found tremen- dous use in applications like object\nrecognition. Conversely, studying the space of shapes represented by curves\nsatisfying certain constraints is also intriguing. In this paper, we model and\nanalyze the space of shapes represented by a 3D curve (space curve) formed by\nconnecting n pieces of quarter of a unit circle. Such a space curve is what we\ncall a Tangle, the name coming from a toy built on the same principle. We\nprovide two models for the shape space of n-link open and closed tangles, and\nwe show that tangles are a subset of trigonometric splines of a certain order.\nWe give algorithms for curve approximation using open/closed tangles, computing\ngeodesics on these shape spaces, and to find the deformation that takes one\ngiven tangle to another given tangle, i.e., the Log map. The algorithms\nprovided yield tangles upto a small and acceptable tolerance, as shown by the\nresults given in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 23:31:18 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Tatu", "Aditya", ""]]}, {"id": "1610.03151", "submitter": "Matthias Nie{\\ss}ner", "authors": "Justus Thies, Michael Zollh\\\"ofer, Marc Stamminger, Christian\n  Theobalt, Matthias Nie{\\ss}ner", "title": "FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in Virtual\n  Reality", "comments": "Video: https://youtu.be/jIlujM5avU8 Presented at Siggraph'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FaceVR, a novel image-based method that enables video\nteleconferencing in VR based on self-reenactment. State-of-the-art face\ntracking methods in the VR context are focused on the animation of rigged 3d\navatars. While they achieve good tracking performance the results look\ncartoonish and not real. In contrast to these model-based approaches, FaceVR\nenables VR teleconferencing using an image-based technique that results in\nnearly photo-realistic outputs. The key component of FaceVR is a robust\nalgorithm to perform real-time facial motion capture of an actor who is wearing\na head-mounted display (HMD), as well as a new data-driven approach for eye\ntracking from monocular videos. Based on reenactment of a prerecorded stereo\nvideo of the person without the HMD, FaceVR incorporates photo-realistic\nre-rendering in real time, thus allowing artificial modifications of face and\neye appearances. For instance, we can alter facial expressions or change gaze\ndirections in the prerecorded target video. In a live setup, we apply these\nnewly-introduced algorithmic components.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 01:35:56 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 21:35:57 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Stamminger", "Marc", ""], ["Theobalt", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1610.03155", "submitter": "Miao Sun", "authors": "Miao Sun, Tony X. Han, Ming-Chang Liu and Ahmad Khodayari-Rostamabad", "title": "Multiple Instance Learning Convolutional Neural Networks for Object\n  Recognition", "comments": "International Conference on Pattern Recognition(ICPR) 2016, Oral\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have demon- strated its successful\napplications in computer vision, speech recognition, and natural language\nprocessing. For object recog- nition, CNNs might be limited by its strict label\nrequirement and an implicit assumption that images are supposed to be target-\nobject-dominated for optimal solutions. However, the labeling procedure,\nnecessitating laying out the locations of target ob- jects, is very tedious,\nmaking high-quality large-scale dataset prohibitively expensive. Data\naugmentation schemes are widely used when deep networks suffer the insufficient\ntraining data problem. All the images produced through data augmentation share\nthe same label, which may be problematic since not all data augmentation\nmethods are label-preserving. In this paper, we propose a weakly supervised CNN\nframework named Multiple Instance Learning Convolutional Neural Networks\n(MILCNN) to solve this problem. We apply MILCNN framework to object recognition\nand report state-of-the-art performance on three benchmark datasets: CIFAR10,\nCIFAR100 and ILSVRC2015 classification dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 02:02:16 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Sun", "Miao", ""], ["Han", "Tony X.", ""], ["Liu", "Ming-Chang", ""], ["Khodayari-Rostamabad", "Ahmad", ""]]}, {"id": "1610.03341", "submitter": "Hamed Saghaei Mr.", "authors": "Hamed Saghaei", "title": "Proposal for Automatic License and Number Plate Recognition System for\n  Vehicle Identification", "comments": "5 pages, 3 figures, 2016 1st International Conference on New Research\n  Achievements in Electrical and Computer Engineering", "journal-ref": null, "doi": "10.1016/S0550-3213(01)00405-9", "report-no": "1610.03341", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an automatic and mechanized license and number\nplate recognition (LNPR) system which can extract the license plate number of\nthe vehicles passing through a given location using image processing\nalgorithms. No additional devices such as GPS or radio frequency identification\n(RFID) need to be installed for implementing the proposed system. Using special\ncameras, the system takes pictures from each passing vehicle and forwards the\nimage to the computer for being processed by the LPR software. Plate\nrecognition software uses different algorithms such as localization,\norientation, normalization, segmentation and finally optical character\nrecognition (OCR). The resulting data is applied to compare with the records on\na database. Experimental results reveal that the presented system successfully\ndetects and recognizes the vehicle number plate on real images. This system can\nalso be used for security and traffic control.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 11:33:34 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Saghaei", "Hamed", ""]]}, {"id": "1610.03368", "submitter": "J\\\"orn Schrieber", "authors": "J\\\"orn Schrieber, Dominic Schuhmacher, Carsten Gottschlich", "title": "DOTmark - A Benchmark for Discrete Optimal Transport", "comments": null, "journal-ref": "IEEE Access, vol. 5, pp. 271-282, 2017", "doi": "10.1109/ACCESS.2016.2639065", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein metric or earth mover's distance (EMD) is a useful tool in\nstatistics, machine learning and computer science with many applications to\nbiological or medical imaging, among others. Especially in the light of\nincreasingly complex data, the computation of these distances via optimal\ntransport is often the limiting factor. Inspired by this challenge, a variety\nof new approaches to optimal transport has been proposed in recent years and\nalong with these new methods comes the need for a meaningful comparison.\n  In this paper, we introduce a benchmark for discrete optimal transport,\ncalled DOTmark, which is designed to serve as a neutral collection of problems,\nwhere discrete optimal transport methods can be tested, compared to one\nanother, and brought to their limits on large-scale instances. It consists of a\nvariety of grayscale images, in various resolutions and classes, such as\nseveral types of randomly generated images, classical test images and real data\nfrom microscopy.\n  Along with the DOTmark we present a survey and a performance test for a cross\nsection of established methods ranging from more traditional algorithms, such\nas the transportation simplex, to recently developed approaches, such as the\nshielding neighborhood method, and including also a comparison with commercial\nsolvers.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 14:36:46 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Schrieber", "J\u00f6rn", ""], ["Schuhmacher", "Dominic", ""], ["Gottschlich", "Carsten", ""]]}, {"id": "1610.03393", "submitter": "Nahum Kiryati", "authors": "Adi Perry, Dor Verbin, Nahum Kiryati", "title": "Crossing the Road Without Traffic Lights: An Android-based Safety Device", "comments": "Planned submission to \"Pattern Recognition Letters\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of pedestrian crossing lights, finding a safe moment to cross\nthe road is often hazardous and challenging, especially for people with visual\nimpairments. We present a reliable low-cost solution, an Android device\nattached to a traffic sign or lighting pole near the crossing, indicating\nwhether it is safe to cross the road. The indication can be by sound, display,\nvibration, and various communication modalities provided by the Android device.\nThe integral system camera is aimed at approaching traffic. Optical flow is\ncomputed from the incoming video stream, and projected onto an influx map,\nautomatically acquired during a brief training period. The crossing safety is\ndetermined based on a 1-dimensional temporal signal derived from the\nprojection. We implemented the complete system on a Samsung Galaxy K-Zoom\nAndroid smartphone, and obtained real-time operation. The system achieves\npromising experimental results, providing pedestrians with sufficiently early\nwarning of approaching vehicles. The system can serve as a stand-alone safety\ndevice, that can be installed where pedestrian crossing lights are ruled out.\nRequiring no dedicated infrastructure, it can be powered by a solar panel and\nremotely maintained via the cellular network.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 15:33:00 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Perry", "Adi", ""], ["Verbin", "Dor", ""], ["Kiryati", "Nahum", ""]]}, {"id": "1610.03437", "submitter": "Jo\\~ao Oliveira", "authors": "Jo\\~ao P. Oliveira and Ana Bragan\\c{c}a and Jos\\'e Bioucas-Dias and\n  M\\'ario Figueiredo and Lu\\'is Alc\\'acer and Jorge Morgado and Quirina\n  Ferreira", "title": "Restoring STM images via Sparse Coding: noise and artifact removal", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a denoising algorithm to improve the\ninterpretation and quality of scanning tunneling microscopy (STM) images. Given\nthe high level of self-similarity of STM images, we propose a denoising\nalgorithm by reformulating the true estimation problem as a sparse regression,\noften termed sparse coding. We introduce modifications to the algorithm to cope\nwith the existence of artifacts, mainly dropouts, which appear in a structured\nway as consecutive line segments on the scanning direction. The resulting\nalgorithm treats the artifacts as missing data, and the estimated values\noutperform those algorithms that substitute the outliers by a local filtering.\nWe provide code implementations for both Matlab and Gwyddion.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 17:37:47 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Oliveira", "Jo\u00e3o P.", ""], ["Bragan\u00e7a", "Ana", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Figueiredo", "M\u00e1rio", ""], ["Alc\u00e1cer", "Lu\u00eds", ""], ["Morgado", "Jorge", ""], ["Ferreira", "Quirina", ""]]}, {"id": "1610.03466", "submitter": "Xianzhi Du", "authors": "Xianzhi Du and Mostafa El-Khamy and Jungwon Lee and Larry S. Davis", "title": "Fused DNN: A deep neural network fusion approach to fast and robust\n  pedestrian detection", "comments": "WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep neural network fusion architecture for fast and robust\npedestrian detection. The proposed network fusion architecture allows for\nparallel processing of multiple networks for speed. A single shot deep\nconvolutional network is trained as a object detector to generate all possible\npedestrian candidates of different sizes and occlusions. This network outputs a\nlarge variety of pedestrian candidates to cover the majority of ground-truth\npedestrians while also introducing a large number of false positives. Next,\nmultiple deep neural networks are used in parallel for further refinement of\nthese pedestrian candidates. We introduce a soft-rejection based network fusion\nmethod to fuse the soft metrics from all networks together to generate the\nfinal confidence scores. Our method performs better than existing\nstate-of-the-arts, especially when detecting small-size and occluded\npedestrians. Furthermore, we propose a method for integrating pixel-wise\nsemantic segmentation network into the network fusion architecture as a\nreinforcement to the pedestrian detector. The approach outperforms\nstate-of-the-art methods on most protocols on Caltech Pedestrian dataset, with\nsignificant boosts on several protocols. It is also faster than all other\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 18:59:12 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 15:45:56 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Du", "Xianzhi", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""], ["Davis", "Larry S.", ""]]}, {"id": "1610.03467", "submitter": "Dayong Wang", "authors": "Manan Shah, Christopher Rubadue, David Suster, Dayong Wang", "title": "Deep Learning Assessment of Tumor Proliferation in Breast Cancer\n  Histological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current analysis of tumor proliferation, the most salient prognostic\nbiomarker for invasive breast cancer, is limited to subjective mitosis counting\nby pathologists in localized regions of tissue images. This study presents the\nfirst data-driven integrative approach to characterize the severity of tumor\ngrowth and spread on a categorical and molecular level, utilizing multiple\nbiologically salient deep learning classifiers to develop a comprehensive\nprognostic model. Our approach achieves pathologist-level performance on\nthree-class categorical tumor severity prediction. It additionally pioneers\nprediction of molecular expression data from a tissue image, obtaining a\nSpearman's rank correlation coefficient of 0.60 with ex vivo mean calculated\nRNA expression. Furthermore, our framework is applied to identify over two\nhundred unprecedented biomarkers critical to the accurate assessment of tumor\nproliferation, validating our proposed integrative pipeline as the first to\nholistically and objectively analyze histopathological images.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 19:00:35 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Shah", "Manan", ""], ["Rubadue", "Christopher", ""], ["Suster", "David", ""], ["Wang", "Dayong", ""]]}, {"id": "1610.03548", "submitter": "Mathias Gehrig", "authors": "Mathias Gehrig, Elena Stumm, Timo Hinzmann and Roland Siegwart", "title": "Visual Place Recognition with Probabilistic Vertex Voting", "comments": "8 pages", "journal-ref": "2017 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": "10.1109/ICRA.2017.7989362", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel scoring concept for visual place recognition based on\nnearest neighbor descriptor voting and demonstrate how the algorithm naturally\nemerges from the problem formulation. Based on the observation that the number\nof votes for matching places can be evaluated using a binomial distribution\nmodel, loop closures can be detected with high precision. By casting the\nproblem into a probabilistic framework, we not only remove the need for\ncommonly employed heuristic parameters but also provide a powerful score to\nclassify matching and non-matching places. We present methods for both a 2D-2D\npose-graph vertex matching and a 2D-3D landmark matching based on the above\nscoring. The approach maintains accuracy while being efficient enough for\nonline application through the use of compact (low dimensional) descriptors and\nfast nearest neighbor retrieval techniques. The proposed methods are evaluated\non several challenging datasets in varied environments, showing\nstate-of-the-art results with high precision and high recall.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 22:16:59 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 12:32:32 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Gehrig", "Mathias", ""], ["Stumm", "Elena", ""], ["Hinzmann", "Timo", ""], ["Siegwart", "Roland", ""]]}, {"id": "1610.03604", "submitter": "Yu Song", "authors": "Yu Song, Yiquan Wu", "title": "Subspace clustering based on low rank representation and weighted\n  nuclear norm minimization", "comments": "17 pages, 3 figures, 5 tables This paper is also submitted to the\n  journal 'pattern recognition'. arXiv admin note: text overlap with\n  arXiv:1203.1005 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of segmenting a set of data points\napproximately drawn from a union of multiple linear subspaces. Aiming at the\nsubspace clustering problem, various subspace clustering algorithms have been\nproposed and low rank representation based subspace clustering is a very\npromising and efficient subspace clustering algorithm. Low rank representation\nmethod seeks the lowest rank representation among all the candidates that can\nrepresent the data points as linear combinations of the bases in a given\ndictionary. Nuclear norm minimization is adopted to minimize the rank of the\nrepresentation matrix. However, nuclear norm is not a very good approximation\nof the rank of a matrix and the representation matrix thus obtained can be of\nhigh rank which will affect the final clustering accuracy. Weighted nuclear\nnorm (WNN) is a better approximation of the rank of a matrix and WNN is adopted\nin this paper to describe the rank of the representation matrix. The convex\nprogram is solved via conventional alternation direction method of multipliers\n(ADMM) and linearized alternating direction method of multipliers (LADMM) and\nthey are respectively refer to as WNNM-LRR and WNNM-LRR(L). Experimental\nresults show that, compared with low rank representation method and several\nother state-of-the-art subspace clustering methods, WNNM-LRR and WNNM-LRR(L)\ncan get higher clustering accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 06:11:34 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 14:26:17 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 03:08:52 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Song", "Yu", ""], ["Wu", "Yiquan", ""]]}, {"id": "1610.03612", "submitter": "Xiaodong Zhuang", "authors": "Xiaodong Zhuang, N. E. Mastorakis", "title": "The Analysis of Local Motion and Deformation in Image Sequences Inspired\n  by Physical Electromagnetic Interaction", "comments": "15 pages, 23 figures. arXiv admin note: substantial text overlap with\n  arXiv:1610.03615, arXiv:1610.02762", "journal-ref": "WSEAS TRANSACTIONS on COMPUTERS, pp. 231-245, Volume 14, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to analyze the moving and deforming of the objects in image\nsequence, a novel way is presented to analyze the local changes of object edges\nbetween two related images (such as two adjacent frames in a video sequence),\nwhich is inspired by the physical electromagnetic interaction. The changes of\nedge between adjacent frames in sequences are analyzed by simulation of virtual\ncurrent interaction, which can reflect the change of the object's position or\nshape. The virtual current along the main edge line is proposed based on the\nsignificant edge extraction. Then the virtual interaction between the current\nelements in the two related images is studied by imitating the interaction\nbetween physical current-carrying wires. The experimental results prove that\nthe distribution of magnetic forces on the current elements in one image\napplied by the other can reflect the local change of edge lines from one image\nto the other, which is important in further analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 06:39:15 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Zhuang", "Xiaodong", ""], ["Mastorakis", "N. E.", ""]]}, {"id": "1610.03614", "submitter": "Xiaodong Zhuang", "authors": "Xiaodong Zhuang, N. E. Mastorakis", "title": "A Model of Virtual Carrier Immigration in Digital Images for Region\n  Segmentation", "comments": "11 pages, 17 figures. arXiv admin note: text overlap with\n  arXiv:1610.02760", "journal-ref": "WSEAS TRANSACTIONS on COMPUTERS, pp. 708-718, Volume 14, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel model for image segmentation is proposed, which is inspired by the\ncarrier immigration mechanism in physical P-N junction. The carrier diffusing\nand drifting are simulated in the proposed model, which imitates the physical\nself-balancing mechanism in P-N junction. The effect of virtual carrier\nimmigration in digital images is analyzed and studied by experiments on test\nimages and real world images. The sign distribution of net carrier at the\nmodel's balance state is exploited for region segmentation. The experimental\nresults for both test images and real-world images demonstrate self-adaptive\nand meaningful gathering of pixels to suitable regions, which prove the\neffectiveness of the proposed method for image region segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 06:43:34 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Zhuang", "Xiaodong", ""], ["Mastorakis", "N. E.", ""]]}, {"id": "1610.03615", "submitter": "Xiaodong Zhuang", "authors": "Xiaodong Zhuang, N. E. Mastorakis", "title": "The Virtual Electromagnetic Interaction between Digital Images for Image\n  Matching with Shifting Transformation", "comments": "17 pages, 39 figures. arXiv admin note: substantial text overlap with\n  arXiv:1610.03612, arXiv:1610.02762", "journal-ref": "WSEAS Transactions on Computers, pp. 107-123, Volume 14, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel way of matching two images with shifting transformation is studied.\nThe approach is based on the presentation of the virtual edge current in\nimages, and also the study of virtual electromagnetic interaction between two\nrelated images inspired by electromagnetism. The edge current in images is\nproposed as a discrete simulation of the physical current, which is based on\nthe significant edge line extracted by Canny-like edge detection. Then the\nvirtual interaction of the edge currents between related images is studied by\nimitating the electro-magnetic interaction between current-carrying wires.\nBased on the virtual interaction force between two related images, a novel\nmethod is presented and applied in image matching for shifting transformation.\nThe preliminary experimental results indicate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 06:48:04 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Zhuang", "Xiaodong", ""], ["Mastorakis", "N. E.", ""]]}, {"id": "1610.03623", "submitter": "Pedro Porto Buarque De Gusmao", "authors": "Pedro Porto Buarque de Gusm\\~ao, Gianluca Francini, Skjalg Leps{\\o}y,\n  Enrico Magli", "title": "Fast Training of Convolutional Neural Networks via Kernel Rescaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep Convolutional Neural Networks (CNN) is a time consuming task\nthat may take weeks to complete. In this article we propose a novel,\ntheoretically founded method for reducing CNN training time without incurring\nany loss in accuracy. The basic idea is to begin training with a pre-train\nnetwork using lower-resolution kernels and input images, and then refine the\nresults at the full resolution by exploiting the spatial scaling property of\nconvolutions. We apply our method to the ImageNet winner OverFeat and to the\nmore recent ResNet architecture and show a reduction in training time of nearly\n20% while test set accuracy is preserved in both cases.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 07:25:34 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["de Gusm\u00e3o", "Pedro Porto Buarque", ""], ["Francini", "Gianluca", ""], ["Leps\u00f8y", "Skjalg", ""], ["Magli", "Enrico", ""]]}, {"id": "1610.03628", "submitter": "Carlos Ciller Mr.", "authors": "Stefanos Apostolopoulos, Carlos Ciller, Sandro I. De Zanet, Sebastian\n  Wolf and Raphael Sznitman", "title": "RetiNet: Automatic AMD identification in OCT volumetric data", "comments": "14 pages, 10 figures, Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) provides a unique ability to image the eye\nretina in 3D at micrometer resolution and gives ophthalmologist the ability to\nvisualize retinal diseases such as Age-Related Macular Degeneration (AMD).\nWhile visual inspection of OCT volumes remains the main method for AMD\nidentification, doing so is time consuming as each cross-section within the\nvolume must be inspected individually by the clinician. In much the same way,\nacquiring ground truth information for each cross-section is expensive and time\nconsuming. This fact heavily limits the ability to acquire large amounts of\nground truth, which subsequently impacts the performance of learning-based\nmethods geared at automatic pathology identification. To avoid this burden, we\npropose a novel strategy for automatic analysis of OCT volumes where only\nvolume labels are needed. That is, we train a classifier in a semi-supervised\nmanner to conduct this task. Our approach uses a novel Convolutional Neural\nNetwork (CNN) architecture, that only needs volume-level labels to be trained\nto automatically asses whether an OCT volume is healthy or contains AMD. Our\narchitecture involves first learning a cross-section pathology classifier using\npseudo-labels that could be corrupted and then leverage these towards a more\naccurate volume-level classification. We then show that our approach provides\nexcellent performances on a publicly available dataset and outperforms a number\nof existing automatic techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 07:56:24 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Apostolopoulos", "Stefanos", ""], ["Ciller", "Carlos", ""], ["De Zanet", "Sandro I.", ""], ["Wolf", "Sebastian", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1610.03640", "submitter": "Xiaohua Huang", "authors": "Xiaohua Huang, Abhinav Dhall, Xin Liu, Guoying Zhao, Jingang Shi,\n  Roland Goecke, Matti Pietikainen", "title": "Analyzing the Affect of a Group of People Using Multi-modal Framework", "comments": "11 pages. Submitted to the IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of images on the web enable us to explore images from social events\nsuch as a family party, thus it is of interest to understand and model the\naffect exhibited by a group of people in images. But analysis of the affect\nexpressed by multiple people is challenging due to varied indoor and outdoor\nsettings, and interactions taking place between various numbers of people. A\nfew existing works on Group-level Emotion Recognition (GER) have investigated\non face-level information. Due to the challenging environments, face may not\nprovide enough information to GER. Relatively few studies have investigated\nmulti-modal GER. Therefore, we propose a novel multi-modal approach based on a\nnew feature description for understanding emotional state of a group of people\nin an image. In this paper, we firstly exploit three kinds of rich information\ncontaining face, upperbody and scene in a group-level image. Furthermore, in\norder to integrate multiple person's information in a group-level image, we\npropose an information aggregation method to generate three features for face,\nupperbody and scene, respectively. We fuse face, upperbody and scene\ninformation for robustness of GER against the challenging environments.\nIntensive experiments are performed on two challenging group-level emotion\ndatabases to investigate the role of face, upperbody and scene as well as\nmulti-modal framework. Experimental results demonstrate that our framework\nachieves very promising performance for GER.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 08:45:52 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 21:43:39 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Huang", "Xiaohua", ""], ["Dhall", "Abhinav", ""], ["Liu", "Xin", ""], ["Zhao", "Guoying", ""], ["Shi", "Jingang", ""], ["Goecke", "Roland", ""], ["Pietikainen", "Matti", ""]]}, {"id": "1610.03660", "submitter": "Yihong Wu", "authors": "Yihong Wu, Fulin Tang, Heping Li", "title": "Image Based Camera Localization: an Overview", "comments": null, "journal-ref": "Invited Paper by Visual Computing for Industry, Biomedicine and\n  Art, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, virtual reality, augmented reality, robotics, autonomous driving et\nal attract much attention of both academic and industrial community, in which\nimage based camera localization is a key task. However, there has not been a\ncomplete review on image-based camera localization. It is urgent to map this\ntopic to help people enter the field quickly. In this paper, an overview of\nimage based camera localization is presented. A new and complete kind of\nclassifications for image based camera localization is provided and the related\ntechniques are introduced. Trends for the future development are also\ndiscussed. It will be useful to not only researchers but also engineers and\nother people interested.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 10:19:36 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 08:43:29 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 09:55:59 GMT"}, {"version": "v4", "created": "Thu, 3 May 2018 11:09:03 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Wu", "Yihong", ""], ["Tang", "Fulin", ""], ["Li", "Heping", ""]]}, {"id": "1610.03670", "submitter": "Qi Dong", "authors": "Qi Dong, Shaogang Gong, Xiatian Zhu", "title": "Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising detailed clothing characteristics (fine-grained attributes) in\nunconstrained images of people in-the-wild is a challenging task for computer\nvision, especially when there is only limited training data from the wild\nwhilst most data available for model learning are captured in well-controlled\nenvironments using fashion models (well lit, no background clutter, frontal\nview, high-resolution). In this work, we develop a deep learning framework\ncapable of model transfer learning from well-controlled shop clothing images\ncollected from web retailers to in-the-wild images from the street.\nSpecifically, we formulate a novel Multi-Task Curriculum Transfer (MTCT) deep\nlearning method to explore multiple sources of different types of web\nannotations with multi-labelled fine-grained attributes. Our multi-task loss\nfunction is designed to extract more discriminative representations in training\nby jointly learning all attributes, and our curriculum strategy exploits the\nstaged easy-to-complex transfer learning motivated by cognitive studies. We\ndemonstrate the advantages of the MTCT model over the state-of-the-art methods\non the X-Domain benchmark, a large scale clothing attribute dataset. Moreover,\nwe show that the MTCT model has a notable advantage over contemporary models\nwhen the training data size is small.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 11:17:16 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 12:11:55 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 10:32:54 GMT"}, {"version": "v4", "created": "Sun, 25 Dec 2016 23:43:22 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Dong", "Qi", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""]]}, {"id": "1610.03677", "submitter": "Suchet Bargoti", "authors": "Suchet Bargoti and James Underwood", "title": "Deep Fruit Detection in Orchards", "comments": "Submitted to the IEEE International Conference on Robotics and\n  Automation 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate and reliable image based fruit detection system is critical for\nsupporting higher level agriculture tasks such as yield mapping and robotic\nharvesting. This paper presents the use of a state-of-the-art object detection\nframework, Faster R-CNN, in the context of fruit detection in orchards,\nincluding mangoes, almonds and apples. Ablation studies are presented to better\nunderstand the practical deployment of the detection network, including how\nmuch training data is required to capture variability in the dataset. Data\naugmentation techniques are shown to yield significant performance gains,\nresulting in a greater than two-fold reduction in the number of training images\nrequired. In contrast, transferring knowledge between orchards contributed to\nnegligible performance gain over initialising the Deep Convolutional Neural\nNetwork directly from ImageNet features. Finally, to operate over orchard data\ncontaining between 100-1000 fruit per image, a tiling approach is introduced\nfor the Faster R-CNN framework. The study has resulted in the best yet\ndetection performance for these orchards relative to previous works, with an\nF1-score of >0.9 achieved for apples and mangoes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 11:40:24 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 01:03:55 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bargoti", "Suchet", ""], ["Underwood", "James", ""]]}, {"id": "1610.03684", "submitter": "Jie Chen", "authors": "Jie Chen, Junhui Hou and Lap-Pui Chau", "title": "Light Field Compression with Disparity Guided Sparse Coding based on\n  Structural Key Views", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2750413", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent imaging technologies are rapidly evolving for sampling richer and more\nimmersive representations of the 3D world. And one of the emerging technologies\nare light field (LF) cameras based on micro-lens arrays. To record the\ndirectional information of the light rays, a much larger storage space and\ntransmission bandwidth are required by a LF image as compared with a\nconventional 2D image of similar spatial dimension, and the compression of LF\ndata becomes a vital part of its application.\n  In this paper, we propose a LF codec that fully exploits the intrinsic\ngeometry between the LF sub-views by first approximating the LF with disparity\nguided sparse coding over a perspective shifted light field dictionary. The\nsparse coding is only based on several optimized Structural Key Views (SKV);\nhowever the entire LF can be recovered from the coding coefficients. By keeping\nthe approximation identical between encoder and decoder, only the residuals of\nthe non-key views, disparity map and the SKVs need to be compressed into the\nbit stream. An optimized SKV selection method is proposed such that most LF\nspatial information could be preserved. And to achieve optimum dictionary\nefficiency, the LF is divided into several Coding Regions (CR), over which the\nreconstruction works individually. Experiments and comparisons have been\ncarried out over benchmark LF dataset, which show that the proposed SC-SKV\ncodec produces convincing compression results in terms of both rate-distortion\nperformance and visual quality compared with High Efficiency Video Coding\n(HEVC): with 47.87% BD-rate reduction and 1.59 dB BD-PSNR improvement achieved\non average, especially with up to 4 dB improvement for low bit rate scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 12:23:49 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 07:30:34 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Chen", "Jie", ""], ["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1610.03708", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Christof Monz, Arnold W.M. Smeulders", "title": "Generating captions without looking beyond objects", "comments": "This paper was presented at the ECCV2016 2nd Workshop on Storytelling\n  with Images and Videos (VisStory)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores new evaluation perspectives for image captioning and\nintroduces a noun translation task that achieves comparative image caption\ngeneration performance by translating from a set of nouns to captions. This\nimplies that in image captioning, all word categories other than nouns can be\nevoked by a powerful language model without sacrificing performance on n-gram\nprecision. The paper also investigates lower and upper bounds of how much\nindividual word categories in the captions contribute to the final BLEU score.\nA large possible improvement exists for nouns, verbs, and prepositions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 13:42:03 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 09:35:03 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Heuer", "Hendrik", ""], ["Monz", "Christof", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1610.03761", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Babak Taati", "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble\n  of Autoencoders", "comments": "25 pages, 6 figures, 4 Tables", "journal-ref": "Expert Systems with Applications, Volume 87, 30 November 2017,\n  Pages 280-290", "doi": "10.1016/j.eswa.2017.06.011", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fall is an abnormal activity that occurs rarely, so it is hard to collect\nreal data for falls. It is, therefore, difficult to use supervised learning\nmethods to automatically detect falls. Another challenge in using machine\nlearning methods to automatically detect falls is the choice of engineered\nfeatures. In this paper, we propose to use an ensemble of autoencoders to\nextract features from different channels of wearable sensor data trained only\non normal activities. We show that the traditional approach of choosing a\nthreshold as the maximum of the reconstruction error on the training normal\ndata is not the right way to identify unseen falls. We propose two methods for\nautomatic tightening of reconstruction error from only the normal activities\nfor better identification of unseen falls. We present our results on two\nactivity recognition datasets and show the efficacy of our proposed method\nagainst traditional autoencoder models and two standard one-class\nclassification methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 15:55:06 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 18:00:04 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 20:51:59 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Taati", "Babak", ""]]}, {"id": "1610.03777", "submitter": "Edward Grant", "authors": "Edward Grant, Pushmeet Kohli, Marcel van Gerven", "title": "Deep disentangled representations for volumetric reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a convolutional neural network for inferring a compact\ndisentangled graphical description of objects from 2D images that can be used\nfor volumetric reconstruction. The network comprises an encoder and a\ntwin-tailed decoder. The encoder generates a disentangled graphics code. The\nfirst decoder generates a volume, and the second decoder reconstructs the input\nimage using a novel training regime that allows the graphics code to learn a\nseparate representation of the 3D object and a description of its lighting and\npose conditions. We demonstrate this method by generating volumes and\ndisentangled graphical descriptions from images and videos of faces and chairs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 16:36:37 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Grant", "Edward", ""], ["Kohli", "Pushmeet", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1610.03782", "submitter": "Christian Richardt", "authors": "Hyeongwoo Kim and Christian Richardt and Christian Theobalt", "title": "Video Depth-From-Defocus", "comments": "13 pages, supplemental document included as appendix, 3DV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many compelling video post-processing effects, in particular aesthetic focus\nediting and refocusing effects, are feasible if per-frame depth information is\navailable. Existing computational methods to capture RGB and depth either\npurposefully modify the optics (coded aperture, light-field imaging), or employ\nactive RGB-D cameras. Since these methods are less practical for users with\nnormal cameras, we present an algorithm to capture all-in-focus RGB-D video of\ndynamic scenes with an unmodified commodity video camera. Our algorithm turns\nthe often unwanted defocus blur into a valuable signal. The input to our method\nis a video in which the focus plane is continuously moving back and forth\nduring capture, and thus defocus blur is provoked and strongly visible. This\ncan be achieved by manually turning the focus ring of the lens during\nrecording. The core algorithmic ingredient is a new video-based\ndepth-from-defocus algorithm that computes space-time-coherent depth maps,\ndeblurred all-in-focus video, and the focus distance for each frame. We\nextensively evaluate our approach, and show that it enables compelling video\npost-processing effects, such as different types of refocusing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 16:43:10 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Kim", "Hyeongwoo", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""]]}, {"id": "1610.03819", "submitter": "Haizhao Yang", "authors": "Jieren Xu and Haizhao Yang and Ingrid Daubechies", "title": "Recursive Diffeomorphism-Based Regression for Shape Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a recursive diffeomorphism based regression method for\none-dimensional generalized mode decomposition problem that aims at extracting\ngeneralized modes $\\alpha_k(t)s_k(2\\pi N_k\\phi_k(t))$ from their superposition\n$\\sum_{k=1}^K \\alpha_k(t)s_k(2\\pi N_k\\phi_k(t))$. First, a one-dimensional\nsynchrosqueezed transform is applied to estimate instantaneous information,\ne.g., $\\alpha_k(t)$ and $N_k\\phi_k(t)$. Second, a novel approach based on\ndiffeomorphisms and nonparametric regression is proposed to estimate wave shape\nfunctions $s_k(t)$. These two methods lead to a framework for the generalized\nmode decomposition problem under a weak well-separation condition. Numerical\nexamples of synthetic and real data are provided to demonstrate the fruitful\napplications of these methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 18:43:51 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 04:44:15 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Xu", "Jieren", ""], ["Yang", "Haizhao", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1610.03898", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Jonathan Wu, Janusz Konrad, Prakash Ishwar", "title": "Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at\n  Extremely Low Resolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (ConvNets) have been recently shown to\nattain state-of-the-art performance for action recognition on\nstandard-resolution videos. However, less attention has been paid to\nrecognition performance at extremely low resolutions (eLR) (e.g., 16 x 12\npixels). Reliable action recognition using eLR cameras would address privacy\nconcerns in various application environments such as private homes, hospitals,\nnursing/rehabilitation facilities, etc. In this paper, we propose a\nsemi-coupled filter-sharing network that leverages high resolution (HR) videos\nduring training in order to assist an eLR ConvNet. We also study methods for\nfusing spatial and temporal ConvNets customized for eLR videos in order to take\nadvantage of appearance and motion information. Our method outperforms\nstate-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB\n(29.2%) datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 23:19:57 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 19:00:02 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Chen", "Jiawei", ""], ["Wu", "Jonathan", ""], ["Konrad", "Janusz", ""], ["Ishwar", "Prakash", ""]]}, {"id": "1610.04032", "submitter": "Francois Fleuret", "authors": "Fran\\c{c}ois Fleuret", "title": "Predicting the dynamics of 2d objects with a deep residual network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate how a residual network can learn to predict the dynamics of\ninteracting shapes purely as an image-to-image regression task.\n  With a simple 2d physics simulator, we generate short sequences composed of\nrectangles put in motion by applying a pulling force at a point picked at\nrandom. The network is trained with a quadratic loss to predict the image of\nthe resulting configuration, given the image of the starting configuration and\nan image indicating the point of grasping.\n  Experiments show that the network learns to predict accurately the resulting\nimage, which implies in particular that (1) it segments rectangles as distinct\ncomponents, (2) it infers which one contains the grasping point, (3) it models\nproperly the dynamic of a single rectangle, including the torque, (4) it\ndetects and handles collisions to some extent, and (5) it re-synthesizes\nproperly the entire scene with displaced rectangles.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 11:27:07 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 11:12:52 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1610.04057", "submitter": "Baotian Hu", "authors": "Baotian Hu, Xin Liu, Xiangping Wu, Qingcai Chen", "title": "Stroke Sequence-Dependent Deep Convolutional Neural Network for Online\n  Handwritten Chinese Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel model, named Stroke Sequence-dependent Deep\nConvolutional Neural Network (SSDCNN), using the stroke sequence information\nand eight-directional features for Online Handwritten Chinese Character\nRecognition (OLHCCR). On one hand, SSDCNN can learn the representation of\nOnline Handwritten Chinese Character (OLHCC) by incorporating the natural\nsequence information of the strokes. On the other hand, SSDCNN can incorporate\neight-directional features in a natural way. In order to train SSDCNN, we\ndivide the process of training into two stages: 1) The training data is used to\npre-train the whole architecture until the performance tends to converge. 2)\nFully-connected neural network which is used to combine the stroke\nsequence-dependent representation with eight-directional features and softmax\nlayer are further trained. Experiments were conducted on the OLHCCR competition\ntasks of ICDAR 2013. Results show that, SSDCNN can reduce the recognition error\nby 50\\% (5.13\\% vs 2.56\\%) compared to the model which only use\neight-directional features. The proposed SSDCNN achieves 97.44\\% accuracy which\nreduces the recognition error by about 1.9\\% compared with the best submitted\nsystem on ICDAR2013 competition. These results indicate that SSDCNN can exploit\nthe stroke sequence information to learn high-quality representation of OLHCC.\nIt also shows that the learnt representation and the classical\neight-directional features complement each other within the SSDCNN\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 12:54:37 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hu", "Baotian", ""], ["Liu", "Xin", ""], ["Wu", "Xiangping", ""], ["Chen", "Qingcai", ""]]}, {"id": "1610.04062", "submitter": "Dong Zhang", "authors": "Amir Mazaheri, Dong Zhang, Mubarak Shah", "title": "Video Fill in the Blank with Merging LSTMs", "comments": "for Large Scale Movie Description and Understanding Challenge (LSMDC)\n  2016, \"Movie fill-in-the-blank\" Challenge, UCF_CRCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video and its incomplete textural description with missing words, the\nVideo-Fill-in-the-Blank (ViFitB) task is to automatically find the missing\nword. The contextual information of the sentences are important to infer the\nmissing words; the visual cues are even more crucial to get a more accurate\ninference. In this paper, we presents a new method which intuitively takes\nadvantage of the structure of the sentences and employs merging LSTMs (to merge\ntwo LSTMs) to tackle the problem with embedded textural and visual cues. In the\nexperiments, we have demonstrated the superior performance of the proposed\nmethod on the challenging \"Movie Fill-in-the-Blank\" dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 13:05:41 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mazaheri", "Amir", ""], ["Zhang", "Dong", ""], ["Shah", "Mubarak", ""]]}, {"id": "1610.04079", "submitter": "Albert Vilamala", "authors": "Albert Vilamala, Kristoffer Hougaard Madsen and Lars Kai Hansen", "title": "Towards end-to-end optimisation of functional image analysis pipelines", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of neurocognitive tasks requiring accurate localisation of activity\noften rely on functional Magnetic Resonance Imaging, a widely adopted technique\nthat makes use of a pipeline of data processing modules, each involving a\nvariety of parameters. These parameters are frequently set according to the\nlocal goal of each specific module, not accounting for the rest of the\npipeline. Given recent success of neural network research in many different\ndomains, we propose to convert the whole data pipeline into a deep neural\nnetwork, where the parameters involved are jointly optimised by the network to\nbest serve a common global goal. As a proof of concept, we develop a module\nable to adaptively apply the most suitable spatial smoothing to every brain\nvolume for each specific neuroimaging task, and we validate its results in a\nstandard brain decoding experiment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 13:57:55 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Vilamala", "Albert", ""], ["Madsen", "Kristoffer Hougaard", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1610.04097", "submitter": "Anant Vemuri", "authors": "Anant S. Vemuri, Stephane A. Nicolau, Jacques Marescaux, Luc Soler,\n  Nicholas Ayache", "title": "Automatic View-Point Selection for Inter-Operative Endoscopic\n  Surveillance", "comments": "Medical Content-based Retrieval for Clinical Decision Support and\n  Treatment Planning, MICCAI Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Esophageal adenocarcinoma arises from Barrett's esophagus, which is the most\nserious complication of gastroesophageal reflux disease. Strategies for\nscreening involve periodic surveillance and tissue biopsies. A major challenge\nin such regular examinations is to record and track the disease evolution and\nre-localization of biopsied sites to provide targeted treatments. In this\npaper, we extend our original inter-operative relocalization framework to\nprovide a constrained image based search for obtaining the best view-point\nmatch to the live view. Within this context we investigate the effect of: the\nchoice of feature descriptors and color-space; filtering of uninformative\nframes and endoscopic modality, for view-point localization. Our experiments\nindicate an improvement in the best view-point retrieval rate to [92%,87%] from\n[73%,76%] (in our previous approach) for NBI and WL.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 14:24:33 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Vemuri", "Anant S.", ""], ["Nicolau", "Stephane A.", ""], ["Marescaux", "Jacques", ""], ["Soler", "Luc", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1610.04121", "submitter": "Daniel Hernandez-Juarez", "authors": "Daniel Hernandez-Juarez and Alejandro Chac\\'on and Antonio Espinosa\n  and David V\\'azquez and Juan Carlos Moure and Antonio Manuel L\\'opez", "title": "Embedded real-time stereo estimation via Semi-Global Matching on the GPU", "comments": null, "journal-ref": null, "doi": "10.1016/j.procs.2016.05.305", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense, robust and real-time computation of depth information from\nstereo-camera systems is a computationally demanding requirement for robotics,\nadvanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global\nMatching (SGM) is a widely used algorithm that propagates consistency\nconstraints along several paths across the image. This work presents a\nreal-time system producing reliable disparity estimation results on the new\nembedded energy-efficient GPU devices. Our design runs on a Tegra X1 at 42\nframes per second (fps) for an image size of 640x480, 128 disparity levels, and\nusing 4 path directions for the SGM method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 15:15:11 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hernandez-Juarez", "Daniel", ""], ["Chac\u00f3n", "Alejandro", ""], ["Espinosa", "Antonio", ""], ["V\u00e1zquez", "David", ""], ["Moure", "Juan Carlos", ""], ["L\u00f3pez", "Antonio Manuel", ""]]}, {"id": "1610.04124", "submitter": "Daniel Hernandez-Juarez", "authors": "Daniel Hernandez-Juarez and Antonio Espinosa and David V\\'azquez and\n  Antonio Manuel L\\'opez and Juan Carlos Moure", "title": "GPU-accelerated real-time stixel computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stixel World is a medium-level, compact representation of road scenes\nthat abstracts millions of disparity pixels into hundreds or thousands of\nstixels. The goal of this work is to implement and evaluate a complete\nmulti-stixel estimation pipeline on an embedded, energy-efficient,\nGPU-accelerated device. This work presents a full GPU-accelerated\nimplementation of stixel estimation that produces reliable results at 26 frames\nper second (real-time) on the Tegra X1 for disparity images of 1024x440 pixels\nand stixel widths of 5 pixels, and achieves more than 400 frames per second on\na high-end Titan X GPU card.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 15:20:40 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hernandez-Juarez", "Daniel", ""], ["Espinosa", "Antonio", ""], ["V\u00e1zquez", "David", ""], ["L\u00f3pez", "Antonio Manuel", ""], ["Moure", "Juan Carlos", ""]]}, {"id": "1610.04156", "submitter": "Vladimir Saveljev", "authors": "Vladimir Saveljev and Irina Palchikova", "title": "Theory and computer simulation of the moir\\'e patterns in single-layer\n  cylindrical particles", "comments": "8 pages, 14 figures, 45 equations; first written on July 3, 2016,\n  last modified on October 12, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basing on the theory for arbitrary oriented surfaces, we developed the theory\nof the moir\\'e effect for cylindrical single-layer objects in the paraxial\napproximation. With using the dual grids, the moir\\'e effect in the plane\ngratings is simulated, as well as the near-axis moir\\'e effect in cylinders\nincluding the chiral layouts. The results can be applied to the graphene\nlayers, to single-walled nanotubes, and to cylinders in general.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 08:09:34 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Saveljev", "Vladimir", ""], ["Palchikova", "Irina", ""]]}, {"id": "1610.04256", "submitter": "Abigail Graese", "authors": "Abigail Graese, Andras Rozsa, Terrance E. Boult", "title": "Assessing Threat of Adversarial Examples on Deep Neural Networks", "comments": "This is a pre-print version to appear in IEEE ICMLA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are facing a potential security threat from adversarial\nexamples, inputs that look normal but cause an incorrect classification by the\ndeep neural network. For example, the proposed threat could result in\nhand-written digits on a scanned check being incorrectly classified but looking\nnormal when humans see them. This research assesses the extent to which\nadversarial examples pose a security threat, when one considers the normal\nimage acquisition process. This process is mimicked by simulating the\ntransformations that normally occur in acquiring the image in a real world\napplication, such as using a scanner to acquire digits for a check amount or\nusing a camera in an autonomous car. These small transformations negate the\neffect of the carefully crafted perturbations of adversarial examples,\nresulting in a correct classification by the deep neural network. Thus just\nacquiring the image decreases the potential impact of the proposed security\nthreat. We also show that the already widely used process of averaging over\nmultiple crops neutralizes most adversarial examples. Normal preprocessing,\nsuch as text binarization, almost completely neutralizes adversarial examples.\nThis is the first paper to show that for text driven classification,\nadversarial examples are an academic curiosity, not a security threat.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 20:34:48 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Graese", "Abigail", ""], ["Rozsa", "Andras", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1610.04261", "submitter": "Canlin Zhou", "authors": "Guangliang Du, Minmin Wang, Canlin Zhou, Shuchun Si, Hui Li, Zhenkun\n  Lei and Yanjie Li", "title": "Improved phase-unwrapping method using geometric constraints", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": "10.1080/09500340.2017.1284279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional dual-frequency fringe projection algorithm often suffers from\nphase unwrapping failure when the frequency ratio between the high frequency\nand the low one is too large. Zhang et.al. proposed an enhanced two-frequency\nphase-shifting method to use geometric constraints of digital fringe\nprojection(DFP) to reduce the noise impact due to the large frequency ratio.\nHowever, this method needs to calibrate the DFP system and calculate the\nminimum phase map at the nearest position from the camera perspective, these\nprocedures are are relatively complex and more time-cosuming. In this paper, we\nproposed an improved method, which eliminates the system calibration and\ndetermination in Zhang's method,meanwhile does not need to use the low\nfrequency fringe pattern. In the proposed method,we only need a set of high\nfrequency fringe patterns to measure the object after the high frequency is\ndirectly estimated by the experiment. Thus the proposed method can simplify the\nprocedure and improve the speed. Finally, the experimental evaluation is\nconducted to prove the validity of the proposed method.The results demonstrate\nthat the proposed method can overcome the main disadvantages encountered by\nZhang's method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 01:52:13 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Du", "Guangliang", ""], ["Wang", "Minmin", ""], ["Zhou", "Canlin", ""], ["Si", "Shuchun", ""], ["Li", "Hui", ""], ["Lei", "Zhenkun", ""], ["Li", "Yanjie", ""]]}, {"id": "1610.04308", "submitter": "Min Liu", "authors": "Min Liu, Yifei Shi, Lintao Zheng, Kai Xu, Hui Huang and Dinesh Manocha", "title": "Recurrent 3D Attentional Networks for End-to-End Active Object\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active vision is inherently attention-driven: The agent actively selects\nviews to attend in order to fast achieve the vision task while improving its\ninternal representation of the scene being observed. Inspired by the recent\nsuccess of attention-based models in 2D vision tasks based on single RGB\nimages, we propose to address the multi-view depth-based active object\nrecognition using attention mechanism, through developing an end-to-end\nrecurrent 3D attentional network. The architecture takes advantage of a\nrecurrent neural network (RNN) to store and update an internal representation.\nOur model, trained with 3D shape datasets, is able to iteratively attend to the\nbest views targeting an object of interest for recognizing it. To realize 3D\nview selection, we derive a 3D spatial transformer network which is\ndifferentiable for training with backpropagation, achieving much faster\nconvergence than the reinforcement learning employed by most existing\nattention-based models. Experiments show that our method, with only depth\ninput, achieves state-of-the-art next-best-view performance in time efficiency\nand recognition accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 01:25:09 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 17:29:40 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 21:00:53 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Liu", "Min", ""], ["Shi", "Yifei", ""], ["Zheng", "Lintao", ""], ["Xu", "Kai", ""], ["Huang", "Hui", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1610.04322", "submitter": "Wei Li", "authors": "Wei Li, Zhigang Zhu", "title": "Learning and Fusing Multimodal Features from and for Multi-task Facial\n  Computing", "comments": "An experiment to feature fusion in deep learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning-based feature fusion approach for facial computing\nincluding face recognition as well as gender, race and age detection. Instead\nof training a single classifier on face images to classify them based on the\nfeatures of the person whose face appears in the image, we first train four\ndifferent classifiers for classifying face images based on race, age, gender\nand identification (ID). Multi-task features are then extracted from the\ntrained models and cross-task-feature training is conducted which shows the\nvalue of fusing multimodal features extracted from multi-tasks. We have found\nthat features trained for one task can be used for other related tasks. More\ninterestingly, the features trained for a task with more classes (e.g. ID) and\nthen used in another task with fewer classes (e.g. race) outperforms the\nfeatures trained for the other task itself. The final feature fusion is\nperformed by combining the four types of features extracted from the images by\nthe four classifiers. The feature fusion approach improves the classifications\naccuracy by a 7.2%, 20.1%, 22.2%, 21.8% margin, respectively, for ID, age, race\nand gender recognition, over the results of single classifiers trained only on\ntheir individual features. The proposed method can be applied to applications\nin which different types of data or features can be extracted.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 04:17:13 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Li", "Wei", ""], ["Zhu", "Zhigang", ""]]}, {"id": "1610.04325", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha,\n  Byoung-Tak Zhang", "title": "Hadamard Product for Low-rank Bilinear Pooling", "comments": "13 pages, 1 figure, & appendix. ICLR 2017 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear models provide rich representations compared with linear models.\nThey have been applied in various visual tasks, such as object recognition,\nsegmentation, and visual question-answering, to get state-of-the-art\nperformances taking advantage of the expanded representations. However,\nbilinear representations tend to be high-dimensional, limiting the\napplicability to computationally complex tasks. We propose low-rank bilinear\npooling using Hadamard product for an efficient attention mechanism of\nmultimodal learning. We show that our model outperforms compact bilinear\npooling in visual question-answering tasks with the state-of-the-art results on\nthe VQA dataset, having a better parsimonious property.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 04:29:52 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 05:31:27 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 05:22:01 GMT"}, {"version": "v4", "created": "Sun, 26 Mar 2017 16:22:47 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["On", "Kyoung-Woon", ""], ["Lim", "Woosang", ""], ["Kim", "Jeonghee", ""], ["Ha", "Jung-Woo", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1610.04460", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and David Schultz", "title": "On the Existence of a Sample Mean in Dynamic Time Warping Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of sample mean in dynamic time warping (DTW) spaces has been\nsuccessfully applied to improve pattern recognition systems and generalize\ncentroid-based clustering algorithms. Its existence has neither been proved nor\nchallenged. This article presents sufficient conditions for existence of a\nsample mean in DTW spaces. The proposed result justifies prior work on\napproximate mean algorithms, sets the stage for constructing exact mean\nalgorithms, and is a first step towards a statistical theory of DTW spaces.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 13:42:47 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 13:00:08 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 08:57:17 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Schultz", "David", ""]]}, {"id": "1610.04490", "submitter": "Casper Kaae S{\\o}nderby", "authors": "Casper Kaae S{\\o}nderby, Jose Caballero, Lucas Theis, Wenzhe Shi,\n  Ferenc Husz\\'ar", "title": "Amortised MAP Inference for Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution (SR) is an underdetermined inverse problem, where a\nlarge number of plausible high-resolution images can explain the same\ndownsampled image. Most current single image SR methods use empirical risk\nminimisation, often with a pixel-wise mean squared error (MSE) loss. However,\nthe outputs from such methods tend to be blurry, over-smoothed and generally\nappear implausible. A more desirable approach would employ Maximum a Posteriori\n(MAP) inference, preferring solutions that always have a high probability under\nthe image prior, and thus appear more plausible. Direct MAP estimation for SR\nis non-trivial, as it requires us to build a model for the image prior from\nsamples. Furthermore, MAP inference is often performed via optimisation-based\niterative algorithms which don't compare well with the efficiency of\nneural-network-based alternatives. Here we introduce new methods for amortised\nMAP inference whereby we calculate the MAP estimate directly using a\nconvolutional neural network. We first introduce a novel neural network\narchitecture that performs a projection to the affine subspace of valid SR\nsolutions ensuring that the high resolution output of the network is always\nconsistent with the low resolution input. We show that, using this\narchitecture, the amortised MAP inference problem reduces to minimising the\ncross-entropy between two distributions, similar to training generative models.\nWe propose three methods to solve this optimisation problem: (1) Generative\nAdversarial Networks (GAN) (2) denoiser-guided SR which backpropagates\ngradient-estimates from denoising to train the network, and (3) a baseline\nmethod using a maximum-likelihood-trained image prior. Our experiments show\nthat the GAN based approach performs best on real image data. Lastly, we\nestablish a connection between GANs and amortised variational inference as in\ne.g. variational autoencoders.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 14:58:44 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 14:56:42 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 13:08:24 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["S\u00f8nderby", "Casper Kaae", ""], ["Caballero", "Jose", ""], ["Theis", "Lucas", ""], ["Shi", "Wenzhe", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1610.04531", "submitter": "Hamid Laga", "authors": "Hamid Laga, Qian Xie, Ian H. Jermyn, and Anuj Srivastava", "title": "Numerical Inversion of SRNF Maps for Elastic Shape Analysis of\n  Genus-Zero Surfaces", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2017", "doi": "10.1109/TPAMI.2016.2647596", "report-no": "Volume: 39 , Issue: 12", "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in elastic shape analysis (ESA) are motivated by the fact\nthat it provides comprehensive frameworks for simultaneous registration,\ndeformation, and comparison of shapes. These methods achieve computational\nefficiency using certain square-root representations that transform invariant\nelastic metrics into Euclidean metrics, allowing for applications of standard\nalgorithms and statistical tools. For analyzing shapes of embeddings of\n$\\mathbb{S}^2$ in $\\mathbb{R}^3$, Jermyn et al. introduced square-root normal\nfields (SRNFs) that transformed an elastic metric, with desirable invariant\nproperties, into the $\\mathbb{L}^2$ metric. These SRNFs are essentially surface\nnormals scaled by square-roots of infinitesimal area elements. A critical need\nin shape analysis is to invert solutions (deformations, averages, modes of\nvariations, etc) computed in the SRNF space, back to the original surface space\nfor visualizations and inferences. Due to the lack of theory for understanding\nSRNFs maps and their inverses, we take a numerical approach and derive an\nefficient multiresolution algorithm, based on solving an optimization problem\nin the surface space, that estimates surfaces corresponding to given SRNFs.\nThis solution is found effective, even for complex shapes, e.g. human bodies\nand animals, that undergo significant deformations including bending and\nstretching. Specifically, we use this inversion for computing elastic shape\ndeformations, transferring deformations, summarizing shapes, and for finding\nmodes of variability in a given collection, while simultaneously registering\nthe surfaces. We demonstrate the proposed algorithms using a statistical\nanalysis of human body shapes, classification of generic surfaces and analysis\nof brain structures.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 16:56:49 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Laga", "Hamid", ""], ["Xie", "Qian", ""], ["Jermyn", "Ian H.", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1610.04542", "submitter": "Yicong Tian", "authors": "Yicong Tian, Mubarak Shah", "title": "On Duality Of Multiple Target Tracking and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, object tracking and segmentation are treated as two separate\nproblems and solved independently. However, in this paper, we argue that\ntracking and segmentation are actually closely related and solving one should\nhelp the other. On one hand, the object track, which is a set of bounding boxes\nwith one bounding box in every frame, would provide strong high-level guidance\nfor the target/background segmentation task. On the other hand, the object\nsegmentation would separate object from other objects and background, which\nwill be useful for determining track locations in every frame. We propose a\nnovel framework which combines online multiple target tracking and segmentation\nin a video. In our approach, the tracking and segmentation problems are coupled\nby Lagrange dual decomposition, which leads to more accurate segmentation\nresults and also \\emph{helps resolve typical difficulties in multiple target\ntracking, such as occlusion handling, ID-switch and track drifting}. To track\ntargets, an individual appearance model is learned for each target via\nstructured learning and network flow is employed to generate tracks from\ndensely sampled candidates. For segmentation, multi-label Conditional Random\nField (CRF) is applied to a superpixel based spatio-temporal graph in a segment\nof video to assign background or target labels to every superpixel. The\nexperiments on diverse sequences show that our method outperforms\nstate-of-the-art approaches for multiple target tracking as well as\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 17:18:46 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Tian", "Yicong", ""], ["Shah", "Mubarak", ""]]}, {"id": "1610.04563", "submitter": "Manuel G\\\"unther", "authors": "Andras Rozsa, Manuel G\\\"unther, and Terrance E. Boult", "title": "Are Accuracy and Robustness Correlated?", "comments": "Accepted for publication at ICMLA 2016", "journal-ref": null, "doi": "10.1109/ICMLA.2016.0045", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial examples formed by\napplying small carefully chosen perturbations to inputs that cause unexpected\nclassification errors. In this paper, we perform experiments on various\nadversarial example generation approaches with multiple deep convolutional\nneural networks including Residual Networks, the best performing models on\nImageNet Large-Scale Visual Recognition Challenge 2015. We compare the\nadversarial example generation techniques with respect to the quality of the\nproduced images, and measure the robustness of the tested machine learning\nmodels to adversarial examples. Finally, we conduct large-scale experiments on\ncross-model adversarial portability. We find that adversarial examples are\nmostly transferable across similar network topologies, and we demonstrate that\nbetter machine learning models are less vulnerable to adversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:10:04 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 00:54:14 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Rozsa", "Andras", ""], ["G\u00fcnther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1610.04574", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues", "title": "Generalization Error of Invariant Classifiers", "comments": "Accepted to AISTATS. This version has updated references", "journal-ref": "Conference on Artificial Intelligence and Statistics (AISTATS),\n  2017, pp. 1094-1103", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:40:52 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 11:32:03 GMT"}, {"version": "v3", "created": "Sun, 2 Jul 2017 18:58:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sokolic", "Jure", ""], ["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1610.04575", "submitter": "Jyothi Korra", "authors": "Jyothi Korra", "title": "Comparing Face Detection and Recognition Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper implements and compares different techniques for face detection\nand recognition. One is find where the face is located in the images that is\nface detection and second is face recognition that is identifying the person.\nWe study three techniques in this paper: Face detection using self organizing\nmap (SOM), Face recognition by projection and nearest neighbor and Face\nrecognition using SVM.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 03:16:14 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Korra", "Jyothi", ""]]}, {"id": "1610.04579", "submitter": "Andrew Leifer", "authors": "Jeffrey P. Nguyen, Ashley N. Linder, George S. Plummer, Joshua W.\n  Shaevitz, and Andrew M. Leifer", "title": "Automatically tracking neurons in a moving and deforming brain", "comments": "33 pages, 7 figures, code available", "journal-ref": "PLoS Comput Biol 13(5): e1005517 (2017)", "doi": "10.1371/journal.pcbi.1005517", "report-no": null, "categories": "q-bio.NC cs.CV physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in optical neuroimaging techniques now allow neural activity to be\nrecorded with cellular resolution in awake and behaving animals. Brain motion\nin these recordings pose a unique challenge. The location of individual neurons\nmust be tracked in 3D over time to accurately extract single neuron activity\ntraces. Recordings from small invertebrates like C. elegans are especially\nchallenging because they undergo very large brain motion and deformation during\nanimal movement. Here we present an automated computer vision pipeline to\nreliably track populations of neurons with single neuron resolution in the\nbrain of a freely moving C. elegans undergoing large motion and deformation. 3D\nvolumetric fluorescent images of the animal's brain are straightened, aligned\nand registered, and the locations of neurons in the images are found via\nsegmentation. Each neuron is then assigned an identity using a new\ntime-independent machine-learning approach we call Neuron Registration Vector\nEncoding. In this approach, non-rigid point-set registration is used to match\neach segmented neuron in each volume with a set of reference volumes taken from\nthroughout the recording. The way each neuron matches with the references\ndefines a feature vector which is clustered to assign an identity to each\nneuron in each volume. Finally, thin-plate spline interpolation is used to\ncorrect errors in segmentation and check consistency of assigned identities.\nThe Neuron Registration Vector Encoding approach proposed here is uniquely well\nsuited for tracking neurons in brains undergoing large deformations. When\napplied to whole-brain calcium imaging recordings in freely moving C. elegans,\nthis analysis pipeline located 150 neurons for the duration of an 8 minute\nrecording and consistently found more neurons more quickly than manual or\nsemi-automated approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:51:30 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Nguyen", "Jeffrey P.", ""], ["Linder", "Ashley N.", ""], ["Plummer", "George S.", ""], ["Shaevitz", "Joshua W.", ""], ["Leifer", "Andrew M.", ""]]}, {"id": "1610.04583", "submitter": "Alexander Wein", "authors": "Amelia Perry, Alexander S. Wein, Afonso S. Bandeira, Ankur Moitra", "title": "Message-passing algorithms for synchronization problems over compact\n  groups", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": "10.1002/cpa.21750", "report-no": null, "categories": "cs.IT cs.CV cs.DS math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various alignment problems arising in cryo-electron microscopy, community\ndetection, time synchronization, computer vision, and other fields fall into a\ncommon framework of synchronization problems over compact groups such as Z/L,\nU(1), or SO(3). The goal of such problems is to estimate an unknown vector of\ngroup elements given noisy relative observations. We present an efficient\niterative algorithm to solve a large class of these problems, allowing for any\ncompact group, with measurements on multiple 'frequency channels' (Fourier\nmodes, or more generally, irreducible representations of the group). Our\nalgorithm is a highly efficient iterative method following the blueprint of\napproximate message passing (AMP), which has recently arisen as a central\ntechnique for inference problems such as structured low-rank estimation and\ncompressed sensing. We augment the standard ideas of AMP with ideas from\nrepresentation theory so that the algorithm can work with distributions over\ncompact groups. Using standard but non-rigorous methods from statistical\nphysics we analyze the behavior of our algorithm on a Gaussian noise model,\nidentifying phases where the problem is easy, (computationally) hard, and\n(statistically) impossible. In particular, such evidence predicts that our\nalgorithm is information-theoretically optimal in many cases, and that the\nremaining cases show evidence of statistical-to-computational gaps.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 19:05:32 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""], ["Moitra", "Ankur", ""]]}, {"id": "1610.04631", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Feiping Nie, Chris Ding, Heng Huang", "title": "A Harmonic Mean Linear Discriminant Analysis for Robust Image\n  Classification", "comments": "IEEE 28th International Conference on Tools with Artificial\n  Intelligence, ICTAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality\nreduction method in computer vision and pattern recognition. In null space\nbased LDA (NLDA), a well-known LDA extension, between-class distance is\nmaximized in the null space of the within-class scatter matrix. However, there\nare some limitations in NLDA. Firstly, for many data sets, null space of\nwithin-class scatter matrix does not exist, thus NLDA is not applicable to\nthose datasets. Secondly, NLDA uses arithmetic mean of between-class distances\nand gives equal consideration to all between-class distances, which makes\nlarger between-class distances can dominate the result and thus limits the\nperformance of NLDA. In this paper, we propose a harmonic mean based Linear\nDiscriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image\nclassification, which minimizes the reciprocal of weighted harmonic mean of\npairwise between-class distance. More importantly, MCDA gives higher priority\nto maximize small between-class distances. MCDA can be extended to multi-label\ndimension reduction. Results on 7 single-label data sets and 4 multi-label data\nsets show that MCDA has consistently better performance than 10 other\nsingle-label approaches and 4 other multi-label approaches in terms of\nclassification accuracy, macro and micro average F1 score.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 20:36:57 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 16:38:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Zheng", "Shuai", ""], ["Nie", "Feiping", ""], ["Ding", "Chris", ""], ["Huang", "Heng", ""]]}, {"id": "1610.04662", "submitter": "Noel Codella", "authors": "Noel Codella, Quoc-Bao Nguyen, Sharath Pankanti, David Gutman, Brian\n  Helba, Allan Halpern, John R. Smith", "title": "Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images", "comments": "URL for the IBM Journal of Research and Development:\n  http://www.research.ibm.com/journal/", "journal-ref": "IBM Journal of Research and Development, vol. 61, no. 4/5, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is the deadliest form of skin cancer. While curable with early\ndetection, only highly trained specialists are capable of accurately\nrecognizing the disease. As expertise is in limited supply, automated systems\ncapable of identifying disease could save lives, reduce unnecessary biopsies,\nand reduce costs. Toward this goal, we propose a system that combines recent\ndevelopments in deep learning with established machine learning approaches,\ncreating ensembles of methods that are capable of segmenting skin lesions, as\nwell as analyzing the detected area and surrounding tissue for melanoma\ndetection. The system is evaluated using the largest publicly available\nbenchmark dataset of dermoscopic images, containing 900 training and 379\ntesting images. New state-of-the-art performance levels are demonstrated,\nleading to an improvement in the area under receiver operating characteristic\ncurve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624),\nand in specificity measured at the clinically relevant 95% sensitivity\noperating point 2.9 times higher than the previous state-of-the-art (36.8%\nspecificity compared to 12.5%). Compared to the average of 8 expert\ndermatologists on a subset of 100 test images, the proposed system produces a\nhigher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an\nequivalent sensitivity (82%).\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 22:31:34 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 00:25:35 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Codella", "Noel", ""], ["Nguyen", "Quoc-Bao", ""], ["Pankanti", "Sharath", ""], ["Gutman", "David", ""], ["Helba", "Brian", ""], ["Halpern", "Allan", ""], ["Smith", "John R.", ""]]}, {"id": "1610.04673", "submitter": "Sheng Xu Sheng Xu", "authors": "Sheng Xu, Ruisheng Wang, Han Zheng", "title": "Road Curb Extraction from Mobile LiDAR Point Clouds", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Volume:PP ,\n  Issue: 99, 2016", "doi": "10.1109/TGRS.2016.2617819", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of road curbs from uneven, unorganized, noisy and\nmassive 3D point clouds is a challenging task. Existing methods often project\n3D point clouds onto 2D planes to extract curbs. However, the projection causes\nloss of 3D information which degrades the performance of the detection. This\npaper presents a robust, accurate and efficient method to extract road curbs\nfrom 3D mobile LiDAR point clouds. Our method consists of two steps: 1)\nextracting the candidate points of curbs based on the proposed novel energy\nfunction and 2) refining the candidate points using the proposed least cost\npath model. We evaluated our method on a large-scale of residential area\n(16.7GB, 300 million points) and an urban area (1.07GB, 20 million points)\nmobile LiDAR point clouds. Results indicate that the proposed method is\nsuperior to the state-of-the-art methods in terms of robustness, accuracy and\nefficiency. The proposed curb extraction method achieved a completeness of\n78.62% and a correctness of 83.29%. These experiments demonstrate that the\nproposed method is a promising solution to extract road curbs from mobile LiDAR\npoint clouds.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 00:18:59 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 16:19:31 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Xu", "Sheng", ""], ["Wang", "Ruisheng", ""], ["Zheng", "Han", ""]]}, {"id": "1610.04725", "submitter": "Takoua Kefi", "authors": "Takoua Kefi, Riadh Ksantini, M.Becha Kaaniche and Adel Bouhoula", "title": "Incremental One-Class Models for Data Classification", "comments": "4 pages, accepted in PhD Forum Session of the ECML-PKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we outline a PhD research plan. This research contributes to\nthe field of one-class incremental learning and classification in case of\nnon-stationary environments. The goal of this PhD is to define a new\nclassification framework able to deal with very small learning dataset at the\nbeginning of the process and with abilities to adjust itself according to the\nvariability of the incoming data which create large scale datasets. As a\npreliminary work, incremental Covariance-guided One-Class Support Vector\nMachine is proposed to deal with sequentially obtained data. It is inspired\nfrom COSVM which put more emphasis on the low variance directions while keeping\nthe basic formulation of incremental One-Class Support Vector Machine\nuntouched. The incremental procedure is introduced by controlling the possible\nchanges of support vectors after the addition of new data points, thanks to the\nKarush-Kuhn-Tucker conditions, that have to be maintained on all previously\nacquired data. Comparative experimental results with contemporary incremental\nand non-incremental one-class classifiers on numerous artificial and real data\nsets show that our method results in significantly better classification\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 12:06:12 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Kefi", "Takoua", ""], ["Ksantini", "Riadh", ""], ["Kaaniche", "M. Becha", ""], ["Bouhoula", "Adel", ""]]}, {"id": "1610.04787", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Makarand Tapaswi, Rainer Stiefelhagen", "title": "Recovering the Missing Link: Predicting Class-Attribute Associations for\n  Unsupervised Zero-Shot Learning", "comments": "Published as a conference paper at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting training images for all visual categories is not only expensive\nbut also impractical. Zero-shot learning (ZSL), especially using attributes,\noffers a pragmatic solution to this problem. However, at test time most\nattribute-based methods require a full description of attribute associations\nfor each unseen class. Providing these associations is time consuming and often\nrequires domain specific knowledge. In this work, we aim to carry out\nattribute-based zero-shot classification in an unsupervised manner. We propose\nan approach to learn relations that couples class embeddings with their\ncorresponding attributes. Given only the name of an unseen class, the learned\nrelationship model is used to automatically predict the class-attribute\nassociations. Furthermore, our model facilitates transferring attributes across\ndata sets without additional effort. Integrating knowledge from multiple\nsources results in a significant additional improvement in performance. We\nevaluate on two public data sets: Animals with Attributes and aPascal/aYahoo.\nOur approach outperforms state-of-the-art methods in both predicting\nclass-attribute associations and unsupervised ZSL by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 21:02:39 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Tapaswi", "Makarand", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1610.04805", "submitter": "Archith Bency", "authors": "Archith J. Bency, Swati Rallapalli, Raghu K. Ganti, Mudhakar Srivatsa\n  and B. S. Manjunath", "title": "Beyond Spatial Auto-Regressive Models: Predicting Housing Prices with\n  Satellite Imagery", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling geo-spatial data, it is critical to capture spatial\ncorrelations for achieving high accuracy. Spatial Auto-Regression (SAR) is a\ncommon tool used to model such data, where the spatial contiguity matrix (W)\nencodes the spatial correlations. However, the efficacy of SAR is limited by\ntwo factors. First, it depends on the choice of contiguity matrix, which is\ntypically not learnt from data, but instead, is assumed to be known apriori.\nSecond, it assumes that the observations can be explained by linear models. In\nthis paper, we propose a Convolutional Neural Network (CNN) framework to model\ngeo-spatial data (specifi- cally housing prices), to learn the spatial\ncorrelations automatically. We show that neighborhood information embedded in\nsatellite imagery can be leveraged to achieve the desired spatial smoothing. An\nadditional upside of our framework is the relaxation of linear assumption on\nthe data. Specific challenges we tackle while implementing our framework\ninclude, (i) how much of the neighborhood is relevant while estimating housing\nprices? (ii) what is the right approach to capture multiple resolutions of\nsatellite imagery? and (iii) what other data-sources can help improve the\nestimation of spatial correlations? We demonstrate a marked improvement of 57%\non top of the SAR baseline through the use of features from deep neural\nnetworks for the cities of London, Birmingham and Liverpool.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 01:17:19 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Bency", "Archith J.", ""], ["Rallapalli", "Swati", ""], ["Ganti", "Raghu K.", ""], ["Srivatsa", "Mudhakar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1610.04823", "submitter": "Joel Brogan Joel R Brogan", "authors": "Sandipan Banerjee, Joel Brogan, Janez Krizaj, Aparna Bharati, Brandon\n  RichardWebster, Vitomir Struc, Patrick Flynn and Walter Scheirer", "title": "To Frontalize or Not To Frontalize: Do We Really Need Elaborate\n  Pre-processing To Improve Face Recognition?", "comments": "Accepted to WACV 2018 - Fixed title to correct working version Code\n  available here:\n  https://github.com/joelb92/ND_Frontalization_Project/tree/master/Release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition performance has improved remarkably in the last decade. Much\nof this success can be attributed to the development of deep learning\ntechniques such as convolutional neural networks (CNNs). While CNNs have pushed\nthe state-of-the-art forward, their training process requires a large amount of\nclean and correctly labelled training data. If a CNN is intended to tolerate\nfacial pose, then we face an important question: should this training data be\ndiverse in its pose distribution, or should face images be normalized to a\nsingle pose in a pre-processing step? To address this question, we evaluate a\nnumber of popular facial landmarking and pose correction algorithms to\nunderstand their effect on facial recognition performance. Additionally, we\nintroduce a new, automatic, single-image frontalization scheme that exceeds the\nperformance of current algorithms. CNNs trained using sets of different\npre-processing methods are used to extract features from the Point and Shoot\nChallenge (PaSC) and CMU Multi-PIE datasets. We assert that the subsequent\nverification and recognition performance serves to quantify the effectiveness\nof each pose correction scheme.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 06:17:47 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 19:38:13 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 20:59:31 GMT"}, {"version": "v4", "created": "Tue, 27 Mar 2018 20:08:41 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Brogan", "Joel", ""], ["Krizaj", "Janez", ""], ["Bharati", "Aparna", ""], ["RichardWebster", "Brandon", ""], ["Struc", "Vitomir", ""], ["Flynn", "Patrick", ""], ["Scheirer", "Walter", ""]]}, {"id": "1610.04834", "submitter": "Mohsen Ghafoorian", "authors": "Mohsen Ghafoorian, Nico Karssemeijer, Tom Heskes, Inge van Uden, Clara\n  Sanchez, Geert Litjens, Frank-Erik de Leeuw, Bram van Ginneken, Elena\n  Marchiori and Bram Platel", "title": "Location Sensitive Deep Convolutional Neural Networks for Segmentation\n  of White Matter Hyperintensities", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The anatomical location of imaging features is of crucial importance for\naccurate diagnosis in many medical tasks. Convolutional neural networks (CNN)\nhave had huge successes in computer vision, but they lack the natural ability\nto incorporate the anatomical location in their decision making process,\nhindering success in some medical image analysis tasks.\n  In this paper, to integrate the anatomical location information into the\nnetwork, we propose several deep CNN architectures that consider multi-scale\npatches or take explicit location features while training. We apply and compare\nthe proposed architectures for segmentation of white matter hyperintensities in\nbrain MR images on a large dataset. As a result, we observe that the CNNs that\nincorporate location information substantially outperform a conventional\nsegmentation method with hand-crafted features as well as CNNs that do not\nintegrate location information. On a test set of 46 scans, the best\nconfiguration of our networks obtained a Dice score of 0.791, compared to 0.797\nfor an independent human observer. Performance levels of the machine and the\nindependent human observer were not statistically significantly different\n(p-value=0.17).\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 09:35:36 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 15:10:46 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ghafoorian", "Mohsen", ""], ["Karssemeijer", "Nico", ""], ["Heskes", "Tom", ""], ["van Uden", "Inge", ""], ["Sanchez", "Clara", ""], ["Litjens", "Geert", ""], ["de Leeuw", "Frank-Erik", ""], ["van Ginneken", "Bram", ""], ["Marchiori", "Elena", ""], ["Platel", "Bram", ""]]}, {"id": "1610.04861", "submitter": "Asad Khan Mr.", "authors": "Asad Khan, Muhammad Ahmad, Yudong Guo, Ligang Liu", "title": "Digital Makeup from Internet Images", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach of color transfer between images by exploring\ntheir high-level semantic information. First, we set up a database which\nconsists of the collection of downloaded images from the internet, which are\nsegmented automatically by using matting techniques. We then, extract image\nforegrounds from both source and multiple target images. Then by using image\nmatting algorithms, the system extracts the semantic information such as faces,\nlips, teeth, eyes, eyebrows, etc., from the extracted foregrounds of the source\nimage. And, then the color is transferred between corresponding parts with the\nsame semantic information. Next we get the color transferred result by\nseamlessly compositing different parts together using alpha blending. In the\nfinal step, we present an efficient method of color consistency to optimize the\ncolor of a collection of images showing the common scene. The main advantage of\nour method over existing techniques is that it does not need face matching, as\none could use more than one target images. It is not restricted to head shot\nimages as we can also change the color style in the wild. Moreover, our\nalgorithm does not require to choose the same color style, same pose and image\nsize between source and target images. Our algorithm is not restricted to\none-to-one image color transfer and can make use of more than one target images\nto transfer the color in different parts in the source image. Comparing with\nother approaches, our algorithm is much better in color blending in the input\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 13:47:18 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 12:59:37 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Khan", "Asad", ""], ["Ahmad", "Muhammad", ""], ["Guo", "Yudong", ""], ["Liu", "Ligang", ""]]}, {"id": "1610.04889", "submitter": "Srinath Sridhar", "authors": "Srinath Sridhar, Franziska Mueller, Michael Zollh\\\"ofer, Dan Casas,\n  Antti Oulasvirta, Christian Theobalt", "title": "Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D\n  Input", "comments": "Proceedings of ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time simultaneous tracking of hands manipulating and interacting with\nexternal objects has many potential applications in augmented reality, tangible\ncomputing, and wearable computing. However, due to difficult occlusions, fast\nmotions, and uniform hand appearance, jointly tracking hand and object pose is\nmore challenging than tracking either of the two separately. Many previous\napproaches resort to complex multi-camera setups to remedy the occlusion\nproblem and often employ expensive segmentation and optimization steps which\nmakes real-time tracking impossible. In this paper, we propose a real-time\nsolution that uses a single commodity RGB-D camera. The core of our approach is\na 3D articulated Gaussian mixture alignment strategy tailored to hand-object\ntracking that allows fast pose optimization. The alignment energy uses novel\nregularizers to address occlusions and hand-object contacts. For added\nrobustness, we guide the optimization with discriminative part classification\nof the hand and segmentation of the object. We conducted extensive experiments\non several existing datasets and introduce a new annotated hand-object dataset.\nQuantitative and qualitative results show the key advantages of our method:\nspeed, accuracy, and robustness.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 17:11:58 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sridhar", "Srinath", ""], ["Mueller", "Franziska", ""], ["Zollh\u00f6fer", "Michael", ""], ["Casas", "Dan", ""], ["Oulasvirta", "Antti", ""], ["Theobalt", "Christian", ""]]}, {"id": "1610.04936", "submitter": "Zongliang Zhang", "authors": "Zongliang Zhang, Jonathan Li, Yulan Guo, Yangbin Lin, Ming Cheng,\n  Cheng Wang", "title": "Partial Procedural Geometric Model Fitting for Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric model fitting is a fundamental task in computer graphics and\ncomputer vision. However, most geometric model fitting methods are unable to\nfit an arbitrary geometric model (e.g. a surface with holes) to incomplete\ndata, due to that the similarity metrics used in these methods are unable to\nmeasure the rigid partial similarity between arbitrary models. This paper hence\nproposes a novel rigid geometric similarity metric, which is able to measure\nboth the full similarity and the partial similarity between arbitrary geometric\nmodels. The proposed metric enables us to perform partial procedural geometric\nmodel fitting (PPGMF). The task of PPGMF is to search a procedural geometric\nmodel space for the model rigidly similar to a query of non-complete point set.\nModels in the procedural model space are generated according to a set of\nparametric modeling rules. A typical query is a point cloud. PPGMF is very\nuseful as it can be used to fit arbitrary geometric models to non-complete\n(incomplete, over-complete or hybrid-complete) point cloud data. For example,\nmost laser scanning data is non-complete due to occlusion. Our PPGMF method\nuses Markov chain Monte Carlo technique to optimize the proposed similarity\nmetric over the model space. To accelerate the optimization process, the method\nalso employs a novel coarse-to-fine model dividing strategy to reject\ndissimilar models in advance. Our method has been demonstrated on a variety of\ngeometric models and non-complete data. Experimental results show that the\nPPGMF method based on the proposed metric is able to fit non-complete data,\nwhile the method based on other metrics is unable. It is also shown that our\nmethod can be accelerated by several times via early rejection.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 00:47:36 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Zhang", "Zongliang", ""], ["Li", "Jonathan", ""], ["Guo", "Yulan", ""], ["Lin", "Yangbin", ""], ["Cheng", "Ming", ""], ["Wang", "Cheng", ""]]}, {"id": "1610.04957", "submitter": "Arnold Wiliem", "authors": "Liangchen Liu, Arnold Wiliem, Shaokang Chen, Brian C. Lovell", "title": "What is the Best Way for Extracting Meaningful Attributes from Pictures?", "comments": "Submission to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic attribute discovery methods have gained in popularity to extract\nsets of visual attributes from images or videos for various tasks. Despite\ntheir good performance in some classification tasks, it is difficult to\nevaluate whether the attributes discovered by these methods are meaningful and\nwhich methods are the most appropriate to discover attributes for visual\ndescriptions. In its simplest form, such an evaluation can be performed by\nmanually verifying whether there is any consistent identifiable visual concept\ndistinguishing between positive and negative exemplars labelled by an\nattribute. This manual checking is tedious, expensive and labour intensive. In\naddition, comparisons between different methods could also be problematic as it\nis not clear how one could quantitatively decide which attribute is more\nmeaningful than the others. In this paper, we propose a novel attribute\nmeaningfulness metric to address this challenging problem. With this metric,\nautomatic quantitative evaluation can be performed on the attribute sets; thus,\nreducing the enormous effort to perform manual evaluation. The proposed metric\nis applied to some recent automatic attribute discovery and hashing methods on\nfour attribute-labelled datasets. To further validate the efficacy of the\nproposed method, we conducted a user study. In addition, we also compared our\nmetric with a semi-supervised attribute discover method using the mixture of\nprobabilistic PCA. In our evaluation, we gleaned several insights that could be\nbeneficial in developing new automatic attribute discovery methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 02:51:43 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Liu", "Liangchen", ""], ["Wiliem", "Arnold", ""], ["Chen", "Shaokang", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1610.04973", "submitter": "Amine Ben Khalifa", "authors": "Amine Ben Khalifa and Hichem Frigui", "title": "Multiple Instance Fuzzy Inference Neural Networks", "comments": "Submitted to IEEE Transactions On Cybernetics for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy logic is a powerful tool to model knowledge uncertainty, measurements\nimprecision, and vagueness. However, there is another type of vagueness that\narises when data have multiple forms of expression that fuzzy logic does not\naddress quite well. This is the case for multiple instance learning problems\n(MIL). In MIL, an object is represented by a collection of instances, called a\nbag. A bag is labeled negative if all of its instances are negative, and\npositive if at least one of its instances is positive. Positive bags encode\nambiguity since the instances themselves are not labeled. In this paper, we\nintroduce fuzzy inference systems and neural networks designed to handle bags\nof instances as input and capable of learning from ambiguously labeled data.\nFirst, we introduce the Multiple Instance Sugeno style fuzzy inference\n(MI-Sugeno) that extends the standard Sugeno style inference to handle\nreasoning with multiple instances. Second, we use MI-Sugeno to define and\ndevelop Multiple Instance Adaptive Neuro Fuzzy Inference System (MI-ANFIS). We\nexpand the architecture of the standard ANFIS to allow reasoning with bags and\nderive a learning algorithm using backpropagation to identify the premise and\nconsequent parameters of the network. The proposed inference system is tested\nand validated using synthetic and benchmark datasets suitable for MIL problems.\nWe also apply the proposed MI-ANFIS to fuse the output of multiple\ndiscrimination algorithms for the purpose of landmine detection using Ground\nPenetrating Radar.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 05:07:09 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Khalifa", "Amine Ben", ""], ["Frigui", "Hichem", ""]]}, {"id": "1610.04997", "submitter": "Mihai Zanfir", "authors": "Mihai Zanfir, Elisabeta Marinoiu, Cristian Sminchisescu", "title": "Spatio-Temporal Attention Models for Grounded Video Captioning", "comments": "To appear in Asian Conference on Computer Vision (ACCV), Taipei,\n  Taiwan, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video captioning is challenging due to the complex interactions in\ndynamic real scenes. A comprehensive system would ultimately localize and track\nthe objects, actions and interactions present in a video and generate a\ndescription that relies on temporal localization in order to ground the visual\nconcepts. However, most existing automatic video captioning systems map from\nraw video data to high level textual description, bypassing localization and\nrecognition, thus discarding potentially valuable information for content\nlocalization and generalization. In this work we present an automatic video\ncaptioning model that combines spatio-temporal attention and image\nclassification by means of deep neural network structures based on long\nshort-term memory. The resulting system is demonstrated to produce\nstate-of-the-art results in the standard YouTube captioning benchmark while\nalso offering the advantage of localizing the visual concepts (subjects, verbs,\nobjects), with no grounding supervision, over space and time.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 08:05:12 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 08:27:23 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Zanfir", "Mihai", ""], ["Marinoiu", "Elisabeta", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1610.05036", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal Ertugrul and Mete Ozay and Fatos T. Yarman Vural", "title": "Encoding the Local Connectivity Patterns of fMRI for Cognitive State\n  Classification", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel framework to encode the local connectivity\npatterns of brain, using Fisher Vectors (FV), Vector of Locally Aggregated\nDescriptors (VLAD) and Bag-of-Words (BoW) methods. We first obtain local\ndescriptors, called Mesh Arc Descriptors (MADs) from fMRI data, by forming\nlocal meshes around anatomical regions, and estimating their relationship\nwithin a neighborhood. Then, we extract a dictionary of relationships, called\n\\textit{brain connectivity dictionary} by fitting a generative Gaussian mixture\nmodel (GMM) to a set of MADs, and selecting the codewords at the mean of each\ncomponent of the mixture. Codewords represent the connectivity patterns among\nanatomical regions. We also encode MADs by VLAD and BoW methods using the\nk-Means clustering.\n  We classify the cognitive states of Human Connectome Project (HCP) task fMRI\ndataset, where we train support vector machines (SVM) by the encoded MADs.\nResults demonstrate that, FV encoding of MADs can be successfully employed for\nclassification of cognitive tasks, and outperform the VLAD and BoW\nrepresentations. Moreover, we identify the significant Gaussians in mixture\nmodels by computing energy of their corresponding FV parts, and analyze their\neffect on classification accuracy. Finally, we suggest a new method to\nvisualize the codewords of brain connectivity dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 10:08:09 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Ertugrul", "Itir Onal", ""], ["Ozay", "Mete", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1610.05047", "submitter": "Arne Schumann", "authors": "Arne Schumann, Shaogang Gong, Tobias Schuchert", "title": "Deep Learning Prototype Domains for Person Re-Identification", "comments": "A version of this paper has been published at ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) is the task of matching multiple occurrences\nof the same person from different cameras, poses, lighting conditions, and a\nmultitude of other factors which alter the visual appearance. Typically, this\nis achieved by learning either optimal features or matching metrics which are\nadapted to specific pairs of camera views dictated by the pairwise labelled\ntraining datasets. In this work, we formulate a deep learning based novel\napproach to automatic prototype-domain discovery for domain perceptive\n(adaptive) person re-id (rather than camera pair specific learning) for any\ncamera views scalable to new unseen scenes without training data. We learn a\nseparate re-id model for each of the discovered prototype-domains and during\nmodel deployment, use the person probe image to select automatically the model\nof the closest prototype domain. Our approach requires neither supervised nor\nunsupervised domain adaptation learning, i.e. no data available from the target\ndomains. We evaluate extensively our model under realistic re-id conditions\nusing automatically detected bounding boxes with low-resolution and partial\nocclusion. We show that our approach outperforms most of the state-of-the-art\nsupervised and unsupervised methods on the latest CUHK-SYSU and PRW benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 11:26:19 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 14:00:50 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Schumann", "Arne", ""], ["Gong", "Shaogang", ""], ["Schuchert", "Tobias", ""]]}, {"id": "1610.05174", "submitter": "Aznul Qalid Md Sabri", "authors": "Aznul Qalid Md Sabri, Jacques Boonaert, Erma Rahayu Mohd Faizal\n  Abdullah and Ali Mohammed Mansoor", "title": "Spatio-temporal Co-Occurrence Characterizations for Human Action\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human action classification task is a widely researched topic and is\nstill an open problem. Many state-of-the-arts approaches involve the usage of\nbag-of-video-words with spatio-temporal local features to construct\ncharacterizations for human actions. In order to improve beyond this standard\napproach, we investigate the usage of co-occurrences between local features. We\npropose the usage of co-occurrences information to characterize human actions.\nA trade-off factor is used to define an optimal trade-off between vocabulary\nsize and classification rate. Next, a spatio-temporal co-occurrence technique\nis applied to extract co-occurrence information between labeled local features.\nNovel characterizations for human actions are then constructed. These include a\nvector quantized correlogram-elements vector, a highly discriminative PCA\n(Principal Components Analysis) co-occurrence vector and a Haralick texture\nvector. Multi-channel kernel SVM (support vector machine) is utilized for\nclassification. For evaluation, the well known KTH as well as the challenging\nUCF-Sports action datasets are used. We obtained state-of-the-arts\nclassification performance. We also demonstrated that we are able to fully\nutilize co-occurrence information, and improve the standard bag-of-video-words\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 02:22:42 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sabri", "Aznul Qalid Md", ""], ["Boonaert", "Jacques", ""], ["Abdullah", "Erma Rahayu Mohd Faizal", ""], ["Mansoor", "Ali Mohammed", ""]]}, {"id": "1610.05211", "submitter": "Chun-Guang Li", "authors": "Chun-Guang Li, Chong You, and Ren\\'e Vidal", "title": "Structured Sparse Subspace Clustering: A Joint Affinity Learning and\n  Subspace Clustering Framework", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TIP.2017.2691557", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of segmenting data drawn from a\nunion of subspaces. State-of-the-art approaches for solving this problem follow\na two-stage approach. In the first step, an affinity matrix is learned from the\ndata using sparse or low-rank minimization techniques. In the second step, the\nsegmentation is found by applying spectral clustering to this affinity. While\nthis approach has led to state-of-the-art results in many applications, it is\nsub-optimal because it does not exploit the fact that the affinity and the\nsegmentation depend on each other. In this paper, we propose a joint\noptimization framework --- Structured Sparse Subspace Clustering (S$^3$C) ---\nfor learning both the affinity and the segmentation. The proposed S$^3$C\nframework is based on expressing each data point as a structured sparse linear\ncombination of all other data points, where the structure is induced by a norm\nthat depends on the unknown segmentation. Moreover, we extend the proposed\nS$^3$C framework into Constrained Structured Sparse Subspace Clustering\n(CS$^3$C) in which available partial side-information is incorporated into the\nstage of learning the affinity. We show that both the structured sparse\nrepresentation and the segmentation can be found via a combination of an\nalternating direction method of multipliers with spectral clustering.\nExperiments on a synthetic data set, the Extended Yale B data set, the Hopkins\n155 motion segmentation database, and three cancer data sets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 17:08:48 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 14:22:11 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Li", "Chun-Guang", ""], ["You", "Chong", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1610.05267", "submitter": "Tameru Hailesilassie", "authors": "Tameru Hailesilassie", "title": "Rule Extraction Algorithm for Deep Neural Networks: A Review", "comments": "6 pages,2 figures,IEEE Publication format, Keywords- Artificial\n  neural network; Deep neural network; Rule extraction; Decompositional;\n  Pedagogical; Eclectic", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security,Vol. 14, No. 7, July 2016, page 371-381", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the highest classification accuracy in wide varieties of application\nareas, artificial neural network has one disadvantage. The way this Network\ncomes to a decision is not easily comprehensible. The lack of explanation\nability reduces the acceptability of neural network in data mining and decision\nsystem. This drawback is the reason why researchers have proposed many rule\nextraction algorithms to solve the problem. Recently, Deep Neural Network (DNN)\nis achieving a profound result over the standard neural network for\nclassification and recognition problems. It is a hot machine learning area\nproven both useful and innovative. This paper has thoroughly reviewed various\nrule extraction algorithms, considering the classification scheme:\ndecompositional, pedagogical, and eclectics. It also presents the evaluation of\nthese algorithms based on the neural network structure with which the algorithm\nis intended to work. The main contribution of this review is to show that there\nis a limited study of rule extraction algorithm from DNN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 18:10:18 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Hailesilassie", "Tameru", ""]]}, {"id": "1610.05432", "submitter": "Markus Eich", "authors": "Markus Eich and Sareh Shirazi and Gordon Wyeth", "title": "ARTiS: Appearance-based Action Recognition in Task Space for Real-Time\n  Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To have a robot actively supporting a human during a collaborative task, it\nis crucial that robots are able to identify the current action in order to\npredict the next one. Common approaches make use of high-level knowledge, such\nas object affordances, semantics or understanding of actions in terms of pre-\nand post-conditions. These approaches often require hand-coded a priori\nknowledge, time- and resource-intensive or supervised learning techniques.\n  We propose to reframe this problem as an appearance-based place recognition\nproblem. In our framework, we regard sequences of visual images of human\nactions as a map in analogy to the visual place recognition problem. Observing\nthe task for the second time, our approach is able to recognize pre-observed\nactions in a one-shot learning approach and is thereby able to recognize the\ncurrent observation in the task space. We propose two new methods for creating\nand aligning action observations within a task map. We compare and verify our\napproaches with real data of humans assembling several types of IKEA flat\npacks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 04:45:03 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 03:58:38 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Eich", "Markus", ""], ["Shirazi", "Sareh", ""], ["Wyeth", "Gordon", ""]]}, {"id": "1610.05465", "submitter": "Gwenole Quellec", "authors": "Katia Charri\\`ere, Gwenol\\'e Quellec, Mathieu Lamard, David Martiano,\n  Guy Cazuguel, Gouenou Coatrieux, B\\'eatrice Cochener", "title": "Real-time analysis of cataract surgery videos using statistical models", "comments": "This is an extended version of a paper presented at the CBMI 2016\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic analysis of the surgical process, from videos recorded during\nsurgeries, could be very useful to surgeons, both for training and for\nacquiring new techniques. The training process could be optimized by\nautomatically providing some targeted recommendations or warnings, similar to\nthe expert surgeon's guidance. In this paper, we propose to reuse videos\nrecorded and stored during cataract surgeries to perform the analysis. The\nproposed system allows to automatically recognize, in real time, what the\nsurgeon is doing: what surgical phase or, more precisely, what surgical step he\nor she is performing. This recognition relies on the inference of a multilevel\nstatistical model which uses 1) the conditional relations between levels of\ndescription (steps and phases) and 2) the temporal relations among steps and\namong phases. The model accepts two types of inputs: 1) the presence of\nsurgical tools, manually provided by the surgeons, or 2) motion in videos,\nautomatically analyzed through the Content Based Video retrieval (CBVR)\nparadigm. Different data-driven statistical models are evaluated in this paper.\nFor this project, a dataset of 30 cataract surgery videos was collected at\nBrest University hospital. The system was evaluated in terms of area under the\nROC curve. Promising results were obtained using either the presence of\nsurgical tools ($A_z$ = 0.983) or motion analysis ($A_z$ = 0.759). The\ngenerality of the method allows to adapt it to any kinds of surgeries. The\nproposed solution could be used in a computer assisted surgery tool to support\nsurgeons during the surgery.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 07:55:48 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Charri\u00e8re", "Katia", ""], ["Quellec", "Gwenol\u00e9", ""], ["Lamard", "Mathieu", ""], ["Martiano", "David", ""], ["Cazuguel", "Guy", ""], ["Coatrieux", "Gouenou", ""], ["Cochener", "B\u00e9atrice", ""]]}, {"id": "1610.05518", "submitter": "Gianni D'Angelo", "authors": "Gianni D'Angelo, Salvatore Rampone", "title": "Shape-based defect classification for Non Destructive Testing", "comments": "5 pages, IEEE International Workshop", "journal-ref": "IEEE International Workshop on Metrology for Aerospace, Benevento,\n  Italy, June 4-5, 2015", "doi": "10.1109/MetroAeroSpace.2015.7180691", "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to classify the aerospace structure defects detected\nby eddy current non-destructive testing. The proposed method is based on the\nassumption that the defect is bound to the reaction of the probe coil impedance\nduring the test. Impedance plane analysis is used to extract a feature vector\nfrom the shape of the coil impedance in the complex plane, through the use of\nsome geometric parameters. Shape recognition is tested with three different\nmachine-learning based classifiers: decision trees, neural networks and Naive\nBayes. The performance of the proposed detection system are measured in terms\nof accuracy, sensitivity, specificity, precision and Matthews correlation\ncoefficient. Several experiments are performed on dataset of eddy current\nsignal samples for aircraft structures. The obtained results demonstrate the\nusefulness of our approach and the competiveness against existing descriptors.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 10:03:25 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["D'Angelo", "Gianni", ""], ["Rampone", "Salvatore", ""]]}, {"id": "1610.05541", "submitter": "R\\'emi Cad\\`ene", "authors": "R\\'emi Cad\\`ene, Thomas Robert, Nicolas Thome, Matthieu Cord", "title": "M2CAI Workflow Challenge: Convolutional Neural Networks with Time\n  Smoothing and Hidden Markov Model for Video Frames Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our approach is among the three best to tackle the M2CAI Workflow challenge.\nThe latter consists in recognizing the operation phase for each frames of\nendoscopic videos. In this technical report, we compare several classification\nmodels and temporal smoothing methods. Our submitted solution is a fine tuned\nResidual Network-200 on 80% of the training set with temporal smoothing using\nsimple temporal averaging of the predictions and a Hidden Markov Model modeling\nthe sequence.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 11:35:22 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 11:07:39 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cad\u00e8ne", "R\u00e9mi", ""], ["Robert", "Thomas", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""]]}, {"id": "1610.05566", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan", "title": "Edge Based Grid Super-Imposition for Crowd Emotion Recognition", "comments": "6 pages, 6 figure, 1 table, emotion, crowd, group, spontaneous,\n  indoor, edge, grid, mesh, tracking, temporal feature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous automatic continuous emotion detection system studies have examined\nmostly use of videos and images containing individual person expressing\nemotions. This study examines the detection of spontaneous emotions in a group\nand crowd settings. Edge detection was used with a grid of lines\nsuperimposition to extract the features. The feature movement in terms of\nmovement from the reference point was used to track across sequences of images\nfrom the color channel. Additionally the video data capturing was done on\nspontaneous emotions invoked by watching sports events from group of\nparticipants. The method was view and occlusion independent and the results\nwere not affected by presence of multiple people chaotically expressing various\nemotions. The edge thresholds of 0.2 and grid thresholds of 20 showed the best\naccuracy results. The overall accuracy of the group emotion classifier was\n70.9%.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 17:00:17 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Patwardhan", "Amol", ""]]}, {"id": "1610.05567", "submitter": "R\\'emi Cad\\`ene", "authors": "R\\'emi Cad\\`ene, Nicolas Thome, Matthieu Cord", "title": "Master's Thesis : Deep Learning for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our research is to develop methods advancing automatic visual\nrecognition. In order to predict the unique or multiple labels associated to an\nimage, we study different kind of Deep Neural Networks architectures and\nmethods for supervised features learning. We first draw up a state-of-the-art\nreview of the Convolutional Neural Networks aiming to understand the history\nbehind this family of statistical models, the limit of modern architectures and\nthe novel techniques currently used to train deep CNNs. The originality of our\nwork lies in our approach focusing on tasks with a low amount of data. We\nintroduce different models and techniques to achieve the best accuracy on\nseveral kind of datasets, such as a medium dataset of food recipes (100k\nimages) for building a web API, or a small dataset of satellite images (6,000)\nfor the DSG online challenge that we've won. We also draw up the\nstate-of-the-art in Weakly Supervised Learning, introducing different kind of\nCNNs able to localize regions of interest. Our last contribution is a\nframework, build on top of Torch7, for training and testing deep models on any\nvisual recognition tasks and on datasets of any scale.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 12:26:49 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Cad\u00e8ne", "R\u00e9mi", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""]]}, {"id": "1610.05586", "submitter": "Mu Li", "authors": "Mu Li and Wangmeng Zuo and David Zhang", "title": "Deep Identity-aware Transfer of Facial Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Deep convolutional network model for Identity-Aware\nTransfer (DIAT) of facial attributes. Given the source input image and the\nreference attribute, DIAT aims to generate a facial image that owns the\nreference attribute as well as keeps the same or similar identity to the input\nimage. In general, our model consists of a mask network and an attribute\ntransform network which work in synergy to generate a photo-realistic facial\nimage with the reference attribute. Considering that the reference attribute\nmay be only related to some parts of the image, the mask network is introduced\nto avoid the incorrect editing on attribute irrelevant region. Then the\nestimated mask is adopted to combine the input and transformed image for\nproducing the transfer result. For joint training of transform network and mask\nnetwork, we incorporate the adversarial attribute loss, identity-aware adaptive\nperceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising\nnetwork is presented to serve for perceptual regularization to suppress the\nartifacts in transfer result, while an attribute ratio regularization is\nintroduced to constrain the size of attribute relevant region. Our DIAT can\nprovide a unified solution for several representative facial attribute transfer\ntasks, e.g., expression transfer, accessory removal, age progression, and\ngender transfer, and can be extended for other face enhancement tasks such as\nface hallucination. The experimental results validate the effectiveness of the\nproposed method. Even for the identity-related attribute (e.g., gender), our\nDIAT can obtain visually impressive results by changing the attribute while\nretaining most identity-aware features.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 12:56:47 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 13:36:08 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Li", "Mu", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "David", ""]]}, {"id": "1610.05613", "submitter": "Aditya Singh", "authors": "Aditya Singh, Saurabh Saini, Rajvi Shah, and P J Narayanan", "title": "From Traditional to Modern : Domain Adaptation for Action Classification\n  in Short Social Video Clips", "comments": "9 pages, GCPR, 2016", "journal-ref": "Pattern Recognition,38th German Conference, GCPR 2016, Hannover,\n  Germany, September 12-15, 2016, Proceedings,pp 245-257", "doi": "10.1007/978-3-319-45886-1_20", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Short internet video clips like vines present a significantly wild\ndistribution compared to traditional video datasets. In this paper, we focus on\nthe problem of unsupervised action classification in wild vines using\ntraditional labeled datasets. To this end, we use a data augmentation based\nsimple domain adaptation strategy. We utilise semantic word2vec space as a\ncommon subspace to embed video features from both, labeled source domain and\nunlablled target domain. Our method incrementally augments the labeled source\nwith target samples and iteratively modifies the embedding function to bring\nthe source and target distributions together. Additionally, we utilise a\nmulti-modal representation that incorporates noisy semantic information\navailable in form of hash-tags. We show the effectiveness of this simple\nadaptation technique on a test set of vines and achieve notable improvements in\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 13:45:32 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Singh", "Aditya", ""], ["Saini", "Saurabh", ""], ["Shah", "Rajvi", ""], ["Narayanan", "P J", ""]]}, {"id": "1610.05693", "submitter": "Eren Aksoy", "authors": "Eren Erdal Aksoy, Adil Orhan, Florentin Woergoetter", "title": "Semantic Decomposition and Recognition of Long and Complex Manipulation\n  Action Sequences", "comments": "IJCV preprint manuscript", "journal-ref": "International Journal of Computer Vision, 2017", "doi": "10.1007/s11263-016-0956-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding continuous human actions is a non-trivial but important problem\nin computer vision. Although there exists a large corpus of work in the\nrecognition of action sequences, most approaches suffer from problems relating\nto vast variations in motions, action combinations, and scene contexts. In this\npaper, we introduce a novel method for semantic segmentation and recognition of\nlong and complex manipulation action tasks, such as \"preparing a breakfast\" or\n\"making a sandwich\". We represent manipulations with our recently introduced\n\"Semantic Event Chain\" (SEC) concept, which captures the underlying\nspatiotemporal structure of an action invariant to motion, velocity, and scene\ncontext. Solely based on the spatiotemporal interactions between manipulated\nobjects and hands in the extracted SEC, the framework automatically parses\nindividual manipulation streams performed either sequentially or concurrently.\nUsing event chains, our method further extracts basic primitive elements of\neach parsed manipulation. Without requiring any prior object knowledge, the\nproposed framework can also extract object-like scene entities that exhibit the\nsame role in semantically similar manipulations. We conduct extensive\nexperiments on various recent datasets to validate the robustness of the\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 16:13:59 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Aksoy", "Eren Erdal", ""], ["Orhan", "Adil", ""], ["Woergoetter", "Florentin", ""]]}, {"id": "1610.05712", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "Fast L1-NMF for Multiple Parametric Model Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a comprehensive algorithmic pipeline for multiple\nparametric model estimation. The proposed approach analyzes the information\nproduced by a random sampling algorithm (e.g., RANSAC) from a machine\nlearning/optimization perspective, using a \\textit{parameterless} biclustering\nalgorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed\nframework exploits consistent patterns that naturally arise during the RANSAC\nexecution, while explicitly avoiding spurious inconsistencies. Contrarily to\nthe main trends in the literature, the proposed technique does not impose\nnon-intersecting parametric models. A new accelerated algorithm to compute\nL1-NMFs allows to handle medium-sized problems faster while also extending the\nusability of the algorithm to much larger datasets. This accelerated algorithm\nhas applications in any other context where an L1-NMF is needed, beyond the\nbiclustering approach to parameter estimation here addressed. We accompany the\nalgorithmic presentation with theoretical foundations and numerous and diverse\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 17:20:38 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 15:54:14 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1610.05824", "submitter": "Li Sun Mr", "authors": "Li Sun and Gerardo Aragon-Camarasa and Simon Rogers and J. Paul\n  Siebert", "title": "Robot Vision Architecture for Autonomous Clothes Manipulation", "comments": "14 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel robot vision architecture for perceiving generic\n3D clothes configurations. Our architecture is hierarchically structured,\nstarting from low-level curvatures, across mid-level geometric shapes \\&\ntopology descriptions; and finally approaching high-level semantic surface\nstructure descriptions. We demonstrate our robot vision architecture in a\ncustomised dual-arm industrial robot with our self-designed, off-the-self\nstereo vision system, carrying out autonomous grasping and dual-arm flattening.\nIt is worth noting that the proposed dual-arm flattening approach is unique\namong the state-of-the-art robot autonomous system, which is the major\ncontribution of this paper. The experimental results show that the proposed\ndual-arm flattening using stereo vision system remarkably outperforms the\nsingle-arm flattening and widely-cited Kinect-based sensing system for\ndexterous manipulation tasks. In addition, the proposed grasping approach\nachieves satisfactory performance on grasping various kind of garments,\nverifying the capability of proposed visual perception architecture to be\nadapted to more than one clothing manipulation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:56:16 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Sun", "Li", ""], ["Aragon-Camarasa", "Gerardo", ""], ["Rogers", "Simon", ""], ["Siebert", "J. Paul", ""]]}, {"id": "1610.05834", "submitter": "Guy Satat", "authors": "Guy Satat, Matthew Tancik and Ramesh Raskar", "title": "Lensless Imaging with Compressive Ultrafast Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lensless imaging is an important and challenging problem. One notable\nsolution to lensless imaging is a single pixel camera which benefits from ideas\ncentral to compressive sampling. However, traditional single pixel cameras\nrequire many illumination patterns which result in a long acquisition process.\nHere we present a method for lensless imaging based on compressive ultrafast\nsensing. Each sensor acquisition is encoded with a different illumination\npattern and produces a time series where time is a function of the photon's\norigin in the scene. Currently available hardware with picosecond time\nresolution enables time tagging photons as they arrive to an omnidirectional\nsensor. This allows lensless imaging with significantly fewer patterns compared\nto regular single pixel imaging. To that end, we develop a framework for\ndesigning lensless imaging systems that use ultrafast detectors. We provide an\nalgorithm for ideal sensor placement and an algorithm for optimized active\nillumination patterns. We show that efficient lensless imaging is possible with\nultrafast measurement and compressive sensing. This paves the way for novel\nimaging architectures and remote sensing in extreme situations where imaging\nwith a lens is not possible.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 01:08:55 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 02:04:49 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Satat", "Guy", ""], ["Tancik", "Matthew", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1610.05854", "submitter": "Haiming Sun", "authors": "Haiming Sun, Di Xie, Shiliang Pu", "title": "Mixed context networks for semantic segmentation", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is challenging as it requires both object-level\ninformation and pixel-level accuracy. Recently, FCN-based systems gained great\nimprovement in this area. Unlike classification networks, combining features of\ndifferent layers plays an important role in these dense prediction models, as\nthese features contains information of different levels. A number of models\nhave been proposed to show how to use these features. However, what is the best\narchitecture to make use of features of different layers is still a question.\nIn this paper, we propose a module, called mixed context network, and show that\nour presented system outperforms most existing semantic segmentation systems by\nmaking use of this module.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 03:00:47 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Sun", "Haiming", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1610.05861", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt, Henrik I. Christensen and James Hays", "title": "StuffNet: Using 'Stuff' to Improve Object Detection", "comments": "Camera-ready version for IEEE WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Convolutional Neural Network (CNN) based algorithm - StuffNet -\nfor object detection. In addition to the standard convolutional features\ntrained for region proposal and object detection [31], StuffNet uses\nconvolutional features trained for segmentation of objects and 'stuff'\n(amorphous categories such as ground and water). Through experiments on Pascal\nVOC 2010, we show the importance of features learnt from stuff segmentation for\nimproving object detection performance. StuffNet improves performance from\n18.8% mAP to 23.9% mAP for small objects. We also devise a method to train\nStuffNet on datasets that do not have stuff segmentation labels. Through\nexperiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of\nthis method and show that StuffNet also significantly improves object detection\nperformance on such datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 04:44:51 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:10:20 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Christensen", "Henrik I.", ""], ["Hays", "James", ""]]}, {"id": "1610.05883", "submitter": "Thanh Nguyen", "authors": "Duc Thanh Nguyen, Binh-Son Hua, Lap-Fai Yu, and Sai-Kit Yeung", "title": "A Robust 3D-2D Interactive Tool for Scene Segmentation and Annotation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of 3D acquisition devices have enabled large-scale\nacquisition of 3D scene data. Such data, if completely and well annotated, can\nserve as useful ingredients for a wide spectrum of computer vision and graphics\nworks such as data-driven modeling and scene understanding, object detection\nand recognition. However, annotating a vast amount of 3D scene data remains\nchallenging due to the lack of an effective tool and/or the complexity of 3D\nscenes (e.g. clutter, varying illumination conditions). This paper aims to\nbuild a robust annotation tool that effectively and conveniently enables the\nsegmentation and annotation of massive 3D data. Our tool works by coupling 2D\nand 3D information via an interactive framework, through which users can\nprovide high-level semantic annotation for objects. We have experimented our\ntool and found that a typical indoor scene could be well segmented and\nannotated in less than 30 minutes by using the tool, as opposed to a few hours\nif done manually. Along with the tool, we created a dataset of over a hundred\n3D scenes associated with complete annotations using our tool. The tool and\ndataset are available at www.scenenn.net.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 06:54:02 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Nguyen", "Duc Thanh", ""], ["Hua", "Binh-Son", ""], ["Yu", "Lap-Fai", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1610.05929", "submitter": "Luyan Ji", "authors": "Luyan Ji, Xiurui Geng, Yongchao Zhao, Fuxiang Wang", "title": "An automatic bad band preremoval algorithm for hyperspectral imagery", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most hyperspectral remote sensing applications, removing bad bands, such\nas water absorption bands, is a required preprocessing step. Currently, the\ncommonly applied method is by visual inspection, which is very time-consuming\nand it is easy to overlook some noisy bands. In this study, we find an inherent\nconnection between target detection algorithms and the corrupted band removal.\nAs an example, for the matched filter (MF), which is the most widely used\ntarget detection method for hyperspectral data, we present an automatic\nMF-based algorithm for bad band identification. The MF detector is a filter\nvector, and the resulting filter output is the sum of all bands weighted by the\nMF coefficients. Therefore, we can identify bad bands only by using the MF\nfilter vector itself, the absolute value of whose entry accounts for the\nimportance of each band for the target detection. For a specific target of\ninterest, the bands with small MF weights correspond to the noisy or bad ones.\nBased on this fact, we develop an automatic bad band preremoval algorithm by\nutilizing the average absolute value of MF weights for multiple targets within\na scene. Experiments with three well known hyperspectral datasets show that our\nmethod can always identify the water absorption and other low signal-to-noise\n(SNR) bands that are usually chosen as bad bands manually.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 09:31:31 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ji", "Luyan", ""], ["Geng", "Xiurui", ""], ["Zhao", "Yongchao", ""], ["Wang", "Fuxiang", ""]]}, {"id": "1610.05949", "submitter": "Raul Mur-Artal", "authors": "Raul Mur-Artal and Juan D. Tardos", "title": "Visual-Inertial Monocular SLAM with Map Reuse", "comments": "Accepted for publication in IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2017.2653359", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there have been excellent results in Visual-Inertial Odometry\ntechniques, which aim to compute the incremental motion of the sensor with high\naccuracy and robustness. However these approaches lack the capability to close\nloops, and trajectory estimation accumulates drift even if the sensor is\ncontinually revisiting the same place. In this work we present a novel\ntightly-coupled Visual-Inertial Simultaneous Localization and Mapping system\nthat is able to close loops and reuse its map to achieve zero-drift\nlocalization in already mapped areas. While our approach can be applied to any\ncamera configuration, we address here the most general problem of a monocular\ncamera, with its well-known scale ambiguity. We also propose a novel IMU\ninitialization method, which computes the scale, the gravity direction, the\nvelocity, and gyroscope and accelerometer biases, in a few seconds with high\naccuracy. We test our system in the 11 sequences of a recent micro-aerial\nvehicle public dataset achieving a typical scale factor error of 1% and\ncentimeter precision. We compare to the state-of-the-art in visual-inertial\nodometry in sequences with revisiting, proving the better accuracy of our\nmethod due to map reuse and no drift accumulation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:17:16 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 15:45:14 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Mur-Artal", "Raul", ""], ["Tardos", "Juan D.", ""]]}, {"id": "1610.05985", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Ido Freeman, Hendrik P.A. Lensch", "title": "Learning Robust Video Synchronization without Annotations", "comments": "International Conference On Machine Learning And Applications (ICMLA\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning video sequences is a fundamental yet still unsolved component for a\nbroad range of applications in computer graphics and vision. Most classical\nimage processing methods cannot be directly applied to related video problems\ndue to the high amount of underlying data and their limit to small changes in\nappearance. We present a scalable and robust method for computing a non-linear\ntemporal video alignment. The approach autonomously manages its training data\nfor learning a meaningful representation in an iterative procedure each time\nincreasing its own knowledge. It leverages on the nature of the videos\nthemselves to remove the need for manually created labels. While previous\nalignment methods similarly consider weather conditions, season and\nillumination, our approach is able to align videos from data recorded months\napart.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 12:43:56 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 18:05:28 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 00:32:02 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Freeman", "Ido", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1610.06049", "submitter": "Michael Breu{\\ss}", "authors": "Martin B\\\"ahr, Michael Breu{\\ss}, Yvain Qu\\'eau, Ali Sharifi\n  Boroujerdi, Jean-Denis Durou", "title": "Fast and Accurate Surface Normal Integration on Non-Rectangular Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of surface normals for the purpose of computing the shape of\na surface in 3D space is a classic problem in computer vision. However, even\nnowadays it is still a challenging task to devise a method that combines the\nflexibility to work on non-trivial computational domains with high accuracy,\nrobustness and computational efficiency. By uniting a classic approach for\nsurface normal integration with modern computational techniques we construct a\nsolver that fulfils these requirements. Building upon the Poisson integration\nmodel we propose to use an iterative Krylov subspace solver as a core step in\ntackling the task. While such a method can be very efficient, it may only show\nits full potential when combined with a suitable numerical preconditioning and\na problem-specific initialisation. We perform a thorough numerical study in\norder to identify an appropriate preconditioner for our purpose. To address the\nissue of a suitable initialisation we propose to compute this initial state via\na recently developed fast marching integrator. Detailed numerical experiments\nilluminate the benefits of this novel combination. In addition, we show on\nreal-world photometric stereo datasets that the developed numerical framework\nis flexible enough to tackle modern computer vision applications.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:01:09 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["B\u00e4hr", "Martin", ""], ["Breu\u00df", "Michael", ""], ["Qu\u00e9au", "Yvain", ""], ["Boroujerdi", "Ali Sharifi", ""], ["Durou", "Jean-Denis", ""]]}, {"id": "1610.06136", "submitter": "Fengwei Yu", "authors": "Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi, Junjie Yan", "title": "POI: Multiple Object Tracking with High Performance Detection and\n  Appearance Feature", "comments": "ECCV workshop BMTT 2016", "journal-ref": "ECCV 2016 Workshops, Part II, LNCS 9914, paper approval (Chapter\n  3, 978-3-319-48880-6, 434776_1_En", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Detection and learning based appearance feature play the central role in data\nassociation based multiple object tracking (MOT), but most recent MOT works\nusually ignore them and only focus on the hand-crafted feature and association\nalgorithms. In this paper, we explore the high-performance detection and deep\nlearning based appearance feature, and show that they lead to significantly\nbetter MOT results in both online and offline setting. We make our detection\nand appearance feature publicly available. In the following part, we first\nsummarize the detection and appearance feature, and then introduce our tracker\nnamed Person of Interest (POI), which has both online and offline version.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 18:10:21 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Yu", "Fengwei", ""], ["Li", "Wenbo", ""], ["Li", "Quanquan", ""], ["Liu", "Yu", ""], ["Shi", "Xiaohua", ""], ["Yan", "Junjie", ""]]}, {"id": "1610.06204", "submitter": "Mustafa Uzunba\\c{s}", "authors": "Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, Ser Nam Lim", "title": "A Reinforcement Learning Approach to the View Planning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Reinforcement Learning (RL) solution to the view planning\nproblem (VPP), which generates a sequence of view points that are capable of\nsensing all accessible area of a given object represented as a 3D model. In\ndoing so, the goal is to minimize the number of view points, making the VPP a\nclass of set covering optimization problem (SCOP). The SCOP is NP-hard, and the\ninapproximability results tell us that the greedy algorithm provides the best\napproximation that runs in polynomial time. In order to find a solution that is\nbetter than the greedy algorithm, (i) we introduce a novel score function by\nexploiting the geometry of the 3D model, (ii) we model an intuitive human\napproach to VPP using this score function, and (iii) we cast VPP as a Markovian\nDecision Process (MDP), and solve the MDP in RL framework using well-known RL\nalgorithms. In particular, we use SARSA, Watkins-Q and TD with function\napproximation to solve the MDP. We compare the results of our method with the\nbaseline greedy algorithm in an extensive set of test objects, and show that we\ncan out-perform the baseline in almost all cases.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 20:29:20 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 20:26:51 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Kaba", "Mustafa Devrim", ""], ["Uzunbas", "Mustafa Gokhan", ""], ["Lim", "Ser Nam", ""]]}, {"id": "1610.06266", "submitter": "Yusuke Uchida", "authors": "Yusuke Uchida, Shigeyuki Sakazawa, Shin'ichi Satoh", "title": "Adaptive Substring Extraction and Modified Local NBNN Scoring for Binary\n  Feature-based Local Mobile Visual Search without False Positives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a stand-alone mobile visual search system based on\nbinary features and the bag-of-visual words framework. The contribution of this\nstudy is three-fold: (1) We propose an adaptive substring extraction method\nthat adaptively extracts informative bits from the original binary vector and\nstores them in the inverted index. These substrings are used to refine visual\nword-based matching. (2) A modified local NBNN scoring method is proposed in\nthe context of image retrieval, which considers the density of binary features\nin scoring each feature matching. (3) In order to suppress false positives, we\nintroduce a convexity check step that imposes a convexity constraint on the\nconfiguration of a transformed reference image. The proposed system improves\nretrieval accuracy by 11% compared with a conventional method without\nincreasing the database size. Furthermore, our system with the convexity check\ndoes not lead to false positive results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 02:13:50 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Uchida", "Yusuke", ""], ["Sakazawa", "Shigeyuki", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1610.06368", "submitter": "Samaneh Abbasi Sureshjani", "authors": "Samaneh Abbasi-Sureshjani and Jiong Zhang and Remco Duits and Bart ter\n  Haar Romeny", "title": "Retrieving challenging vessel connections in retinal images by line\n  co-occurrence statistics", "comments": null, "journal-ref": null, "doi": "10.1007/s00422-017-0718-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images contain often curvilinear structures, which might be\ndisconnected, or partly occluded. Recovering the missing connection of\ndisconnected structures is an open issue and needs appropriate geometric\nreasoning. We propose to find line co-occurrence statistics from the\ncenterlines of blood vessels in retinal images and show its remarkable\nsimilarity to a well-known probabilistic model for the connectivity pattern in\nthe primary visual cortex. Furthermore, the probabilistic model is trained from\nthe data via statistics and used for automated grouping of interrupted vessels\nin a spectral clustering based approach. Several challenging image patches are\ninvestigated around junction points, where successful results indicate the\nperfect match of the trained model to the profiles of blood vessels in retinal\nimages. Also, comparisons among several statistical models obtained from\ndifferent datasets reveals their high similarity i.e., they are independent of\nthe dataset. On top of that, the best approximation of the statistical model\nwith the symmetrized extension of the probabilistic model on the projective\nline bundle is found with a least square error smaller than 2%. Apparently, the\ndirection process on the projective line bundle is a good continuation model\nfor vessels in retinal images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 11:31:06 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Abbasi-Sureshjani", "Samaneh", ""], ["Zhang", "Jiong", ""], ["Duits", "Remco", ""], ["Romeny", "Bart ter Haar", ""]]}, {"id": "1610.06395", "submitter": "Sumalini Vartak", "authors": "Anne Veenendaal, Eddie Jones, Zhao Gang, Elliot Daly, Sumalini Vartak,\n  Rahul Patwardhan", "title": "Dynamic Probabilistic Network Based Human Action Recognition", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines use of dynamic probabilistic networks (DPN) for human\naction recognition. The actions of lifting objects and walking in the room,\nsitting in the room and neutral standing pose were used for testing the\nclassification. The research used the dynamic interrelation between various\ndifferent regions of interest (ROI) on the human body (face, body, arms, legs)\nand the time series based events related to the these ROIs. This dynamic links\nare then used to recognize the human behavioral aspects in the scene. First a\nmodel is developed to identify the human activities in an indoor scene and this\nmodel is dependent on the key features and interlinks between the various\ndynamic events using DPNs. The sub ROI are classified with DPN to associate the\ncombined interlink with a specific human activity. The recognition accuracy\nperformance between indoor (controlled lighting conditions) is compared with\nthe outdoor lighting conditions. The accuracy in outdoor scenes was lower than\nthe controlled environment.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 03:19:58 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Veenendaal", "Anne", ""], ["Jones", "Eddie", ""], ["Gang", "Zhao", ""], ["Daly", "Elliot", ""], ["Vartak", "Sumalini", ""], ["Patwardhan", "Rahul", ""]]}, {"id": "1610.06421", "submitter": "Hao Dong", "authors": "Hao Dong, Akara Supratak, Wei Pan, Chao Wu, Paul M. Matthews and Yike\n  Guo", "title": "Mixed Neural Network Approach for Temporal Sleep Stage Classification", "comments": "THIS ARTICLE HAS BEEN PUBLISHED IN IEEE TRANSACTIONS ON NEURAL\n  SYSTEMS AND REHABILITATION ENGINEERING", "journal-ref": null, "doi": "10.1109/TNSRE.2017.2733220", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a practical approach to addressing limitations posed by\nuse of single active electrodes in applications for sleep stage classification.\nElectroencephalography (EEG)-based characterizations of sleep stage progression\ncontribute the diagnosis and monitoring of the many pathologies of sleep.\nSeveral prior reports have explored ways of automating the analysis of sleep\nEEG and of reducing the complexity of the data needed for reliable\ndiscrimination of sleep stages in order to make it possible to perform sleep\nstudies at lower cost in the home (rather than only in specialized clinical\nfacilities). However, these reports have involved recordings from electrodes\nplaced on the cranial vertex or occiput, which can be uncomfortable or\ndifficult for subjects to position. Those that have utilized single EEG\nchannels which contain less sleep information, have showed poor classification\nperformance. We have taken advantage of Rectifier Neural Network for feature\ndetection and Long Short-Term Memory (LSTM) network for sequential data\nlearning to optimize classification performance with single electrode\nrecordings. After exploring alternative electrode placements, we found a\ncomfortable configuration of a single-channel EEG on the forehead and have\nshown that it can be integrated with additional electrodes for simultaneous\nrecording of the electroocuolgram (EOG). Evaluation of data from 62 people\n(with 494 hours sleep) demonstrated better performance of our analytical\nalgorithm for automated sleep classification than existing approaches using\nvertex or occipital electrode placements. Use of this recording configuration\nwith neural network deconvolution promises to make clinically indicated home\nsleep studies practical.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 18:48:00 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 17:39:53 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 15:00:48 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dong", "Hao", ""], ["Supratak", "Akara", ""], ["Pan", "Wei", ""], ["Wu", "Chao", ""], ["Matthews", "Paul M.", ""], ["Guo", "Yike", ""]]}, {"id": "1610.06449", "submitter": "Hamed R.-Tavakoli", "authors": "Hamed R.-Tavakoli, Ali Borji, Jorma Laaksonen, Esa Rahtu", "title": "Exploiting inter-image similarity and ensemble of extreme learners for\n  fixation prediction using deep features", "comments": null, "journal-ref": "Neurocomputing 244 (2017) 10-18", "doi": "10.1016/j.neucom.2017.03.018", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel fixation prediction and saliency modeling\nframework based on inter-image similarities and ensemble of Extreme Learning\nMachines (ELM). The proposed framework is inspired by two observations, 1) the\ncontextual information of a scene along with low-level visual cues modulates\nattention, 2) the influence of scene memorability on eye movement patterns\ncaused by the resemblance of a scene to a former visual experience. Motivated\nby such observations, we develop a framework that estimates the saliency of a\ngiven image using an ensemble of extreme learners, each trained on an image\nsimilar to the input image. That is, after retrieving a set of similar images\nfor a given image, a saliency predictor is learnt from each of the images in\nthe retrieved image set using an ELM, resulting in an ensemble. The saliency of\nthe given image is then measured in terms of the mean of predicted saliency\nvalue by the ensemble's members.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 14:55:29 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["-Tavakoli", "Hamed R.", ""], ["Borji", "Ali", ""], ["Laaksonen", "Jorma", ""], ["Rahtu", "Esa", ""]]}, {"id": "1610.06453", "submitter": "Ye Ye", "authors": "Stephanie Allen, David Madras, Ye Ye, Greg Zanotti", "title": "Change-point Detection Methods for Body-Worn Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body-worn video (BWV) cameras are increasingly utilized by police departments\nto provide a record of police-public interactions. However, large-scale BWV\ndeployment produces terabytes of data per week, necessitating the development\nof effective computational methods to identify salient changes in video. In\nwork carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel\ntwo-stage framework for video change-point detection. First, we employ\nstate-of-the-art machine learning methods including convolutional neural\nnetworks and support vector machines for scene classification. We then develop\nand compare change-point detection algorithms utilizing mean squared-error\nminimization, forecasting methods, hidden Markov models, and maximum likelihood\nestimation to identify noteworthy changes. We test our framework on detection\nof vehicle exits and entrances in a BWV data set provided by the Los Angeles\nPolice Department and achieve over 90% recall and nearly 70% precision --\ndemonstrating robustness to rapid scene changes, extreme luminance differences,\nand frequent camera occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:11:42 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Allen", "Stephanie", ""], ["Madras", "David", ""], ["Ye", "Ye", ""], ["Zanotti", "Greg", ""]]}, {"id": "1610.06461", "submitter": "Abbas Kazemipour", "authors": "Abbas Kazemipour, Ji Liu, Patrick Kanold, Min Wu, Behtash Babadi", "title": "Efficient Estimation of Compressible State-Space Models with Application\n  to Calcium Signal Deconvolution", "comments": "2016 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Dec. 7-9, 2016, Washington D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT math.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider linear state-space models with compressible\ninnovations and convergent transition matrices in order to model\nspatiotemporally sparse transient events. We perform parameter and state\nestimation using a dynamic compressed sensing framework and develop an\nefficient solution consisting of two nested Expectation-Maximization (EM)\nalgorithms. Under suitable sparsity assumptions on the innovations, we prove\nrecovery guarantees and derive confidence bounds for the state estimates. We\nprovide simulation studies as well as application to spike deconvolution from\ncalcium imaging data which verify our theoretical results and show significant\nimprovement over existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:37:53 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Kazemipour", "Abbas", ""], ["Liu", "Ji", ""], ["Kanold", "Patrick", ""], ["Wu", "Min", ""], ["Babadi", "Behtash", ""]]}, {"id": "1610.06475", "submitter": "Raul Mur-Artal", "authors": "Raul Mur-Artal and Juan D. Tardos", "title": "ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D\n  Cameras", "comments": "Accepted for publication in IEEE Transactions on Robotics", "journal-ref": null, "doi": "10.1109/TRO.2017.2705103", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D\ncameras, including map reuse, loop closing and relocalization capabilities. The\nsystem works in real-time on standard CPUs in a wide variety of environments\nfrom small hand-held indoors sequences, to drones flying in industrial\nenvironments and cars driving around a city. Our back-end based on bundle\nadjustment with monocular and stereo observations allows for accurate\ntrajectory estimation with metric scale. Our system includes a lightweight\nlocalization mode that leverages visual odometry tracks for unmapped regions\nand matches to map points that allow for zero-drift localization. The\nevaluation on 29 popular public sequences shows that our method achieves\nstate-of-the-art accuracy, being in most cases the most accurate SLAM solution.\nWe publish the source code, not only for the benefit of the SLAM community, but\nwith the aim of being an out-of-the-box SLAM solution for researchers in other\nfields.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 16:04:31 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 04:44:33 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mur-Artal", "Raul", ""], ["Tardos", "Juan D.", ""]]}, {"id": "1610.06492", "submitter": "Tomasz Kornuta", "authors": "Tomasz Kornuta and Kamil Rocki", "title": "Utilization of Deep Reinforcement Learning for saccadic-based object\n  visual search", "comments": "Paper submitted to special session on Machine Intelligence organized\n  during 23rd International AUTOMATION Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the problem of learning saccades enabling visual object\nsearch. The developed system combines reinforcement learning with a neural\nnetwork for learning to predict the possible outcomes of its actions. We\nvalidated the solution in three types of environment consisting of\n(pseudo)-randomly generated matrices of digits. The experimental verification\nis followed by the discussion regarding elements required by systems mimicking\nthe fovea movement and possible further research directions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 16:34:08 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Kornuta", "Tomasz", ""], ["Rocki", "Kamil", ""]]}, {"id": "1610.06494", "submitter": "Ahmed Ibrahim", "authors": "Ahmed Ibrahim, A. Lynn Abbott, Mohamed E. Hussein", "title": "An Image Dataset of Text Patches in Everyday Scenes", "comments": "Accepted in the 12th International Symposium on Visual Computing\n  (ISVC'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a dataset containing small images of text from everyday\nscenes. The purpose of the dataset is to support the development of new\nautomated systems that can detect and analyze text. Although much research has\nbeen devoted to text detection and recognition in scanned documents, relatively\nlittle attention has been given to text detection in other types of images,\nsuch as photographs that are posted on social-media sites. This new dataset,\nknown as COCO-Text-Patch, contains approximately 354,000 small images that are\neach labeled as \"text\" or \"non-text\". This dataset particularly addresses the\nproblem of text verification, which is an essential stage in the end-to-end\ntext detection and recognition pipeline. In order to evaluate the utility of\nthis dataset, it has been used to train two deep convolution neural networks to\ndistinguish text from non-text. One network is inspired by the GoogLeNet\narchitecture, and the second one is based on CaffeNet. Accuracy levels of 90.2%\nand 90.9% were obtained using the two networks, respectively. All of the\nimages, source code, and deep-learning trained models described in this paper\nwill be publicly available\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 16:38:42 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Ibrahim", "Ahmed", ""], ["Abbott", "A. Lynn", ""], ["Hussein", "Mohamed E.", ""]]}, {"id": "1610.06620", "submitter": "Omid Bakhshandeh", "authors": "Omid Bakhshandeh, Trung Bui, Zhe Lin, Walter Chang", "title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering open-ended questions is an essential capability for any intelligent\nagent. One of the most interesting recent open-ended question answering\nchallenges is Visual Question Answering (VQA) which attempts to evaluate a\nsystem's visual understanding through its answers to natural language questions\nabout images. There exist many approaches to VQA, the majority of which do not\nexhibit deeper semantic understanding of the candidate answers they produce. We\nstudy the importance of generating plausible answers to a given question by\nintroducing the novel task of `Answer Proposal': for a given open-ended\nquestion, a system should generate a ranked list of candidate answers informed\nby the semantics of the question. We experiment with various models including a\nneural generative model as well as a semantic graph matching one. We provide\nboth intrinsic and extrinsic evaluations for the task of Answer Proposal,\nshowing that our best model learns to propose plausible answers with a high\nrecall and performs competitively with some other solutions to VQA.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 22:01:36 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 00:12:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Bakhshandeh", "Omid", ""], ["Bui", "Trung", ""], ["Lin", "Zhe", ""], ["Chang", "Walter", ""]]}, {"id": "1610.06666", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler", "title": "Short-term prediction of localized cloud motion using ground-based sky\n  imagers", "comments": "Accepted in Proc. TENCON 2016 - 2016 IEEE Region 10 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-scale short-term cloud motion prediction is needed for several\napplications, including solar energy generation and satellite communications.\nIn tropical regions such as Singapore, clouds are mostly formed by convection;\nthey are very localized, and evolve quickly. We capture hemispherical images of\nthe sky at regular intervals of time using ground-based cameras. They provide a\nhigh resolution and localized cloud images. We use two successive frames to\ncompute optical flow and predict the future location of clouds. We achieve good\nprediction accuracy for a lead time of up to 5 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:28:35 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1610.06667", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, Stefan Winkler", "title": "Detecting Rainfall Onset Using Sky Images", "comments": "Accepted in Proc. TENCON 2016 - 2016 IEEE Region 10 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-based sky cameras (popularly known as Whole Sky Imagers) are\nincreasingly used now-a-days for continuous monitoring of the atmosphere. These\nimagers have higher temporal and spatial resolutions compared to conventional\nsatellite images. In this paper, we use ground-based sky cameras to detect the\nonset of rainfall. These images contain additional information about cloud\ncoverage and movement and are therefore useful for accurate rainfall nowcast.\nWe validate our results using rain gauge measurement recordings and achieve an\naccuracy of 89% for correct detection of rainfall onset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:31:03 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Manandhar", "Shilpa", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1610.06669", "submitter": "Chris Mattmann", "authors": "Chris Mattmann and Madhav Sharan", "title": "Scalable Pooled Time Series of Big Video Data from the Deep Web", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We contribute a scalable implementation of Ryoo et al's Pooled Time Series\nalgorithm from CVPR 2015. The updated algorithm has been evaluated on a large\nand diverse dataset of approximately 6800 videos collected from a crawl of the\ndeep web related to human trafficking on DARPA's MEMEX effort. We describe the\nproperties of Pooled Time Series and the motivation for using it to relate\nvideos collected from the deep web. We highlight issues that we found while\nrunning Pooled Time Series on larger datasets and discuss solutions for those\nissues. Our solution centers are re-imagining Pooled Time Series as a\nHadoop-based algorithm in which we compute portions of the eventual solution in\nparallel on large commodity clusters. We demonstrate that our new Hadoop-based\nalgorithm works well on the 6800 video dataset and shares all of the properties\ndescribed in the CVPR 2015 paper. We suggest avenues of future work in the\nproject.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:43:52 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Mattmann", "Chris", ""], ["Sharan", "Madhav", ""]]}, {"id": "1610.06671", "submitter": "Dewei Li", "authors": "Dewei Li and Yingjie Tian", "title": "Multi-view metric learning for multi-instance image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is critical and meaningful to make image classification since it can help\nhuman in image retrieval and recognition, object detection, etc. In this paper,\nthree-sides efforts are made to accomplish the task. First, visual features\nwith bag-of-words representation, not single vector, are extracted to\ncharacterize the image. To improve the performance, the idea of multi-view\nlearning is implemented and three kinds of features are provided, each one\ncorresponds to a single view. The information from three views is complementary\nto each other, which can be unified together. Then a new distance function is\ndesigned for bags by computing the weighted sum of the distances between\ninstances. The technique of metric learning is explored to construct a\ndata-dependent distance metric to measure the relationships between instances,\nmeanwhile between bags and images, more accurately. Last, a novel approach,\ncalled MVML, is proposed, which optimizes the joint probability that every\nimage is similar with its nearest image. MVML learns multiple distance metrics,\neach one models a single view, to unifies the information from multiple views.\nThe method can be solved by alternate optimization iteratively. Gradient ascent\nand positive semi-definite projection are utilized in the iterations. Distance\ncomparisons verified that the new bag distance function is prior to previous\nfunctions. In model evaluation, numerical experiments show that MVML with\nmultiple views performs better than single view condition, which demonstrates\nthat our model can assemble the complementary information efficiently and\nmeasure the distance between images more precisely. Experiments on influence of\nparameters and instance number validate the consistency of the method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:46:53 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Li", "Dewei", ""], ["Tian", "Yingjie", ""]]}, {"id": "1610.06688", "submitter": "Ahmed Ben Said", "authors": "Ahmed Ben Said and Rachid Hadjidj and Kamel Eddine Melkemi and Sebti\n  Foufou", "title": "Multispectral image denoising with optimized vector non-local mean\n  filter", "comments": "30 pages, 17 figures, journal paper", "journal-ref": null, "doi": "10.1016/j.dsp.2016.07.017", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many applications rely on images of high quality to ensure good\nperformance in conducting their tasks. However, noise goes against this\nobjective as it is an unavoidable issue in most applications. Therefore, it is\nessential to develop techniques to attenuate the impact of noise, while\nmaintaining the integrity of relevant information in images. We propose in this\nwork to extend the application of the Non-Local Means filter (NLM) to the\nvector case and apply it for denoising multispectral images. The objective is\nto benefit from the additional information brought by multispectral imaging\nsystems. The NLM filter exploits the redundancy of information in an image to\nremove noise. A restored pixel is a weighted average of all pixels in the\nimage. In our contribution, we propose an optimization framework where we\ndynamically fine tune the NLM filter parameters and attenuate its computational\ncomplexity by considering only pixels which are most similar to each other in\ncomputing a restored pixel. Filter parameters are optimized using Stein's\nUnbiased Risk Estimator (SURE) rather than using ad hoc means. Experiments have\nbeen conducted on multispectral images corrupted with additive white Gaussian\nnoise and PSNR and similarity comparison with other approaches are provided to\nillustrate the efficiency of our approach in terms of both denoising\nperformance and computation complexity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 07:34:57 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Said", "Ahmed Ben", ""], ["Hadjidj", "Rachid", ""], ["Melkemi", "Kamel Eddine", ""], ["Foufou", "Sebti", ""]]}, {"id": "1610.06740", "submitter": "Helge Rhodin", "authors": "Nadia Robertini, Dan Casas, Helge Rhodin, Hans-Peter Seidel, Christian\n  Theobalt", "title": "Model-based Outdoor Performance Capture", "comments": "3DV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model-based method to accurately reconstruct human\nperformances captured outdoors in a multi-camera setup. Starting from a\ntemplate of the actor model, we introduce a new unified implicit representation\nfor both, articulated skeleton tracking and nonrigid surface shape refinement.\nOur method fits the template to unsegmented video frames in two stages - first,\nthe coarse skeletal pose is estimated, and subsequently non-rigid surface shape\nand body pose are jointly refined. Particularly for surface shape refinement we\npropose a new combination of 3D Gaussians designed to align the projected model\nwith likely silhouette contours without explicit segmentation or edge\ndetection. We obtain reconstructions of much higher quality in outdoor settings\nthan existing methods, and show that we are on par with state-of-the-art\nmethods on indoor scenes for which they were designed\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 11:06:49 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Robertini", "Nadia", ""], ["Casas", "Dan", ""], ["Rhodin", "Helge", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1610.06756", "submitter": "Erik Rodner", "authors": "Erik Rodner and Marcel Simon and Robert B. Fisher and Joachim Denzler", "title": "Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of\n  Convolutional Neural Networks Approaches", "comments": "BMVC 2016 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the sensitivity of CNN outputs with respect to image\ntransformations and noise in the area of fine-grained recognition. In\nparticular, we answer the following questions (1) how sensitive are CNNs with\nrespect to image transformations encountered during wild image capture?; (2)\nhow can we predict CNN sensitivity?; and (3) can we increase the robustness of\nCNNs with respect to image degradations? To answer the first question, we\nprovide an extensive empirical sensitivity analysis of commonly used CNN\narchitectures (AlexNet, VGG19, GoogleNet) across various types of image\ndegradations. This allows for predicting CNN performance for new domains\ncomprised by images of lower quality or captured from a different viewpoint. We\nalso show how the sensitivity of CNN outputs can be predicted for single\nimages. Furthermore, we demonstrate that input layer dropout or pre-filtering\nduring test time only reduces CNN sensitivity for high levels of degradation.\n  Experiments for fine-grained recognition tasks reveal that VGG19 is more\nrobust to severe image degradations than AlexNet and GoogleNet. However, small\nintensity noise can lead to dramatic changes in CNN performance even for VGG19.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 12:14:50 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rodner", "Erik", ""], ["Simon", "Marcel", ""], ["Fisher", "Robert B.", ""], ["Denzler", "Joachim", ""]]}, {"id": "1610.06781", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Michael Milford, Peter Corke", "title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies", "comments": "Australasian Conference on Robotics and Automation (ACRA) 2017,\n  Student Paper Award Finalist", "journal-ref": "The proceedings of the Australasian Conference on Robotics and\n  Automation (ACRA) 2017", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has had significant successes in computer vision thanks\nto the abundance of visual data, collecting sufficiently large real-world\ndatasets for robot learning can be costly. To increase the practicality of\nthese techniques on real robots, we propose a modular deep reinforcement\nlearning method capable of transferring models trained in simulation to a\nreal-world robotic task. We introduce a bottleneck between perception and\ncontrol, enabling the networks to be trained independently, but then merged and\nfine-tuned in an end-to-end manner to further improve hand-eye coordination. On\na canonical, planar visually-guided robot reaching task a fine-tuned accuracy\nof 1.6 pixels is achieved, a significant improvement over naive transfer (17.5\npixels), showing the potential for more complicated and broader applications.\nOur method provides a technique for more efficient learning and transfer of\nvisuo-motor policies for real robotic systems without relying entirely on large\nreal-world robot datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 13:36:25 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 09:59:51 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 09:59:35 GMT"}, {"version": "v4", "created": "Tue, 19 Dec 2017 04:59:03 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Milford", "Michael", ""], ["Corke", "Peter", ""]]}, {"id": "1610.06815", "submitter": "Jiang Li", "authors": "Feng Li, Guangfan Zhang, Wei Wang, Roger Xu, Tom Schnell, Jonathan\n  Wen, Frederic McKenzie and Jiang Li", "title": "Deep Models for Engagement Assessment With Scarce Label Information", "comments": "9 pages, 8 figures and 3 tables", "journal-ref": null, "doi": "10.1109/THMS.2016.2608933", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task engagement is defined as loadings on energetic arousal (affect), task\nmotivation, and concentration (cognition). It is usually challenging and\nexpensive to label cognitive state data, and traditional computational models\ntrained with limited label information for engagement assessment do not perform\nwell because of overfitting. In this paper, we proposed two deep models (i.e.,\na deep classifier and a deep autoencoder) for engagement assessment with scarce\nlabel information. We recruited 15 pilots to conduct a 4-h flight simulation\nfrom Seattle to Chicago and recorded their electroencephalograph (EEG) signals\nduring the simulation. Experts carefully examined the EEG signals and labeled\n20 min of the EEG data for each pilot. The EEG signals were preprocessed and\npower spectral features were extracted. The deep models were pretrained by the\nunlabeled data and were fine-tuned by a different proportion of the labeled\ndata (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for\nengagement assessment. The models were then tested on the remaining labeled\ndata. We compared performances of the new data representations with the\noriginal EEG features for engagement assessment. Experimental results show that\nthe representations learned by the deep models yielded better accuracies for\nthe six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based\non different proportions of the labeled data for training, as compared with the\ncorresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%)\nachieved by the original EEG features. Deep models are effective for engagement\nassessment especially when less label information was used for training.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 15:05:16 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Li", "Feng", ""], ["Zhang", "Guangfan", ""], ["Wang", "Wei", ""], ["Xu", "Roger", ""], ["Schnell", "Tom", ""], ["Wen", "Jonathan", ""], ["McKenzie", "Frederic", ""], ["Li", "Jiang", ""]]}, {"id": "1610.06903", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee and Sungmin Eum and Joel Levis and Heesung Kwon and James\n  Michaelis and Michael Kolodny", "title": "Exploitation of Semantic Keywords for Malicious Event Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an event classifier is challenging when the scenes are semantically\ndifferent but visually similar. However, as humans, we typically handle such\ntasks painlessly by adding our background semantic knowledge. Motivated by this\nobservation, we aim to provide an empirical study about how additional\ninformation such as semantic keywords can boost up the discrimination of such\nevents. To demonstrate the validity of this study, we first construct a novel\nMalicious Crowd Dataset containing crowd images with two events, benign and\nmalicious, which look visually similar. Note that the primary focus of this\npaper is not to provide the state-of-the-art performance on this dataset but to\nshow the beneficial aspects of using semantically-driven keyword information.\nBy leveraging crowd-sourcing platforms, such as Amazon Mechanical Turk, we\ncollect semantic keywords associated with images and then subsequently identify\na subset of keywords (e.g. police, fire, etc.) unique to specific events. We\nfirst show that by using recently introduced attention models, a naive\nCNN-based event classifier actually learns to primarily focus on local\nattributes associated with the discriminant semantic keywords identified by the\nTurks. We further show that incorporating the keyword-driven information into\nearly- and late-fusion approaches can significantly enhance malicious event\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 19:33:46 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 15:29:39 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lee", "Hyungtae", ""], ["Eum", "Sungmin", ""], ["Levis", "Joel", ""], ["Kwon", "Heesung", ""], ["Michaelis", "James", ""], ["Kolodny", "Michael", ""]]}, {"id": "1610.06906", "submitter": "Soo Min Kang", "authors": "Soo Min Kang and Richard P. Wildes", "title": "Review of Action Recognition and Detection Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": "EECS-2016-04", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, action recognition refers to the act of classifying an\naction that is present in a given video and action detection involves locating\nactions of interest in space and/or time. Videos, which contain photometric\ninformation (e.g. RGB, intensity values) in a lattice structure, contain\ninformation that can assist in identifying the action that has been imaged. The\nprocess of action recognition and detection often begins with extracting useful\nfeatures and encoding them to ensure that the features are specific to serve\nthe task of action recognition and detection. Encoded features are then\nprocessed through a classifier to identify the action class and their spatial\nand/or temporal locations. In this report, a thorough review of various action\nrecognition and detection algorithms in computer vision is provided by\nanalyzing the two-step process of a typical action recognition and detection\nalgorithm: (i) extraction and encoding of features, and (ii) classifying\nfeatures into action classes. In efforts to ensure that computer vision-based\nalgorithms reach the capabilities that humans have of identifying actions\nirrespective of various nuisance variables that may be present within the field\nof view, the state-of-the-art methods are reviewed and some remaining problems\nare addressed in the final chapter.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 19:36:49 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 16:06:47 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Kang", "Soo Min", ""], ["Wildes", "Richard P.", ""]]}, {"id": "1610.06907", "submitter": "Hyungtae Lee", "authors": "Yilun Cao and Hyungtae Lee and Heesung Kwon", "title": "Enhanced Object Detection via Fusion With Prior Beliefs from Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel fusion method that can enhance object\ndetection performance by fusing decisions from two different types of computer\nvision tasks: object detection and image classification. In the proposed work,\nthe class label of an image obtained from image classification is viewed as\nprior knowledge about existence or non-existence of certain objects. The prior\nknowledge is then fused with the decisions of object detection to improve\ndetection accuracy by mitigating false positives of an object detector that are\nstrongly contradicted with the prior knowledge. A recently introduced novel\nfusion approach called dynamic belief fusion (DBF) is used to fuse the detector\noutput with the classification prior. Experimental results show that the\ndetection performance of all the detection algorithms used in the proposed work\nis improved on benchmark datasets via the proposed fusion framework.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 19:38:45 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Cao", "Yilun", ""], ["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""]]}, {"id": "1610.06920", "submitter": "Jorge Albericio", "authors": "J. Albericio, P. Judd, A. Delm\\'as, S. Sharify, A. Moshovos", "title": "Bit-pragmatic Deep Neural Network Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify a source of ineffectual computations when processing the\nmultiplications of the convolutional layers in Deep Neural Networks (DNNs) and\npropose Pragmatic (PRA), an architecture that exploits it improving performance\nand energy efficiency. The source of these ineffectual computations is best\nunderstood in the context of conventional multipliers which generate internally\nmultiple terms, that is, products of the multiplicand and powers of two, which\nadded together produce the final product [1]. At runtime, many of these terms\nare zero as they are generated when the multiplicand is combined with the\nzero-bits of the multiplicator. While conventional bit-parallel multipliers\ncalculate all terms in parallel to reduce individual product latency, PRA\ncalculates only the non-zero terms using a) on-the-fly conversion of the\nmultiplicator representation into an explicit list of powers of two, and b)\nhybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA\nexploits two sources of ineffectual computations: 1) the aforementioned zero\nproduct terms which are the result of the lack of explicitness in the\nmultiplicator representation, and 2) the excess in the representation precision\nused for both multiplicants and multiplicators, e.g., [2]. Measurements\ndemonstrate that for the convolutional layers, a straightforward variant of PRA\nimproves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by\n1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on\naverage compared to DaDN and STR. An improved cross lane synchronication scheme\nboosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits\npersist even with an 8-bit quantized representation [5].\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 22:16:05 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Albericio", "J.", ""], ["Judd", "P.", ""], ["Delm\u00e1s", "A.", ""], ["Sharify", "S.", ""], ["Moshovos", "A.", ""]]}, {"id": "1610.06924", "submitter": "KrishnaKanth Nakka", "authors": "Krishna Kanth Nakka", "title": "Automatic Image De-fencing System", "comments": "Master Thesis, EE IIT KGP, May 2015. arXiv admin note: text overlap\n  with arXiv:1405.3531 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tourists and Wild-life photographers are often hindered in capturing their\ncherished images or videos by a fence that limits accessibility to the scene of\ninterest. The situation has been exacerbated by growing concerns of security at\npublic places and a need exists to provide a tool that can be used for\npost-processing such fenced videos to produce a de-fenced image. There are\nseveral challenges in this problem, we identify them as Robust detection of\nfence/occlusions and Estimating pixel motion of background scenes and Filling\nin the fence/occlusions by utilizing information in multiple frames of the\ninput video. In this work, we aim to build an automatic post-processing tool\nthat can efficiently rid the input video of occlusion artifacts like fences.\nOur work is distinguished by two major contributions. The first is the\nintroduction of learning based technique to detect the fences patterns with\ncomplicated backgrounds. The second is the formulation of objective function\nand further minimization through loopy belief propagation to fill-in the fence\npixels. We observe that grids of Histogram of oriented gradients descriptor\nusing Support vector machines based classifier significantly outperforms\ndetection accuracy of texels in a lattice. We present results of experiments\nusing several real-world videos to demonstrate the effectiveness of the\nproposed fence detection and de-fencing algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 19:59:41 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Nakka", "Krishna Kanth", ""]]}, {"id": "1610.06985", "submitter": "Utsav Gewali", "authors": "Utsav B. Gewali and Sildomar T. Monteiro", "title": "Spectral Angle Based Unary Energy Functions for Spatial-Spectral\n  Hyperspectral Classification using Markov Random Fields", "comments": "In Proc. 8th IEEE Workshop on Hyperspectral Image and Signal\n  Processing : Evolution in Remote Sensing (WHISPERS), August 2016", "journal-ref": null, "doi": "10.1109/WHISPERS.2016.8071716", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and compare two spectral angle based approaches for\nspatial-spectral classification. Our methods use the spectral angle to generate\nunary energies in a grid-structured Markov random field defined over the pixel\nlabels of a hyperspectral image. The first approach is to use the exponential\nspectral angle mapper (ESAM) kernel/covariance function, a spectral angle based\nfunction, with the support vector machine and the Gaussian process classifier.\nThe second approach is to directly use the minimum spectral angle between the\ntest pixel and the training pixels as the unary energy. We compare the proposed\nmethods with the state-of-the-art Markov random field methods that use support\nvector machines and Gaussian processes with squared exponential\nkernel/covariance function. In our experiments with two datasets, it is seen\nthat using minimum spectral angle as unary energy produces better or comparable\nresults to the existing methods at a smaller running time.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 01:44:37 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Gewali", "Utsav B.", ""], ["Monteiro", "Sildomar T.", ""]]}, {"id": "1610.06987", "submitter": "Utsav Gewali", "authors": "Utsav B. Gewali and Sildomar T. Monteiro", "title": "Multitask Learning of Vegetation Biochemistry from Hyperspectral Data", "comments": "In Proc. 8th IEEE Workshop on Hyperspectral Image and Signal\n  Processing : Evolution in Remote Sensing (WHISPERS), August 2016", "journal-ref": null, "doi": "10.1109/WHISPERS.2016.8071800", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models have been successful in accurately estimating the\nbiochemical contents of vegetation from the reflectance spectra. However, their\nperformance deteriorates when there is a scarcity of sizable amount of ground\ntruth data for modeling the complex non-linear relationship occurring between\nthe spectrum and the biochemical quantity. We propose a novel Gaussian process\nbased multitask learning method for improving the prediction of a biochemical\nthrough the transfer of knowledge from the learned models for predicting\nrelated biochemicals. This method is most advantageous when there are few\nground truth data for the biochemical of interest, but plenty of ground truth\ndata for related biochemicals. The proposed multitask Gaussian process\nhypothesizes that the inter-relationship between the biochemical quantities is\nbetter modeled by using a combination of two or more covariance functions and\ninter-task correlation matrices. In the experiments, our method outperformed\nthe current methods on two real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 01:52:12 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Gewali", "Utsav B.", ""], ["Monteiro", "Sildomar T.", ""]]}, {"id": "1610.07008", "submitter": "Mete Ozay", "authors": "Mete Ozay and Takayuki Okatani", "title": "Optimization on Submanifolds of Convolution Kernels in CNNs", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel normalization methods have been employed to improve robustness of\noptimization methods to reparametrization of convolution kernels, covariate\nshift, and to accelerate training of Convolutional Neural Networks (CNNs).\nHowever, our understanding of theoretical properties of these methods has\nlagged behind their success in applications. We develop a geometric framework\nto elucidate underlying mechanisms of a diverse range of kernel normalization\nmethods. Our framework enables us to expound and identify geometry of space of\nnormalized kernels. We analyze and delineate how state-of-the-art kernel\nnormalization methods affect the geometry of search spaces of the stochastic\ngradient descent (SGD) algorithms in CNNs. Following our theoretical results,\nwe propose a SGD algorithm with assurance of almost sure convergence of the\nmethods to a solution at single minimum of classification loss of CNNs.\nExperimental results show that the proposed method achieves state-of-the-art\nperformance for major image classification benchmarks with CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 07:21:26 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1610.07031", "submitter": "Terry Taewoong Um", "authors": "Terry Taewoong Um, Vahid Babakeshizadeh and Dana Kuli\\'c", "title": "Exercise Motion Classification from Large-Scale Wearable Sensor Data\n  Using Convolutional Neural Networks", "comments": "will appear in IROS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately identify human activities is essential for\ndeveloping automatic rehabilitation and sports training systems. In this paper,\nlarge-scale exercise motion data obtained from a forearm-worn wearable sensor\nare classified with a convolutional neural network (CNN). Time-series data\nconsisting of accelerometer and orientation measurements are formatted as\nimages, allowing the CNN to automatically extract discriminative features. A\ncomparative study on the effects of image formatting and different CNN\narchitectures is also presented. The best performing configuration classifies\n50 gym exercises with 92.1% accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 10:46:01 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 04:35:49 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 22:09:17 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Um", "Terry Taewoong", ""], ["Babakeshizadeh", "Vahid", ""], ["Kuli\u0107", "Dana", ""]]}, {"id": "1610.07086", "submitter": "Gwenole Quellec", "authors": "Gwenol\\'e Quellec, Katia Charri\\`ere, Yassine Boudi, B\\'eatrice\n  Cochener, Mathieu Lamard", "title": "Deep image mining for diabetic retinopathy screening", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": "Med Image Anal., 39: 178-193 (2017)", "doi": "10.1016/j.media.2017.04.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is quickly becoming the leading methodology for medical image\nanalysis. Given a large medical archive, where each image is associated with a\ndiagnosis, efficient pathology detectors or classifiers can be trained with\nvirtually no expert knowledge about the target pathologies. However, deep\nlearning algorithms, including the popular ConvNets, are black boxes: little is\nknown about the local patterns analyzed by ConvNets to make a decision at the\nimage level. A solution is proposed in this paper to create heatmaps showing\nwhich pixels in images play a role in the image-level predictions. In other\nwords, a ConvNet trained for image-level classification can be used to detect\nlesions as well. A generalization of the backpropagation method is proposed in\norder to train ConvNets that produce high-quality heatmaps. The proposed\nsolution is applied to diabetic retinopathy (DR) screening in a dataset of\nalmost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy\ncompetition and a private dataset of almost 110,000 photographs (e-ophtha). For\nthe task of detecting referable DR, very good detection performance was\nachieved: $A_z = 0.954$ in Kaggle's dataset and $A_z = 0.949$ in e-ophtha.\nPerformance was also evaluated at the image level and at the lesion level in\nthe DiaretDB1 dataset, where four types of lesions are manually segmented:\nmicroaneurysms, hemorrhages, exudates and cotton-wool spots. The proposed\ndetector outperforms recent algorithms trained to detect those lesions\nspecifically, as well as competing heatmap generation algorithms for ConvNets.\nThis detector is part of the Messidor system for mobile eye pathology\nscreening. Because it does not rely on expert knowledge or manual segmentation\nfor detecting relevant patterns, the proposed solution is a promising image\nmining tool, which has the potential to discover new biomarkers in images.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 18:41:06 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 10:02:47 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 07:57:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Quellec", "Gwenol\u00e9", ""], ["Charri\u00e8re", "Katia", ""], ["Boudi", "Yassine", ""], ["Cochener", "B\u00e9atrice", ""], ["Lamard", "Mathieu", ""]]}, {"id": "1610.07126", "submitter": "Yuan Xie", "authors": "Yuan Xie and Dacheng Tao and Wensheng Zhang and Lei Zhang and Yan Liu\n  and Yanyun Qu", "title": "On Unifying Multi-View Self-Representations for Clustering by Tensor\n  Multi-Rank Minimization", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the multi-view subspace clustering problem. Our\nmethod utilizes the circulant algebra for tensor, which is constructed by\nstacking the subspace representation matrices of different views and then\nrotating, to capture the low rank tensor subspace so that the refinement of the\nview-specific subspaces can be achieved, as well as the high order correlations\nunderlying multi-view data can be explored.} By introducing a recently proposed\ntensor factorization, namely tensor-Singular Value Decomposition (t-SVD)\n\\cite{kilmer13}, we can impose a new type of low-rank tensor constraint on the\nrotated tensor to capture the complementary information from multiple views.\nDifferent from traditional unfolding based tensor norm, this low-rank tensor\nconstraint has optimality properties similar to that of matrix rank derived\nfrom SVD, so the complementary information among views can be explored more\nefficiently and thoroughly. The established model, called t-SVD based\nMulti-view Subspace Clustering (t-SVD-MSC), falls into the applicable scope of\naugmented Lagrangian method, and its minimization problem can be efficiently\nsolved with theoretical convergence guarantee and relatively low computational\ncomplexity. Extensive experimental testing on eight challenging image dataset\nshows that the proposed method has achieved highly competent objective\nperformance compared to several state-of-the-art multi-view clustering methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 06:48:29 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 07:19:29 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 16:50:01 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Xie", "Yuan", ""], ["Tao", "Dacheng", ""], ["Zhang", "Wensheng", ""], ["Zhang", "Lei", ""], ["Liu", "Yan", ""], ["Qu", "Yanyun", ""]]}, {"id": "1610.07159", "submitter": "Michael Zollhoefer", "authors": "Lucas Thies, Michael Zollh\\\"ofer, Christian Richardt, Christian\n  Theobalt, G\\\"unther Greiner", "title": "Real-time Halfway Domain Reconstruction of Motion and Geometry", "comments": "Proc. of the International Conference on 3D Vision 2016 (3DV 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for real-time joint reconstruction of 3D scene\nmotion and geometry from binocular stereo videos. Our approach is based on a\nnovel variational halfway-domain scene flow formulation, which allows us to\nobtain highly accurate spatiotemporal reconstructions of shape and motion. We\nsolve the underlying optimization problem at real-time frame rates using a\nnovel data-parallel robust non-linear optimization strategy. Fast convergence\nand large displacement flows are achieved by employing a novel hierarchy that\nstores delta flows between hierarchy levels. High performance is obtained by\nthe introduction of a coarser warp grid that decouples the number of unknowns\nfrom the input resolution of the images. We demonstrate our approach in a live\nsetup that is based on two commodity webcams, as well as on publicly available\nvideo data. Our extensive experiments and evaluations show that our approach\nproduces high-quality dense reconstructions of 3D geometry and scene flow at\nreal-time frame rates, and compares favorably to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 12:23:30 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Thies", "Lucas", ""], ["Zollh\u00f6fer", "Michael", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""], ["Greiner", "G\u00fcnther", ""]]}, {"id": "1610.07214", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Jianbo Jiao, Mingliang Chen, Liangqiong Qu, Xiaobin Xu,\n  and Qingxiong Yang", "title": "3D Hand Pose Tracking and Estimation Using Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose tracking/estimation will be very important in the next\ngeneration of human-computer interaction. Most of the currently available\nalgorithms rely on low-cost active depth sensors. However, these sensors can be\neasily interfered by other active sources and require relatively high power\nconsumption. As a result, they are currently not suitable for outdoor\nenvironments and mobile devices. This paper aims at tracking/estimating hand\nposes using passive stereo which avoids these limitations. A benchmark with\n18,000 stereo image pairs and 18,000 depth images captured from different\nscenarios and the ground-truth 3D positions of palm and finger joints (obtained\nfrom the manual label) is thus proposed. This paper demonstrates that the\nperformance of the state-of-the art tracking/estimation algorithms can be\nmaintained with most stereo matching algorithms on the proposed benchmark, as\nlong as the hand segmentation is correct. As a result, a novel stereo-based\nhand segmentation algorithm specially designed for hand tracking/estimation is\nproposed. The quantitative evaluation demonstrates that the proposed algorithm\nis suitable for the state-of-the-art hand pose tracking/estimation algorithms\nand the tracking quality is comparable to the use of active depth sensors under\ndifferent challenging scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 18:39:53 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Zhang", "Jiawei", ""], ["Jiao", "Jianbo", ""], ["Chen", "Mingliang", ""], ["Qu", "Liangqiong", ""], ["Xu", "Xiaobin", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1610.07231", "submitter": "Nazanin Hashemi", "authors": "Nazanin Sadat Hashemi, Roya Babaie Aghdam, Atieh Sadat Bayat Ghiasi\n  and Parastoo Fatemi", "title": "Template Matching Advances and Applications in Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most computer vision and image analysis problems, it is necessary to\ndefine a similarity measure between two or more different objects or images.\nTemplate matching is a classic and fundamental method used to score\nsimilarities between objects using certain mathematical algorithms. In this\npaper, we reviewed the basic concept of matching, as well as advances in\ntemplate matching and applications such as invariant features or novel\napplications in medical image analysis. Additionally, deformable models and\ntemplates originating from classic template matching were discussed. These\nmodels have broad applications in image registration, and they are a\nfundamental aspect of novel machine vision or deep learning algorithms, such as\nconvolutional neural networks (CNN), which perform shift and scale invariant\nfunctions followed by classification. In general, although template matching\nmethods have restrictions which limit their application, they are recommended\nfor use with other object recognition methods as pre- or post-processing steps.\nCombining a template matching technique such as normalized cross-correlation or\ndice coefficient with a robust decision-making algorithm yields a significant\nimprovement in the accuracy rate for object detection and recognition.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 20:48:17 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Hashemi", "Nazanin Sadat", ""], ["Aghdam", "Roya Babaie", ""], ["Ghiasi", "Atieh Sadat Bayat", ""], ["Fatemi", "Parastoo", ""]]}, {"id": "1610.07238", "submitter": "Francois-Xavier Derue Fx", "authors": "Fran\\c{c}ois-Xavier Derue, Guillaume-Alexandre Bilodeau, Robert\n  Bergevin", "title": "SPiKeS: Superpixel-Keypoints Structure for Robust Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual tracking, part-based trackers are attractive since they are robust\nagainst occlusion and deformation. However, a part represented by a rectangular\npatch does not account for the shape of the target, while a superpixel does\nthanks to its boundary evidence. Nevertheless, tracking superpixels is\ndifficult due to their lack of discriminative power. Therefore, to enable\nsuperpixels to be tracked discriminatively as object parts, we propose to\nenhance them with keypoints. By combining properties of these two features, we\nbuild a novel element designated as a Superpixel-Keypoints structure (SPiKeS).\nBeing discriminative, these new object parts can be located efficiently by a\nsimple nearest neighbor matching process. Then, in a tracking process, each\nmatch votes for the target's center to give its location. In addition, the\ninteresting properties of our new feature allows the development of an\nefficient model update for more robust tracking. According to experimental\nresults, our SPiKeS-based tracker proves to be robust in many challenging\nscenarios by performing favorably against the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 22:00:07 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Derue", "Fran\u00e7ois-Xavier", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bergevin", "Robert", ""]]}, {"id": "1610.07324", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang, Jian Zhang, Qiang Wu, Lixin Fan, Chun Yuan", "title": "A coarse-to-fine algorithm for registration in 3D street-view\n  cross-source point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of numerous 3D sensing technologies, object registration\non cross-source point cloud has aroused researchers' interests. When the point\nclouds are captured from different kinds of sensors, there are large and\ndifferent kinds of variations. In this study, we address an even more\nchallenging case in which the differently-source point clouds are acquired from\na real street view. One is produced directly by the LiDAR system and the other\nis generated by using VSFM software on image sequence captured from RGB\ncameras. When it confronts to large scale point clouds, previous methods mostly\nfocus on point-to-point level registration, and the methods have many\nlimitations.The reason is that the least mean error strategy shows poor ability\nin registering large variable cross-source point clouds. In this paper,\ndifferent from previous ICP-based methods, and from a statistic view, we\npropose a effective coarse-to-fine algorithm to detect and register a small\nscale SFM point cloud in a large scale Lidar point cloud. Seen from the\nexperimental results, the model can successfully run on LiDAR and SFM point\nclouds, hence it can make a contribution to many applications, such as robotics\nand smart city development.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 08:22:32 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Huang", "Xiaoshui", ""], ["Zhang", "Jian", ""], ["Wu", "Qiang", ""], ["Fan", "Lixin", ""], ["Yuan", "Chun", ""]]}, {"id": "1610.07336", "submitter": "Steffen Urban", "authors": "Steffen Urban and Stefan Hinz", "title": "MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System", "comments": "15 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basis for most vision based applications like robotics, self-driving cars\nand potentially augmented and virtual reality is a robust, continuous\nestimation of the position and orientation of a camera system w.r.t the\nobserved environment (scene). In recent years many vision based systems that\nperform simultaneous localization and mapping (SLAM) have been presented and\nreleased as open source. In this paper, we extend and improve upon a\nstate-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled\nmulti-camera systems (MCS) using the MultiCol model. In addition, we include a\nperformance evaluation on accurate ground truth and compare the robustness of\nthe proposed method to a single camera version of the SLAM system. An open\nsource implementation of the proposed multi-fisheye camera SLAM system can be\nfound on-line https://github.com/urbste/MultiCol-SLAM.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 09:27:47 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Urban", "Steffen", ""], ["Hinz", "Stefan", ""]]}, {"id": "1610.07381", "submitter": "Christos Sakaridis", "authors": "Christos Sakaridis, Kimon Drakopoulos, and Petros Maragos", "title": "Theoretical Analysis of Active Contours on Graphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active contour models based on partial differential equations have proved\nsuccessful in image segmentation, yet the study of their geometric formulation\non arbitrary geometric graphs is still at an early stage. In this paper, we\nintroduce geometric approximations of gradient and curvature, which are used in\nthe geodesic active contour model. We prove convergence in probability of our\ngradient approximation to the true gradient value and derive an asymptotic\nupper bound for the error of this approximation for the class of random\ngeometric graphs. Two different approaches for the approximation of curvature\nare presented and both are also proved to converge in probability in the case\nof random geometric graphs. We propose neighborhood-based filtering on graphs\nto improve the accuracy of the aforementioned approximations and define two\nvariants of Gaussian smoothing on graphs which include normalization in order\nto adapt to graph non-uniformities. The performance of our active contour\nframework on graphs is demonstrated in the segmentation of regular images and\ngeographical data defined on arbitrary graphs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:20:55 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Sakaridis", "Christos", ""], ["Drakopoulos", "Kimon", ""], ["Maragos", "Petros", ""]]}, {"id": "1610.07393", "submitter": "Samuele Capobianco", "authors": "Samuele Capobianco, Simone Marinai", "title": "Record Counting in Historical Handwritten Documents with Convolutional\n  Neural Networks", "comments": "Accepted to ICPR workshop on Deep Learning for Pattern Recognition\n  (DLPR 2016)", "journal-ref": null, "doi": "10.1016/j.patrec.2017.10.023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of Convolutional Neural Networks for\ncounting the number of records in historical handwritten documents. With this\nwork we demonstrate that training the networks only with synthetic images\nallows us to perform a near perfect evaluation of the number of records printed\non historical documents. The experiments have been performed on a benchmark\ndataset composed by marriage records and outperform previous results on this\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:56:20 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 10:23:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Capobianco", "Samuele", ""], ["Marinai", "Simone", ""]]}, {"id": "1610.07432", "submitter": "Douwe Kiela", "authors": "Douwe Kiela and Luana Bulat and Anita L. Vero and Stephen Clark", "title": "Virtual Embodiment: A Scalable Long-Term Strategy for Artificial\n  Intelligence Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meaning has been called the \"holy grail\" of a variety of scientific\ndisciplines, ranging from linguistics to philosophy, psychology and the\nneurosciences. The field of Artifical Intelligence (AI) is very much a part of\nthat list: the development of sophisticated natural language semantics is a\nsine qua non for achieving a level of intelligence comparable to humans.\nEmbodiment theories in cognitive science hold that human semantic\nrepresentation depends on sensori-motor experience; the abundant evidence that\nhuman meaning representation is grounded in the perception of physical reality\nleads to the conclusion that meaning must depend on a fusion of multiple\n(perceptual) modalities. Despite this, AI research in general, and its\nsubdisciplines such as computational linguistics and computer vision in\nparticular, have focused primarily on tasks that involve a single modality.\nHere, we propose virtual embodiment as an alternative, long-term strategy for\nAI research that is multi-modal in nature and that allows for the kind of\nscalability required to develop the field coherently and incrementally, in an\nethically responsible fashion.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:37:27 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Kiela", "Douwe", ""], ["Bulat", "Luana", ""], ["Vero", "Anita L.", ""], ["Clark", "Stephen", ""]]}, {"id": "1610.07442", "submitter": "Mohsen Ghafoorian", "authors": "Mohsen Ghafoorian, Nico Karssemeijer, Tom Heskes, Mayra Bergkamp,\n  Joost Wissink, Jiri Obels, Karlijn Keizer, Frank-Erik de Leeuw, Bram van\n  Ginneken, Elena Marchiori and Bram Platel", "title": "Deep Multi-scale Location-aware 3D Convolutional Neural Networks for\n  Automated Detection of Lacunes of Presumed Vascular Origin", "comments": "11 pages, 7 figures", "journal-ref": "Neuroimage Clin 14 (2017) 391-399", "doi": "10.1016/j.nicl.2017.01.033", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lacunes of presumed vascular origin (lacunes) are associated with an\nincreased risk of stroke, gait impairment, and dementia and are a primary\nimaging feature of the small vessel disease. Quantification of lacunes may be\nof great importance to elucidate the mechanisms behind neuro-degenerative\ndisorders and is recommended as part of study standards for small vessel\ndisease research. However, due to the different appearance of lacunes in\nvarious brain regions and the existence of other similar-looking structures,\nsuch as perivascular spaces, manual annotation is a difficult, elaborative and\nsubjective task, which can potentially be greatly improved by reliable and\nconsistent computer-aided detection (CAD) routines.\n  In this paper, we propose an automated two-stage method using deep\nconvolutional neural networks (CNN). We show that this method has good\nperformance and can considerably benefit readers. We first use a fully\nconvolutional neural network to detect initial candidates. In the second step,\nwe employ a 3D CNN as a false positive reduction tool. As the location\ninformation is important to the analysis of candidate structures, we further\nequip the network with contextual information using multi-scale analysis and\nintegration of explicit location features. We trained, validated and tested our\nnetworks on a large dataset of 1075 cases obtained from two different studies.\nSubsequently, we conducted an observer study with four trained observers and\ncompared our method with them using a free-response operating characteristic\nanalysis. Shown on a test set of 111 cases, the resulting CAD system exhibits\nperformance similar to the trained human observers and achieves a sensitivity\nof 0.974 with 0.13 false positives per slice. A feasibility study also showed\nthat a trained human observer would considerably benefit once aided by the CAD\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:51:47 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 13:14:32 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Ghafoorian", "Mohsen", ""], ["Karssemeijer", "Nico", ""], ["Heskes", "Tom", ""], ["Bergkamp", "Mayra", ""], ["Wissink", "Joost", ""], ["Obels", "Jiri", ""], ["Keizer", "Karlijn", ""], ["de Leeuw", "Frank-Erik", ""], ["van Ginneken", "Bram", ""], ["Marchiori", "Elena", ""], ["Platel", "Bram", ""]]}, {"id": "1610.07475", "submitter": "Siqi Bao", "authors": "Siqi Bao and Albert C. S. Chung", "title": "Feature Sensitive Label Fusion with Random Walker for Atlas-based Image\n  Segmentation", "comments": "This manuscript has been accepted for TIP2017", "journal-ref": null, "doi": "10.1109/TIP.2017.2691799", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel label fusion method is proposed for brain magnetic\nresonance image segmentation. This label fusion method is formulated on a\ngraph, which embraces both label priors from atlases and anatomical priors from\ntarget image. To represent a pixel in a comprehensive way, three kinds of\nfeature vectors are generated, including intensity, gradient and structural\nsignature. To select candidate atlas nodes for fusion, rather than exact\nsearching, randomized k-d tree with spatial constraint is introduced as an\nefficient approximation for high-dimensional feature matching. Feature\nSensitive Label Prior (FSLP), which takes both the consistency and variety of\ndifferent features into consideration, is proposed to gather atlas priors. As\nFSLP is a non-convex problem, one heuristic approach is further designed to\nsolve it efficiently. Moreover, based on the anatomical knowledge, parts of the\ntarget pixels are also employed as graph seeds to assist the label fusion\nprocess and an iterative strategy is utilized to gradually update the label\nmap. The comprehensive experiments carried out on two publicly available\ndatabases give results to demonstrate that the proposed method can obtain\nbetter segmentation quality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 16:22:07 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 15:13:11 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Bao", "Siqi", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "1610.07488", "submitter": "Yu Song", "authors": "Yu Song, Yiquan Wu", "title": "Laplacian regularized low rank subspace clustering", "comments": "17 pages, 4 figures, 5 tables. The paper is submitted to \"computer\n  vision and image understanding\". arXiv admin note: text overlap with\n  arXiv:1610.03604; text overlap with arXiv:1203.1005 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of fitting a union of subspaces to a collection of data points\ndrawn from multiple subspaces is considered in this paper. In the traditional\nlow rank representation model, the dictionary used to represent the data points\nis chosen as the data points themselves and thus the dictionary is corrupted\nwith noise. This problem is solved in the low rank subspace clustering model\nwhich decomposes the corrupted data matrix as the sum of a clean and\nself-expressive dictionary plus a matrix of noise and gross errors. Also, the\nclustering results of the low rank representation model can be enhanced by\nusing a graph of data similarity. This model is called Laplacian regularized\nlow rank representation model with a graph regularization term added to the\nobjective function. Inspired from the above two ideas, in this paper a\nLaplacian regularized low rank subspace clustering model is proposed. This\nmodel uses a clean dictionary to represent the data points and a graph\nregularization term is also incorporated in the objective function.\nExperimental results show that, compared with the traditional low rank\nrepresentation model, low rank subspace clustering model and several other\nstate-of-the-art subspace clustering model, the model proposed in this paper\ncan get better subspace clustering results with lower clustering error.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 16:51:05 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 01:24:55 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Song", "Yu", ""], ["Wu", "Yiquan", ""]]}, {"id": "1610.07557", "submitter": "Mohammad-Parsa Hosseini", "authors": "Mohammad-Parsa Hosseini, Mohammad-Reza Nazem-Zadeh, Dario Pompili,\n  Kourosh Jafari-Khouzani, Kost Elisevich, and Hamid Soltanian-Zadeh", "title": "Automatic and Manual Segmentation of Hippocampus in Epileptic Patients\n  MRI", "comments": "Presented in the 6th Annual New York Medical Imaging Informatics\n  Symposium (NYMIIS), New York, NY, USA, Sept. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hippocampus is a seminal structure in the most common surgically-treated\nform of epilepsy. Accurate segmentation of the hippocampus aids in establishing\nasymmetry regarding size and signal characteristics in order to disclose the\nlikely site of epileptogenicity. With sufficient refinement, it may ultimately\naid in the avoidance of invasive monitoring with its expense and risk for the\npatient. To this end, a reliable and consistent method for segmentation of the\nhippocampus from magnetic resonance imaging (MRI) is needed. In this work, we\npresent a systematic and statistical analysis approach for evaluation of\nautomated segmentation methods in order to establish one that reliably\napproximates the results achieved by manual tracing of the hippocampus.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:17:18 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 15:54:42 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Hosseini", "Mohammad-Parsa", ""], ["Nazem-Zadeh", "Mohammad-Reza", ""], ["Pompili", "Dario", ""], ["Jafari-Khouzani", "Kourosh", ""], ["Elisevich", "Kost", ""], ["Soltanian-Zadeh", "Hamid", ""]]}, {"id": "1610.07560", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury, Dara D. Koozekanani, Michael Reinsbach and Keshab\n  K. Parhi", "title": "Automated OCT Segmentation for Images with DME", "comments": "31 pages, 7 figures, CRC Press Book Chapter, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel automated system that segments six sub-retinal\nlayers from optical coherence tomography (OCT) image stacks of healthy patients\nand patients with diabetic macular edema (DME). First, each image in the OCT\nstack is denoised using a Wiener deconvolution algorithm that estimates the\nadditive speckle noise variance using a novel Fourier-domain based structural\nerror. This denoising method enhances the image SNR by an average of 12dB.\nNext, the denoised images are subjected to an iterative multi-resolution\nhigh-pass filtering algorithm that detects seven sub-retinal surfaces in six\niterative steps. The thicknesses of each sub-retinal layer for all scans from a\nparticular OCT stack are then compared to the manually marked groundtruth. The\nproposed system uses adaptive thresholds for denoising and segmenting each\nimage and hence it is robust to disruptions in the retinal micro-structure due\nto DME. The proposed denoising and segmentation system has an average error of\n1.2-5.8 $\\mu m$ and 3.5-26$\\mu m$ for segmenting sub-retinal surfaces in normal\nand abnormal images with DME, respectively. For estimating the sub-retinal\nlayer thicknesses, the proposed system has an average error of 0.2-2.5 $\\mu m$\nand 1.8-18 $\\mu m$ in normal and abnormal images, respectively. Additionally,\nthe average inner sub-retinal layer thickness in abnormal images is estimated\nas 275$\\mu m (r=0.92)$ with an average error of 9.3 $\\mu m$, while the average\nthickness of the outer layers in abnormal images is estimated as 57.4$\\mu m\n(r=0.74)$ with an average error of 3.5 $\\mu m$. The proposed system can be\nuseful for tracking the disease progression for DME over a period of time.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:24:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Roychowdhury", "Sohini", ""], ["Koozekanani", "Dara D.", ""], ["Reinsbach", "Michael", ""], ["Parhi", "Keshab K.", ""]]}, {"id": "1610.07570", "submitter": "Christoforos Charalambous", "authors": "Christoforos C. Charalambous and Anil A. Bharath", "title": "A data augmentation methodology for training machine/deep learning gait\n  recognition algorithms", "comments": "The paper and supplementary material are available on\n  http://www.bmva.org/bmvc/2016/papers/paper110/index.html Dataset is available\n  on http://www.bicv.org/datasets/m Proceedings of the BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several confounding factors that can reduce the accuracy of gait\nrecognition systems. These factors can reduce the distinctiveness, or alter the\nfeatures used to characterise gait, they include variations in clothing,\nlighting, pose and environment, such as the walking surface. Full invariance to\nall confounding factors is challenging in the absence of high-quality labelled\ntraining data. We introduce a simulation-based methodology and a\nsubject-specific dataset which can be used for generating synthetic video\nframes and sequences for data augmentation. With this methodology, we generated\na multi-modal dataset. In addition, we supply simulation files that provide the\nability to simultaneously sample from several confounding variables. The basis\nof the data is real motion capture data of subjects walking and running on a\ntreadmill at different speeds. Results from gait recognition experiments\nsuggest that information about the identity of subjects is retained within\nsynthetically generated examples. The dataset and methodology allow studies\ninto fully-invariant identity recognition spanning a far greater number of\nobservation conditions than would otherwise be possible.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:35:35 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Charalambous", "Christoforos C.", ""], ["Bharath", "Anil A.", ""]]}, {"id": "1610.07584", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B.\n  Tenenbaum", "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D\n  Generative-Adversarial Modeling", "comments": "NIPS 2016. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of 3D object generation. We propose a novel framework,\nnamely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects\nfrom a probabilistic space by leveraging recent advances in volumetric\nconvolutional networks and generative adversarial nets. The benefits of our\nmodel are three-fold: first, the use of an adversarial criterion, instead of\ntraditional heuristic criteria, enables the generator to capture object\nstructure implicitly and to synthesize high-quality 3D objects; second, the\ngenerator establishes a mapping from a low-dimensional probabilistic space to\nthe space of 3D objects, so that we can sample objects without a reference\nimage or CAD models, and explore the 3D object manifold; third, the adversarial\ndiscriminator provides a powerful 3D shape descriptor which, learned without\nsupervision, has wide applications in 3D object recognition. Experiments\ndemonstrate that our method generates high-quality 3D objects, and our\nunsupervisedly learned features achieve impressive performance on 3D object\nrecognition, comparable with those of supervised learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:53:41 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 18:35:52 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Wu", "Jiajun", ""], ["Zhang", "Chengkai", ""], ["Xue", "Tianfan", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1610.07629", "submitter": "Jonathon Shlens", "authors": "Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur", "title": "A Learned Representation For Artistic Style", "comments": "9 pages. 15 pages of Appendix, International Conference on Learning\n  Representations (ICLR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of painting styles represents a rich visual vocabulary for the\nconstruction of an image. The degree to which one may learn and parsimoniously\ncapture this visual vocabulary measures our understanding of the higher level\nfeatures of paintings, if not images in general. In this work we investigate\nthe construction of a single, scalable deep network that can parsimoniously\ncapture the artistic style of a diversity of paintings. We demonstrate that\nsuch a network generalizes across a diversity of artistic styles by reducing a\npainting to a point in an embedding space. Importantly, this model permits a\nuser to explore new painting styles by arbitrarily combining the styles learned\nfrom individual paintings. We hope that this work provides a useful step\ntowards building rich models of paintings and offers a window on to the\nstructure of the learned representation of artistic style.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:06:54 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 16:24:40 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 01:20:17 GMT"}, {"version": "v4", "created": "Tue, 27 Dec 2016 23:05:51 GMT"}, {"version": "v5", "created": "Thu, 9 Feb 2017 16:29:09 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Shlens", "Jonathon", ""], ["Kudlur", "Manjunath", ""]]}, {"id": "1610.07728", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Shikui Wei, Ruizhen Zhao, Yao Zhao, Xindong Wu", "title": "Camera Fingerprint: A New Perspective for Identifying User's Identity", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying user's identity is a key problem in many data mining\napplications, such as product recommendation, customized content delivery and\ncriminal identification. Given a set of accounts from the same or different\nsocial network platforms, user identification attempts to identify all accounts\nbelonging to the same person. A commonly used solution is to build the\nrelationship among different accounts by exploring their collective patterns,\ne.g., user profile, writing style, similar comments. However, this kind of\nmethod doesn't work well in many practical scenarios, since the information\nposted explicitly by users may be false due to various reasons. In this paper,\nwe re-inspect the user identification problem from a novel perspective, i.e.,\nidentifying user's identity by matching his/her cameras. The underlying\nassumption is that multiple accounts belonging to the same person contain the\nsame or similar camera fingerprint information. The proposed framework, called\nUser Camera Identification (UCI), is based on camera fingerprints, which takes\nfully into account the problems of multiple cameras and reposting behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 04:31:55 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Jiang", "Xiang", ""], ["Wei", "Shikui", ""], ["Zhao", "Ruizhen", ""], ["Zhao", "Yao", ""], ["Wu", "Xindong", ""]]}, {"id": "1610.07753", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei", "title": "A Novel Boundary Matching Algorithm for Video Temporal Error Concealment", "comments": "arXiv admin note: text overlap with arXiv:1610.07386", "journal-ref": "International Journal of Image, Graphics, and Signal Processing\n  (IJIGSP), vol. 6, no. 6, pp. 1-10, May. 2014", "doi": "10.5815/ijigsp.2014.06.01", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast growth of communication networks, the video data transmission\nfrom these networks is extremely vulnerable. Error concealment is a technique\nto estimate the damaged data by employing the correctly received data at the\ndecoder. In this paper, an efficient boundary matching algorithm for estimating\ndamaged motion vectors (MVs) is proposed. The proposed algorithm performs error\nconcealment for each damaged macro block (MB) according to the list of\nidentified priority of each frame. It then uses a classic boundary matching\ncriterion or the proposed boundary matching criterion adaptively to identify\nmatching distortion in each boundary of candidate MB. Finally, the candidate MV\nwith minimum distortion is selected as an MV of damaged MB and the list of\npriorities is updated. Experimental results show that the proposed algorithm\nimproves both objective and subjective qualities of reconstructed frames\nwithout any significant increase in computational cost. The PSNR for test\nsequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to\nthe classic boundary matching, directional boundary matching, and directional\ntemporal boundary matching algorithm, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:11:41 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1610.07758", "submitter": "Malay Bhattacharyya", "authors": "Abhisek Dash, Sujoy Chatterjee, Tripti Prasad, and Malay Bhattacharyya", "title": "Image Clustering without Ground Truth", "comments": "GroupSight Workshop, Fourth AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2016), Austin, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis has become one of the most exercised research areas over the\npast few decades in computer science. As a consequence, numerous clustering\nalgorithms have already been developed to find appropriate partitions of a set\nof objects. Given multiple such clustering solutions, it is a challenging task\nto obtain an ensemble of these solutions. This becomes more challenging when\nthe ground truth about the number of clusters is unavailable. In this paper, we\nintroduce a crowd-powered model to collect solutions of image clustering from\nthe general crowd and pose it as a clustering ensemble problem with variable\nnumber of clusters. The varying number of clusters basically reflects the crowd\nworkers' perspective toward a particular set of objects. We allow a set of\ncrowd workers to independently cluster the images as per their perceptions. We\naddress the problem by finding out centroid of the clusters using an\nappropriate distance measure and prioritize the likelihood of similarity of the\nindividual cluster sets. The effectiveness of the proposed method is\ndemonstrated by applying it on multiple artificial datasets obtained from\ncrowd.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:34:47 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Dash", "Abhisek", ""], ["Chatterjee", "Sujoy", ""], ["Prasad", "Tripti", ""], ["Bhattacharyya", "Malay", ""]]}, {"id": "1610.07804", "submitter": "Steffen Urban", "authors": "Steffen Urban and Stefan Hinz", "title": "mdBrief - A Fast Online Adaptable, Distorted Binary Descriptor for\n  Real-Time Applications Using Calibrated Wide-Angle Or Fisheye Cameras", "comments": "18 pages, 3 tables, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast binary descriptors build the core for many vision based applications\nwith real-time demands like object detection, Visual Odometry or SLAM. Commonly\nit is assumed, that the acquired images and thus the patches extracted around\nkeypoints originate from a perspective projection ignoring image distortion or\ncompletely different types of projections such as omnidirectional or fisheye.\nUsually the deviations from a perfect perspective projection are corrected by\nundistortion. Latter, however, introduces severe artifacts if the cameras\nfield-of-view gets larger. In this paper, we propose a distorted and masked\nversion of the BRIEF descriptor for calibrated cameras. Instead of correcting\nthe distortion holistically, we distort the binary tests and thus adapt the\ndescriptor to different image regions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 09:42:57 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Urban", "Steffen", ""], ["Hinz", "Stefan", ""]]}, {"id": "1610.07882", "submitter": "Michael Blot", "authors": "Michael Blot, Matthieu Cord, Nicolas Thome", "title": "Maxmin convolutional neural networks for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) are widely used in computer vision,\nespecially in image classification. However, the way in which information and\ninvariance properties are encoded through in deep CNN architectures is still an\nopen question. In this paper, we propose to modify the standard convo- lutional\nblock of CNN in order to transfer more information layer after layer while\nkeeping some invariance within the net- work. Our main idea is to exploit both\npositive and negative high scores obtained in the convolution maps. This behav-\nior is obtained by modifying the traditional activation func- tion step before\npooling. We are doubling the maps with spe- cific activations functions, called\nMaxMin strategy, in order to achieve our pipeline. Extensive experiments on two\nclassical datasets, MNIST and CIFAR-10, show that our deep MaxMin convolutional\nnet outperforms standard CNN.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 14:04:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Blot", "Michael", ""], ["Cord", "Matthieu", ""], ["Thome", "Nicolas", ""]]}, {"id": "1610.07930", "submitter": "Upal Mahbub", "authors": "Upal Mahbub, Sayantan Sarkar, Vishal M. Patel, Rama Chellappa", "title": "Active User Authentication for Smartphones: A Challenge Data Set and\n  Benchmark Results", "comments": "8 pages, 12 figures, 6 tables. Best poster award at BTAS 2016", "journal-ref": null, "doi": "10.1109/BTAS.2016.7791155", "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, automated user verification techniques for smartphones are\ninvestigated. A unique non-commercial dataset, the University of Maryland\nActive Authentication Dataset 02 (UMDAA-02) for multi-modal user authentication\nresearch is introduced. This paper focuses on three sensors - front camera,\ntouch sensor and location service while providing a general description for\nother modalities. Benchmark results for face detection, face verification,\ntouch-based user identification and location-based next-place prediction are\npresented, which indicate that more robust methods fine-tuned to the mobile\nplatform are needed to achieve satisfactory verification accuracy. The dataset\nwill be made available to the research community for promoting additional\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:56:07 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Mahbub", "Upal", ""], ["Sarkar", "Sayantan", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1610.07931", "submitter": "Ayushi Sinha", "authors": "Seth D. Billings, Ayushi Sinha, Austin Reiter, Simon Leonard, Masaru\n  Ishii, Gregory D. Hager, Russell H. Taylor", "title": "Anatomically Constrained Video-CT Registration via the V-IMLOP Algorithm", "comments": "8 pages, 4 figures, MICCAI", "journal-ref": "Medical Image Computing and Computer-Assisted Intervention --\n  MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21,\n  2016, Proceedings, Part III. Vol. 9902, pp. 133-141", "doi": "10.1007/978-3-319-46726-9_16", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional endoscopic sinus surgery (FESS) is a surgical procedure used to\ntreat acute cases of sinusitis and other sinus diseases. FESS is fast becoming\nthe preferred choice of treatment due to its minimally invasive nature.\nHowever, due to the limited field of view of the endoscope, surgeons rely on\nnavigation systems to guide them within the nasal cavity. State of the art\nnavigation systems report registration accuracy of over 1mm, which is large\ncompared to the size of the nasal airways. We present an anatomically\nconstrained video-CT registration algorithm that incorporates multiple video\nfeatures. Our algorithm is robust in the presence of outliers. We also test our\nalgorithm on simulated and in-vivo data, and test its accuracy against\ndegrading initializations.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:56:13 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Billings", "Seth D.", ""], ["Sinha", "Ayushi", ""], ["Reiter", "Austin", ""], ["Leonard", "Simon", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory D.", ""], ["Taylor", "Russell H.", ""]]}, {"id": "1610.07935", "submitter": "Upal Mahbub", "authors": "Upal Mahbub and Rama Chellappa", "title": "PATH: Person Authentication using Trace Histories", "comments": "8 pages, 9 figures. Best Paper award at IEEE UEMCON 2016", "journal-ref": null, "doi": "10.1109/UEMCON.2016.7777911", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a solution to the problem of Active Authentication using trace\nhistories is addressed. Specifically, the task is to perform user verification\non mobile devices using historical location traces of the user as a function of\ntime. Considering the movement of a human as a Markovian motion, a modified\nHidden Markov Model (HMM)-based solution is proposed. The proposed method,\nnamely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilities\nof location and timing information of the observations to smooth-out the\nemission probabilities while training. Hence, it can efficiently handle\nunforeseen observations during the test phase. The verification performance of\nthis method is compared to a sequence matching (SM) method , a Markov\nChain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap).\nExperimental results using the location information of the UMD Active\nAuthentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. The\nproposed MSHMM method outperforms the compared methods in terms of equal error\nrate (EER). Additionally, the effects of different parameters on the proposed\nmethod are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:57:41 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Mahbub", "Upal", ""], ["Chellappa", "Rama", ""]]}, {"id": "1610.07940", "submitter": "Albert Gordo", "authors": "Albert Gordo and Jon Almazan and Jerome Revaud and Diane Larlus", "title": "End-to-end Learning of Deep Visual Representations for Image Retrieval", "comments": "Accepted for publication at the International Journal of Computer\n  Vision (IJCV). Extended version of our ECCV2016 paper \"Deep Image Retrieval:\n  Learning global representations for image search\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has become a key ingredient in the top performing methods\nfor many computer vision tasks, it has failed so far to bring similar\nimprovements to instance-level image retrieval. In this article, we argue that\nreasons for the underwhelming results of deep methods on image retrieval are\nthreefold: i) noisy training data, ii) inappropriate deep architecture, and\niii) suboptimal training procedure. We address all three issues.\n  First, we leverage a large-scale but noisy landmark dataset and develop an\nautomatic cleaning method that produces a suitable training set for deep\nretrieval. Second, we build on the recent R-MAC descriptor, show that it can be\ninterpreted as a deep and differentiable architecture, and present improvements\nto enhance it. Last, we train this network with a siamese architecture that\ncombines three streams with a triplet loss. At the end of the training process,\nthe proposed architecture produces a global image representation in a single\nforward pass that is well suited for image retrieval. Extensive experiments\nshow that our approach significantly outperforms previous retrieval approaches,\nincluding state-of-the-art methods based on costly local descriptor indexing\nand spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively\nreport 94.7, 96.6, and 94.8 mean average precision. Our representations can\nalso be heavily compressed using product quantization with little loss in\naccuracy. For additional material, please see\nwww.xrce.xerox.com/Deep-Image-Retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 16:02:42 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 15:34:09 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Gordo", "Albert", ""], ["Almazan", "Jon", ""], ["Revaud", "Jerome", ""], ["Larlus", "Diane", ""]]}, {"id": "1610.07995", "submitter": "Chethana Kumara B M", "authors": "B. M. Chethana Kumara, H. S. Nagendraswamy, R. Lekha Chinmayi", "title": "Spatial Relationship Based Features for Indian Sign Language Recognition", "comments": "7 Pages, 9 Figures, 4 Tables", "journal-ref": "Int'l Journal of Computing, Communications & Instrumentation Engg.\n  (IJCCIE) Vol. 3, Issue 2 (2016) ISSN 2349-1469 EISSN 2349-1477", "doi": "10.15242/IJCCIE.IAE0516005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the task of recognizing signs made by hearing impaired people\nat sentence level has been addressed. A novel method of extracting spatial\nfeatures to capture hand movements of a signer has been proposed. Frames of a\ngiven video of a sign are preprocessed to extract face and hand components of a\nsigner. The local centroids of the extracted components along with the global\ncentroid are exploited to extract spatial features. The concept of interval\nvalued type symbolic data has been explored to capture variations in the same\nsign made by the different signers at different instances of time. A suitable\nsymbolic similarity measure is studied to establish matching between test and\nreference signs and a simple nearest neighbour classifier is used to recognize\nan unknown sign as one among the known signs by specifying a desired level of\nthreshold. An extensive experimentation is conducted on a considerably large\ndatabase of signs created by us during the course of research work in order to\nevaluate the performance of the proposed system\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 06:50:17 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Kumara", "B. M. Chethana", ""], ["Nagendraswamy", "H. S.", ""], ["Chinmayi", "R. Lekha", ""]]}, {"id": "1610.08015", "submitter": "Nicola Wadeson Dr", "authors": "Nicola Wadeson, Mark Basham", "title": "Savu: A Python-based, MPI Framework for Simultaneous Processing of\n  Multiple, N-dimensional, Large Tomography Datasets", "comments": "10 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diamond Light Source (DLS), the UK synchrotron facility, attracts scientists\nfrom across the world to perform ground-breaking x-ray experiments. With over\n3000 scientific users per year, vast amounts of data are collected across the\nexperimental beamlines, with the highest volume of data collected during\ntomographic imaging experiments. A growing interest in tomography as an imaging\ntechnique, has led to an expansion in the range of experiments performed, in\naddition to a growth in the size of the data per experiment.\n  Savu is a portable, flexible, scientific processing pipeline capable of\nprocessing multiple, n-dimensional datasets in serial on a PC, or in parallel\nacross a cluster. Developed at DLS, and successfully deployed across the\nbeamlines, it uses a modular plugin format to enable experiment-specific\nprocessing and utilises parallel HDF5 to remove RAM restrictions. The Savu\ndesign, described throughout this paper, focuses on easy integration of\nexisting and new functionality, flexibility and ease of use for users and\ndevelopers alike.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 13:22:09 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wadeson", "Nicola", ""], ["Basham", "Mark", ""]]}, {"id": "1610.08119", "submitter": "Mel McCurrie", "authors": "Mel McCurrie, Fernando Beletti, Lucas Parzianello, Allen Westendorp,\n  Samuel Anthony, Walter Scheirer", "title": "Predicting First Impressions with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describable visual facial attributes are now commonplace in human biometrics\nand affective computing, with existing algorithms even reaching a sufficient\npoint of maturity for placement into commercial products. These algorithms\nmodel objective facets of facial appearance, such as hair and eye color,\nexpression, and aspects of the geometry of the face. A natural extension, which\nhas not been studied to any great extent thus far, is the ability to model\nsubjective attributes that are assigned to a face based purely on visual\njudgements. For instance, with just a glance, our first impression of a face\nmay lead us to believe that a person is smart, worthy of our trust, and perhaps\neven our admiration - regardless of the underlying truth behind such\nattributes. Psychologists believe that these judgements are based on a variety\nof factors such as emotional states, personality traits, and other physiognomic\ncues. But work in this direction leads to an interesting question: how do we\ncreate models for problems where there is no ground truth, only measurable\nbehavior? In this paper, we introduce a new convolutional neural network-based\nregression framework that allows us to train predictive models of crowd\nbehavior for social attribute assignment. Over images from the AFLW face\ndatabase, these models demonstrate strong correlations with human crowd\nratings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:36:57 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 22:46:28 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["McCurrie", "Mel", ""], ["Beletti", "Fernando", ""], ["Parzianello", "Lucas", ""], ["Westendorp", "Allen", ""], ["Anthony", "Samuel", ""], ["Scheirer", "Walter", ""]]}, {"id": "1610.08120", "submitter": "Suchet Bargoti", "authors": "Suchet Bargoti, James Underwood", "title": "Image Segmentation for Fruit Detection and Yield Estimation in Apple\n  Orchards", "comments": "This paper is the initial version of the manuscript submitted to The\n  Journal of Field Robotics in May 2016. Following reviews and revisions, the\n  paper has been accepted for publication. The reviewed version includes\n  extended comparison between the different classification frameworks and a\n  more in-depth literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground vehicles equipped with monocular vision systems are a valuable source\nof high resolution image data for precision agriculture applications in\norchards. This paper presents an image processing framework for fruit detection\nand counting using orchard image data. A general purpose image segmentation\napproach is used, including two feature learning algorithms; multi-scale\nMulti-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These\nnetworks were extended by including contextual information about how the image\ndata was captured (metadata), which correlates with some of the appearance\nvariations and/or class distributions observed in the data. The pixel-wise\nfruit segmentation output is processed using the Watershed Segmentation (WS)\nand Circular Hough Transform (CHT) algorithms to detect and count individual\nfruits. Experiments were conducted in a commercial apple orchard near\nMelbourne, Australia. The results show an improvement in fruit segmentation\nperformance with the inclusion of metadata on the previously benchmarked MLP\nnetwork. We extend this work with CNNs, bringing agrovision closer to the\nstate-of-the-art in computer vision, where although metadata had negligible\ninfluence, the best pixel-wise F1-score of $0.791$ was achieved. The WS\nalgorithm produced the best apple detection and counting results, with a\ndetection F1-score of $0.858$. As a final step, image fruit counts were\naccumulated over multiple rows at the orchard and compared against the\npost-harvest fruit counts that were obtained from a grading and counting\nmachine. The count estimates using CNN and WS resulted in the best performance\nfor this dataset, with a squared correlation coefficient of $r^2=0.826$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:38:02 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Bargoti", "Suchet", ""], ["Underwood", "James", ""]]}, {"id": "1610.08133", "submitter": "Elaheh Raisi", "authors": "Hamid Abrishami Moghaddam and Elaheh Raisi", "title": "Incremental Nonparametric Weighted Feature Extraction for OnlineSubspace\n  Pattern Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new online method based on nonparametric weighted feature\nextraction (NWFE) is proposed. NWFE was introduced to enjoy optimum\ncharacteristics of linear discriminant analysis (LDA) and nonparametric\ndiscriminant analysis (NDA) while rectifying their drawbacks. It emphasizes the\npoints near decision boundary by putting greater weights on them and\ndeemphasizes other points. Incremental nonparametric weighted feature\nextraction (INWFE) is the online version of NWFE. INWFE has advantages of NWFE\nmethod such as extracting more than L-1 features in contrast to LDA. It is\nindependent of the class distribution and performs well in complex distributed\ndata. The effects of outliers are reduced due to the nature of its\nnonparametric scatter matrix. Furthermore, it is possible to add new samples\nasynchronously, i.e. whenever a new sample becomes available at any given time,\nit can be added to the algorithm. This is useful for many real world\napplications since all data cannot be available in advance. This method is\nimplemented on Gaussian and non-Gaussian multidimensional data, a number of UCI\ndatasets and Indian Pine dataset. Results are compared with NWFE in terms of\nclassification accuracy and execution time. For nearest neighbour classifier it\nshows that this technique converges to NWFE at the end of learning process. In\naddition, the computational complexity is reduced in comparison with NWFE in\nterms of execution time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 01:02:01 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Moghaddam", "Hamid Abrishami", ""], ["Raisi", "Elaheh", ""]]}, {"id": "1610.08336", "submitter": "Elias Mueggler", "authors": "Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, Davide\n  Scaramuzza", "title": "The Event-Camera Dataset and Simulator: Event-based Data for Pose\n  Estimation, Visual Odometry, and SLAM", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": "International Journal of Robotics Research, Vol. 36, Issue 2, pp.\n  142-149, Feb. 2017", "doi": "10.1177/0278364917691115", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New vision sensors, such as the Dynamic and Active-pixel Vision sensor\n(DAVIS), incorporate a conventional global-shutter camera and an event-based\nsensor in the same pixel array. These sensors have great potential for\nhigh-speed robotics and computer vision because they allow us to combine the\nbenefits of conventional cameras with those of event-based sensors: low\nlatency, high temporal resolution, and very high dynamic range. However, new\nalgorithms are required to exploit the sensor characteristics and cope with its\nunconventional output, which consists of a stream of asynchronous brightness\nchanges (called \"events\") and synchronous grayscale frames. For this purpose,\nwe present and release a collection of datasets captured with a DAVIS in a\nvariety of synthetic and real environments, which we hope will motivate\nresearch on new algorithms for high-speed and high-dynamic-range robotics and\ncomputer-vision applications. In addition to global-shutter intensity images\nand asynchronous events, we provide inertial measurements and ground-truth\ncamera poses from a motion-capture system. The latter allows comparing the pose\naccuracy of ego-motion estimation algorithms quantitatively. All the data are\nreleased both as standard text files and binary files (i.e., rosbag). This\npaper provides an overview of the available data and describes a simulator that\nwe release open-source to create synthetic event-camera data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 13:59:39 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 13:38:04 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 13:51:11 GMT"}, {"version": "v4", "created": "Wed, 8 Nov 2017 08:40:14 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Mueggler", "Elias", ""], ["Rebecq", "Henri", ""], ["Gallego", "Guillermo", ""], ["Delbruck", "Tobi", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1610.08400", "submitter": "Babak Taati", "authors": "Babak Taati, Pranay Lohia, Avril Mansfield, Ahmed Ashraf", "title": "Video Analysis of \"YouTube Funnies\" to Aid the Study of Human Gait and\n  Falls - Preliminary Results and Proof of Concept", "comments": "4 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because falls are funny, YouTube and other video sharing sites contain a\nlarge repository of real-life falls. We propose extracting gait and balance\ninformation from these videos to help us better understand some of the factors\nthat contribute to falls. Proof-of-concept is explored in a single video\ncontaining multiple (n=14) falls/non-falls in the presence of an unexpected\nobstacle. The analysis explores: computing spatiotemporal parameters of gait in\na video captured from an arbitrary viewpoint; the relationship between\nparameters of gait from the last few steps before the obstacle and falling vs.\nnot falling; and the predictive capacity of a multivariate model in predicting\na fall in the presence of an unexpected obstacle. Homography transformations\ncorrect the perspective projection distortion and allow for the consistent\ntracking of gait parameters as an individual walks in an arbitrary direction in\nthe scene. A synthetic top view allows for computing the average stride length\nand a synthetic side view allows for measuring up and down motions of the head.\nIn leave-one-out cross-validation, we were able to correctly predict whether a\nperson would fall or not in 11 out of the 14 cases (78.6%), just by looking at\nthe average stride length and the range of vertical head motion during the 1-4\nmost recent steps prior to reaching the obstacle.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:28:15 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Taati", "Babak", ""], ["Lohia", "Pranay", ""], ["Mansfield", "Avril", ""], ["Ashraf", "Ahmed", ""]]}, {"id": "1610.08401", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard", "title": "Universal adversarial perturbations", "comments": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a state-of-the-art deep neural network classifier, we show the\nexistence of a universal (image-agnostic) and very small perturbation vector\nthat causes natural images to be misclassified with high probability. We\npropose a systematic algorithm for computing universal perturbations, and show\nthat state-of-the-art deep neural networks are highly vulnerable to such\nperturbations, albeit being quasi-imperceptible to the human eye. We further\nempirically analyze these universal perturbations and show, in particular, that\nthey generalize very well across neural networks. The surprising existence of\nuniversal perturbations reveals important geometric correlations among the\nhigh-dimensional decision boundary of classifiers. It further outlines\npotential security breaches with the existence of single directions in the\ninput space that adversaries can possibly exploit to break a classifier on most\nnatural images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:30:45 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 07:15:00 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 17:01:25 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""]]}, {"id": "1610.08436", "submitter": "Alexandre de Siqueira", "authors": "Alexandre Fioravante de Siqueira, Fl\\'avio Camargo Cabrera, Aylton\n  Pagamisse, Aldo Eloizo Job", "title": "Estimating the concentration of gold nanoparticles incorporated on\n  Natural Rubber membranes using Multi-Level Starlet Optimal Segmentation", "comments": "22 pages, 8 figures", "journal-ref": "J Nanopart Res (2014) 16: 2809", "doi": "10.1007/s11051-014-2809-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study consolidates Multi-Level Starlet Segmentation (MLSS) and\nMulti-Level Starlet Optimal Segmentation (MLSOS), techniques for\nphotomicrograph segmentation that use starlet wavelet detail levels to separate\nareas of interest in an input image. Several segmentation levels can be\nobtained using Multi-Level Starlet Segmentation; after that, Matthews\ncorrelation coefficient (MCC) is used to choose an optimal segmentation level,\ngiving rise to Multi-Level Starlet Optimal Segmentation. In this paper, MLSOS\nis employed to estimate the concentration of gold nanoparticles with diameter\naround 47 nm, reducted on natural rubber membranes. These samples were used on\nthe construction of SERS/SERRS substrates and in the study of natural rubber\nmembranes with incorporated gold nanoparticles influence on Leishmania\nbraziliensis physiology. Precision, recall and accuracy are used to evaluate\nthe segmentation performance, and MLSOS presents accuracy greater than 88% for\nthis application.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 17:49:49 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Cabrera", "Fl\u00e1vio Camargo", ""], ["Pagamisse", "Aylton", ""], ["Job", "Aldo Eloizo", ""]]}, {"id": "1610.08481", "submitter": "Yajie Zhao", "authors": "Yajie Zhao, Qingguo Xu, Xinyu Huang, Ruigang Yang", "title": "Mask-off: Synthesizing Face Images in the Presence of Head-mounted\n  Displays", "comments": "12 pages,15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A head-mounted display (HMD) could be an important component of augmented\nreality system. However, as the upper face region is seriously occluded by the\ndevice, the user experience could be affected in applications such as\ntelecommunication and multi-player video games. In this paper, we first present\na novel experimental setup that consists of two near-infrared (NIR) cameras to\npoint to the eye regions and one visible-light RGB camera to capture the\nvisible face region. The main purpose of this paper is to synthesize realistic\nface images without occlusions based on the images captured by these cameras.\nTo this end, we propose a novel synthesis framework that contains four modules:\n3D head reconstruction, face alignment and tracking, face synthesis, and eye\nsynthesis. In face synthesis, we propose a novel algorithm that can robustly\nalign and track a personalized 3D head model given a face that is severely\noccluded by the HMD. In eye synthesis, in order to generate accurate eye\nmovements and dynamic wrinkle variations around eye regions, we propose another\nnovel algorithm to colorize the NIR eye images and further remove the \"red eye\"\neffects caused by the colorization. Results show that both hardware setup and\nsystem framework are robust to synthesize realistic face images in video\nsequences.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:41:37 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 05:44:38 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Zhao", "Yajie", ""], ["Xu", "Qingguo", ""], ["Huang", "Xinyu", ""], ["Yang", "Ruigang", ""]]}, {"id": "1610.08589", "submitter": "Abhishek Dubey", "authors": "Abhishek Kumar Dubey, Alexandros-Stavros Iliopoulos, Xiaobai Sun,\n  Fang-Fang Yin, Lei Ren", "title": "Iterative Inversion of Deformation Vector Fields with Feedback Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Often, the inverse deformation vector field (DVF) is needed together\nwith the corresponding forward DVF in 4D reconstruction and dose calculation,\nadaptive radiation therapy, and simultaneous deformable registration. This\nstudy aims at improving both accuracy and efficiency of iterative algorithms\nfor DVF inversion, and advancing our understanding of divergence and latency\nconditions. Method: We introduce a framework of fixed-point iteration\nalgorithms with active feedback control for DVF inversion. Based on rigorous\nconvergence analysis, we design control mechanisms for modulating the inverse\nconsistency (IC) residual of the current iterate, to be used as feedback into\nthe next iterate. The control is designed adaptively to the input DVF with the\nobjective to enlarge the convergence area and expedite convergence. Three\nparticular settings of feedback control are introduced: constant value over the\ndomain throughout the iteration; alternating values between iteration steps;\nand spatially variant values. We also introduce three spectral measures of the\ndisplacement Jacobian for characterizing a DVF. These measures reveal the\ncritical role of what we term the non-translational displacement component\n(NTDC) of the DVF. We carry out inversion experiments with an analytical DVF\npair, and with DVFs associated with thoracic CT images of 6 patients at end of\nexpiration and end of inspiration. Results: NTDC-adaptive iterations are shown\nto attain a larger convergence region at a faster pace compared to previous\nnon-adaptive DVF inversion iteration algorithms. By our numerical experiments,\nalternating control yields smaller IC residuals and inversion errors than\nconstant control. Spatially variant control renders smaller residuals and\nerrors by at least an order of magnitude, compared to other schemes, in no more\nthan 10 steps. Inversion results also show remarkable quantitative agreement\nwith analysis-based predictions. Conclusion: Our analysis captures properties\nof DVF data associated with clinical CT images, and provides new understanding\nof iterative DVF inversion algorithms with a simple residual feedback control.\nAdaptive control is necessary and highly effective in the presence of non-small\nNTDCs. The adaptive iterations or the spectral measures, or both, may\npotentially be incorporated into deformable image registration methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 01:32:57 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 01:01:23 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 15:42:28 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 23:49:55 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Dubey", "Abhishek Kumar", ""], ["Iliopoulos", "Alexandros-Stavros", ""], ["Sun", "Xiaobai", ""], ["Yin", "Fang-Fang", ""], ["Ren", "Lei", ""]]}, {"id": "1610.08606", "submitter": "Tiep H. Vu", "authors": "Tiep Vu and Vishal Monga", "title": "Fast Low-rank Shared Dictionary Learning for Image Classification", "comments": "Accepted version", "journal-ref": null, "doi": "10.1109/TIP.2017.2729885", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that different objects possess distinct class-specific\nfeatures, they also usually share common patterns. This observation has been\nexploited partially in a recently proposed dictionary learning framework by\nseparating the particularity and the commonality (COPAR). Inspired by this, we\npropose a novel method to explicitly and simultaneously learn a set of common\npatterns as well as class-specific features for classification with more\nintuitive constraints. Our dictionary learning framework is hence characterized\nby both a shared dictionary and particular (class-specific) dictionaries. For\nthe shared dictionary, we enforce a low-rank constraint, i.e. claim that its\nspanning subspace should have low dimension and the coefficients corresponding\nto this dictionary should be similar. For the particular dictionaries, we\nimpose on them the well-known constraints stated in the Fisher discrimination\ndictionary learning (FDDL). Further, we develop new fast and accurate\nalgorithms to solve the subproblems in the learning step, accelerating its\nconvergence. The said algorithms could also be applied to FDDL and its\nextensions. The efficiencies of these algorithms are theoretically and\nexperimentally verified by comparing their complexities and running time with\nthose of other well-known dictionary learning methods. Experimental results on\nwidely used image datasets establish the advantages of our method over\nstate-of-the-art dictionary learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 03:58:17 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 15:57:15 GMT"}, {"version": "v3", "created": "Sun, 16 Jul 2017 02:39:50 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Vu", "Tiep", ""], ["Monga", "Vishal", ""]]}, {"id": "1610.08616", "submitter": "Hua Lan", "authors": "Hua Lan and Shuai Sun and Zengfu Wang and Quan Pan and Zhishan Zhang", "title": "Joint Target Detection and Tracking in Multipath Environment: A\n  Variational Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multitarget detection and tracking problem for a class of\nmultipath detection system where one target may generate multiple measurements\nvia multiple propagation paths, and the association relationship among targets,\nmeasurements and propagation paths is unknown. In order to effectively utilize\nmultipath measurements from one target to improve detection and tracking\nperformance, a tracker has to handle high-dimensional estimation of latent\nvariables including target active/dormant meta-state, target kinematic state,\nand multipath data association. Based on variational Bayesian inference, we\npropose a novel joint detection and tracking algorithm that incorporates\nmultipath data association, target detection and target state estimation in a\nunified Bayesian framework. The posterior probabilities of these latent\nvariables are derived in a closed-form iterative manner, which is effective for\nreducing the performance deterioration caused by the coupling between\nestimation errors and identification errors. Loopy belief propagation is\nexploited to approximately calculate the probability of multipath data\nassociation, saving the computational cost significantly. Simulation results of\nover-the-horizon radar multitarget tracking show that the proposed algorithm\noutperforms multihypothesis multipath track fusion and multi-detection\n(hypothesis-oriented) multiple hypothesis tracker, especially under low\nsignal-to-noise ratio circumstance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 05:05:42 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 09:13:37 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Lan", "Hua", ""], ["Sun", "Shuai", ""], ["Wang", "Zengfu", ""], ["Pan", "Quan", ""], ["Zhang", "Zhishan", ""]]}, {"id": "1610.08619", "submitter": "Jianjia Zhang", "authors": "Jianjia Zhang, Lei Wang, Luping Zhou and Wanqing Li", "title": "Exploiting Structure Sparsity for Covariance-based Visual Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed increasing research interest on\ncovariance-based feature representation. A variety of methods have been\nproposed to boost its efficacy, with some recent ones resorting to nonlinear\nkernel technique. Noting that the essence of this feature representation is to\ncharacterise the underlying structure of visual features, this paper argues\nthat an equally, if not more, important approach to boosting its efficacy shall\nbe to improve the quality of this characterisation. Following this idea, we\npropose to exploit the structure sparsity of visual features in skeletal human\naction recognition, and compute sparse inverse covariance estimate (SICE) as\nfeature representation. We discuss the advantage of this new representation on\ndealing with small sample, high dimensionality, and modelling capability.\nFurthermore, utilising the monotonicity property of SICE, we efficiently\ngenerate a hierarchy of SICE matrices to characterise the structure of visual\nfeatures at different sparsity levels, and two discriminative learning\nalgorithms are then developed to adaptively integrate them to perform\nrecognition. As demonstrated by extensive experiments, the proposed\nrepresentation leads to significantly improved recognition performance over the\nstate-of-the-art comparable methods. In particular, as a method fully based on\nlinear technique, it is comparable or even better than those employing\nnonlinear kernel technique. This result well demonstrates the value of\nexploiting structure sparsity for covariance-based feature representation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 05:17:14 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 23:50:11 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Zhang", "Jianjia", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Li", "Wanqing", ""]]}, {"id": "1610.08624", "submitter": "PeiXin Hou", "authors": "Peixin Hou, Hao Deng, Jiguang Yue, and Shuguang Liu", "title": "PCM and APCM Revisited: An Uncertainty Perspective", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take a new look at the possibilistic c-means (PCM) and\nadaptive PCM (APCM) clustering algorithms from the perspective of uncertainty.\nThis new perspective offers us insights into the clustering process, and also\nprovides us greater degree of flexibility. We analyze the clustering behavior\nof PCM-based algorithms and introduce parameters $\\sigma_v$ and $\\alpha$ to\ncharacterize uncertainty of estimated bandwidth and noise level of the dataset\nrespectively. Then uncertainty (fuzziness) of membership values caused by\nuncertainty of the estimated bandwidth parameter is modeled by a conditional\nfuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show\nthat parameters $\\sigma_v$ and $\\alpha$ make the clustering process more easy\nto control, and main features of PCM and APCM are unified in this new\nclustering framework (UPCM). More specifically, UPCM reduces to PCM when we set\na small $\\alpha$ or a large $\\sigma_v$, and UPCM reduces to APCM when clusters\nare confined in their physical clusters and possible cluster elimination are\nensured. Finally we present further researches of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 05:41:23 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Hou", "Peixin", ""], ["Deng", "Hao", ""], ["Yue", "Jiguang", ""], ["Liu", "Shuguang", ""]]}, {"id": "1610.08627", "submitter": "Abhinav Kumar", "authors": "Abhinav Kumar and Animesh Kumar", "title": "Estimation of Bandlimited Grayscale Images From the Single Bit\n  Observations of Pixels Affected by Additive Gaussian Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of grayscale images using their single-bit zero mean Gaussian\nnoise-affected pixels is presented in this paper. The images are assumed to be\nbandlimited in the Fourier Cosine transform (FCT) domain. The images are\noversampled over their Nyquist rate in the FCT domain. We propose a\nnon-recursive approach based on first order approximation of Cumulative\nDistribution Function (CDF) to estimate the image from single bit pixels which\nitself is based on Banach's contraction theorem. The decay rate for mean\nsquared error of estimating such images is found to be independent of the\nprecision of the quantizer and it varies as $O(1/N)$ where $N$ is the\n\"effective\" oversampling ratio with respect to the Nyquist rate in the FCT\ndomain.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 06:15:26 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Kumar", "Abhinav", ""], ["Kumar", "Animesh", ""]]}, {"id": "1610.08762", "submitter": "Haoyu Li", "authors": "Haoyu Li, Changliang Guo, Inbarasan Muniraj, Bryce C. Schroeder, John\n  T. Sheridan, and Shu Jia", "title": "Volumetric Light-field Encryption at the Microscopic Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV physics.bio-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a light-field based method that allows the optical encryption of\nthree-dimensional (3D) volumetric information at the microscopic scale in a\nsingle 2D light-field image. The system consists of a microlens array and an\narray of random phase/amplitude masks. The method utilizes a wave optics model\nto account for the dominant diffraction effect at this new scale, and the\nsystem point-spread function (PSF) serves as the key for encryption and\ndecryption. We successfully developed and demonstrated a deconvolution\nalgorithm to retrieve spatially multiplexed discrete and continuous volumetric\ndata from 2D light-field images. Showing that the method is practical for data\ntransmission and storage, we obtained a faithful reconstruction of the 3D\nvolumetric information from a digital copy of the encrypted light-field image.\nThe method represents a new level of optical encryption, paving the way for\nbroad industrial and biomedical applications in processing and securing 3D data\nat the microscopic scale.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:41:19 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Li", "Haoyu", ""], ["Guo", "Changliang", ""], ["Muniraj", "Inbarasan", ""], ["Schroeder", "Bryce C.", ""], ["Sheridan", "John T.", ""], ["Jia", "Shu", ""]]}, {"id": "1610.08808", "submitter": "Madhura Katageri", "authors": "Madhura Katageri, Manisha Mandal, Mansi Gandhi, Navin Koregaonkar and\n  Prof. Sharmila Sengupta", "title": "Automated Management of Pothole related Disasters Using Image Processing\n  and Geotagging", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Technology (IJCSIT) December 2015, volume 7, number 6", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potholes though seem inconsequential, may cause accidents resulting in loss\nof human life. In this paper, we present an automated system to efficiently\nmanage the potholes in a ward by deploying geotagging and image processing\ntechniques that overcomes the drawbacks associated with the existing\nsurvey-oriented systems. Image processing is used for identification of target\npothole regions in the 2D images using edge detection and morphological image\nprocessing operations. A method is developed to accurately estimate the\ndimensions of the potholes from their images, analyze their area and depth,\nestimate the quantity of filling material required and therefore enabling\npothole attendance on a priority basis. This will further enable the government\nofficial to have a fully automated system for effectively managing pothole\nrelated disasters.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 13:04:30 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Katageri", "Madhura", ""], ["Mandal", "Manisha", ""], ["Gandhi", "Mansi", ""], ["Koregaonkar", "Navin", ""], ["Sengupta", "Prof. Sharmila", ""]]}, {"id": "1610.08844", "submitter": "Andru Putra Twinanda", "authors": "Andru P. Twinanda, Didier Mutter, Jacques Marescaux, Michel de\n  Mathelin, Nicolas Padoy", "title": "Single- and Multi-Task Architectures for Surgical Workflow Challenge at\n  M2CAI 2016", "comments": "The dataset is available at http://camma.u-strasbg.fr/m2cai2016/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surgical workflow challenge at M2CAI 2016 consists of identifying 8\nsurgical phases in cholecystectomy procedures. Here, we propose to use deep\narchitectures that are based on our previous work where we presented several\narchitectures to perform multiple recognition tasks on laparoscopic videos. In\nthis technical report, we present the phase recognition results using two\narchitectures: (1) a single-task architecture designed to perform solely the\nsurgical phase recognition task and (2) a multi-task architecture designed to\nperform jointly phase recognition and tool presence detection. On top of these\narchitectures we propose to use two different approaches to enforce the\ntemporal constraints of the surgical workflow: (1) HMM-based and (2) LSTM-based\npipelines. The results show that the LSTM-based approach is able to outperform\nthe HMM-based approach and also to properly enforce the temporal constraints\ninto the recognition process.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:42:08 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 09:12:57 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Twinanda", "Andru P.", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["de Mathelin", "Michel", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1610.08851", "submitter": "Andru Putra Twinanda", "authors": "Andru P. Twinanda, Didier Mutter, Jacques Marescaux, Michel de\n  Mathelin, Nicolas Padoy", "title": "Single- and Multi-Task Architectures for Tool Presence Detection\n  Challenge at M2CAI 2016", "comments": "The dataset is available at http://camma.u-strasbg.fr/m2cai2016/ .\n  arXiv admin note: text overlap with arXiv:1610.08844", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tool presence detection challenge at M2CAI 2016 consists of identifying\nthe presence/absence of seven surgical tools in the images of cholecystectomy\nvideos. Here, we propose to use deep architectures that are based on our\nprevious work where we presented several architectures to perform multiple\nrecognition tasks on laparoscopic videos. In this technical report, we present\nthe tool presence detection results using two architectures: (1) a single-task\narchitecture designed to perform solely the tool presence detection task and\n(2) a multi-task architecture designed to perform jointly phase recognition and\ntool presence detection. The results show that the multi-task network only\nslightly improves the tool presence detection results. In constrast, a\nsignificant improvement is obtained when there are more data available to train\nthe networks. This significant improvement can be regarded as a call for action\nfor other institutions to start working toward publishing more datasets into\nthe community, so that better models could be generated to perform the task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:51:53 GMT"}], "update_date": "2016-10-30", "authors_parsed": [["Twinanda", "Andru P.", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["de Mathelin", "Michel", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1610.08854", "submitter": "Manish Sahu", "authors": "Manish Sahu, Anirban Mukhopadhyay, Angelika Szengel and Stefan Zachow", "title": "Tool and Phase recognition using contextual CNN features", "comments": "MICCAI M2CAI 2016 Surgical tool & phase detection challenge report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A transfer learning method for generating features suitable for surgical\ntools and phase recognition from the ImageNet classification features [1] is\nproposed here. In addition, methods are developed for generating contextual\nfeatures and combining them with time series analysis for final classification\nusing multi-class random forest. The proposed pipeline is tested over the\ntraining and testing datasets of M2CAI16 challenges: tool and phase detection.\nEncouraging results are obtained by leave-one-out cross validation evaluation\non the training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:54:41 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Sahu", "Manish", ""], ["Mukhopadhyay", "Anirban", ""], ["Szengel", "Angelika", ""], ["Zachow", "Stefan", ""]]}, {"id": "1610.08871", "submitter": "Nicholas Westlake", "authors": "Nicholas Westlake, Hongping Cai and Peter Hall", "title": "Detecting People in Artwork with CNNs", "comments": "14 pages, plus 3 pages of references; 7 figures in ECCV 2016\n  Workshops", "journal-ref": null, "doi": "10.1007/978-3-319-46604-0_57", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs have massively improved performance in object detection in photographs.\nHowever research into object detection in artwork remains limited. We show\nstate-of-the-art performance on a challenging dataset, People-Art, which\ncontains people from photos, cartoons and 41 different artwork movements. We\nachieve this high performance by fine-tuning a CNN for this task, thus also\ndemonstrating that training CNNs on photos results in overfitting for photos:\nonly the first three or four layers transfer from photos to artwork. Although\nthe CNN's performance is the highest yet, it remains less than 60\\% AP,\nsuggesting further work is needed for the cross-depiction problem. The final\npublication is available at Springer via\nhttp://dx.doi.org/10.1007/978-3-319-46604-0_57\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 16:30:15 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Westlake", "Nicholas", ""], ["Cai", "Hongping", ""], ["Hall", "Peter", ""]]}, {"id": "1610.08904", "submitter": "Chen Huang", "authors": "Chen Huang, Chen Change Loy, Xiaoou Tang", "title": "Local Similarity-Aware Deep Feature Embedding", "comments": "9 pages, 4 figures, 2 tables. Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep embedding methods in vision tasks are capable of learning a\ncompact Euclidean space from images, where Euclidean distances correspond to a\nsimilarity metric. To make learning more effective and efficient, hard sample\nmining is usually employed, with samples identified through computing the\nEuclidean feature distance. However, the global Euclidean distance cannot\nfaithfully characterize the true feature similarity in a complex visual feature\nspace, where the intraclass distance in a high-density region may be larger\nthan the interclass distance in low-density regions. In this paper, we\nintroduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of\nlearning a similarity metric adaptive to local feature structure. The metric\ncan be used to select genuinely hard samples in a local neighborhood to guide\nthe deep embedding learning in an online and robust manner. The new layer is\nappealing in that it is pluggable to any convolutional networks and is trained\nend-to-end. Our local similarity-aware feature embedding not only demonstrates\nfaster convergence and boosted performance on two complex image retrieval\ndatasets, its large margin nature also leads to superior generalization results\nunder the large and open set scenarios of transfer learning and zero-shot\nlearning on ImageNet 2010 and ImageNet-10K datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 17:51:18 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Huang", "Chen", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1610.09001", "submitter": "Yusuf Aytar", "authors": "Yusuf Aytar, Carl Vondrick, Antonio Torralba", "title": "SoundNet: Learning Sound Representations from Unlabeled Video", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn rich natural sound representations by capitalizing on large amounts\nof unlabeled sound data collected in the wild. We leverage the natural\nsynchronization between vision and sound to learn an acoustic representation\nusing two-million unlabeled videos. Unlabeled video has the advantage that it\ncan be economically acquired at massive scales, yet contains useful signals\nabout natural sound. We propose a student-teacher training procedure which\ntransfers discriminative visual knowledge from well established visual\nrecognition models into the sound modality using unlabeled video as a bridge.\nOur sound representation yields significant performance improvements over the\nstate-of-the-art results on standard benchmarks for acoustic scene/object\nclassification. Visualizations suggest some high-level semantics automatically\nemerge in the sound network, even though it is trained without ground truth\nlabels.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:23:39 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Aytar", "Yusuf", ""], ["Vondrick", "Carl", ""], ["Torralba", "Antonio", ""]]}, {"id": "1610.09003", "submitter": "Carl Vondrick", "authors": "Yusuf Aytar, Lluis Castrejon, Carl Vondrick, Hamed Pirsiavash, Antonio\n  Torralba", "title": "Cross-Modal Scene Networks", "comments": "See more at http://cmplaces.csail.mit.edu/. arXiv admin note: text\n  overlap with arXiv:1607.07295", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize scenes well, they also learn an intermediate representation not\naligned across modalities, which is undesirable for cross-modal transfer\napplications. We present methods to regularize cross-modal convolutional neural\nnetworks so that they have a shared representation that is agnostic of the\nmodality. Our experiments suggest that our scene representation can help\ntransfer representations across modalities for retrieval. Moreover, our\nvisualizations suggest that units emerge in the shared representation that tend\nto activate on consistent concepts independently of the modality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:24:36 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Aytar", "Yusuf", ""], ["Castrejon", "Lluis", ""], ["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1610.09013", "submitter": "Zihao Wang", "authors": "Zihao Wang, Leonidas Spinoulas, Kuan He, Huaijin Chen, Lei Tian,\n  Aggelos K. Katsaggelos and Oliver Cossairt", "title": "Compressive Holographic Video", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": "10.1364/OE.25.000250", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing has been discussed separately in spatial and temporal\ndomains. Compressive holography has been introduced as a method that allows 3D\ntomographic reconstruction at different depths from a single 2D image. Coded\nexposure is a temporal compressed sensing method for high speed video\nacquisition. In this work, we combine compressive holography and coded exposure\ntechniques and extend the discussion to 4D reconstruction in space and time\nfrom one coded captured image. In our prototype, digital in-line holography was\nused for imaging macroscopic, fast moving objects. The pixel-wise temporal\nmodulation was implemented by a digital micromirror device. In this paper we\ndemonstrate $10\\times$ temporal super resolution with multiple depths recovery\nfrom a single image. Two examples are presented for the purpose of recording\nsubtle vibrations and tracking small particles within 5 ms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:57:42 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Zihao", ""], ["Spinoulas", "Leonidas", ""], ["He", "Kuan", ""], ["Chen", "Huaijin", ""], ["Tian", "Lei", ""], ["Katsaggelos", "Aggelos K.", ""], ["Cossairt", "Oliver", ""]]}, {"id": "1610.09032", "submitter": "Felix Gonda", "authors": "Felix Gonda, Verena Kaynig, Ray Thouis, Daniel Haehn, Jeff Lichtman,\n  Toufiq Parag, Hanspeter Pfister", "title": "Icon: An Interactive Approach to Train Deep Neural Networks for\n  Segmentation of Neuronal Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive approach to train a deep neural network pixel\nclassifier for the segmentation of neuronal structures. An interactive training\nscheme reduces the extremely tedious manual annotation task that is typically\nrequired for deep networks to perform well on image segmentation problems. Our\nproposed method employs a feedback loop that captures sparse annotations using\na graphical user interface, trains a deep neural network based on recent and\npast annotations, and displays the prediction output to users in almost\nreal-time. Our implementation of the algorithm also allows multiple users to\nprovide annotations in parallel and receive feedback from the same classifier.\nQuick feedback on classifier performance in an interactive setting enables\nusers to identify and label examples that are more important than others for\nsegmentation purposes. Our experiments show that an interactively-trained pixel\nclassifier produces better region segmentation results on Electron Microscopy\n(EM) images than those generated by a network of the same architecture trained\noffline on exhaustive ground-truth labels.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:23:56 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Gonda", "Felix", ""], ["Kaynig", "Verena", ""], ["Thouis", "Ray", ""], ["Haehn", "Daniel", ""], ["Lichtman", "Jeff", ""], ["Parag", "Toufiq", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1610.09087", "submitter": "Sanket Shinde", "authors": "Sanket Shinde, Girija Chiddarwar", "title": "Recent advances in content based video copy detection", "comments": null, "journal-ref": null, "doi": "10.1109/PERVASIVE.2015.7087093", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the immense number of videos being uploaded to the video sharing sites,\nissue of copyright infringement arises with uploading of illicit copies or\ntransformed versions of original video. Thus safeguarding copyright of digital\nmedia has become matter of concern. To address this concern, it is obliged to\nhave a video copy detection system which is sufficiently robust to detect these\ntransformed videos with ability to pinpoint location of copied segments. This\npaper outlines recent advancement in content based video copy detection, mainly\nfocusing on different visual features employed by video copy detection systems.\nFinally we evaluate performance of existing video copy detection systems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 06:21:06 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Shinde", "Sanket", ""], ["Chiddarwar", "Girija", ""]]}, {"id": "1610.09157", "submitter": "Francesco Ciompi", "authors": "Francesco Ciompi, Kaman Chung, Sarah J. van Riel, Arnaud Arindra\n  Adiyoso Setio, Paul K. Gerke, Colin Jacobs, Ernst Th. Scholten, Cornelia\n  Schaefer-Prokop, Mathilde M. W. Wille, Alfonso Marchiano, Ugo Pastorino,\n  Mathias Prokop, and Bram van Ginneken", "title": "Towards automatic pulmonary nodule management in lung cancer screening\n  with deep learning", "comments": "Published on Scientific Reports", "journal-ref": "Sci. Rep. 7, 46479; (2017)", "doi": "10.1038/srep46479", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of lung cancer screening programs will produce an\nunprecedented amount of chest CT scans in the near future, which radiologists\nwill have to read in order to decide on a patient follow-up strategy. According\nto the current guidelines, the workup of screen-detected nodules strongly\nrelies on nodule size and nodule type. In this paper, we present a deep\nlearning system based on multi-stream multi-scale convolutional networks, which\nautomatically classifies all nodule types relevant for nodule workup. The\nsystem processes raw CT data containing a nodule without the need for any\nadditional information such as nodule segmentation or nodule size and learns a\nrepresentation of 3D data by analyzing an arbitrary number of 2D views of a\ngiven nodule. The deep learning system was trained with data from the Italian\nMILD screening trial and validated on an independent set of data from the\nDanish DLCST screening trial. We analyze the advantage of processing nodules at\nmultiple scales with a multi-stream convolutional network architecture, and we\nshow that the proposed deep learning system achieves performance at classifying\nnodule type that surpasses the one of classical machine learning approaches and\nis within the inter-observer variability among four experienced human\nobservers.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 10:25:11 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 12:53:49 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Ciompi", "Francesco", ""], ["Chung", "Kaman", ""], ["van Riel", "Sarah J.", ""], ["Setio", "Arnaud Arindra Adiyoso", ""], ["Gerke", "Paul K.", ""], ["Jacobs", "Colin", ""], ["Scholten", "Ernst Th.", ""], ["Schaefer-Prokop", "Cornelia", ""], ["Wille", "Mathilde M. W.", ""], ["Marchiano", "Alfonso", ""], ["Pastorino", "Ugo", ""], ["Prokop", "Mathias", ""], ["van Ginneken", "Bram", ""]]}, {"id": "1610.09204", "submitter": "Brian Kenji Iwana", "authors": "Brian Kenji Iwana, Syed Tahseen Raza Rizvi, Sheraz Ahmed, Andreas\n  Dengel, Seiichi Uchida", "title": "Judging a Book By its Cover", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Book covers communicate information to potential readers, but can that same\ninformation be learned by computers? We propose using a deep Convolutional\nNeural Network (CNN) to predict the genre of a book based on the visual clues\nprovided by its cover. The purpose of this research is to investigate whether\nrelationships between books and their covers can be learned. However,\ndetermining the genre of a book is a difficult task because covers can be\nambiguous and genres can be overarching. Despite this, we show that a CNN can\nextract features and learn underlying design rules set by the designer to\ndefine a genre. Using machine learning, we can bring the large amount of\nresources available to the book cover design process. In addition, we present a\nnew challenging dataset that can be used for many pattern recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 13:26:55 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 11:59:23 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 03:26:08 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Iwana", "Brian Kenji", ""], ["Rizvi", "Syed Tahseen Raza", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1610.09237", "submitter": "Oleg Grinchuk", "authors": "Oleg Grinchuk, Vadim Lebedev, Victor Lempitsky", "title": "Learnable Visual Markers", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to designing visual markers (analogous to QR-codes,\nmarkers for augmented reality, and robotic fiducial tags) based on the advances\nin deep generative networks. In our approach, the markers are obtained as color\nimages synthesized by a deep network from input bit strings, whereas another\ndeep network is trained to recover the bit strings back from the photos of\nthese markers. The two networks are trained simultaneously in a joint\nbackpropagation process that takes characteristic photometric and geometric\ndistortions associated with marker fabrication and marker scanning into\naccount. Additionally, a stylization loss based on statistics of activations in\na pretrained classification network can be inserted into the learning in order\nto shift the marker appearance towards some texture prototype. In the\nexperiments, we demonstrate that the markers obtained using our approach are\ncapable of retaining bit strings that are long enough to be practical. The\nability to automatically adapt markers according to the usage scenario and the\ndesired capacity as well as the ability to combine information encoding with\nartistic stylization are the unique properties of our approach. As a byproduct,\nour approach provides an insight on the structure of patterns that are most\nsuitable for recognition by ConvNets and on their ability to distinguish\ncomposite patterns.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 14:31:02 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Grinchuk", "Oleg", ""], ["Lebedev", "Vadim", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1610.09278", "submitter": "Ralf Stauder", "authors": "Ralf Stauder, Daniel Ostler, Michael Kranzfelder, Sebastian Koller,\n  Hubertus Feu{\\ss}ner, Nassir Navab", "title": "The TUM LapChole dataset for the M2CAI 2016 workflow challenge", "comments": "5 pages, 2 figures, preliminary reference for published dataset\n  (until larger comparison study of workshop organizers is published)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this technical report we present our collected dataset of laparoscopic\ncholecystectomies (LapChole). Laparoscopic videos of a total of 20 surgeries\nwere recorded and annotated with surgical phase labels, of which 15 were\nrandomly pre-determined as training data, while the remaining 5 videos are\nselected as test data. This dataset was later included as part of the M2CAI\n2016 workflow detection challenge during MICCAI 2016 in Athens.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:36:58 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 14:27:37 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Stauder", "Ralf", ""], ["Ostler", "Daniel", ""], ["Kranzfelder", "Michael", ""], ["Koller", "Sebastian", ""], ["Feu\u00dfner", "Hubertus", ""], ["Navab", "Nassir", ""]]}, {"id": "1610.09334", "submitter": "Seungryul Baek", "authors": "Seungryul Baek, Kwang In Kim, Tae-Kyun Kim", "title": "Real-time Online Action Detection Forests using Spatio-temporal Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online action detection (OAD) is challenging since 1) robust yet\ncomputationally expensive features cannot be straightforwardly used due to the\nreal-time processing requirements and 2) the localization and classification of\nactions have to be performed even before they are fully observed. We propose a\nnew random forest (RF)-based online action detection framework that addresses\nthese challenges. Our algorithm uses computationally efficient skeletal joint\nfeatures. High accuracy is achieved by using robust convolutional neural\nnetwork (CNN)-based features which are extracted from the raw RGBD images, plus\nthe temporal relationships between the current frame of interest, and the past\nand future frames. While these high-quality features are not available in\nreal-time testing scenario, we demonstrate that they can be effectively\nexploited in training RF classifiers: We use these spatio-temporal contexts to\ncraft RF's new split functions improving RFs' leaf node statistics. Experiments\nwith challenging MSRAction3D, G3D, and OAD datasets demonstrate that our\nalgorithm significantly improves the accuracy over the state-of-the-art online\naction detection algorithms while achieving the real-time efficiency of\nexisting skeleton-based RF classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 18:15:31 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Baek", "Seungryul", ""], ["Kim", "Kwang In", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1610.09386", "submitter": "Richard Obermeier", "authors": "Richard Obermeier and Jose Angel Martinez-Lorenzo", "title": "Detecting Breast Cancer using a Compressive Sensing Unmixing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional breast cancer imaging methods using microwave Nearfield Radar\nImaging (NRI) seek to recover the complex permittivity of the tissues at each\nvoxel in the imaging region. This approach is suboptimal, in that it does not\ndirectly consider the permittivity values that healthy and cancerous breast\ntissues typically have. In this paper, we describe a novel unmixing algorithm\nfor detecting breast cancer. In this approach, the breast tissue is separated\ninto three components, low water content (LWC), high water content (HWC), and\ncancerous tissues, and the goal of the optimization procedure is to recover the\nmixture proportions for each component. By utilizing this approach in a hybrid\nDBT / NRI system, the unmixing reconstruction process can be posed as a sparse\nrecovery problem, such that compressive sensing (CS) techniques can be\nemployed. A numerical analysis is performed, which demonstrates that cancerous\nlesions can be detected from their mixture proportion under the appropriate\nconditions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 20:01:55 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Obermeier", "Richard", ""], ["Martinez-Lorenzo", "Jose Angel", ""]]}, {"id": "1610.09414", "submitter": "Iuri Frosio", "authors": "Jingming Dong, Iuri Frosio, Jan Kautz", "title": "Learning Adaptive Parameter Tuning for Image Processing", "comments": null, "journal-ref": "Jinming Dong, Iuri Frosio, Jan Kautz, Learning Adaptive Parameter\n  Tuning for Image Processing, Proc. EI 2018, Image Processing: Algorithms and\n  Systems XVI, Burlingame, USA, 28 Jan - 2 Feb 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-stationary nature of image characteristics calls for adaptive\nprocessing, based on the local image content. We propose a simple and flexible\nmethod to learn local tuning of parameters in adaptive image processing: we\nextract simple local features from an image and learn the relation between\nthese features and the optimal filtering parameters. Learning is performed by\noptimizing a user defined cost function (any image quality metric) on a\ntraining set. We apply our method to three classical problems (denoising,\ndemosaicing and deblurring) and we show the effectiveness of the learned\nparameter modulation strategies. We also show that these strategies are\nconsistent with theoretical results from the literature.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 21:56:52 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 21:18:08 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Dong", "Jingming", ""], ["Frosio", "Iuri", ""], ["Kautz", "Jan", ""]]}, {"id": "1610.09455", "submitter": "Arjun Chaudhuri", "authors": "Arjun Chaudhuri", "title": "Selective De-noising of Sparse-Coloured Images", "comments": "4 pages, 5 figures, International Journal of Computer Science and\n  Information Technologies, ISSN: 0975-9646, March-April, 2016, Website:\n  http://www.ijcsit.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since time immemorial, noise has been a constant source of disturbance to the\nvarious entities known to mankind. Noise models of different kinds have been\ndeveloped to study noise in more detailed fashion over the years. Image\nprocessing, particularly, has extensively implemented several algorithms to\nreduce noise in photographs and pictorial documents to alleviate the effect of\nnoise. Images with sparse colours-lesser number of distinct colours in them-are\ncommon nowadays, especially in astronomy and astrophysics where black and white\ncolours form the main components. Additive noise of Gaussian type is the most\ncommon form of noise to be studied and analysed in majority of communication\nchannels, namely-satellite links, mobile base station to local cellular tower\ncommunication channel,et. al. Most of the time, we encounter images from\nastronomical sources being distorted with noise maximally as they travel long\ndistance from telescopes in outer space to Earth. Considering Additive White\nGaussian Noise(AWGN) to be the common noise in these long distance channels,\nthis paper provides an insight and an algorithmic approach to pixel-specific\nde-noising of sparse-coloured images affected by AWGN. The paper concludes with\nsome essential future avenues and applications of this de-noising method in\nindustry and academia.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 04:59:24 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Chaudhuri", "Arjun", ""]]}, {"id": "1610.09493", "submitter": "Witold Dyrka", "authors": "Jakub Czakon, Filip Drapejkowski, Grzegorz Zurek, Piotr Giedziun,\n  Jacek Zebrowski, Witold Dyrka", "title": "Machine learning methods for accurate delineation of tumors in PET\n  images", "comments": "19th International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI 2016) - PETSEG challenge, Athens, Greece,\n  21/10/2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In oncology, Positron Emission Tomography imaging is widely used in\ndiagnostics of cancer metastases, in monitoring of progress in course of the\ncancer treatment, and in planning radiotherapeutic interventions. Accurate and\nreproducible delineation of the tumor in the Positron Emission Tomography scans\nremains a difficult task, despite being crucial for delivering appropriate\nradiation dose, minimizing adverse side-effects of the therapy, and reliable\nevaluation of treatment. In this piece of research we attempt to solve the\nproblem of automated delineation of the tumor using 3d implementations of the\nspatial distance weighted fuzzy c-means, the deep convolutional neural network\nand a dictionary model. The methods, in diverse ways, combine intensity and\nspatial information.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 11:56:38 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Czakon", "Jakub", ""], ["Drapejkowski", "Filip", ""], ["Zurek", "Grzegorz", ""], ["Giedziun", "Piotr", ""], ["Zebrowski", "Jacek", ""], ["Dyrka", "Witold", ""]]}, {"id": "1610.09498", "submitter": "Sreekanth Madhusoodhanan", "authors": "Sreekanth Madhusoodhanan, Joseph Suresh Paul", "title": "A MAP-MRF filter for phase-sensitive coil combination in autocalibrating\n  partially parallel susceptibility weighted MRI", "comments": "Submitted to IEEE TMI, At the end of the document the rebuttal is\n  added. Expecting comments from other researchers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical approach for combination of channel phases is developed for\noptimizing the Contrast-to-Noise Ratio (CNR) in Susceptibility Weighted Images\n(SWI) acquired using autocalibrating partially parallel techniques. The\nunwrapped phase images of each coil are filtered using local random field based\nprobabilistic weights, derived using energy functions representative of noisy\nsensitivity and tissue information pertaining to venous structure in the\nindividual channel phase images. The channel energy functions are obtained as\nfunctions of local image intensities, first or second order clique phase\ndifference and a threshold scaling parameter dependent on the input noise\nlevel. Whereas the expectation of the individual energy functions with respect\nto the noise distribution in clique phase differences is to be maximized for\noptimal filtering, the expectation of tissue energy function decreases and\nnoise energy function increases with increase in threshold scale parameter. The\noptimum scaling parameter is shown to occur at the point where expectations of\nboth energy functions contribute to the largest possible extent. It is shown\nthat implementation of the filter in the same lines as that of Iterated\nConditional Modes (ICM) algorithm provides structural enhancement in the coil\ncombined phase, with reduced noise amplification. Application to simulated and\nin vivo multi-channel SWI shows that CNR of combined phase obtained using\nMAP-MRF filter is higher as compared to that of coil combination using weighted\naverage.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 12:38:41 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Madhusoodhanan", "Sreekanth", ""], ["Paul", "Joseph Suresh", ""]]}, {"id": "1610.09520", "submitter": "Xudong Ma", "authors": "Xudong Ma", "title": "Multi-Camera Occlusion and Sudden-Appearance-Change Detection Using\n  Hidden Markovian Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper was originally submitted to Xinova as a response to a Request for\nInvention (RFI) on new event monitoring methods. In this paper, a new object\ntracking algorithm using multiple cameras for surveillance applications is\nproposed. The proposed system can detect sudden-appearance-changes and\nocclusions using a hidden Markovian statistical model. The experimental results\nconfirm that our system detect the sudden-appearance changes and occlusions\nreliably.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:54:28 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ma", "Xudong", ""]]}, {"id": "1610.09534", "submitter": "Lan Xu", "authors": "Lan Xu, Lu Fang, Wei Cheng, Kaiwen Guo, Guyue Zhou, Qionghai Dai, and\n  Yebin Liu", "title": "FlyCap: Markerless Motion Capture Using Multiple Autonomous Flying\n  Cameras", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at automatic, convenient and non-instrusive motion capture, this paper\npresents a new generation markerless motion capture technique, the FlyCap\nsystem, to capture surface motions of moving characters using multiple\nautonomous flying cameras (autonomous unmanned aerial vehicles(UAV) each\nintegrated with an RGBD video camera). During data capture, three cooperative\nflying cameras automatically track and follow the moving target who performs\nlarge scale motions in a wide space. We propose a novel non-rigid surface\nregistration method to track and fuse the depth of the three flying cameras for\nsurface motion tracking of the moving target, and simultaneously calculate the\npose of each flying camera. We leverage the using of visual-odometry\ninformation provided by the UAV platform, and formulate the surface tracking\nproblem in a non-linear objective function that can be linearized and\neffectively minimized through a Gaussian-Newton method. Quantitative and\nqualitative experimental results demonstrate the competent and plausible\nsurface and motion reconstruction results\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 15:44:07 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 05:33:30 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 08:30:19 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Xu", "Lan", ""], ["Fang", "Lu", ""], ["Cheng", "Wei", ""], ["Guo", "Kaiwen", ""], ["Zhou", "Guyue", ""], ["Dai", "Qionghai", ""], ["Liu", "Yebin", ""]]}, {"id": "1610.09582", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Ahnaf Masroor, Pavan Turaga", "title": "Diversity Promoting Online Sampling for Streaming Video Summarization", "comments": "Published at ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications benefit from sampling algorithms where a small number of\nwell chosen samples are used to generalize different properties of a large\ndataset. In this paper, we use diverse sampling for streaming video\nsummarization. Several emerging applications support streaming video, but\nexisting summarization algorithms need access to the entire video which\nrequires a lot of memory and computational power. We propose a memory efficient\nand computationally fast, online algorithm that uses competitive learning for\ndiverse sampling. Our algorithm is a generalization of online K-means such that\nthe cost function reduces clustering error, while also ensuring a diverse set\nof samples. The diversity is measured as the volume of a convex hull around the\nsamples. Finally, the performance of the proposed algorithm is measured against\nhuman users for 50 videos in the VSUMM dataset. The algorithm performs better\nthan batch mode summarization, while requiring significantly lower memory and\ncomputational requirements.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 23:51:24 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Anirudh", "Rushil", ""], ["Masroor", "Ahnaf", ""], ["Turaga", "Pavan", ""]]}, {"id": "1610.09585", "submitter": "Augustus Odena", "authors": "Augustus Odena, Christopher Olah, Jonathon Shlens", "title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high resolution photorealistic images has been a long-standing\nchallenge in machine learning. In this paper we introduce new methods for the\nimproved training of generative adversarial networks (GANs) for image\nsynthesis. We construct a variant of GANs employing label conditioning that\nresults in 128x128 resolution image samples exhibiting global coherence. We\nexpand on previous work for image quality assessment to provide two new\nanalyses for assessing the discriminability and diversity of samples from\nclass-conditional image synthesis models. These analyses demonstrate that high\nresolution samples provide class information not present in low resolution\nsamples. Across 1000 ImageNet classes, 128x128 samples are more than twice as\ndiscriminable as artificially resized 32x32 samples. In addition, 84.7% of the\nclasses have samples exhibiting diversity comparable to real ImageNet data.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 00:29:31 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 23:31:57 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 23:22:30 GMT"}, {"version": "v4", "created": "Thu, 20 Jul 2017 20:23:31 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Odena", "Augustus", ""], ["Olah", "Christopher", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1610.09590", "submitter": "Shreenath Dutt Sharma", "authors": "Shreenath Dutt, Ankita Kalra", "title": "A Scalable and Robust Framework for Intelligent Real-time Video\n  Surveillance", "comments": "4 pages, 3 figures, Presented in International Conference on Advances\n  in Computing, Communications and Informatics (ICACCI-2016), September 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an intelligent, reliable and storage-efficient\nvideo surveillance system using Apache Storm and OpenCV. As a Storm topology,\nwe have added multiple information extraction modules that only write important\ncontent to the disk. Our topology is extensible, capable of adding novel\nalgorithms as per the use case without affecting the existing ones, since all\nthe processing is independent of each other. This framework is also highly\nscalable and fault tolerant, which makes it a best option for organisations\nthat need to monitor a large network of surveillance cameras.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 01:22:52 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Dutt", "Shreenath", ""], ["Kalra", "Ankita", ""]]}, {"id": "1610.09609", "submitter": "Keyu Lu", "authors": "Keyu Lu and Jian Li and Xiangjing An and Hangen He", "title": "Generalized Haar Filter based Deep Networks for Real-Time Object\n  Detection in Traffic Scene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based object detection is one of the fundamental functions in numerous\ntraffic scene applications such as self-driving vehicle systems and advance\ndriver assistance systems (ADAS). However, it is also a challenging task due to\nthe diversity of traffic scene and the storage, power and computing source\nlimitations of the platforms for traffic scene applications. This paper\npresents a generalized Haar filter based deep network which is suitable for the\nobject detection tasks in traffic scene. In this approach, we first decompose a\nobject detection task into several easier local regression tasks. Then, we\nhandle the local regression tasks by using several tiny deep networks which\nsimultaneously output the bounding boxes, categories and confidence scores of\ndetected objects. To reduce the consumption of storage and computing resources,\nthe weights of the deep networks are constrained to the form of generalized\nHaar filter in training phase. Additionally, we introduce the strategy of\nsparse windows generation to improve the efficiency of the algorithm. Finally,\nwe perform several experiments to validate the performance of our proposed\napproach. Experimental results demonstrate that the proposed approach is both\nefficient and effective in traffic scene compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 07:02:57 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Keyu", ""], ["Li", "Jian", ""], ["An", "Xiangjing", ""], ["He", "Hangen", ""]]}, {"id": "1610.09615", "submitter": "Amir Adler", "authors": "Amir Adler, Michael Elad and Michael Zibulevsky", "title": "Compressed Learning: A Deep Neural Network Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Learning (CL) is a joint signal processing and machine learning\nframework for inference from a signal, using a small number of measurements\nobtained by linear projections of the signal. In this paper we present an\nend-to-end deep learning approach for CL, in which a network composed of\nfully-connected layers followed by convolutional layers perform the linear\nsensing and non-linear inference stages. During the training phase, the sensing\nmatrix and the non-linear inference operator are jointly optimized, and the\nproposed approach outperforms state-of-the-art for the task of image\nclassification. For example, at a sensing rate of 1% (only 8 measurements of 28\nX 28 pixels images), the classification error for the MNIST handwritten digits\ndataset is 6.46% compared to 41.06% with state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 07:54:19 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Adler", "Amir", ""], ["Elad", "Michael", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1610.09625", "submitter": "Daniel Harari", "authors": "Shimon Ullman, Nimrod Dorfman, Daniel Harari", "title": "Discovering containment: from infants to machines", "comments": null, "journal-ref": "Cognition 183 (2019) 67-81", "doi": "10.1016/j.cognition.2018.11.001", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current artificial learning systems can recognize thousands of visual\ncategories, or play Go at a champion\"s level, but cannot explain infants\nlearning, in particular the ability to learn complex concepts without guidance,\nin a specific order. A notable example is the category of 'containers' and the\nnotion of containment, one of the earliest spatial relations to be learned,\nstarting already at 2.5 months, and preceding other common relations (e.g.,\nsupport). Such spontaneous unsupervised learning stands in contrast with\ncurrent highly successful computational models, which learn in a supervised\nmanner, that is, by using large data sets of labeled examples. How can\nmeaningful concepts be learned without guidance, and what determines the\ntrajectory of infant learning, making some notions appear consistently earlier\nthan others?\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 10:26:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ullman", "Shimon", ""], ["Dorfman", "Nimrod", ""], ["Harari", "Daniel", ""]]}, {"id": "1610.09645", "submitter": "Shicong Liu", "authors": "Shicong Liu, Hongtao Lu", "title": "Accurate Deep Representation Quantization with Gradient Snapping Layer\n  for Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advance of large scale similarity search involves using deeply learned\nrepresentations to improve the search accuracy and use vector quantization\nmethods to increase the search speed. However, how to learn deep\nrepresentations that strongly preserve similarities between data pairs and can\nbe accurately quantized via vector quantization remains a challenging task.\nExisting methods simply leverage quantization loss and similarity loss, which\nresult in unexpectedly biased back-propagating gradients and affect the search\nperformances. To this end, we propose a novel gradient snapping layer (GSL) to\ndirectly regularize the back-propagating gradient towards a neighboring\ncodeword, the generated gradients are un-biased for reducing similarity loss\nand also propel the learned representations to be accurately quantized. Joint\ndeep representation and vector quantization learning can be easily performed by\nalternatively optimize the quantization codebook and the deep neural network.\nThe proposed framework is compatible with various existing vector quantization\napproaches. Experimental results demonstrate that the proposed framework is\neffective, flexible and outperforms the state-of-the-art large scale similarity\nsearch methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 13:24:54 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Liu", "Shicong", ""], ["Lu", "Hongtao", ""]]}, {"id": "1610.09652", "submitter": "Kaihua Zhang", "authors": "Kaihua Zhang, Qingshan Liu, and Ming-Hsuan Yang", "title": "Visual Tracking via Boolean Map Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple yet effective Boolean map based\nrepresentation that exploits connectivity cues for visual tracking. We describe\na target object with histogram of oriented gradients and raw color features, of\nwhich each one is characterized by a set of Boolean maps generated by uniformly\nthresholding their values. The Boolean maps effectively encode multi-scale\nconnectivity cues of the target with different granularities. The fine-grained\nBoolean maps capture spatially structural details that are effective for\nprecise target localization while the coarse-grained ones encode global shape\ninformation that are robust to large target appearance variations. Finally, all\nthe Boolean maps form together a robust representation that can be approximated\nby an explicit feature map of the intersection kernel, which is fed into a\nlogistic regression classifier with online update, and the target location is\nestimated within a particle filter framework. The proposed representation\nscheme is computationally efficient and facilitates achieving favorable\nperformance in terms of accuracy and robustness against the state-of-the-art\ntracking methods on a large benchmark dataset of 50 image sequences.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 14:17:05 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Zhang", "Kaihua", ""], ["Liu", "Qingshan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1610.09712", "submitter": "Paolo Di Febbo", "authors": "Paolo Di Febbo, Stefano Mattoccia, Carlo Dal Mutto", "title": "Real-Time Image Distortion Correction: Analysis and Evaluation of\n  FPGA-Compatible Algorithms", "comments": "To be published in Proceedings of the International Conference on\n  Reconfigurable Computing and FPGAs, Cancun, Mexico, 30 November, 2 December\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image distortion correction is a critical pre-processing step for a variety\nof computer vision and image processing algorithms. Standard real-time software\nimplementations are generally not suited for direct hardware porting, so\nappropriated versions need to be designed in order to obtain implementations\ndeployable on FPGAs. In this paper, hardware-compatible techniques for image\ndistortion correction are introduced and analyzed in details. The considered\nsolutions are compared in terms of output quality by using a\ngeometrical-error-based approach, with particular emphasis on robustness with\nrespect to increasing lens distortion. The required amount of hardware\nresources is also estimated for each considered approach.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 21:14:15 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Di Febbo", "Paolo", ""], ["Mattoccia", "Stefano", ""], ["Mutto", "Carlo Dal", ""]]}, {"id": "1610.09736", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Junhong Min, Jong Chul Ye", "title": "A deep convolutional neural network using directional wavelets for\n  low-dose X-ray CT reconstruction", "comments": "Will appear in Medical Physics (invited paper); 2016 AAPM low-dose CT\n  Grand Challenge 2nd Place Award", "journal-ref": null, "doi": "10.1002/mp.12344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the potential risk of inducing cancers, radiation dose of X-ray CT\nshould be reduced for routine patient scanning. However, in low-dose X-ray CT,\nsevere artifacts usually occur due to photon starvation, beamhardening, etc,\nwhich decrease the reliability of diagnosis. Thus, high quality reconstruction\nfrom low-dose X-ray CT data has become one of the important research topics in\nCT community. Conventional model-based denoising approaches are, however,\ncomputationally very expensive, and image domain denoising approaches hardly\ndeal with CT specific noise patterns. To address these issues, we propose an\nalgorithm using a deep convolutional neural network (CNN), which is applied to\nwavelet transform coefficients of low-dose CT images. Specifically, by using a\ndirectional wavelet transform for extracting directional component of artifacts\nand exploiting the intra- and inter-band correlations, our deep network can\neffectively suppress CT specific noises. Moreover, our CNN is designed to have\nvarious types of residual learning architecture for faster network training and\nbetter denoising. Experimental results confirm that the proposed algorithm\neffectively removes complex noise patterns of CT images, originated from the\nreduced X-ray dose. In addition, we show that wavelet domain CNN is efficient\nin removing the noises from low-dose CT compared to an image domain CNN. Our\nresults were rigorously evaluated by several radiologists and won the second\nplace award in 2016 AAPM Low-Dose CT Grand Challenge. To the best of our\nknowledge, this work is the first deep learning architecture for low-dose CT\nreconstruction that has been rigorously evaluated and proven for its efficacy.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 00:02:56 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 00:35:21 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 14:26:48 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kang", "Eunhee", ""], ["Min", "Junhong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1610.09766", "submitter": "Ian Cheong", "authors": "Muthukaruppan Swaminathan, Pankaj Kumar Yadav, Obdulio Piloto, Tobias\n  Sj\\\"oblom and Ian Cheong", "title": "A New Distance Measure for Non-Identical Data with Application to Image\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2016.10.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance measures are part and parcel of many computer vision algorithms. The\nunderlying assumption in all existing distance measures is that feature\nelements are independent and identically distributed. However, in real-world\nsettings, data generally originate from heterogeneous sources even if they do\npossess a common data-generating mechanism. Since these sources are not\nidentically distributed by necessity, the assumption of identical distribution\nis inappropriate. Here, we use statistical analysis to show that feature\nelements of local image descriptors are indeed non-identically distributed. To\ntest the effect of omitting the unified distribution assumption, we created a\nnew distance measure called the Poisson-Binomial Radius (PBR). PBR is a\nbin-to-bin distance which accounts for the dispersion of bin-to-bin\ninformation. PBR's performance was evaluated on twelve benchmark data sets\ncovering six different classification and recognition applications: texture,\nmaterial, leaf, scene, ear biometrics and category-level image classification.\nResults from these experiments demonstrate that PBR outperforms\nstate-of-the-art distance measures for most of the data sets and achieves\ncomparable performance on the rest, suggesting that accounting for different\ndistributions in distance measures can improve performance in classification\nand recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 03:04:48 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Swaminathan", "Muthukaruppan", ""], ["Yadav", "Pankaj Kumar", ""], ["Piloto", "Obdulio", ""], ["Sj\u00f6blom", "Tobias", ""], ["Cheong", "Ian", ""]]}, {"id": "1610.09816", "submitter": "Qin Zou", "authors": "Qin Zou, Lihao Ni, Qian Wang, Qingquan Li, and Song Wang", "title": "Robust Gait Recognition by Integrating Inertial and RGBD Sensors", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics, vol. 48, no. 4, pp. 1136-1150,\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait has been considered as a promising and unique biometric for person\nidentification. Traditionally, gait data are collected using either color\nsensors, such as a CCD camera, depth sensors, such as a Microsoft Kinect, or\ninertial sensors, such as an accelerometer. However, a single type of sensors\nmay only capture part of the dynamic gait features and make the gait\nrecognition sensitive to complex covariate conditions, leading to fragile\ngait-based person identification systems. In this paper, we propose to combine\nall three types of sensors for gait data collection and gait recognition, which\ncan be used for important identification applications, such as identity\nrecognition to access a restricted building or area. We propose two new\nalgorithms, namely EigenGait and TrajGait, to extract gait features from the\ninertial data and the RGBD (color and depth) data, respectively. Specifically,\nEigenGait extracts general gait dynamics from the accelerometer readings in the\neigenspace and TrajGait extracts more detailed sub-dynamics by analyzing 3D\ndense trajectories. Finally, both extracted features are fed into a supervised\nclassifier for gait recognition and person identification. Experiments on 50\nsubjects, with comparisons to several other state-of-the-art gait-recognition\napproaches, show that the proposed approach can achieve higher recognition\naccuracy and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 08:06:50 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zou", "Qin", ""], ["Ni", "Lihao", ""], ["Wang", "Qian", ""], ["Li", "Qingquan", ""], ["Wang", "Song", ""]]}, {"id": "1610.09908", "submitter": "Hendrik Dirks", "authors": "Hendrik Dirks", "title": "Joint Large-Scale Motion Estimation and Image Reconstruction", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the implementation of the joint motion estimation and\nimage reconstruction framework presented by Burger, Dirks and Sch\\\"onlieb and\nextends this framework to large-scale motion between consecutive image frames.\nThe variational framework uses displacements between consecutive frames based\non the optical flow approach to improve the image reconstruction quality on the\none hand and the motion estimation quality on the other. The energy functional\nconsists of a data-fidelity term with a general operator that connects the\ninput sequence to the solution, it has a total variation term for the image\nsequence and is connected to the underlying flow using an optical flow term.\nAdditional spatial regularity for the flow is modeled by a total variation\nregularizer for both components of the flow. The numerical minimization is\nperformed in an alternating manner using primal-dual techniques. The resulting\nschemes are presented as pseudo-code together with a short numerical\nevaluation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 13:16:38 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Dirks", "Hendrik", ""]]}, {"id": "1610.10033", "submitter": "Pia Bideau", "authors": "Pia Bideau and Erik Learned-Miller", "title": "A Detailed Rubric for Motion Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion segmentation is currently an active area of research in computer\nVision. The task of comparing different methods of motion segmentation is\ncomplicated by the fact that researchers may use subtly different definitions\nof the problem. Questions such as \"Which objects are moving?\", \"What is\nbackground?\", and \"How can we use motion of the camera to segment objects,\nwhether they are static or moving?\" are clearly related to each other, but lead\nto different algorithms, and imply different versions of the ground truth. This\nreport has two goals. The first is to offer a precise definition of motion\nsegmentation so that the intent of an algorithm is as well-defined as possible.\nThe second is to report on new versions of three previously existing data sets\nthat are compatible with this definition. We hope that this more detailed\ndefinition, and the three data sets that go with it, will allow more meaningful\ncomparisons of certain motion segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 17:57:23 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Bideau", "Pia", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1610.10042", "submitter": "Serge Dmitrieff", "authors": "Serge Dmitrieff, Fran\\c{c}ois N\\'ed\\'elec", "title": "ConfocalGN : a minimalistic confocal image simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SUMMARY : We developed a user-friendly software to generate synthetic\nconfocal microscopy images from a ground truth specified as a 3D bitmap with\npixels of arbitrary size. The software can analyze a real confocal stack to\nderivate noise parameters and will use them directly to generate new images\nwith similar noise characteristics. Such synthetic images can then be used to\nassert the quality and robustness of an image analysis pipeline, as well as be\nused to train machine-learning image analysis procedures. We illustrate the\napproach with closed curves corresponding to the microtubule ring present in\nblood platelet. AVAILABILITY AND IMPLEMENTATION: ConfocalGN is written in\nMatlab but does not require any toolbox. The source code is distributed under\nthe GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:08:54 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 13:50:11 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Dmitrieff", "Serge", ""], ["N\u00e9d\u00e9lec", "Fran\u00e7ois", ""]]}, {"id": "1610.10048", "submitter": "Arulkumar Subramaniam", "authors": "Arulkumar Subramaniam, Vismay Patel, Ashish Mishra, Prashanth\n  Balasubramanian, Anurag Mittal", "title": "Bi-modal First Impressions Recognition using Temporally Ordered Deep\n  Audio and Stochastic Visual Features", "comments": "to be published in: ECCV 2016 Workshops proceedings (Apparent\n  Personality Analysis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for First Impressions Recognition in terms of the\nBig Five personality-traits from short videos. The Big Five personality traits\nis a model to describe human personality using five broad categories:\nExtraversion, Agreeableness, Conscientiousness, Neuroticism and Openness. We\ntrain two bi-modal end-to-end deep neural network architectures using\ntemporally ordered audio and novel stochastic visual features from few frames,\nwithout over-fitting. We empirically show that the trained models perform\nexceptionally well, even after training from a small sub-portions of inputs.\nOur method is evaluated in ChaLearn LAP 2016 Apparent Personality Analysis\n(APA) competition using ChaLearn LAP APA2016 dataset and achieved excellent\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:21:13 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Subramaniam", "Arulkumar", ""], ["Patel", "Vismay", ""], ["Mishra", "Ashish", ""], ["Balasubramanian", "Prashanth", ""], ["Mittal", "Anurag", ""]]}]