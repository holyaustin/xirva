[{"id": "0810.2311", "submitter": "Nikolaos Vasiloglou", "authors": "Nikolaos Vasiloglou, Alexander G. Gray, David V. Anderson", "title": "Non-Negative Matrix Factorization, Convexity and Isometry", "comments": "accpepted in SIAM Data Mining 2009, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we explore avenues for improving the reliability of\ndimensionality reduction methods such as Non-Negative Matrix Factorization\n(NMF) as interpretive exploratory data analysis tools. We first explore the\ndifficulties of the optimization problem underlying NMF, showing for the first\ntime that non-trivial NMF solutions always exist and that the optimization\nproblem is actually convex, by using the theory of Completely Positive\nFactorization. We subsequently explore four novel approaches to finding\nglobally-optimal NMF solutions using various ideas from convex optimization. We\nthen develop a new method, isometric NMF (isoNMF), which preserves\nnon-negativity while also providing an isometric embedding, simultaneously\nachieving two properties which are helpful for interpretation. Though it\nresults in a more difficult optimization problem, we show experimentally that\nthe resulting method is scalable and even achieves more compact spectra than\nstandard NMF.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2008 20:43:24 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2009 16:05:22 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Vasiloglou", "Nikolaos", ""], ["Gray", "Alexander G.", ""], ["Anderson", "David V.", ""]]}, {"id": "0810.2434", "submitter": "Edward Rosten", "authors": "Edward Rosten, Reid Porter, Tom Drummond", "title": "Faster and better: a machine learning approach to corner detection", "comments": "35 pages, 11 figures", "journal-ref": "IEEE Trans. PAMI, 32 (2010), 105--119", "doi": "10.1109/TPAMI.2008.275", "report-no": "07-3912", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The repeatability and efficiency of a corner detector determines how likely\nit is to be useful in a real-world application. The repeatability is importand\nbecause the same scene viewed from different positions should yield features\nwhich correspond to the same real-world 3D locations [Schmid et al 2000]. The\nefficiency is important because this determines whether the detector combined\nwith further processing can operate at frame rate.\n  Three advances are described in this paper. First, we present a new heuristic\nfor feature detection, and using machine learning we derive a feature detector\nfrom this which can fully process live PAL video using less than 5% of the\navailable processing time. By comparison, most other detectors cannot even\noperate at frame rate (Harris detector 115%, SIFT 195%). Second, we generalize\nthe detector, allowing it to be optimized for repeatability, with little loss\nof efficiency. Third, we carry out a rigorous comparison of corner detectors\nbased on the above repeatability criterion applied to 3D scenes. We show that\ndespite being principally constructed for speed, on these stringent tests, our\nheuristic detector significantly outperforms existing feature detectors.\nFinally, the comparison demonstrates that using machine learning produces\nsignificant improvements in repeatability, yielding a detector that is both\nvery fast and very high quality.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2008 14:22:05 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Rosten", "Edward", ""], ["Porter", "Reid", ""], ["Drummond", "Tom", ""]]}, {"id": "0810.3418", "submitter": "Elka Korutcheva", "authors": "K.Koroutchev and E. Korutcheva", "title": "Detecting the Most Unusual Part of a Digital Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce an algorithm that can detect the\nmost unusual part of a digital image. The most unusual part of a given shape is\ndefined as a part of the image that has the maximal distance to all non\nintersecting shapes with the same form.\n  The method can be used to scan image databases with no clear model of the\ninteresting part or large image databases, as for example medical databases.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2008 18:04:51 GMT"}], "update_date": "2008-10-21", "authors_parsed": [["Koroutchev", "K.", ""], ["Korutcheva", "E.", ""]]}, {"id": "0810.3579", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (GREYC), Luc Brun (GREYC)", "title": "Hierarchical Bag of Paths for Kernel Based Shape Classification", "comments": null, "journal-ref": "Joint IAPR International Workshops on Structural and Syntactic\n  Pattern Recognition (SSPR 2008), Orlando : \\'Etats-Unis d'Am\\'erique (2008)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph kernels methods are based on an implicit embedding of graphs within a\nvector space of large dimension. This implicit embedding allows to apply to\ngraphs methods which where until recently solely reserved to numerical data.\nWithin the shape classification framework, graphs are often produced by a\nskeletonization step which is sensitive to noise. We propose in this paper to\nintegrate the robustness to structural noise by using a kernel based on a bag\nof path where each path is associated to a hierarchy encoding successive\nsimplifications of the path. Several experiments prove the robustness and the\nflexibility of our approach compared to alternative shape classification\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2008 15:13:18 GMT"}], "update_date": "2008-10-21", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "GREYC"], ["Brun", "Luc", "", "GREYC"]]}, {"id": "0810.3851", "submitter": "David W. Hogg", "authors": "David W. Hogg (NYU), Dustin Lang (Toronto)", "title": "Astronomical imaging: The theory of everything", "comments": "a talk given at \"Classification and Discovery in Large Astronomical\n  Surveys\", Ringberg Castle, 2008-10-16", "journal-ref": null, "doi": "10.1063/1.3059072", "report-no": null, "categories": "astro-ph cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are developing automated systems to provide homogeneous calibration\nmeta-data for heterogeneous imaging data, using the pixel content of the image\nalone where necessary. Standardized and complete calibration meta-data permit\ngenerative modeling: A good model of the sky through wavelength and time--that\nis, a model of the positions, motions, spectra, and variability of all stellar\nsources, plus an intensity map of all cosmological sources--could synthesize or\ngenerate any astronomical image ever taken at any time with any equipment in\nany configuration. We argue that the best-fit or highest likelihood model of\nthe data is also the best possible astronomical catalog constructed from those\ndata. A generative model or catalog of this form is the best possible platform\nfor automated discovery, because it is capable of identifying informative\nfailures of the model in new data at the pixel level, or as statistical\nanomalies in the joint distribution of residuals from many images. It is also,\nin some sense, an astronomer's \"theory of everything\".\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2008 14:47:38 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Hogg", "David W.", "", "NYU"], ["Lang", "Dustin", "", "Toronto"]]}, {"id": "0810.4401", "submitter": "Nic Schraudolph", "authors": "Nicol N. Schraudolph and Dmitry Kamenetsky", "title": "Efficient Exact Inference in Planar Ising Models", "comments": "Fixed a number of bugs in v1; added 10 pages of additional figures,\n  explanations, proofs, and experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give polynomial-time algorithms for the exact computation of lowest-energy\n(ground) states, worst margin violators, log partition functions, and marginal\nedge probabilities in certain binary undirected graphical models. Our approach\nprovides an interesting alternative to the well-known graph cut paradigm in\nthat it does not impose any submodularity constraints; instead we require\nplanarity to establish a correspondence with perfect matchings (dimer\ncoverings) in an expanded dual graph. We implement a unified framework while\ndelegating complex but well-understood subproblems (planar embedding,\nmaximum-weight perfect matching) to established algorithms for which efficient\nimplementations are freely available. Unlike graph cut methods, we can perform\npenalized maximum-likelihood as well as maximum-margin parameter estimation in\nthe associated conditional random fields (CRFs), and employ marginal posterior\nprobabilities as well as maximum a posteriori (MAP) states for prediction.\nMaximum-margin CRF parameter estimation on image denoising and segmentation\nproblems shows our approach to be efficient and effective. A C++ implementation\nis available from http://nic.schraudolph.org/isinf/\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2008 08:49:09 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2008 06:47:01 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Schraudolph", "Nicol N.", ""], ["Kamenetsky", "Dmitry", ""]]}, {"id": "0810.4426", "submitter": "Edward Rosten", "authors": "Edward Rosten, Rohan Loveland", "title": "Camera distortion self-calibration using the plumb-line constraint and\n  minimal Hough entropy", "comments": "9 pages, 5 figures Corrected errors in equation 18", "journal-ref": null, "doi": "10.1007/s00138-009-0196-9", "report-no": "08-2665", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a simple and robust method for self-correction of\ncamera distortion using single images of scenes which contain straight lines.\nSince the most common distortion can be modelled as radial distortion, we\nillustrate the method using the Harris radial distortion model, but the method\nis applicable to any distortion model. The method is based on transforming the\nedgels of the distorted image to a 1-D angular Hough space, and optimizing the\ndistortion correction parameters which minimize the entropy of the\ncorresponding normalized histogram. Properly corrected imagery will have fewer\ncurved lines, and therefore less spread in Hough space. Since the method does\nnot rely on any image structure beyond the existence of edgels sharing some\ncommon orientations and does not use edge fitting, it is applicable to a wide\nvariety of image types. For instance, it can be applied equally well to images\nof texture with weak but dominant orientations, or images with strong vanishing\npoints. Finally, the method is performed on both synthetic and real data\nrevealing that it is particularly robust to noise.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2008 10:50:59 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2009 11:49:44 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Rosten", "Edward", ""], ["Loveland", "Rohan", ""]]}, {"id": "0810.4617", "submitter": "Effrosyni Kokiopoulou", "authors": "Effrosyni Kokiopoulou and Pascal Frossard", "title": "Graph-based classification of multiple observation sets", "comments": "New content added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classification of an object given multiple\nobservations that possibly include different transformations. The possible\ntransformations of the object generally span a low-dimensional manifold in the\noriginal signal space. We propose to take advantage of this manifold structure\nfor the effective classification of the object represented by the observation\nset. In particular, we design a low complexity solution that is able to exploit\nthe properties of the data manifolds with a graph-based algorithm. Hence, we\nformulate the computation of the unknown label matrix as a smoothing process on\nthe manifold under the constraint that all observations represent an object of\none single class. It results into a discrete optimization problem, which can be\nsolved by an efficient and low complexity algorithm. We demonstrate the\nperformance of the proposed graph-based algorithm in the classification of sets\nof multiple images. Moreover, we show its high potential in video-based face\nrecognition, where it outperforms state-of-the-art solutions that fall short of\nexploiting the manifold structure of the face image data sets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2008 16:02:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2009 09:35:33 GMT"}], "update_date": "2009-07-27", "authors_parsed": [["Kokiopoulou", "Effrosyni", ""], ["Frossard", "Pascal", ""]]}, {"id": "0810.5325", "submitter": "Effrosyni Kokiopoulou", "authors": "R. Sala Llonch, E. Kokiopoulou, I. Tosic and P. Frossard", "title": "3D Face Recognition with Sparse Spherical Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D face recognition using simultaneous\nsparse approximations on the sphere. The 3D face point clouds are first aligned\nwith a novel and fully automated registration process. They are then\nrepresented as signals on the 2D sphere in order to preserve depth and geometry\ninformation. Next, we implement a dimensionality reduction process with\nsimultaneous sparse approximations and subspace projection. It permits to\nrepresent each 3D face by only a few spherical functions that are able to\ncapture the salient facial characteristics, and hence to preserve the\ndiscriminant facial information. We eventually perform recognition by effective\nmatching in the reduced space, where Linear Discriminant Analysis can be\nfurther activated for improved recognition performance. The 3D face recognition\nalgorithm is evaluated on the FRGC v.1.0 data set, where it is shown to\noutperform classical state-of-the-art solutions that work with depth images.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2008 17:43:54 GMT"}], "update_date": "2008-10-30", "authors_parsed": [["Llonch", "R. Sala", ""], ["Kokiopoulou", "E.", ""], ["Tosic", "I.", ""], ["Frossard", "P.", ""]]}, {"id": "0810.5573", "submitter": "David Correa Martins Jr", "authors": "Marcelo Ris, Junior Barrera, David C. Martins Jr", "title": "A branch-and-bound feature selection algorithm for U-shaped cost\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents the formulation of a combinatorial optimization problem\nwith the following characteristics: i.the search space is the power set of a\nfinite set structured as a Boolean lattice; ii.the cost function forms a\nU-shaped curve when applied to any lattice chain. This formulation applies for\nfeature selection in the context of pattern recognition. The known approaches\nfor this problem are branch-and-bound algorithms and heuristics, that explore\npartially the search space. Branch-and-bound algorithms are equivalent to the\nfull search, while heuristics are not. This paper presents a branch-and-bound\nalgorithm that differs from the others known by exploring the lattice structure\nand the U-shaped chain curves of the search space. The main contribution of\nthis paper is the architecture of this algorithm that is based on the\nrepresentation and exploration of the search space by new lattice properties\nproven here. Several experiments, with well known public data, indicate the\nsuperiority of the proposed method to SFFS, which is a popular heuristic that\ngives good results in very short computational time. In all experiments, the\nproposed method got better or equal results in similar or even smaller\ncomputational time.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2008 20:24:28 GMT"}], "update_date": "2008-11-03", "authors_parsed": [["Ris", "Marcelo", ""], ["Barrera", "Junior", ""], ["Martins", "David C.", "Jr"]]}]