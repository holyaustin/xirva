[{"id": "0901.0065", "submitter": "Alireza Avanaki", "authors": "Alireza Avanaki", "title": "Exact Histogram Specification Optimized for Structural Similarity", "comments": null, "journal-ref": null, "doi": "10.1007/s10043-009-0119-z", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exact histogram specification (EHS) method modifies its input image to\nhave a specified histogram. Applications of EHS include image (contrast)\nenhancement (e.g., by histogram equalization) and histogram watermarking.\nPerforming EHS on an image, however, reduces its visual quality. Starting from\nthe output of a generic EHS method, we maximize the structural similarity index\n(SSIM) between the original image (before EHS) and the result of EHS\niteratively. Essential in this process is the computationally simple and\naccurate formula we derive for SSIM gradient. As it is based on gradient\nascent, the proposed EHS always converges. Experimental results confirm that\nwhile obtaining the histogram exactly as specified, the proposed method\ninvariably outperforms the existing methods in terms of visual quality of the\nresult. The computational complexity of the proposed method is shown to be of\nthe same order as that of the existing methods.\n  Index terms: histogram modification, histogram equalization, optimization for\nperceptual visual quality, structural similarity gradient ascent, histogram\nwatermarking, contrast enhancement.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2008 06:13:39 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Avanaki", "Alireza", ""]]}, {"id": "0901.0760", "submitter": "Marco Duarte", "authors": "Mark A. Davenport, Chinmay Hegde, Marco F. Duarte, and Richard G.\n  Baraniuk", "title": "A Theoretical Analysis of Joint Manifolds", "comments": "24 pages, 4 figures. Corrected typo on grant number in\n  acknowledgements, page 1", "journal-ref": null, "doi": null, "report-no": "TREE0901, Department of Electrical and Computer Engineering, Rice\n  University", "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of low-cost sensor architectures for diverse modalities has\nmade it possible to deploy sensor arrays that capture a single event from a\nlarge number of vantage points and using multiple modalities. In many\nscenarios, these sensors acquire very high-dimensional data such as audio\nsignals, images, and video. To cope with such high-dimensional data, we\ntypically rely on low-dimensional models. Manifold models provide a\nparticularly powerful model that captures the structure of high-dimensional\ndata when it is governed by a low-dimensional set of parameters. However, these\nmodels do not typically take into account dependencies among multiple sensors.\nWe thus propose a new joint manifold framework for data ensembles that exploits\nsuch dependencies. We show that simple algorithms can exploit the joint\nmanifold structure to improve their performance on standard signal processing\napplications. Additionally, recent results concerning dimensionality reduction\nfor manifolds enable us to formulate a network-scalable data compression scheme\nthat uses random projections of the sensed data. This scheme efficiently fuses\nthe data from all sensors through the addition of such projections, regardless\nof the data modalities and dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2009 06:47:47 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2009 07:18:30 GMT"}], "update_date": "2009-12-09", "authors_parsed": [["Davenport", "Mark A.", ""], ["Hegde", "Chinmay", ""], ["Duarte", "Marco F.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "0901.2954", "submitter": "Kenichi Horie", "authors": "Kenichi Horie", "title": "An Upper Limit of AC Huffman Code Length in JPEG Compression", "comments": "US patent application 11/947936", "journal-ref": null, "doi": null, "report-no": "OIMC07P03556", "categories": "cs.IT cs.CC cs.CE cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strategy for computing upper code-length limits of AC Huffman codes for an\n8x8 block in JPEG Baseline coding is developed. The method is based on a\ngeometric interpretation of the DCT, and the calculated limits are as close as\n14% to the maximum code-lengths. The proposed strategy can be adapted to other\ntransform coding methods, e.g., MPEG 2 and 4 video compressions, to calculate\nclose upper code length limits for the respective processing blocks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 23:19:00 GMT"}], "update_date": "2009-01-21", "authors_parsed": [["Horie", "Kenichi", ""]]}, {"id": "0901.3590", "submitter": "Chunhua Shen", "authors": "Chunhua Shen and Hanxi Li", "title": "On the Dual Formulation of Boosting Algorithms", "comments": "16 pages. To publish/Published in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 2010", "journal-ref": null, "doi": "10.1109/TPAMI.2010.47", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study boosting algorithms from a new perspective. We show that the\nLagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with\ngeneralized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of\nboosting algorithms can be understood in terms of maintaining a better margin\ndistribution by maximizing margins and at the same time controlling the margin\nvariance.We also theoretically prove that, approximately, AdaBoost maximizes\nthe average margin, instead of the minimum margin. The duality formulation also\nenables us to develop column generation based optimization algorithms, which\nare totally corrective. We show that they exhibit almost identical\nclassification results to that of standard stage-wise additive boosting\nalgorithms but with much faster convergence rates. Therefore fewer weak\nclassifiers are needed to build the ensemble using our proposed optimization\ntechnique.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 02:14:42 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2009 04:02:54 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2009 04:54:15 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2009 02:31:35 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Shen", "Chunhua", ""], ["Li", "Hanxi", ""]]}, {"id": "0901.3762", "submitter": "Rubab Khan", "authors": "Rubab Khan, Shourov Chatterji", "title": "Enhancing the capabilities of LIGO time-frequency plane searches through\n  clustering", "comments": "17 pages, 6 figures. Submitted to CQG on Dec 12, 2008; accepted on\n  June 18, 2009", "journal-ref": "Class.Quant.Grav.26:155009,2009", "doi": "10.1088/0264-9381/26/15/155009", "report-no": null, "categories": "gr-qc astro-ph.IM cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One class of gravitational wave signals LIGO is searching for consists of\nshort duration bursts of unknown waveforms. Potential sources include core\ncollapse supernovae, gamma ray burst progenitors, and mergers of binary black\nholes or neutron stars. We present a density-based clustering algorithm to\nimprove the performance of time-frequency searches for such gravitational-wave\nbursts when they are extended in time and/or frequency, and not sufficiently\nwell known to permit matched filtering. We have implemented this algorithm as\nan extension to the QPipeline, a gravitational-wave data analysis pipeline for\nthe detection of bursts, which currently determines the statistical\nsignificance of events based solely on the peak significance observed in\nminimum uncertainty regions of the time-frequency plane. Density based\nclustering improves the performance of such a search by considering the\naggregate significance of arbitrarily shaped regions in the time-frequency\nplane and rejecting the isolated minimum uncertainty features expected from the\nbackground detector noise. In this paper, we present test results for simulated\nsignals and demonstrate that density based clustering improves the performance\nof the QPipeline for signals extended in time and/or frequency.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 20:59:45 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2009 21:41:46 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2009 20:12:08 GMT"}], "update_date": "2009-07-22", "authors_parsed": [["Khan", "Rubab", ""], ["Chatterji", "Shourov", ""]]}, {"id": "0901.3923", "submitter": "Jayant Gupchup A", "authors": "Jayant Gupchup, Andreas Terzis, Randal Burns and Alex Szalay", "title": "Model-Based Event Detection in Wireless Sensor Networks", "comments": null, "journal-ref": "Workshop for Data Sharing and Interoperability on the World Wide\n  Web (DSI 2007). April 2007, In Proceedings", "doi": null, "report-no": null, "categories": "cs.NI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an application of techniques from statistical signal\nprocessing to the problem of event detection in wireless sensor networks used\nfor environmental monitoring. The proposed approach uses the well-established\nPrincipal Component Analysis (PCA) technique to build a compact model of the\nobserved phenomena that is able to capture daily and seasonal trends in the\ncollected measurements. We then use the divergence between actual measurements\nand model predictions to detect the existence of discrete events within the\ncollected data streams. Our preliminary results show that this event detection\nmechanism is sensitive enough to detect the onset of rain events using the\ntemperature modality of a wireless sensor network.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2009 21:36:27 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Gupchup", "Jayant", ""], ["Terzis", "Andreas", ""], ["Burns", "Randal", ""], ["Szalay", "Alex", ""]]}, {"id": "0901.4953", "submitter": "Marcelo Hashimoto", "authors": "Marcelo Hashimoto and Roberto M. Cesar Jr", "title": "A Keygraph Classification Framework for Real-Time Object Detection", "comments": "9 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach for keypoint-based object detection.\nTraditional keypoint-based methods consist in classifying individual points and\nusing pose estimation to discard misclassifications. Since a single point\ncarries no relational features, such methods inherently restrict the usage of\nstructural information to the pose estimation phase. Therefore, the classifier\nconsiders purely appearance-based feature vectors, thus requiring\ncomputationally expensive feature extraction or complex probabilistic modelling\nto achieve satisfactory robustness. In contrast, our approach consists in\nclassifying graphs of keypoints, which incorporates structural information\nduring the classification phase and allows the extraction of simpler feature\nvectors that are naturally robust. In the present work, 3-vertices graphs have\nbeen considered, though the methodology is general and larger order graphs may\nbe adopted. Successful experimental results obtained for real-time object\ndetection in video sequences are reported.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2009 19:38:44 GMT"}], "update_date": "2009-02-02", "authors_parsed": [["Hashimoto", "Marcelo", ""], ["Cesar", "Roberto M.", "Jr"]]}]